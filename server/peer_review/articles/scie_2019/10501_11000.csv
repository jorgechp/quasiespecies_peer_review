PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	RP	EM	RI	OI	FU	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	D2	EA	PG	WC	SC	GA	UT	PM	OA	HC	HP	DA
J								I Feel I Feel You: A Theory of Mind Experiment in Games	KUNSTLICHE INTELLIGENZ										Theory of mind; Affective computing; Digital games; Artificial agents; Preference learning	SOMATIC MARKER HYPOTHESIS; FRUSTRATION	In this study into the player's emotional theory of mind (ToM) of gameplaying agents, we investigate how an agent's behaviour and the player's own performance and emotions shape the recognition of a frustrated behaviour. We focus on the perception of frustration as it is a prevalent affective experience in human-computer interaction. We present a testbed game tailored towards this end, in which a player competes against an agent with a frustration model based on theory. We collect gameplay data, an annotated ground truth about the player's appraisal of the agent's frustration, and apply face recognition to estimate the player's emotional state. We examine the collected data through correlation analysis and predictive machine learning models, and find that the player's observable emotions are not correlated highly with the perceived frustration of the agent. This suggests that our subject's ToM is a cognitive process based on the gameplay context. Our predictive models-using ranking support vector machines-corroborate these results, yielding moderately accurate predictors of players' ToM.																	0933-1875	1610-1987				MAR	2020	34	1			SI		45	55		10.1007/s13218-020-00641-2		FEB 2020											
J								One Explanation Does Not Fit All The Promise of Interactive Explanations for Machine Learning Transparency	KUNSTLICHE INTELLIGENZ										Interactive; Personalised; Explanations; Counterfactuals		The need for transparency of predictive systems based on Machine Learning algorithms arises as a consequence of their ever-increasing proliferation in the industry. Whenever black-box algorithmic predictions influence human affairs, the inner workings of these algorithms should be scrutinised and their decisions explained to the relevant stakeholders, including the system engineers, the system's operators and the individuals whose case is being decided. While a variety of interpretability and explainability methods is available, none of them is a panacea that can satisfy all diverse expectations and competing objectives that might be required by the parties involved. We address this challenge in this paper by discussing the promises of Interactive Machine Learning for improved transparency of black-box systems using the example of contrastive explanations-a state-of-the-art approach to Interpretable Machine Learning. Specifically, we show how to personalise counterfactual explanations by interactively adjusting their conditional statements and extract additional explanations by asking follow-up "What if?" questions. Our experience in building, deploying and presenting this type of system allowed us to list desired properties as well as potential limitations, which can be used to guide the development of interactive explainers. While customising the medium of interaction, i.e., the user interface comprising of various communication channels, may give an impression of personalisation, we argue that adjusting the explanation itself and its content is more important. To this end, properties such as breadth, scope, context, purpose and target of the explanation have to be considered, in addition to explicitly informing the explainee about its limitations and caveats. Furthermore, we discuss the challenges of mirroring the explainee's mental model, which is the main building block of intelligible human-machine interactions. We also deliberate on the risks of allowing the explainee to freely manipulate the explanations and thereby extracting information about the underlying predictive model, which might be leveraged by malicious actors to steal or game the model. Finally, building an end-to-end interactive explainability system is a challenging engineering task; unless the main goal is its deployment, we recommend "Wizard of Oz" studies as a proxy for testing and evaluating standalone interactive explainability algorithms.																	0933-1875	1610-1987				JUN	2020	34	2			SI		235	250		10.1007/s13218-020-00637-y		FEB 2020											
J								Bregman Itoh-Abe Methods for Sparse Optimisation	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Non-convex optimisation; Non-smooth optimisation; Bregman iteration; Inverse scale space; Geometric numerical integration; Discrete gradient methods	DISCRETE GRADIENT METHODS; SCALE-SPACE; GEOMETRIC INTEGRATION; REGULARIZATION; ALGORITHMS; ITERATIONS	In this paper we propose optimisation methods for variational regularisation problems based on discretising the inverse scale space flow with discrete gradient methods. Inverse scale space flow generalises gradient flows by incorporating a generalised Bregman distance as the underlying metric. Its discrete-time counterparts, Bregman iterations and linearised Bregman iterations are popular regularisation schemes for inverse problems that incorporate a priori information without loss of contrast. Discrete gradient methods are tools from geometric numerical integration for preserving energy dissipation of dissipative differential systems. The resultant Bregman discrete gradient methods are unconditionally dissipative and achieve rapid convergence rates by exploiting structures of the problem such as sparsity. Building on previous work on discrete gradients for non-smooth, non-convex optimisation, we prove convergence guarantees for these methods in a Clarke subdifferential framework. Numerical results for convex and non-convex examples are presented.																	0924-9907	1573-7683				JUL	2020	62	6-7			SI		842	857		10.1007/s10851-020-00944-x		FEB 2020											
J								Crouzeix-Raviart Approximation of the Total Variation on Simplicial Meshes	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Image processing; Total variation; Non-conforming P1 (Crouzeix-Raviart) finite elements; Error estimates	TOTAL VARIATION MINIMIZATION; FINITE-ELEMENT METHODS; IMAGE; REGULARIZATION; CONVERGENCE; MODEL; INTERPOLATION; EVOLUTION; FLOW	We propose an adaptive implementation of a Crouzeix-Raviart-based discretization of the total variation, which has the property of approximating from below the total variation, with metrication errors only depending on the local curvature, rather than on the orientation as is usual for other approaches.																	0924-9907	1573-7683				JUL	2020	62	6-7			SI		872	899		10.1007/s10851-019-00939-3		FEB 2020											
J								Mapping for meaning: the embodied sonification listening model and its implications for the mapping problem in sonic information design	JOURNAL ON MULTIMODAL USER INTERFACES										Auditory display; Sonification; Conceptual metaphor; Image schema; Conceptual blending	METAPHOR	This is a theoretical paper that considers the mapping problem, a foundational issue which arises when designing a sonification, as it applies to sonic information design. We argue that this problem can be addressed by using models from the field of embodied cognitive science, including embodied image schema theory, conceptual metaphor theory and conceptual blends, and from research which treats sound and musical structures using these models, when mapping data to sound. However, there are currently very few theoretical frameworks for applying embodied cognition principles in a sonic information design context. This article describes one such framework, the embodied sonification listening model, which provides a theoretical description of sonification listening in terms of conceptual metaphor theory.																	1783-7677	1783-8738				JUN	2020	14	2			SI		143	151		10.1007/s12193-020-00318-y		FEB 2020											
J								VOSTR: Video Object Segmentation via Transferable Representations	INTERNATIONAL JOURNAL OF COMPUTER VISION										Video object segmentation; Transfer learning; Weakly-supervised learning		In order to learn video object segmentation models, conventional methods require a large amount of pixel-wise ground truth annotations. However, collecting such supervised data is time-consuming and labor-intensive. In this paper, we exploit existing annotations in source images and transfer such visual information to segment videos with unseen object categories. Without using any annotations in the target video, we propose a method to jointly mine useful segments and learn feature representations that better adapt to the target frames. The entire process is decomposed into three tasks: (1) refining the responses with fully-connected CRFs, (2) solving a submodular function for selecting object-like segments, and (3) learning a CNN model with a transferable module for adapting seen categories in the source domain to the unseen target video. We present an iterative update scheme between three tasks to self-learn the final solution for object segmentation. Experimental results on numerous benchmark datasets demonstrate that the proposed method performs favorably against the state-of-the-art algorithms.																	0920-5691	1573-1405				APR	2020	128	4			SI		931	949		10.1007/s11263-019-01224-x		FEB 2020											
J								Visual Social Relationship Recognition	INTERNATIONAL JOURNAL OF COMPUTER VISION										Social relationship; Label ambiguity; Context-driven analysis; Attention	PEOPLE	Social relationships form the basis of social structure of humans. Developing computational models to understand social relationships from visual data is essential for building intelligent machines that can better interact with humans in a social environment. In this work, we study the problem of visual social relationship recognition in images. We propose a dual-glance model for social relationship recognition, where the first glance fixates at the person of interest and the second glance deploys attention mechanism to exploit contextual cues. To enable this study, we curated a large scale People in Social Context dataset, which comprises of 23,311 images and 79,244 person pairs with annotated social relationships. Since visually identifying social relationship bears certain degree of uncertainty, we further propose an adaptive focal loss to leverage the ambiguous annotations for more effective learning. We conduct extensive experiments to quantitatively and qualitatively demonstrate the efficacy of our proposed method, which yields state-of-the-art performance on social relationship recognition.																	0920-5691	1573-1405				JUN	2020	128	6					1750	1764		10.1007/s11263-020-01295-1		FEB 2020											
J								RGB-IR Person Re-identification by Cross-Modality Similarity Preservation	INTERNATIONAL JOURNAL OF COMPUTER VISION										Person re-identification; Cross-modality model; RGB-infrared matching	FACE	Person re-identification (Re-ID) is an important problem in video surveillance for matching pedestrian images across non-overlapping camera views. Currently, most works focus on RGB-based Re-ID. However, RGB images are not well suited to a dark environment; consequently, infrared (IR) imaging becomes necessary for indoor scenes with low lighting and 24-h outdoor scene surveillance systems. In such scenarios, matching needs to be performed between RGB images and IR images, which exhibit different visual characteristics; this cross-modality matching problem is more challenging than RGB-based Re-ID due to the lack of visible colour information in IR images. To address this challenge, we study the RGB-IR cross-modality Re-ID (RGB-IR Re-ID) problem. Rather than applying existing cross-modality matching models that operate under the assumption of identical data distributions between training and testing sets to handle the discrepancy between RGB and IR modalities for Re-ID, we cast learning shared knowledge for cross-modality matching as the problem of cross-modality similarity preservation. We exploit same-modality similarity as the constraint to guide the learning of cross-modality similarity along with the alleviation of modality-specific information, and finally propose a Focal Modality-Aware Similarity-Preserving Loss. To further assist the feature extractor in extracting shared knowledge, we design a modality-gated node as a universal representation of both modality-specific and shared structures for constructing a structure-learnable feature extractor called Modality-Gated Extractor. For validation, we construct a new multi-modality Re-ID dataset, called SYSU-MM01, to enable wider study of this problem. Extensive experiments on this SYSU-MM01 dataset show the effectiveness of our method. Download link of dataset: https://github.com/wuancong/SYSU-MM01.																	0920-5691	1573-1405				JUN	2020	128	6					1765	1785		10.1007/s11263-019-01290-1		FEB 2020											
J								DRIT plus plus : Diverse Image-to-Image Translation via Disentangled Representations	INTERNATIONAL JOURNAL OF COMPUTER VISION												Image-to-image translation aims to learn the mapping between two visual domains. There are two main challenges for this task: (1) lack of aligned training pairs and (2) multiple possible outputs from a single input image. In this work, we present an approach based on disentangled representation for generating diverse outputs without paired training images. To synthesize diverse outputs, we propose to embed images onto two spaces: a domain-invariant content space capturing shared information across domains and a domain-specific attribute space. Our model takes the encoded content features extracted from a given input and attribute vectors sampled from the attribute space to synthesize diverse outputs at test time. To handle unpaired training data, we introduce a cross-cycle consistency loss based on disentangled representations. Qualitative results show that our model can generate diverse and realistic images on a wide range of tasks without paired training data. For quantitative evaluations, we measure realism with user study and Frechet inception distance, and measure diversity with the perceptual distance metric, Jensen-Shannon divergence, and number of statistically-different bins.																	0920-5691	1573-1405				NOV	2020	128	10-11			SI		2402	2417		10.1007/s11263-019-01284-z		FEB 2020											
J								Multi-label feature selection based on information entropy fusion in multi-source decision system	EVOLUTIONARY INTELLIGENCE										Multi-source data; Multi-label feature selection; Rough set; Information fusion	ROUGH SETS; APPROXIMATION; ALGORITHM; MODEL	Feature selection plays an important role in high-dimensional multi-source data, which can improve classification performance of learning algorithm. Most of existing multi-source information fusion focus on the single decision system without considering multi-source and multi-label problems together. Nevertheless, data from different sources along with multiple labels simultaneously are absolutely frequent in many real-world applications. For this issue, in this paper, a multi-source multi-label decision system is proposed, which has more than one decision label. To remove some redundant or irrelevant features in multi-source multi-label decision system, a feature selection algorithm based on positive region for multi-source multi-label data is explored, which uses the feature dependency carried on the fusion decision table. Finally, examples are introduced to elaborate the detail process of the proposed algorithm, and experimental results show the effective performance of the proposed algorithm on multi-source and multi-label data.																	1864-5909	1864-5917				JUN	2020	13	2			SI		255	268		10.1007/s12065-019-00349-9		FEB 2020											
J								Statistical Inter-stimulus Interval Window Estimation for Transient Neuromodulation via Paired Mechanical and Brain Stimulation	FRONTIERS IN NEUROROBOTICS										brain stimulation; transcranial magnetic stimulation; mechanical stimulation; motor evoked potential; statistical estimation	MOTOR CORTEX EXCITABILITY; LONG-TERM POTENTIATION; CORTICOSPINAL EXCITABILITY; SENSORIMOTOR INTEGRATION; FACILITATION EXERCISE; CONVALESCENT PATIENTS; INTENSIVE REPETITION; AFFERENT INHIBITION; FUNCTIONAL RECOVERY; NERVE-CONDUCTION	For achieving motor recovery in individuals with sensorimotor deficits, augmented activation of the appropriate sensorimotor system, and facilitated induction of neural plasticity are essential. An emerging procedure that combines peripheral nerve stimulation and its associative stimulation with central brain stimulation is known to enhance the excitability of the motor cortex. In order to effectively apply this paired stimulation technique, timing between central and peripheral stimuli must be individually adjusted. There is a small range of effective timings between two stimuli, or the inter-stimulus interval window (ISI-W). Properties of ISI-W from neuromodulation in response to mechanical stimulation (Mstim) of muscles have been understudied because of the absence of a versatile and reliable mechanical stimulator. This paper adopted a combination of transcranial magnetic stimulation (TMS) and Mstim by using a high-precision robotic mechanical stimulator. A pneumatically operated robotic tendon tapping device was applied. A low-friction linear cylinder achieved high stimulation precision in time and low electromagnetic artifacts in physiological measurements. This paper describes a procedure to effectively estimate an individual ISI-W from the transiently enhanced motor evoked potential (MEP) with a reduced number of paired Mstim and sub-threshold TMS trials by applying statistical sampling and regression technique. This paper applied a total of four parametric and non-parametric statistical regression methods for ISI-W estimation. The developed procedure helps to reduce time for individually adjusting effective ISI, reducing physical burden on the subject.																	1662-5218					FEB 3	2020	14								1	10.3389/fnbot.2020.00001													
J								Horizontal gene transfer for recombining graphs	GENETIC PROGRAMMING AND EVOLVABLE MACHINES										Graph-based genetic programming; Neuroevolution; Horizontal gene transfer	EVOLUTION	We introduce a form of neutral horizontal gene transfer (HGT) to evolving graphs by graph programming (EGGP). We introduce the mu x lambda evolutionary algorithm (EA), where mu parents each produce lambda children who compete only with their parents. HGT events then copy the entire active component of one surviving parent into the inactive component of another parent, exchanging genetic information without reproduction. Experimental results from symbolic regression problems show that the introduction of the mu x lambda EA and HGT events improve the performance of EGGP. Comparisons with genetic programming and Cartesian genetic programming strongly favour our proposed approach. We also investigate the effect of using HGT events in neuroevolution tasks. We again find that the introduction of HGT improves the performance of EGGP, demonstrating that HGT is an effective cross-domain mechanism for recombining graphs.																	1389-2576	1573-7632				SEP	2020	21	3			SI		321	347		10.1007/s10710-020-09378-1		FEB 2020											
J								Fault-Tolerant Synchronization of Chaotic Systems with Fuzzy Sampled Data Controller Based on Adaptive Event-Triggered Scheme	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Adaptive event-triggered scheme; Fault-tolerant synchronization; Takagi-Sugeno (T-S) fuzzy model; Master-slave systems	NETWORKS; COMMUNICATION	This paper deals with the problem of the fault-tolerant fuzzy master-slave systems synchronization through an adaptive event-triggered scheme (AETS). First, a Takagi-Sugeno (T-S) fuzzy model is employed to represent the master-slave system dynamics. Second, an AETS is introduced to judge whether the newly sampled controller's signals should be released to the slave system or not. Consequently, the less computation resources and network bandwidth are utilized under the AETS. Meanwhile, a novel adaptive law is designed to achieve the threshold of event-triggering condition on-line. Third, a novel fuzzy controller is designed, containing a state feedback controller and a fault compensator to achieve the fault-tolerant synchronization, which is formulated to study the global asymptotical stability of the error system. As a results, applying Lyapunov theory and inequality technique, new sufficient condition is obtained to guarantee the stability of the error system. Further, the controller gain and the weight of event-triggering condition are designed through linear matrix inequalities (LMIs) approach. Finally, a numerical simulation example is employed to demonstrate the practical utility of this method.																	1562-2479	2199-3211				APR	2020	22	3					917	929		10.1007/s40815-019-00786-9		FEB 2020											
J								Optimal Design of Adaptive Robust Control for Bounded Constraint-Following Error in Fuzzy Mechanical Systems	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Fuzzy mechanical systems; Bounded constraint-following; Fuzzy set theory; Adaptive robust control; Optimal design	ARRIVAL PERFORMANCE; IDENTIFICATION	This paper proposes an optimal indirect approach for asymmetric bounds in constraint-following in mechanical systems with (possibly fast) time-varying fuzzy uncertainty. The uncertainty is described with fuzzy set theory. We aim at optimal controller to drive the constraint-following error of the concerned fuzzy system to lie within a desired (possibly asymmetric) bound all the time and get to be sufficiently small eventually. For deterministic performance, we transform the fuzzy original system into a constructed fuzzy system, for which a deterministic (not the usual if-then rules-based) adaptive robust control is designed for uniform boundedness and uniform ultimate boundedness. For optimal performance, a performance index is proposed based on the fuzzy information, by minimizing which an optimal control gain design problem is formulated and solved. When the constructed fuzzy system is uniform boundedness and uniform ultimate boundedness, the constraint-following error of the original fuzzy system is proved to be bounded. As a result, the control design can render out deterministic performance and minimum performance index.																	1562-2479	2199-3211				APR	2020	22	3					970	984		10.1007/s40815-019-00792-x		FEB 2020											
J								Improved Delay-Range-Dependent Stability Condition for T-S Fuzzy Systems with Variable Delays Using New Extended Affine Wirtinger Inequality	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										DRD stability; T-S fuzzy model; LKF; LMI	TIME-VARYING DELAY; H-INFINITY CONTROL; STABILIZATION CONDITIONS; ROBUST STABILITY; INTEGRAL-INEQUALITIES; CRITERIA; DESIGN; CONTROLLER	In this paper, a new integral inequality lemma along with an appropriate Lyapunov-Krasovskii functional (LKF) is proposed for fuzzy time-delay system to enhance the delay upper bound estimate. The proposed lemma is referred to hereafter as Extended Affine Wirtinger inequality. The novelty of the lemma is twofold, it has the ability to integrate uncertain delay information utilizing the convex combination of certain and uncertain delay intervals involved in the proposed LKF, and it is compatible to derive delay-range-dependent (DRD) stability conditions for continuous time Takagi-Sugeno (T-S) fuzzy time-delay system. One noteworthy advantage of the proposed stability condition in a linear matrix inequality (LMI) framework is that it requires less number of matrix variables compared to the existing integral inequalities of the similar type, thus reducing the computational burden too. The efficacy of the proposed DRD stability condition over existing conditions is validated numerically by solving three examples related to fuzzy time-delay system.																	1562-2479	2199-3211				APR	2020	22	3					985	998		10.1007/s40815-019-00795-8		FEB 2020											
J								The examination of the effect of the criterion for neural network's learning on the effectiveness of the qualitative analysis of multidimensional data	KNOWLEDGE AND INFORMATION SYSTEMS										Multidimensional data analysis; Data mining; Multidimensional visualization; Self-organized neural network; Autoassociative neural network	CLASSIFICATION; VISUALIZATION; REDUCTION	A variety of multidimensional visualization methods are applied for the qualitative analysis of multidimensional data. One of the multidimensional data visualization methods is a method using autoassociative neural networks. In order to perform visualizations of n-dimensional data, such a network has n inputs, n outputs and one of the interlayers consisting of two outputs whose values represent coordinates of the analyzed sample's image on the screen. Such a criterion for the network's learning consists in that the same value as the one at the ith input appears at each ith output. If the network is trained in this way, the whole information from n inputs was compressed to two outputs of the interlayer and then decompressed to n network outputs. The paper shows the application of different learning criteria can be more beneficial from the point of view of the results' readability. Overall analysis was conducted on seven-dimensional real data representing three coal classes, five-dimensional data representing printed characters, 216-dimensional data representing hand-written digits and, additionally, in order to illustrate additional explanations using artificially generated seven-dimensional data. Readability of results of the qualitative analysis of these data was compared using the multidimensional visualization utilizing neural networks for different learning criteria. Also, the obtained results of applying all analyzed criteria on 20 randomly selected sets of multidimensional data obtained from one of the publicly available repositories are presented.																	0219-1377	0219-3116				AUG	2020	62	8					3263	3289		10.1007/s10115-020-01441-8		FEB 2020											
J								A comparative study between artificial bee colony (ABC) algorithm and its variants on big data optimization	MEMETIC COMPUTING										ABC algorithm; Big data optimization; Signal decomposition	COOPERATIVE COEVOLUTION; PROMISES	The big data term and its formal definition have changed the properties of some of the computational problems. One of the problems for which the fundamental properties change with the existence of the big data is the optimization problems. Artificial bee colony (ABC) algorithm inspired by the intelligent source search, consumption and communication characteristics of the real honey bees has proven its efficiency on solving different numerical and combinatorial optimization problems. In this study, the standard ABC algorithm and its well-known variants including the gbest-guided ABC algorithm, the differential evolution based ABC/best/1 and ABC/best/2 algorithms, crossover ABC algorithm, converge-onlookers ABC algorithm and quick ABC algorithm were assessed using the electroencephalographic signal decomposition based optimization problems introduced at the 2015 Congress on Evolutionary Computing Big Data Competition. The experimental studies on solving big data optimization problems showed that the phase-divided structure of the standard ABC algorithm still protects its advantageous sides when the candidate food sources or solutions are generated by referencing the global best solution in the onlooker bee phase.																	1865-9284	1865-9292				JUN	2020	12	2					129	150		10.1007/s12293-020-00298-2		FEB 2020											
J								Designing pulse-coupled neural networks with spike-synchronization-dependent plasticity rule: image segmentation and memristor circuit application	NEURAL COMPUTING & APPLICATIONS										Spike-synchronization-dependent plasticity; Pulse-coupled neural network; Memristor-based circuit; Biomedical image segmentation	GAMMA-BAND SYNCHRONIZATION; SOMATOSENSORY CORTEX; FEATURE-LINKING; MODEL; AREA; MODULATION; PASSIVITY; PCNN	Pulse-coupled neural network (PCNN) is a powerful unsupervised learning model with many parameters to be determined empirically. In particular, the weight matrix is invariable in the iterative process, which is inconsistent with the actual biological system. Based on the existing research foundation of biology and neural network, we propose a spike-synchronization-dependent plasticity (SSDP) rule. In this paper, the mathematical model and algorithm of SSDP are presented. Furthermore, a novel memristor-based circuit model of SSDP is designed. Finally, experimental results demonstrate that SSDP has greatly improved the image processing capabilities of PCNN.																	0941-0643	1433-3058				SEP	2020	32	17					13441	13452		10.1007/s00521-020-04752-7		FEB 2020											
J								Efficient and hardware-friendly methods to implement competitive learning for spiking neural networks	NEURAL COMPUTING & APPLICATIONS										Spiking neural network; Homeostasis; Lateral inhibition; Competitive learning; Memristor	SYNAPTIC PLASTICITY; STDP; CLASSIFICATION; NEURONS; DEVICE	Spiking neural network (SNN) trained by spike-timing-dependent plasticity (STDP) is a promising computing paradigm for energy-efficient artificial intelligence systems. During the learning procedure of SNN trained by STDP, another two bio-inspired mechanisms of lateral inhibition and homeostasis are usually implemented to achieve competitive learning. However, the previous methods to implement lateral inhibition and homeostasis are not designed with hardware in mind, resulting in solutions that are not efficient for deployment on neuromorphic hardware. For example, the existing lateral inhibition methods induce a great number of connections that are proportional to the square of the number of learning neurons. The classical homeostasis methods depend on the fine-tuned membrane threshold with no hardware solution provided. In this paper, we propose two hardware-friendly and scalable methods to achieve lateral inhibition and homeostasis. Using only one inhibitory neuron for one learning layer, our proposed lateral inhibition method can reduce inhibitory connection number from N-2 to 2N and hardware overhead by sharing refractory control circuits. Utilizing the adaptive resistance of memristor, we propose a novel homeostasis method through adapting the leaky current of spiking neurons. In addition, the learning efficiency of different homeostasis methods are studied for the first time by simulating on the cognitive task of digital recognition of MNIST dataset. Simulation results show that our proposed homeostasis method can improve the learning efficiency by 30-50% while maintaining the state-of-the-art performance.																	0941-0643	1433-3058				SEP	2020	32	17					13479	13490		10.1007/s00521-020-04755-4		FEB 2020											
J								Analysis of collective action propagation with multiple recurrences	NEURAL COMPUTING & APPLICATIONS										Collective action propagation; Recurrence and burst; Complex network; Crowdfunding; Online content	NETWORKS	Collective action propagation, which can be as large as billions of people adopting Facebook or as small as a few researchers citing a paper, exists in various real-life scenarios. Here, we perform a large-scale investigation of collective action propagation with "recurrence" phenomena. We consider actions that propagate in a social network with multiple communities and find the growth in the propagation breadth of collective action can be explained by a simple mathematical model with an analytical solution. We use datasets on the growth of total views of TED and YouTube videos, the prize pool of Dota 2 tournaments, and a total gross of movies to investigate collective action propagation with recurrence phenomena. Experimental results reveal that our model can capture universal features of collective action propagation, validating the idea that collective action propagation with recurrence results from an action being transmitted from communities to communities.																	0941-0643	1433-3058				SEP	2020	32	17					13491	13504		10.1007/s00521-020-04756-3		FEB 2020											
J								Global exponential anti-synchronization for delayed memristive neural networks via event-triggering method	NEURAL COMPUTING & APPLICATIONS										Memristive neural network; Time-varying delay; Event-triggered control; Global exponential anti-synchronization; Zeno behavior	FINITE-TIME SYNCHRONIZATION; EXTREME LEARNING-MACHINE; OPERATION RULE; PASSIVITY; PASSIFICATION; STABILIZATION; DISSIPATIVITY; STABILITY	This paper studies the exponential anti-synchronization problem of memristive delayed neural networks under the event-triggered controller. To reduce the recalculation of the control signals, two event-triggered control strategies including static and dynamic are proposed. A novel Lyapunov function is constructed to analyze the global exponential anti-synchronization problem. By analysis, we can choose the suitable parameter of the controller to realize global exponential anti-synchronization with a given convergence rate gamma without wasting a lot of control resources. Moreover, under event-triggering conditions given in our theorem, we derive that the Zeno behavior will not happen. Finally, numerical examples are given to validate our theorem.																	0941-0643	1433-3058				SEP	2020	32	17					13521	13535		10.1007/s00521-020-04762-5		FEB 2020											
J								Evidence of power-law behavior in cognitive IoT applications	NEURAL COMPUTING & APPLICATIONS										Internet of things; Wireless sensor networks; Power-law; Scalability; Interconnectivity	SMALL-WORLD; STATISTICAL-MECHANICS; ATTACK TOLERANCE; FREQUENCY; NETWORK; GENERATION; ERROR	The motivations induced due to the presence of scale-free characteristics of neural systems governed by the well-known power-law distribution of neuronal activities have led to its convergence with the Internet of things (IoT) framework. The IoT is one such framework, where the self-organization of the connected devices is a momentous aspect. The devices involved in these networks inherently relate to the collection of several consolidated devices like the sensory devices, consumer appliances, wearables, and other associated applications, which facilitate a ubiquitous connectivity among the devices. This is one of the most significant prerequisites of IoT systems as several interconnected devices need to be included in the convolution for the uninterrupted execution of the services. Thus, in order to understand the scalability and the heterogeneity of these interconnected devices, the exponent of power-law plays a significant role. In this paper, an analytical framework to illustrate the ubiquitous power-law behavior of the IoT devices is derived. An emphasis regarding the mathematical insights for the characterization of the dynamic behavior of these devices is conceptualized. The observations made in this direction are illustrated through simulation results. Further, the traits of the wireless sensor networks, in context with the contemporary scale-free architecture, are discussed.																	0941-0643	1433-3058				OCT	2020	32	20			SI		16043	16055		10.1007/s00521-020-04705-0		FEB 2020											
J								Multilingual audio information management system based on semantic knowledge in complex environments	NEURAL COMPUTING & APPLICATIONS										Evolutionary computing; Artificial neural networks; Internet information management; Management of complex systems	SPEECH RECOGNITION; BROADCAST NEWS; MODELS	This paper proposes a multilingual audio information management system based on semantic knowledge in complex environments. The complex environment is defined by the limited resources (financial, material, human, and audio resources); the poor quality of the audio signal taken from an internet radio channel; the multilingual context (Spanish, French, and Basque that is in under-resourced situation in some areas); and the regular appearance of cross-lingual elements between the three languages. In addition to this, the system is also constrained by the requirements of the local multilingual industrial sector. We present the first evolutionary system based on a scalable architecture that is able to fulfill these specifications with automatic adaptation based on automatic semantic speech recognition, folksonomies, automatic configuration selection, machine learning, neural computing methodologies, and collaborative networks. As a result, it can be said that the initial goals have been accomplished and the usability of the final application has been tested successfully, even with non-experienced users.																	0941-0643	1433-3058															10.1007/s00521-019-04618-7		FEB 2020											
J								LASSO multi-objective learning algorithm for feature selection	SOFT COMPUTING										Supervised learning; Feature selection; Multi-objective; LASSO		This work proposes a new algorithm for training neural networks to solve the problems of feature selection and function approximation. The algorithm applies different weight constraint functions for the hidden and the output layers of a multilayer perceptron neural network. The LASSO operator is applied to the hidden layer; therefore, the training provides automatic selection of relevant features and the standard norm regularization function is applied to the output layer. Therefore, we propose a multi-objective training algorithm that is able to select the important features while solving the approximation problem.																	1432-7643	1433-7479				SEP	2020	24	17					13209	13217		10.1007/s00500-020-04734-w		FEB 2020											
J								Topological residuated lattices	SOFT COMPUTING										Residuated lattice; Topological residuated lattice; Filter; Separation axioms; Completion; Linear topology		The notion of a (semi)topological residuated lattice is introduced, and its properties are investigated. Some separation axioms on topological residuated lattices are studied. The notion of completion of a residuated lattice is introduced and characterized by means of the inverse limit of an inverse system. A residuated lattice with a given system of filters is illustrated with the linear topology, and it is shown that a compact and Hausdorff residuated lattice with the linear topology is complete.																	1432-7643	1433-7479															10.1007/s00500-020-04709-x		FEB 2020											
J								Image-based effective feature generation for protein structural class and ligand binding prediction	PEERJ COMPUTER SCIENCE										Protein structural class prediction; Protein-ligand binding; Image-based features; Similarity based supervised learning algorithms	RETRIEVAL-SYSTEM; CLASSIFICATION; SCALE	Proteins are the building blocks of all cells in both human and all living creatures of the world. Most of the work in the living organism is performed by proteins. Proteins are polymers of amino acid monomers which are biomolecules or macromolecules. The tertiary structure of protein represents the three-dimensional shape of a protein. The functions, classification and binding sites are governed by the protein's tertiary structure. If two protein structures are alike, then the two proteins can be of the same kind implying similar structural class and ligand binding properties. In this paper, we have used the protein tertiary structure to generate effective features for applications in structural similarity to detect structural class and ligand binding. Firstly, we have analyzed the effectiveness of a group of image-based features to predict the structural class of a protein. These features are derived from the image generated by the distance matrix of the tertiary structure of a given protein. They include local binary pattern (LBP) histogram, Gabor filtered LBP histogram, separate row multiplication matrix with uniform LBP histogram, neighbor block subtraction matrix with uniform LBP histogram and atom bond. Separate row multiplication matrix and neighbor block subtraction matrix filters, as well as atom bond, are our novels. The experiments were done on a standard benchmark dataset. We have demonstrated the effectiveness of these features over a large variety of supervised machine learning algorithms. Experiments suggest support vector machines is the best performing classifier on the selected dataset using the set of features. We believe the excellent performance of Hybrid LBP in terms of accuracy would motivate the researchers and practitioners to use it to identify protein structural class. To facilitate that, a classification model using Hybrid LBP is readily available for use at http://brl.uiu.ac.bd/PL/. Protein-ligand binding is accountable for managing the tasks of biological receptors that help to cure diseases and many more. Therefore, binding prediction between protein and ligand is important for understanding a protein's activity or to accelerate docking computations in virtual screening- based drug design. Protein-ligand binding prediction requires three-dimensional tertiary structure of the target protein to be searched for ligand binding. In this paper, we have proposed a supervised learning algorithm for predicting protein-ligand binding, which is a similarity-based clustering approach using the same set of features. Our algorithmworks better than the most popular and widely used machine learning algorithms.																	2376-5992					FEB 3	2020									e253	10.7717/peerj-cs.253													
J								Global exponential stability on anti-periodic solutions in proportional delayed HIHNNs	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Proportional delayed high-order inertial Hopfield neural networks; anti-periodic solution; generalised exponential stability; non-reduced order method	INERTIAL NEURAL-NETWORKS; PERIODIC-SOLUTIONS; LIMIT-CYCLES; DYNAMICS; SYSTEMS; EXISTENCE; EQUATIONS; SYNCHRONIZATION; CONTROLLABILITY; MODEL	This article mainly explores a class of proportional delayed high-order inertial Hopfield neural networks (HIHNNs) with time-varying coefficients. Without using the reduced-order method, by combining Lyapunov function method with differential inequality approach, some sufficient criteria on the existence of anti-periodic solutions including its uniqueness and exponential stability are built up. The obtained results provide us some lights for designing a stable HIHNNs and complement some earlier publications. In addition, simulations show that the theoretical anti-periodic dynamics are in excellent agreement with the numerically observed behaviour.																	0952-813X	1362-3079															10.1080/0952813X.2020.1721571		FEB 2020											
J								Mobile phone data statistics as a dynamic proxy indicator in assessing regional economic activity and human commuting patterns	EXPERT SYSTEMS										commuting; efficiency; principal component analysis	POSITIONING DATA; TRAJECTORIES; MODEL; FLOW	Various studies demonstrate that data on mobile phone use are useful when analysing problems in the fields of human activity or population dynamics, including tourism, transportation planning, public administration, etc. However, one of the biggest challenges is related to the restrictions contained in the General Data Protection Regulation that force the use of statistics about mobile operator client activities instead of allowing the analysis of mobile operator data. Therefore, a data analytics approach that does not involve information on the mobility of particular persons was developed, providing economically relevant data on aggregate mobility while protecting personal data. The activity data aggregation was conducted at 15-min intervals in the area of each cellular base station; "activity" is defined as the number of outgoing and incoming calls and sent and received text messages (short message service or SMS) and, in some instances, as the count of unique users. The case study examines all of Latvia's municipalities, analysing the economic activity level in each municipality in comparison to the mobile phone activity in three periods: 2015-2016, 2017, and 2018. It was concluded that the economic activity in municipalities can be estimated, and positive dynamics of regional development have been detected. Such data and the data analytics method, which provides an understanding of how economic activities evolve in real time in particular locations and economic activity centres, can improve regional development planning and plan implementation. In order to assess which are the centres of economic activity in each municipality and its sphere of influence, the patterns of human commuting and fluctuations of internal activity on workdays and weekends/holidays in 2017-2018 were determined. In general, there is a shortage of reliable data on human commuting within Latvia and its specific regions; therefore, the method described here provides a practical tool for regional governments to keep track of strategy implementation and for strategic gap analysis.																	0266-4720	1468-0394				OCT	2020	37	5			SI				e12530	10.1111/exsy.12530		FEB 2020											
J								Using Virtual Programming Lab to improve learning programming: The case of Algorithms and Programming	EXPERT SYSTEMS										automatic assessment; Java; teaching programming; virtual programming lab (VPL); eduScrum		Programming is one of the basic skills that students must acquire. However, learning to program is not an easy task. Also teaching programming is an arduous but challenging task, requiring close follow-up and constant and meaningful feedback. So the main question is: how can we help teachers and students to achieve these goals? We identified a tool that can be useful to this purpose. That is Virtual Programming Lab (VPL), a Moodle plugin that allows students to submit their code and get prompt feedback without the teacher's intervention. In order to test this concept, an experiment was performed with several classes of beginner programming students, in two editions of Algorithms and Programming course unit of the degree in Informatics Engineering lectured at the Informatics Engineering Department at the School of Engineering, Polytechnic Institute of Porto. The students were challenged to test their assignments in VPL with a set of test values previously defined by the teachers. After the experiments, we used surveys to gather the involved students' and teachers' opinion, and more than 70% of the students answered that they considered the VPL an added value for the teaching-learning process. The dynamics verified in the classes, the general opinion of the teachers, and the acceptance and participation of the students allow to classify the experience as positive.																	0266-4720	1468-0394														e12531	10.1111/exsy.12531		FEB 2020											
J								An ensemble just-in-time learning soft-sensor model for residual lithium concentration prediction of ternary cathode materials	JOURNAL OF CHEMOMETRICS										just-in-time learning; moving window; prediction of residual lithium content; semisupervised learning	COMPONENT ANALYSIS; REGRESSION-MODEL; DYNAMIC-SYSTEM	The surface-free lithium content, which is a critical index that reflecting the quality of ternary cathode materials, usually cannot be monitored in real time due to technical restriction. To this end, soft sensor technique is applied to predict the surface-free lithium content by readily available process variables, making timely control on operation parameters. In this paper, a modeling method based on ensemble just-in-time learning (JITL) soft-sensor is developed. In the model, data feature indices are firstly designed to properly extract the real-time and short-time features between the batching and the loading procedure. Accordingly, ensemble JITL soft-sensor model is constructed with the semisupervised local weighted probability principal component regression. Moreover, considering varying working conditions, an adaptive moving window technique is adopted to improve the adaptability of the model. The validation and the flexibility of the developed modeling method are testified with the practical manufacturing data.																	0886-9383	1099-128X				MAY	2020	34	5							e3225	10.1002/cem.3225		FEB 2020											
J								Mixtures of QSAR models: Learning application domains of pK a predicto rs	JOURNAL OF CHEMOMETRICS										ensemble models; mixture of models; model selection; pK(a) prediction	QUANTITATIVE STRUCTURE-ACTIVITY; APPLICABILITY DOMAIN; NEURAL-NETWORK; DRUG DESIGN; EXPERTS; VALUES	Quantitative structure-activity relationship models (QSAR models) predict the physical properties or biological effects based on physicochemical properties or molecular descriptors of chemical structures. Our work focuses on the construction of optimal linear and nonlinear weighted mixes of individual QSAR models to more accurately predict their performance. How the splitting of the application domain by a nonlinear gating network in a "mixture of experts" model structure is suitable for the determination of the optimal domain-specific QSAR model and how the optimal QSAR model for certain chemical groups can be determined is highlighted. The input of the gating network is arbitrarily formed by the various molecular structure descriptors and/or even the prediction of the individual QSAR models. The applicability of the method is demonstrated on the pK a values of the OASIS database (1912 chemicals) by the combination of four acidic pK a predictions of the OECD QSAR Toolbox. According to the results, the prediction performance was enhanced by more than 15% (root-mean-square error [RMSE] value) compared with the predictions of the best individual QSAR model.																	0886-9383	1099-128X														e3223	10.1002/cem.3223		FEB 2020											
J								Heterofusion: Fusing genomics data of different measurement scales	JOURNAL OF CHEMOMETRICS										data fusion; data integration; heterogeneous data; low-level fusion; measurement scales	LEAST-SQUARES METHOD; STATISTICS; BREAST	In systems biology, it is becoming increasingly common to measure biochemical entities at different levels of the same biological system. Hence, data fusion problems are abundant in the life sciences. With the availability of a multitude of measuring techniques, one of the central problems is the heterogeneity of the data. In this paper, we discuss a specific form of heterogeneity, namely, that of measurements obtained at different measurement scales, such as binary, ordinal, interval, and ratio-scaled variables. Three generic fusion approaches are presented of which two are new to the systems biology community. The methods are presented, put in context, and illustrated with a real-life genomics example.																	0886-9383	1099-128X														e3200	10.1002/cem.3200		FEB 2020											
J								On the avoidance of crossing of singular values in the evolving factor analysis	JOURNAL OF CHEMOMETRICS										avoidance of crossing; EFA plot; evolving factor analysis	RESOLUTION	Evolving factor analysis (EFA) investigates the evolution of the singular values of matrices formed by a series of measured spectra, typically, resulting from the spectral observation of an ongoing chemical process. In the original EFA, the logarithms of the singular values are plotted for submatrices that include an increasing number of spectra. A typical observation in these plots is that pairs of trajectories of the singular values are on a collision course, but finally, the curves seem to repel each other and then run in different directions. For parameter-dependent square matrices, such a behaviour is known for the eigenvalues under the keyword of an avoidance of crossing. Here, we adjust the explanation of this avoidance of crossing to the curves of singular values of EFA. Further, a condition is studied that breaks this avoidance of crossing. We demonstrate that the understanding of this noncrossing allows us to design model data sets with a predictable crossing behaviour.																	0886-9383	1099-128X				MAY	2020	34	5							e3217	10.1002/cem.3217		FEB 2020											
J								Deep learning robotic guidance for autonomous vascular access	NATURE MACHINE INTELLIGENCE											PERIPHERAL VENOUS ACCESS; ULTRASOUND GUIDANCE; COMPLICATIONS; CARE; 3D; DEVICE; LEVEL; TIME; SKIN	Medical robots have demonstrated the ability to manipulate percutaneous instruments into soft tissue anatomy while working beyond the limits of human perception and dexterity. Robotic technologies further offer the promise of autonomy in carrying out critical tasks with minimal supervision when resources are limited. Here, we present a portable robotic device capable of introducing needles and catheters into deformable tissues such as blood vessels to draw blood or deliver fluids autonomously. Robotic cannulation is driven by predictions from a series of deep convolutional neural networks that encode spatiotemporal information from multimodal image sequences to guide real-time servoing. We demonstrate, through imaging and robotic tracking studies in volunteers, the ability of the device to segment, classify, localize and track peripheral vessels in the presence of anatomical variability and motion. We then evaluate robotic performance in phantom and animal models of difficult vascular access and show that the device can improve success rates and procedure times compared to manual cannulations by trained operators, particularly in challenging physiological conditions. These results suggest the potential for autonomous systems to outperform humans on complex visuomotor tasks, and demonstrate a step in the translation of such capabilities into clinical use. Getting safe and fast access to blood vessels is vital to many methods of treatment and diagnosis in medicine. Robot-assisted or even fully autonomous methods can potentially do the task more reliably than humans, especially when veins are hard to detect. In this work, a method is tested that uses deep learning to find blood vessels and track the movement of a patient's arm.																		2522-5839				FEB	2020	2	2					104	+		10.1038/s42256-020-0148-7													
J								A topology-based network tree for the prediction of protein-protein binding affinity changes following mutation	NATURE MACHINE INTELLIGENCE											ANTIBODY THERAPEUTICS; WEB SERVER; DATABASE; FORCE; AB	The ability to predict protein-protein interactions is crucial to our understanding of a wide range of biological activities and functions in the human body, and for guiding drug discovery. Despite considerable efforts to develop suitable computational methods, predicting protein-protein interaction binding affinity changes following mutation (Delta Delta G) remains a severe challenge. Algebraic topology, a champion in recent worldwide competitions for protein-ligand binding affinity predictions, is a promising approach to simplifying the complexity of biological structures. Here we introduce element- and site-specific persistent homology (a new branch of algebraic topology) to simplify the structural complexity of protein-protein complexes and embed crucial biological information into topological invariants. We also propose a new deep learning algorithm called NetTree to take advantage of convolutional neural networks and gradient-boosting trees. A topology-based network tree is constructed by integrating the topological representation and NetTree for predicting protein-protein interaction Delta Delta G. Tests on major benchmark datasets indicate that the proposed topology-based network tree is an important improvement over the current state of the art in predicting Delta Delta G. Persistent homology provides an efficient approach to simplifying the complexity of protein structure. Wang et al. combine this approach with convolutional neural networks and gradient-boosting trees to improve predictions of protein-protein interactions.																		2522-5839				FEB	2020	2	2					116	123		10.1038/s42256-020-0149-6													
J								Deep learning of circulating tumour cells	NATURE MACHINE INTELLIGENCE											METASTATIC BREAST-CANCER; PREDICT PROGRESSION-FREE; SURVIVAL; CHEMOTHERAPY; BEVERLY-2; DISEASE	Circulating tumour cells (CTCs) found in the blood of cancer patients are a promising biomarker in precision medicine. However, their use is currently hindered by their low frequency, tedious manual scoring and extensive cell heterogeneities. Those challenges limit the effectiveness of classical machine-learning methods for automated CTC analysis. Here, we combine autoencoding convolutional neural networks with advanced visualization techniques. This provides a very informative view on the data that opens the way for new biomedical research questions. We unravel hidden information in the raw image data of fluorescent images of blood samples enriched for CTCs. Our network classifies fluorescent images of single cells in five different classes with an accuracy, sensitivity and specificity of over 96%, and the obtained CTC counts predict the overall survival of cancer patients as well as state-of-the-art manual counts. Moreover, our network excelled in identifying different important subclasses of objects. Deep learning was faster and superior to classical image analysis approaches and enabled the identification of new biological phenomena. Counting different types of circulating tumour cells can give valuable information on the severity of the disease and on whether treatments are effective for a specific patient. In this work, the authors show that their method based on autoencoders can identify and count cells more accurately and faster than human experts.																		2522-5839				FEB	2020	2	2					124	+		10.1038/s42256-020-0153-x													
J								Predicting drug-protein interaction using quasi-visual question answering system	NATURE MACHINE INTELLIGENCE												Identifying novel drug-protein interactions is crucial for drug discovery. For this purpose, many machine learning-based methods have been developed based on drug descriptors and one-dimensional protein sequences. However, protein sequences cannot accurately reflect the interactions in three-dimensional space. However, direct input of three-dimensional structure is of low efficiency due to the sparse three-dimensional matrix, and is also prevented by the limited number of co-crystal structures available for training. Here we propose an end-to-end deep learning framework to predict the interactions by representing proteins with a two-dimensional distance map from monomer structures (Image) and drugs with molecular linear notation (String), following the visual question answering mode. For efficient training of the system, we introduce a dynamic attentive convolutional neural network to learn fixed-size representations from the variable-length distance maps and a self-attentional sequential model to automatically extract semantic features from the linear notations. Extensive experiments demonstrate that our model obtains competitive performance against state-of-the-art baselines on the directory of useful decoys, enhanced (DUD-E), human and BindingDB benchmark datasets. Further attention visualization provides biological interpretation to depict highlighted regions of both protein and drug molecules. When predicting the interaction of proteins with potential drugs, the protein can be encoded as its one-dimensional sequence or a three-dimensional structure, which can capture more relevant features of the protein, but also makes the task to predict the interactions harder. A new method predicts these interactions using a two-dimensional distance matrix representation of a protein, which can be processed like a two-dimensional image, striking a balance between the data being simple to process and rich in relevant structures.																		2522-5839				FEB	2020	2	2					134	140		10.1038/s42256-020-0152-y													
J								Deep-learning-based prediction of late age-related macular degeneration progression	NATURE MACHINE INTELLIGENCE											GENETIC SUSCEPTIBILITY; DIABETIC-RETINOPATHY; EYE DISEASE; IMAGES; CLASSIFICATION; PATTERNS; AREDS; AMD	Both genetic and environmental factors influence the etiology of age-related macular degeneration (AMD), a leading cause of blindness. AMD severity is primarily measured by images of the fundus of the retina and recently developed machine learning methods can successfully predict AMD progression using image data. However, none of these methods have used both genetic and image data for predicting AMD progression. Here we used both genotypes and fundus images to predict whether an eye had progressed to late AMD with a modified deep convolutional neural network. In total, we used 31,262 fundus images and 52 AMD-associated genetic variants from 1,351 subjects from the Age-Related Eye Disease Study, which provided disease severity phenotypes and fundus images available at baseline and follow-up visits over a period of 12 years. Our results showed that fundus images coupled with genotypes could predict late AMD progression with an averaged area-under-the-curve value of 0.85 (95%confidence interval 0.83-0.86). The results using fundus images alone showed an averaged area under the receiver operating characteristic curve value of 0.81 (95%confidence interval 0.80-0.83). We implemented our model in a cloud-based application for individual risk assessment. Age-related macular degeneration is a serious eye disease which should be detected as early as possible. Using both fundus images and genetic information, a deep neural network is able to detect the severity of the disease and predict its progression seven years into the future.																		2522-5839				FEB	2020	2	2					141	+		10.1038/s42256-020-0154-9													
J								Focal Loss for Dense Object Detection	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Detectors; Training; Object detection; Entropy; Proposals; Convolutional neural networks; Feature extraction; Computer vision; object detection; machine learning; convolutional neural networks		The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a classifier is applied to a sparse set of candidate object locations. In contrast, one-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors thus far. In this paper, we investigate why this is the case. We discover that the extreme foreground-background class imbalance encountered during training of dense detectors is the central cause. We propose to address this class imbalance by reshaping the standard cross entropy loss such that it down-weights the loss assigned to well-classified examples. Our novel Focal Loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. To evaluate the effectiveness of our loss, we design and train a simple dense detector we call RetinaNet. Our results show that when trained with the focal loss, RetinaNet is able to match the speed of previous one-stage detectors while surpassing the accuracy of all existing state-of-the-art two-stage detectors. Code is at: https://github.com/facebookresearch/Detectron.																	0162-8828	1939-3539				FEB	2020	42	2					318	327		10.1109/TPAMI.2018.2858826													
J								Adaptive Control of Nonlinear Semi-Markovian Jump T-S Fuzzy Systems With Immeasurable Premise Variables via Sliding Mode Observer	IEEE TRANSACTIONS ON CYBERNETICS										Adaptive sliding mode; immeasurable premise variables; semi-Markovian jump systems (MJSs); Takagi-Sugeno (T-S) fuzzy systems	TIME-VARYING DELAY; H-INFINITY; ROBUST STABILIZATION; SWITCHING SYSTEMS; DESIGN; STABILITY	The issue of observer-based adaptive sliding mode control of nonlinear Takagi-Sugeno fuzzy systems with semi-Markov switching and immeasurable premise variables is investigated. More general nonlinear systems are described in the model since the selections of premise variables are the states of the system. First, a novel integral sliding surface function is proposed on the observer space, then the sliding mode dynamics and error dynamics are obtained in accordance with estimated premise variables. Second, sufficient conditions for stochastic stability with an H-infinity performance disturbance attenuation level gamma of the sliding mode dynamics with different input matrices are obtained based on generally uncertain transition rates. Third, an observer-based adaptive controller is synthesized to ensure the finite time reachability of a predefined sliding surface. Finally, the single-link robot arm model is provided to verify the control scheme numerically.																	2168-2267	2168-2275				FEB	2020	50	2					810	820		10.1109/TCYB.2018.2874166													
J								A Distributed Dynamic Event-Triggered Control Approach to Consensus of Linear Multiagent Systems With Directed Networks	IEEE TRANSACTIONS ON CYBERNETICS										Multi-agent systems; Protocols; Communication networks; Symmetric matrices; Decentralized control; Heuristic algorithms; Directed communication networks; dynamic event-triggering mechanism; event-triggered consensus; linear dynamics	COOPERATIVE CONTROL; COMMUNICATION; TIME	In this paper, we study the consensus problem for a class of linear multiagent systems, where the communication networks are directed. First, a dynamic event-triggering mechanism is introduced, including some existing static event-triggering mechanisms as its special cases. Second, based on the dynamic event-triggering mechanism, a distributed control protocol is developed, which ensures that all agents can reach consensus with an exponential convergence rate. Third, it is shown that, with the dynamic event-triggering mechanism, the minimum interevent time between any two consecutive triggering instants can be prolonged and no agent exhibits Zeno behavior. Finally, an algorithm is provided to avoid continuous communication when the dynamic event-triggering mechanism is implemented. The effectiveness of the results is confirmed through a numerical example.																	2168-2267	2168-2275				FEB	2020	50	2					869	874		10.1109/TCYB.2018.2868778													
J								Linguistic q-rung orthopair fuzzy sets and their interactional partitioned Heronian mean aggregation operators	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										linguistic intuitionistic fuzzy set; linguistic Pythagorean fuzzy set; multiattribute group decision-making; partitioned Heronian mean; q-rung orthopair fuzzy set	PYTHAGOREAN MEMBERSHIP GRADES; GROUP DECISION-MAKING; NUMBERS; TERMS	The linguistic intuitionistic fuzzy sets (LIFSs) and linguistic Pythagorean fuzzy sets (LPFSs) are two linguistic orthopair fuzzy sets whose membership grades are pairs of linguistic terms from the predefined linguistic term sets (LTSs). One linguistic term indicates the membership degree (MD), while the other one gives the nonmembership degree (NMD). In each LIFS, the sum of the subscripts of MD and NMD is less than the cardinality of LTS. In the LPFSs, the sum of the squares of the subscripts of MD and NMD is less than the square of the cardinality of LTS. In this paper, we propose a general form of these two linguistic orthopair fuzzy sets, which can be named linguistic q-rung orthopair fuzzy sets. We devise the operational laws, based on which, the linguistic q-rung orthopair fuzzy weighted averaging (LqROFWA) operator and linguistic q-rung orthopair fuzzy weighted geometric (LqROFWG) operator are developed to aggregate the linguistic q-rung orthopair fuzzy numbers (LqROFNs). Then, the novel interactional operational laws that consider the interactions between the MD and NMD from different LqROFNs are given. The partitioned geometric Heronian mean (PGHM) operator can effectively solve the decision-making problems in which the attributes grouped into the same clusters have interrelationships and the attributes belonging to different clusters have no interrelationship. Based on these novel operational laws and PGHM operator, the linguistic q-rung orthopair fuzzy interactional PGHM (LqROFIPGHM) operator and linguistic q-rung orthopair fuzzy interactional weighted PGHM (LqROFIWPGHM) operator are proposed and their properties are discussed. Based on the LqROFIWPGHM operator, an efficient multiattribute group decision-making model is given to deal with the linguistic q-rung orthopair fuzzy information. Finally, the superiorities of the interactional operational laws and LqROFIWPGHM operator are tested using some illustrative examples.																	0884-8173	1098-111X				FEB	2020	35	2					217	249		10.1002/int.22136													
J								Social Science-guided Feature Engineering: A Novel Approach to Signed Link Analysis	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										Signed link analysis; social theory; emotional information; diffusion of innovation; individual personality; feature engineering; data sparsity	DISTINGUISHING OPTIMISM; TRAIT ANXIETY; PERSONALITY; PESSIMISM; NEUROTICISM; DIMENSIONS; PREDICTION; DISTRUST; NETWORKS; FACEBOOK	Many real-world relations can be represented by signed networks with positive links (e.g., friendships and trust) and negative links (e.g., foes and distrust). Link prediction helps advance tasks in social network analysis such as recommendation systems. Most existing work on link analysis focuses on unsigned social networks. The existence of negative links piques research interests in investigating whether properties and principles of signed networks differ from those of unsigned networks and mandates dedicated efforts on link analysis for signed social networks. Recent findings suggest that properties of signed networks substantially differ from those of unsigned networks and negative links can be of significant help in signed link analysis in complementary ways. In this article, we center our discussion on a challenging problem of signed link analysis. Signed link analysis faces the problem of data sparsity, i.e., only a small percentage of signed links are given. This problem can even getworse when negative links are much sparser than positive ones as users are inclined more toward positive disposition rather than negative. We investigate how we can take advantage of other sources of information for signed link analysis. This research is mainly guided by three social science theories, Emotional Information, Diffusion of Innovations, and Individual Personality. Guided by these, we extract three categories of related features and leverage them for signed link analysis. Experiments showthe significance of the features gleaned from social theories for signed link prediction and addressing the data sparsity challenge.																	2157-6904	2157-6912				FEB	2020	11	1							11	10.1145/3364222													
J								Trembr: Exploring Road Networks for Trajectory Representation Learning	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										Trajectory; neural networks; representation learning; road network		In this article, we propose a novel representation learning framework, namely TRajectory EMBedding via Road networks (Trembr), to learn trajectory embeddings (low-dimensional feature vectors) for use in a variety of trajectory applications. The novelty of Trembr lies in (1) the design of a recurrent neural network-(RNN) based encoder-decoder model, namely Traj2Vec, that encodes spatial and temporal properties inherent in trajectories into trajectory embeddings by exploiting the underlying road networks to constrain the learning process in accordance with the matched road segments obtained using road network matching techniques (e.g., Barefoot [24, 27]), and (2) the design of a neural network-based model, namely Road2Vec, to learn road segment embeddings in road networks that captures various relationships amongst road segments in preparation for trajectory representation learning. In addition to model design, several unique technical issues raising in Trembr, including data preparation in Road2Vec, the road segment relevance-aware loss, and the network topology constraint in Traj2Vec, are examined. To validate our ideas, we learn trajectory embeddings using multiple large-scale real-world trajectory datasets and use them in three tasks, including trajectory similarity measure, travel time prediction, and destination prediction. Empirical results show that Trembr soundly outperforms the state-of-the-art trajectory representation learning models, trajectory2vec and t2vec, by at least one order of magnitude in terms of mean rank in trajectory similarity measure, 23.3% to 41.7% in terms of mean absolute error (MAE) in travel time prediction, and 39.6% to 52.4% in terms of MAE in destination prediction.																	2157-6904	2157-6912				FEB	2020	11	1							10	10.1145/3361741													
J								Robust Fake News Detection Over Time and Attack	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										Fake news; biased news; misleading news; fake news detection; misinformation; disinformation; concept drift; robust machine learning; adversarial machine learning	MEDIA BIAS	In this study, we examine the impact of time on state-of-the-art news veracity classifiers. We show that, as time progresses, classification performance for both unreliable and hyper-partisan news classification slowly degrade. While this degradation does happen, it happens slower than expected, illustrating that hand-crafted, content-based features, such as style of writing, are fairly robust to changes in the news cycle. We show that this small degradation can bemitigated using online learning. Last, we examine the impact of adversarial content manipulation by malicious news producers. Specifically, we test three types of attack based on changes in the input space and data availability. We show that static models are susceptible to content manipulation attacks, but online models can recover from such attacks.																	2157-6904	2157-6912				FEB	2020	11	1							7	10.1145/3363818													
J								Graph-based Recommendation Meets Bayes and Similarity Measures	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										Collaborative filtering; graph-based recommendation; Bayesian statistics; similarity measures		Graph-based approaches provide an effective memory-based alternative to latent factor models for collaborative recommendation. Modern approaches rely on either sampling short walks or enumerating short paths starting from the target user in a user-item bipartite graph. While the effectiveness of random walk sampling heavily depends on the underlying path sampling strategy, path enumeration is sensitive to the strategy adopted for scoring each individual path. In this article, we demonstrate how both strategies can be improved through Bayesian reasoning. In particular, we propose to improve random walk sampling by exploiting distributional aspects of items' ratings on the sampled paths. Likewise, we extend existing path enumeration approaches to leverage categorical ratings and to scale the score of each path proportionally to the affinity of pairs of users and pairs of items on the path. Experiments on several publicly available datasets demonstrate the effectiveness of our proposed approaches compared to state-of-the-art graph-based recommenders.																	2157-6904	2157-6912				FEB	2020	11	1							3	10.1145/3356882													
J								Exploring Correlation Network for Cheating Detection	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										Correlation network analysis; cheating detection; distribution channel; time series; graph cut	TIME-SERIES; APPROXIMATION ALGORITHMS; OPTIMIZATION; CUT	The correlation network, typically formed by computing pairwise correlations between variables, has recently become a competitive paradigm to discover insights in various application domains, such as climate prediction, financial marketing, and bioinformatics. In this study, we adopt this paradigm to detect cheating behavior hidden in business distribution channels, where falsified big deals are often made by collusive partners to obtain lower product prices-a behavior deemed to be extremely harmful to the sale ecosystem. To this end, we assume that abnormal deals are likely to occur between two partners if their purchase-volume sequences have a strong negative correlation. This seemingly intuitive rule, however, imposes several research challenges. First, existing correlation measures are usually symmetric and thus cannot distinguish the different roles of partners in cheating. Second, the tick-to-tick correspondence between two sequences might be violated due to the possible delay of purchase behavior, which should also be captured by correlation measures. Finally, the fact that any pair of sequences could be correlated may result in a number of false-positive cheating pairs, which need to be corrected in a systematic manner. To address these issues, we propose a correlation network analysis framework for cheating detection. In the framework, we adopt an asymmetric correlation measure to distinguish the two roles, namely, cheating seller and cheating buyer, in a cheating alliance. Dynamic TimeWarping is employed to address the time offset between two sequences in computing the correlation. We further propose two graph-cut methods to convert the correlation network into a bipartite graph to rank cheating partners, which simultaneously helps to remove false-positive correlation pairs. Based on a 4-year real-world channel dataset from a worldwide IT company, we demonstrate the effectiveness of the proposed method in comparison to competitive baseline methods.																	2157-6904	2157-6912				FEB	2020	11	1							12	10.1145/3364221													
J								Market Clearing-based Dynamic Multi-agent Task Allocation	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										Distributed Task Allocation; Multi agent system	RESOURCE-ALLOCATION; DECENTRALIZED COORDINATION; MULTIROBOT COORDINATION; MAX-SUM; SEARCH	Realistic multi-agent team applications often feature dynamic environments with soft deadlines that penalize late execution of tasks. This puts a premium on quickly allocating tasks to agents. However, when such problems include temporal and spatial constraints that require tasks to be executed sequentially by agents, they are NP-hard, and thus are commonly solved using general and specifically designed incomplete heuristic algorithms. We propose FMC_TA, a novel such incomplete task allocation algorithm that allows tasks to be easily sequenced to yield high-quality solutions. FMC_TA first finds allocations that are fair (envy-free), balancing the load and sharing important tasks among agents, and efficient (Pareto optimal) in a simplified version of the problem. It computes such allocations in polynomial or pseudo-polynomial time (centrally or distributedly, respectively) using a Fisher market with agents as buyers and tasks as goods. It then heuristically schedules the allocations, taking into account inter-agent constraints on shared tasks. We empirically compare our algorithm to state-of-the-art incomplete methods, both centralized and distributed, on law enforcement problems inspired by real police logs. We present a novel formalization of the law enforcement problem, which we use to perform our empirical study. The results show a clear advantage for FMC_TA in total utility and in measures in which law enforcement authorities measure their own performance. Besides problems with realistic properties, the algorithms were compared on synthetic problems in which we increased the size of different elements of the problem to investigate the algorithm's behavior when the problem scales. The domination of the proposed algorithm was found to be consistent.																	2157-6904	2157-6912				FEB	2020	11	1							4	10.1145/3356467													
J								DHPA: Dynamic Human Preference Analytics Framework: A Case Study on Taxi Drivers' Learning Curve Analysis	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										Urban computing; inverse reinforcement learning; preference dynamics		Many real-world human behaviors can be modeled and characterized as sequential decision-making processes, such as a taxi driver's choices of working regions and times. Each driver possesses unique preferences on the sequential choices over time and improves the driver's working efficiency. Understanding the dynamics of such preferences helps accelerate the learning process of taxi drivers. Prior works on taxi operation management mostly focus on finding optimal driving strategies or routes, lacking in-depth analysis on what the drivers learned during the process and how they affect the performance of the driver. In this work, we make the first attempt to establish Dynamic Human Preference Analytics. We inversely learn the taxi drivers' preferences from data and characterize the dynamics of such preferences over time. We extract two types of features (i.e., profile features and habit features) to model the decision space of drivers. Then through inverse reinforcement learning, we learn the preferences of drivers with respect to these features. The results illustrate that self-improving drivers tend to keep adjusting their preferences to habit features to increase their earning efficiency while keeping the preferences to profile features invariant. However, experienced drivers have stable preferences over time. The exploring drivers tend to randomly adjust the preferences over time.																	2157-6904	2157-6912				FEB	2020	11	1							8	10.1145/3360312													
J								Transfer Learning with Dynamic Distribution Adaptation	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										Transfer learning; domain adaptation; distribution alignment; deep learning; subspace learning; kernel method	REGULARIZATION; REPRESENTATION; FRAMEWORK; KERNEL	Transfer learning aims to learn robust classifiers for the target domain by leveraging knowledge from a source domain. Since the source and the target domains are usually from different distributions, existing methods mainly focus on adapting the cross-domain marginal or conditional distributions. However, in real applications, the marginal and conditional distributions usually have different contributions to the domain discrepancy. Existing methods fail to quantitatively evaluate the different importance of these two distributions, which will result in unsatisfactory transfer performance. In this article, we propose a novel concept called Dynamic Distribution Adaptation (DDA), which is capable of quantitatively evaluating the relative importance of each distribution. DDA can be easily incorporated into the framework of structural risk minimization to solve transfer learning problems. On the basis of DDA, we propose two novel learning algorithms: (1) ManifoldDynamic DistributionAdaptation (MDDA) for traditional transfer learning, and (2) Dynamic Distribution Adaptation Network (DDAN) for deep transfer learning. Extensive experiments demonstrate that MDDA and DDAN significantly improve the transfer learning performance and set up a strong baseline over the latest deep and adversarial methods on digits recognition, sentiment analysis, and image classification. More importantly, it is shown that marginal and conditional distributions have different contributions to the domain divergence, and our DDA is able to provide good quantitative evaluation of their relative importance, which leads to better performance. We believe this observation can be helpful for future research in transfer learning.																	2157-6904	2157-6912				FEB	2020	11	1							6	10.1145/3360309													
J								FROST: Movement History-Conscious Facility Relocation	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										Facility relocation; movement history; spatial database	NEAREST-NEIGHBOR QUERIES; DIST LOCATION SELECTION	The facility relocation (FR) problem, which aims to optimize the placement of facilities to accommodate the changes of users' locations, has a broad spectrum of applications. Despite the significant progress made by existing solutions to the FR problem, they all assume each user is stationary and represented as a single point. Unfortunately, in reality, objects (e.g., people, animals) are mobile. For example, a car-sharing user picks up a vehicle from a station close to where he or she is currently located. Consequently, these efforts may fail to identify a superior solution to the FR problem. In this article, for the first time, we take into account the movement history of users and introduce a novel FR problem, called motion-fr, to address the preceding limitation. Specifically, we present a framework called frost to address it. frost comprises two exact algorithms: index based and index free. The former is designed to address the scenario when facilities and objects are known a priori, whereas the latter solves the motion-fr problem by jettisoning this assumption. Further, we extend the index-based algorithm to solve the general k-motion-fr problem, which aims to relocate k inferior facilities. We devise an approximate solution due to NP-hardness of the problem. Experimental study over both real-world and synthetic datasets demonstrates the superiority of our framework in comparison to state-of-the-art FR techniques in efficiency and effectiveness.																	2157-6904	2157-6912				FEB	2020	11	1							9	10.1145/3361740													
J								Strategic Attack & Defense in Security Diffusion Games	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										Security games; social networks; network diffusion	EFFICIENT COMPUTATION; NETWORK; CONTAGIONS; BEHAVIOR	Security games model the confrontation between a defender protecting a set of targets and an attacker who tries to capture them. A variant of these games assumes security interdependence between targets, facilitating contagion of an attack. So far, only stochastic spread of an attack has been considered. In this work, we introduce a version of security games, where the attacker strategically drives the entire spread of attack and where interconnections between nodes affect their susceptibility to be captured. We find that the strategies effective in the settings without contagion or with stochastic contagion are no longer feasible when spread of attack is strategic. While in the former settings it was possible to efficiently find optimal strategies of the attacker, doing so in the latter setting turns out to be an NP-complete problem for an arbitrary network. However, for some simpler network structures, such as cliques, stars, and trees, we show that it is possible to efficiently find optimal strategies of both players. For arbitrary networks, we study and compare the efficiency of various heuristic strategies. As opposed to previous works with no or stochastic contagion, we find that centrality-based defense is often effectivewhen spread of attack is strategic, particularly for centrality measures based on the Shapley value.																	2157-6904	2157-6912				FEB	2020	11	1							5	10.1145/3357605													
J								Discovering Interesting Subpaths with Statistical Significance from Spatiotemporal Datasets	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										Interesting sub-paths; statistical significance; spatial; temporal	MODEL	Given a path in a spatial or temporal framework, we aim to find all contiguous subpaths that are both interesting (e.g., abrupt changes) and statistically significant (i.e., persistent trends rather than local fluctuations). Discovering interesting subpaths can provide meaningful information for a variety of domains including Earth science, environmental science, urban planning, and the like. Existing methods are limited to detecting individual points of interest along an input path but cannot find interesting subpaths. Our preliminary work provided a Subpath Enumeration and Pruning (SEP) algorithm to detect interesting subpaths of arbitrary length. However, SEP is not effective in avoiding detections that are random variations rather than meaningful trends, which hampers clear and proper interpretations of the results. In this article, we extend our previous work by proposing a significance testing framework to eliminate these random variations. To compute the statistical significance, we first show a baseline Monte-Carlo method based on our previous work and then propose a Dynamic Search-and-Prune (D-SAP) algorithm to improve its computational efficiency. Our experiments show that the significance testing can greatly suppress the noisy detections in the output and D-SAP can greatly reduce the execution time.																	2157-6904	2157-6912				FEB	2020	11	1							2	10.1145/3354189													
J								Forecasting Price Trend of Bulk Commodities Leveraging Cross-domain Open Data Fusion	ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY										Price trend; cross-domain data; data fusion; multi-class prediction; bulk commodity	IRON; MODEL	Forecasting price trend of bulk commodities is important in international trade, not only for markets participants to schedule production and marketing plans but also for government administrators to adjust policies. Previous studies cannot support accurate fine-grained short-term prediction, since they mainly focus on coarse-grained long-term prediction using historical data. Recently, cross-domain open data provides possibilities to conduct fine-grained price forecasting, since they can be leveraged to extract various direct and indirect factors of the price. In this article, we predict the price trend over upcoming days, by leveraging cross-domain open data fusion. More specifically, we formulate the price trend into three classes (rise, slight-change, and fall), and then we predict the specific class in which the price trend of the future day lies. We take three factors into consideration: (1) supply factor considering sources providing bulk commodities, (2) demand factor focusing on vessel transportation with reflection of short time needs, and (3) expectation factor encompassing indirect features (e.g., air quality) with latent influences. A hybrid classification framework is proposed for the price trend forecasting. Evaluation conducted on nine real-world cross-domain open datasets shows that our framework can forecast the price trend accurately, outperforming multiple state-of-the-art baselines.																	2157-6904	2157-6912				FEB	2020	11	1							1	10.1145/3354287													
J								A new uncertain DEA model and application to scientific research personnel	SOFT COMPUTING										Data envelopment analysis; Hurwicz criterion; Ranking method; Uncertainty theory; Uncertain measure	DATA ENVELOPMENT ANALYSIS; DECISION	Data envelopment analysis (DEA), which has been widely used since it was introduced by Charnes et al. (J Econom 30:91-107, 1985), is an effective method for evaluating the relative efficiency of decision-making units. DEA models require accurate input data and output data; however, if no sample is available to estimate accurate data, then uncertain DEA is introduced. This paper reports on several new studies on uncertain DEA using the Hurwicz criterion, which attempts to find the intermediate area between extremes. Some uncertain DEA models, as well as their crisp equivalent models, are presented. Then, the Hurwicz ranking method is proposed based on these models, which can give an evaluation to all the decision-making units. By varying the parameter beta in the Hurwicz criterion, which reflects the optimism of the decision maker, the new ranking method can exhibit various forms. Finally, an application to scientific personnel is provided to prove the advantage of the proposed method.																	1432-7643	1433-7479				FEB	2020	24	4			SI		2841	2847		10.1007/s00500-019-04555-6													
J								Mask R-CNN	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Task analysis; Semantics; Feature extraction; Object detection; Proposals; Image segmentation; Quantization (signal); Instance segmentation; object detection; pose estimation; convolutional neural network		We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without bells and whistles, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code has been made available at: https://github.com/facebookresearch/Detectron.																	0162-8828	1939-3539				FEB	2020	42	2					386	397		10.1109/TPAMI.2018.2844175													
J								Identification of contribution and lacks of the ecodesign education to the achievement of sustainability issues by analyzing the French education system	AI EDAM-ARTIFICIAL INTELLIGENCE FOR ENGINEERING DESIGN ANALYSIS AND MANUFACTURING										Ecodesign education and learning; knowledge engineering; sustainability	ENGINEERING-EDUCATION; DESIGN; IMPLEMENTATION; COMPETENCES; CHALLENGES; MANAGEMENT; PROGRAM; TOOLS; LCA	The sustainability concept is becoming ingrained in the international engineering community. The next generation of engineers has to be trained to appreciate economic, environmental, and societal impacts of its decisions, with an international perspective and at a local and global scale. The aim of this paper is to contribute to identifying the strengths and limits of current sustainability education practices to will be able to improve future decision-makers' and product designers' training. The first step of our research concerns the identification of knowledge, skills, attitudes, and values associated with sustainability and ecodesign to highlight what kind of engineers' competencies for sustainability have to be trained. Based on this identification, we examine the French education system from secondary school to university to study sustainability and ecodesign integration in different technological French curricula and practices. The objective is to estimate the effective place of sustainable development and ecodesign in the technological training courses. We analyze programs and complete our approach by observing teachers in their classrooms or by conducting interviews. Even if concepts, knowledge and skills related to ecodesign appear in many different programs, our study emphasizes that there is not a continuum between pre-secondary school, high school, and university. As a consequence, teachers have difficulties to understand the aims and the coherence of the program, and students are not really comfortable with the issues of sustainability and ecodesign. We propose to improve programs and practices by developing multidisciplinary curricula covering most of the sustainable issues and integrating best practices of international universities.																	0890-0604	1469-1760				FEB	2020	34	1			SI		4	16	PII S0890060419000465	10.1017/S0890060419000465													
J								The relationship between knowledge mapping and the open innovation process: the case of education system	AI EDAM-ARTIFICIAL INTELLIGENCE FOR ENGINEERING DESIGN ANALYSIS AND MANUFACTURING										BKMDM method; data source; knowledge management; knowledge mapping; knowledge visualization; open innovation process; project memory	MANAGEMENT	In an increasingly competitive economic environment, innovation has become an invaluable asset to the organization and for effective knowledge management (KM). Nowadays, organizations are knowledge based and their success and survival depend on creativity, diversity, and innovation. A knowledge map is a vital tool for better KM and innovation. To this effect, the innovation processes on KMin education system through knowledge transfer activities will facilitate the shift from teaching as knowledge transmission to teaching as learning facilitation. In this context, we present a new approach based on, the one hand, the critical knowledge mapping being based on an extraction work of the expert knowledge and on the other hand, the description of the conceptual framework design which allows one to exploit and integrate knowledge capitalized and external knowledge by the open innovation process. In addition, this practice makes it possible to examine how KM, in particular, the knowledge mapping, can be used to establish the flow of the internal and external information in order to increase the efficiency of creativity and invention.																	0890-0604	1469-1760				FEB	2020	34	1			SI		17	29	PII S0890060419000325	10.1017/S0890060419000325													
J								A conceptual tool for environmentally benign design: development and evaluation of a "proof of concept"	AI EDAM-ARTIFICIAL INTELLIGENCE FOR ENGINEERING DESIGN ANALYSIS AND MANUFACTURING										Conceptual stage of design; decision-making; design for sustainability; design tool; environmental impact assessment; SAPPhIRE model of causality	LIFE-CYCLE ASSESSMENT; MATERIALS SELECTION; BUILDING-MATERIALS; CONCEPT ONTOLOGY; ASSESSMENT LCA; REPRESENTATION; INTEGRATION; FRAMEWORK; IMPACT; STAGE	Design is a decision-making process for which knowledge is a prerequisite. Most decisions are taken at the conceptual stage and have pronounced influence on the final design. The literature, therefore, recommends the incorporation of sustainability criteria, such as environment, at this stage. Difficulty in performing life cycle assessment (LCA) due to low availability of information at the conceptual stage for evaluation and highly abstract nature of solutions, inadequate incorporation of DfE (Design for Environment) guidelines and LCA reports into the design process, and a lack of effective communication of the same to the designers for prompt decision-making are major motivations for the development of a support. This paper discusses a "conceptual Tool for environmentally benign design" - concepTe - that supports designers in decision-making during the conceptual design stage, by offering environmental impact (EI) estimates of abstract solutions with associated uncertainty, for evaluation and selection of the most environmentally benign solution as concept. The EI estimates are calculated by a module in the tool based on a proposed EI estimation method, which requires the support of a knowledge base to fetch appropriate LCA information corresponding to the design element being conceptualized. This knowledge base is grounded in the domain-agnostic SAPPhIRE model ontology, allows semantic operability of the knowledge, and offers the results to the designers in a familiar domain language to aid decision-making. A "proof of concept" of the tool is developed for application in design of building in the AEC (Architectural design, Engineering, and Construction) domain. Further, empirical studies are conducted to evaluate the effectiveness of the "proof of concept" to support decision-making and results are found favorable. The paper also discusses the future scope for further development of the tool into a holistic design decision-making platform.																	0890-0604	1469-1760				FEB	2020	34	1			SI		30	44	PII S0890060419000313	10.1017/S0890060419000313													
J								Evaluating the effectiveness of InDeaTe tool in supporting design for sustainability	AI EDAM-ARTIFICIAL INTELLIGENCE FOR ENGINEERING DESIGN ANALYSIS AND MANUFACTURING										Design methods; manufacturing systems; product development; service design; sustainability		In today's aggressive global market, innovation is key for success and design solutions require not only to achieve competitive edge, but also to address the growing environmental, social, and economic needs of the community at large. Consideration of these three pillars of sustainability makes a design inclusive, and life cycle thinking is found to be a promising approach across the literature. However, most supports for design address certain facets or aid singular tasks, and the use of design methods and tools, which have the potential to significantly improve the design process, is low due to inappropriate use and selection of these methods. InDeaTe (Innovation Design database and Template) is a holistic, knowledge-driven, computer-based tool for design of sustainable systems, such as products, manufacturing systems andservice systems and has been developed to address and integrate the aspects of sustainability on a singular design platform. It comprises of the generic design process Template that imbibes life cycle thinking into the process by incorporating consideration of every life cycle phase in each design stage, where design activities are performed iteratively. It further supports the design process by aiding the use and selection of appropriate design methods and tools in concurrence with the primary motivation of improving sustainability of the system with the aid of the InDeaTe Design Database. This paper discusses the ontological underpinnings behind the conceptualization of the InDeaTe methodology and the development of the supporting tool. The paper further reports empirical findings from six different case studies conducted for evaluating the effectiveness of InDeaTe tool in supporting design for sustainability (DfS). The results show that InDeaTe tool has potential in supporting DfS.																	0890-0604	1469-1760				FEB	2020	34	1			SI		45	54	PII S0890060419000337	10.1017/S0890060419000337													
J								A personalized requirement identifying model for design improvement based on user profiling	AI EDAM-ARTIFICIAL INTELLIGENCE FOR ENGINEERING DESIGN ANALYSIS AND MANUFACTURING										Design improvement; personalized requirement; requirement identifying; user profiling	PRODUCT; MANAGEMENT	The personalization of products and services has become an inevitable trend in the manufacturing and service industry, but it is very difficult to identify users' personalized requirements accurately. This paper solves this problem by constructing an identifying model for personalized requirement based on user profiling. Firstly, the framework of the proposed model and the process of identifying the user's personalized requirements with this model are introduced, and then an experimental scheme for obtaining users' profiling data is designed. On this basis, an experiment is performed by investigating users' requirements for the computer to obtain the data, and the data are used for the analysis based on the proposed model. The analysis result shows that the model can reveal the difference among heterogeneous users well, find out the implicit requirements of users, and identify the gap between existing products and users' personalized requirements, which provides support to the subsequent improvement of product design.																	0890-0604	1469-1760				FEB	2020	34	1			SI		55	67	PII S0890060419000301	10.1017/S0890060419000301													
J								A method for choosing adapted life cycle assessment indicators as a driver of environmental learning: a French textile case study	AI EDAM-ARTIFICIAL INTELLIGENCE FOR ENGINEERING DESIGN ANALYSIS AND MANUFACTURING										Life cycle assessment; knowledge; learning; choice of LCIA indicators and methods; eco-design tools	ECODESIGN TOOLS; END-POINTS; IMPLEMENTATION; DESIGNERS; BARRIERS	Despite alefforts for a sustainable production system, many companies are still struggling to implement environmental aspects in their daily product development processes. Among the evaluation and improvement methods, life cycle assessment (LCA) is one of the most popular tools to achieve this goal. Up to date, LCA has been applied to many products, services, and industrial systems to evaluate their environmental impact aspects. However, there is a wide range of indicators available to be applied for LCA, and choosing an inappropriate indicator may lead the product designer to achieve wrong and weak results. Therefore, this paper proposes to overcome this difficulty by developing a method that can be used as a knowledge transfer to product designers and LCA practitioners in order to help them to make the most appropriate choice of LCA indicators. This method should have some characteristics, such as (a) to be adaptable to a given context and (b) to be dynamic, scalable, and easy to learn. The purpose of this paper is to present the Evaluation Method for Choosing Indicator (EMCI) developed to facilitate the learning process of LCA methods and to quickly select their most appropriate indicators. To validate the EMCI method, a case study on a French textile industry has been implemented. The focus was to evaluate how LCA indicators and methods were chosen to be integrated into the suitable eco-design LCA tool.																	0890-0604	1469-1760				FEB	2020	34	1			SI		68	79	PII S0890060419000234	10.1017/S0890060419000234													
J								A computer-aided approach improving the Axiomatic Design theory with the distributed design resource environment	AI EDAM-ARTIFICIAL INTELLIGENCE FOR ENGINEERING DESIGN ANALYSIS AND MANUFACTURING										Axiomatic Design; design resource; distributed environment; functional domain; physical domain	CONCEPTUAL DESIGN; FUNCTIONAL BASIS; SYSTEM; DECOMPOSITION; OPTIMIZATION	In the traditional Axiomatic Design (AD) theory, the mapping from the functional domain to the physical domain is based on the designers' own knowledge and experience, and there is no systematical approach including the design resources provided outside the designers themselves' access. Thus, the raw materials for the design process are largely limited, which means they can hardly support the designers' increasingly creative and innovative conceptions. To help AD theory better support the design process, this paper proposes a computer-aided approach for the mapping from the functional domain to the physical domain within a distributed design resource environment, which consists of numerous design resources offered on the Internet by the providers widely distributed in different locations, institutes, and disciplines. To prove the feasibility of this proposed approach, a software prototype is established, and a natural leisure hotel is designed as an implementation case.																	0890-0604	1469-1760				FEB	2020	34	1			SI		80	103	PII S0890060419000283	10.1017/S0890060419000283													
J								Fleet optimization considering overcapacity and load sharing restrictions using genetic algorithms and ant colony optimization	AI EDAM-ARTIFICIAL INTELLIGENCE FOR ENGINEERING DESIGN ANALYSIS AND MANUFACTURING										Ant colony optimization; availability analysis; fleet design; life cycle cost; overcapacity	RELIABILITY; MANAGEMENT; MODEL	This paper presents a fleet model explained through a complex configuration of load sharing that considers overcapacity and is based on a life cycle cost (LCC) approach for cost-related decision-making. By analyzing the variables needed to optimize the fleet size, which must be evaluated in combination with the event space method (ESM), the solution to this problem would normally require high computing performance and long computing times. Considering this, the combined use of an integer genetic algorithm (GA) and the ant colony optimization (ACO) method was proposed in order to determine the optimal solution. In order to analyze and highlight the added value of this proposal, several empirical simulations were performed. The results showed the potential strengths of the proposal related to its flexibility and capacity in solving large problems with a near optimal solution for large fleet size and potential real-world applications. Even larger problems can be solved this way than by using the complete enumeration approach and a non-family fleet approach. Thus, this allows for a more real solution to fleet design that also considers overcapacity, availability, and an LCC approach. The simulations showed that the model can be solved in much less time compared with the base model and allows for the resolution of a fleet of at least 64 trucks using GA and 130 using ACO, respectively. Thus, the proposed framework can solve real-world problems, such as the fleet design of mining companies, by offering a more realistic approach.																	0890-0604	1469-1760				FEB	2020	34	1			SI		104	113	PII S0890060419000428	10.1017/S0890060419000428													
J								Developing a new hybrid soft computing technique in predicting ultimate pile bearing capacity using cone penetration test data	AI EDAM-ARTIFICIAL INTELLIGENCE FOR ENGINEERING DESIGN ANALYSIS AND MANUFACTURING										ANFIS algorithm; CPT-based models; GMDH type neural network; pile bearing capacity; PSO algorithm	PARTICLE SWARM OPTIMIZATION; SCOUR DEPTH; NEURAL-NETWORKS; DESIGN OPTIMIZATION; GENETIC ALGORITHM; AXIAL CAPACITY; DRIVEN PILES; GMDH; SYSTEMS; PERFORMANCE	This research intends to investigate a new hybrid artificial intelligence (AI) technique compared to some common CPT methods in estimating axial ultimate pile bearing capacity (UPBC) using cone penetration test (CPT) data in geotechnical engineering applications. A data series of 108 samples was collected in order to develop a new hybrid structure of an adaptive neuro-fuzzy inference system (ANFIS) network, and the group method of the data handling (GMDH) type neural network was optimized by applying the particle swarm optimization (PSO) algorithm over the hybrid ANFIS-GMDH topology, which leads to a new hybrid AI model called as ANFIS-GMDH-PSO. The derived database provides information related to pile load tests, in situ field CPT data, and soil-pile information for introducing the proposed hybrid neural system. The cross-section of the pile toe, average cone tip resistance along embedded pile length, and sleeve frictional resistance along the shaft had been considered as input parameters for the proposed network. The results of this research indicated that the proposed ANFIS-GMDH-PSO model predicted the UPBC with an acceptable precision compared to various CPT methods, including Schmertmann, De Kuiter & Bringen, and LPC/LPCT methods. Moreover, ANFIS-GMDH-PSO network model performance was compared to CPT-based models in terms of statistical criteria in order to achieve a best fitted model. From the statistical results, it was found that the developed ANFIS-GMDH-PSO model has achieved a higher accuracy level in terms of statistical indices compared to CPT-based empirical methods, such as Schmertmann model, De Kuiter & Beringen model, and Bustamante & Gianeselli for predicting driven pile ultimate bearing capacity.																	0890-0604	1469-1760				FEB	2020	34	1			SI		114	126	PII S0890060420000025	10.1017/S0890060420000025													
J								Face Representations via Tensorfaces of Various Complexities	NEURAL COMPUTATION											INFERIOR TEMPORAL CORTEX; VENTRAL VISUAL PATHWAY; HIGHER-ORDER TENSOR; INFEROTEMPORAL CORTEX; SPATIAL-FREQUENCY; OBJECT REPRESENTATIONS; UNIQUENESS CONDITIONS; NEURONAL RESPONSES; FACIAL IDENTITY; NATURAL SCENES	Neurons selective for faces exist in humans and monkeys. However, characteristics of face cell receptive fields are poorly understood. In this theoretical study, we explore the effects of complexity, defined as algorithmic information (Kolmogorov complexity) and logical depth, on possible ways that face cells may be organized. We use tensor decompositions to decompose faces into a set of components, called tensorfaces, and their associated weights, which can be interpreted as model face cells and their firing rates. These tensorfaces form a high-dimensional representation space in which each tensorface forms an axis of the space. A distinctive feature of the decomposition algorithm is the ability to specify tensorface complexity. We found that low-complexity tensorfaces have blob-like appearances crudely approximating faces, while high-complexity tensorfaces appear clearly face-like. Low-complexity tensorfaces require a larger population to reach a criterion face reconstruction error than medium- or high-complexity tensorfaces, and thus are inefficient by that criterion. Low-complexity tensorfaces, however, generalize better when representing statistically novel faces, which are faces falling beyond the distribution of face description parameters found in the tensorface training set. The degree to which face representations are parts based or global forms a continuum as a function of tensorface complexity, with low and medium tensorfaces being more parts based. Given the computational load imposed in creating high-complexity face cells (in the form of algorithmic information and logical depth) and in the absence of a compelling advantage to using high-complexity cells, we suggest face representations consist of a mixture of low- and medium-complexity face cells.																	0899-7667	1530-888X				FEB	2020	32	2					281	329		10.1162/neco_a_01258													
J								Transition Scale-Spaces: A Computational Theory for the Discretized Entorhinal Cortex	NEURAL COMPUTATION											HIPPOCAMPAL THETA-RHYTHM; GRID CELLS; PATH-INTEGRATION; PHASE PRECESSION; PLACE CELLS; POPULATION CODES; COGNITIVE MAP; MEMORY; REPRESENTATION; MODELS	Although hippocampal grid cells are thought to be crucial for spatial navigation, their computational purpose remains disputed. Recently, they were proposed to represent spatial transitions and convey this knowledge downstream to place cells. However, a single scale of transitions is insufficient to plan long goal-directed sequences in behaviorally acceptable time. Here, a scale-space data structure is suggested to optimally accelerate retrievals from transition systems, called transition scale-space (TSS). Remaining exclusively on an algorithmic level, the scale increment is proved to be ideally 2 for biologically plausible receptive fields. It is then argued that temporal buffering is necessary to learn the scale-space online. Next, two modes for retrieval of sequences from the TSS are presented: top down and bottom up. The two modes are evaluated in symbolic simulations (i.e., without biologically plausible spiking neurons). Additionally, a TSS is used for short-cut discovery in a simulated Morris water maze. Finally, the results are discussed in depth with respect to biological plausibility, and several testable predictions are derived. Moreover, relations to other grid cell models, multiresolution path planning, and scale-space theory are highlighted. Summarized, reward-free transition encoding is shown here, in a theoretical model, to be compatible with the observed discretization along the dorso-ventral axis of the medial entorhinal cortex. Because the theoretical model generalizes beyond navigation, the TSS is suggested to be a general-purpose cortical data structure for fast retrieval of sequences and relational knowledge. Source code for all simulations presented in this paper can be found at https://github.com/rochus/transitionscalespace.																	0899-7667	1530-888X				FEB	2020	32	2					330	394		10.1162/neco_a_01255													
J								From Synaptic Interactions to Collective Dynamics in Random Neuronal Networks Models: Critical Role of Eigenvectors and Transient Behavior	NEURAL COMPUTATION											SMALL-WORLD; MATRICES; STATISTICS; AVALANCHES; EXCITATION; VARIABLES; SYNAPSES; BALANCE	The study of neuronal interactions is at the center of several big collaborative neuroscience projects (including the Human Connectome Project, the Blue Brain Project, and the Brainome) that attempt to obtain a detailed map of the entire brain. Under certain constraints, mathematical theory can advance predictions of the expected neural dynamics based solely on the statistical properties of the synaptic interaction matrix. This work explores the application of free random variables to the study of large synaptic interaction matrices. Besides recovering in a straightforward way known results on eigenspectra in types of models of neural networks proposed by Rajan and Abbott (2006), we extend them to heavy-tailed distributions of interactions. More important, we analytically derive the behavior of eigenvector overlaps, which determine the stability of the spectra. We observe that on imposing the neuronal excitation/inhibition balance, despite the eigenvalues remaining unchanged, their stability dramatically decreases due to the strong nonorthogonality of associated eigenvectors. This leads us to the conclusion that understanding the temporal evolution of asymmetric neural networks requires considering the entangled dynamics of both eigenvectors and eigenvalues, which might bear consequences for learning and memory processes in these models. Considering the success of free random variables theory in a wide variety of disciplines, we hope that the results presented here foster the additional application of these ideas in the area of brain sciences.																	0899-7667	1530-888X				FEB	2020	32	2					395	423		10.1162/neco_a_01253													
J								Synaptic Scaling Improves the Stability of Neural Mass Models Capable of Simulating Brain Plasticity	NEURAL COMPUTATION											GRAPH-THEORETICAL ANALYSIS; FUNCTIONAL CONNECTIVITY; WHITE-MATTER; STRUCTURAL PLASTICITY; SMALL-WORLD; DYNAMICS; RECOVERY; NETWORKS; BALANCE; RULE	Neural mass models offer a way of studying the development and behavior of large-scale brain networks through computer simulations. Such simulations are currently mainly research tools, but as they improve, they could soon play a role in understanding, predicting, and optimizing patient treatments, particularly in relation to effects and outcomes of brain injury. To bring us closer to this goal, we took an existing state-of-the-art neural mass model capable of simulating connection growth through simulated plasticity processes. We identified and addressed some of the model's limitations by implementing biologically plausible mechanisms. The main limitation of the original model was its instability, which we addressed by incorporating a representation of the mechanism of synaptic scaling and examining the effects of optimizing parameters in the model. We show that the updated model retains all the merits of the original model, while being more stable and capable of generating networks that are in several aspects similar to those found in real brains.																	0899-7667	1530-888X				FEB	2020	32	2					424	446		10.1162/neco_a_01257													
J								Scaled Coupled Norms and Coupled Higher-Order Tensor Completion	NEURAL COMPUTATION											MATRIX	Recently, a set of tensor norms known as coupled norms has been proposed as a convex solution to coupled tensor completion. Coupled norms have been designed by combining low-rank inducing tensor norms with the matrix trace norm. Though coupled norms have shown good performances, they have two major limitations: they do not have a method to control the regularization of coupled modes and uncoupled modes, and they are not optimal for couplings among higher-order tensors. In this letter, we propose a method that scales the regularization of coupled components against uncoupled components to properly induce the low-rankness on the coupled mode. We also propose coupled norms for higher-order tensors by combining the square norm to coupled norms. Using the excess risk-bound analysis, we demonstrate that our proposed methods lead to lower risk bounds compared to existing coupled norms. We demonstrate the robustness of our methods through simulation and real-data experiments.																	0899-7667	1530-888X				FEB	2020	32	2					447	484		10.1162/neco_a_01254													
J								Improving Generalization via Attribute Selection on Out-of-the-Box Data	NEURAL COMPUTATION											MULTICLASS	Zero-shot learning (ZSL) aims to recognize unseen objects (test classes) given some other seen objects (training classes) by sharing information of attributes between different objects. Attributes are artificially annotated for objects and treated equally in recent ZSL tasks. However, some inferior attributes with poor predictability or poor discriminability may have negative impacts on the ZSL system performance. This letter first derives a generalization error bound for ZSL tasks. Our theoretical analysis verifies that selecting the subset of key attributes can improve the generalization performance of the original ZSL model, which uses all the attributes. Unfortunately, previous attribute selection methods have been conducted based on the seen data, and their selected attributes have poor generalization capability to the unseen data, which is unavailable in the training stage of ZSL tasks. Inspired by learning from pseudo-relevance feedback, this letter introduces out-of-the-box data-pseudo-data generated by an attribute-guided generative model-to mimic the unseen data. We then present an iterative attribute selection (IAS) strategy that iteratively selects key attributes based on the out-of-the-box data. Since the distribution of the generated out-of-the-box data is similar to that of the test data, the key attributes selected by IAS can be effectively generalized to test data. Extensive experiments demonstrate that IAS can significantly improve existing attribute-based ZSL methods and achieve state-of-the-art performance.																	0899-7667	1530-888X				FEB	2020	32	2					485	514		10.1162/neco_a_01256													
J								Non-iterative Knowledge Fusion in Deep Convolutional Neural Networks	NEURAL PROCESSING LETTERS										Knowledge fusion; Transfer learning; Convolutional neural networks; Non-iterative learning	RULES; CONSTRAINTS; ENSEMBLE	Incorporation of new knowledge into neural networks with simultaneous preservation of the previous knowledge is known to be a nontrivial problem. This problem becomes even more complex when the new knowledge is contained not in new training examples, but inside the parameters (e.g., connection weights) of another neural network. In this correspondence, we propose and test two methods of combining knowledge contained in separate networks. The first method is based on a summation of weights. The second incorporates new knowledge by modification of weights nonessential for the preservation of previously stored information. We show that with these methods, the knowledge can be transferred non-iteratively from one network to another without requiring additional training sessions. The fused network operates efficiently, performing classification at a level similar to that of an ensemble of networks. The efficiency of the methods is quantified on several publicly available data sets in classification tasks both for shallow and deep feedforward neural networks.																	1370-4621	1573-773X				FEB	2020	51	1					1	22		10.1007/s11063-019-10074-0													
J								Domain Adaptation with Few Labeled Source Samples by Graph Regularization	NEURAL PROCESSING LETTERS										Domain adaptation; Graph regularization; Maximum mean discrepancy (MMD); Manifold learning; Transfer learning	ALIGNMENT; KERNEL; MODEL	Domain Adaptation aims at utilizing source data to establish an exact model for a related but different target domain. In recent years, many effective models have been proposed to propagate label information across domains. However, these models rely on large-scale labeled data in source domain and cannot handle the case where the source domain lacks label information. In this paper, we put forward a Graph Regularized Domain Adaptation (GDA) to tackle this problem. Specifically, the proposed GDA integrates graph regularization with maximum mean discrepancy (MMD). Hence GDA enables sufficient unlabeled source data to facilitate knowledge transfer by utilizing the geometric property of source domain, simultaneously, due to the embedding of MMD, GDA can reduce source and target distribution divergency to learn a generalized classifier. Experimental results validate that our GDA outperforms the traditional algorithms when there are few labeled source samples.																	1370-4621	1573-773X				FEB	2020	51	1					23	39		10.1007/s11063-019-10075-z													
J								Entropy-Based Fuzzy Least Squares Twin Support Vector Machine for Pattern Classification	NEURAL PROCESSING LETTERS										Pattern classification; Information entropy; Least squares twin support vector machine; Fuzzy membership	CLASSIFIERS	Least squares twin support vector machine (LSTSVM) is a new machine learning method, as opposed to solving two quadratic programming problems in twin support vector machine (TWSVM), which generates two nonparallel hyperplanes by solving a pair of linear system of equations. However, LSTSVM obtains the resultant classifier by giving same importance to all training samples which may be important for classification performance. In this paper, by considering the fuzzy membership value for each sample, we propose an entropy-based fuzzy least squares twin support vector machine where fuzzy membership values are assigned based on the entropy values of all training samples. The proposed method not only retains the superior characteristics of LSTSVM which is simple and fast algorithm, but also implements the structural risk minimization principle to overcome the possible over-fitting problem. Experiments are performed on several synthetic as well as benchmark datasets and the experimental results illustrate the effectiveness of our method.																	1370-4621	1573-773X				FEB	2020	51	1					41	66		10.1007/s11063-019-10078-w													
J								Finite-Time Stabilization for Static Neural Networks with Leakage Delay and Time-Varying Delay	NEURAL PROCESSING LETTERS										Finite-time stability; Static neural networks; Time-varying delay; Leakage delay; Linear matrix inequality (LMI)	EXPONENTIAL STABILITY; SYSTEMS; SYNCHRONIZATION; BOUNDEDNESS; MODEL	The problem of finite-time stabilization (FTS) for static neural networks (SNNs) with leakage delay and time-varying delay is investigated in this paper. By introducing an auxiliary function and utilizing the Lyapunov stability theory, we derive some sufficient criteria for FTS in terms of linear matrix inequalities (LMIs). Two feedback controllers are designed based on two different Lyapunov functions, which can be easily solved via MATLAB LMI toolbox, to guarantee the FTS for the SNNs. Finally, two numerical examples are given to illustrate the efficiency of our results.																	1370-4621	1573-773X				FEB	2020	51	1					67	81		10.1007/s11063-019-10065-1													
J								Robust Output Feedback Stabilization for Uncertain Discrete-Time Stochastic Neural Networks with Time-Varying Delay	NEURAL PROCESSING LETTERS										Discrete-time stochastic neural networks; Exponentially stabization; Output feedback control; Lyapunov-Krasovskii functional; Time-varying delay	STABILITY ANALYSIS; EXPONENTIAL STABILITY; SYSTEMS; DESIGN	This paper investigates the problem of robust exponential stabilization of uncertain discrete-time stochastic neural networks with time-varying delay based on output feedback control. By choosing an augmented Lyapunov-Krasovskii functional, we established the sufficient conditions of the delay-dependent asymptotical stabilization in the mean square for a class of discrete-time stochastic neural networks with time-varying delay. Furthermore, we obtain the criteria of robust global exponential stabilization in the mean square for uncertain discrete-time stochastic neural networks with time-varying delay. Finally, we give numerical examples to illustrate the effectiveness of the proposed results.																	1370-4621	1573-773X				FEB	2020	51	1					83	103		10.1007/s11063-019-10077-x													
J								Methodologies of Compressing a Stable Performance Convolutional Neural Networks in Image Classification	NEURAL PROCESSING LETTERS										Convolutional neural network (CNN); Fixed-point; Quantization; Pruning; Clustering; K-means; Hybrid quantization; Incremental pruning; Partial quantization; Histogram		Deep learning has made a real revolution in the embedded computing environment. Convolutional neural network (CNN) revealed itself as a reliable fit to many emerging problems. The next step, is to enhance the CNN role in the embedded devices including both implementation details and performance. Resources needs of storage and computational ability are limited and constrained, resulting in key issues we have to consider in embedded devices. Compressing (i.e., quantizing) the CNN network is a valuable solution. In this paper, Our main goals are: memory compression and complexity reduction (both operations and cycles reduction) of CNNs, using methods (including quantization and pruning) that don't require retraining (i.e., allowing us to exploit them in mobile system, or robots). Also, exploring further quantization techniques for further complexity reduction. To achieve these goals, we compress a CNN model layers (i.e., parameters and outputs) into suitable precision formats using several quantization methodologies. The methodologies are: First, we describe a pruning approach, which allows us to reduce the required storage and computation cycles in embedded devices. Such enhancement can drastically reduce the consumed power and the required resources. Second, a hybrid quantization approach with automatic tuning for the network compression. Third, a K-means quantization approach. With a minor degradation relative to the floating-point performance, the presented pruning and quantization methods are able to produce a stable performance fixed-point reduced networks. A precise fixed-point calculations for coefficients, input/output signals and accumulators are considered in the quantization process.																	1370-4621	1573-773X				FEB	2020	51	1					105	127		10.1007/s11063-019-10076-y													
J								ELMAENet: A Simple, Effective and Fast Deep Architecture for Image Classification	NEURAL PROCESSING LETTERS										Image classification; Deep learning; Convolutional neural network; ELMAENet	GENERIC OBJECT RECOGNITION; LEARNING ALGORITHM; MACHINES	Deep learning has drawn extensive attention in machine learning because of its excellent performance, especially the convolutional neural network (CNN) architecture for image classification task. Therefore, many variant deep models based on CNN have been proposed in the past few years. However, the success of these models depends mostly on fine-tuning using backpropagation, which is a time-consuming process and suffers from troubles including slow convergence rate, local minima, intensive human intervention,etc. And these models achieve excellent performance only when their architectures are deeper enough. To overcome the above problems, we propose a simple, effective and fast deep architecture called ELMAENet, which uses extreme learning machines auto-encoder (ELM-AE) to get the filters of convolutional layer. ELMAENet incorporates the power of convolutional layer and ELM-AE (Kasun et al. in IEEE Intell Syst 28(6):31-34, 2013), which no longer need parameter tuning but still has a good performance for image classification. Experiments on several datasets have shown that the proposed ELMAENet achieves comparable or even better performance than that of the state-of-the-art models.																	1370-4621	1573-773X				FEB	2020	51	1					129	146		10.1007/s11063-019-10079-9													
J								Kernel-Based Subspace Learning on Riemannian Manifolds for Visual Recognition	NEURAL PROCESSING LETTERS										Riemannian manifolds; Gaussian RBF kernel; Log-Euclidean metric; Symmetric positive definite matrix; Recognition	DISCRIMINANT-ANALYSIS; COVARIANCE; CLASSIFICATION; FACE	Covariance matrices have attracted increasing attention for data representation in many computer vision tasks. The nonsingular covariance matrices are regarded as points on Riemannian manifolds rather than Euclidean space. A common technique for classification on Riemannian manifolds is to embed the covariance matrices into a reproducing kernel Hilbert space (RKHS), and then construct a map from RKHS to Euclidean space, while the explicit map from RKHS to Euclidean space in most kernel-based methods only depends on a linear hypothesis. In this paper, we propose a subspace learning framework to project Riemannian manifolds to Euclidean space, and give the theoretical derivation for it. Specifically, the Euclidean space is isomorphic to the subspace of RKHS. Under the framework, firstly we define an improved Log-Euclidean Gaussian radial basis function kernel for embedding. The first order statistical features of input images are incorporated into the kernel function to increase the discriminative power. After that we seek the optimal projection matrix of the subspace of the RKHS by conducting a graph embedding discriminant analysis. Texture recognition and object categorization experiments with region covariance descriptors demonstrate the considerable effectiveness of the improved Log-Euclidean Gaussian RBK kernel and the proposed method.																	1370-4621	1573-773X				FEB	2020	51	1					147	165		10.1007/s11063-019-10083-z													
J								Data-Based Online Optimal Temperature Tracking Control in Continuous Microwave Heating System by Adaptive Dynamic Programming	NEURAL PROCESSING LETTERS										Continuous microwave heating system; Data-based; Optimal temperature tracking control; Adaptive dynamic programming; Neural networks		Control of continuous microwave heating system (CMHS) is truly a complex problem with time variance, uncertainty and nonlinearity, which becomes prohibitive to use a conventional model-based approach. To overcome this, a novel data-based optimal temperature tracking control is designed for CMHS in this paper. In order to obtain the complex dynamics of CMHS, a neural network model is first constructed driven by process data. After transforming the original temperature tracking problem into an error regulation problem, adaptive dynamic programming is introduced to deal with the regulation problem as well as to decrease operation cost. The design and operation of this controller depend mainly on the online data, and minor prior knowledge is required. Simulation results show that the proposed method can effectively control the CMHS in terms of temperature tracking and energy utilization.																	1370-4621	1573-773X				FEB	2020	51	1					167	191		10.1007/s11063-019-10081-1													
J								Exponential Stability and Sampled-Data Synchronization of Delayed Complex-Valued Memristive Neural Networks	NEURAL PROCESSING LETTERS										Memristive neural networks; Complex-valued; Global exponential stability; Synchronization	FINITE-TIME; DYNAMICAL NETWORKS; STABILIZATION; SYSTEMS	This paper is devoted to the global exponential stability (GES) and synchronization control of delayed complex-valued memristive neural networks (CVMNNs). The criterion on the existence, uniqueness and GES of the equilibrium point (EP) for delayed CVMNNs is established by constructing a suitable Lyapunov functional and using the homeomorphism theory as well as linear matrix inequality. Meanwhile, a sampled-data controller is designed to synchronize the master and slave systems under the framework of inequality techniques and Lyapunov method. Finally, two examples are presented to show the validity of the obtained results.																	1370-4621	1573-773X				FEB	2020	51	1					193	209		10.1007/s11063-019-10082-0													
J								Attentive Semantic and Perceptual Faces Completion Using Self-attention Generative Adversarial Networks	NEURAL PROCESSING LETTERS										Attention mechanism; Images completion; Non-local neural net; Semantics completion	IMAGE; ALGORITHM	We propose an approach based on self-attention generative adversarial networks to accomplish the task of image completion where completed images become globally and locally consistent. Using self-attention GANs with contextual and other constraints, the generator can draw realistic images, where fine details are generated in the damaged region and coordinated with the whole image semantically. To train the consistent generator, i.e. image completion network, we employ global and local discriminators where the global discriminator is responsible for evaluating the consistency of the entire image, while the local discriminator assesses the local consistency by analyzing local areas containing completed regions only. Last but not least, attentive recurrent neural block is introduced to obtain the attention map about the missing part in the image, which will help the subsequent completion network to fill contents better. By comparing the experimental results of different approaches on CelebA dataset, our method shows relatively good results.																	1370-4621	1573-773X				FEB	2020	51	1					211	229		10.1007/s11063-019-10080-2													
J								Speed Up the Training of Neural Machine Translation	NEURAL PROCESSING LETTERS										Neural machine translation (NMT); Bidirectional LSTM; Nolinear activation function		Neural machine translation (NMT) has achieved notable achievements in recent years. Although existing models provide reasonable translation performance, they cost too much training time. Especially, when the corpus is enormous, their computational cost will be extremely high. In this paper, we propose a novel NMT model based on the conventional bidirectional recurrent neural network (bi-RNN). In this model, we apply a tanh activation function, which can learn the future and history context information more sufficiently, to speed up the training process. Experimental results on tasks of German-English and English-French translation demonstrate that the proposed model can save much training time compared with the state-of-the-art models and provide better translation performances.																	1370-4621	1573-773X				FEB	2020	51	1					231	249		10.1007/s11063-019-10084-y													
J								Synchronous Reluctance Motor Speed Tracking Using a Modified Second-Order Sliding Mode Control Method	NEURAL PROCESSING LETTERS										Modified second-order sliding mode control (MSOSMC); Synchronous reluctance motor (SynRM); Radial basis function network estimator; Speed control	REDUNDANT MANIPULATORS; MOTION CONTROL; SYSTEM; OPTIMIZATION; REJECTION; STATE	A modified second-order sliding mode control (MSOSMC) combined with radial basis function (RBF) network estimator is developed and proposed to achieve accurate speed tracking performance for synchronous reluctance motor (SynRM). The dynamic model of SynRM system has the properties of parameter variations, external disturbance, and nonlinear friction force. The MSOSMC method that utilizes continuous control input is applied to reduce the chattering phenomenon. Also, this method utilizes two sliding surfaces to solve the problem of system uncertainty and reduce motor power consumption. The RBF network is developed in MSOSMC scheme to estimate the lumped uncertainty in an on-line fashion. The proposed MSOSMC method uses the system error and control input as the convergence criteria. The adaptation scheme adjusts the parameter vectors based on the Lyapunov theorem approach, so that the asymptotic stability of the developed motor system can be guaranteed. Experimental results show that the MSOSMC structure achieves the better tracking performances in terms of root-mean-square error compared with the traditional SOSMC method under different speed tracking conditions.																	1370-4621	1573-773X				FEB	2020	51	1					251	270		10.1007/s11063-019-10085-x													
J								Mean-Square Exponential Input-to-State Stability of Stochastic Gene Regulatory Networks with Multiple Time Delays	NEURAL PROCESSING LETTERS										Input-to-state stability; Stochastic gene regulatory networks; Lyapunov functionals; Ito formula	ROBUST STABILITY; SYSTEMS; SYNCHRONIZATION; BIFURCATION; FEEDBACK; SUBJECT; LOGIC	This paper is concerned with the input-to-state stability of stochastic gene regulatory networks with multiple time delays. It is well acknowledged that stochastic systems can accurately describe some complex systems with random disturbances. So it is significant that stochastic systems are applied to model gene regulatory networks because of the complex relationship between genes and proteins from a micro perspective. Considering the differences between stochastic differential equations and ordinary differential equations, we introduce the new stability criterion which is different from the general stability criteria. Making use of Lyapunov functionals, Ito formula and Dynkin formula, we present sufficient conditions to guarantee that the proposed system is mean-square exponentially input-to-state stable. Moreover, numerical examples are given to illustrate validity and feasibility of the obtained results.																	1370-4621	1573-773X				FEB	2020	51	1					271	286		10.1007/s11063-019-10087-9													
J								Action Recognition with Multiple Relative Descriptors of Trajectories	NEURAL PROCESSING LETTERS										Action recognition; Dense trajectories; Multiple relative descriptors		Dense trajectory has become one of the most successful hand-crafted features for action recognition. However, most of the existing dense trajectories based methods ignore the relationship between trajectories. In this paper, we propose multiple relative descriptors of trajectories to model the relative information of pairs of trajectories. Specifically, we present relative motion descriptors and relative location descriptors, which are utilized to capture the relative motion information and relative location information respectively. Moreover, we present relative deep feature descriptors which combine the deep features with hand-crafted features. By aggregating the above descriptors, we obtain the fixed-length representation regardless of the various duration of input video. The experimental results on three standard datasets demonstrate the superiority of our method.																	1370-4621	1573-773X				FEB	2020	51	1					287	302		10.1007/s11063-019-10091-z													
J								A Novel Identification-Based Convex Control Scheme via Recurrent High-Order Neural Networks: An Application to the Internal Combustion Engine	NEURAL PROCESSING LETTERS										Recurrent neural networks; Takagi-Sugeno model; Polynomial model; Linear matrix inequalities; Sum-of-squares; Internal combustion engines	NONLINEAR-SYSTEMS; FUZZY CONTROL; STABILITY	This paper proposes an identification-based nonlinear control scheme which casts the plant model as a recurrent high-order neural network. The model thus obtained consists on polynomials of a fixed number of nonlinearities, a fact that is exploited by transforming it into an exact tensor-product representation whose nested convex sums may increase with the network order while preserving the number of different interpolating functions. Convexity is then used along the direct Lyapunov method to find conditions for controller design in the form of linear matrix inequalities or sum-of-squares; thanks to the fixed number of nonlinearities, they can be made progressively more relaxed while preventing the computational burden usually associated with Polya-like relaxations. The control law thus obtained is a generalization of the well-known parallel distributed compensation; its effectiveness is illustrated in academic examples and an internal combustion engine setup.																	1370-4621	1573-773X				FEB	2020	51	1					303	324		10.1007/s11063-019-10095-9													
J								Complex Projection Synchronization of Fractional-Order Complex-Valued Memristive Neural Networks with Multiple Delays	NEURAL PROCESSING LETTERS										Multiple delays; Memristor; Complex-valued neural network; Complex projection synchronization; Fractional-order	STABILITY ANALYSIS; DISCRETE	In this paper, the complex projection synchronization problem of fractional-order complex-valued memristive neural networks is investigated, in which the projection factor is set to complex value and multiple time delays are considered. Under the framework of set-valued mapping and differential inclusion theory, a hybrid control strategy is designed to analyze the complex projection synchronization problem of the system. Moreover, some criterion to ensure the synchronization of drive response network is obtained by applying the stability theorem and comparison principle of the fractional order systems with multiple time delays. Finally, numerical simulation example is provided to verify the correctness and effectiveness of the complex projection synchronization strategy.																	1370-4621	1573-773X				FEB	2020	51	1					325	345		10.1007/s11063-019-10093-x													
J								Maximum Mean and Covariance Discrepancy for Unsupervised Domain Adaptation	NEURAL PROCESSING LETTERS										Domain adaptation; Image classification; Dimensionality reduction; Transfer learning	KERNEL	A fundamental research topic in domain adaptation is how best to evaluate the distribution discrepancy across domains. The maximum mean discrepancy (MMD) is one of the most commonly used statistical distances in this field. However, information about distributions could be lost when adopting non-characteristic kernels by MMD. To address this issue, we devise a new distribution metric named maximum mean and covariance discrepancy (MMCD) by combining MMD and the proposed maximum covariance discrepancy (MCD). MCD probes the second-order statistics in reproducing kernel Hilbert space, which equips MMCD to capture more information compared to MMD alone. To verify the efficacy of MMCD, an unsupervised learning model based on MMCD abbreviated as McDA was proposed and efficiently optimized to resolve the domain adaptation problem. Experiments on image classification conducted on two benchmark datasets show that McDA outperforms other representative domain adaptation methods, which implies the effectiveness of MMCD in domain adaptation.																	1370-4621	1573-773X				FEB	2020	51	1					347	366		10.1007/s11063-019-10090-0													
J								Global Asymptotic Stability of Periodic Solutions for Neutral-Type BAM Neural Networks with Delays	NEURAL PROCESSING LETTERS										Neutral-type; BAM neural networks; Continuation theorem; Periodic solutions; Global asymptotic stability	EXPONENTIAL STABILITY; HOMOCLINIC SOLUTIONS; EXISTENCE; SYSTEMS	In this paper, we study the neutral-type BAM neural networks with time-varying delays. By applying the continuation theorem and some analysis techniques, some sufficient conditions to guarantee the neutral-type BAM neural networks have at least one periodic solution are proposed. Moreover, we also consider the asymptotic behaviours of periodic solutions by Lyapunov function and inequality 2ab <= a(2) + b(2). At last, an example is given to illustrate the effectiveness and feasibility of the obtain results.																	1370-4621	1573-773X				FEB	2020	51	1					367	382		10.1007/s11063-019-10092-y													
J								A High Generalizable Feature Extraction Method Using Ensemble Learning and Deep Auto-Encoders for Operational Reliability Assessment of Bearings	NEURAL PROCESSING LETTERS										Operational reliability assessment; Feature extraction; Ensemble deep auto-encoder; Feature ensemble strategy	FAULT-DIAGNOSIS; ROTATING MACHINERY; LIFE PREDICTION; NETWORK; MODEL; EMD	Feature extraction is a major challenge in operational reliability assessment, which requires techniques and prior knowledge. Deep auto-encoder (DAE) is a popular deep learning method and is widely used in feature extraction. However, low generalization ability and structure parameters design are still the major problems of DAE for operational reliability assessment. To overcome the two problems, an ensemble DAE is proposed for operational reliability assessment. Firstly, different structure parameters are employed to design a series of DAEs for feature learning from the measured data. Secondly, a feature ensemble strategy is designed to enhance the generalization ability of the DAE model, in which the features learned by different DAEs are clustered to remove the irrelevant DAEs and select the more general feature subset. Finally, the operational reliability indicator is defined by the Euclidean distance of the selected features and the operational reliability model is developed. The proposed method is utilized to analyze the experimental bearings and the results indicate that the proposed method is effective for operational reliability assessment.																	1370-4621	1573-773X				FEB	2020	51	1					383	406		10.1007/s11063-019-10094-w													
J								Finite Time Stability Analysis of Fractional-Order Complex-Valued Memristive Neural Networks with Proportional Delays	NEURAL PROCESSING LETTERS										Finite-time stability; Fractional-order complex-valued neural networks; Memristor; Proportional delays	MITTAG-LEFFLER STABILITY; EXPONENTIAL STABILITY; VARYING DELAYS; SYNCHRONIZATION; DYNAMICS; CRITERIA; CNNS	In this paper, finite time stability analysis of fractional-order complex-valued memristive neural networks with proportional delays is investigated. Under the framework of Filippov solution and differential inclusion theory, by using Holder inequality, Gronwall inequality and inequality scaling skills, some sufficient conditions are derived to ensure the finite-time stability of concerned fractional-order complex-valued memristive neural networks with fractional order alpha: 0 < alpha < 1/2 and 1/2 <= alpha < 1. In the end, two numerical examples are provided to illustrate the availability of the obtained results.																	1370-4621	1573-773X				FEB	2020	51	1					407	426		10.1007/s11063-019-10097-7													
J								Improved Delay-Derivative-Dependent Stability Analysis for Generalized Recurrent Neural Networks with Interval Time-Varying Delays	NEURAL PROCESSING LETTERS										Delay-derivative-dependent stability; Delayed-decomposition approach; Interval time-varying delay; Static recurrent neural network; Integral inequality approach	GLOBAL ASYMPTOTIC STABILITY; ROBUST STABILITY; CRITERIA; SYNCHRONIZATION	In this paper, the problem of delay-derivative-dependent stability analysis for generalized neural networks with interval time-varying delays is considered. First, we divide the whole delay interval into two segmentations with an unequal width and checking the variation of the Lyapunov-Krasovskii functional (LKF) for each subinterval of delay, where the information on the lower and upper bounds of time delay and its derivative are fully exploited. Second, a new delay-derivative-dependent stability condition for time-varying delay systems with interval time-varying delays, which expressed in terms of quadratic forms of linear matrix inequalities (LMIs), and has been derived by constructing the LKF from the delayed-decomposition approach and integral inequality approach. Third, all the conditions are presented in terms of LMIs can be easily calculated by using Matlab LMI control tool-box. Fourth, the computational complexity of newly obtained stability conditions is reduced because fewer variables are involved. Finally, four numerical examples are provided to verify the effectiveness of the proposed criteria.																	1370-4621	1573-773X				FEB	2020	51	1					427	448		10.1007/s11063-019-10088-8													
J								On Infinite Horizon Optimal Control Problems with a Feed Forward Neural Network Scheme	NEURAL PROCESSING LETTERS										Infinite-horizon problems; Pontryagin minimum principle; Optimal control problem; Artificial neural networks; Unconstrained optimization; Stability; Convergent	DIFFERENTIAL-EQUATIONS; VARIATIONAL-PROBLEMS; NUMERICAL-SOLUTION; PROGRAMMING PROBLEMS; MAXIMUM PRINCIPLE; OPTIMIZATION; SYSTEMS; MODEL; ORDER	In this paper, a class of infinite-horizon nonlinear optimal control problems is considered. The main idea is to convert the infinite horizon problem to an equivalent finite-horizon optimal control problem. According to the Pontryagin minimum principle for optimal control problems and by constructing an error function, we define an unconstrained minimization problem. In the optimization problem, we use trial solutions for the state, costate and control functions where these trial solutions are constructed by using two-layer perceptron. We then minimize the error function where weights and biases associated with all neurons are unknown. Substituting the optimal values of the weights and biases into the trial solutions, we obtain the optimal solution of the original problem. We also use a dynamic optimization scheme to learning process and discuss the stability and convergence properties of it. Some examples are given to show the efficiency of the method.																	1370-4621	1573-773X				FEB	2020	51	1					449	471		10.1007/s11063-019-10099-5													
J								A New ExtendFace Representation Method for Face Recognition	NEURAL PROCESSING LETTERS										ExtendFace recognition; Face recognition; Complex numbers; Collaborative representation	FEATURE-EXTRACTION; IMAGE; SAMPLE; PCA	Many traditional face recognition methods based on Fisher discriminant analysis and locally graph embedding are proposed for dimensional reduction in nonlinear data. However, these methods are not effective by using the face images with non-ideal conditions (such as, variations of expression, pose, illumination and noisy environment). That is to say, face recognition methods are difficult to achieve good performance due to the absence of appropriate and sufficient front training images. Unfortunately, most existing discriminant analysis approaches fail to work especially for single image per person problem because there is only a single training sample per person such that the within-class variation of this person cannot be calculated in such case. In this paper, we present a new face recognition method by using complex number based data augmentation. The proposed method first deals with the information provided by the original face images and obtains the new representations. Then, fuse original face images and the obtained new images into complex numbers by using a simple combination. Then, the samples can be mapped into the new representation space for classification by using the kernel function, and a test face image can be expressed by the linear combination of all the training face images. Finally, the classification predication can be completed via using collaborative representation based classification. The proposed method is abbreviated as ExtendFace. The performance of ExtendFace method is evaluated on ORL and Yale databases. Experimental results show that the ExtendFace method outperforms the other related methods in terms of recognition rates.																	1370-4621	1573-773X				FEB	2020	51	1					473	486		10.1007/s11063-019-10100-1													
J								Deep Learning Architectures for Accurate Millimeter Wave Positioning in 5G	NEURAL PROCESSING LETTERS										5G; Deep learning; Millimeter wave; Outdoor positioning; Temporal convolutional networks	LOCALIZATION; NETWORK; MIMO	The introduction of 5G's millimeter wave transmissions brings a new paradigm to wireless communications. Whereas physical obstacles were mostly associated with signal attenuation, their presence now adds complex, non-linear phenomena, including reflections and scattering. The result is a multipath propagation environment, shaped by the obstacles encountered, indicating a strong presence of hidden spatial information within the received signal. To untangle said information into a mobile device position, this paper proposes the usage of neural networks over beamformed fingerprints, enabling a single-anchor positioning approach. Depending on the mobile device target application, positioning can also be enhanced with tracking techniques, which leverage short-term historical data. The main contributions of this paper are to discuss and evaluate typical neural network architectures suitable to the beamformed fingerprint positioning problem, including convolutional neural networks, hierarchy-based techniques, and sequence learning approaches. Using short sequences with temporal convolutional networks, simulation results show that stable average estimation errors of down to 1.78 m are obtained on realistic outdoor scenarios, containing mostly non-line-of-sight positions. These results establish a new state-of-the-art accuracy value for non-line-of-sight millimeter wave outdoor positioning, making the proposed methods very competitive and promising alternatives in the field.																	1370-4621	1573-773X				FEB	2020	51	1					487	514		10.1007/s11063-019-10073-1													
J								A Novel GeometricMean Feature Space Discriminant Analysis Method for Hyperspectral Image Feature Extraction	NEURAL PROCESSING LETTERS										Hyperspectral image; Feature extraction; Geometric mean vector; Feature space discriminant analysis; Classification	DIMENSIONALITY REDUCTION; CLASSIFICATION; SELECTION; SOLVE	Hyperspectral image contains abundant spectral information with hundreds of spectral continuous bands that allow us to distinguish different classes with more details. However, the number of available training samples is limited and the high dimensionality of hyperspectral data increases the computational complexity and even also may degrade the classification accuracy. In addition, the bottom line is that only original spectral is difficult to well represent or reveal intrinsic geometry structure of the hyperspectral image. Thus, feature extraction is an important step before classification of high dimensional data. In this paper, we proposed a novel supervised feature extraction method that uses a new geometric mean vector to construct geometric between-class scatter matrix (S-b(G)) and geometric within-class scatter matrix (S-w(G)) instead of traditional mean vector of state-of-the-art methods. The geometric mean vector not only can reveal intrinsic geometry structure of the hyperspectral image, but also can improve the ability of learning nonlinear correlation features by maximum likelihood classification (MLC). The proposed method is called geometric mean feature space discriminant analysis (GmFSDA) that uses three measures to produce the extracted features. GmFSDA, at first, maximizes the geometric between-spectral scatter matrix to increase the difference between extracted features. In the second step of GmFSDA, maximizes the between-class scatter and minimizes the within-class scatter simultaneously. The experimental results on three real-world hyperspectral image datasets show the better performance of GmFSDA in comparison with other feature extraction methods in small sample size situation by using MLC.																	1370-4621	1573-773X				FEB	2020	51	1					515	542		10.1007/s11063-019-10101-0													
J								Novel Sufficient Conditions on Periodic Solutions for Discrete-Time Neutral-Type Neural Networks	NEURAL PROCESSING LETTERS										Periodic solutions; A class of neutral-type neural networks with time delays; Combining Mawhin's continuation theorem of coincidence degree theory with graph theory as well as Lyapunov sequence method	GLOBAL EXPONENTIAL STABILITY; COUPLED SYSTEMS; ASYMPTOTIC STABILITY; EXISTENCE; BIFURCATION	In this paper, we consider the existence and global exponential stability of periodic solutions for a class of delayed discrete-time neutral-type neural networks. Novel sufficient conditions to guarantee the existence and global exponential stability of periodic solutions are established for above discrete-time neutral-type neural networks by combining Mawhin's continuation theorem of coincidence degree theory with graph theory as well as Lyapunov sequence method. Our results on the existence and global exponential stability of periodic solutions are more concise and easily verified than those obtained in Du et al. (J Frankl Inst 353:448-461, 2016).																	1370-4621	1573-773X				FEB	2020	51	1					543	557		10.1007/s11063-019-10066-0													
J								Distributed Neuro-Dynamic Algorithm for Price-Based Game in Energy Consumption System	NEURAL PROCESSING LETTERS										Distributed neuro-dynamic algorithm; Dynamic average consensus; Projection neural network; Noncooperative game; Nash equilibrium	ELECTRIC VEHICLES; CONSTRAINED OPTIMIZATION; MANAGEMENT; NETWORK	In this paper, a plug-in hybrid electric vehicles energy consumption system is studied. In order to protect each player's privacy, the information exchange is going on the neighboring players, and a connected undirected graph is used to pattern the information flow between the players. Hence, it is impossible for each player to access the aggregate electricity consumption directly, which determines the electricity price. Under the noncooperative game frame, a distributed neuro-dynamic algorithm is proposed to optimize the benefit of each individual player base on the pricing strategies. A dynamic average consensus is applied to estimate the aggregate consumption and a projection neural network is employed to seek the Nash equilibrium point. The convergence of the proposed distributed algorithm is analyzed through the Lyapunov stability analysis. Finally, the effectiveness of the distributed neuro-dynamic algorithm is manifested in the simulation.																	1370-4621	1573-773X				FEB	2020	51	1					559	575		10.1007/s11063-019-10102-z													
J								Improved Data Modeling Using Coupled Artificial Neural Networks	NEURAL PROCESSING LETTERS										Letter recognition; Noisy environment; Coupled artificial neural networks; Hidden layer entropy		Our senses perceive the world, but what happens if one of the senses is degraded through illness or injury? In such situations, the brain compensates by enhancing the remaining senses. This suggests that networks that process the data received by the senses are coupled. Similar situations can occur in scientific and engineering problems when independent measurement methods, based on different principles, are used to study the same characteristics of a system. In such situation, one can develop reliable artificial neural network (ANN) based models; each trained using data obtained by a different measurement method. This raises the question if it is possible to couple these different models to obtain and improved more accurate model. In this paper, we explore this possibility by training two ANN models that can recognize alphabet letters in a noisy environment. The performance of these ANNs are optimized by varying the number of hidden neurons (HN). The first ANN model trained using pictorial presentation of the letters while the second by corresponding audio signals. The two separate ANNs are trained using the two alphabet letters presentation to which different levels of white noise are added. Different schemes to couple the two systems are examined. For some coupling schemes, the combined system result in highly improved letter recognition than the two original separate ANNs did. Examination of the entropy related to the number of HNs showed that increased entropy is related to a higher error in letter recognition.																	1370-4621	1573-773X				FEB	2020	51	1					577	590		10.1007/s11063-019-10089-7													
J								A Unified Self-Stabilizing Neural Network Algorithm for Principal Takagi Component Extraction	NEURAL PROCESSING LETTERS										Complex-valued eural network; Lyapunov function; Self-stability; The fixed-point analysis method; Takagi factorization; The principal Takagi vector; Principal Takagi component; Principal Takagi subspace	COMPLEX GRADIENT; OPTIMIZATION	In this paper, we develop efficient methods for the computation of the Takagi components and the Takagi subspaces of complex symmetric matrices via the complex-valued neural network models. Firstly, we present a unified self-stabilizing neural network learning algorithm for principal Takagi components and study the stability of the proposed unified algorithms via the fixed-point analysis method. Secondly, the unified algorithm for extracting principal Takagi components is generalized to compute the principal Takagi subspace. Thirdly, we prove that the associated differential equations will globally asymptotically converge to an invariance set and the corresponding energy function attains a unique global minimum if and only if its state matrices span the principal Takagi subspace. Finally, numerical simulations are carried out to illustrate the theoretical results.																	1370-4621	1573-773X				FEB	2020	51	1					591	610		10.1007/s11063-019-10109-6													
J								Dimensionality Reduction Using Discriminant Collaborative Locality Preserving Projections	NEURAL PROCESSING LETTERS										Dimensionality reduction; Manifold learning; Collaborative representation; Discriminant learning; Image recognition	FACE-RECOGNITION; SPARSE REPRESENTATION; CLASSIFICATION; EIGENFACES; FRAMEWORK	In this paper, we propose an effective dimensionality reduction algorithm named Discriminant Collaborative Locality Preserving Projections (DCLPP), which takes advantage of manifold learning and collaborative representation. Firstly, two adjacency graphs of the input data are adaptively constructed by an l2-optimization problem to model discriminant manifold structure. The adjacency graphs characterize the important properties such as the intra-class compactness and the inter-class separability. Next, based on collaborative representation reconstruction weights, both intra-class collaborative representation scatter and inter-class collaborative representation scatter can be calculated. Then, motivated by MMC, DCLPP can obtain optimal projection directions which could maximize the between-class scatter and minimize the within-class compactness. DCLPP naturally avoids the small sample size problem. Finally, after dimension reduction and data projection by DCLPP, the NN classifier is employed for classification. To evaluate the performance of DCLPP, we compare it with the most existing DR methods such as CRP and DSNPE on publicly available face databases and COIL-20 database. The experimental results demonstrate that DCLPP is feasible and effective.																	1370-4621	1573-773X				FEB	2020	51	1					611	638		10.1007/s11063-019-10104-x													
J								Discriminative Face Recognition Methods with Structure and Label Information via l(2)-Norm Regularization	NEURAL PROCESSING LETTERS										Sparse representation; Structure and label information; Locality information; l(2)-Norm regularization; Classification	SPARSE REPRESENTATION; DICTIONARY; ALGORITHMS	Existing sparse representation methods either fail to incorporate the structure and label information of training samples, or suffer from expensive computation for l(1)- or l(2,1)-norm. In this paper, we propose three discriminative sparse representation classification methods with structure and label information based on l(2)-norm regularization for robust face recognition. We propose the first classification method with structure and label information by enforcing competition among the representation results of training samples from different classes in representing a test sample. To make the classification more discriminative, we present the decorrelation classification method with structure and label information by jointly considering the competition and decorrelation regularizations. In addition, by incorporating the locality information of samples, we propose the third method called locality-constrained decorrelation classification method with structure and label information. The proposed methods not only contain the structure and label information of training samples, but also have low computational cost owing to the use of l(2)-norm. All three methods have closed-form solutions, rendering them easy to solve and calculate efficiently. Importantly, the proposed methods can achieve better recognition results than most existing state-of-the-art sparse representation methods. Furthermore, based on the proposed methods, we illustrate the effect of different regularization constraints on the recognition performance. Experiments on the ORL, Extended YaleB, FERET, and LFW databases validate the effectiveness of the proposed methods.																	1370-4621	1573-773X				FEB	2020	51	1					639	655		10.1007/s11063-019-10106-9													
J								Unsupervised Video Object Segmentation Based on Mixture Models and Saliency Detection	NEURAL PROCESSING LETTERS										Video object segmentation; Gaussian mixture model; Markov random field; saliency detection	IMAGE; EXTRACTION	In this paper, we propose an unsupervised video object segmentation approach which is mainly based on a saliency detection method and the Gaussian mixture model with Markov random field. In our approach, the saliency detection method is developed as a preprocessing technique to calculate the probability of each pixel as the target object. In contrast to traditional saliency detection methods which are normally difficult to obtain the object's precise boundary and are therefore hard to segment consistent objects, the developed saliency detection method can calculate the saliency of each frame in the video sequence and extract the position and region of the target object with more accurate object boundary. The refined extracted object region is then taken as the prior information and incorporated into the Gaussian mixture model with Markov random field to obtain the precise pixel-wise segmentation result of each frame. The effectiveness of the proposed unsupervised video object segmentation approach is validated through experimental results using both the SegTrack and the SegTrack v2 data sets.																	1370-4621	1573-773X				FEB	2020	51	1					657	674		10.1007/s11063-019-10110-z													
J								An Improved Flower Pollination Algorithm with Three Strategies and Its Applications	NEURAL PROCESSING LETTERS										FPA; Double-direction learning strategy; Greedy strategy; Dynamic switch probability	PARTICLE SWARM OPTIMIZATION; SEARCH; COLONY	The flower pollination algorithm is a recently presented meta-heuristic algorithm, but limited in searching precision and convergence rate when solving some complex problems. In order to enhance its performance, this paper proposes an improved flower pollination algorithm, combined with three strategies, i.e., a new double-direction learning strategy to advance the local searching ability, a new greedy strategy to strengthen the diversity of population and a new dynamic switching probability strategy to balance global and local searching. These strategies can increase searching precision and make solution more accurate. Then 12 standard test functions and two structural design examples are selected to appraise the performance of the newly proposed algorithm. The results show that our new algorithm has outstanding performance, such as high accuracy, fast convergence speed and strong stability on solving some complex optimization problems.																	1370-4621	1573-773X				FEB	2020	51	1					675	695		10.1007/s11063-019-10103-y													
J								Higher-Order ZNN Dynamics	NEURAL PROCESSING LETTERS										Zeroing neural network; Time-varying matrix; Matrix inverse; Hyperpower iterative methods; Convergence	RECURRENT NEURAL-NETWORK; COMPLEX ZFS; MODELS; HYBRID	Several improvements of the Zhang neural network (ZNN) dynamics for solving the time varying matrix inversion problem are presented. Introduced ZNN dynamical design is termed as ZNN models of the order p, p >= 2, and it is based on the analogy between the proposed continuous-time dynamical systems and underlying discrete-time pth order hyperpower iterative methods for computing the constant matrix inverse. Such ZNN design is denoted by ZNN(H)(p). Particularly, the ZNN(H)(2) design coincides with the standard ZNN design. Moreover, ZNN(H)(3) design represents a time-varying generalization of the previously defined ZNNCM model. In addition, an integration-enhanced noise-handling ZNN(H)(p) model, termed as IENHZNN(H)(p) H, is introduced. In the time-invariant case, we present a hybrid enhancement of the ZNN(H)(p) model, shortly termed as HZNN(H)(p), and investigate it theoretically and numerically. Theoretical and numerical comparisons between the improved and standard ZNN dynamics are considered. Keywords Zeroing																	1370-4621	1573-773X				FEB	2020	51	1					697	721		10.1007/s11063-019-10107-8													
J								Non-negative Matrix Factorization with Symmetric Manifold Regularization	NEURAL PROCESSING LETTERS										Structure retrieving; Manifold learning; Non-negative matrix factorization; Divergence; Symmetric regularization	MULTIPLICATIVE UPDATE ALGORITHMS; CONVERGENCE	Non-negative matrix factorization (NMF) is becoming an important tool for information retrieval and pattern recognition. However, in the applications of image decomposition, it is not enough to discover the intrinsic geometrical structure of the observation samples by only considering the similarity of different images. In this paper, symmetric manifold regularized objective functions are proposed to develop NMF based learning algorithms (called SMNMF), which explore both the global and local features of the manifold structures for image clustering and at the same time improve the convergence of the graph regularized NMF algorithms. For different initializations, simulations are utilized to confirm the theoretical results obtained in the convergence analysis of the new algorithms. Experimental results on COIL20, ORL, and JAFFE data sets demonstrate the clustering effectiveness of the proposed algorithms by comparing with the state-of-the-art algorithms.																	1370-4621	1573-773X				FEB	2020	51	1					723	748		10.1007/s11063-019-10111-y													
J								A Discriminative Approach to Sentiment Classification	NEURAL PROCESSING LETTERS										TFIDF; Sentiment classification; Term weighting; Natural language processing		Due to the explosive growth of user-generated contents, understanding opinions (such as reviews on products) generated by Internet users is important for optimizing business decision. To achieve such understanding, this paper investigates a discriminative approach to classifying opinions according to sentiments. The discriminative approach builds a model with the prior knowledge of the categorization information in order to extract meaningful features from the unstructured texts. The prior knowledge includes ratio factors to reinforce terms' sentiment polarity by using TF-IDF, short for term frequency-inverse document frequency. Experimental results with four datasets show the proposed approach is very competitive, compared with some of the previous works.																	1370-4621	1573-773X				FEB	2020	51	1					749	758		10.1007/s11063-019-10108-7													
J								Random Regrouping and Factorization in Cooperative Particle Swarm Optimization Based Large-Scale Neural Network Training	NEURAL PROCESSING LETTERS										Feed forward neural network; Particle swarm optimization; Random regrouping; Factorization; Variable interdependence; Saturation		Previous studies have shown that factorization and random regrouping significantly improve the performance of the cooperative particle swarm optimization (CPSO) algorithm. However, few studies have examined whether this trend continues when CPSO is applied to the training of feed forward neural networks. Neural network training problems often have very high dimensionality and introduce the issue of saturation, which has been shown to significantly affect the behavior of particles in the swarm; thus it should not be assumed that these trends hold. This study identifies the benefits of random regrouping and factorization to CPSO based neural network training, and proposes a number of approaches to problem decomposition for use in neural network training. Experiments are performed on 11 problems with sizes ranging from 35 up to 32,811 weights and biases, using a number of general approaches to problem decomposition, and state of the art algorithms taken from the literature. This study found that the impact of factorization and random regrouping on solution quality and swarm behavior depends heavily on the general approach to problem decomposition. It is shown that a random problem decomposition is effective in feed forward neural network training. A random problem decomposition has the benefit of reducing the issue of problem decomposition to the tuning of a single parameter.																	1370-4621	1573-773X				FEB	2020	51	1					759	796		10.1007/s11063-019-10112-x													
J								Learning Biased SVM with Weighted Within-Class Scatter for Imbalanced Classification	NEURAL PROCESSING LETTERS										Support vector machine; Class imbalanced; Biased support vector machine; Weighted within-class scatter	SUPPORT VECTOR MACHINE	Support vector machine (SVM) is a powerful tool for pattern classification and regression estimation. However, for the class imbalanced problem, conventional SVMs are not suitable to the imbalanced learning tasks since they tend to misclassify the minority class, which is always the more important class. In this paper, we propose an improved biased SVM with weighted within-class structure for imbalanced classification. The new algorithm makes the minority class more clustered by assigning a small weight for the within-class scatter matrix of minority class, which can improve the classification performance. The experimental results on several benchmark datasets demonstrate the effectiveness of the proposed algorithm for imbalanced data classification problems.																	1370-4621	1573-773X				FEB	2020	51	1					797	817		10.1007/s11063-019-10096-8													
J								AR-ARCH Type Artificial Neural Network for Forecasting	NEURAL PROCESSING LETTERS										Artificial neural networks; Hybrid models; Autoregressive models; Autoregressive conditional heteroscedasticity models; Particle swarm optimization	PARTICLE SWARM OPTIMIZATION; TIME-SERIES; COMPUTATIONAL INTELLIGENCE; MODEL; PREDICTION	Real-world time series such as econometric time series are rarely linear and they have characteristics of volatility. Although autoregressive conditional heteroscedasticity models have used for forecasting financial time series, these models are specific models for time series, so they are not generally applied for all-time series. ARCH-GARCH models usually applied on financial time series. Because, since these time series include features like volatility clustering and leptokurtic and therefore cause problem of heteroscedastic. These problems can be handled thanks to these models. However, These model can be modelled by ARCH-GARCH models only if they include arch effect after being checked that whether ARCH effect exists or not. Therefore, in recent years artificial neural networks have been commonly used various fields by many researchers for any nonlinear-or linear time series, especially multiplicative neuron model-based artificial neural networks are commonly used that have successful forecasting results. It is known that hybrid methods in artificial neural networks are useful techniques for forecasting time series. In this study, a new hybrid forecasting method has a multiplicative neural network structure AR-ARCHANN model has been proposed. The proposed method is a recurrent model and also it can model volatility with having autoregressive conditional heteroscedasticity structure. In the proposed approach, particle swarm optimization is used for training neural network. Possibilities of avoiding local minimum traps are increased by this algorithm in using trained process. Istanbul Stock Exchange daily data sets from 2011 to 2013 and some time series in using for 2016 International Time Series Forecasting Competition are obtained to evaluate the forecasting performance of AR-ARCH-ANN. Then, results produced by the proposed method were compared with other methods and it has better performance from other methods.																	1370-4621	1573-773X				FEB	2020	51	1					819	836		10.1007/s11063-019-10117-6													
J								Traffic Signs Detection for Real-World Application of an Advanced Driving Assisting System Using Deep Learning	NEURAL PROCESSING LETTERS										Advanced driving assisting system; Deep learning; Convolutional neural networks; Embedded implementation		Recent advanced driving systems are used as luxury tools to handle a difficult or repetitive task. One of the most important tasks is traffic signs detection that provides the driver with a global view of traffic signs on the road. A traffic signs detection application should be able to detect and understand each traffic sign. To develop a robust traffic sign detection application, we propose to use the deep learning technique to process visual data. The proposed application is used for an embedded implementation. To solve this task, we propose to use the deep learning technique based on convolutional neural networks. As known, a convolutional neural network needs a big amount of data to be trained. To solve the problem, we build a dataset for traffic signs detection. The dataset contains 10,500 images from 73 traffic signs classes. The images are captured from the Chinese roads under real environmental conditions. The proposed application achieves high performance on the proposed dataset with a mean average precision of 84.22%. Also, the proposed application can be easily used for embedded implementation because of its lightweight model size and its fast inference speed.																	1370-4621	1573-773X				FEB	2020	51	1					837	851		10.1007/s11063-019-10115-8													
J								Deep Feature Fusion for High-Resolution Aerial Scene Classification	NEURAL PROCESSING LETTERS										Deep learning; Deep convolutional neural network; Aerial scene classification; Feature fusion; Saliency detection; Remote sensing	SCALE; SHAPE	The rapid development of remote sensing technology let us acquire a large collection of remote sensing scene images with high resolution. Aerial scene classification has become a crucial problem for understanding high-resolution remote sensing imagery. In this letter, we propose a novel framework for aerial scene classification. Unlike some traditional methods in which the features are produced by using handcrafted feature descriptors, our proposed method uses the raw RGB network stream and the saliency coded network stream to extract two different types of informative features. Then, we further propose a deep feature fusion model to fuse these two sets of features for final classification. The comprehensive performance evaluation of our proposed method is tested on two publicly available remote sensing scene classification benchmarks, i.e., the UC-Merced dataset and the AID dataset. Experimental results show that our proposed method achieves satisfactory results and outperforms the state-of-the-art approaches.																	1370-4621	1573-773X				FEB	2020	51	1					853	865		10.1007/s11063-019-10119-4													
J								A Novel Delay-Dependent Criterion for Global Power Stability of Cellular Neural Networks with Proportional Delay	NEURAL PROCESSING LETTERS										Global power stability; Cellular neural networks; Proportional delay; Integral inequality; Lyapunov-Krasovskii functional; LMI approach	ROBUST EXPONENTIAL STABILITY; TIME-VARYING DELAY; ASYMPTOTIC STABILITY; SYNCHRONIZATION; SYSTEMS	The global power stability of a class of cellular neural networks with proportional delay is considered in this paper. By proposing a new integral inequality and constructing a Lyapunov functional candidate, a novel delay-dependent condition formulated by linear matrix inequalities is derived to ensure that the equilibrium point of the addressed networks achieves global power stability. The proposed criteria complement and improve some existing results in the recent publications, and their effectiveness and advantage are demonstrated by two numerical examples.																	1370-4621	1573-773X				FEB	2020	51	1					867	880		10.1007/s11063-019-10126-5													
J								Learning the Graph Edit Costs Based on a Learning Model Applied to Sub-optimal Graph Matching	NEURAL PROCESSING LETTERS										Graph edit distance; Learning the graph edit costs; Neural network; Probability density function; Graph embedding into vector	DISTANCE; COMPUTATION; OPTIMALITY	Attributed graphs are used to represent patterns composed of several parts in pattern recognition. The nature of these patterns can be diverse, from images, to handwritten characters, maps or fingerprints. Graph edit distance has become an important tool in structural pattern recognition since it allows us to measure the dissimilarity of attributed graphs. It is based on transforming one graph into another through some edit operations such as substitution, deletion and insertion of nodes and edges. It has two main constraints: it requires an adequate definition of the costs of these operations and its computation cost is exponential with regard to the number of nodes. In this paper, we first present a general framework to automatically learn these edit costs considering graph edit distance is computed in a sub-optima way. Then, we specify this framework in two different models based on neural networks and probability density functions. An exhaustive practical validation on 14 public databases, which have different features such as the size of the graphs, the number of attributes or the number of graphs per class have been performed. This validation shows that with the learned edit costs, the accuracy is higher than with some manually imposed costs or other costs automatically learned by previous methods.																	1370-4621	1573-773X				FEB	2020	51	1					881	904		10.1007/s11063-019-10121-w													
J								Training a Neural Network for Cyberattack Classification Applications Using Hybridization of an Artificial Bee Colony and Monarch Butterfly Optimization	NEURAL PROCESSING LETTERS										Intrusion detection system (IDS); Neural network training; A multilayer perceptron (MLP); Swarm Intelligence (SI); Artificial Bee Colony Algorithm (ABC); Monarch Butterfly Optimization (MBO)	PARTICLE SWARM OPTIMIZATION; INTRUSION DETECTION SYSTEM; SUPPORT VECTOR MACHINE; DECISION TREE; ALGORITHM; INTELLIGENCE; DESIGN	Arguably the most recurring issue concerning network security is building an approach that is capable of detecting intrusions into network systems. This issue has been addressed in numerous works using various approaches, of which the most popular one is to consider intrusions as anomalies with respect to the normal traffic in the network and classify network packets as either normal or abnormal. Improving the accuracy and efficiency of this classification is still an open problem to be solved. The study carried out in this article is based on a new approach for intrusion detection that is mainly implemented using the Hybrid Artificial Bee Colony algorithm (ABC) and Monarch Butterfly optimization (MBO). This approach is implemented for preparing an artificial neural system (ANN) in order to increase the precision degree of classification for malicious and non-malicious traffic in systems. The suggestion taken into consideration was to place side-by-side nine other metaheuristic algorithms that are used to evaluate the proposed approach alongside the related works. In the beginning the system is prepared in such a way that it selects the suitable biases and weights utilizing a hybrid (ABC) and (MBO). Subsequently the artificial neural network is retrained by using the information gained from the ideal weights and biases which are obtained from the hybrid algorithm (HAM) to get the intrusion detection approach able to identify new attacks. Three types of intrusion detection evaluation datasets namely KDD Cup 99, ISCX 2012, and UNSW-NB15 were used to compare and evaluate the proposed technique against the other algorithms. The experiment clearly demonstrated that the proposed technique provided significant enhancement compared to the other nine classification algorithms, and that it is more efficient with regards to network intrusion detection.																	1370-4621	1573-773X				FEB	2020	51	1					905	946		10.1007/s11063-019-10120-x													
J								Stochastic Quasi-Synchronization of Delayed Neural Networks: Pinning Impulsive Scheme	NEURAL PROCESSING LETTERS										Quasi-synchronization; Stochastic neural networks; Delay; Pinning impulsive control	COMPLEX DYNAMICAL NETWORKS; STABILITY	This paper studies stochastic quasi-synchronization of delayed neural networks with parameter mismatches and stochastic perturbation mismatch via pinning impulsive control. By pinning selected nodes of stochastic neural network at impulse time, an impulsive control scheme is proposed. Some sufficient conditions are obtained to ensure that the error system can converge to small region in the mean square. Meanwhile, numerical example is provided to illustrate the effectiveness of theoretical results.																	1370-4621	1573-773X				FEB	2020	51	1					947	962		10.1007/s11063-019-10118-5													
J								Second Order Training and Sizing for the Multilayer Perceptron	NEURAL PROCESSING LETTERS										Second order algorithms; Median filtering; Hessian matrix; Growing and pruning; Sigmoidal inputs	NEURAL-NETWORK CLASSIFIERS; LINEAR INEQUALITIES; FACE RECOGNITION; ALGORITHM	An algorithm is developed for automated training of a multilayer perceptron with two non-linear layers. The initial algorithm approximately minimizes validation error with respect to the numbers of both hidden units and training epochs. A median filtering approach is added to reduce deviations between validation and testing errors. Next, the mean-squared error objective function is modified for use with classifiers using a method similar to Ho-Kashyap. Then, both theoretical and practical reasons are provided for introducing growing steps into the algorithm. Lastly, a sigmoidal input layer is added to limit the effects of input outliers and further improve the method. Using widely available datasets, the final network's average testing error is shown to be less than that of several other competing algorithms reported in the literature.																	1370-4621	1573-773X				FEB	2020	51	1					963	991		10.1007/s11063-019-10116-7													
J								Adaptively Denoising Proposal Collection for Weakly Supervised Object Localization	NEURAL PROCESSING LETTERS										Weakly supervised object localization; Proposal subset optimization; Re-weighting; Retraining	AUTOENCODER	In this paper, we address the problem of weakly supervised object localization, which trains a detection network on the dataset with only image-level annotations. The proposed approach is built on the observation that the proposal set from the training dataset is a collection of background, object parts, and objects. Several strategies are taken to adaptively eliminate the noisy proposals and generate pseudo object-level annotations for the weakly labeled dataset. A multiple instance learning algorithm enhanced by mask-out strategy is adopted to collect the class-specific object proposals, which are then utilized to adapt a pre-trained classification network to a detection network. In addition, the detection results from the detection network are re-weighted by jointly considering the detection scores and the overlap ratio of proposals in a proposal subset optimization framework. The optimal proposals work as object-level labels that enable a pseudo-strongly supervised dataset for training the detection network. Consequently, we establish a fully adaptive detection network. Extensive evaluations on the PASCAL VOC 2007 and 2012 datasets demonstrate a significant improvement compared with the state-of-the-art methods.																	1370-4621	1573-773X				FEB	2020	51	1					993	1006		10.1007/s11063-019-10124-7													
J								Regularized siamese neural network for unsupervised outlier detection on brain multiparametric magnetic resonance imaging: Application to epilepsy lesion screening	MEDICAL IMAGE ANALYSIS										Regularized siamese network; Wasserstein autoencoder; Unsupervised representation learning; Brain lesions; Anomaly detection; Deep learning	FOCAL CORTICAL DYSPLASIA; IMPROVES DETECTION; MRI; SURGERY; MALFORMATIONS; SEGMENTATION; OUTCOMES; ATLAS	In this study, we propose a novel anomaly detection model targeting subtle brain lesions in multiparametric MRI. To compensate for the lack of annotated data adequately sampling the heterogeneity of such pathologies, we cast this problem as an outlier detection problem and introduce a novel configuration of unsupervised deep siamese networks to learn normal brain representations using a series of non-pathological brain scans. The proposed siamese network, composed of stacked convolutional autoencoders as subnetworks is designed to map patches extracted from healthy control scans only and centered at the same spatial localization to 'close' representations with respect to the chosen metric in a latent space. It is based on a novel loss function combining a similarity term and a regularization term compensating for the lack of dissimilar pairs. These latent representations are then fed into oc-SVM models at voxel-level to produce anomaly score maps. We evaluate the performance of our brain anomaly detection model to detect subtle epilepsy lesions in multiparametric (T1-weighted, FLAIR) MRI exams considered as normal (MRI-negative). Our detection model trained on 75 healthy subjects and validated on 21 epilepsy patients (with 18 MRI-negatives) achieves a maximum sensitivity of 61% on the MRI-negative lesions, identified among the 5 most suspicious detections on average. It is shown to outperform detection models based on the same architecture but with stacked convolutional or Wasserstein autoencoders as unsupervised feature extraction mechanisms. (C) 2019 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				FEB	2020	60								101618	10.1016/j.media.2019.101618													
J								An automatic genetic algorithm framework for the optimization of three-dimensional surgical plans of forearm corrective osteotomies	MEDICAL IMAGE ANALYSIS										3D Surgical planning; Automatic; Forearm; Corrective osteotomy; Multi-objective optimization	COMPUTER-ASSISTED NAVIGATION; PEDICLE SCREW PLACEMENT; DISTAL RADIUS OSTEOTOMY; PATIENT-SPECIFIC GUIDES; SINGLE-CUT; FRACTURES; KNEE; ACCURACY; RECONSTRUCTION; DEFORMITIES	Three-dimensional (3D) computer-assisted corrective osteotomy has become the state-of-the-art for surgical treatment of complex bone deformities. Despite available technologies, the automatic generation of clinically acceptable, ready-to-use preoperative planning solutions is currently not possible for such pathologies. Multiple contradicting and mutually dependent objectives have to be considered, as well as clinical and technical constraints, which generally require iterative manual adjustments. This leads to unnecessary surgeon efforts and unbearable clinical costs, hindering also the quality of patient treatment due to the reduced number of solutions that can be investigated in a clinically acceptable timeframe. In this paper, we propose an optimization framework for the generation of ready-to-use preoperative planning solutions in a fully automatic fashion. An automatic diagnostic assessment using patient-specific 3D models is performed for 3D malunion quantification and definition of the optimization parameters' range. Afterward, clinical objectives are translated into the optimization module, and controlled through tailored fitness functions based on a weighted and multi-staged optimization approach. The optimization is based on a genetic algorithm capable of solving multi-objective optimization problems with non-linear constraints. The framework outputs a complete preoperative planning solution including position and orientation of the osteotomy plane, transformation to achieve the bone reduction, and position and orientation of the fixation plate and screws. A qualitative validation was performed on 36 consecutive cases of radius osteotomy where solutions generated by the optimization algorithm (OA) were compared against the gold standard solutions generated by experienced surgeons (Gold Standard; GS). Solutions were blinded and presented to 6 readers (4 surgeons, 2 planning engineers), who voted OA solutions to be better in 55% of the time. The quantitative evaluation was based on different error measurements, showing average improvements with respect to the GS from 20% for the reduction alignment and up to 106% for the position of the fixation screws. Notably, our algorithm was able to generate feasible clinical solutions which were not possible to obtain with the current state-of-the-art method. (C) 2019 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				FEB	2020	60								101598	10.1016/j.media.2019.101598													
J								Conflict management in the fusion of complementary segmentations of deformed kidneys and nephroblastoma	MEDICAL IMAGE ANALYSIS										Fusion; Conflict management; Segmentation; Cancer tumour	MULTISENSOR IMAGE FUSION; COLOR; SUPERPIXEL; ALGORITHM; FRAMEWORK; TEXTURE; MODEL	The fusion of multiple segmentations aims to improve their accuracy in order to make them exploitable. However, conflicts may appear. In this paper, two conflict-management models are proposed for the fusion of complementary segmentations. This conflict-management and fusion procedure, integrated into the SAIAD project, carries out the fusion of deformed kidneys and nephroblastoma using the combination of six independent methods. These methods are based on different criteria, like the adjacent segmented slices, the variation of information, the Dice, the neighbouring labels, the pixel intensity by scanner images, and the fully connected CRFs. The performances of our fusion models was evaluated on 139 scans for three patients with nephroblastoma, and the results demonstrate its effectiveness and the improvement of the resulting segmentations. (C) 2019 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				FEB	2020	60								101629	10.1016/j.media.2019.101629													
J								Siam-U-Net: encoder-decoder siamese network for knee cartilage tracking in ultrasound images	MEDICAL IMAGE ANALYSIS										Knee arthroscopy; Knee cartilage; Ultrasound; Ultrasound guidance; Visual tracking; Fully convolutional siamese networks; Deep learning	SEGMENTATION	The tracking of the knee femoral condyle cartilage during ultrasound-guided minimally invasive procedures is important to avoid damaging this structure during such interventions. In this study, we propose a new deep learning method to track, accurately and efficiently, the femoral condyle cartilage in ultrasound sequences, which were acquired under several clinical conditions, mimicking realistic surgical setups. Our solution, that we name Siam-U-Net, requires minimal user initialization and combines a deep learning segmentation method with a siamese framework for tracking the cartilage in temporal and spatio-temporal sequences of 2D ultrasound images. Through extensive performance validation given by the Dice Similarity Coefficient, we demonstrate that our algorithm is able to track the femoral condyle cartilage with an accuracy which is comparable to experienced surgeons. It is additionally shown that the proposed method outperforms state-of-the-art segmentation models and trackers in the localization of the cartilage. We claim that the proposed solution has the potential for ultrasound guidance in minimally invasive knee procedures. Crown Copyright (C) 2019 Published by Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				FEB	2020	60								101631	10.1016/j.media.2019.101631													
J								Multi-resolution convolutional neural networks for fully automated segmentation of acutely injured lungs in multiple species	MEDICAL IMAGE ANALYSIS										Computed tomography; Segmentation; Deep learning	QUANTITATIVE COMPUTED-TOMOGRAPHY; END-EXPIRATORY PRESSURE; CT; ACETYLCYSTEINE; CLASSIFICATION; STRAIN; CANCER; IMAGES; MODEL; SCANS	Segmentation of lungs with acute respiratory distress syndrome (ARDS) is a challenging task due to diffuse opacification in dependent regions which results in little to no contrast at the lung boundary. For segmentation of severely injured lungs, local intensity and texture information, as well as global contextual information, are important factors for consistent inclusion of intrapulmonary structures. In this study, we propose a deep learning framework which uses a novel multi-resolution convolutional neural network (ConvNet) for automated segmentation of lungs in multiple mammalian species with injury models similar to ARDS. The multi-resolution model eliminates the need to tradeoffbetween high-resolution and global context by using a cascade of low-resolution to high-resolution networks. Transfer learning is used to accommodate the limited number of training datasets. The model was initially pre-trained on human CT images, and subsequently fine-tuned on canine, porcine, and ovine CT images with lung injuries similar to ARDS. The multi-resolution model was compared to both high-resolution and low-resolution networks alone. The multi-resolution model outperformed both the low- and high-resolution models, achieving an overall mean Jacaard index of 0.963 +/- 0.025 compared to 0.919 +/- 0.027 and 0.950 +/- 0.036, respectively, for the animal dataset (N = 287). The multi-resolution model achieves an overall average symmetric surface distance of 0.438 +/- 0.315 mm, compared to 0.971 +/- 0.368 mm and 0.657 +/- 0.519 mm for the low-resolution and high-resolution models, respectively. We conclude that the multi-resolution model produces accurate segmentations in severely injured lungs, which is attributed to the inclusion of both local and global features. (C) 2019 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				FEB	2020	60								101592	10.1016/j.media.2019.101592													
J								Deformable mapping using biomechanical models to relate corresponding lesions in digital breast tomosynthesis and automated breast ultrasound images	MEDICAL IMAGE ANALYSIS										Deformable registration; Biomechanical modeling; Digital breast tomosynthesis; Breast ultrasound; Automated breast ultrasound; Finite element methods; Multi-modality imaging	X-RAY MAMMOGRAPHY; MR-IMAGES; CANCER; REGISTRATION; DENSE; VOLUME; WOMEN; US; CT; CLASSIFICATION	This work investigates the application of a deformable localization/mapping method to register lesions between the digital breast tomosynthesis (DBT) craniocaudal (CC) and mediolateral oblique (MLO) views and automated breast ultrasound (ABUS) images. This method was initially validated using compressible breast phantoms. This methodology was applied to 7 patient data sets containing 9 lesions. The automated deformable mapping algorithm uses finite element modeling and analysis to determine corresponding lesions based on the distance between their centers of mass (d(COM)) in the deformed DBT model and the reference ABUS model. This technique shows that location information based on external fiducial markers is helpful in the improvement of registration results. However, use of external markers are not required for deformable registration results described by this methodology. For DBT (CC view) mapped to ABUS, the mean d(COM) was 14.9 +/- 6.8 mm based on 9 lesions using 6 markers in deformable analysis. For DBT (MLO view) mapped to ABUS, the mean d(COM) was 13.7 +/- 6.8 mm based on 8 lesions using 6 markers in analysis. Both DBT views registered to ABUS lesions showed statistically significant improvements (p <= 0.05) in registration using the deformable technique in comparison to a rigid registration. Application of this methodology could help improve a radiologist's characterization and accuracy in relating corresponding lesions between DBT and ABUS image datasets, especially for cases of high breast densities and multiple masses. (C) 2019 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				FEB	2020	60								101599	10.1016/j.media.2019.101599													
J								Multi-modal neuroimaging feature selection with consistent metric constraint for diagnosis of Alzheimer's disease	MEDICAL IMAGE ANALYSIS										Similarity measures; Multi-modal neuroimaging; Feature selection; Alzheimer's disease; Mild cognitive impairment	MILD COGNITIVE IMPAIRMENT; FEATURE REPRESENTATION; CLASSIFICATION; MRI; MEMORY; SEGMENTATION; MORPHOMETRY; BIOMARKERS; FUSION; MODEL	The accurate diagnosis of Alzheimer's disease (AD) and its early stage, e.g., mild cognitive impairment (MCI), is essential for timely treatment or possible intervention to slow down AD progression. Recent studies have demonstrated that multiple neuroimaging and biological measures contain complementary information for diagnosis and prognosis. Therefore, information fusion strategies with multi-modal neuroimaging data, such as voxel-based measures extracted from structural MRI (VBM-MRI) and fluorodeoxyglucose positron emission tomography (FDG-PET), have shown their effectiveness for AD diagnosis. However, most existing methods are proposed to simply integrate the multi-modal data, but do not make full use of structure information across the different modalities. In this paper, we propose a novel multi-modal neuroimaging feature selection method with consistent metric constraint (MFCC) for AD analysis. First, the similarity is calculated for each modality (i.e. VBM-MRI or FDG-PET) individually by random forest strategy, which can extract pairwise similarity measures for multiple modalities. Then the group sparsity regularization term and the sample similarity constraint regularization term are used to constrain the objective function to conduct feature selection from multiple modalities. Finally, the multi-kernel support vector machine (MK-SVM) is used to fuse the features selected from different models for final classification. The experimental results on the Alzheimer's Disease Neuroimaging Initiative (ADNI) show that the proposed method has better classification performance than the startof-the-art multimodality-based methods. Specifically, we achieved higher accuracy and area under the curve (AUC) for AD versus normal controls (NC), MCI versus NC, and MCI converters (MCI-C) versus MCI non-converters (MCI-NC) on ADNI datasets. Therefore, the proposed model not only outperforms the traditional method in terms of AD/MCI classification, but also discovers the characteristics associated with the disease, demonstrating its promise for improving disease-related mechanistic understanding. (C) 2019 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				FEB	2020	60								101625	10.1016/j.media.2019.101625													
J								Tracing in 2D to reduce the annotation effort for 3D deep delineation of linear structures	MEDICAL IMAGE ANALYSIS										Delineation; Segmentation; Deep learning; Nerves; Vessels; Microscopy; Angiography		The difficulty of obtaining annotations to build training databases still slows down the adoption of recent deep learning approaches for biomedical image analysis. In this paper, we show that we can train a Deep Net to perform 3D volumetric delineation given only 2D annotations in Maximum Intensity Projections (MIP) of the training volumes. This significantly reduces the annotation time: We conducted a user study that suggests that annotating 2D projections is on average twice as fast as annotating the original 3D volumes. Our technical contribution is a loss function that evaluates a 3D prediction against annotations of 2D projections. It is inspired by space carving, a classical approach to reconstructing complex 3D shapes from arbitrarily-positioned cameras. It can be used to train any deep network with volumetric output, without the need to change the network's architecture. Substituting the loss is all it takes to enable 2D annotations in an existing training setup. In extensive experiments on 3D light microscopy images of neurons and retinal blood vessels, and on Magnetic Resonance Angiography (MRA) brain scans, we show that, when trained on projection annotations, deep delineation networks perform as well as when they are trained using costlier 3D annotations. (C) 2019 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				FEB	2020	60								101590	10.1016/j.media.2019.101590													
J								Shape and margin-aware lung nodule classification in low-dose CT images via soft activation mapping	MEDICAL IMAGE ANALYSIS										Low-dose CT; Lung nodule classification; Fine-grained features; Convolutional neural network; Soft activation mapping	FALSE-POSITIVE REDUCTION; CANCER; NETWORKS; SCANS	A number of studies on lung nodule classification lack clinical/biological interpretations of the features extracted by convolutional neural network (CNN). The methods like class activation mapping (CAM) and gradient-based CAM (Grad-CAM) are tailored for interpreting localization and classification tasks while they ignored fine-grained features. Therefore, CAM and Grad-CAM cannot provide optimal interpretation for lung nodule categorization task in low-dose CT images, in that fine-grained pathological clues like discrete and irregular shape and margins of nodules are capable of enhancing sensitivity and specificity of nodule classification with regards to CNN. In this paper, we first develop a soft activation mapping (SAM) to enable fine-grained lung nodule shape & margin (LNSM) feature analysis with a CNN so that it can access rich discrete features. Secondly, by combining high-level convolutional features with SAM, we further propose a high-level feature enhancement scheme (HESAM) to localize LNSM features. Experiments on the LIDC-IDRI dataset indicate that 1) SAM captures more fine-grained and discrete attention regions than existing methods, 2) HESAM localizes more accurately on LNSM features and obtains the state-of-the-art predictive performance, reducing the false positive rate, and 3) we design and conduct a visually matching experiment which incorporates radiologists study to increase the confidence level of applying our method to clinical diagnosis. (C) 2019 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				FEB	2020	60								101628	10.1016/j.media.2019.101628													
J								LinSEM: Linearizing segmentation evaluation metrics for medical images	MEDICAL IMAGE ANALYSIS										Medical image segmentation; Evaluation metrics; Acceptability score; Linear relationship	LIVER SEGMENTATION; SHAPE MODELS; ATLAS	Numerous algorithms are available for segmenting medical images. Empirical discrepancy metrics are commonly used in measuring the similarity or difference between segmentations by algorithms and "true" segmentations. However, one issue with the commonly used metrics is that the same metric value often represents different levels of "clinical acceptability" for different objects depending on their size, shape, and complexity of form. An ideal segmentation evaluation metric should be able to reflect degrees of acceptability directly from metric values and be able to show the same acceptability meaning by the same metric value for objects of different shape, size, and form. Intuitively, metrics which have a linear relationship with degree of acceptability will satisfy these conditions of the ideal metric. This issue has not been addressed in the medical image segmentation literature. In this paper, we propose a method called LinSEM for linearizing commonly used segmentation evaluation metrics based on corresponding degrees of acceptability evaluated by an expert in a reader study. LinSEM consists of two main parts: (a) estimating the relationship between metric values and degrees of acceptability separately for each considered metric and object, and (b) linearizing any given metric value corresponding to a given segmentation of an object based on the estimated relationship. Since algorithmic segmentations do not usually cover the full range of variability of acceptability, we create a set (S-S) of simulated segmentations for each object that guarantee such coverage by using image transformations applied to a set (S-T) of true segmentations of the object. We then conduct a reader study wherein the reader assigns an acceptability score (AS) for each sample in S-S, expressing the acceptability of the sample on a 1 to 5 scale. Then the metric-AS relationship is constructed for the object by using an estimation method. With the idea that the ideal metric should be linear with respect to acceptability, we can then linearize the metric value of any segmentation sample of the object from a set (S-A) of actual segmentations to its linearized value by using the constructed metric-acceptability relationship curve. Experiments are conducted involving three metrics - Dice coefficient (DC), Jaccard index (JI), and HausdorffDistance (HD) - on five objects: skin outer boundary of the head and neck (cervico-thoracic) body region superior to the shoulders, right parotid gland, mandible, cervical esophagus, and heart. Actual segmentations (S-A) of these objects are generated via our Automatic Anatomy Recognition (AAR) method. Our results indicate that, generally, JI has a more linear relationship with acceptability before linearization than other metrics. LinSEM achieves significantly improved uniformity of meaning post-linearization across all tested objects and metrics, except in a few cases where the departure from linearity was insignificant. This improvement is generally the largest for DC and HD reaching 8-25% for many tested cases. Although some objects (such as right parotid gland and esophagus for DC and JI) are close in their meaning between themselves before linearization, they are distant in this meaning from other objects but are brought close to other objects after linearization. This suggests the importance of performing linearization considering all objects in a body region and body-wide. (C) 2019 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				FEB	2020	60								101601	10.1016/j.media.2019.101601													
J								Atrial scar quantification via multi-scale CNN in the graph-cuts framework	MEDICAL IMAGE ANALYSIS										Atrial fibrillation; Left atrium; LGE MRI; Scar segmentation; Graph learning; Multi-scale CNN	WHOLE HEART SEGMENTATION; CATHETER ABLATION; PULMONARY VEIN; FIBRILLATION; MRI; MANAGEMENT; THICKNESS; SELECTION; MODEL; CT	Late gadolinium enhancement magnetic resonance imaging (LGE MRI) appears to be a promising alternative for scar assessment in patients with atrial fibrillation (AF). Automating the quantification and analysis of atrial scars can be challenging due to the low image quality. In this work, we propose a fully automated method based on the graph-cuts framework, where the potentials of the graph are learned on a surface mesh of the left atrium (LA) using a multi-scale convolutional neural network (MS-CNN). For validation, we have included fifty-eight images with manual delineations. MS-CNN, which can efficiently incorporate both the local and global texture information of the images, has been shown to evidently improve the segmentation accuracy of the proposed graph-cuts based method. The segmentation could be further improved when the contribution between the t-link and n-link weights of the graph is balanced. The proposed method achieves a mean accuracy of 0.856 +/- 0.033 and mean Dice score of 0.702 +/- 0.071 for LA scar quantification. Compared to the conventional methods, which are based on the manual delineation of LA for initialization, our method is fully automatic and has demonstrated significantly better Dice score and accuracy (p < 0.01). The method is promising and can be potentially useful in diagnosis and prognosis of AF. (C) 2019 The Author(s). Published by Elsevier B.V.																	1361-8415	1361-8423				FEB	2020	60								101595	10.1016/j.media.2019.101595													
J								Major depressive disorder identification by referenced multiset canonical correlation analysis with clinical scores	MEDICAL IMAGE ANALYSIS										Canonical correlation analysis; Functional connection; Functional magnetic resonance imaging; Linear discrimination analysis; Major depressive disorder identification	INDEPENDENT VECTOR ANALYSIS; FUNCTIONAL CONNECTIVITY; RATING-SCALE; FMRI; SINGLE; SETS; ICA	A novel method based on multiset canonical correlation analysis (mCCA) and linear discriminant analysis (LDA) is presented to identify the major depressive disorder (MDD). The new method comprises two parts, namely, the mCCA-rreg and sparse LDA models. The mCCA-rreg model extends the classical canonical correlation model to calculate functional connections by restricting the references to a reference space and adding a spatial regularization term. The reference space is used to ensure that the model extracts important components first from several datasets simultaneously by decreasing the importance of the components in which we are uninterested. The spatial regularization term helps in avoiding the multicollinearity and overfitting problems under the low signal-to-noise ratio circumstance. The sparse LDA model extends the classical LDA model to extract a small subset of discriminative classification features by fusing clinical scores. In the real data experiment, we extract two functional connection modes from 45 subjects by the mCCA-rreg model. Then, we construct classifiers to identify the patients with MDD based on the connections selected by the sparse LDA model. The best accuracy is higher than 95%. The results show that the mCCA-rreg model can retrieve the important components characterized by a preassigned reference space and exclude the noise or components of no interest. The sparse LDA model can extract discriminative classification features related to clinical scores. (C) 2019 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				FEB	2020	60								101600	10.1016/j.media.2019.101600													
J								Non-invasive estimation of relative pressure in turbulent flow using virtual work-energy	MEDICAL IMAGE ANALYSIS										Relative pressure; 4D flow MRI; Virtual work-energy; Turbulence; Turbulent energy dissipation; Fluid mechanics	STENOTIC AORTIC FLOW; ECHOCARDIOGRAPHIC-ASSESSMENT; STENOSIS SEVERITY; KINETIC-ENERGY; VALVE; QUANTIFICATION; RECOVERY; MRI; HEMODYNAMICS; COMPONENTS	Vascular pressure differences are established risk markers for a number of cardiovascular diseases. Relative pressures are, however, often driven by turbulence-induced flow fluctuations, where conventional non-invasive methods may yield inaccurate results. Recently, we proposed a novel method for non-turbulent flows, nu WERP, utilizing the concept of virtual work-energy to accurately probe relative pressure through complex branching vasculature. Here, we present an extension of this approach for turbulent flows: nu WERP-t. We present a theoretical method derivation based on flow covariance, quantifying the impact of flow fluctuations on relative pressure. nu WERP-t is tested on a set of in-vitro stenotic flow phantoms with data acquired by 4D flow MRI with six-directional flow encoding, as well as on a patientspecific in-silico model of an acute aortic dissection. Over all tests nu WERP-t shows improved accuracy over alternative energy-based approaches, with excellent recovery of estimated relative pressures. In particular, the use of a guaranteed divergence-free virtual field improves accuracy in cases where turbulent flows skew the apparent divergence of the acquired field. With the original nu WERP allowing for assessment of relative pressure into previously inaccessible vasculatures, the extended nu WERP-t further enlarges the method's clinical scope, underlining its potential as a novel tool for assessing relative pressure in-vivo. (C) 2019 The Authors. Published by Elsevier B.V.																	1361-8415	1361-8423				FEB	2020	60								101627	10.1016/j.media.2019.101627													
J								Joint functional brain network atlas estimation and feature selection for neurological disorder diagnosis with application to autism	MEDICAL IMAGE ANALYSIS										Functional network atlas estimation; Brain network fusion; Connectomic feature selection; Multi-kernel network manifold learning; Discriminative biomarker identification; Brain connectome; Autism spectrum disorder; Classification	SPECTRUM DISORDERS; IMAGE REGISTRATION; FRONTAL-LOBE; CHILDREN; CLASSIFICATION; CONNECTIVITY; PREDICTION	Image-based brain maps, generally coined as `intensity or image atlases', have led the field of brain mapping in health and disease for decades, while investigating a wide spectrum of neurological disorders. Estimating representative brain atlases constitute a fundamental step in several MRI-based neurological disorder mapping, diagnosis, and prognosis. However, these are strikingly lacking in the field of brain connectomics, where connectional brain atlases derived from functional MRI (fRMI) or diffusion MRI (dMRI) are almost absent. On the other hand, conventional connectomic-based classification methods traditionally resort to feature selection methods to decrease the high-dimensionality of connectomic data for learning how to diagnose new patients. However, these are generally limited by high computational cost and a large variability in performance across different datasets, which might hinder the identification of reproducible biomarkers. To address both limitations, we unprecedentedly propose a brain network atlas-guided feature selection (NAG-FS) method to disentangle the healthy from the disordered connectome. To this aim, given a population of brain connectomes, we propose to learn how estimate a centered and representative functional brain network atlas (i.e., a population center) to reliably map the functional connectome and its variability across training individuals, thereby capturing their shared traits (i.e., connectional fingerprint of a population). Essentially, we first learn the pairwise similarities between connectomes in the population to map them into different subspaces. Next, we non-linearly diffuse and fuse connectomes living in each subspace, respectively. By integrating the produced subspace-specific network atlases we ultimately estimate the population network atlas. Last, we compute the difference between healthy and disordered network atlases to identify the most discriminative features, which are then used to train a predictive learner. Our method boosted the classification performance by 6% in comparison to state-of-the-art FS methods when classifying autistic and healthy subjects. (C) 2019 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				FEB	2020	60								101596	10.1016/j.media.2019.101596													
J								Analysis of nonstandardized stress echocardiography sequences using multiview dimensionality reduction	MEDICAL IMAGE ANALYSIS										Multiview dimensionality reduction; Multiple kernel learning; Stress echocardiography; Pattern analysis	EXERCISE; DOBUTAMINE; STRAIN	Alternative stress echocardiography protocols such as handgrip exercise are potentially more favorable towards large-scale screening scenarios than those currently adopted in clinical practice. However, these are still underexplored because the maximal exercise levels are not easily quantified and regulated, requiring the analysis of the complete data sequences (thousands of images), which represents a challenging task for the clinician. We propose a framework for the analysis of these complex datasets, and illustrate it on a handgrip exercise dataset including complete acquisitions of 10 healthy controls and 5 ANT1 mutation patients (1377 cardiac cycles). The framework is based on an unsupervised formulation of multiple kernel learning, which is used to integrate information coming from myocardial velocity traces and heart rate to obtain a lower-dimensional representation of the data. Such simplified representation is then explored to discriminate groups of response and understand the underlying pathophysiological mechanisms. The analysis pipeline involves the reconstruction of population-specific signatures using multiscale kernel regression, and the clustering of subjects based on the trajectories defined by their projected sequences. The results confirm that the proposed framework is able to detect distinctive clusters of response and to provide insight regarding the underlying pathophysiology. (C) 2019 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				FEB	2020	60								101594	10.1016/j.media.2019.101594													
J								A partial augmented reality system with live ultrasound and registered preoperative MRI for guiding robot-assisted radical prostatectomy	MEDICAL IMAGE ANALYSIS										Image-guidance; Surgery; Ultrasound; Magnetic resonance imaging; Image fusion; Registration	RESONANCE-IMAGING MRI; GUIDANCE; FUSION; BIOPSY; ULTRASONOGRAPHY; NAVIGATION; SURGERY; MODEL	We propose an image guidance system for robot assisted laparoscopic radical prostatectomy (RALRP). A virtual 3D reconstruction of the surgery scene is displayed underneath the endoscope's feed on the surgeon's console. This scene consists of an annotated preoperative Magnetic Resonance Image (MRI) registered to intraoperative 3D Trans-rectal Ultrasound (TRUS) as well as real-time sagittal 2D TRUS images of the prostate, 3D models of the prostate, the surgical instrument and the TRUS transducer. We display these components with accurate real-time coordinates with respect to the robot system. Since the scene is rendered from the viewpoint of the endoscope, given correct parameters of the camera, an augmented scene can be overlaid on the video output. The surgeon can rotate the ultrasound transducer and determine the position of the projected axial plane in the MRI using one of the registered da Vinci instruments. This system was tested in the laboratory on custom-made agar prostate phantoms. We achieved an average total registration accuracy of 3.2 +/- 1.3 mm. We also report on the successful application of this system in the operating room in 12 patients. The average registration error between the TRUS and the da Vinci system for the last 8 patients was 1.4 +/- 0.3 mm and average target registration error of 2.1 +/- 0.8 mm, resulting in an in vivo overall robot system to MRI mean registration error of 3.5 mm or less, which is consistent with our laboratory studies. (C) 2019 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				FEB	2020	60								101588	10.1016/j.media.2019.101588													
J								Hierarchical Bayesian myocardial perfusion quantification	MEDICAL IMAGE ANALYSIS										Bayesian inference; Tracer-kinetic modelling; Myocardial perfusion MRI	MAGNETIC-RESONANCE PERFUSION; CORONARY-ARTERY-DISEASE; DIAGNOSTIC PERFORMANCE; DCE-MRI; MODEL; RESERVE; ACCURACY; FLOW	Myocardial blood flow can be quantified from dynamic contrast-enhanced magnetic resonance (MR) images through the fitting of tracer-kinetic models to the observed imaging data. The use of multi-compartment exchange models is desirable as they are physiologically motivated and resolve directly for both blood flow and microvascular function. However, the parameter estimates obtained with such models can be unreliable. This is due to the complexity of the models relative to the observed data which is limited by the low signal-to-noise ratio, the temporal resolution, the length of the acquisitions and other complex imaging artefacts. In this work, a Bayesian inference scheme is proposed which allows the reliable estimation of the parameters of the two-compartment exchange model from myocardial perfusion MR data. The Bayesian scheme allows the incorporation of prior knowledge on the physiological ranges of the model parameters and facilitates the use of the additional information that neighbouring voxels are likely to have similar kinetic parameter values. Hierarchical priors are used to avoid making a priori assumptions on the health of the patients. We provide both a theoretical introduction to Bayesian inference for tracer-kinetic modelling and specific implementation details for this application. This approach is validated in both in silico and in vivo settings. In silico, there was a significant reduction in mean-squared error with the ground-truth parameters using Bayesian inference as compared to using the standard non-linear least squares fitting. When applied to patient data the Bayesian inference scheme returns parameter values that are in-line with those previously reported in the literature, as well as giving parameter maps that match the independant clinical diagnosis of those patients. (C) 2019 The Authors. Published by Elsevier B.V.																	1361-8415	1361-8423				FEB	2020	60								101611	10.1016/j.media.2019.101611													
J								Graph temporal ensembling based semi-supervised convolutional neural network with noisy labels for histopathology image analysis	MEDICAL IMAGE ANALYSIS										Semi-supervised; Noisy labels; Convolutional neural network; Histopathology image classification		Although convolutional neural networks have achieved tremendous success on histopathology image classification, they usually require large-scale clean annotated data and are sensitive to noisy labels. Unfortunately, labeling large-scale images is laborious, expensive and lowly reliable for pathologists. To address these problems, in this paper, we propose a novel self-ensembling based deep architecture to leverage the semantic information of annotated images and explore the information hidden in unlabeled data, and meanwhile being robust to noisy labels. Specifically, the proposed architecture first creates ensemble targets for feature and label predictions of training samples, by using exponential moving average (EMA) to aggregate feature and label predictions within multiple previous training epochs. Then, the ensemble targets within the same class are mapped into a cluster so that they are further enhanced. Next, a consistency cost is utilized to form consensus predictions under different configurations. Finally, we validate the proposed method with extensive experiments on lung and breast cancer datasets that contain thousands of images. It can achieve 90.5% and 89.5% image classification accuracy using only 20% labeled patients on the two datasets, respectively. This performance is comparable to that of the baseline method with all labeled patients. Experiments also demonstrate its robustness to small percentage of noisy labels. (C) 2019 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				FEB	2020	60								101624	10.1016/j.media.2019.101624													
J								Tensor-cut: A tensor-based graph-cut blood vessel segmentation method and its application to renal artery segmentation	MEDICAL IMAGE ANALYSIS										Blood vessel segmentation; Graph-cut; Renal artery; Tensor; Hessian matrix; Riemannian manifold	RIEMANNIAN FRAMEWORK; TREE; EXTRACTION; MRI	Blood vessel segmentation plays a fundamental role in many computer-aided diagnosis (CAD) systems, such as coronary artery stenosis quantification, cerebral aneurysm quantification, and retinal vascular tree analysis. Fine blood vessel segmentation can help build a more accurate computer-aided diagnosis system and help physicians gain a better understanding of vascular structures. The purpose of this article is to develop a blood vessel segmentation method that can improve segmentation accuracy in tiny blood vessels. In this work, we propose a tensor-based graph-cut method for blood vessel segmentation. With our method, each voxel can be modeled by a second-order tensor, allowing the capture of the intensity information and the geometric information for building a more accurate model for blood vessel segmentation. We compared our proposed method's accuracy to several state-of-the-art blood vessel segmentation algorithms and performed experiments on both simulated and clinical CT datasets. Both experiments showed that our method achieved better state-of-the-art results than the competing techniques. The mean centerline overlap ratio of our proposed method is 84% on clinical CT data. Our proposed blood vessel segmentation method outperformed other state-of-the-art methods by 10% on clinical CT data. Tiny blood vessels in clinical CT data with a 1-mm radius can be extracted using the proposed technique. The experiments on a clinical dataset showed that the proposed method significantly improved the segmentation accuracy in tiny blood vessels. (C) 2019 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				FEB	2020	60								101623	10.1016/j.media.2019.101623													
J								Uncertainty and interpretability in convolutional neural networks for semantic segmentation of colorectal polyps	MEDICAL IMAGE ANALYSIS										Polyp segmentation; Decision support systems; Fully convolutional networks; Monte carlo dropout; Guided backpropagation; Monte carlo guided backpropagation	CANCER STATISTICS; COLONOSCOPY; VALIDATION	Colorectal polyps are known to be potential precursors to colorectal cancer, which is one of the leading causes of cancer-related deaths on a global scale. Early detection and prevention of colorectal cancer is primarily enabled through manual screenings, where the intestines of a patient is visually examined. Such a procedure can be challenging and exhausting for the person performing the screening. This has resulted in numerous studies on designing automatic systems aimed at supporting physicians during the examination. Recently, such automatic systems have seen a significant improvement as a result of an increasing amount of publicly available colorectal imagery and advances in deep learning research for object image recognition. Specifically, decision support systems based on Convolutional Neural Networks (CNNs) have demonstrated state-of-the-art performance on both detection and segmentation of colorectal polyps. However, CNN-based models need to not only be precise in order to be helpful in a medical context. In addition, interpretability and uncertainty in predictions must be well understood. In this paper, we develop and evaluate recent advances in uncertainty estimation and model interpretability in the context of semantic segmentation of polyps from colonoscopy images. Furthermore, we propose a novel method for estimating the uncertainty associated with important features in the input and demonstrate how interpretability and uncertainty can be modeled in DSSs for semantic segmentation of colorectal polyps. Results indicate that deep models are utilizing the shape and edge information of polyps to make their prediction. Moreover, inaccurate predictions show a higher degree of uncertainty compared to precise predictions. (C) 2019 The Authors. Published by Elsevier B.V.																	1361-8415	1361-8423				FEB	2020	60								101619	10.1016/j.media.2019.101619													
J								A robust deep neural network for denoising task-based fMRI data: An application to working memory and episodic memory	MEDICAL IMAGE ANALYSIS										fMRI denoising; Deep neural network; Working memory; Episodic memory	RESTING-STATE FMRI; FUNCTIONAL CONNECTIVITY MRI; GLOBAL SIGNAL; MOTION ARTIFACT; NOISE; BOLD; FLUCTUATIONS; ICA; REDUCTION; ANTICORRELATIONS	In this study, a deep neural network (DNN) is proposed to reduce the noise in task-based fMRI data without explicitly modeling noise. The DNN artificial neural network consists of one temporal convolutional layer, one long short-term memory (LSTM) layer, one time-distributed fully-connected layer, and one unconventional selection layer in sequential order. The LSTM layer takes not only the current time point but also what was perceived in a previous time point as its input to characterize the temporal autocorrelation of fMRI data. The fully-connected layer weights the output of the LSTM layer, and the output denoised fMRI time series is selected by the selection layer. Assuming that task-related neural response is limited to gray matter, the model parameters in the DNN network are optimized by maximizing the correlation difference between gray matter voxels and white matter or ventricular cerebrospinal fluid voxels. Instead of targeting a particular noise source, the proposed neural network takes advantage of the task design matrix to better extract task-related signal in fMRI data. The DNN network, along with other traditional denoising techniques, has been applied on simulated data, working memory task fMRI data acquired from a cohort of healthy subjects and episodic memory task fMRI data acquired from a small set of healthy elderly subjects. Qualitative and quantitative measurements were used to evaluate the performance of different denoising techniques. In the simulation, DNN improves fMRI activation detection and also adapts to varying hemodynamic response functions across different brain regions. DNN efficiently reduces physiological noise and generates more homogeneous task-response correlation maps in real data. (C) 2019 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				FEB	2020	60								101622	10.1016/j.media.2019.101622													
J								Automatic kidney segmentation in ultrasound images using subsequent boundary distance regression and pixelwise classification networks	MEDICAL IMAGE ANALYSIS										Ultrasound images; Boundary detection; Boundary distance regression; Pixelwise classification		It remains challenging to automatically segment kidneys in clinical ultrasound (US) images due to the kidneys' varied shapes and image intensity distributions, although semi-automatic methods have achieved promising performance. In this study, we propose subsequent boundary distance regression and pixel classification networks to segment the kidneys automatically. Particularly, we first use deep neural networks pre-trained for classification of natural images to extract high-level image features from US images. These features are used as input to learn kidney boundary distance maps using a boundary distance regression network and the predicted boundary distance maps are classified as kidney pixels or non-kidney pixels using a pixelwise classification network in an end-to-end learning fashion. We also adopted a data-augmentation method based on kidney shape registration to generate enriched training data from a small number of US images with manually segmented kidney labels. Experimental results have demonstrated that our method could automatically segment the kidney with promising performance, significantly better than deep learning-based pixel classification networks. (C) 2019 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				FEB	2020	60								101602	10.1016/j.media.2019.101602													
J								Context-guided fully convolutional networks for joint craniomaxillofacial bone segmentation and landmark digitization	MEDICAL IMAGE ANALYSIS										Cone-beam computed tomography; Landmark digitization; Bone segmentation; Fully convolutional networks	EFFICIENT ANATOMY DETECTION; REGRESSION FORESTS; LOCALIZATION; CBCT	Cone-beam computed tomography (CBCT) scans are commonly used in diagnosing and planning surgical or orthodontic treatment to correct craniomaxillofacial (CMF) deformities. Based on CBCT images, it is clinically essential to generate an accurate 3D model of CMF structures (e.g., midface, and mandible) and digitize anatomical landmarks. This process often involves two tasks, i.e., bone segmentation and anatomical landmark digitization. Because landmarks usually lie on the boundaries of segmented bone regions, the tasks of bone segmentation and landmark digitization could be highly associated. Also, the spatial context information (e.g., displacements from voxels to landmarks) in CBCT images is intuitively important for accurately indicating the spatial association between voxels and landmarks. However, most of the existing studies simply treat bone segmentation and landmark digitization as two standalone tasks without considering their inherent relationship, and rarely take advantage of the spatial context information contained in CBCT images. To address these issues, we propose a Joint bone Segmentation and landmark Digitization (JSD) framework via context-guided fully convolutional networks (FCNs). Specifically, we first utilize displacement maps to model the spatial context information in CBCT images, where each element in the displacement map denotes the displacement from a voxel to a particular landmark. An FCN is learned to construct the mapping from the input image to its corresponding displacement maps. Using the learned displacement maps as guidance, we further develop a multi-task FCN model to perform bone segmentation and landmark digitization jointly. We validate the proposed JSD method on 107 subjects, and the experimental results demonstrate that our method is superior to the state-of-the-art approaches in both tasks of bone segmentation and landmark digitization. (C) 2019 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				FEB	2020	60								101621	10.1016/j.media.2019.101621													
J								Multi-indices quantification of optic nerve head in fundus image via multitask collaborative learning	MEDICAL IMAGE ANALYSIS										Multi-indices quantification; Optic nerve head assessment; Collaborative learning; Glaucoma diagnosis	CUP SEGMENTATION; DISC	Multi-indices quantification of optic nerve head (ONH), measuring ONH appearance with multiple types of indices simultaneously from fundus images, is the most clinically significant tasks for accurate ONH assessment and ophthalmic disease diagnosis. However, no attempt has been reported due to its challenges of the large variation of fundus appearance across patients, heavy overlap and extremely weak contrast between optic nerve head areas. In this paper, we propose a multitask collaborative learning framework (MCL-Net) for multi-indices ONH quantification. The proposed MCL-Net, a two-branch neural network, first obtains expressive shared and task-specific representations with the backbone network and its two branches; then models the feature exchanges and aggregations between two branches with a well-designed feature interaction module (FIM) to promote each other collaboratively. After that, it estimates multiple types of ONH indices under a multitask ensemble module (MEM) that is capable of learning aggregation of multiple outputs automatically. Therefore, the proposed MCL-Net is consisted of the feature representation, inter-task feature interaction, dual-branch task-specific prediction, and multitask quantification ensemble, which establish an effective framework which takes full advantages of segmentation and estimation tasks for multi-indices ONH quantification. Rather than the low-level feature sharing and individual prediction, the proposed MCL-Net collaboratively learns an optimal combination of shared and task-specific representation, as well as the aggregated prediction, therefore leads to accurate quantification of ONH with multiple types of indices. Experimental results on the dataset of 650 fundus images show that MCL-Net successfully delivers accurate quantification of all the three types of ONH indices, with average mean absolute error of 0.98 +/- 0.20, 0.97 +/- 0.16, 1.19 +/- 0.18, as well as average correlation coefficient of 0.699, 0.708 and 0.691, for diameters, whole areas and regional areas, respectively. In addition, the experiments demonstrate that quantitative indices obtained by our method provide more effective glaucoma diagnosis with AUC of 0.8698. This endows our proposed MCL-Net a great potential in clinical assessment from focal to global for ophthalmic disease diagnosis. (C) 2019 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				FEB	2020	60								101593	10.1016/j.media.2019.101593													
J								Multi-modal latent space inducing ensemble SVM classifier for early dementia diagnosis with neuroimaging data	MEDICAL IMAGE ANALYSIS										Alzheimer's disease (AD); Multi-modality data; Missing modality; Latent space; Multiple diversified classifiers	MILD COGNITIVE IMPAIRMENT; ALZHEIMERS-DISEASE DIAGNOSIS; FEATURE-SELECTION; BRAIN ATROPHY; MR-IMAGES; SEGMENTATION; CONVERSION	Fusing multi-modality data is crucial for accurate identification of brain disorder as different modalities can provide complementary perspectives of complex neurodegenerative disease. However, there are at least four common issues associated with the existing fusion methods. First, many existing fusion methods simply concatenate features from each modality without considering the correlations among different modalities. Second, most existing methods often make prediction based on a single classifier, which might not be able to address the heterogeneity of the Alzheimer's disease (AD) progression. Third, many existing methods often employ feature selection (or reduction) and classifier training in two independent steps, without considering the fact that the two pipelined steps are highly related to each other. Forth, there are missing neuroimaging data for some of the participants (e.g., missing PET data), due to the participants' "no-show" or dropout. In this paper, to address the above issues, we propose an early AD diagnosis framework via novel multi-modality latent space inducing ensemble SVM classifier. Specifically, we first project the neuroimaging data from different modalities into a latent space, and then map the learned latent representations into the label space to learn multiple diversified classifiers. Finally, we obtain the more reliable classification results by using an ensemble strategy. More importantly, we present a Complete Multi-modality Latent Space (CMLS) learning model for complete multi-modality data and also an Incomplete Multi-modality Latent Space (IMLS) learning model for incomplete multi-modality data. Extensive experiments using the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset have demonstrated that our proposed models outperform other state-of-the-art methods. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				FEB	2020	60								101630	10.1016/j.media.2019.101630													
J								A computational Framework for generating rotation invariant features and its application in diffusion MRI	MEDICAL IMAGE ANALYSIS										Diffusion MRI; Rotation invariants; Spherical harmonics; Gaunt coefficients; Biomarkers	AXON DIAMETER; DENSITY; COEFFICIENTS; ANISOTROPY	In this work, we present a novel computational framework for analytically generating a complete set of algebraically independent Rotation Invariant Features (RIF) given the Laplace-series expansion of a spherical function. Our computational framework provides a closed-form solution for these new invariants, which are the natural expansion of the well known spherical mean, power-spectrum and bispectrum invariants. We highlight the maximal number of algebraically independent invariants which can be obtained from a truncated Spherical Harmonic (SH) representation of a spherical function and show that most of these new invariants can be linked to statistical and geometrical measures of spherical functions, such as the mean, the variance and the volume of the spherical signal. Moreover, we demonstrate their application to dMRI signal modeling including the Apparent Diffusion Coefficient (ADC), the diffusion signal and the fiber Orientation Distribution Function (fODF). In addition, using both synthetic and real data, we test the ability of our invariants to estimate brain tissue microstructure in healthy subjects and show that our framework provides more flexibility and open up new opportunities for innovative development in the domain of microstructure recovery from diffusion MRI. (C) 2019 The Authors. Published by Elsevier B.V.																	1361-8415	1361-8423				FEB	2020	60								101597	10.1016/j.media.2019.101597													
J								A New Family of Fuzzy Discrete Choice Models	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Biological system modeling; Decision making; Probabilistic logic; Sociology; Statistics; Optimization; Fuzzy sets; Attribute interaction; choice behavior; decision analysis; discrete choice probability; fuzzy	MULTINOMIAL LOGIT MODEL; CORPORATE BANKRUPTCY; UTILITY-THEORY; RISK; AGGREGATION; CRITERIA	Often in real-world decision making, it is difficult to crisply evaluate the utility values as required in the case of conventional choice models. Besides, a decision maker (DM) has his/her own relative importance for each of the attributes. The attributes may also be interacting positively (synergy) or negatively, the degree of which is specific to the DM. A new family of discrete choice models is introduced with a motivation that takes into account the human factors in real-world multiattribute decision making. More specifically, the proposed choice models are based on fuzzy subjective utilities that are easier to elicit. The proposed models are further extended to take into account the unique attitudinal character of the DM, the relative weight vector, and the degree of interaction among the different attributes. A real case study illustrates the usefulness of the study.																	1063-6706	1941-0034				FEB	2020	28	2					205	214		10.1109/TFUZZ.2019.2902108													
J								Symmetric and Right-Hand-Side Hesitant Fuzzy Linear Programming	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Fuzzy sets; Hafnium; Linear programming; Mathematical programming; Decision making; Mathematical model; Fuzzy linear programming (FLP); hesitant fuzzy linear programming (HFLP); hesitant fuzzy set (HFS)	DECISION-MAKING; INFORMATION FUSION; RANKING; NUMBERS; SETS; AGGREGATION; PREFERENCE; OPERATORS; DISTANCE	Fuzzy set theory has been extensively employed in mathematical programming, especially in linear programming problems. As a generalization of fuzzy sets, a hesitant fuzzy set is a very useful tool in places where there are some hesitations in determining the membership of an element to a set. There are few studies on hesitant fuzzy linear programming problems; therefore, in this paper, we have studied such problems. For this purpose, at first, the motivation of this paper is explained; then, types of hesitant fuzzy linear programming models are introduced. Since it is not easy to examine all of the hesitant fuzzy models for the linear programming problems in one paper, we have restricted ourselves to symmetric and right-hand-side hesitant fuzzy linear programming problems with the flexible approach and then proposed two new approaches to solve them. Finally, to illustrate the applicability of the proposed approaches, three examples under hesitant fuzzy information are given.																	1063-6706	1941-0034				FEB	2020	28	2					215	227		10.1109/TFUZZ.2019.2902109													
J								Modeling of Multivariable Fuzzy Systems by Semitensor Product	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Fuzzy systems; Mathematical model; Fuzzy sets; MIMO communication; Fuzzy logic; Data models; Matrix converters; Fuzzy relation matrix (FRM) models; semitensor product (STP); system identification; system optimization	NONLINEAR-SYSTEMS; DESIGN; STABILIZATION	A new fuzzy formulation technique based on semitensor product of matrices is proposed in this paper to construct multivariable fuzzy logic systems. A fuzzy relation matrix (FRM) model is suggested for direct modeling and indirect identification of the fuzzy matrix expression using the observed input-output data of fuzzy dynamical process. The center points of fuzzy sets for every output variable are optimized by using least squares estimation. The effectiveness of the proposed fuzzy formulation technology is verified by simulations. Test results show that the proposed formulation technique is an effective design method to construct and optimize fuzzy relation structure matrices for multivariable fuzzy systems. It can be used to design FRM models for multi-input multi-output systems.																	1063-6706	1941-0034				FEB	2020	28	2					228	235		10.1109/TFUZZ.2019.2902820													
J								H-infinity Observer Design for Fuzzy System With Immeasurable State Variables via a New Lyapunov Function	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Fuzzy system; H-infinity observers; immeasurable state variables; Lyapunov function	OUTPUT-FEEDBACK CONTROL; STABILITY ANALYSIS; FAULT-DETECTION; CONTROLLER-DESIGN; PERFORMANCE	This paper is concerned with the design of observers for fuzzy system with immeasurable state variables via a new Lyapunov function. By decomposing the space of premise variables into crisp regions and fuzzy regions, piecewise fuzzy observers are established at different instants to cope with the challenges caused by the mismatch of the premise variables and exogenous disturbances. Based on the proposed new Lyapunov functions that depend on the different regions where the premise variables are located, sufficient conditions in terms of linear matrix inequalities are obtained to guarantee the stability and desired performance of the error systems. Finally, a simulation example is provided to show that the new design technology proposed in this paper achieves better performance than the existing design methods.																	1063-6706	1941-0034				FEB	2020	28	2					236	245		10.1109/TFUZZ.2019.2903762													
J								Fixed-Time Stabilization for IT2 T-S Fuzzy Interconnected Systems via Event-Triggered Mechanism: An Exponential Gain Method	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Interconnected systems; Fuzzy sets; Uncertainty; Silicon; Control systems; Fuzzy logic; Convergence; Event-triggering mechanism (ETM); exponential gain method; fixed-time stabilization; IT2 T-S fuzzy interconnected system	INTERVAL TYPE-2; ADAPTIVE-CONTROL; MULTIAGENT SYSTEMS; CONTROLLER-DESIGN; LOGIC SYSTEMS; FEEDBACK; CONSENSUS; DELAY	This paper investigates the fixed-time stabilization for IT2 T-S fuzzy interconnected systems via event-triggered mechanism. In order to reduce the amount of triggering events, an exponential gain method is proposed. The main novelty of this new method lies in the introduction of an exponential term, which makes control gains alterable in the interval of two consecutive event times. Then, by designing controllers with exponential gains, some sufficient conditions are derived which guarantee the fixed-time stabilization of the concerned system. Since sign functions are no longer contained in the newly designed controller, the chattering phenomena are also avoided. Additionally, the existence of a positive lower bound for the inter-execution is verified, which implies that the concerned system does not exhibit Zeno behaviors. Finally, several illustrative examples are provided to show the effectiveness of the main results.																	1063-6706	1941-0034				FEB	2020	28	2					246	258		10.1109/TFUZZ.2019.2904192													
J								Automated Trading Point Forecasting Based on Bicluster Mining and Fuzzy Inference	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Fuzzy logic; Indexes; Market research; Artificial neural networks; Forecasting; Support vector machines; Stock markets; Biclustering; fuzzy inference system; particle swarm optimization; technical analysis; trading point prediction; trading rules	PIECEWISE-LINEAR REPRESENTATION; TECHNICAL ANALYSIS; FEATURE-SELECTION; NEURAL-NETWORK; STOCK-PRICE; SYSTEM; CLASSIFIERS; DISCOVERY; MODELS; RULES	Historical financial data are frequently used in technical analysis to identify patterns that can be exploited to achieve trading profits. Although technical analysis using a variety of technical indicators has proven to be useful for the prediction of price trends, it is difficult to use them to formulate trading rules that could be used in an automatic trading system due to the vague nature of the rules. Moreover, it is challenging to determine a specified combination of technical indicators that can be used to detect good trading points and trading rules since different stock may be affected by different set of factors. In this paper, we propose a novel trading point forecasting framework that incorporates a bicluster mining technique to discover significant trading patterns, a method to establish the fuzzy rule base, and a fuzzy inference system optimized for trading point prediction. The proposed method (called BM-FM) was tested on several historical stock datasets and the average performance was compared with the conventional buy-and-hold strategy and five previously reported intelligent trading systems. Experimental results demonstrated the superior performance of the proposed trading system.																	1063-6706	1941-0034				FEB	2020	28	2					259	272		10.1109/TFUZZ.2019.2904920													
J								A Structural Evolving Approach for Fuzzy Systems	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Fuzzy systems; Optimization; Complexity theory; Linguistics; Indexes; Adaptive systems; Training; Evolving fuzzy systems (EFSs); error-driven method; evolving methods; greedy algorithm; incremental learning	STRUCTURE IDENTIFICATION; ONLINE IDENTIFICATION; INFERENCE SYSTEM; NEURAL-NETWORK; MODELS; LOGIC; BASE	A structural evolving approach (SEA) based on incremental partitioning learning is proposed in this paper. SEA starts with a simple fuzzy system with one fuzzy rule containing no fuzzy term for the antecedent part. After that, it keeps evolving by adding new fuzzy terms to the selected attribute in the selected partition. A new partitioning technique that locates sufficient splitting points is proposed. Furthermore, a dynamic partition-selection technique that leads to the best tradeoff between accuracy and interpretability is presented. In addition, a rule reduction mechanism that is able to locate rules with low and high impact on the system is developed. Finally, a local linear optimization is used to find the consequent parameters, which means that SEA uses only linear systems to find the antecedents and consequents parameters. Therefore, a simple and high interpretability system is offered. Six data sets are used to validate the performance of SEA with similar works. Despite the low complexity of SEA, the results show that SEA outperforms the existing methods with fewer fuzzy rules and fewer antecedent conditions too.																	1063-6706	1941-0034				FEB	2020	28	2					273	287		10.1109/TFUZZ.2019.2904928													
J								A New Nonlinear Choquet-Like Integral With Applications in Normal Distributions Based on Monotone Measures	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Gaussian distribution; Distortion measurement; Physics; Algebra; Indexes; STEM; Choquet integral; Choquet-like integral; Dow Jones Industrial Average (DJIA) Index; fuzzy measures; normal distribution		In the theory of fuzzy measures, Choquet integral is one of the most important tools. The calculation of Choquet integral on real line is difficult for many cases such as nonmonotone functions. In this paper, by the geometric interpretation of Choquet integral, we introduce a new Choquet-like integral with a different algebraic interpretation of Choquet integral on real line. The calculation of this integral is simpler than Choquet integral on real line. Based on this integral, we introduce a general class of normal distribution on monotone measures. Finally, as an application, the real dataset obtained from the daily price of Dow Jones Industrial Average Index in period of June 2, 2008 to June 2, 2018 is analyzed.																	1063-6706	1941-0034				FEB	2020	28	2					288	293		10.1109/TFUZZ.2019.2904929													
J								Membership Affinity Lasso for Fuzzy Clustering	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Clustering algorithms; Clustering methods; Optimization; Linear programming; Convex functions; Kernel; Convergence; Alternating direction method of multiplier (ADMM); fuzzy clustering; membership affinity lasso (MAL); prior knowledge	C-MEANS; GRAPH; CLASSIFICATION; REGULARIZATION; INFORMATION	Fuzzy clustering generates a membership vector for each data point in the dataset to indicate its belongingness to different clusters. This procedure can be regarded as an encoding process and the obtained vectors of memberships are the new representations of original data. Naturally, the affinities between new representations or the vectors of memberships should be consistent with the ones between original data points. For example, the data points close to each other should also take similar membership vectors. Such constraints on the affinities of memberships are valuable prior knowledge that should be imposed to the objective function of fuzzy clustering for better performance. To this end, we introduce the membership affinity lasso for fuzzy clustering in this paper. Utilizing alternating direction method of multipliers, an efficient approach is derived to optimize the general membership affinity lasso regularized fuzzy clustering model in offline manner. As illustrative examples, three new fuzzy clustering algorithms with the membership affinity lasso are proposed. Experiments on the synthetic and real data demonstrate the superiority and flexibility of the proposed algorithms.																	1063-6706	1941-0034				FEB	2020	28	2					294	307		10.1109/TFUZZ.2019.2905114													
J								Graph Model Under Unknown and Fuzzy Preferences	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Analytical models; Uncertainty; Stability criteria; Decision making; Process control; Fuzzy sets; Fuzzy preference; graph model for conflict resolution (GMCR); stability definitions; unknown preference; water diversion conflict	DECISION-SUPPORT-SYSTEM; CONFLICT-RESOLUTION; ATTITUDES	A new hybrid preference framework of the graph model for conflict resolution (GMCR) is proposed which allows decision makers (DMs) having both unknown preference and fuzzy preference to be taken into account in conflict modeling and analysis. The novel hybrid preference structure provides DMs with a more flexible technique to express preference. It is capable of covering the unknown preference of one feasible state over another, as well as fuzzy preference. Moreover, within the new hybrid preference structure, four extension forms of unknown preference are defined for different fuzzy stability definitions. These stability definitions under the new hybrid preference can be employed to thoroughly investigate complex conflicts existing in practical applications, and can offer enhanced strategic insights regarding the conflicts. A specific real-world water diversion conflict occurring in China, which includes multiple DMs and hybrid preference, is utilized to investigate how the new hybrid preference framework of the GMCR can be conveniently applied in practice.																	1063-6706	1941-0034				FEB	2020	28	2					308	320		10.1109/TFUZZ.2019.2905222													
J								Low-Computation Adaptive Fuzzy Tracking Control of Unknown Nonlinear Systems With Unmatched Disturbances	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Fuzzy logic; Fuzzy control; Disturbance observers; Approximation error; Trajectory; Backstepping; Aerospace electronics; Adaptive fuzzy control (AFC); functional uncertainties; fuzzy systems; low computation; nonlinear systems; unmatched disturbances	DYNAMIC SURFACE CONTROL; ORDER CHAOTIC SYSTEMS; NEURAL-NETWORKS; OBSERVER; DESIGN; SYNCHRONIZATION; STABILIZATION	This paper investigates the tracking control problem for a family of strict-feedback systems with unknown nonlinear functions as well as unmatched disturbances. A low-computation adaptive fuzzy control strategy combined with a constraint-handling technique is proposed to achieve accurate trajectory tracking and boundedness of the closed-loop signals. In contrast to the existing results: first, without the expense of introducing auxiliary filters, iterative calculation of virtual control signal derivatives at each step of the backstepping design that may cause the explosion of complexity issue is obviated; second, the need for disturbance observers and robust compensators to suppress disturbances and approximation errors that require a mass of online learning parameters for estimation is evaded; and third, a small number of closed-loop signals are incorporated into the input space of fuzzy logic systems for approximation. The result from a comparative simulation further illustrates the superiority of the presented approach.																	1063-6706	1941-0034				FEB	2020	28	2					321	332		10.1109/TFUZZ.2019.2905809													
J								Global Adaptive Fuzzy Distributed Tracking Control for Interconnected Nonlinear Systems With Communication Constraints	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Adaptive distributed control; communication constraints; large-scale systems; switching strategy	LARGE-SCALE SYSTEMS; STRICT-FEEDBACK SYSTEMS; DECENTRALIZED CONTROL; STABILIZATION; NETWORK; DESIGN	This paper investigates the adaptive distributed tracking control problem for a class of interconnected nonlinear systems with communication constraints where the subsystems are allowed to exchange output information only when the tracking error exceeds a given threshold. Considering the communication constraints problem, a novel adaptive distributed control method is developed. In this framework, a fuzzy adaptive controller is designed within a given compact set to compensate the unknown mismatching interconnections. Specially, to achieve the global tracking performance, a robust controller is activated outside the compact set to pull back the system states. By using the Lyapunov stability theory and the modification technique, it is proven that the tracking errors are globally asymptotically convergent and all the signals in the resulting closed-loop system are bounded. Finally, the effectiveness of the proposed method is demonstrated by two examples.																	1063-6706	1941-0034				FEB	2020	28	2					333	345		10.1109/TFUZZ.2019.2905827													
J								Tracking a Maneuvering Target by Multiple Sensors Using Extended Kalman Filter With Nested Probabilistic-Numerical Linguistic Information	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Target tracking; Linguistics; Kalman filters; Probabilistic logic; Sensor systems; Extended Kalman filter (EKF); nested probabilistic-numerical linguistic information (NPN-EKFTO); trace optimization; tracking maneuvering target	ORDER CHAOTIC SYSTEMS; GROUP DECISION-MAKING; TERM SETS; BATTERY; SYNCHRONIZATION	Tracking a maneuvering target is an important technology. Due to complex environment and diversity of sensors, errors need to be optimized with respect to various motion states during the tracking process. In this paper, we first propose how to unify the coordinate system and data preprocessing in case of tracking using multiple sensors. We then combine fuzzy sets with a novel trace optimization method based on extended Kalman filter (EKF) with nested probabilistic-numerical linguistic information (NPN-EKFTO). We present a case study of trace optimization of an unknown maneuvering target in Sichuan province in China. We solve the case by using both the proposed method and the traditional EKF and offer comparative analysis to validate the proposed approach.																	1063-6706	1941-0034				FEB	2020	28	2					346	360		10.1109/TFUZZ.2019.2906577													
J								A Novel Classification Method From the Perspective of Fuzzy Social Networks Based on Physical and Implicit Style Features of Data	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Social networking (online); Testing; Task analysis; Correlation; Support vector machines; Fuzzy sets; Media; Data classification; fuzzy influences; fuzzy social networks; implicit style features; physical features	SYSTEMS; SEARCH	Many practical scenarios have demanded that we should classify unlabeled data more accurately based on both physical features (e.g., color, distance, or similarity) and implicit style features of data. As most extant classification algorithms classify unlabeled data based only on their physical features, they become weak in achieving expected classification results for many scenarios. To work around this drawback in this paper, a novel classification method (FuCM) from the perspective of fuzzy social network based on both physical and implicit style features of data is proposed. Based on the proposed fuzzy social network and its dynamics about fuzzy influences of nodes, FuCM comprises two stages. In its training stage, after the fuzzy social network has been built, it learns the topological structure, reflecting physical features and implicit style features of data by carrying out fuzzy influence dynamics in the built network. In its prediction stage, both physical and implicit style features of data are effectively integrated to yield the double structure efficiency characterized by fuzzy influences of nodes. FuCM classifies unlabeled data according to the strongest connection measure based on the proposed double structure efficiency. FuCM does not assume that both data distribution and the classification by physical features or by both physical and implicit style features of data must be known in advance. Thus, it is a novel unified classification framework in this sense. In contrast to all the nine comparative methods, FuCM experimentally demonstrates its comparable classification performance on most synthetic, UCI and KEEL datasets, which can be well classified based only on physical features of data. Furthermore, it displays distinctive superiority on five case studies where satisfactory classification certainly depends on both physical and implicit style features.																	1063-6706	1941-0034				FEB	2020	28	2					361	375		10.1109/TFUZZ.2019.2906855													
J								Linguistic Distribution-Based Optimization Approach for Large-Scale GDM With Comparative Linguistic Information: An Application on the Selection of Wastewater Disinfection Technology	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Linguistics; Optimization; Computational modeling; Wastewater; Decision making; Sports; Biological system modeling; Comparative linguistic expressions (CLEs); consistency; fuzzy and interval fuzzy preference relations; large-scale group decision making (GDM); linguistic distribution assessments (LDAs)	GROUP DECISION-MAKING; CONSENSUS MODEL; TERM SETS; PREFERENCE RELATIONS; DISTRIBUTION ASSESSMENTS; REPRESENTATION MODEL; RISK-ASSESSMENT; CONSISTENCY; FUSION; METHODOLOGY	Managing comparative linguistic expressions (CLEs) information is a key issue in group decision-making (GDM). A transformation approach has been previously defined to convert CLEs into hesitant fuzzy linguistic terms sets (HFLTSs). However, it is noted that the occurring possibilities of the linguistic terms in the HFLTSs are assumed equal. This assumption might sometimes not capture the real opinions of the decision makers. Linguistic distribution assessments (LDAs) are an effective way to deal with this issue. This paper develops a linguistic distribution-based optimization approach for converting CLEs into LDAs, in which we assume that decision makers provide their opinions using preference relations with CLEs. Particularly, the proposed optimization approach is based on the use of a consistency-driven methodology, which seeks to minimize the inconsistency level of LDA preference relations obtained by transforming the original CLE preference relations elicited from decision makers. The linguistic distribution-based optimization approach is further developed to transform CLEs into interval LDAs to increase their flexibility. Moreover, society and technology trends make it possible to involve and manage large groups of decision makers in GDM environment. Therefore, a large-scale GDM framework with CLE information is designed based on the linguistic distribution-based optimization approach. To justify the effectiveness and applicability of the proposed methodology, it is applied to solve a real large-scale GDM problem, pertaining the selection of the best sustainable disinfection technique for wastewater reuse projects. A comparison against a baseline method is likewise provided to highlight the advantages and innovations of our proposal.																	1063-6706	1941-0034				FEB	2020	28	2					376	389		10.1109/TFUZZ.2019.2906856													
J								Event-Triggered Control for T-S Fuzzy Systems Under Asynchronous Network Communications	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Fuzzy systems; Output feedback; Stability analysis; Bandwidth; Symmetric matrices; Asymptotic stability; Dynamic output feedback; event-triggered control (ETC); fuzzy systems	H-INFINITY CONTROL; OUTPUT-FEEDBACK CONTROL; FAULT-DETECTION; STABILITY	This paper is concerned with the event-triggered dynamic output feedback control problem for Takagi-Sugeno (T-S) fuzzy systems under asynchronous network communications. Compared with the existing event-triggered output feedback results for the T-S fuzzy systems, two event-triggered mechanisms are predefined independently to check in an asynchronous manner whether the measurement output and the control input should be transmitted over networks or not. Consequently, network resources can be further saved. By introducing an auxiliary function in modeling, a delay system model is constructed. Then, a new stability criterion is presented such that the resulting closed-loop system is asymptotically stable. Furthermore, event-triggered parameters and controller gains can be codesigned if the related linear matrix inequalities are feasible. Finally, the validity of the theoretical result is illustrated by two examples.																	1063-6706	1941-0034				FEB	2020	28	2					390	399		10.1109/TFUZZ.2019.2906857													
J								Stabilization of T-S Fuzzy System With Time Delay Under Sampled-Data Control Using a New Looped-Functional	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Lyapunov-Krasovskii functional; sampled-data control; Takagi-Sugeno (T-S) fuzzy delay system; two-side looped-functional	STABILITY ANALYSIS; CHAOTIC SYSTEMS; MATRIX INEQUALITY; VARYING DELAY	This paper investigates the sampled-data stabilization problem for a Takagi-Sugeno (T-S) fuzzy system with time delay. By taking the information of states within the intervals from into account, a new two-side delay-dependent looped-functional is introduced which can not only relax the monotonic constraint of Lyapunov-Krasovskii functional (LKF), but also make better use of the actual sampling pattern. Furthermore the sampled-data fuzzy controller is designed to contain both the present and delayed state information, thereby enhancing the control performance and design flexibility. Based on the novel augmented LKF and improved bounding technique, less conservative stability criteria are derived in the form of linear matrix inequalities. The superiority of proposed results is shown by two simulation examples.																	1063-6706	1941-0034				FEB	2020	28	2					400	407		10.1109/TFUZZ.2019.2906040													
J								Selection and Optimization of Temporal Spike Encoding Methods for Spiking Neural Networks	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Signal processing; spike encoding; spiking neural networks (SNNs); stimulus estimation; temporal contrast		Spiking neural networks (SNNs) receive trains of spiking events as inputs. In order to design efficient SNN systems, real-valued signals must be optimally encoded into spike trains so that the task-relevant information is retained. This paper provides a systematic quantitative and qualitative analysis and guidelines for optimal temporal encoding. It proposes a methodology of a three-step encoding workflow: method selection by signal characteristics, parameter optimization by error metrics between original and reconstructed signals, and validation by comparison of the original signal and the encoded spike train. Four encoding methods are analyzed: one stimulus estimation [Ben's Spiker algorithm (BSA)] and three temporal contrast [threshold-based, step-forward (SW), and moving-window (MW)] encodings. A short theoretical analysis is provided, and the extended quantitative analysis is carried out applying four types of test signals: step-wise signal, smooth (sinusoid) signal with added noise, trended smooth signal, and event-like smooth signal. Various time-domain and frequency spectrum properties are explored, and a comparison is provided. BSA, the only method providing unipolar spikes, was shown to be ineffective for step-wise signals, but it can follow smoothly changing signals if filter coefficients are scaled appropriately. Producing bipolar (positive and negative) spike trains, SW encoding was most effective for all types of signals as it proved to be robust and easy to optimize. Signal-to-noise ratio (SNR) can be recommended as the error metric for parameter optimization. Currently, only a visual check is available for final validation.																	2162-237X	2162-2388				FEB	2020	31	2					358	370		10.1109/TNNLS.2019.2906158													
J								A White-Box Equivalent Neural Network Circuit Model for SoC Estimation of Electrochemical Cells	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Neural networks; Mathematical model; Estimation; Integrated circuit modeling; Computational modeling; Electronic countermeasures; Task analysis; Battery management system (BMS); electrochemical cell modeling; neural networks; state of charge (SoC) estimation; system identification; white-box modeling	CHARGE ESTIMATION; STATE; OPTIMIZATION; SYSTEM; IDENTIFICATION; MANAGEMENT	Smart grids, microgrids, and pure electric powertrains are the key technologies for achieving the expected goals concerning the restraint of CO2 emissions and global warming. In this context, an effective use of electrochemical energy storage systems (ESSs) is mandatory. In particular, accurate state of charge (SoC) estimations are helpful for improving the ESS performances. To this aim, developing accurate models of electrochemical cells is necessary for implementing effective SoC estimators. Therefore, a novel neural network modeling technique is proposed in this paper. The main contribution consists in the development of a white-box neural design that provides helpful insights into the cell physics, together with a powerful nonlinear approximation capability, and a flexible system identification procedure. In order to do that, the system equations of a white-box equivalent circuit model (ECM) have been combined with computational intelligence techniques by approximating each circuit element with a dedicated neural network. The model performances have been analyzed in terms of model accuracy, SoC estimation effectiveness, and computational cost over two realistic data sets. Moreover, the proposed model has been compared with a white-box ECM and a gray-box neural network model. The results prove that the proposed modeling technique is able to provide useful improvements in the SoC estimation task with a competing computational cost.																	2162-237X	2162-2388				FEB	2020	31	2					371	382		10.1109/TNNLS.2019.2901062													
J								Deep Decision Tree Transfer Boosting	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Decision trees; Boosting; Complexity theory; Task analysis; Training data; Training; Decision tree; deep boosting (DeepBoost); instance transfer learning; transfer boosting	MARGIN	Instance transfer approaches consider source and target data together during the training process, and borrow examples from the source domain to augment the training data, when there is limited or no label in the target domain. Among them, boosting-based transfer learning methods (e.g., TrAdaBoost) are most widely used. When dealing with more complex data, we may consider the more complex hypotheses (e.g., a decision tree with deeper layers). However, with the fixed and high complexity of the hypotheses, TrAdaBoost and its variants may face the overfitting problems. Even worse, in the transfer learning scenario, a decision tree with deep layers may overfit different distribution data in the source domain. In this paper, we propose a new instance transfer learning method, i.e., Deep Decision Tree Transfer Boosting (DTrBoost), whose weights are learned and assigned to base learners by minimizing the data-dependent learning bounds across both source and target domains in terms of the Rademacher complexities. This guarantees that we can learn decision trees with deep layers without overfitting. The theorem proof and experimental results indicate the effectiveness of our proposed method.																	2162-237X	2162-2388				FEB	2020	31	2					383	395		10.1109/TNNLS.2019.2901273													
J								H-infinity Static Output-Feedback Control Design for Discrete-Time Systems Using Reinforcement Learning	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Discrete-time (DT) systems; H-infinity static output feedback (OPFB); reinforcement learning (RL)	ADAPTIVE OPTIMAL-CONTROL; LINEAR-SYSTEMS; TRACKING CONTROL	This paper provides necessary and sufficient conditions for the existence of the static output-feedback (OPFB) solution to the H-infinity control problem for linear discrete-time systems. It is shown that the solution of the static OPFB H-infinity control is a Nash equilibrium point. Furthermore, a Q-learning algorithm is developed to find the H-infinity OPFB solution online using data measured along the system trajectories and without knowing the system matrices. This is achieved by solving a game algebraic Riccati equation online and using the measured data. A simulation example shows the effectiveness of the proposed method.																	2162-237X	2162-2388				FEB	2020	31	2					396	406		10.1109/TNNLS.2019.2901889													
J								Neural Network-Based Distributed Cooperative Learning Control for Multiagent Systems via Event-Triggered Communication	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Artificial neural networks; Eigenvalues and eigenfunctions; Multi-agent systems; Symmetric matrices; Trajectory; Adaptive neural control; distributed cooperative learning (DCL); event-triggered communication; neural network (NN)	BARRIER LYAPUNOV FUNCTIONS; NONLINEAR-SYSTEMS; ADAPTIVE-CONTROL; STATE; IDENTIFICATION; SYNCHRONIZATION; PERSISTENCY; EXCITATION	In this paper, an event-based distributed cooperative learning (DCL) law is proposed for a group of adaptive neural control systems. The plants to be controlled have identical structures, but reference signals for each plant are different. During control process, each agent intermittently broadcasts its neural network (NN) weight estimation to its neighboring agents under an event-triggered condition that is only based on its own estimated NN weights. If communication topology is connected and undirected, the NN weights of all neural control systems can converge to a small neighborhood of their optimal values. The generalization ability of NNs is guaranteed in the event-triggered context, that is, the approximation domain of each NN is the union of all system trajectories. Furthermore, a strictly positive lower bound on the interevent intervals is also guaranteed to avoid the Zeno behavior. Finally, a numerical example is given to illustrate the effectiveness of the proposed learning law.																	2162-237X	2162-2388				FEB	2020	31	2					407	419		10.1109/TNNLS.2019.2904253													
J								Joint Principal Component and Discriminant Analysis for Dimensionality Reduction	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Joint principal component and discriminant analysis (JPCDA); small sample size problem; the most discriminant information; the null space of total scatter matrix	SPACE	Linear discriminant analysis (LDA) is the most widely used supervised dimensionality reduction approach. After removing the null space of the total scatter matrix S-t via principal component analysis (PCA), the LDA algorithm can avoid the small sample size problem. Most existing supervised dimensionality reduction methods extract the principal component of data first, and then conduct LDA on it. However, "most variance" is very often the most important, but not always in PCA. Thus, this two-step strategy may not be able to obtain the most discriminant information for classification tasks. Different from traditional approaches which conduct PCA and LDA in sequence, we propose a novel method referred to as joint principal component and discriminant analysis (JPCDA) for dimensionality reduction. Using this method, we are able to not only avoid the small sample size problem but also extract discriminant information for classification tasks. An iterative optimization algorithm is proposed to solve the method. To validate the efficacy of the proposed method, we perform extensive experiments on several benchmark data sets in comparison with some state-of-the-art dimensionality reduction methods. A large number of experimental results illustrate that the proposed method has quite promising classification performance.																	2162-237X	2162-2388				FEB	2020	31	2					433	444		10.1109/TNNLS.2019.2904701													
J								Term Selection for a Class of Separable Nonlinear Models	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Least absolute shrinkage and selection operator (LASSO); separable nonlinear models; sparse solution; variable projection (VP)	VARIABLE PROJECTION METHOD; HAMMERSTEIN SYSTEMS; LEAST-SQUARES; PARAMETER-ESTIMATION; FUZZY MODELS; IDENTIFICATION; RECOVERY	In this paper, we consider the term selection problem for a class of separable nonlinear models. The strategy is a two-step process in which the nonlinear parameters of the model are first optimized by a variable projection method, and then the least absolute shrinkage and selection operator are adopted to obtain a sparse solution by picking out the critical terms automatically. This process may be repeated several times. The proposed algorithm is tested on parameter estimation problems for an exponential model and a neural network-based model. The numerical results show that the proposed algorithm can pick out the appropriate terms from the overparameterized model and the obtained parsimonious model performs better than other methods.																	2162-237X	2162-2388				FEB	2020	31	2					445	451		10.1109/TNNLS.2019.2904952													
J								Local Synchronization of Interconnected Boolean Networks With Stochastic Disturbances	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Interconnected Boolean networks (BNs); limit set; local synchronization; semitensor product; stochastic disturbances	STABILIZATION; STABILITY; CONTROLLABILITY; DESIGN	This paper is concerned with the local synchronization problem for the interconnected Boolean networks (BNs) without and with stochastic disturbances. For the case without stochastic disturbances, first, the limit set and the transient period of the interconnected BNs are discussed by resorting to the properties of the reachable set for the global initial states set. Second, in terms of logical submatrices of a certain Boolean vector, a compact algebraic expression is presented for the limit set of the given initial states set. Based on it, several necessary and sufficient conditions are derived assuring the local synchronization of the interconnected BNs. Subsequently, an efficient algorithm is developed to calculate the largest domain of attraction. As for the interconnected BNs with stochastic disturbances, first, mutually independent two-valued random logical variables are introduced to describe the stochastic disturbances. Then, the corresponding local synchronization criteria are also established, and the algorithm to calculate the largest domain of attraction is designed. Finally, numerical examples are employed to illustrate the effectiveness of the obtained results/ algorithms.																	2162-237X	2162-2388				FEB	2020	31	2					452	463		10.1109/TNNLS.2019.2904978													
J								Information Transmitted From Bioinspired Neuron-Astrocyte Network Improves Cortical Spiking Network's Pattern Recognition Performance	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Analogical reasoning; cortical neuron-astrocyte network (CNAN); cortical spiking network (CSN); information transmission; prolate spheroidal wave functions (PSWF); synaptic weights	WAVE-FUNCTIONS; SIGNAL; MODEL; CORRENTROPY	We trained two spiking neural networks (SNNs), the cortical spiking network (CSN) and the cortical neuron-astrocyte network (CNAN), using a spike-based unsupervised method, on the MNIST and alpha-digit data sets and achieve an accuracy of 96.1% and 77.35%, respectively. We then connected CNAN to CSN by preserving maximum synchronization between them thanks to the concept of prolate spheroidal wave functions (PSWF). As a result, CSN receives additional information from CNAN without retraining. The important outcome is that CSN reaches 70.57% correct classification rate on capital letters without being trained on them. The overall contribution of transfer is 87.47%. We observed that for CSN the classifying neurons that relate to digits 0-9 of the alpha-digit data set are completely supported by the ones that relate to digits 0-9 of the MNIST data set. This means that CSN recognizes the similarity between the digits of the MNIST and alpha-digit data sets and classifies each digit of both data sets in the same class.																	2162-237X	2162-2388				FEB	2020	31	2					464	474		10.1109/TNNLS.2019.2905003													
J								Flow Adversarial Networks: Flowrate Prediction for Gas-Liquid Multiphase Flows Across Different Domains	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Convolutional neural network (CNN); domain adaptation; domain adversarial network; flowrate prediction; multiphase flow; regression; transfer learning; venturi tube	WET-GAS; IDENTIFICATION; REGIME	The solution of how to accurately and timely predict the flowrate of gas-liquid mixtures is the key to help petroleum and other related industries to reduce costs, improve efficiency, and optimize management. Although numerous studies have been carried out over the past decades, the problem is still significantly challenging due to the complexity of multiphase flows. This paper attempts to seek new possibilities for multiphase flow measurement and novel application scenarios for state-of-the-art machine learning (ML) techniques. Convolutional neural networks (CNNs) are applied to predict the flowrate of multiphase flows for the first time and can achieve promising performance. In addition, considering the difference between data distributions of training and testing samples and its negative impact on prediction accuracy of the CNN models on testing samples, we propose flow adversarial networks (FANs) that can distill both domain-invariant and flowrate-discriminative features from the raw input. The method is evaluated on dynamic experimental data of different multiphase flows on different flow conditions and operating environments. The experimental results demonstrate that FANs can effectively prevent the accuracy degradation caused by the gap between training and testing samples and have better performance than state-of-the-art approaches in the flowrate prediction field.																	2162-237X	2162-2388				FEB	2020	31	2					475	487		10.1109/TNNLS.2019.2905082													
J								PCNN Mechanism and its Parameter Settings	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Firing and fire-extinguishing time of neurons; mathematically coupled fire-extinguishing characteristics; network comprehensive performance; parameter constraint conditions	COUPLED NEURAL-NETWORKS; IMAGE SEGMENTATION; FUSION ALGORITHM; FILTERING METHOD; SHORTEST-PATH; LINKING; RECOGNITION; MODEL; TRANSFORM	The pulse-coupled neural network (PCNN) model is a third-generation artificial neural network without training that uses the synchronous pulse bursts of neurons to process digital images, but the lack of in-depth theoretical research limits its extensive application. By analyzing the working mechanism of the PCNN, we present an expression for the fire-extinguishing time of neurons that fire in the second iteration and an expression for the firing time of neurons that extinguish in the second iteration. In addition, we find a phenomenon of the PCNN and name it mathematically coupled fire extinguishing. Based on the above analysis, we propose a new working mode for the PCNN, where the refiring of fire-extinguishing neurons is only allowed when all firing neurons are extinguished. We also work out the constraint conditions of the parameter settings under this mode. Furthermore, we analyze the relationship between the network parameters and mathematically coupled fire extinguishing, the coupling of neighboring neurons, and the convergence rate of the PCNN, respectively. In addition, we demonstrate the essential regularity of extinguished neuron in the PCNN and then propose an optimal parameter setting to achieve the best comprehensive performance of the PCNN.																	2162-237X	2162-2388				FEB	2020	31	2					488	501		10.1109/TNNLS.2019.2905113													
J								Hidden Bursting Firings and Bifurcation Mechanisms in Memristive Neuron Model With Threshold Electromagnetic Induction	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Bifurcation; bursting firing; chaotic dynamics; electromagnetic induction; memristor emulator; neuron model	ELECTRICAL-ACTIVITY; NETWORK; DYNAMICS; BEHAVIOR; SPIKING	Memristors can be employed to mimic biological neural synapses or to describe electromagnetic induction effects. To exhibit the threshold effect of electromagnetic induction, this paper presents a threshold flux-controlled memristor and examines its frequency-dependent pinched hysteresis loops. Using an electromagnetic induction current generated by the threshold memristor to replace the external current in 2-D Hindmarsh-Rose (HR) neuron model, a 3-D memristive HR (mHR) neuron model with global hidden oscillations is established and the corresponding numerical simulations are performed. It is found that due to no equilibrium point, the obtained mHR neuron model always operates in hidden bursting firing patterns, including coexisting hidden bursting firing patterns with bistability also. In addition, the model exhibits complex dynamics of the actual neuron electrical activities, which acts like the 3-D HR neuron model, indicating its feasibility. In particular, by constructing the fold and Hopf bifurcation sets of the fast-scale subsystem, the bifurcation mechanisms of hidden bursting firings are expounded. Finally, circuit experiments on hardware breadboards are deployed and the captured results well match with the numerical results, validating the physical mechanism of biological neuron and the reliability of electronic neuron.																	2162-237X	2162-2388				FEB	2020	31	2					502	511		10.1109/TNNLS.2019.2905137													
J								The Hierarchical Continuous Pursuit Learning Automation: A Novel Scheme for Environments With Large Numbers of Actions	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Estimator-based learning automata (LA); hierarchical LA; LA; LA with large number of actions; pursuit LA	BANDIT PROBLEMS; ALGORITHMS; SIMULATION	Although the field of learning automata (LA) has made significant progress in the past four decades, the LA-based methods to tackle problems involving environments with a large number of actions is, in reality, relatively unresolved. The extension of the traditional LA to problems within this domain cannot be easily established when the number of actions is very large. This is because the dimensionality of the action probability vector is correspondingly large, and so, most components of the vector will soon have values that are smaller than the machine accuracy permits, implying that they will never be chosen. This paper presents a solution that extends the continuous pursuit paradigm to such large-actioned problem domains. The beauty of the solution is that it is hierarchical, where all the actions offered by the environment reside as leaves of the hierarchy. Furthermore, at every level, we merely require a two-action LA that automatically resolves the problem of dealing with arbitrarily small action probabilities. In addition, since all the LA invoke the pursuit paradigm, the best action at every level trickles up toward the root. Thus, by invoking the property of the "max" operator, in which the maximum of numerous maxima is the overall maximum, the hierarchy of LA converges to the optimal action. This paper describes the scheme and formally proves its epsilon-optimal convergence. The results presented here can, rather trivially, be extended for the families of discretized and Bayesian pursuit LA too. This paper also reports extensive experimental results (including for environments having 128 and 256 actions) that demonstrate the power of the scheme and its computational advantages. As far as we know, there are no comparable pursuit-based results in the field of LA. In some cases, the hierarchical continuous pursuit automaton requires less than 18% of the number of iterations than the benchmark LR-I scheme, which is, by all metrics, phenomenal.																	2162-237X	2162-2388				FEB	2020	31	2					512	526		10.1109/TNNLS.2019.2905162													
J								Greedy Projected Gradient-Newton Method for Sparse Logistic Regression	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Convergence analysis; greedy projected gradient-Newton (GPGN) algorithm; model analysis; numerical experiment; sparse logistic regression (SLR)	VARIABLE SELECTION; OPTIMALITY CONDITIONS; GENE SELECTION; ALGORITHM; PURSUIT; REGULARIZATION; APPROXIMATION; FRAMEWORK; MODELS	Sparse logistic regression (SLR), which is widely used for classification and feature selection in many fields, such as neural networks, deep learning, and bioinformatics, is the classical logistic regression model with sparsity constraints. In this paper, we perform theoretical analysis on the existence and uniqueness of the solution to the SLR, and we propose a greedy projected gradient-Newton (GPGN) method for solving the SLR. The GPGN method is a combination of the projected gradient method and the Newton method. The following characteristics show that the GPGN method achieves not only elegant theoretical results but also a remarkable numerical performance in solving the SLR: 1) the full iterative sequence generated by the GPGN method converges to a global/local minimizer of the SLR under weaker conditions; 2) the GPGN method has the properties of afinite identification for an optimal support set and local quadratic convergence; and 3) the GPGN method achieves higher accuracy and higher speed compared with a number of state-of-the-art solvers according to numerical experiments.																	2162-237X	2162-2388				FEB	2020	31	2					527	538		10.1109/TNNLS.2019.2905261													
J								Metacognitive Octonion-Valued Neural Networks as They Relate to Time Series Analysis	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Neural networks; Prediction algorithms; Forecasting; Time series analysis; Mathematical model; Learning systems; Real-time systems; Metacognitive network; octonion-valued neural networks (OVNNs); prediction; renewable energy	FUZZY; IDENTIFICATION; PREDICTION; QUATERNIONS; ANFIS	In this paper, a metacognitive octonion-valued neural network (Mc-OVNN) learning algorithm and its application to diverse time series prediction are presented. The Mc-OVNN is comprised of two components: the octonion-valued neural network that represents the cognitive component and the metacognitive component that serves to self-regulate the learning algorithm. At each epoch, the metacognitive component decides if, how, and when learning occurs. The algorithm deletes unneeded samples and only stores those that will be used. This decision is determined by the octonion magnitude and the seven phases. To evaluate the Mc-OVNN algorithm's performance, it is applied to five real-world forecasting problems: the power consumption of a home in Honolulu, HI, USA, Box and Jenkins J series, Euro to Algerian Dinar (DZ) real-time conversion rates, the Mackey-Glass equation, and Europe Brent oil price prediction in a time series. When comparing the Mc-OVNN to other relevant techniques, Mc-OVNN displays its capability for efficient time series prediction. The real-time evaluation of the proposed algorithm is presented using the power consumption of a home in Boumerdes, Algeria, as a case study.																	2162-237X	2162-2388				FEB	2020	31	2					539	548		10.1109/TNNLS.2019.2905643													
J								Adaptive Optimal Control for a Class of Nonlinear Systems: The Online Policy Iteration Approach	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Adaptive optimal control; algebraic Riccati equation (ARE); linear differential inclusion (LDI); nonlinear systems; policy iteration (PI)	STABILITY ANALYSIS; LINEAR-SYSTEMS; CONTROL DESIGN; TIME; ALGORITHM	This paper studies the online adaptive optimal controller design for a class of nonlinear systems through a novel policy iteration (PI) algorithm. By using the technique of neural network linear differential inclusion (LDI) to linearize the nonlinear terms in each iteration, the optimal law for controller design can be solved through the relevant algebraic Riccati equation (ARE) without using the system internal parameters. Based on PI approach, the adaptive optimal control algorithm is developed with the online linearization and the two-step iteration, i.e., policy evaluation and policy improvement. The convergence of the proposed PI algorithm is also proved. Finally, two numerical examples are given to illustrate the effectiveness and applicability of the proposed method.																	2162-237X	2162-2388				FEB	2020	31	2					549	558		10.1109/TNNLS.2019.2905715													
J								Joint and Direct Optimization for Dictionary Learning in Convolutional Sparse Representation	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Dictionaries; Convolution; Convergence; Convolutional codes; Optimization; Approximation algorithms; Machine learning; Convolutional sparse representation; dictionary learning; nonconvex nonsmooth optimization	MORPHOLOGICAL COMPONENT ANALYSIS; RAIN STREAKS REMOVAL; THRESHOLDING ALGORITHM; VARIABLE SELECTION; FACE RECOGNITION; LEAST-SQUARES; SHRINKAGE	Convolutional sparse coding (CSC) is a useful tool in many image and audio applications. Maximizing the performance of CSC requires that the dictionary used to store the features of signals can be learned from real data. The so-called convolutional dictionary learning (CDL) problem is formulated within a nonconvex, nonsmooth optimization framework. Most existing CDL solvers alternately update the coefficients and dictionary in an iterative manner. However, these approaches are prone to running redundant iterations, and their convergence properties are difficult to analyze. Moreover, most of those methods approximate the original nonconvex sparse inducing function using a convex regularizer to promote computational efficiency. This approach to approximation may result in nonsparse representations and, thereby, hinder the performance of the applications. In this paper, we deal with the nonconvex, nonsmooth constraints of the original CDL directly using the modified forward-backward splitting approach, in which the coefficients and dictionary are simultaneously updated in each iteration. We also propose a novel parameter adaption scheme to increase the speed of the algorithm used to obtain a usable dictionary and in so doing prove convergence. We also show that the proposed approach is applicable to parallel processing to reduce the computing time required by the algorithm to achieve convergence. The experimental results demonstrate that our method requires less time than the existing methods to achieve the convergence point while using a smaller final functional value. We also applied the dictionaries learned using the proposed and existing methods to an application involving signal separation. The dictionary learned using the proposed approach provides performance superior to that of comparable methods.																	2162-237X	2162-2388				FEB	2020	31	2					559	573		10.1109/TNNLS.2019.2906074													
J								Marginalized Multiview Ensemble Clustering	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Multiview clustering; ensemble clustering; low-rank representation; auto-encoders	CONSENSUS; REPRESENTATION; ALGORITHM; MODELS	Multiview clustering (MVC), which aims to explore the underlying cluster structure shared by multiview data, has drawn more research efforts in recent years. To exploit the complementary information among multiple views, existing methods mainly learn a common latent subspace or develop a certain loss across different views, while ignoring the higher level information such as basic partitions (BPs) generated by the single-view clustering algorithm. In light of this, we propose a novel marginalized multiview ensemble clustering ((MVEC)-V-2) method in this paper. Specifically, we solve MVC in an EC way, which generates BPs for each view individually and seeks for a consensus one. By this means, we naturally leverage the complementary information of multiview data upon the same partition space. In order to boost the robustness of our approach, the marginalized denoising process is adopted to mimic the data corruptions and noises, which provides robust partition-level representations for each view by training a single-layer autoencoder. A low-rank and sparse decomposition is seamlessly incorporated into the denoising process to explicitly capture the consistency information and meanwhile compensate the distinctness between heterogeneous features. Spectral consensus graph partitioning is also involved by our model to make (MVEC)-V-2 as a unified optimization framework. Moreover, a multilayer (MVEC)-V-2 is eventually delivered in a stacked fashion to encapsulate nonlinearity into partition-level representations for handling complex data. Experimental results on eight real-world data sets show the efficacy of our approach compared with several state-of-the-art multiview and EC methods. We also showcase our method performs well with partial multiview data.																	2162-237X	2162-2388				FEB	2020	31	2					600	611		10.1109/TNNLS.2019.2906867													
J								Distribution-Free Probability Density Forecast Through Deep Neural Networks	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Predictive models; Wind forecasting; Forecasting; Artificial neural networks; Probabilistic logic; Training; Adaptation models; Deep learning; monotone neural network (NN); NNs; probability density forecast	OPTIMAL PREDICTION INTERVALS; WIND POWER	Probability density forecast offers the whole distributions of forecasting targets, which brings greater flexibility and practicability than the other probabilistic forecast models such as prediction interval (PI) and quantile forecast. However, existing density forecast models have introduced various constraints on forecasted distributions, which has limited their ability to approximate real distributions and may result in suboptimality. In this paper, a distribution-free density forecast model based on deep learning is proposed, in which the real cumulative density functions (CDFs) of forecasting target are approximated by a large-capacity positive-weighted deep neural network (NN). Benefiting from the universal approximation ability of NNs, the range of forecasted distributions has been proven to contain all the distributions with continuous CDFs, which is superior to existing models' considering both width and accordance with reality. Three tests from different scenarios were implemented for evaluation, i.e., very-short-term wind power, wind speed, and day-ahead electricity price forecast, in which the proposed density forecast model has shown superior performance over the state of the art.																	2162-237X	2162-2388				FEB	2020	31	2					612	625		10.1109/TNNLS.2019.2907305													
J								Fast Semisupervised Learning With Bipartite Graph for Large-Scale Data	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Bipartite graph; large-scale data; out-of-sample; semisupervised learning (SSL)	SEMI-SUPERVISED CLASSIFICATION; VECTOR MACHINES; SEARCH	As the captured information in our real word is very scare and labeling sample is time cost and expensive, semisupervised learning (SSL) has an important application in computer vision and machine learning. Among SSL approaches, a graph-based SSL (GSSL) model has recently attracted much attention for high accuracy. However, for most traditional GSSL methods, the large-scale data bring higher computational complexity, which acquires a better computing platform. In order to dispose of these issues, we propose a novel approach, bipartite GSSL normalized (BGSSL-normalized) method, in this paper. This method consists of three parts. First, the bipartite graph between the original data and the anchor points is constructed, which is parameter-insensitive, scale-invariant, naturally sparse, and simple operation. Then, the label of the original data and anchors can be inferred through the graph. Besides, we extend our algorithm to handle out-of-sample for large-scale data by the inferred label of anchors, which not only retains good classification result but also saves a large amount of time. The computational complexity of BGSSL-normalized can be reduced to O(ndm+nm(2)), which is a significant improvement compared with traditional GSSL methods that need O(n(2)d+n(3)), where n, d, and m are the number of samples, features, and anchors, respectively. The experimental results on several publicly available data sets demonstrate that our approaches can achieve better classification accuracy with less time costs.																	2162-237X	2162-2388				FEB	2020	31	2					626	638		10.1109/TNNLS.2019.2908504													
J								Passivity Analysis for Quaternion-Valued Memristor-Based Neural Networks With Time-Varying Delay	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Delays; Neurons; Biological neural networks; Memristors; Quaternions; Linear matrix inequalities; Stability criteria; Exponential passivity; nondecomposition approach; quaternion-valued memristor-based neural networks (QVMNNS); time-varying delay	EXPONENTIAL STABILITY; SYNCHRONIZATION CRITERIA; SYSTEMS; DISSIPATIVITY	This paper is concerned with the problem of global exponential passivity for quaternion-valued memristor-based neural networks (QVMNNs) with time-varying delay. The QVMNNs can be seen as a switched system due to the memristor parameters are switching according to the states of the network. This is the first time that the global exponential passivity of QVMNNs with time-varying delay is investigated. By means of a nondecomposition method and structuring novel Lyapunov functional in form of quaternion self-conjugate matrices, the delay-dependent passivity criteria are derived in the forms of quaternion-valued linear matrix inequalities (LMIs) as well as complex-valued LMIs. Furthermore, the asymptotical stability criteria can be obtained from the proposed passivity criteria. Finally, a numerical example is presented to illustrate the effectiveness of the theoretical results.																	2162-237X	2162-2388				FEB	2020	31	2					639	650		10.1109/TNNLS.2019.2908755													
J								Probabilistic Neural Network With Complex Exponential Activation Functions in Image Recognition	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Training; Kernel; Image recognition; Task analysis; Feature extraction; Complexity theory; Biological neural networks; Complex exponential activation functions; deep neural networks; image recognition; orthogonal series kernel; probabilistic neural network (PNN)	TRIGONOMETRIC SERIES; MAXIMUM-LIKELIHOOD; DENSITY; CLASSIFICATION	If the training data set in image recognition task is not very large, the feature extraction with a convolutional neural network is usually applied. Here, we focus on the nonparametric classification of extracted feature vectors using the probabilistic neural network (PNN). The latter is characterized by the high runtime and memory space complexity. We propose to overcome these drawbacks by replacing the exponential activation function in the Gaussian kernel to the complex exponential functions. Such complex nonlinearities make it possible to accurately approximate the unknown density function using the network with the number of neurons proportional to only cubic root of the database size. As a result, the proposed approach decreases the runtime and memory complexities of the PNN without losing its main advantages, namely, fast training and convergence to the Bayesian decision. In the experimental study, we describe a protocol for comparing recognition methods using the well-known visual object category data sets in the context of the small sample size problem. It has been experimentally shown that our approach rapidly obtains accurate decisions when compared to the known classifiers including the baseline PNN.																	2162-237X	2162-2388				FEB	2020	31	2					651	660		10.1109/TNNLS.2019.2908973													
J								Spatial Pyramid-Enhanced NetVLAD With Weighted Triplet Loss for Place Recognition	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Feature extraction; Global Positioning System; Image recognition; Training; Deep learning; Vocabulary; Optimization; Place recognition; spatial pyramid pooling; triplet loss (T-loss); vector of locally aggregated descriptors (VLAD)	DIMENSIONALITY REDUCTION; IMAGE	We propose an end-to-end place recognition model based on a novel deep neural network. First, we propose to exploit the spatial pyramid structure of the images to enhance the vector of locally aggregated descriptors (VLAD) such that the enhanced VLAD features can reflect the structural information of the images. To encode this feature extraction into the deep learning method, we build a spatial pyramid-enhanced VLAD (SPE-VLAD) layer. Next, we impose weight constraints on the terms of the traditional triplet loss (T-loss) function such that the weighted T-loss (WT-loss) function avoids the suboptimal convergence of the learning process. The loss function can work well under weakly supervised scenarios in that it determines the semantically positive and negative samples of each query through not only the GPS tags but also the Euclidean distance between the image representations. The SPE-VLAD layer and the WT-loss layer are integrated with the VGG-16 network or ResNet-18 network to form a novel end-to-end deep neural network that can be easily trained via the standard backpropagation method. We conduct experiments on three benchmark data sets, and the results demonstrate that the proposed model defeats the state-of-the-art deep learning approaches applied to place recognition.																	2162-237X	2162-2388				FEB	2020	31	2					661	674		10.1109/TNNLS.2019.2908982													
J								Stability Analysis for Delayed Neural Networks With an Improved General Free-Matrix-Based Integral Inequality	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Delayed neural network; integral inequality; Lyapunov-Krasovskii functional; stability analysis	TIME-VARYING DELAY; GLOBAL ASYMPTOTIC STABILITY; DEPENDENT STABILITY; DISSIPATIVITY ANALYSIS; CRITERIA; DISCRETE; SYSTEMS	This paper revisits the problem of stability analysis for neural networks with a time-varying delay. An improved general free-matrix-based (FMB) integral inequality is proposed with an undetermined number m. Compared with the conventional FMB ones, the improved inequality involves a much smaller number of free matrix variables. In particular, the improved FMB integral inequality is expressed in a concrete form for any value of m. By employing the new inequality with a properly constructed Lyapunov-Krasovskii functional, a new stability condition is derived for neural networks with a time-varying delay. Two commonly used numerical examples are given to show strong competitiveness of the proposed approach in both the conservatism and computation burdens.																	2162-237X	2162-2388				FEB	2020	31	2					675	684		10.1109/TNNLS.2019.2909350													
J								G-Softmax: Improving Intraclass Compactness and Interclass Separability of Features	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Compactness and separability; deep learning; Gaussian-based softmax; multilabel classification	DEEP; CLASSIFICATION; RECOGNITION; NETWORKS	Intraclass compactness and interclass separability are crucial indicators to measure the effectiveness of a model to produce discriminative features, where intraclass compactness indicates how close the features with the same label are to each other and interclass separability indicates how far away the features with different labels are. In this paper, we investigate intraclass compactness and interclass separability of features learned by convolutional networks and propose a Gaussian-based softmax ( G-softmax) function that can effectively improve intraclass compactness and interclass separability. The proposed function is simple to implement and can easily replace the softmax function. We evaluate the proposed G-softmax function on classification data sets (i.e., CIFAR-10, CIFAR-100, and Tiny ImageNet) and on multilabel classification data sets (i.e., MS COCO and NUS-WIDE). The experimental results show that the proposed G-softmax function improves the state-of-the-art models across all evaluated data sets. In addition, the analysis of the intraclass compactness and interclass separability demonstrates the advantages of the proposed function over the softmax function, which is consistent with the performance improvement. More importantly, we observe that high intraclass compactness and interclass separability are linearly correlated with average precision on MS COCO and NUS-WIDE. This implies that the improvement of intraclass compactness and interclass separability would lead to the improvement of average precision.																	2162-237X	2162-2388				FEB	2020	31	2					685	699		10.1109/TNNLS.2019.2909737													
J								Stabilization of Second-Order Memristive Neural Networks With Mixed Time Delays via Nonreduced Order	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Delays; Artificial neural networks; Delay effects; Stability criteria; Asymptotic stability; Synchronization; Adaptive control; memristive neural networks (MNNs); mixed time delays; stabilization	VARYING DELAYS; EXPONENTIAL STABILIZATION; SYNCHRONIZATION ANALYSIS; STABILITY; DYNAMICS	In this brief, we investigate a class of second-order memristive neural networks (SMNNs) with mixed time-varying delays. Based on nonsmooth analysis, the Lyapunov stability theory, and adaptive control theory, several new results ensuring global stabilization of the SMNNs are obtained. In addition, compared with the reduced-order method used in the existing research studies, we consider the global stabilization directly from the SMNNs themselves without the reduced-order method. Finally, we give some numerical simulations to show the effectiveness of the results.																	2162-237X	2162-2388				FEB	2020	31	2					700	706		10.1109/TNNLS.2019.2910125													
J								Automated diabetic retinopathy grading and lesion detection based on the modified R-FCN object-detection algorithm	IET COMPUTER VISION										feature extraction; biomedical optical imaging; image segmentation; medical image processing; object detection; eye; diseases; image classification; blood vessels; feature pyramid network; modified region proposal network; DR-grading model; Messidor data; Shanghai Eye Hospital; hospital data; modified R-FCN lesion-detection model; lesion detection; modified R-FCN object-detection algorithm; computer-aided retinal image screening system; retinal fundus images; modified object-detection method; region-based fully convolutional network	NETWORKS	In this work, we develop a computer-aided retinal image screening system that can perform automated diabetic retinopathy (DR) grading and DR lesion detection in retinal fundus images. We propose a modified object-detection method for this task via a region-based fully convolutional network (R-FCN). A feature pyramid network and a modified region proposal network are applied to enhance the detection of small objects. The DR-grading model based on the modified R-FCN is evaluated on the Messidor data set and images provided by the Shanghai Eye Hospital. High sensitivity of 99.39% and specificity of 99.93% are obtained on the hospital data. Moreover, high sensitivity of 92.59% and specificity of 96.20% are obtained on the Messidor data set. The modified R-FCN lesion-detection model is validated on the hospital data set and achieves a 92.15% mean average precision. The proposed R-FCN can efficiently accomplish DR grading and lesion detection with high accuracy.																	1751-9632	1751-9640				FEB	2020	14	1					1	8		10.1049/iet-cvi.2018.5508													
J								Automatic drug pills detection based on enhanced feature pyramid network and convolution neural networks	IET COMPUTER VISION										neural nets; image classification; medical computing; feature extraction; drugs; learning (artificial intelligence); automatic drug pills detection; enhanced feature pyramid network; convolution neural networks; Drug pill detection; medication safety; fixed viewing angle; multiple drugs; randomly placed drugs; different drugs; convolution neural network-based detector; drug identification; drug localisation; drug classification; Drug Pills Image Database; drug datasets	SYSTEM	Drug pill detection is one of the most important tasks in medication safety. The correct identification of drug based on the visual appearance is a key step towards the improvement of medication safety. Previous studies have aimed to recognise a drug based on the front or back view of the drug under a fixed viewing angle. In cases with multiple drugs and randomly placed drugs, the previous methods have difficulties in detecting and recognising different drugs in practical applications. A convolution neural network-based detector is proposed in this work to overcome the difficulties and to assist patients in drug identification. The proposed system includes a localisation stage and a classification stage. The enhanced feature pyramid network (EFPN), is proposed for drug localisation, and Inception-ResNet v2 is used in drug classification. The proposed Drug Pills Image Database contains a collection of 612 categories of drug datasets for deep learning research in the pharmaceutical field. The proposed EFPN achieves over 96% accuracy in the localisation experiment. In the complete system evaluation, the proposed system has obtained the Top-1, Top-3, and Top-5 accuracies of 82.1, 92.4, and 94.7%, respectively.																	1751-9632	1751-9640				FEB	2020	14	1					9	17		10.1049/iet-cvi.2019.0171													
J								Ensemble of fine-tuned convolutional neural networks for urine sediment microscopic image classification	IET COMPUTER VISION										image classification; medical image processing; feature extraction; learning (artificial intelligence); convolutional neural nets; urine sediment microscopic image processing; pre-trained CNNs; urine sediment microscopic image dataset; learning rate; cascading features; convolutional layers; fully connected neural network; fine-tuned convolutional neural networks; urine sediment microscopic image classification; CNN training; impurity interference; classification accuracy	DIAGNOSIS	In this study, an ensemble of fine-tuned convolutional neural networks (CNNs) is proposed. As CNN training requires large annotated data, which are lacking in the field of urine sediment microscopic image processing, the authors first pre-trained the CNNs, including ResNet50 and GoogLeNet, and developed AlexNet on an ImageNet dataset. Thereafter, some of the weights of the pre-trained CNNs were transferred to the urine sediment microscopic image dataset. To guide fine-tuning of the learning rate and cascading features, the hierarchical nature of features in different convolutional layers was investigated by visualising the CNN. Then, they combined three CNNs as an ensemble of CNNs to decrease the differences and impurity interference among features of urine sediment microscopic image. These fusion features were employed to train the fully connected neural network for classification. In this study, they improved the accuracy of each CNN by an average of 2.2% through fine-tuning of the learning rate and cascading features. Moreover, the better experimental results were achieved compared with other state-of-the-art methods and indicated that a 97% classification accuracy can be attained.																	1751-9632	1751-9640				FEB	2020	14	1					18	25		10.1049/iet-cvi.2018.5829													
J								Incremental transfer learning for video annotation via grouped heterogeneous sources	IET COMPUTER VISION										video signal processing; Internet; learning (artificial intelligence); source domain data; entire target domain videos; source group; group weights; incremental learning process; negative transfer; large-scale consumer video datasets; video annotation; grouped heterogeneous sources; heterogeneous knowledge; internet datasets; tedious labelling efforts; expensive labelling efforts; incremental transfer learning framework; heterogeneous source knowledge; annotation model; transfer learning process; web images; existing action videos; labelled static motion information		Here, the authors focus on incrementally acquiring heterogeneous knowledge from both internet and publicly available datasets to reduce the tedious and expensive labelling efforts required in video annotation. An incremental transfer learning framework is presented to integrate heterogeneous source knowledge and update the annotation model incrementally during the transfer learning process. Under this framework, web images and existing action videos form the source domain to provide labelled static and motion information of the target domain videos, respectively. Moreover, according to the semantic of the source domain data, all the source domain data are partitioned into several groups. Different from traditional methods, which compare the entire target domain videos with each source group from the source domain, the authors treat the group weights as sample-specific variables and optimise them along with new adding data. Two regularisers are used to prevent the incremental learning process from negative transfer. Experimental results on the two large-scale consumer video datasets (i.e. multimedia event detection (MED) and Columbia consumer video (CCV)) show the effectiveness of the proposed method.																	1751-9632	1751-9640				FEB	2020	14	1					26	35		10.1049/iet-cvi.2018.5730													
J								Multiscale bilateral filtering to detect 3D interest points	IET COMPUTER VISION										computer vision; mesh generation; feature extraction; solid modelling; filtering theory; image matching; multiscale bilateral filtering; repeatedly the input 3D; k multiresolution meshes; final interest points	MESH; FEATURES	The detection of 3D interest points is a central problem in computer graphics, computer vision, and pattern recognition. It is also an important preprocessing step in the analysis of 3D model matching. Although studied for decades, detecting 3D interest points remains a challenge. In this study, a novel multiscale bilateral filtering method is presented to detect 3D interest points. This method first simplifies repeatedly the input 3D mesh to form k multiresolution meshes. For each mesh, on the basis of the computed saliency of the mesh vertex, the bilateral filtering is used to remove the noise of the mesh saliencies and the global contrast to normalise the saliencies, and then the interest points are extracted on the basis of the normalised saliency. The proposed method then gathers and clusters all interest points detected on the k multiresolution meshes, and the centres of these clusters are treated as the final interest points. In this method, both the spatial closeness and the geometric similarities of the mesh vertices are considered during the bilateral filtering process. The experimental results validate the effectiveness of the proposed method to detect 3D interest points. This method is also tested the potential to distinguish 3D models.																	1751-9632	1751-9640				FEB	2020	14	1					36	47		10.1049/iet-cvi.2018.5405													
J								Kinship Verification Through Facial Images Using CNN-Based Features	TRAITEMENT DU SIGNAL										kinship verification; deep learning; VGG-Face; fisher score; SVM		The use of facial images in the kinship verification is a challenging research problem in soft biometrics and computer vision. In our work, we present a kinship verification system that starts with pair of facial images of the child and parent, then as a final result is determine whether two persons have a kin relation or not. our approach contains five steps as follows: (i) the face preprocessing step to get aligned and cropped facial images of the pair (ii), extracting deep features based on the deep learning model called Visual Geometry Group (VGG) Face, (iii) applying our proposed pair feature representation function alongside with a features normalization, (iv) the use of Fisher Score (FS) to select the best discriminative features, (v) decide whether there is a kinship or not based on the Support Vector Machine (SVM) classifier. We conducted several experiments to demonstrate the effectiveness of our approach that we tested on five benchmark databases (Cornell KinFace, UB KinFace, Familly101, KinFace W-I, and KinFace W-II). Our results indicate that our system is robust compared to other existing approaches.																	0765-0019	1958-5608				FEB	2020	37	1					1	8		10.18280/ts.370101													
J								Spectral and Statistical Analysis for Damage Detection in Ceramic Materials	TRAITEMENT DU SIGNAL										ceramic materials; crack analysis; impulse noise method; Wigner Ville distribution; bispectrum; trispectrum; mean value; Peak to RMS	DEFECT DETECTION	In this study, the internal or superficial cracks that may occur during the production of ceramic plates were determined using the impact noise method. In the industry, ceramic materials are frequently used in areas such as kitchenware and construction. Many different methods are used in the quality control processes of ceramic materials. In this study, the sound produced by the impact applied to the ceramic material was analyzed. As a result of the analysis, the material was found to be undamaged or damaged. This method is called the Impulse Noise method. In this study, damaged and undamaged ceramic plates with different cracks were selected and impact plates were applied to the plates by impact pendulum. The ceramic plates used in the application have the same characteristics and dimensions produced by the same company of the same type. The noise generated as a result of the impact applied to a determined point on the tested materials was examined by Wigner-Ville method which is one of the time-frequency analysis methods. In addition, bispectrum and trispectrum analyzes, which are high-order spectral analysis (HOSA) methods, were used. Statistically, signal analysis with running minimum and maximum, mean value and peak to RMS examinations were also added. The applied methods give good results in differentiating between undamaged and damaged ceramic plates.																	0765-0019	1958-5608				FEB	2020	37	1					9	16		10.18280/ts.370102													
J								Automated Analysis of Leaf Shape, Texture, and Color Features for Plant Classification	TRAITEMENT DU SIGNAL										plants; GIST; best-guide binary particle swarm optimization; geometrics; machine learning		The main purpose of this research is to apply image processing for plant identification in agriculture. This application field has so far received less attention rather than the other image processing applications domains. This is called the plant identification system. In the plant identification system, the conventional technique is dealt with looking at the leaves and fruits of the plants. However, it does not take into account as a cost effective approach because of its time consumption. The image processing technique can lead to identify the specimens more quickly and classify them through a visual machine method. This paper proposes a methodology for identifying the plant leaf images through several items including GIST and Local Binary Pattern (LBP) features, three kinds of geometric features, as well as color moments, vein features, and texture features based on lacunarity. After completion of the processing phase, the features are normalized, and then Pbest-guide binary particle swarm optimization (PBPSO) is developed as a novel method for reduction of the features. In the next phase, these features are employed for classification of the plant species. Different machine learning classifiers are evaluated including k-nearest neighbor, decision tree, naive Bayes, and multi-SVM. We tested our proposed technique on Flavia and Folio leaf datasets. The final results demonstrated that the decision tree has the best performance. The results of the experiments reveal that the proposed algorithm shows the accuracy of 98.58% and 90.02% for the "Flavia" and "Folio" datasets, respectively.																	0765-0019	1958-5608				FEB	2020	37	1					17	28		10.18280/ts.370103													
J								A Salient Object Detection Algorithm Based on Hierarchical Cognitive Mechanism	TRAITEMENT DU SIGNAL										cognitive mechanism; salient object detection; RGB-D image; saliency map	ENHANCEMENT; RECOGNITION; DARK	The key of salient object detection is to extract the most attractive area in the scene. This paper fully explores the hierarchical cognitive mechanism of visual information, combines color contrast and depth contrast, and puts forward a salient object detection algorithm for the RGB-D image. Three saliency maps were prepared, namely, initial saliency map, middle saliency map and advanced saliency map. The three maps were then fused into a final saliency map. The proposed method was compared with six popular salient object detection methods on three RGB-D image datasets. The comparison shows that our algorithm achieved the best results in accuracy, recall rate and F-value. The research findings shed important new light on salient object detection in RGB-D images.																	0765-0019	1958-5608				FEB	2020	37	1					29	35		10.18280/ts.370104													
J								Human Action Recognition in Video Sequences Using Deep Belief Networks	TRAITEMENT DU SIGNAL										human action recognition; deep belief network; restricted Boltzmann machine; deep learning		For the last several decades, Human Activity Recognition (HAR) has been an intriguing topic in the domain of artificial intelligence research, since it has applications in many areas, such as image and signal processing. Generally, every recognition system can be either an end-to-end system or including two phases: feature extraction and classification. In order to create an optimal HAR system that offers a better quality of classification prediction, in this paper we propose a new approach within two-phase recognition system paradigm. Probabilistic generative models, known as Deep Belief Networks (DBNs), are introduced. These DBNs comprise a series of Restricted Boltzmann Machines (RBMs) and are responsible for data reconstruction, feature construction and classification. We tested our approach on the KTH and UIUC human action datasets. The results obtained are very promising, with the recognition accuracy outperforming the recent state-of-the-art.																	0765-0019	1958-5608				FEB	2020	37	1					37	44		10.18280/ts.370105													
J								An Image Recognition Algorithm for Defect Detection of Underground Pipelines Based on Convolutional Neural Network	TRAITEMENT DU SIGNAL										image recognition; convolution neural network (CNN); cost function; recursive neural network (RNN); underground pipelines	CLASSIFICATION; SYSTEM	The proliferation of underground pipelines is a defining feature of urbanization. Regular inspection and maintenance are necessary to reduce the economic loss caused by pipeline defects. This paper aims to detect pipeline defects in video images with the aid of computer vision. Firstly, the recursive neural network (RNN) was added to the classic convolutional neural network (CNN) to acquire various features from the images. Then, the Fisher criterion was weighted and improved, and introduced to the least square error cost function, enhancing the recognition rate of the improved CNN. Finally, the improved CNN algorithm was verified through contrastive experiments on actual underground pipeline images. The research results shed new light on the defect detection and maintenance of underground pipelines in urban areas.																	0765-0019	1958-5608				FEB	2020	37	1					45	50		10.18280/ts.370106													
J								Comparison of the Effects of Mel Coefficients and Spectrogram Images via Deep Learning in Emotion Classification	TRAITEMENT DU SIGNAL										speech emotion recognition; Deep Neural Network (DNN); Convolutional Neural Network (CNN); deep learning algorithm; Mel-Frequency Cepstrum Coefficients (MFCC)	NEURAL-NETWORK; SPEECH; RECOGNITION; ARCHITECTURES	In the present paper, an approach was developed for emotion recognition from speech data using deep learning algorithms, a problem that has gained importance in recent years. Feature extraction manually and feature selection steps were more important in traditional methods for speech emotion recognition. In spite of this, deep learning algorithms were applied to data without any data reduction. The study implemented the triple emotion groups of EmoDB emotion data: Boredom, Neutral, and Sadness-BNS; and Anger, Happiness, and Fear-AHF. Firstly, the spectrogram images resulting from the signal data after preprocessing were classified using AlexNET. Secondly, the results formed from the MelFrequency Cepstrum Coefficients (MFCC) extracted by feature extraction methods to Deep Neural Networks (DNN) were compared. The importance and necessity of using manual feature extraction in deep learning was investigated, which remains a very important part of emotion recognition. The experimental results show that emotion recognition through the implementation of the AlexNet architecture to the spectrogram images was more discriminative than that through the implementation of DNN to manually extracted features.																	0765-0019	1958-5608				FEB	2020	37	1					51	57		10.18280/ts.370107													
J								A Novel Geometrical Method for Discrimination of Normal, Interictal and Ictal EEG Signals	TRAITEMENT DU SIGNAL										ictal EEG signal; geometrical features; computer-aided diagnosis; SVM; KNN	EPILEPTIC SEIZURES; WAVELET TRANSFORM; CLASSIFICATION; ENTROPY	The electroencephalogram (EEG) signal is known as a nonlinear and complex signal. The EEG signal has very important information about brain activities and disorders which can detect by an accurate Computer-aided diagnosis system. The performance of the Computer-aided diagnosis system directly depends on using features in the classifiers. In this paper, we proposed nonlinear geometrical features for the classification of EEG signals. The normal, interictal and ictal EEG signals of the Bonn university EEG database are plotted in 2D space by a novel approach and considering their patterns, six features namely: area of the octagon (AOO), circle area (CA), the summation of vectors length (SVL), centroid to centroid (CTC) and triangle area (TA) are extracted on different aspects of distance in Cartesian space. Based on the Kruskal-Wallis statistical test, all of the features were found statistically significant in the discrimination of normal vs. ictal and interictal vs. ictal EEG signals (p-value approximate to 0). Also, the edges of 2D projection EEG signals in the ictal group were sharper than normal and interictal groups. Besides, 2D projection of normal and interictal EEG signals has more regular geometrical shapes than the ictal group. Our proposed features were applied as input on support vector machine (SVM) and k-nearest neighbors (KNN) classifiers which resulted in more than 99% classification accuracy in a ten-fold cross-validation strategy.																	0765-0019	1958-5608				FEB	2020	37	1					59	68		10.18280/ts.370108													
J								An Adaptive Filtering Algorithm for Non-Gaussian Signals in Alpha-Stable Distribution	TRAITEMENT DU SIGNAL										Alpha (alpha)-stable distribution; non-Gaussian distribution; fractional lower-order statistics (FLOS); adaptive filtering algorithm; least mean square (LMS); subspace minimum norm (SMN) algorithm	LOWER ORDER MOMENTS; STOCHASTIC-ANALYSIS; NOISE; LMS	Currently, many adaptive filtering algorithms are available for the non-Gaussian environment, namely, least mean square (LMS) algorithm, recursive least square (RLS) algorithm, least mean fourth (LMF) algorithm, and subspace minimum norm (SMN) algorithm. Most of them can converge to the steady-state, but face various constraints in the presence of alpha (alpha)-stable noises. To solve the problem, this paper aims to develop an adaptive filtering algorithm for non-Gaussian signals in alpha-stable distribution, drawing on the merits of existing adaptive filtering algorithms. Firstly, the authors introduced the theory of alpha-stable distribution, the central limit theorem and fractional lower-order statistics (FLOS). Next, two classic adaptive filtering algorithms, RLS and LMS, were summarized, and compared through tests. On this basis, the FLOS-SMN algorithm was designed in the light of the features of the LMS and the SMN, which applies to the filtering of non-Gaussian signals in a- stable distribution. Finally, the proposed algorithm was proved as faster, more stable and more adaptable than the traditional method.																	0765-0019	1958-5608				FEB	2020	37	1					69	75		10.18280/ts.370109													
J								An Efficient Antilogarithmic Converter by Using Correction Scheme for DSP Processor	TRAITEMENT DU SIGNAL										antilogarithmic converter; computer arithmetic; DSP processor; error analysis; FIR filter; logarithmic converter; logarithmic multiplication	LOGARITHMIC MULTIPLICATION; ELEMENTARY-FUNCTIONS; VLSI IMPLEMENTATION; POWER; APPROXIMATE; DESIGN; ERROR	Digital Signal Processing (DSP) applications demand error-free and compact hardware architecture of arithmetic operations. A logarithmic operation provides an efficient option in place of binary arithmetic. In this paper, it is suggested that 11-region and 17-region error correction schemes for developing an efficient antilogarithm converter. It is used for developing the most accurate and compact logarithm multiplier which is used in the DSP processor. Implementations of reported and proposed designs are investigate based on accuracy and hardware overhead and it found outperform in comparisons of previously reported designs. The proposed 11-region converter involves 61% less Area Delay Product (ADP) and 49.82% less energy in comparisons of the reported 11-region antilogarithmic converter and 17-region converter involves 48.02% less ADP and 32.53% less energy in comparisons of the reported 14-region antilogarithmic converter. The proposed antilogarithmic converter achieves 1.697% and 1.084% error for 11-region and 17-region designs respectively than of reported designs of 1.876% and 1.351% for 11-region and 17-region respectively.																	0765-0019	1958-5608				FEB	2020	37	1					77	83		10.18280/ts.370110													
J								A Deep Learning Model for Striae Identification in End Images of Float Glass	TRAITEMENT DU SIGNAL										striae identification; end image; float glass; deep learning (DL); liquid layers; U-Net		For float glass, there is a correlation between the striae in end image and the manufacturing process. If clearly understood, the correlation helps to optimize and fine-tune the manufacturing process of float glass. This paper attempts to extract the striae from the end image of float glass with deep learning (DL) neural network (NN). For this purpose, an image segmentation model was established based on improved U-Net, a fully convolutional network (FCN), and used to accurately divide the glass liquid on the end image into different layers. Firstly, the improved U-Net model was constructed to extract the striae from each liquid layer on the end image. Next, the activation function and convolutional mode of the improved U-Net model were optimized to enhance the segmentation accuracy and shorten the training/prediction time. Finally, the proposed model was tested on the float glass production line of Hebei CSG Glass Co., Ltd. The test results show that our model achieved an accuracy of 94%. The research findings lay a solid basis for striae identification on end image of float glass, and provide guidance for optimization and fine-tuning of float glass manufacturing process.																	0765-0019	1958-5608				FEB	2020	37	1					85	93		10.18280/ts.370111													
J								Cryptanalysis of a Pixel Permutation Based Image Encryption Technique Using Chaotic Map	TRAITEMENT DU SIGNAL										chaos; chosen-plaintext attack; brute-force attack; image encryption	SYSTEM	Recently, a new image encryption algorithm was proposed by Anwar and Meghana. This encryption scheme uses the Arnold's chaotic cat map to permute the image pixels. The resulting image is then confused with both a secret image provided as part of the secret key and a secret value selected randomly from the permuted image. Using a random image as part of the secret key, gives this algorithm an infinite key space which increases its efficiency against brute-force attacks. In order to help improving the security of modern image encryption schemes, this paper presents a cryptanalysis of the proposed algorithm using a combination of chosen-plaintext and brute-force attacks. First, the infinite key space that the secret image offers is broken using a chosen-plaintext attack. Then, the permutation phase is reversed through a series of chosen-plaintext attacks too. Finally, the secret value chosen randomly from the permuted image is easily brute-forced due to its reduced number of possible values. By applying the above method, it is possible to restore the plain version of any image that was encrypted using the former encryption algorithm. Thus, relying on this algorithm to encrypt real sensitive data is not secure.																	0765-0019	1958-5608				FEB	2020	37	1					95	100		10.18280/ts.370112													
J								Query Rewriting and Semantic Annotation in Semantic-Based Image Retrieval under Heterogeneous Ontologies of Big Data	TRAITEMENT DU SIGNAL										semantic web; ontology mapping; query rewriting; big data; semantic annotation	HARMONY SEARCH ALGORITHM; SHOP SCHEDULING PROBLEM; BEE COLONY ALGORITHM	In the era of big data, it is of great significance to retrieve the semantic features from images by big data technique. However, most semantic query models perform poorly in actual images, which are distributed heterogeneously. Image ontology mapping provides a solution to the problem. This paper applies the H-Match algorithm to find the mapping relationship between image ontologies in peer-to-peer (P2P) environment, and rewrite user queries for heterogeneous image ontologies. The H-Match algorithm was developed under the framework called Helios evolving interaction-based ontology knowledge sharing (Helios). The weights of semantic annotation were calculated by a novel method, involving word frequency, position and feedback. The research results have great application potentials in various fields.																	0765-0019	1958-5608				FEB	2020	37	1					101	105		10.18280/ts.370113													
J								Performance Optimization of LS/LMMSE Using Swarm Intelligence in 3D MIMO-OFDM Systems	TRAITEMENT DU SIGNAL										bit error rate; 3D-PACE; multi input multi output; orthogonal frequency division multiplexing; particle swarm optimization	CHANNEL ESTIMATION; PSO	The capability of Least square (LS) and least minimum mean squared error (LMMSE) channel estimation techniques are limited due to one or two factors (inherent additive Gaussian noise and Inter Carrier Interference, higher computational complexity). These factors tend to be severe when the system grows in terms of numbers of transmitting and receiving antennas, channel parameters, noise etc. Accurate channel parameters estimation using these techniques is still not possible even with smaller Multi input multi output (MIMO) systems at higher signal to noise ratios (SNR) due to complex nature of channel parameters. Swarm Intelligence consisting of agents spread in search space having limited capabilities and random behaviour when interacts with each other and within their own locality are capable of finding solution for a complex problem. When the constructive behaviour of such particles in particle swarm optimization (PSO) within the search space limited to some constraint is applied to optimize the performance of 3D-Pilot Aided Channel Estimation (3D-PACE) of MIMO-OFDM system, results showed that the bit error rate (BER) is significantly decreased. The channel parameters at the receiver obtained using LS and LMMSE are further optimized using PSO with proper and careful setting of PSO initial parameters.																	0765-0019	1958-5608				FEB	2020	37	1					107	112		10.18280/ts.370114													
J								A Malicious Webpage Detection Algorithm Based on Image Semantics	TRAITEMENT DU SIGNAL										deep learning; malicious attack; image semantics; backpropagation neural network (BPNN)		In the era of the Internet, malicious attacks have put user information at risk. Many malicious webpages use images as the carrier of malicious codes. If extracted accurately, the features of these images will help to improve the detection of malicious webpages. This paper aims to develop an accurate malicious webpage detection method based on the features of the said images. Since static images contain a small amount of information, semantic segmentation was performed to predict the semantics of the target attitude. Then, the final semantics of target the image were derived by the backpropagation neural network (BPNN). After that, the image semantics were fused with the other features of the malicious webpage, and sent to the classifier for recognition. Finally, the proposed algorithm was tested on an actual dataset, in comparison with other malicious webpage detection methods. The results show that our algorithm can accurately detect malicious webpages, thanks to the introduction of image semantic features.																	0765-0019	1958-5608				FEB	2020	37	1					113	118		10.18280/ts.370115													
J								Implementation of Effective Hybrid Window Function for ECG Signal Denoising	TRAITEMENT DU SIGNAL										additive white gaussian noise; electrocardiogram denoising; finite impulse response low pass filter; window functions		The primary objective of this research paper, is to introduce an effective hybrid window function for low pass finite impulse response (FIR) filter design which is useful for denoising the electrocardiogram (ECG) signals corrupted by additive white gaussian noise (AWGN) even at low signal to noise ratio (SNR) condition. The noise may be introduced during ambulatory patient monitoring in wireless ECG recording environment. For proper diagnosis, it is very essential to receive noiseless signal even at very low SNR. To reach this objective, a hybrid window function is proposed and a linear phase FIR low pass filter is designed by using the proposed windowing technique. The proposed hybrid window is a product of Blackman and flattop window functions with modified window coefficients. Stopband attenuation of the filter constructed using proposed hybrid window is very high with respect to other traditional window functions and different hybrid window functions created by different combinations of some well-known traditional windows. Filter designed with the proposed hybrid window function have comparable transition bandwidth with respect to other hybrid window functions. ECG denoising performance of the proposed filter is better with respect to others in low SNR environment.																	0765-0019	1958-5608				FEB	2020	37	1					119	128		10.18280/ts.370116													
J								Novel Image Steganography Based on Preprocessing of Secrete Messages to Attain Enhanced Data Security and Improved Payload Capacity	TRAITEMENT DU SIGNAL										data security; hidden communication; Steganography	WATERMARKING	Image steganography hides secret messages in cover images by manipulating their content. There is a fundamental requirement of maximizing secrecy and payload capacity. This paper presents a novel algorithm for achieving two layer security as well improved payload capacity by prepossessing the secret images before hiding it in a cover image. The secret image is divided in to small windows of fixed sizes and is searched from a database of 255 images. Each window of secret image is replaced by the address of its closet match in database. Instead of hiding a secret image this algorithm hides the address of database. Consequently makes it impossible for any third party to retrieve the secret image without having access to the database. The size of the database address is less than the size of secret message thus it improves the payload capacity of the proposed algorithm.																	0765-0019	1958-5608				FEB	2020	37	1					129	136		10.18280/ts.370117													
J								An Evaluation Algorithm for the Interoperability of Global Navigation Satellite Systems	TRAITEMENT DU SIGNAL										global navigation satellite systems (GNSSs); Compass/BeiDou Navigation Satellite System (Compass); interoperability; evaluation; service performance		The interoperability of global navigation satellite systems (GNSSs) has a significant impact on the service performance of GNSSs. To evaluate the GNSS interoperability, this paper creates an evaluation algorithm with such modules as space section, user section and environment section. The proposed algorithm evaluates the service performance with several common parameters, namely, the dilution of precision (DOP), navigation satellite system precision (NSSP), navigation satellite system integrity (NSSI), availability and continuity. The parameters of the algorithm could be configured based on the existing GNSS data or our self-developed data. Then, the proposed algorithm was applied to evaluate the service performance of three difference scenarios: Compass-only, Compass + GPS and Compass + GPS + Galileo. The results show that the combination of different GNSSs can greatly improve the service performance of GNSSs in both time and space.																	0765-0019	1958-5608				FEB	2020	37	1					137	144		10.18280/ts.370118													
J								Kernel Affine Projection P-norm (KAPP) Filtering under Alpha Stable Distribution Noise Environment	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Kernel filtering; affine projection; alpha stable distribution; Mackey-Glass chaotic time series	LEAST-MEAN-SQUARE; LMS ALGORITHM	Aiming at improving the performance of the nonlinear adaptive filtering under the alpha-stable distribution noise environment, Kernel Affine Projection P-norm (KAPP) algorithm based on the minimum dispersion coefficient criterion and the affine projection is deduced. The accuracy of the gradient estimation is enhanced by using the input signals and the error signals at multiple times. The simulation results on Mackey-Glass chaotic time series prediction show that the KAPP algorithm has faster convergence speed, better steady-state performance and stronger robustness under the Gaussian noise and stable distributed noise environment.																	0218-0014	1793-6381				FEB	2020	34	2							2054006	10.1142/S0218001420540063													
J								Application of Cloud Video Information Processing Technology in Alleviating the Food Safety Trust Crisis	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Cloud video; map reduce frame; food safety; particle filtering algorithm	ALGORITHM; FILTER; MAPREDUCE	In view of the current consumer trust crisis in food safety, some researchers proposed to build a reliable food safety traceability system solution. Because the food safety traceability system requires storing and processing of massive video data, this paper proposes to build a reliable food safety traceability system by introducing cloud storage technology into video surveillance system. Hadoop platform is built to store and process massive monitoring of video data. In addition, in order to make use of Map Reduce framework for parallel computing, this paper optimizes the traditional particle filter target tracking algorithm, and proposes using the parallel computing framework Map Reduce in Hadoop to achieve the parallel computing of all particles in particle filter. Finally, the simulation results show that when the number of videos is large, the time of parallel processing is obviously shorter than that of single-machine mode. And as the video data becomes larger and larger, the advantages of parallel processing become more and more obvious.																	0218-0014	1793-6381				FEB	2020	34	2							2055007	10.1142/S0218001420550071													
J								An Improved Group Similarity-Based Association Rule Mining Algorithm in Complex Scenes	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Association rule mining; speeding up; business big data; apache spark		Association rule (AR) mining in complex scene has attracted extensive attention of researchers in recent years. Typically, many researchers focused on an algorithm itself and ignored a generalization method to improve the performance of AR mining. Tuna et al., presented a general data structure Speeding-Up AR Structure with Inverted Index Compression (SAII) which could be utilized in most of the existing algorithms to improve their performance IEEE Trans. Cybern. 46(12) (2016) 3059-3072. However, we found that this algorithm consumes a lot of time in reordering data because a one-to-one comparison method is used in this process, which is the main reason that the speeding-up structure is difficult to establish when coping with much more large amount of data. To overcome these problems, this paper aims to propose an improved speeding-up AR algorithm based on group similarity and Apache Spark framework to further reduce the memory requirements and runtime. Our simulation results on the police business big dataset make clear that our improved approach performs well and is more suitable for a big data environment.																	0218-0014	1793-6381				FEB	2020	34	2							2059005	10.1142/S0218001420590053													
J								The Identification and Evaluation Model for Test Paper's Color and Substance Concentration	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Substance concentration; Pearson's Chi-squared test; LM neural network; generalization ability		The colorimetric method is usually used to test the concentration of substances. However, this method has a big error since different people have different sensitivities to colors. In this paper, in order to solve the identification problem of the color and the concentration of the test paper, firstly, we found out that the concentration of substance is correlated with the color reading by using the Pearson's Chi-squared test method. And by the concentration coefficient of Pearson correlation analysis, the concentration of substance and color reading is highly correlated. Secondly, according to the RGB value of the paper image, the color moments of the image are calculated as the characteristics of the image, and the Levenberg-Marquardt (LM) neural network is established to classify the concentration of the substance. The accuracy of the training set model is 94.5%, and the accuracy of the test set model is 87.5%. The model precision is high, and the model has stronger generalization ability. Therefore, according to the RGB value of the test paper image, it is effective to establish the LM neural network model to identify the substance concentration.																	0218-0014	1793-6381				FEB	2020	34	2							2055004	10.1142/S0218001420550046													
J								Lane Marking Detection Based on Segments with Upper and Lower Structure	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Lane marking detection; intelligent vehicle; upper and lower structure; dashed and curve road		An effective and accurate lane marking detection algorithm is a fundamental element of the intelligent vehicle system and the advanced driver assistant system, which can provide important information to ensure the vehicle runs in the lane or warn the driver in case of lane departure. However, in the complex urban environment, lane markings are always affected by illumination, shadow, rut, water, other vehicles, abandoned old lane markings and non-lane markings, etc. Meanwhile, the lane markings are weak caused by hard use over time. The dash and curve lane marking detection is also a challenge. In this paper, a new lane marking detection algorithm for urban traffic is proposed. In the low-level phase, an iterative adaptive threshold method is used for image segmentation, which is especially suitable for the blurred and weakened lane markings caused by low illumination or wear. In the middle-level phase, the algorithm clusters the candidate pixels into line segments, and the upper and lower structure is used to cluster the line segments into candidate lanes, which is more suitable for curve and dashed lane markings. In the high-level phase, we compute the highest scores to get the two optimal lane markings. The optimal strategy can exclude interference similar to lane markings. We test our algorithm on Future Challenge TSD-Lane dataset and KITTI UM dataset. The results show our algorithm can effectively detect lane markings under multiple disturbance, occlusions and sharp curves.																	0218-0014	1793-6381				FEB	2020	34	2							2055005	10.1142/S0218001420550058													
J								Analysis and Research on the Structure Selection of BeiDou Navigation Array Antenna Based on Optimized PI Algorithm	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Anti-jamming; BeiDou navigation system; array antenna; space anti-jamming	ADAPTIVE-ARRAY; INTERFERENCE; NOISE	To overcome the disadvantages of the traditional power inversion algorithm, which has obvious suppression effect on strong interference signals but unobvious effect on suppression of weak interference signals, a matrix decomposition method is proposed to extract the interference submatrix and noise sub-matrix, so that the power inversion algorithm can be adopted to weaken and filter out the signals in the case of weak interference. Through the optimized power inversion algorithm, the interference signal can be weakened obviously at the antenna receiving end, and the quality of the effective signal received by the antenna can be improved significantly. Afterwards, the algorithm was used to carry out MATLAB simulation in space based on the anti-jamming performance of different array models, which determined the anti-jamming antenna array structure as a uniform matrix and optimized the array structure. The simulation results show that the optimized power inversion algorithm can filter strong and weak interference better, and the anti-jamming performance is better under different array models and two kinds of interferences. Through comparative analysis, it is concluded that the four elements have the best interference suppression effect in uniform circular array structure, which provides an important reference for the selection of antenna array in adaptive interference suppression hardware structure. At the same time, the BeiDou four-element adaptive circular array antenna, as an important part of intelligent geological disaster monitoring and early warning system, has good anti-jamming ability.																	0218-0014	1793-6381				FEB	2020	34	2							2058002	10.1142/S0218001420580021													
J								Self-Adapting Patch Strategies for Face Recognition	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Two-dimensional discrete wavelet transform; self-adapting patch strategy; edge recovery; local binary pattern; adaptive forward-backward greedy algorithm; sparse constraint	BACKWARD GREEDY ALGORITHM; REPRESENTATION; CLASSIFICATION; FEATURES; SCALE	In this paper, we propose two self-adapting patch strategies, which are obtained by employing the integral projection technique on images' edge images, while the edge images are recovered by the two-dimensional discrete wavelet transform. The patch strategies are equipped with the advantage of considering the single image's unique properties and maintaining the integrity of some particular local information. Combining the self-adapting patch strategies with local binary pattern feature extraction and the classifier of the forward and backward greedy algorithms under strong sparse constraint, we propose two new face recognition methods. Experiments are run on the Georgia Tech, LFW and AR face databases. The obtained numerical results show that the new methods outperform some related patch-based methods to a larger extent.																	0218-0014	1793-6381				FEB	2020	34	2							2056002	10.1142/S0218001420560029													
J								A New Method for Railway Fastener Detection Using the Symmetrical Image and Its EA-HOG Feature	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Railway fastener detection; computer vision; edge authentication; symmetrical image; improved sparse representation	CLASSIFICATION; VISION	Railway fastener recognition and detection is an important task for railway operation safety. However, the current automatic inspection methods based on computer vision can effectively detect the intact or completely missing fasteners, but they have weaker ability to recognize the partially worn ones. In our method, we exploit the EA-HOG feature fastener image, generate two symmetrical images of original test image and turn the detection of the original test image into the detection of two symmetrical images, then integrate the two recognition results of symmetrical image to reach exact recognition of original test image. The potential advantages of the proposed method are as follows: First, we propose a simple yet efficient method to extract the fastener edge, as well as the EA-HOG feature of the fastener image. Second, the symmetry images indeed reflect some possible appearance of the fastener image which are not shown in the original images, these changes are helpful for us to judge the status of the symmetry samples based on the improved sparse representation algorithm and then obtain an exact judgment of the original test image by combining the two corresponding judgments of its symmetry images. The experiment results show that the proposed approach achieves a rather high recognition result and meets the demand of railway fastener detection.																	0218-0014	1793-6381				FEB	2020	34	2							2055006	10.1142/S021800142055006X													
J								Method for Detecting Chinese Texts in Natural Scenes Based on Improved Faster R-CNN	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Faster RCNN; inception ResNet; text detection		In this study, a natural scene text detection method based on the improved faster region-based convolutional neural network (R-CNN) is proposed. This method extracts image features with the Inception-ResNet architecture, adopts a region proposal network to generate region proposals for the extracted features, merges the fine-tuned features with the region proposals, and finally, uses Fast R-CNN to classify and locate text. The proposed method solves the problems of varying text sizes and the text being obscured in the image. Compared with the original Faster R-CNN, the multilevel Inception-ResNet network model presented in this study can extract deeper text features. The extracted feature map is further sparsely represented by Reduction B, Inception ResNet C and Avg Pool, and then is fused with text regions obtained by the text feature mapping lower layer network to acquire the exact text regions. The text detection method presented in this study is tested on the 2017 dataset of ICDAR2017 Competition on Reading Chinese Text in the Wild (RCTW-17), which contains a large number of distorted, blurry, different scale and size texts. An accuracy of 76.4% is achieved in this platform, thereby proving the efficiency of the proposed method.																	0218-0014	1793-6381				FEB	2020	34	2							2053002	10.1142/S021800142053002X													
J								Performance Analysis of Different Optimizers for Deep Learning-Based Image Recognition	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Convolutional neural network; image recognition; sgdm adam; rmsprop		Deep learning refers to Convolutional Neural Network (CNN). CNN is used for image recognition for this study. The dataset is named Fruits-360 and it is obtained from the Kaggle dataset. Seventy percent of the pictures are selected as training data and the rest of the images are used for testing. In this study, an image size is 100 x 100 x 3. Training is realized using Stochastic Gradient Descent with Momentum (sgdm), Adaptive Moment Estimation (adam) and Root Mean Square Propogation (rmsprop) techniques. The threshold value is determined as 98% for the training. When the accuracy reaches more than 98%, training is stopped. Calculation of the final validation accuracy is done using trained network. In this study, more than 98% of the predicted labels match the true labels of the validation set. Accuracies are calculated using test data for sgdm, adam and rmsprop techniques. The results are 98.08%, 98.85%, 98.88%, respectively. It is clear that fruits are recognized with good accuracy.																	0218-0014	1793-6381				FEB	2020	34	2							2051003	10.1142/S0218001420510039													
J								An Improved Real-Time Recommendation for Microblogs Based on Topic	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Recommendation system; clustering; microblog; topic	SYSTEM	With the rapid development of the Internet, people are confronted with information overload. Many recommendation methods are designed to solve this problem. The main contributions of recommendation methods proposed in this paper are as follows: (1) An improved collaborative filtering recommendation algorithm based on user clustering is proposed. Clustering is performed according to user similarity based on the user-item rating matrix. So the search space of recommendation algorithm is reduced. (2) Considering the factor that user's interests may dynamically change over time, a time decay function is introduced. (3) A method of real-time recommendation based on topic for microblogs is designed to realize real-time recommendation effectively by preserving intermediate variables of user similarity. Experiments show that the proposed algorithms have been improved in terms of running time and accuracy.																	0218-0014	1793-6381				FEB	2020	34	2							2059006	10.1142/S0218001420590065													
J								Water Level Prediction of Rainwater Pipe Network Using an SVM-Based Machine Learning Method	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Urban flood; real-time prediction; support vector machine; machine learning methods	ARTIFICIAL NEURAL-NETWORKS; FLASH-FLOOD; MODEL; SIMULATION; RUNOFF; PARALLEL; SYSTEM	Model accuracy and running speed are the two key issues for flood warning in urban areas. Traditional hydrodynamic models, which have a rigorous physical mechanism for flood routine, have been widely adopted for water level prediction of rainwater pipe network. However, with the amount of pipes increasing, both the running speed and data availability of hydrodynamic models would be decreased rapidly. To achieve a real-time prediction for the water level of the rainwater pipe network, a new framework based on a machine learning method was proposed in this paper. The spatial and temporal autocorrelation of water levels for adjacent manholes was revealed through theoretical analysis, and then a support vector machine (SVM)-based machine learning model was developed, in which the water levels of adjacent manholes and rivers-near-by-outlets at the last time step were chosen as the independent variables, and then the water levels at the current time step can be computed by the proposed machine learning model with calibrated parameters. The proposed framework was applied in Fuzhou city, China. It turns out that the proposed machine learning method can forecast the water level of the rainwater pipe network with good accuracy and running speed.																	0218-0014	1793-6381				FEB	2020	34	2							2051002	10.1142/S0218001420510027													
J								Study on the Structural Parameters and Response Characteristics of the Tilted Antenna of Directional Electromagnetic Wave Resistivity Measuring Instrument	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Directional well mining; directional logging-while-drilling instrument; tilted antennas; directional response		The directional logging-while-drilling measurements enable to monitor the distance to formation boundaries and their orientation and facilitate proactive well placement, and thus can provide directionality information useful in detecting physical parameters such as bed boundaries, formation dip and formation azimuth, so the oil and gas recovery can be greatly improved. The characteristics of a kind of electromagnetic wave resistivity logging while drilling (LWD) tool with tilted antennas are computed via the magnetic-current-source dyadic Green's function for horizontally stratied anisotropic media. The current characteristic at the formation interface of the tilted antennas is compared with that of the axial antenna, and how the LWD tool with tilted antennas to detect the formation interface position and orientation is revealed. The amplitude-attenuation and phase-shift characteristics of LWD tool with tilted antennas are analyzed, and how to design the angle of tilted antenna is presented. The relationship between the emitting frequency and emitting-receiving antenna space parameters is expounded, and the emitting-receiving antenna space parameter suitable for different frequencies is presented. The detection characteristic of the electromagnetic wave measuring instrument is studied. For the highly deviated well formation model, the directional response characteristics of the different relative dip angle isotropic and anisotropic formation are numerically simulated and analyzed. The directional response characteristics of the two kinds of coil arrangement (unilateral arrangement and symmetrical arrangement) are analyzed by numerical simulation, and the solution to reduce or eliminate the influence of the directional response of the isotropy on the directional response is obtained. The conclusions obtained by theoretical analysis and numerical simulation provide a guide for the instrument principle research and application.																	0218-0014	1793-6381				FEB	2020	34	2							2058001	10.1142/S021800142058001X													
J								Graphical Analysis of Local Sliding Failure of Roller Slope	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Slope; local slump; stereographic projection; solid proportion projection		In this paper, for some typical local slump problems, through the site survey of engineering conditions, we observe the local slumping parts, collect a large number of production data, and adopt the Chiping polar projection method to evaluate the initial stability of the problem area. The unstable structure is combined with the solid scale projection to analyze the sliding direction, the sliding surface, the sliding amount, and the like. The study has a clearer understanding of the local slump problem, and based on the stability judgment results, combined with the site conditions, ultimately serves the engineering examples.																	0218-0014	1793-6381				FEB	2020	34	2							2050001	10.1142/S0218001420500019													
J								Ensemble Method of Feature Selection and Reverse Construction of Gene Logical Network Based on Information Entropy	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Feature selection; entropy; gene network; degree difference; attractor	BREAST-CANCER; DIFFERENTIAL-EQUATIONS; MICROARRAY DATA; EXPRESSION; CLASSIFICATION; ALGORITHM; DYNAMICS; MODEL; RECEPTORS	In this paper, we propose a novel ensemble gene selection method to obtain a gene subset. Then we provide a reverse construction method of gene network derived from expression profile data of the gene subset. The uncertainty coefficient based on information entropy are used to define the existence of logical relations among these genes. If the uncertainty coefficient between some genes exceeds predefined thresholds, the gene nodes will be connected by directed edges. Thus, a gene network is generated, which we define as gene logical network. This method is applied to the breast cancer data including control group and experimental group, with comparisons of the 2nd-order logic type distribution, average degree as well as average path length of the networks. It is found that these structures with different networks are quite distinct. By the comparison of the degree difference between control group and experimental group, the key genes are picked up. By defining the dynamics evolution rules of state transition based on the logical regulation among the key genes in the network, the dynamic behaviors for normal breast cells and cells with cancer of different stages are simulated numerically. Some of them are highly related to the development of breast cancer through literature inquiry. The study may provide a useful revelation to the biological mechanism in the formation and development of cancer.																	0218-0014	1793-6381				FEB	2020	34	2							2059004	10.1142/S0218001420590041													
J								Building a Credit Scoring Model Based on Data Mining Approaches	INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING										Classification; credit scoring; data mining; generalized linear model (GLM); logistic regression (LR)	CLASSIFIERS	Nowadays, one of the biggest challenges in banking sector, certainly, is assessment of the client's creditworthiness. In order to improve the decision-making process and risk management, banks resort to using data mining techniques for hidden patterns recognition within a wide data. The main objective of this study is to build a high-performance customized credit scoring model. The model named Reliable client is based on Bank's real dataset and originally built by applying four different classification algorithms: decision tree (DT), naive Bayes (NB), generalized linear model (GLM) and support vector machine (SVM). Since it showed the greatest results, but also seemed as the most appropriate algorithm, the adopted model is based on GLM algorithm. The results of this model are presented based on many performance measures that showed great predictive confidence and accuracy, but we also demonstrated significant impact of data pre-processing on model performance. Statistical analysis of the model identified the most significant parameters on the model outcome. In the end, created credit scoring model was evaluated using another set of real data of the same Bank.																	0218-1940	1793-6403				FEB	2020	30	2					147	169		10.1142/S0218194020500072													
J								Usability Requirements Extraction Method from Software Document	INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING										Usability requirements; usability; extraction; software document		Extracting the usability requirements are crucial during requirements review and requirements validation for different purposes. Usability requirements are hard to be determined until the real user has experienced the software. It is even more challenging when these usability requirements are documented in natural language, which has an inconsistent and unrestricted structure. Automated requirements extraction has been widely studied to facilitate the process of requirements checking. Nevertheless, the accuracy of requirements extraction method still can be improved. Thus, this paper has presented the usability keywords repository that followed the ISO 9126 and ISO 25010 usability category and has gone through the expert validation process. The usability requirement extraction method is moreover enhanced with extra procedures in conforming the usability requirement statement. First, each statement in the requirement document is checked if there is a keyword usability, whereby the keywords used must match with the usability keyword repository. In order to ensure that the selected statement is a usability requirement, the corresponding usability keyword position should be after the fixed auxiliary verbs. The performance of the improved usability requirements extraction method is then evaluated using precision, recall and accuracy.																	0218-1940	1793-6403				FEB	2020	30	2					171	189		10.1142/S0218194020500084													
J								Case Study Investigation of the Fault Detection and Error Locating Effects of Architecture-based Software Testing	INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING										Software product line testing; architecture-based software testing; points of observation; points of control and observation	GOLDEN-AGE	For software testing, it is well known that the architecture of a software system can be utilized to enhance testability, fault detection and error locating. However, how much and what effects architecture-based software testing has on software testing have been rarely studied. Thus, this paper undertakes case study investigation of the effects of architecture-based software testing specifically with respect to fault detection and error locating. Through comparing the outcomes with the conventional testing approaches that are not based on test architectures, we confirm the effectiveness of architecture-based software testing with respect to fault detection and error locating. The case studies show that using test architecture can improve fault detection rate by 44.1%-88.5% and reduce error locating time by 3%-65.2%, compared to the conventional testing that does not rely on test architecture. With regard to error locating, the scope of relevant components or statements was narrowed by leveraging test architecture for approximately 77% of the detected faults. We also show that architecture-based testing could provide a means of defining an exact oracle or oracles with range values. This study shows by way of case studies the extent to which architecture-based software testing can facilitate detecting certain types of faults and locating the errors that cause such faults. In addition, we discuss the contributing factors of architecture-based software testing which enable such enhancement in fault detection and error locating.																	0218-1940	1793-6403				FEB	2020	30	2					191	216		10.1142/S0218194020500096													
J								Knowledge Deduction and Reuse Application to the Products' Design Process	INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING										Product lifecycle management; product design process; knowledge reuse; knowledge deduction; ontology reasoning; SWRL rules	LIFE-CYCLE; MANAGEMENT; SYSTEM; REPRESENTATION; ARCHITECTURE; ACQUISITION; SELECTION; MODELS	In this paper, we introduce a framework for knowledge reuse and deduction in mechanical products design and development. The proposed system effectively exploits the capitalized and inferred knowledge. To this end, we settled up an ontology dealing with the design process of mechanical products such as "the car". The ontology-based framework is supported by a software tool that brings an automatic and personalized assistance to correspondent actors using the deduction process. Indeed, the systems provides the relevant knowledge to the suitable users in order to facilitate their professional tasks considering their roles and collaboration. Experimental results have demonstrated the effectiveness of reusing knowledge during product development lifecycle.																	0218-1940	1793-6403				FEB	2020	30	2					217	237		10.1142/S0218194020500102													
J								Log-Based Anomaly Detection with the Improved K-Nearest Neighbor	INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING										Selection of training set; logs; anomaly detection; K-nearest neighbor		Logs play an important role in the maintenance of large-scale systems. The number of logs which indicate normal (normal logs) differs greatly from the number of logs that indicate anomalies (abnormal logs), and the two types of logs have certain differences. To automatically obtain faults by K-Nearest Neighbor (KNN) algorithm, an outlier detection method with high accuracy, is an effective way to detect anomalies from logs. However, logs have the characteristics of large scale and very uneven samples, which will affect the results of KNN algorithm on log-based anomaly detection. Thus, we propose an improved KNN algorithm-based method which uses the existing mean-shift clustering algorithm to efficiently select the training set from massive logs. Then we assign different weights to samples with different distances, which reduces the negative effect of unbalanced distribution of the log samples on the accuracy of KNN algorithm. By comparing experiments on log sets from five supercomputers, the results show that the method we proposed can be effectively applied to log-based anomaly detection, and the accuracy, recall rate and F measure with our method are higher than those of traditional keyword search method.																	0218-1940	1793-6403				FEB	2020	30	2					239	262		10.1142/S0218194020500114													
J								A Model-Based Test Case Prioritization Approach Based on Fault Urgency and Severity	INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING										Test case prioritization; HMM; fault urgency; fault severity	VALIDATION; METRICS	With the aggrandizement scale of software system, the number of test cases has grown explosively. Test case prioritization (TCP) has been widely used in software testing to effectively improve testing efficiency. However, traditional TCP methods are mostly based on software code and they are difficult to apply to model-based testing. Moreover, existing model-based TCP techniques often do not take the likely distribution of faults into consideration, yet software faults are not often equally distributed in the system, and test cases that cover more fault prone modules are more likely to reveal faults so that they should be run with a higher priority. Therefore, in this paper, we provide a TCP approach based on Hidden Markov Model (HMM), to detect faults as earlier as possible and reduce the cost of modification. This approach consists of the following main parts: (1) transforming the Unified Modeling Language (UML) sequence diagram to HMM; (2) estimating the fault urgency according to fault priority and probability; (3) estimating the fault severity by analyzing the weight of the state in the HMM; (4) generating test case priority from fault urgency and fault severity, then prioritizing test case. The proposed approach is implemented on unmanned aerial vehicles (UAV) flight control system to perform TCP. The experimental results show that our proposed TCP approach can effectively enhance the probability of earlier fault detection and improve the efficiency and stability as compared to other prioritization techniques, such as original prioritization, random prioritization, additional prioritization and EPS-UML.																	0218-1940	1793-6403				FEB	2020	30	2					263	290		10.1142/S0218194020500126													
J								QFD and Fuzzy Kano model based approach for classification of aesthetic attributes of SUV car profile	JOURNAL OF INTELLIGENT MANUFACTURING										Kano model; Quality function deployment; Fuzzy Kano model; Aesthetic attributes; Product design; Customer satisfaction	QUALITY FUNCTION DEPLOYMENT; PRODUCT APPEARANCE; DESIGN; INTEGRATION; QUANTIFICATION; BRAND; FORM	The aesthetic appearance and features of a product are the most censorious elements for the accomplishment of a product in the industry. An aesthetic is the quality element which adds value to the product design. Product design is a basic need of every manufacturing company in which visual aspects play an important role to enhance the customer satisfaction. Therefore, Quality Function Deployment (QFD) can be considered as an effective tool for translating the customer's voice into the design of the product and its specifications. The Kano model helps to identify the desires of a product that brings greater satisfaction or dissatisfaction level to the customer. Kano model tells the connection between the product's attributes and its satisfaction to the customer. For achieving better results, Fuzzy Kano model has been more favorably applied over traditional Kano model. In this work, an approach of Integration of Kano model into QFD has been applied with an aim to examine the customer satisfaction based on aesthetic sentiments. A Sport Utility Vehicle has been selected for the study. The aesthetic attributes have been selected with the help of QFD and their importance and classification have been calculated using both Fuzzy Kano and Traditional Kano model. The result of Fuzzy Kano and Traditional Kano model has also been compared to calculate the effectiveness of the applied approach.																	0956-5515	1572-8145				FEB	2020	31	2					271	284		10.1007/s10845-018-1444-5													
J								Condition monitoring and prediction of solution quality during a copper electroplating process	JOURNAL OF INTELLIGENT MANUFACTURING										Condition monitoring; Fault prognostics; Prognostics and health management (PHM); Remaining useful life; Copper electroplating process	ELECTROCHEMICAL IMPEDANCE SPECTROSCOPY; DAMASCENE PROCESS; DEPOSITION; DIAGNOSIS	This paper presents a method for the monitoring and prediction of the electrolyte quality during the process of copper electroplating. This is important in industry, as any deviation in the solution quality leads to a deterioration of the quality of the processed products. The aim of the study is to identify some physical parameters that are representative of the quality variation during the deposition process. These parameters are then tracked online to continuously assess the solution quality and predict its remaining useful life. To do this, the process behavior is first characterized to derive a nominal model and to identify the physical parameters that can be used to describe the aging variation in the electrolyte quality. The aging model is then explored to assess the current level of the solution quality and to predict its remaining useful life. The proposed method is verified using real data acquired from a specifically designed test bench. The obtained results reveal the efficiency of the method.																	0956-5515	1572-8145				FEB	2020	31	2					285	300		10.1007/s10845-018-1445-4													
J								Ontology-based module selection in the design of reconfigurable machine tools	JOURNAL OF INTELLIGENT MANUFACTURING										Reconfigurable machine tool; Design; Module selection; Ontology; SWRL rule; Knowledge base	CONFIGURATION	Reconfigurable machine tools (RMTs) are important equipment for enterprises to cope with ever-changing markets because of their flexibility. In design of such equipment, selection of appropriate modules is a very critical decision factor to effectively and efficiently satisfy manufacturing requirements. However, the selection of appropriate modules is a challenging task because it is a multi-domain mapping process relying heavily on experts' domain knowledge, which is usually unstructured and implicit. To effectively support RMT designers, an ontology-based RMT module selection method is proposed. First, a knowledge base is built by development of an ontology to formally represent the taxonomy, properties, and causal relationships of/among three domain core concepts, namely, machining feature, machining operation, and RMT module involved in RMT design. Second, a four-step sequential procedure is established to facilitate the utilization of encoded knowledge from a knowledge base to aid in the selection of appropriate RMT modules. The procedure takes a given part family as the input, automatically infers the required machining operations as well as the RMT modules through rule-based reasoning, and eventually forms a set of RMT configurations that are capable of machining the part family as the output. Finally, the efficacy of the ontology-based RMT module selection method is demonstrated using a plate family manufacturing example. Results show that the approach is effective to support designers by appropriately and rapidly selecting modules and generating configurations in RMT design.																	0956-5515	1572-8145				FEB	2020	31	2					301	317		10.1007/s10845-018-1446-3													
J								Joint modeling of classification and regression for improving faulty wafer detection in semiconductor manufacturing	JOURNAL OF INTELLIGENT MANUFACTURING										Faulty wafer detection; Predictive modeling; Joint modeling; Neural network; Semiconductor manufacturing	COMBINING CLASSIFICATION; VIRTUAL METROLOGY; LEARNING APPROACH; NEURAL-NETWORKS; SYSTEM	In the semiconductor manufacturing process, it is important to identify wafers on which faults have occurred or will occur to avoid unnecessary and costly further processing and physical inspections. This issue can be addressed by formulating the faulty wafer detection problem as a predictive modeling task, in which the process parameters/measurements and subsequent inspection results concerning the faults comprise the input and output variables at the wafer level, respectively. To achieve improved predictive performance, this paper presents a joint modeling method that incorporates classification and regression tasks into a single prediction model. Given the output variables in both binary and continuous forms, the prediction model simultaneously considers both the classification and regression tasks to complement each other, where each task predicts the binary and continuous output variables, respectively. The outputs from these two tasks are combined to predict whether a wafer is faulty. The entire model is implemented as a neural network, and is trained by optimizing a single objective function. The effectiveness of the model is demonstrated with a case study using real-world data from a semiconductor manufacturer.																	0956-5515	1572-8145				FEB	2020	31	2					319	326		10.1007/s10845-018-1447-2													
J								Joint optimization of capacity, production and maintenance planning of leased machines	JOURNAL OF INTELLIGENT MANUFACTURING										Capacity design; Production; Preventive maintenance; Planning; Parallel leased machines; Random demand; Sequential approach; Branch and bound; Random exploration method; Usage rates; Degradation	PREVENTIVE-MAINTENANCE; INTEGRATED PRODUCTION; MANUFACTURING SYSTEMS; MODEL; INVENTORY; POLICY; JOBS; EQUIPMENT	This paper deals with capacity design, production and preventive maintenance planning of a production system made of parallel leased machines. The production system must satisfy a random demand over a finite number of periods. The number of machines to be leased, the quantity to produce by each one and customer demand are variable. A joint optimization approach of a maintenance strategy and an economical production plan is developed by considering the influence of usage rates on the degradation of machines. Firstly, we determine a nearly optimal number of machines to be leased as well as the quantities to produce during each period. Secondly, given the obtained production plan, a periodic preventive maintenance policy with minimal repairs at failure is determined for each machine. Branch and bound and random exploration methods are used to obtain an optimal solution.																	0956-5515	1572-8145				FEB	2020	31	2					351	374		10.1007/s10845-018-1450-7													
J								A deep neural network for classification of melt-pool images in metal additive manufacturing	JOURNAL OF INTELLIGENT MANUFACTURING										Additive manufacturing; Powder bed fusion; Selective laser melting; Melt-pool classification; Deep neural network	PROCESSING PARAMETERS; LASER; MECHANISM; BEHAVIOR	By applying a deep neural network to selective laser melting, we studied a classification model of melt-pool images with respect to 6 laser power labels. Laser power influenced to form pores or cracks determining the part quality and was positively-linearly dependent to the density of the part. Using the neural network of which the number of nodes is dropped with increasing the layer number achieved satisfactory inference when melt-pool images had blurred edges. The proposed neural network showed the classification failure rate under 1.1% for 13,200 test images and was more effective to monitor melt-pool images because it simultaneously handled various shapes, comparing with a simple calculation such as the sum of pixel intensity in melt-pool images. The classification model could be utilized to infer the location to cause the unexpected alteration of microstructures or separate the defective products non-destructively.																	0956-5515	1572-8145				FEB	2020	31	2					375	386		10.1007/s10845-018-1451-6													
J								Adapting Thurstone's Law of Comparative Judgment to fuse preference orderings in manufacturing applications	JOURNAL OF INTELLIGENT MANUFACTURING										Manufacturing; Quality engineering; management; Decision making; Preference ordering; Paired comparison; Law of comparative judgment; Scaling; Ratio scale	PRODUCT; RATINGS; DESIGN; VALUES	A rather common problem in the manufacturing field includes: (i) a collection of objects to be compared on the basis of the degree of some attribute, (ii) a set of judges that individually express their subjective judgments on these objects, and (iii) a single collective judgment, which is obtained by fusing the previous subjective judgments. The goal of this contribution is to develop a new technique that combines the Thurstone's Law of Comparative Judgment with an ad hoc response mode based on preference orderings. Apart from being relatively practical and user-friendly, this technique allows to express the collective judgment of objects on a ratio scale and is applicable to a variety of practical contexts in the field of manufacturing. The description of the proposed technique is integrated with the application to a practical case study.																	0956-5515	1572-8145				FEB	2020	31	2					387	402		10.1007/s10845-018-1452-5													
J								Failure time prediction using adaptive logical analysis of survival curves and multiple machining signals	JOURNAL OF INTELLIGENT MANUFACTURING										Failure time prediction; Logical analysis of data (LAD); Logical analysis of survival curves (LASC); Kaplan-Meier; Pattern recognition; Machine learning	REMAINING USEFUL LIFE; OF-THE-ART; PROGNOSTICS	This paper develops a prognostic technique called the logical analysis of survival curves (LASC). This technique is used to learn the degradation process of any physical asset, and consequently to predict its failure time (T). It combines the reliability information that is obtained from a classical Kaplan-Meier non-parametric curve to that obtained from online measurements of multiple sensed signals of degradation. An analysis of these signals by the machine learning technique, logical analysis of data (LAD), is performed to exploit the instantaneous knowledge about the state of degradation of the asset studied. The experimental results of the predictions of failure times for cutting tools are reported. The results show that LASC prognostic results are better than the results obtained by well-known machine learning techniques. Other advantages of the proposed techniques are also discussed.																	0956-5515	1572-8145				FEB	2020	31	2					403	415		10.1007/s10845-018-1453-4													
J								Adaptive job shop scheduling strategy based on weighted Q-learning algorithm	JOURNAL OF INTELLIGENT MANUFACTURING										Job shop; Adaptive scheduling; Multi-agent technology; Q-learning; Searching strategy	MANUFACTURING SYSTEMS; RULES	Given the dynamic and uncertain production environment of job shops, a scheduling strategy with adaptive features must be developed to fit variational production factors. Therefore, a dynamic scheduling system model based on multi-agent technology, including machine, buffer, state, and job agents, was built. A weighted Q-learning algorithm based on clustering and dynamic search was used to determine the most suitable operation and to optimize production. To address the large state space problem caused by changes in the system state, four state features were extracted. The dimension of the system state was decreased through the clustering method. To reduce the error between the actual system states and clustering ones, the state difference degree was defined and integrated with the iteration formula of the Q function. To select the optimal state-action pair, improved search and iteration update strategies were proposed. Convergence analysis of the proposed algorithm and simulation experiments indicated that the proposed adaptive strategy is well adaptable and effective in different scheduling environments, and shows better performance in complex environments. The two contributions of this research are as follows: (1) a dynamic greedy search strategy was developed to avoid blind searching in traditional strategy. (2) Weighted iteration update of the Q function, including the weighted mean of the maximum fuzzy earning, was designed to improve the speed and accuracy of the improved learning algorithm.																	0956-5515	1572-8145				FEB	2020	31	2					417	432		10.1007/s10845-018-1454-3													
J								Intelligent rotating machinery fault diagnosis based on deep learning using data augmentation	JOURNAL OF INTELLIGENT MANUFACTURING										Fault diagnosis; Data augmentation; Deep residual learning; Rolling bearing; Convolutional neural network	EMPIRICAL MODE DECOMPOSITION; NEURAL-NETWORK; EXTRACTION; PREDICTION	Intelligent machinery fault diagnosis system has been receiving increasing attention recently due to the potential large benefits of maintenance cost reduction, enhanced operation safety and reliability. This paper proposes a novel deep learning method for rotating machinery fault diagnosis. Since accurately labeled data are usually difficult to obtain in real industries, data augmentation techniques are proposed to artificially create additional valid samples for model training, and the proposed method manages to achieve high diagnosis accuracy with small original training dataset. Two augmentation methods are investigated including sample-based and dataset-based methods, and five augmentation techniques are considered in general, i.e. additional Gaussian noise, masking noise, signal translation, amplitude shifting and time stretching. The effectiveness of the proposed method is validated by carrying out experiments on two popular rolling bearing datasets. Fairly high diagnosis accuracy up to 99.9% can be obtained using limited training data. By comparing with the latest advanced researches on the same datasets, the superiority of the proposed method is demonstrated. Furthermore, the diagnostic performance of the deep neural network is extensively evaluated with respect to data augmentation strength, network depth and so forth. The results of this study suggest that the proposed intelligent fault diagnosis method offers a new and promising approach.																	0956-5515	1572-8145				FEB	2020	31	2					433	452		10.1007/s10845-018-1456-1													
J								Solar cell surface defect inspection based on multispectral convolutional neural network	JOURNAL OF INTELLIGENT MANUFACTURING										Machine vision; Solar cell; Deep learning; Defection inspection	CRACK DETECTION; MACHINE VISION; CLASSIFICATION; IMAGES; MODEL	Similar and indeterminate defect detection of solar cell surface with heterogeneous texture and complex background is a challenge of solar cell manufacturing. The traditional manufacturing process relies on human eye detection which requires a large number of workers without a stable and good detection effect. In order to solve the problem, a visual defect detection method based on multi-spectral deep convolutional neural network (CNN) is designed in this paper. Firstly, a selected CNN model is established. By adjusting the depth and width of the model, the influence of model depth and kernel size on the recognition result is evaluated. The optimal CNN model structure is selected. Secondly, the light spectrum features of solar cell color image are analyzed. It is found that a variety of defects exhibited different distinguishable characteristics in different spectral bands. Thus, a multi-spectral CNN model is constructed to enhance the discrimination ability of the model to distinguish between complex texture background features and defect features. Finally, some experimental results and K-fold cross validation show that the multi-spectral deep CNN model can effectively detect the solar cell surface defects with higher accuracy and greater adaptability. The accuracy of defect recognition reaches 94.30%. Applying such an algorithm can increase the efficiency of solar cell manufacturing and make the manufacturing process smarter.																	0956-5515	1572-8145				FEB	2020	31	2					453	468		10.1007/s10845-018-1458-z													
J								A mechanism for scheduling multi robot intelligent warehouse system face with dynamic demand	JOURNAL OF INTELLIGENT MANUFACTURING										Intelligent warehousing system; Multi-robot; Scheduling; Synchronized	GENETIC ALGORITHM; DESIGN; PRODUCTS	Given the evolutionary journey of E-commerce, there have been emerging challenges confronting warehouse logistics, including smaller shipping units, more varieties and batches, and shorter cycles. These challenges are difficult to cope when using conventional scheduling with the robotic approach. Currently, automated storage and retrieval system are becoming preferred for warehouse companies with the help of mobile robots. However, when many orders are received simultaneously, the existing scheduling approach might make unreasonable decisions, leading to delayed packaging of entire orders and reducing the performance of the warehouse. Therefore, this paper addresses this problem and proposes a novel scheduling mechanism for multi-robot and tasks allocation problems which may arise in an intelligent warehouse system. This mechanism proposes into the intelligent warehouse troubled with simultaneous multiple customer demands. The mathematical model for the system is developed by considering a multitask robot facing dynamic customer demand. The proposed model's approach is based on the particle swarm optimization heuristic. The result for this approach then compared with the genetic algorithm (GA). The simulation results demonstrate that the proposed solution is far superior to that of the GA for multi-robot scheduling and tasks allocation problems in the intelligent warehouse.																	0956-5515	1572-8145				FEB	2020	31	2					469	480		10.1007/s10845-018-1459-y													
J								Optimization of cutting conditions using an evolutive online procedure	JOURNAL OF INTELLIGENT MANUFACTURING										Tool wear; Stochastic constraint; Machining; Optimization	NICKEL; WEAR	This paper proposes an online evolutive procedure to optimize the Material Removal Rate in a turning process considering a stochastic constraint. The usual industrial approach in finishing operations is to change the tool insert at the end of each machining feature to avoid defective parts. Consequently, all parts are produced at highly conservative conditions (low levels of feed and speed), and therefore, at low productivity. In this work, a framework to estimate the stochastic constraint of tool wear during the production of a batch is proposed. A simulation campaign was carried out to evaluate the performances of the proposed procedure. The results showed that it was possible to improve the Material Removal Rate during the production of the batch and keeping the probability of defective parts under a desired level.																	0956-5515	1572-8145				FEB	2020	31	2					481	499		10.1007/s10845-018-01460-x													
J								A model for assessment of the impact of configuration changes in complex products	JOURNAL OF INTELLIGENT MANUFACTURING										Complex products; The impact of configuration changes; Configuration change propagation; Propagation impact	PREDICTING CHANGE PROPAGATION; GREY RELATIONAL ANALYSIS; DESIGN	An assessment of the impact of configuration changes in complex products is significant for improving the accuracy of change decision-making. Most related studies lack objectivity, systematicness and applicability. For this reason, a four-phase model for the assessment of this impact is proposed. In Phase I, a network model for parameter relationships of complex products is built to accurately express the complex product structures. In Phase II, an assessment method for the change propagation probability based on the gray comprehensive relation analysis and an assessment method for the propagation impact probability via the analysis of configuration change values are proposed to compute the two probabilities objectively. In Phase III, the change propagation method is introduced to precisely assess the impact of the configuration changes between two parameters. In Phase IV, a method for assessing the overall impact of configuration changes via the division of the parameters into stages is put forward to systematically assess the impact of configuration changes on the whole complex products. The methods in the four-phase model are easy to program and could further improve the assessment efficiency. Besides, a practical application of the proposed assessment model is suggested to verify the validity and applicability of this research.																	0956-5515	1572-8145				FEB	2020	31	2					501	527		10.1007/s10845-018-01461-w													
J								Multi-feature fusion for image retrieval using constrained dominant sets	IMAGE AND VISION COMPUTING										Image retrieval; Multi-feature fusion; Diffusion process	DESCRIPTORS; DIFFUSION	Aggregating different image features for image retrieval has recently shown its effectiveness. While highly effective, though, the question of how to uplift the impact of the best features for a specific query image persists as an open computer vision problem. in this paper, we propose a computationally efficient approach to fuse several hand-crafted and deep features, based on the probabilistic distribution of a given membership score of a constrained cluster in an unsupervised manner. First, we introduce an incremental nearest neighbor (NN) selection method, whereby we dynamically select k-NN to the query. We then build several graphs from the obtained NN sets and employ constrained dominant sets (CDS) on each graph G to assign edge weights which consider the intrinsic manifold structure of the graph, and detect false matches to the query. Finally, we elaborate the computation of feature positive-impact weight (PIW) based on the dispersive degree of the characteristics vector. To this end, we exploit the entropy of a cluster membership-score distribution. In addition, the final NN set bypasses a heuristic voting scheme. Experiments on several retrieval benchmark datasets show that our method can improve the state-of-the-art result. (C) 2019 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				FEB	2020	94								103862	10.1016/j.imavis.2019.103862													
J								SAANet: Spatial adaptive alignment network for object detection in automatic driving	IMAGE AND VISION COMPUTING										Object detection; Fusion-based deep framework; Local orientation encoding; Spatial adaptive alignment; Autonomous driving		images and point clouds are beneficial for object detection in a visual navigation module for autonomous driving. The spatial relationships between different objects at different times in a bimodal space can vary significantly. It is difficult to combine bimodal descriptions into a unified model to effectively detect objects in an efficient amount of time. In addition, conventional voxelization methods resolve point clouds into voxels at a global level, and often overlook local attributes of the voxels. To address these problems, we propose a novel fusionbased deep framework named SAANet. SAANet utilizes a spatial adaptive alignment (SAA) module to align point cloud features and image features, by automatically discovering the complementary information between point clouds and images. Specifically, we transform the point clouds into 3D voxels, and introduce local orientation encoding to represent the point clouds. Then, we use a sparse convolutional neural network to learn a point cloud feature. Simultaneously, a ResNet-like 2D convolutional neural network is used to extract an image feature. Next, the point cloud feature and image feature are fused by our SAA block to derive a comprehensive feature. Then, the labels and 3D boxes for objects are learned using a multi-task learning network Finally, an experimental evaluation on the KITFI benchmark demonstrates the advantages of our method in terms of average precision and inference time, as compared to previous state-of-the-art results for 3D object detection. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				FEB	2020	94								103873	10.1016/j.imavis.2020.103873													
J								Learning reliable-spatial and spatial-variation regularization correlation filters for visual tracking	IMAGE AND VISION COMPUTING										Correlation filters; Visual tracking; Spatial regularization	OBJECT TRACKING; ROBUST	Single-object tracking is a significant and challenging computer vision problem. Recently, discriminative correlation filters (DCF) have shown excellent performance. But there is a theoretical defects that the boundary effect, caused by the periodic assumption of training samples, greatly limit the tracking performance. Spatially regularized DCF (SRDCF) introduces a spatial regularization to penalize the filter coefficients depending on their spatial location, which improves the tracking performance a lot. However, this simple regularization strategy implements unequal penalties for the target area filter coefficients, which makes the filter learn a distorted object appearance model. In this paper, a novel spatial regularization strategy is proposed, utilizing a reliability map to approximate the target area and to keep the penalty coefficients of relevant region consistent. Besides, we introduce a spatial variation regularization component that the second-order difference of the filter, which smooths changes of filter coefficients to prevent the filter over-fitting current frame. Furthermore, an efficient optimization algorithm called alternating direction method of multipliers (ADMM) is developed. Comprehensive experiments are performed on three benchmark datasets: OTB-2013, OTB-2015 and TempleColor-128, and our algorithm achieves a more favorable performance than several state-of-the-art methods. Compared with SRDCF, our approach obtains an absolute gain of 6.6% and 5.1% in mean distance precision on OTB-2013 and OTB-2015, respectively. Our approach runs in real-time on a CPU. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				FEB	2020	94								103869	10.1016/j.imavis.2020.103869													
J								Single image dehazing via a dual-fusion method	IMAGE AND VISION COMPUTING										Image dehazing; Multi-region fusion; Exposure fusion; New speckle reducing anisotropic diffusion model	ENHANCEMENT; VISION; VISIBILITY	Single image dehazing is a challenging task because of the hue and brightness distortion problems. In this paper, we propose a dual-fusion method for single image dehazing. By a segmentation method creating two divided regions, the sky and non-sky regions can be obtained. To properly optimize the transmission, a multi-region fusion method is proposed for single image smooth. An exposure fusion method is constructed by the brightness transform function to effectively remove the haze from a single image. Experimental results show that this method outperforms state-of-the-art dehazing methods in terms of both efficiency and the dehazing effect. (C) 2019 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				FEB	2020	94								103868	10.1016/j.imavis.2019.103868													
J								Coupled generative adversarial network for heterogeneous face recognition	IMAGE AND VISION COMPUTING										Heterogeneous face recognition; Generative adversarial networks; Face verification; Coupled deep neural network; Common latent subspace; Biometrics	FACIAL RECOGNITION	The large modality gap between faces captured in different spectra makes heterogeneous face recognition (HFR) a challenging problem. In this paper, we present a coupled generative adversarial network (CpGAN) to address the problem of matching non-visible facial imagery against a gallery of visible faces. Our CpGAN architecture consists of two sub-networks one dedicated to the visible spectrum and the other sub-network dedicated to the non-visible spectrum. Each sub-network consists of a generative adversarial network (GAN) architecture. Inspired by a dense network which is capable of maximizing the information flow among features at different levels, we utilize a densely connected encoder-decoder structure as the generator in each GAN sub-network. The proposed CpGAN framework uses multiple loss functions to force the features from each sub-network to be as close as possible for the same identities in a common latent subspace. To achieve a realistic photo reconstruction while preserving the discriminative information, we also added a perceptual loss function to the coupling loss function. An ablation study is performed to show the effectiveness of different loss functions in optimizing the proposed method. Moreover, the superiority of the model compared to the state-of-the-art models in HFR is demonstrated using multiple datasets. (C) 2019 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				FEB	2020	94								103861	10.1016/j.imavis.2019.103861													
J								Bottom-up unsupervised image segmentation using FC-Dense u-net based deep representation clustering and multidimensional feature fusion based region merging	IMAGE AND VISION COMPUTING										Unsupervised image segmentation; Deep learning; Feature fusion; Region merging; Image processing	SUPERPIXELS	Recent advances in system resources provide ease in the applicability of deep learning approaches in computer vision. In this paper, we propose a deep learning-based unsupervised image segmentation approach for natural image segmentation. Image segmentation aims to transform an image into regions, representing various objects in the image. Our method consists of a fully convolutional dense network-based unsupervised deep representation oriented clustering, followed by shallow features based high-dimensional region merging to produce the final segmented image. We evaluate our proposed approach on the BSD300 database and perform a comparison with several classical and some recent deep learning-based unsupervised segmentation methods. The experimental results represent that the proposed method is comparable and confirm the efficacy of the proposed approach.																	0262-8856	1872-8138				FEB	2020	94								103871	10.1016/j.imavis.2020.103871													
J								Flow Adaptive Video Object Segmentation	IMAGE AND VISION COMPUTING										Video object segmentation; Optical flow; Online adaptation; Semi-supervised; Interactive; Object tracking		We tackle the task of semi-supervised video object segmentation, i.e. pixel-level object classification of the images in video sequences using very limited ground truth training data of its corresponding video. We present FLow Adaptive Video Object Segmentation, an efficient pipeline based on a novel online adaptation algorithm that utilizes optical flow, capable of tracking objects effectively throughout videos. Comparing with most of the recent deep learning based approaches that trade efficiency for accuracy, we provide extensive complexity analysis and additionally demonstrate that FLAVOS is natural for real world applications by introducing an interactive pipeline that enables the user to provide feedback for online training. Our method achieves state-of-the-art accuracy on three challenging benchmark datasets and nearly ground-truth level segmentation results with interactive user feedback. (C) 2019 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				FEB	2020	94								103864	10.1016/j.imavis.2019.103864													
J								Enhancing deep discriminative feature maps via perturbation for face presentation attack detection	IMAGE AND VISION COMPUTING										Spoofing; Presentation attack detection; CNN; Attention; Face-biometrics	SPOOFING DETECTION; LIVENESS DETECTION; IMAGE QUALITY; TEXTURE	Face presentation attack detection (PAD) in unconstrained conditions is one of the key issues in face biometric-based authentication and security applications. In this paper, we propose a perturbation layer - a learnable preprocessing layer for low-level deep features to enhance the discriminative ability of deep features in face PAD. The perturbation layer takes the deep features - of a candidate layer in Convolutional Neural Network (CNN), the corresponding hand-crafted features of an input image, and produces adaptive convolutional weights for the deep features of the candidate layer. These adaptive convolutional weights determine the importance of the pixels in the deep features of the candidate layer for face PAD. The proposed perturbation layer adds very little overhead to the total trainable parameters in the model. We evaluated the proposed perturbation layer with Local Binary Patterns (LBP), with and without color information, on three publicly available face PAD databases, i.e., CASIA, Idiap Replay-Attack, and OULU-NPU databases. Our experimental results show that the introduction of the proposed perturbation layer in the CNN improved the face PAD performance, in both intra-database and cross-database scenarios. Our results also highlight the attention created by the proposed perturbation layer in the deep features and its effectiveness for face PAD in general. (C) 2019 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				FEB	2020	94								103858	10.1016/j.imavis.2019.103858													
J								A new cast shadow detection method for traffic surveillance video analysis using color and statistical modeling	IMAGE AND VISION COMPUTING										Traffic video analysis; Shadow detection; Global Foreground Modeling (GFM); New chromatic criteria; Shadow region detection method; Statistical shadow modeling	MOVING VEHICLE DETECTION; OBJECT DETECTION; SEGMENTATION; ALGORITHMS; REMOVAL	In traffic surveillance video analysis systems, the cast shadows of vehicles often have a negative effect or video analysis results. A novel cast shadow detection framework, which consists of a new foreground detec. tion method and a cast shadow detection method, is presented in this paper to detect and remove the cal shadows from the foreground. The new foreground detection method applies an innovative Global Fore" ground Modeling (GFM) method, a Gaussian mixture model or GMM, and the Bayes classifier for foregrounc and background classification. While the GFM method is for global foreground modeling, the GMM is foi local background modeling, and the Bayes classifier applies both the foreground and the background model! for foreground detection. The rationale of the GFM method stems from the observation that the foregrounc objects often appear in recent frames and their trajectories often lead them to different locations in these frames. As a result, the statistical models used to characterize the foreground objects should not be pixe based or locally defined. The cast shadow detection method contains four hierarchical steps. First, a set of new chromatic criteria is presented to detect the candidate shadow pixels in the HSV color space. Second a new shadow region detection method is proposed to cluster the candidate shadow pixels into shadow regions. Third, a statistical shadow model, which uses a single Gaussian distribution to model the shadow, class, is presented for classifying shadow pixels. Fourth, an aggregated shadow detection method is presented for final shadow detection. Experiments using the public video data 'Highway-1' and 'Highway-3' and the New Jersey Department of Transportation (NJDOT) real traffic video sequences show the feasibility of the proposed method. In particular, the proposed method achieves better shadow detection performance than the popular shadow detection methods, and is able to improve the traffic video analysis results. (C) 2019 Elsevier B.V. All rights reserved																	0262-8856	1872-8138				FEB	2020	94								103863	10.1016/j.imavis.2019.103863													
J								Skin detection and lightweight encryption for privacy protection in real-time surveillance applications	IMAGE AND VISION COMPUTING										Color-spaces; Human skin detection; Parallel processing; Privacy protection; Segmentation; Skin pixel encryption; Selective encryption	FACE DETECTION; ADAPTIVE APPROACH; SEGMENTATION; SECURITY; RECOGNITION; MODEL	An individual's privacy is a significant concern in surveillance videos. Existing research work into the location of individuals on the basis of detecting their skin is focused either on different techniques for detecting human skin on protecting individuals from the consequences of applying such techniques. This paper considers both lines of research and proposes a hybrid scheme for human skin detection and subsequent privacy protection by utilizing color information in dynamically varying illumination and environmental conditions. For those purposes, dynamic and explicit skin-detection approaches are implemented, simultaneously considering multiple colorspaces, i.e. RGB, perceptual (HSV) and orthogonal (YCbCr) color-spaces, and then detecting the human skin by the proposed Combined Threshold Rule (CTR)-based segmentation. Comparative qualitative and quantitative detection results with an average 93.73% accuracy, imply that the proposed scheme achieves considerable accuracy without incurring a training cost. Once skin detection has been performed, the detected skin pixels (including false positives) are encrypted, when standard AES-CFB encryption of skin pixels is shown to be preferable compared to selective encryption of a whole video frame. The scheme preserves the behavior of the subjects within the video. Hence, subsequent image processing and behavior analysis, if required, can be performed by an authorized user. The experimental results are encouraging, as they show that the average encryption time is 8.268 s and the Encryption Space Ratio (ESR) is an average 7.25% for a high definition video (1280 x 720 pixels/frame). A performance comparison in terms of Correct Detection Rate (CDR) showed an average 91.5% for CTB-based segmentation compared to using only one color space for segmentation, such as using RGB with 85.86%, HSV with 80.93% and YCbCr with an average 84.8%, which implies that the proposed method of combining color-space skin identifications has a higher ability to detect skin accurately. Security analysis confirmed that the proposed scheme could be a suitable choice for real-time surveillance applications operating on resource-constrained devices. (C) 2019 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				FEB	2020	94								103859	10.1016/j.imavis.2019.103859													
J								Post-mortem iris recognition with deep-learning-based image segmentation	IMAGE AND VISION COMPUTING										Biometrics; Iris recognition; Post-mortem; Image segmentation		This paper proposes the first known to us iris recognition methodology designed specifically for postmortem samples. We propose to use deep learning-based iris segmentation models to extract highly irregular iris texture areas in post-mortem iris images. We show how to use segmentation masks predicted by neural networks in conventional, Gabor-based iris recognition method, which employs circular approximations of the pupillary and limbic iris boundaries. As a whole, this method allows for a significant improvement in post-mortem iris recognition accuracy over the methods designed only for ante-mortem irises, including the academic OSIRIS and commercial IriCore implementations. The proposed method reaches the EER less than 1% for samples collected up to 10 hours after death, when compared to 16.89% and 5.37% of EER observed for OSIRIS and IriCore, respectively. For samples collected up to 369 h post-mortem, the proposed method achieves the EER 21.45%, while 33.59% and 25.38% are observed for OSIRIS and IriCore, respectively. Additionally, the method is tested on a database of iris images collected from ophthalmology clinic patients, for which it also offers an advantage over the two other algorithms. This work is the first step towards post-mortem-specific iris recognition, which increases the chances of identification of deceased subjects in forensic investigations. The new database of post-mortem iris images acquired from 42 subjects, as well as the deep learning-based segmentation models are made available along with the paper, to ensure all the results presented in this manuscript are reproducible. (C) 2019 The Authors. Published by Elsevier B.V.																	0262-8856	1872-8138				FEB	2020	94								103866	10.1016/j.imavis.2019.103866													
J								Online maximum a posteriori tracking of multiple objects using sequential trajectory prior	IMAGE AND VISION COMPUTING										Online multi-object tracking; Sequential trajectory prior; Maximum a posteriori; Data association	DATA ASSOCIATION; MULTIOBJECT TRACKING; APPEARANCE; MODEL	In this paper, we address the problem of online multi-object tracking based on the Maximum a Posteriori (MAP) framework. Given the observations up to the current frame, we estimate the optimal object trajectories via two MAP estimation stages: object detection and data association. By introducing the sequential trajectory prior. i.e., the prior information from previous frames about "good" trajectories. into the two MAP stages, the inference of optimal detections is refined and the association correctness between trajectories and detections is enhanced. Furthermore, the sequential trajectory prior allows the two MAP stages to interact with each other in a sequential manner, which jointly optimizes the detections and trajectories to facilitate online multi-object tracking. Compared with existing methods, our approach is able to alleviate the association ambiguity caused by noisy detections and frequent inter-object interactions without using sophisticated association likelihood models. The experiments on publicly available challenging datasets demonstrate that our approach provides superior tracking performance over state-of-the-art algorithms in various complex scenes. (C) 2019 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				FEB	2020	94								103867	10.1016/j.imavis.2019.103867													
J								Variational shape prior segmentation with an initial curve based on image registration technique	IMAGE AND VISION COMPUTING										Shape prior segmentation; Hierarchical image segmentation; Image registration; Free-form deformation		In general images, it is practically hard to distinguish only the desired object using the conventional image segmentation methods. In many cases, we can segment the desired object by using the shape information of the object in addition to the standard image segmentation. Chan and Zhu's model is not robust to the intensity changes of objects. In this paper, we propose a novel model for the shape prior segmentation that produces robust results using the hierarchical image segmentation and an attraction term. Moreover, we adopt an image registration technique and a multi-region image segmentation to get an initial for a given shape prior. Finally, we consider the free-form deformation in obtaining the shape function from the reference shape prior for real-world images. Numerical experiments demonstrate the results independent of intensities of objects and the location of the reference shape prior. All numerical calculations are automatic and progress without any user input. (C) 2019 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				FEB	2020	94								103865	10.1016/j.imavis.2019.103865													
J								Collective Sports: A multi-task dataset for collective activity recognition	IMAGE AND VISION COMPUTING										Collective activity recognition; Action recognition; Convolutional neural networks; Multi-task learning; LSTM		Collective activity recognition is an important subtask of human action recognition, where the existing datasets are mostly limited. In this paper, we look into this issue and introduce the Collective Sports (C-Sports) dataset, which is a novel benchmark dataset for multi-task recognition of both collective activity and sports categories. Various state-of-the-art techniques are evaluated on this dataset, together with multi-task variants which demonstrate increased performance. From the experimental results, we can say that while sports categories of the videos are inferred accurately, there is still room for improvement for collective activity recognition, especially regarding the generalization ability beyond previously unseen sports categories. In order to evaluate this ability, we introduce a novel evaluation protocol called unseen sports, where the training and test are carried out on disjoint sets of sports categories. The relatively lower recognition performances in this evaluation protocol indicate that the recognition models tend to be influenced by the surrounding context, rather than focusing on the essence of the collective activities. We believe that C-Sports dataset will stir further interest in this research direction. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				FEB	2020	94								103870	10.1016/j.imavis.2020.103870													
J								A fast and accurate iterative method for the camera pose estimation problem	IMAGE AND VISION COMPUTING										Camera pose estimation; Direct relative orientation; Iterative method; Parameterization; Gross detection and elimination	ALGORITHM; RANSAC; MOTION	This paper presents a fast and accurate iterative method for camera pose estimation problem. The dependence on initial values is reduced by replacing unknown angular parameters with three independent non-angular parameters. Image point coordinates are treated as observations with errors and a new model is built using a conditional adjustment with parameters for relative orientation. This model allows for the estimation of the errors in the observations. The estimated observation errors are then used iteratively to detect and eliminate gross errors in the adjustment. A total of 22 synthetic datasets and 10 real datasets are used to compare the proposed method with the traditional iterative method, the 5-point-RANSAC and the state-of-the-art 5-point-USAC methods. Preliminary results show that our proposed method is not only faster than the other methods, but also more accurate and stable. (C) 2019 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				FEB	2020	94								103860	10.1016/j.imavis.2019.103860													
J								Deep Learning Based Hand Gesture Recognition and UAV Flight Controls	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Deep learning; neural networks; hand gesture recognition; Leap Motion Controllers; drones	HUMAN-COMPUTER INTERACTION; SENSOR; SYSTEM	Dynamic hand gesture recognition is a desired alternative means for human-computer interactions. This paper presents a hand gesture recognition system that is designed for the control of flights of unmanned aerial vehicles (UAV). A data representation model that represents a dynamic gesture sequence by converting the 4-D spatiotemporal data to 2-D matrix and a 1-D array is introduced. To train the system to recognize designed gestures, skeleton data collected from a Leap Motion Controller are converted to two different data models. As many as 9 124 samples of the training dataset, 1 938 samples of the testing dataset are created to train and test the proposed three deep learning neural networks, which are a 2-layer fully connected neural network, a 5-layer fully connected neural network and an 8-layer convolutional neural network. The static testing results show that the 2-layer fully connected neural network achieves an average accuracy of 96.7% on scaled datasets and 12.3% on non-scaled datasets. The 5-layer fully connected neural network achieves an average accuracy of 98.0% on scaled datasets and 89.1% on non-scaled datasets. The 8-layer convolutional neural network achieves an average accuracy of 89.6% on scaled datasets and 96.9% on non-scaled datasets. Testing on a drone-kit simulator and a real drone shows that this system is feasible for drone flight controls.																	1476-8186	1751-8520				FEB	2020	17	1			SI		17	29		10.1007/s11633-019-1194-7													
J								Multi-Loop Model Reference Proportional Integral Derivative Controls: Design and Performance Evaluations	ALGORITHMS										multi-loop model reference control; PID controllers; disturbance rejection control	REFERENCE ADAPTIVE-CONTROL; PID CONTROLLERS; DISTURBANCE REJECTION; IMPLEMENTATION	Due to unpredictable and fluctuating conditions in real-world control system applications, disturbance rejection is a substantial factor in robust control performance. The inherent disturbance rejection capacity of classical closed loop control systems is limited, and an increase in disturbance rejection performance of single-loop control systems affects the set-point control performance. Multi-loop control structures, which involve model reference control loops, can enhance the inherent disturbance rejection capacity of classical control loops without degrading set-point control performance; while the classical closed Proportional Integral Derivative (PID) control loop deals with stability and set-point control, the additional model reference control loop performs disturbance rejection control. This adaptive disturbance rejection, which does not influence set-point control performance, is achieved by selecting reference models as transfer functions of real control systems. This study investigates six types of multi-loop model reference (ML-MR) control structures for PID control loops and presents straightforward design schemes to enhance the disturbance rejection control performance of existing PID control loops. For this purpose, linear and non-linear ML-MR control structures are introduced, and their control performance improvements and certain inherent drawbacks of these structures are discussed. Design examples demonstrate the benefits of the ML-MR control structures for disturbance rejection performance improvement of PID control loops without severely deteriorating their set-point performance.																		1999-4893				FEB	2020	13	2							38	10.3390/a13020038													
J								PD Steering Controller Utilizing the Predicted Position on Track for Autonomous Vehicles Driven on Slippery Roads	ALGORITHMS										autonomous vehicles; automated steering; slippery road conditions; PD controllers; predictive model	DESIGN	Among the most important characteristics of autonomous vehicles are the safety and robustness in various traffic situations and road conditions. In this paper, we focus on the development and analysis of the extended version of the canonical proportional-derivative PD controllers that are known to provide a good quality of steering on non-slippery (dry) roads. However, on slippery roads, due to the poor yaw controllability of the vehicle (suffering from understeering and oversteering), the quality of control of such controllers deteriorates. The proposed predicted PD controller (PPD controller) overcomes the main drawback of PD controllers, namely, the reactiveness of their steering behavior. The latter implies that steering output is a direct result of the currently perceived lateral- and angular deviation of the vehicle from its intended, ideal trajectory, which is the center of the lane. This reactiveness, combined with the tardiness of the yaw control of the vehicle on slippery roads, results in a significant lag in the control loop that could not be compensated completely by the predictive (derivative) component of these controllers. In our approach, keeping the controller efforts at the same level as in PD controllers by avoiding (i) complex computations and (ii) adding additional variables, the PPD controller shows better quality of steering than that of the evolved (via genetic programming) models.																		1999-4893				FEB	2020	13	2							48	10.3390/a13020048													
J								Constrained Connectivity in Bounded X-Width Multi-Interface Networks	ALGORITHMS										multi-interface networks; constrained connectivity; energy optimization; treewidth; pathwidth; carvingwidth	ALGORITHM; GRAPHS	As technology advances and the spreading of wireless devices grows, the establishment of interconnection networks is becoming crucial. Main activities that involve most of the people concern retrieving and sharing information from everywhere. In heterogeneous networks, devices can communicate by means of multiple interfaces. The choice of the most suitable interfaces to activate (switch-on) at each device results in the establishment of different connections. A connection is established when at its endpoints the devices activate at least one common interface. Each interface is assumed to consume a specific percentage of energy for its activation. This is referred to as the cost of an interface. Due to energy consumption issues, and the fact that most of the devices are battery powered, special effort must be devoted to suitable solutions that prolong the network lifetime. In this paper, we consider the so-called p-Coverage problem where each device can activate at most p of its available interfaces in order to establish all the desired connections of a given network of devices. As the problem has been shown to be NP-hard even for p=2 and unitary costs of the interfaces, algorithmic design activities have focused in particular topologies where the problem is optimally solvable. Following this trend, we first show that the problem is polynomially solvable for graphs (modeling the underlying network) of bounded treewidth by means of the Courcelle's theorem. Then, we provide two optimal polynomial time algorithms to solve the problem in two subclasses of graphs with bounded treewidth that are graphs of bounded pathwidth and graphs of bounded carvingwidth. The two solutions are obtained by means of dynamic programming techniques.																		1999-4893				FEB	2020	13	2							31	10.3390/a13020031													
J								GA-Adaptive Template Matching for Offline Shape Motion Tracking Based on Edge Detection: IAS Estimation from the SURVISHNO 2019 Challenge Video for Machine Diagnostics Purposes	ALGORITHMS										machine vision; machine diagnostics; instantaneous angular speed; SURVISHNO 2019 challenge; video tachometer; motion tracking; edge detection; parametric template modeling; adaptive template matching; genetic algorithm	FAULT-DETECTION; MULTISTAGE GEARBOX; DIESEL-ENGINE; SPEED; ORDER; SIGNAL	The estimation of the Instantaneous Angular Speed (IAS) has in recent years attracted a growing interest in the diagnostics of rotating machines. Measurement of the IAS can be used as a source of information of the machine condition per se, or for performing angular resampling through Computed Order Tracking, a practice which is essential to highlight the machine spectral signature in case of non-stationary operational conditions. In these regards, the SURVISHNO 2019 international conference held at INSA Lyon on 8-10 July 2019 proposed a challenge about the estimation of the instantaneous non-stationary speed of a fan from a video taken by a smartphone, a pocket, low-cost device which can nowadays be found in everyone's pocket. This work originated by the author to produce an offline motion-tracking of the fan (actually, of the head of its locking-screw) and obtaining then a reliable estimate of the IAS. The here proposed algorithm is an update of the established Template Matching (TM) technique (i.e., in the Signal Processing community, a two-dimensional matched filter), which is here integrated into a Genetic Algorithm (GA) search. Using a template reconstructed from a simplified parametric mathematical model of the features of interest (i.e., the known geometry of the edges of the screw head), the GA can be used to adapt the template to match the search image, leading to a hybridization of template-based and feature-based approaches which allows to overcome the well-known issues of the traditional TM related to scaling and rotations of the search image with respect to the template. Furthermore, it is able to resolve the position of the center of the screw head at a resolution that goes beyond the limit of the pixel grid. By repeating the analysis frame after frame and focusing on the angular position of the screw head over time, the proposed algorithm can be used as an effective offline video-tachometer able to estimate the IAS from the video, avoiding the need for expensive high-resolution encoders or tachometers.																		1999-4893				FEB	2020	13	2							33	10.3390/a13020033													
J								New Numerical Treatment for a Family of Two-Dimensional Fractional Fredholm Integro-Differential Equations	ALGORITHMS										haar wavelet method; laplace transform; numerical solutions; two-dimensional fractional integro differential equation	INTEGRAL-EQUATIONS; NONLINEAR FREDHOLM	In this paper, we present a robust algorithm to solve numerically a family of two-dimensional fractional integro differential equations. The Haar wavelet method is upgraded to include in its construction the Laplace transform step. This modification has proven to reduce the accumulative errors that will be obtained in case of using the regular Haar wavelet technique. Different examples are discussed to serve two goals, the methodology and the accuracy of our new approach.																		1999-4893				FEB	2020	13	2							37	10.3390/a13020037													
J								Latency-Bounded Target Set Selection in Signed Networks	ALGORITHMS										social networks; network analysis; diffusion processes; approximation	SPREAD; APPROXIMABILITY	It is well-documented that social networks play a considerable role in information spreading. The dynamic processes governing the diffusion of information have been studied in many fields, including epidemiology, sociology, economics, and computer science. A widely studied problem in the area of viral marketing is the target set selection: in order to market a new product, hoping it will be adopted by a large fraction of individuals in the network, which set of individuals should we "target" (for instance, by offering them free samples of the product)? In this paper, we introduce a diffusion model in which some of the neighbors of a node have a negative influence on that node, namely, they induce the node to reject the feature that is supposed to be spread. We study the target set selection problem within this model, first proving a strong inapproximability result holding also when the diffusion process is required to reach all the nodes in a couple of rounds. Then, we consider a set of restrictions under which the problem is approximable to some extent.																		1999-4893				FEB	2020	13	2							32	10.3390/a13020032													
J								Neural PD Controller for an Unmanned Aerial Vehicle Trained with Extended Kalman Filter	ALGORITHMS										PD controller; multilayer perceptron; extended kalman filter	SLIDING MODE CONTROL; FLIGHT CONTROL DESIGN; NETWORK CONTROL; BACKPROPAGATION; SYSTEMS	Flying robots have gained great interest because of their numerous applications. For this reason, the control of Unmanned Aerial Vehicles (UAVs) is one of the most important challenges in mobile robotics. These kinds of robots are commonly controlled with Proportional-Integral-Derivative (PID) controllers; however, traditional linear controllers have limitations when controlling highly nonlinear and uncertain systems such as UAVs. In this paper, a control scheme for the pose of a quadrotor is presented. The scheme presented has the behavior of a PD controller and it is based on a Multilayer Perceptron trained with an Extended Kalman Filter. The Neural Network is trained online in order to ensure adaptation to changes in the presence of dynamics and uncertainties. The control scheme is tested in real time experiments in order to show its effectiveness.																		1999-4893				FEB	2020	13	2							40	10.3390/a13020040													
J								Adaptive Scatter Search to Solve the Minimum Connected Dominating Set Problem for Efficient Management of Wireless Networks	ALGORITHMS										dominating sets; minimum connected dominating sets; scatter search; local search; wireless network design	OPTIMIZATION ALGORITHM; APPROXIMATION	An efficient routing using a virtual backbone (VB) network is one of the most significant improvements in the wireless sensor network (WSN). One promising method for selecting this subset of network nodes is by finding the minimum connected dominating set (MCDS), where the searching space for finding a route is restricted to nodes in this MCDS. Thus, finding MCDS in a WSN provides a flexible low-cost solution for the problem of event monitoring, particularly in places with limited or dangerous access to humans as is the case for most WSN deployments. In this paper, we proposed an adaptive scatter search (ASS-MCDS) algorithm that finds the near-optimal solution to this problem. The proposed method invokes a composite fitness function that aims to maximize the solution coverness and connectivity and minimize its cardinality. Moreover, the ASS-MCDS methods modified the scatter search framework through new local search and solution update procedures that maintain the search objectives. We tested the performance of our proposed algorithm using different benchmark-test-graph sets available in the literature. Experiments results show that our proposed algorithm gave good results in terms of solution quality.																		1999-4893				FEB	2020	13	2							35	10.3390/a13020035													
J								Designing the Uniform Stochastic Photomatrix Therapeutic Systems	ALGORITHMS										pseudorandom number generator; stochastic sequences; photomatrix therapeutic systems; light-emitting diodes	PHOTOULTRASONIC TREATMENT; LED ARRAYS; ILLUMINATION; OPTIMIZATION; LUMINAIRE; LENS	Photomatrix therapeutic systems (PMTS) are widely used for the tasks of preventive, stimulating and rehabilitation medicine. They consist of low-intensity light-emitting diodes (LEDs) having the quasi-monochromatic irradiation properties. Depending on the LED matrix structures, PMTS are intended to be used for local and large areas of bio-objects. However, in the case of non-uniform irradiation of biological tissues, there is a risk of an inadequate physiological response to this type of exposure. The proposed approach considers a novel technique for designing this type of biomedical technical systems, which use the capabilities of stochastic algorithms for LED switching. As a result, the use of stochastic photomatrix systems based on the technology of uniform twisting generation of random variables significantly expands the possibilities of their medical application.																		1999-4893				FEB	2020	13	2							41	10.3390/a13020041													
J								Modified Migrating Birds Optimization for Energy-Aware Flexible Job Shop Scheduling Problem	ALGORITHMS										flexible job shop; energy-aware scheduling; energy consumption; modified migrating birds optimization	ALGORITHM	In recent decades, workshop scheduling has excessively focused on time-related indicators, while ignoring environmental metrics. With the advent of sustainable manufacturing, the energy-aware scheduling problem has been attracting more and more attention from scholars and researchers. In this study, we investigate an energy-aware flexible job shop scheduling problem to reduce the total energy consumption in the workshop. For the considered problem, the energy consumption model is first built to formulate the energy consumption, such as processing energy consumption, idle energy consumption, setup energy consumption and common energy consumption. Then, a mathematical model is established with the criterion to minimize the total energy consumption. Secondly, a modified migrating birds optimization (MMBO) algorithm is proposed to solve the model. In the proposed MMBO, a population initialization scheme is presented to ensure the initial solutions with a certain quality and diversity. Five neighborhood structures are employed to create neighborhood solutions according to the characteristics of the problem. Furthermore, both a local search method and an aging-based re-initialization mechanism are developed to avoid premature convergence. Finally, the experimental results validate that the proposed algorithm is effective for the problem under study.																		1999-4893				FEB	2020	13	2							44	10.3390/a13020044													
J								Using Biased-Randomized Algorithms for the Multi-Period Product Display Problem with Dynamic Attractiveness	ALGORITHMS										omnichannel retail stores; product display problem; multi-period decisions; dynamic attractiveness; biased-randomized heuristics	VEHICLE-ROUTING PROBLEM; OF-THE-ART; SHELF-SPACE; FLOWSHOP PROBLEM; ASSORTMENT; SEARCH; MANAGEMENT; SYSTEMS	From brick-and-mortar stores to omnichannel retail, the efficient selection of products to be displayed on store tables, advertising brochures, or online front pages has become a critical issue. One possible goal is to maximize the overall 'attractiveness' level of the displayed items, i.e., to enhance the shopping experience of our potential customers as a way to increase sales and revenue. With the goal of maximizing the total attractiveness value for the visiting customers over a multi-period time horizon, this paper studies how to configure an assortment of products to be included in limited display spaces, either physical or online. In order to define a realistic scenario, several constraints are considered for each period and display table: (i) the inclusion of both expensive and non-expensive products on the display tables; (ii) the diversification of product collections; and (iii) the achievement of a minimum profit margin. Moreover, the attractiveness level of each product is assumed to be dynamic, i.e., it is reduced if the product has been displayed in a previous period (loss of novelty) and vice versa. This generates dependencies across periods. Likewise, correlations across items are also considered to account for complementary or substitute products. In the case of brick-and-mortar stores, for instance, solving this rich multi-period product display problem enables them to provide an exciting experience to their customers. As a consequence, an increase in sales revenue should be expected. In order to deal with the underlying optimization problem, which contains a quadratic objective function in its simplest version and a non-smooth one in its complete version, two biased-randomized metaheuristic algorithms are proposed. A set of new instances has been generated to test our approach and compare its performance with that of non-linear solvers.																		1999-4893				FEB	2020	13	2							34	10.3390/a13020034													
J								Approximation Algorithm for Shortest Path in Large Social Networks	ALGORITHMS										parallel programing; complex networks; hierarchical networks; approximation algorithm; shortest path		Proposed algorithms for calculating the shortest paths such as Dijikstra and Flowd-Warshall's algorithms are limited to small networks due to computational complexity and cost. We propose an efficient and a more accurate approximation algorithm that is applicable to large scale networks. Our algorithm iteratively constructs levels of hierarchical networks by a node condensing procedure to construct hierarchical graphs until threshold. The shortest paths between nodes in the original network are approximated by considering their corresponding shortest paths in the highest hierarchy. Experiments on real life data show that our algorithm records high efficiency and accuracy compared with other algorithms.																		1999-4893				FEB	2020	13	2							36	10.3390/a13020036													
J								FADIT: Fast Document Image Thresholding	ALGORITHMS										thresholding; document image analysis; image segmentation; Otsu	BINARIZATION; PERFORMANCE	We propose a fast document image thresholding method (FADIT) and evaluations of the two classic methods for demonstrating the effectiveness of FADIT. We put forward two assumptions: (1) the probability of the occurrence of grayscale text and background is ideally two constants, and (2) a pixel with a low grayscale has a high probability of being classified as text and a pixel with a high grayscale has a high probability of being classified as background. With the two assumptions, a new criterion function is applied to document image thresholding in the Bayesian framework. The effectiveness of the method has been borne of a quantitative metric as well as qualitative comparisons with the state-of-the-art methods.																		1999-4893				FEB	2020	13	2							46	10.3390/a13020046													
J								Transfer Learning: Video Prediction and Spatiotemporal Urban Traffic Forecasting dagger	ALGORITHMS										urban traffic flows; spatiotemporal models; data-driven; graph convolutional neural networks; spatial filtering; network-wide forecasts	FLOW PREDICTION; NETWORK; MULTIVARIATE; MODELS	Transfer learning is a modern concept that focuses on the application of ideas, models, and algorithms, developed in one applied area, for solving a similar problem in another area. In this paper, we identify links between methodologies in two fields: video prediction and spatiotemporal traffic forecasting. The similarities of the video stream and citywide traffic data structures are discovered and analogues between historical development and modern states of the methodologies are presented and discussed. The idea of transferring video prediction models to the urban traffic forecasting domain is validated using a large real-world traffic data set. The list of transferred techniques includes spatial filtering by predefined kernels in combination with time series models and spectral graph convolutional artificial neural networks. The obtained models' forecasting performance is compared to the baseline traffic forecasting models: non-spatial time series models and spatially regularized vector autoregression models. We conclude that the application of video prediction models and algorithms for urban traffic forecasting is effective both in terms of observed forecasting accuracy and development, and training efforts. Finally, we discuss problems and obstacles of transferring methodologies and present potential directions for further research.																		1999-4893				FEB	2020	13	2							39	10.3390/a13020039													
J								Accelerating Binary String Comparisons with a Scalable, Streaming-Based System Architecture Based on FPGAs	ALGORITHMS										FPGA; bioinformatics; multiple string matching; parallel computing; Hamming distance; locality sensitive hashing; hardware acceleration; parallel system architecture	HAMMING WEIGHT; SEARCH; COMPARATORS; VECTORS	This paper is concerned with Field Programmable Gate Arrays (FPGA)-based systems for energy-efficient high-throughput string comparison. Modern applications which involve comparisons across large data sets-such as large sequence sets in molecular biology-are by their nature computationally intensive. In this work, we present a scalable FPGA-based system architecture to accelerate the comparison of binary strings. The current architecture supports arbitrary lengths in the range 16 to 2048-bit, covering a wide range of possible applications. In our example application, we consider DNA sequences embedded in a binary vector space through Locality Sensitive Hashing (LSH) one of several possible encodings that enable us to avoid more costly character-based operations. Here the resulting encoding is a 512-bit binary signature with comparisons based on the Hamming distance. In this approach, most of the load arises from the calculation of the O(m*n) Hamming distances between the signatures, where m is the number of queries and n is the number of signatures contained in the database. Signature generation only needs to be performed once, and we do not consider it further, focusing instead on accelerating the signature comparisons. The proposed FPGA-based architecture is optimized for high-throughput using hundreds of computing elements, arranged in a systolic array. These core computing elements can be adapted to support other string comparison algorithms with little effort, while the other infrastructure stays the same. On a Xilinx Virtex UltraScale+ FPGA (XCVU9P-2), a peak throughput of 75.4 billion comparisons per second-of 512-bit signatures-was achieved, using a design with 384 parallel processing elements and a clock frequency of 200 MHz. This makes our FPGA design 86 times faster than a highly optimized CPU implementation. Compared to a GPU design, executed on an NVIDIA GTX1060, it performs nearly five times faster.																		1999-4893				FEB	2020	13	2							47	10.3390/a13020047													
J								Optimal Model for Carsharing Station Location Based on Multi-Factor Constraints	ALGORITHMS										carsharing; station location modeling; genetic algorithm; fix stations and free stations	GENETIC ALGORITHM APPROACH; SYSTEMS	The development of the sharing economy has made carsharing the main future development model of car rental. Carsharing network investment is enormous, but the resource allocation is limited. Therefore, the reasonable location of the carsharing station is important to the development of carsharing companies. On the basis of the current status of carsharing development, this research considers multiple influencing factors of carsharing to meet the maximum user demand. Meanwhile, the constraint of the limited cost of the company is considered to establish a nonlinear integer programming model for station location of carsharing. A genetic algorithm is designed to solve the problem by analyzing the location model of the carsharing network. Finally, the results of a case study of Lanzhou, China show the effectiveness of the establishment and solution of the station location model.																		1999-4893				FEB	2020	13	2							43	10.3390/a13020043													
J								Learning Manifolds from Dynamic Process Data dagger	ALGORITHMS										manifold learning; time series; dynamic processes	REDUCTION; TRANSISTORS; CELLS	Scientific data, generated by computational models or from experiments, are typically results of nonlinear interactions among several latent processes. Such datasets are typically high-dimensional and exhibit strong temporal correlations. Better understanding of the underlying processes requires mapping such data to a low-dimensional manifold where the dynamics of the latent processes are evident. While nonlinear spectral dimensionality reduction methods, e.g., Isomap, and their scalable variants, are conceptually fit candidates for obtaining such a mapping, the presence of the strong temporal correlation in the data can significantly impact these methods. In this paper, we first show why such methods fail when dealing with dynamic process data. A novel method, Entropy-Isomap, is proposed to handle this shortcoming. We demonstrate the effectiveness of the proposed method in the context of understanding the fabrication process of organic materials. The resulting low-dimensional representation correctly characterizes the process control variables and allows for informative visualization of the material morphology evolution.																		1999-4893				FEB	2020	13	2							30	10.3390/a13020030													
J								Lower and Upper Bounds for the Discrete Bi-Directional Preemptive Conversion Problem with a Constant Price Interval	ALGORITHMS										conversion problem; two-way trading; competitive analysis; error balancing	OPTIMAL-ALGORITHMS; ONLINE ALGORITHMS; K-SEARCH	In the conversion problem, wealth has to be distributed between two assets with the objective to maximize the wealth at the end of the investment horizon. The bi-directional preemptive conversion problem with a constant price interval is the only problem, of the four main variants of the conversion problem, that has not yet been optimally solved by competitive analysis. Assuming a given number of monotonous price trends called runs, lower and upper bounds for the competitive ratio are given. In this work, the assumption of a given number of runs is rejected and lower and upper bounds for the bi-directional preemptive conversion problem with a constant price interval are given. Furthermore, an algorithm based on error balancing is given which at minimum achieves the given upper bound. It can also be shown that this algorithm is optimal for the single-period model.																		1999-4893				FEB	2020	13	2							42	10.3390/a13020042													
J								Optimization of Constrained Stochastic Linear-Quadratic Control on an Infinite Horizon: A Direct-Comparison Based Approach	ALGORITHMS										linear-quadratic; Markov decision process(MDP); conic constraints; stochastic control; direct-comparison based approach	FEEDBACK; SYSTEMS; STATE	In this paper we study the optimization of the discrete-time stochastic linear-quadratic (LQ) control problem with conic control constraints on an infinite horizon, considering multiplicative noises. Stochastic control systems can be formulated as Markov Decision Problems (MDPs) with continuous state spaces and therefore we can apply the direct-comparison based optimization approach to solve the problem. We first derive the performance difference formula for the LQ problem by utilizing the state separation property of the system structure. Based on this, we successfully derive the optimality conditions and the stationary optimal feedback control. By introducing the optimization, we establish a general framework for infinite horizon stochastic control problems. The direct-comparison based approach is applicable to both linear and nonlinear systems. Our work provides a new perspective in LQ control problems; based on this approach, learning based algorithms can be developed without identifying all of the system parameters.																		1999-4893				FEB	2020	13	2							49	10.3390/a13020049													
J								Adaptive Tolerance Dehazing Algorithm Based on Dark Channel Prior	ALGORITHMS										haze removal; adaptive tolerance mechanism; dark channel prior; transmission	IMAGE; CONTRAST; ENHANCEMENT; FILTER	The tolerance mechanism based on dark channel prior (DCP) of a single image dehazing algorithm is less effective when there are large areas of the bright region in the hazy image because it cannot obtain the tolerance adaptively according to the characteristics of the image. It will lead to insufficient improvement of the transmission of image, so it is difficult to eliminate the color distortion and block effects in the restored image completely. Moreover, when a dense haze area or a third-party direct light source (sunlight, headlights and reflected glare) is misjudged as sky area, the use of tolerance will cause an inferior dehazing effect such as details lost. Regarding the issue above, this paper proposes an adaptive tolerance estimation algorithm. The tolerance is obtained according to the statistic characteristics of each image to make the estimation of transmission more accurately. The experimental results show that the proposed algorithm not only maintains high operational efficiency but also effectively compensates for the defects of the dark channel prior to some scenes. The proposed algorithm can effectively solve the problem of color distortion recovered by the DCP method in the bright regions of the image.																		1999-4893				FEB	2020	13	2							45	10.3390/a13020045													
J								Fuzzy-based approach to assess and prioritize privacy risks	SOFT COMPUTING										Privacy risks; Privacy risk assessment; Fuzzy set theory		The newgeneral data protection regulation requires organizations to conduct a data protection impact assessment (DPIA) when the processing of personal information may result in high risk to individual rights and freedoms. DPIA allows organizations to identify, assess and prioritize the risks related to the processing of personal information and select suitable mitigations to reduce the severity of the risks. The existing DPIA methodologies measure the severity of privacy risks according to analysts' opinions about the likelihood and the impact factors of the threats. The assessment is therefore subjective to the expertise of the analysts. To reduce subjectivity, we propose a set of well-defined criteria that analysts can use to measure the likelihood and the impact of a privacy risk. Then, we adopt the fuzzy multi-criteria decision-making approach to systematically measure the severity of privacy risks while modeling the imprecision and vagueness inherent in linguistic assessment. Our approach is illustrated for a realistic scenario with respect to LINDDUN threat categories.																	1432-7643	1433-7479				FEB	2020	24	3					1553	1563		10.1007/s00500-019-03986-5													
J								Feature selection strategy based on hybrid crow search optimization algorithm integrated with chaos theory and fuzzy c-means algorithm for medical diagnosis problems	SOFT COMPUTING										Feature selection; Crow search optimization; Chaos theory; Fuzzy c-means; Medical diagnosis	UNCERTAINTY; MODELS	Powerful knowledge acquisition tools and techniques have the ability to increase both the quality and the quantity of knowledge-based systems for real-world problems. In this paper, we designed a hybrid crow search optimization algorithm integrated with chaos theory and fuzzy c-means algorithm denoted as CFCSA for feature selection problems of medical diagnosis. In the proposed CFCSA framework, the crow search algorithm adopts the global optimization technique to avoid the sensitivity of local optimization. The fuzzy c-means (FCM) objective function is used as a cost function for the chaotic crow search optimization algorithm. The proposed algorithm CFCSA is benchmarked against the binary crow search algorithm (BCSA), chaotic ant lion optimization algorithm (CALO), binary ant lion optimization algorithm (BALO) and bat algorithm relevant methods. The proposed CFCSA algorithm vs. BCSA, CALO, BALO and bat algorithm is tested on diabetes, heart, Radiopaedia CT liver, breast cancer, lung cancer, cardiotocography, ILPD, liver disorders, hepatitis and arrhythmia. Experimental results show the proposed method CFCSA is better against comparative models in feature selection on the medical diagnosis data sets.																	1432-7643	1433-7479				FEB	2020	24	3					1565	1584		10.1007/s00500-019-03988-3													
J								Pythagorean uncertain linguistic hesitant fuzzy weighted averaging operator and its application in financial group decision making	SOFT COMPUTING										Pythagorean uncertain linguistic hesitant fuzzy numbers; Aggregation operators; Group decision making	AGGREGATION OPERATORS	With respect to multiple attribute decision-making problems, in which attribute values take in the form of Pythagorean uncertain linguistic hesitant fuzzy information, a new decision-making method based on the Pythagorean uncertain linguistic hesitant fuzzy weighted averaging (PULHFWA) operator is developed. In this paper, we proposed some operational laws based on Pythagorean uncertain linguistic hesitant fuzzy numbers (PULHFNs) and verified some properties. We also developed some aggregation operators to use the decision information represented by PULHFNs, including the PULHFWA operator, Pythagorean uncertain linguistic hesitant fuzzy ordered weighted averaging operator and Pythagorean uncertain linguistic hesitant fuzzy hybrid averaging operator. We develop a decision-making method based on the proposed operators under the Pythagorean uncertain linguistic hesitant fuzzy environment and illustrated with a numerical example and study the applicability of the new approach on a financial decision-making problem concerning the selection of financial strategies. Finally, a comparison analysis between the proposed and the existing approaches has been performed to illustrate the applicability and feasibility of the developed decision-making method.																	1432-7643	1433-7479				FEB	2020	24	3					1585	1597		10.1007/s00500-019-03989-2													
J								Semi-supervised orthogonal discriminant analysis with relative distance : integration with a MOO approach	SOFT COMPUTING										Semi-supervised learning; Dimension reduction; Data clustering; Bregman projection; Validity indices; Trace ratio	NONDOMINATED SORTING APPROACH; FRAMEWORK; ALGORITHM	In discriminant analysis, trace ratio is an important criterion for minimizing the between-class similarity and maximizing the within-class similarity, simultaneously. In brief, we address the trace ratio problem associated with many semi-supervised discriminant analysis algorithms as they use the normal Euclidean distances between training data samples. Based on this problem, we propose a new semi-supervised orthogonal discriminant analysis technique with relative distance constraints called SSODARD. Different from the existing semi-supervised dimensionality reduction algorithms, our algorithm is more consistent in propagating the label information from the labeled data to the unlabeled data because of the use of relative distance function instead of normal Euclidean distance function. For finding this appropriate relative distance function, we use pairwise constraints generated from labeled data and satisfy them using Bregman projection. Since the projection is not orthogonal, we require an appropriate subset of constraints. In order to select such a subset of constraints, we further develop a framework called MO-SSODARD, which uses evolutionary algorithm while optimizing various validity indices simultaneously. The experimental results on various datasets show that our proposed approaches are superior than the state-of-the-art discriminant algorithms with respect to various validity indices.																	1432-7643	1433-7479				FEB	2020	24	3					1599	1618		10.1007/s00500-019-03990-9													
J								Cuckoo search algorithm-based brightness preserving histogram scheme for low-contrast image enhancement	SOFT COMPUTING										Cuckoo search algorithm; Histogram equalization; Low-contrast satellite images; Optimized Plateau limit and brightness preservation	KNEE TRANSFER-FUNCTION; SEGMENTATION; EQUALIZATION; EVOLUTIONARY; KAPURS	This paper introduces a novel optimized brightness preserving histogram equalization approach to preserve the mean brightness and to improve the contrast of low-contrast image using cuckoo search algorithm. Traditional histogram equalization scheme induces extreme enhancement and brightness change ensuing abnormal appearance. The proposed method utilizes plateau limits to modify histogram of the image. In this method, histogram is divided into two sub-histograms on which histogram statistics are exploited to obtain the plateau limits. The sub-histograms are equalized and modified based on the calculated plateau limits obtained by cuckoo search optimization technique. To demonstrate the effectiveness of proposed method a comparison of the proposed method with different histogram processing techniques is presented. Proposed method outperforms other state-of-art methods in terms of the objective as well as subjective quality evaluation.																	1432-7643	1433-7479				FEB	2020	24	3					1619	1645		10.1007/s00500-019-03992-7													
J								Correlation coefficients for T-spherical fuzzy sets and their applications in clustering and multi-attribute decision making	SOFT COMPUTING										Picture fuzzy sets; Spherical fuzzy sets; T-spherical fuzzy sets; Correlation coefficients; Clustering Multi-attribute decision making	SIMILARITY MEASURES	The framework of T-spherical fuzzy set is a generalization of fuzzy set, intuitionistic fuzzy set and picture fuzzy set having a great potential of dealing with uncertain events with no limitation. A T-spherical fuzzy framework can deal with phenomena of more than yes or no type; for example, consider the scenario of voting where one's voting interest is not limited to "in favor'' or "against'' rather there could be some sort of abstinence or refusal degree also. The objective of this paper is to develop some correlation coefficients for T-spherical fuzzy sets due to the non-applicability of correlations of intuitionistic fuzzy sets and picture fuzzy sets in some certain circumstances. The fitness of new correlation coefficients has been discussed, and their generalization is studied with the help of some results. Clustering and multi-attribute decision-making algorithms have been proposed in the environment of T-spherical fuzzy sets. To demonstrate the viability of proposed algorithms and correlation coefficients, two real-life problems including a clustering problem and a multi-attribute decision-making problem have been solved. A comparative study of the newly presented and pre-existing literature is established showing the superiority of proposed work over the existing theory. Some advantages of new correlation coefficients and drawbacks of the pre-existing work are demonstrated with the help of numerical examples.																	1432-7643	1433-7479				FEB	2020	24	3					1647	1659		10.1007/s00500-019-03993-6													
J								An autoencoder-based spectral clustering algorithm	SOFT COMPUTING										Spectral clustering; Stacked autoencoder; Sparse representation; KL divergence	NORMALIZED CUTS; NETWORK	Spectral clustering algorithm suffers from high computational complexity due to the eigen decomposition of Laplacian matrix and large similarity matrix for large-scale datasets. Some researches explore the possibility of deep learning in spectral clustering and propose to replace the eigen decomposition with autoencoder. K-means clustering is generally used to obtain clustering results on the embedding representation, which can improve efficiency but further increase memory consumption. An efficient spectral algorithm based on stacked autoencoder is proposed to solve this issue. In this paper, we select the representative data points as landmarks and use the similarity of landmarks with all data points as the input of autoencoder instead of similarity matrix of the whole datasets. To further refine clustering result, we combine learning the embedding representation and performing clustering. Clustering loss is used to update the parameters of autoencoder and cluster centers simultaneously. The reconstruction loss is also included to prevent the distortion of embedding space and preserve the local structure of data. Experiments on several large-scale datasets validate the effectiveness of the proposed method.																	1432-7643	1433-7479				FEB	2020	24	3					1661	1671		10.1007/s00500-019-03994-5													
J								Analysis of fuzzy supply chain performance based on different buyback contract configurations	SOFT COMPUTING										Supply chain coordination; Buyback contract; Fuzzy set; Credibility theory	EXPECTED VALUE; COORDINATION	In this study, a two-echelon supply chain is analyzed where the supplier sells the products to retailer, who in turn sells the product to end customers. In such an arrangement, the supplier and the retailer aim to increase their profits individually which causes double marginalization. Several studies have been proposed by researches to solve the problem of "double marginalization'' and its consequences to supply chain performance. Therefore, contractual agreements as coordination mechanisms were developed to improve the supply chain performance. In the literature, many studies have been conducted on these coordination mechanisms under probabilistic demand. However, in the absence of the historical data it is not possible to establish the probability distribution. In such cases, the fuzzy set theory which is another illustration of uncertainty can be used to model the supply chain. In this study, different configurations of buyback contracts on supply chain performance under fuzzy environment are analyzed. Initially, closed-form solution to buyback contract model with fuzzy demand is proposed by using credibility theory. After that, the closed form of this model with fuzzy buyback rate parameter is obtained. And then the effects of the different configurations of the buyback contract model are analyzed by changing the buyback rate and buyback price. Finally, numerical examples are presented to demonstrate the solving processes of the models and the different effects of buyback rate and buyback price on parameters of buyback contract and the fuzzy expected profit value of all members in supply chain.																	1432-7643	1433-7479				FEB	2020	24	3					1673	1682		10.1007/s00500-019-03996-3													
J								A novel fuzzy mechanism for risk assessment in software projects	SOFT COMPUTING										Project risk; Performance; Fuzzy DEMATEL; Adaptive neuro-fuzzy inference system; IF-TODIM; MCDM; Crow search algorithm	MULTICRITERIA DECISION-MAKING; TODIM METHOD; OPTIMIZATION; TOPSIS; ALGORITHM; SYSTEMS; DEMATEL; MODEL; FMCDM	Risk management is a vital factor for ensuring better quality software development processes. Moreover, risks are the events that could adversely affect the organization activities or the development of projects. Effective prioritization of software project risks play a significant role in determining whether the project will be successful in terms of performance characteristics or not. In this work, we develop a new hybrid fuzzy-based machine learning mechanism for performing risk assessment in software projects. This newly developed hybridized risk assessment scheme can be used to determine and rank the significant software project risks that support the decision making during the software project lifecycle. For better assessment of the software project risks, we have incorporated fuzzy decision making trial and evaluation laboratory, adaptive neuro-fuzzy inference system-based multi-criteria decision making (ANFIS MCDM) and intuitionistic fuzzy-based TODIM (IF-TODIM) approaches. More significantly, for the newly introduced ANFIS MCDM approach, the parameters of ANFIS are adjusted using a traditional crow search algorithm (CSA) which applies only a reasonable as well as small changes in variables. The main activity of CSA in ANFIS is to find the best parameter to achieve most accurate software risk estimate. Experimental validation was conducted on NASA 93 dataset having 93 software project values. The result of this method exhibits a vivid picture that provides software risk factors that are key determinant for achievement of the project performance. Experimental outcomes reveal that our proposed integrated fuzzy approaches can exhibit better and accurate performance in the assessment of software project risks compared to other existing approaches.																	1432-7643	1433-7479				FEB	2020	24	3					1683	1705		10.1007/s00500-019-03997-2													
J								Integrating social annotations into topic models for personalized document retrieval	SOFT COMPUTING										Social annotations; Document reconstruction; Topic models; Document retrieval	SEARCH	Social annotations are valuable resources generated by users on the Web, which encode abundant information on user preferences for certain documents. Social annotation-based information retrieval has been studied in recent years for personalizing search results and fulfilling user information needs. However, since social annotations are complicated and associated with users, documents and tags simultaneously, it remains a great challenge to fully capture the potentially useful information for improving retrieval performance. To meet the challenge, we propose a novel method to integrate social annotations into topic models for personalized document retrieval. Our method first reconstructs candidate documents for a given query using social tags of documents to capture user preferences. The reconstructed documents are tailored to user preferences for achieving better performance. We then generalize the latent Dirichlet allocation-based topic models by considering the relationship among users, social tags and documents from social annotations. The modified topic model optimizes the distribution of latent topics of documents for different users to meet user information needs. Experimental results show that our method can significantly outperform the state-of-the-art baseline models for improving the performance of personalized retrieval.																	1432-7643	1433-7479				FEB	2020	24	3					1707	1716		10.1007/s00500-019-03998-1													
J								A modeling error-based adaptive fuzzy observer approach with input saturation analysis for robust control of affine and non-affine systems	SOFT COMPUTING										Modeling error; Adaptive fuzzy observer; Affine and non-affine systems; Estimation of uncertainties; Input saturation	OUTPUT-FEEDBACK CONTROL; MIMO NONLINEAR-SYSTEMS; BACKSTEPPING CONTROL; TRACKING CONTROL; NEURAL-CONTROL; DESIGN	In this paper, a robust control approach is applied for both MIMO/SISO affine/non-affine nonlinear systems based on a modeling error-based adaptive fuzzy observer controller, in the presence of input saturation. In the proposed scheme, non-affine nonlinear systems can be transformed to affine systems and unknown higher-order term of expansion (HOTE) that appears due to the use of this method can be estimated by an adaptive fuzzy technique. Using the modeling error between the system states observer and a serial-parallel estimator model, a modeling error-based adaptive fuzzy observer estimator is proposed that uses the modeling error as the input of fuzzy system to approximate and adaptively compensate the unknown HOTE and also the external disturbance. The proposed scheme is able to hold control performance in the presence of input saturation. An analysis of the controlled system is presented to verify the stability of the system under control. The stability of the closed-loop system is provided based on the strictly positive real condition and Lyapunov theory. The proposed approach is effectual and robust. The simulation results demonstrate the usefulness of the proposed method for both MIMO and SISO systems.																	1432-7643	1433-7479				FEB	2020	24	3					1717	1735		10.1007/s00500-019-03999-0													
J								Clustering-based heterogeneous optimized-HEED protocols for WSNs	SOFT COMPUTING										Clustering; WSNs; Stability region; Network lifetime; Load balancing; HEED; Optimized-HEED; BFOA; Fuzzy logic system	WIRELESS SENSOR NETWORKS; ENERGY-EFFICIENT; ALGORITHM; HYBRID	Clustering-based networks play a vital role in efficient utilization of energy consumption of each sensor node (SN) in wireless sensor networks (WSNs). Furthermore, firstly, prolonged network's lifetime is observed as the key factor to analyze the protocol's efficiency. However, in critical applications, i.e., military surveillance, environmental monitoring and structural health monitoring, stability region is also an important aspect for consideration. This provides reliability of data from each SN in the network. On the other hand, once a SN dies at any region, we are not able to sense that region which leaves the region vulnerable from detection of events. With this reason, it is highly important for an energy efficient protocol to provide good stability region with prolonged network lifetime. Secondly, a protocol should be intelligent enough to handle homogeneous as well as heterogeneous nodes efficiently in the network (i.e., homogeneous and heterogeneous WSNs) because once the network executes, a homogeneous WSN is also transformed in heterogeneous WSN. This is because of different radio communication features, occurrence of random events or morphological attributes of the network field. optimized-HEED protocols are one of the most recent clustering-based algorithms which improved the various shortcomings of classical protocol, i.e., HEED and provided far efficient results in terms of energy consumption, load balancing and network lifetime. However, these demonstrated their efficiency for homogeneous WSN only. In this paper, we extend the optimized-HEED protocols for heterogeneous WSNs model on the basis of varying levels of node heterogeneity (in terms of energy), i.e., 1-level, 2-level, 3-level and multi-level, and propose these as heterogeneous optimized-HEED (Hetero-OHEED) protocols. Simulation results confirm that by increasing the level of node's heterogeneity, stability region of each Hetero-OHEED protocol enhances extremely with prolonged network lifetime. These provide a rich solution in designing of efficient protocols for those applications, where stability region and network lifetime require equal importance.																	1432-7643	1433-7479				FEB	2020	24	3					1737	1761		10.1007/s00500-019-04000-8													
J								Supervised Kohonen network with heterogeneous value difference metric for both numeric and categorical inputs	SOFT COMPUTING										Supervised Kohonen networks; Classification; Heterogeneous value difference metric; Hybrid update rules; Numeric and categorical data	CLASSIFIER; ALGORITHM; EXTENSION; MODEL	The multi-attribute information appears in real world, which also includes numeric and categorical attributes. However, the previous classification algorithms for both numeric and categorical data exist in some limitations on categorical data. In this paper, a supervised Kohonen network with heterogeneous value difference metric is proposed for both numeric and categorical inputs. It employs the framework of supervised Kohonen networks, adopts heterogeneous value difference metric to measure dissimilarity between numeric and categorical data, uses the frequency of each categorical item in the Voronoi set to update the reference vector of categorical attribute on the competitive layer, and updates different competitive learning rules for numeric and categorical data. The effectiveness of the proposed algorithm is verified by UCI Machine Learning Data Repository. The classification accuracy is compared with BP, k-NN, naive Bayes network, C4.5 and SVM; the dissimilarity metric is analyzed. The proposed classification algorithm is applied to the operating mode classification for wind turbines; the effectiveness is illustrated in condition monitoring for pitch system of wind turbines.																	1432-7643	1433-7479				FEB	2020	24	3					1763	1774		10.1007/s00500-019-04001-7													
J								Products and services valuation through unsolicited information from social media	SOFT COMPUTING										Preference relations; Interval data; Majority operators; Decision making; Opinion mining; ORIs; ORS	GROUP DECISION-MAKING; MODEL; CLASSIFICATION; ANALYTICS; OPERATOR	Technological advances and the Internet have changed the way consumers approach the market for assets and services. Increasingly, consumers use the opinions of others to make their decisions. Web sites have assessment indexes (ORIs, ORS), where consumers value products/services using discrete-value scales, like stars or likes. But these ways of assessing are being questioned for several reasons such as the answers' reliability, and the opinions representativeness and aggregation. Lack of reliability is since opinions are requested directly, which makes possible the paradox of the strategic aspect of decisions. Representativeness and aggregation problems are owing to an important loss of decisions information when using a single value, instead of using the interval that includes all feelings/opinions, as well as preferences cardinality in the aggregation that somehow reinforce those assessments. The aim of this work is to propose an index for products/services evaluation over the Internet. This index, called Quorum Valuation Opinion Reputation Index, takes unsolicited information from consumers, and through a semantic analysis and a majority aggregation process, builds a valuation interval. In addition, an opinion interval reliability index is proposed. The index has been tested with real data in the tourism field.																	1432-7643	1433-7479				FEB	2020	24	3					1775	1788		10.1007/s00500-019-04005-3													
J								Jump detection in financial time series usingmachine learning algorithms	SOFT COMPUTING										Recurrent neural network; Anomaly detection; Machine learning; Long short-term memory (LSTM) neural network	OUTLIERS; MARKETS	In this paper, we develop a new Hybrid method based on machine learning algorithms for jump detection in financial time series. Jump is an important behavior in financial time series, since it implies a change in volatility. Ones can buy the volatility instrument if ones expect the volatility will bloom up in the future. A jump detection model attempts to detect short-term market instability, since it could be jumping up or down, instead of a directional prediction. The directional prediction can be considered as a momentum or trend following, which is not the focus of this paper. A jump detection model is commonly applied in a systematic fast-moving strategy, which reallocates the assets automatically. Also, a systematic opening position protection strategy can be driven by a jump detection model. For example, for a tail risk protection strategy, a pair of long call and put option order could be placed in the same time, in order to protect the open position given a huge change in volatility. One of the key differentiations of the proposed model with the classical methods of time-series anomaly detection is that, jump threshold parameters are not required to be predefined in our proposed model. Also the model is a combination of a Long short-term memory (LSTM) neural network model and a machine learning pattern recognition model. The LSTM model is applied for time series prediction, which predicts the next data point. The historical prediction errors sequence can be used as the information source or input of the jump detection model/module. The machine learning pattern recognition model is applied for jump detection. The combined model attempts to determine whether the current data point is a jump or not. LSTM neural network is a type of Recurrent Neural Networks (RNNs). LSTM records not only the recent market, but also the historical status. A stacked RNN is trained on a dataset which is mixed with normal and anomalous data. We compare the performance of the proposed Hybrid jump detection model and different pattern classification algorithms, such as k-nearest neighbors algorithm identifier, Hampel identifier, and Lee Mykland test. The model is trained and tested using real financial market data, including 11 global stock market in both developed and emerging markets in US, China, Hong Kong, Taiwan, Japan, UK, German, and Israel. The experiment result shows that the proposed Hybrid jump detection model is effective to detect jumps in terms of accuracy, comparing to the other classical jump detection methods.																	1432-7643	1433-7479				FEB	2020	24	3					1789	1801		10.1007/s00500-019-04006-2													
J								Valuation of stock loan under uncertain stock model with floating interest rate	SOFT COMPUTING										Uncertainty theory; Uncertain differential equation; Uncertain stock model; Stock loan	OPTION	Stock loan is a special loan with stocks as collateral, which offers the borrower the right to redeem the stocks at any time prior to the loan maturity by repaying the bank the principal and the loan interest. In this paper, we investigate the valuation of stock loan under an uncertain stock model with floating interest rate. The pricing formulas of standard stock loan and capped stock loan for the stock model are derived by using the method of uncertain calculus. Subsequently, some numerical algorithms are designed to calculate the prices of stock loans based on the pricing formulas. Finally, some numerical experiments are presented to study the relationship between stock loan price and some parameters.																	1432-7643	1433-7479				FEB	2020	24	3					1803	1814		10.1007/s00500-019-04007-1													
J								Fast curvelet transform through genetic algorithm for multimodal medical image fusion	SOFT COMPUTING										Ridgelet transform; Curvelet transform; Genetic algorithm; Mutual information; Image fusion	CONTOURLET TRANSFORM; ARTIFACT CORRECTION; WAVELET; EXPRESSION; FRAMEWORK; SCHEME; MODEL; FMRI; CT; MR	Currently, medical imaging modalities produce different types of medical images to help doctors to diagnose illnesses or injuries. Each modality of images has its specific intensity. Many researchers in medical imaging have attempted to combine redundancy and related information from multiple types of medical images to produce fused medical images that can provide additional concentration and image diagnosis inspired by the information for the medical examination. We propose a new method and method of fusion for multimodal medical images based on the curvelet transform and the genetic algorithm (GA). The application of GA in our method can solve the suspicions and diffuse existing in the input image and can further optimize the characteristics of image fusion. The proposed method has been tested in many sets of medical images and is also compared to recent medical image fusion techniques. The results of our quantitative evaluation and visual analysis indicate that our proposed method produces the best advantage of medical fusion images over other methods, by maintaining perfect data information and color compliance at the base image.																	1432-7643	1433-7479				FEB	2020	24	3					1815	1836		10.1007/s00500-019-04011-5													
J								n-ary Cartesian composition of automata	SOFT COMPUTING										Cartesian composition of automata; Complete hypergroup; EL-semihypergroup; Hypergroup; Theory of automata	FUZZY GRADE; HYPERGROUPS; GRAPHS	In our paper, we construct Cartesian composition of automata in a way rather different from the classical approach. In our case, the resulting structure is not an automaton but a quasi-multiautomaton, i.e., a structure whose input alphabet is a semi-hypergroup instead of a set or a free monoid. In our reasoning, we make use of earlier results on complete (semi)hypergroups and show that this approach not only simplifies our construction but also yields some natural applications.																	1432-7643	1433-7479				FEB	2020	24	3					1837	1849		10.1007/s00500-019-04015-1													
J								Classification of gait patterns in patients with unilateral anterior cruciate ligament deficiency based on phase space reconstruction, Euclidean distance and neural networks	SOFT COMPUTING										Anterior cruciate ligament (ACL); Nonlinear gait dynamics; Phase space reconstruction (PSR); Euclidean distance (ED); Neural networks	LARGEST LYAPUNOV EXPONENT; WALKING SPEED; KNEE OSTEOARTHRITIS; ACL DEFICIENCY; VARIABILITY; KINEMATICS; STABILITY; INJURY; MODEL; INDIVIDUALS	The anterior cruciate ligament (ACL) is one of the most important structures of the knee joint which plays a significant role in controlling knee joint stability. Patients with unilateral ACL deficiency often show alterations of their gait patterns in the deficient side in comparison with the unaffected contralateral side. Gait analysis is widely used to detect biomechanical changes in the lower limbs, aiming at diagnosing ACL injury, establishing physical therapy treatments or surgery, monitoring the progression of ACL deficiency over time. This paper proposes new combined methods to classify gait patterns between ACL deficient (ACL-D) knee and contralateral ACL-intact (ACL-I) knee in patients with unilateral ACL deficiency by using phase space reconstruction (PSR), Euclidean distance (ED) and neural networks. First knee, hip and ankle kinematic parameters are extracted and phase space has been reconstructed. The properties associated with the gait system dynamics are preserved in the reconstructed phase space. For the purpose of classification of ACL-D and ACL-I knee gait patterns, three-dimensional (3D) PSR together with EDs has been used. These measured parameters show significant difference in gait dynamics between the two groups and have been utilized to form the feature set. Neural networks are then used as the classifier to distinguish between ACL-D and ACL-I knee gait patterns based on the difference of gait dynamics between the two groups. Finally, experiments are carried out on forty-three patients to assess the effectiveness of the proposed method. By using the leave-one-out cross-validation style under normal and fast walking speed conditions, the correct classification rates for discriminating between ACL-D and ACL-I knees are reported to be 95.4% and 93.3%, respectively. Compared with other state-of-the-art methods, the results demonstrate superior performance and support the validity of the proposed method.																	1432-7643	1433-7479				FEB	2020	24	3					1851	1868		10.1007/s00500-019-04017-z													
J								A decentralized multi-authority ciphertext-policy attribute-based encryption with mediated obfuscation	SOFT COMPUTING										Multi-authority attribute-based encryption; Collusion resistance; Mediated obfuscation; Decentralized; Dynamic management	ACCESS-CONTROL; CP-ABE; PRIVACY; EFFICIENT; SECURITY	To ensure security and obtain fine-grained data access control policies in many management domains, multi-authority attribute-based encryption (MA-ABE) schemes were presented and have been applied in cloud storage system. There exist certain scenes where the application domains managed by different attribute authorities (AAs) often change, and hence domain managements require more autonomous and independent. However, most of existing schemes do not support flexible managements. In order to support dynamic managements, we propose a new decentralized ciphertext-policy MA-ABE scheme with mediated obfuscation (MA-DCP-ABE-WMO) where each of AAs works independently without any interaction with other AAs. When issuing a secret key to a user, each of AAs uses his secret to compute a share of the system master secret. Data are encrypted under the public keys of attribute management domains. To resist collusion attack, a common pseudorandom function PRF(center dot) is shared among AAs and is used to randomize each user's global identifier Gid. The randomized Gid is adopted to unify all target messages which need to be reconstructed from different management domains. We first introduce the mediated obfuscation (MO) model into MA-ABE scheme to provide online service and the interaction works among data owner, data user and the mediator. In the MO model, we define a special functional encryption scheme where the function program can be coded into an element of the multiplicative cyclic group. We obfuscate the function by randomly selecting a blinding factor to conduct exponent arithmetic with the base of the function. A special input of the function is constructed to cancel the blinding factor when calling the obfuscated function. It makes other participants know nothing about the inner function program but can evaluate the function program. Furthermore, the MA-DCP-ABE-WMO scheme is proved to be secure. Compared with related schemes, our scheme is suitable to dynamic domain managements. When the management domains are added or removed, the workload to update original ciphertexts and private keys is dramatically reduced.																	1432-7643	1433-7479				FEB	2020	24	3					1869	1882		10.1007/s00500-019-04018-y													
J								Solving the set-union knapsack problem by a novel hybrid Jaya algorithm	SOFT COMPUTING										The set-union knapsack problem; Jaya algorithm; Differential evolution (DE); Cauchy mutation	DIFFERENTIAL EVOLUTION; OPTIMIZATION; DESIGN; MACHINE; CAUCHY	The set-union knapsack problem (SUKP) is a variation of the 0-1 knapsack problem (KP) in which each item is a set of elements, each item has a nonnegative value, and each element has a nonnegative weight. The weight of one item is given by the total weight of the elements in the union of the items' sets. The SUKP accommodates a number of real-life applications and is more complicated and computationally difficult than the 0-1 KP. In this paper, we propose a novel hybrid Jaya algorithm with double coding (DHJaya) to solve the SUKP. In the DHJaya, double coding is used to represent the individual, which includes the solutions for solving the SUKP by adopting a mapping function. The Jaya algorithm and differential evolution algorithm are combined to improve the exploration ability. To enhance the exploitation ability, the Cauchy mutation is performed on some individuals. Meanwhile, an improved repairing and optimization algorithm (MS-GROA) is proposed to repair the infeasible solutions and optimize the feasible solutions. We test the DHJaya using three sets of SUKP instances to demonstrate its efficiency, and the obtained results are compared with those in the previous study. Extensive experiments show a remarkable performance of the proposed approach.																	1432-7643	1433-7479				FEB	2020	24	3					1883	1902		10.1007/s00500-019-04021-3													
J								Hybrid model for prediction of heart disease	SOFT COMPUTING										Heart disease; Hybrid model; Parallel GA; Effective diagnosis; e-doctor	DIAGNOSIS; CLASSIFICATION; ALGORITHM; NETWORKS	Heart disease is a leading cause of death in the world. In order to drop its rate, effective and timely diagnosis of the disease is very essential. Numerous automated decision support systems have been developed for this purpose. In the present research, a predictive model consisting of two-level optimization is introduced, to save lives and cost via effective diagnosis of the disease. Level-1 optimization of the model first identifies parallelly an optimal proportion (P-opt) for training and test sets for each dataset on parallel machine. Next, the best training set (T-best) for P-opt is again searched parallelly. On the other hand, level-2 optimization refines the rule set (R) generated by the Perfect Rule Induction by Sequential Method (PRISM) learner on T-best employing parallel genetic algorithm. The experimental results obtained by the model over the heart disease datasets (collected from https://archive.ics.uci.edu/ml) are compared and analysed with its base learner and four state-of-the-art learners, namely C4.5 (decision tree-based classifier), Naive Bayes, neural network and support vector machine. The empirical outcomes (based on the top performance metrics-prediction accuracy, precision, recall, area under curve values, true positive and false positive rates) positively demonstrate that the new model is proficient in undertaking heart disease treatment. Importantly, the prediction accuracy of the presented hybrid model exceeds around 6% than that of the sequential GA-based hybrid model over almost all the chosen datasets. After all, the proposed system may work as an e-doctor to predict heart attack and assist clinicians to take precautionary steps.																	1432-7643	1433-7479				FEB	2020	24	3					1903	1925		10.1007/s00500-019-04022-2													
J								Integer cat swarm optimization algorithm for multiobjective integer problems	SOFT COMPUTING										Swarm intelligence; Cat swarm optimization; Multiobjective optimization; Integer optimization	KRILL HERD ALGORITHM; PARTICLE SWARM; GENETIC ALGORITHM	In the literature, several variants of cat swarm optimization (CSO) algorithm are reported. However, CSO for integer multiobjective optimization problems (MOPs) has not yet been investigated. Owing to the frequent occurrence of integer MOPs and their importance in practical design problems, in this work, we investigate a new CSO approach for solving purely integer MOPs. This new approach named as multiobjective integer cat swarm optimization (MO-ICSO) algorithm incorporates the modified version of the CSO algorithm for MOPs. This approach is comprised of the concepts of rounding the floating points to the nearest integer numbers and the probabilistic updating (PU) technique. It uses the idea of Pareto dominance for finding the non-dominated solutions and an external archive for storing these solutions. We demonstrate the power of this new approach via its quantitative analysis and sensitivity test of its several parameters using different performance metrics performed over multiobjective multidimensional knapsack problem and several standard test functions. The simulation results argue that the proposed MO-ICSO approach can be a better candidate for solving the integer MOPs.																	1432-7643	1433-7479				FEB	2020	24	3					1927	1955		10.1007/s00500-019-04023-1													
J								Stabilization of a class of nonlinear control systems via a neural network scheme with convergence analysis	SOFT COMPUTING										Asymptotic stability; Nonlinear control systems; Affine control systems; Hamilton Jacobi Bellman equation; Suboptimal feedback control; Neural network	CONTROL LYAPUNOV FUNCTIONS; APPROXIMATE SOLUTIONS; STABILITY ANALYSIS; HJB EQUATION; FEEDBACK	In this paper, the stability of a class of nonlinear control systems is analyzed. We first construct an optimal control problem by inserting a suitable performance index; this problem is referred to as an infinite horizon problem. By a suitable change of variable, the infinite horizon problem is reduced to a finite horizon problem. We then present a feedback controller designing approach for the obtained finite horizon control problem. This approach involves a neural network scheme for solving the nonlinear Hamilton Jacobi Bellman equation. By using the neural network method, an analytic approximate solution for value function and a suboptimal feedback control law are achieved. A learning algorithm based on a dynamic optimization scheme with stability and convergence properties is also provided. Some illustrative examples are employed to demonstrate the accuracy and efficiency of the proposed plan. As a real-life application in engineering, the stabilization of a micro-electromechanical system is studied.																	1432-7643	1433-7479				FEB	2020	24	3					1957	1970		10.1007/s00500-019-04024-0													
J								Group decision making based on power Heronian aggregation operators under neutrosophic cubic environment	SOFT COMPUTING										MAGDM; Neutrosophic cubic sets; Power average; Heronian mean	INTUITIONISTIC FUZZY-SETS; SIMILARITY MEASURE; ENTROPY	Neutrosophic cubic sets can deal with the complex information by combining the neutrosophic sets and cubic sets, the power average (PA) can weaken some effects of awkward data from biased decision makers, and Heronian mean (HM) can deal with the interrelationship between the aggregated attributes or arguments. In this article, in order to consider the advantages of the PA and HM, we combined and extended them to process neutrosophic cubic information. Firstly, we defined a distance measure for neutrosophic cubic numbers, then we presented the neutrosophic cubic power Heronian aggregation operator and neutrosophic cubic power weighted Heronian aggregation operator, and some characters and special cases of these new aggregation operators were investigated. Furthermore, we gave a new approach for multi-attribute group decision making based on new proposed operators. Finally, two examples were given to explain the validity and advantages of the developed approach by comparing with the existing method.																	1432-7643	1433-7479				FEB	2020	24	3					1971	1997		10.1007/s00500-019-04025-z													
J								Deep packet: a novel approach for encrypted traffic classification using deep learning	SOFT COMPUTING										Network traffic classification; Application identification; Traffic characterization; Deep learning; Convolutional neural networks; Stacked autoencoder; Deep Packet	NEURAL-NETWORKS	Network traffic classification has become more important with the rapid growth of Internet and online applications. Numerous studies have been done on this topic which have led to many different approaches. Most of these approaches use predefined features extracted by an expert in order to classify network traffic. In contrast, in this study, we propose a deep learning-based approach which integrates both feature extraction and classification phases into one system. Our proposed scheme, called "Deep Packet," can handle both traffic characterization in which the network traffic is categorized into major classes (e.g., FTP and P2P) and application identification in which identifying end-user applications (e.g., BitTorrent and Skype) is desired. Contrary to most of the current methods, Deep Packet can identify encrypted traffic and also distinguishes between VPN and non-VPN network traffic. The Deep Packet framework employs two deep neural network structures, namely stacked autoencoder (SAE) and convolution neural network (CNN) in order to classify network traffic. Our experiments show that the best result is achieved when Deep Packet uses CNN as its classification model where it achieves recall of 0.98 in application identification task and 0.94 in traffic categorization task. To the best of our knowledge, Deep Packet outperforms all of the proposed classification methods on UNB ISCX VPN-nonVPN dataset.																	1432-7643	1433-7479				FEB	2020	24	3					1999	2012		10.1007/s00500-019-04030-2													
J								A novel hybrid multi-objective bacterial colony chemotaxis algorithm	SOFT COMPUTING										Global optimization; MOBCC; Hybrid algorithm; Intelligence computation	PARTICLE SWARM OPTIMIZER	In this article, a novel hybrid multi-objective bacterial colony chemotaxis (HMOBCC) algorithm is proposed to solve multi-objective optimization problems. A mechanism of particle swarm optimization is introduced to multi-objective bacterial colony chemotaxis (MOBCC) algorithm to improve the performance of MOBCC algorithm. Also, three other techniques, including dynamic reverse learning operator, external archive multiplying operator and adaptive diversity maintenance operator, are further applied to improve the diversity and convergence of the algorithm. The proposed algorithm is validated using 12 benchmark problems, and three performance measures are implemented for 5 benchmark problems to compare its performance with existing popular algorithms such as MOBCC, multi-objective bacterial colony chemotaxis based on grid algorithm, non-dominated sorting genetic algorithm (NSGA-II) and multi-objective evolutionary algorithm based on decomposition. The results show that the proposed HMOBCC is very effective against existing algorithms.																	1432-7643	1433-7479				FEB	2020	24	3					2013	2032		10.1007/s00500-019-04034-y													
J								Intelligent sales volume forecasting using Google search engine data	SOFT COMPUTING										Sales volume forecasting; Least-square support vector regression; Particle swarm optimization; Deep learning; Google Index	SUPPORT VECTOR REGRESSION; GENETIC ALGORITHM; NEURAL-NETWORKS; PREDICTION; MACHINES; PRICE; PARAMETERS; TUTORIAL; MODELS; SYSTEM	Business forecasting is a critical organizational capability for both strategic and tactical business planning. Improving the quality of forecasts is thus an important organization goal. In this paper, the intelligent sales volume forecasting models are constructed using grey analysis, deep learning (DNN), and least-square support vector regression (LSSVR) optimized through particle swarm optimization or genetic algorithm. First, features (predictors) from economic variables are extracted through grey analysis. The selected features together with Google Index, an exogenous variable used widely by researchers, are then used as the inputs to the DNN and LSSVR to build the models. The experimental results indicate that the grey DNN model, an emerging and pioneering artificial intelligence technology, can accurately predict sales volumes based on non-parametric statistical tests. DNN also outperformed the competing models when using Google Index.																	1432-7643	1433-7479				FEB	2020	24	3					2033	2047		10.1007/s00500-019-04036-w													
J								Structural risk minimization of rough set-based classifier	SOFT COMPUTING										Structural risk minimization; Rough set-based classifiers; Complexity control; Genetic multi-objective optimization	FEATURE-SELECTION; GENETIC ALGORITHM; FUZZY; SVM; EVOLUTIONARY; SYSTEMS; SEARCH	The classification ability in unseen objects, namely generalization ability, remains a long-standing challenge in rough set-based classifier. Current research mainly focuses on introducing thresholds to tolerate some errors in seen objects. The reason for introducing thresholds and the selection of threshold still lack sufficient theoretical support. The structural risk minimization (SRM) inductive principle is one of the most effective theories to control the generalization ability, which suggests a trade-off between errors in seen objects and complexity. Therefore, this paper introduces the SRM principle into rough set-based classifier and proposes SRM algorithm of rough set-based classifier called SRM-R algorithm. SRM-R algorithm uses the number of rules to characterize the actual complexity of rough set-based classifier and obtains the optimal trade-off between errors in seen objects and complexity through genetic multi-objective optimization. The tenfold cross-validation experiment in 12 UCI datasets shows SRM-R algorithm can significantly improve the generalization ability compared with conventional threshold algorithm. Besides, this paper uses other two possible complexity metrics including the number of attributes and attribute space to construct corresponding SRM algorithms, respectively, and compared their classification accuracy with that of SRM-R algorithm. Comparison result shows SRM-R algorithm obtains optimal classification accuracy. This indicates that the number of rules characterizes the complexity more effectively than the number of attributes and attribute space. Further experiments show that SRM-R algorithm obtains fewer rules and larger support coefficient, which means it extracts stronger rules. This explains why it obtains better generalization ability to some extent.																	1432-7643	1433-7479				FEB	2020	24	3					2049	2066		10.1007/s00500-019-04038-8													
J								Aggregating expert advice strategy for online portfolio selection with side information	SOFT COMPUTING										Online portfolio selection; Universal portfolio; Side information; Expert advice; Weak aggregating algorithm	UNIVERSAL PORTFOLIOS; REVERSION STRATEGY; OPTIMIZATION	Online portfolio selection is an important fundamental problem in computational finance, which has been further developed in recent years. As the financial market changes rapidly, investors need to dynamically adjust asset positions according to various financial market information. However, existing online portfolio strategies are always designed without considering this information, which limits their practicability to some extent. To overcome this limitation, this paper exploits the available side information and presents a novel online portfolio strategy named "WAACS". Specifically, all the constant rebalanced portfolio strategies are considered as experts and the weak aggregating algorithm is applied to aggregate all the expert advice according to their previous cumulative returns under the same side information state as the current period. Furthermore, WAACS is theoretically proved to be a universal portfolio, i.e., its growth rate is asymptotically the same as that of the best state constant rebalanced portfolio, which is a benchmark strategy considering side information. Numerical experiments show that WAACS achieves significant performance and demonstrate that considering side information improves the performance of the proposed strategy.																	1432-7643	1433-7479				FEB	2020	24	3					2067	2081		10.1007/s00500-019-04039-7													
J								Normative fish swarm algorithm (NFSA) for optimization	SOFT COMPUTING										Artificial fish swarm algorithm (AFSA); Adaptive visual and step; Particle swarm optimization (PSO); PSO with extended memory (PSOEM); Normative knowledge; Cultural algorithm (CA); t test	AGGREGATION OPERATORS	In this paper, a swarm-based optimization algorithm, normative fish swarm algorithm (NFSA) is proposed as an effective global and local search technique to obtain effective global optima at superior convergence speed. Artificial fish swarm algorithm is a recent swarm-based algorithm that imitates the behavior of fish swarm in the real environment. Many improvements and modifications have been proposed regularly on fish swarm algorithm to improve the performance of optimization, but to date, existing fish swarm algorithms have not yet obtained a global optimum at extremely superior convergence rates. Hence, there still remains a huge potential for the development of fish swarm algorithm. NFSA hybridizes the characteristics of PSOEM-FSA with the normative knowledge as the complementary guidelines for more accurate and precise global optimum approaching. NFSA further improves the adaptive parameters in term of visual and step to balance the contradiction between the exploration and exploitation processes. Random initialization of the initial population is introduced to spread out the solution candidates of artificial fishes over the solution space. For the purpose of experiments, ten benchmark functions have been used in the evaluation process. The proposed algorithm is then compared with other related algorithms published in the literature. The results proved that the proposed NFSA achieved superior results in terms of convergence rate and best optimal solution on a majority of the tested benchmark functions in comparison with other comparative algorithms.																	1432-7643	1433-7479				FEB	2020	24	3					2083	2099		10.1007/s00500-019-04040-0													
J								Data sharing using proxy re-encryption based on DNA computing	SOFT COMPUTING										Proxy; Re-encryption; DNA; Security; Data sharing		Cloud data sharing allows users to access data stored on the web object. Security and protecting cloud data sharing form different attacks is recently considered one of the most challenges. In this paper, we are proposing a framework for cloud data sharing protection against unauthorized access. The proposed framework is based on DNA-proxy re-encryption. Firstly, three keys are generated for the owner, proxy and the user who need to access the data. Then, the owner stores his data encrypted on the cloud using his key. If the user wants to access this data then he can access it via the proxy after re-encrypting using the second generated key for the proxy. Finally, the user can decrypt the re-encrypted data with the third generated key. The framework was implemented using various plaintext files and real DNA sequences. The experimental results show that the framework has an outstanding performance in terms of execution time.																	1432-7643	1433-7479				FEB	2020	24	3					2101	2108		10.1007/s00500-019-04041-z													
J								Application of artificial neural network (ANN) for estimating reliable service life of reinforced concrete (RC) structure bookkeeping factors responsible for deterioration mechanism	SOFT COMPUTING										Corrosion; RC; Service life; Prediction; Models; ANN	RELIABILITY-ANALYSIS; COVER CRACKING; CORROSION; MODEL; PREDICTION; STEEL; TIME	Degradation of RC structures due to corrosion induced mechanism in the reinforcing steel is a serious durability problem worldwide. It occurs essentially when the reinforcement within the concrete is subjected to marine or aggressive environment. The aim of the present work is to predict the reliable service life of the RC structures by taking into consideration of various prominent models of corrosion and comparing the output with the predicted output of ANN model. Parametric studies have been conducted on four different models to study the effect of various parameters such as corrosion rate, cover thickness, bar diameter, and perimeter of bar which actively participates in the time dependent degradation of RC structures. The outcomes of the parametric inspection of the four chosen degradation models are shown in the present study. The acceptability of the prediction models in forecasting the service life of RC structures are shown through circumstantial illustrative analysis and the best suited model sorted out. However, with the application of soft computing such as ANN, a prediction has been made to determine the service life of RC structures, and the predicted outputs validated with the intended outputs thereby yielding good outcomes for envisaging service life of RC structure.																	1432-7643	1433-7479				FEB	2020	24	3					2109	2123		10.1007/s00500-019-04042-y													
J								On generalizations of fuzzy quasi-prime ideals in LA-semigroups	SOFT COMPUTING										(alpha, beta)-Fuzzy LA-subsemigroup; (alpha, beta)-Fuzzy left ideal; (alpha, beta)-Fuzzy completely prime subset; (alpha, beta)-Fuzzy quasi-prime ideal; (alpha)-Fuzzy subset ((alpha)-fuzzy subset)	AGGREGATION OPERATORS	In this paper, we extend the concept of fuzzy subsets given by Zadeh (Inf Control 8:338-353, 1965) to the context of (alpha)-fuzzy and (alpha)-fuzzy subsets. The aim of this paper is to investigate the concept of (alpha)-fuzzy and (alpha)-fuzzy subsets in LA-semigroups. Some characterizations of (alpha, beta)-fuzzy LA-subsemigroup, (alpha, beta)-fuzzy left, (alpha, beta)-fuzzy completely prime and (alpha, beta)-fuzzy quasi-prime ideals are obtained. Moreover, we investigate relationships between (alpha, beta)-fuzzy completely prime and (alpha, beta)-fuzzy quasi-prime ideals of LA-semigroups. Finally, we obtain sufficient conditions of an (alpha, beta)-fuzzy quasi-prime ideal in order to be an (alpha, beta)-fuzzy completely prime subset.																	1432-7643	1433-7479				FEB	2020	24	3					2125	2137		10.1007/s00500-019-04043-x													
J								An effective quality analysis of XML web data using hybrid clustering and classification approach	SOFT COMPUTING										XML web; Keyword search; Weighted fuzzy c means clustering; Neural network; Whale optimization; RMSE	SECURE	An effective quality analysis of XML web data using clustering and classification approach is used in our proposed method. XML is turning into a standard in representation of data, it is attractive to support keyword search in XML database. A keyword search searches for words anyplace in record. It is developed as best worldview for finding data on web. The most imperative prerequisite for the keyword search is to rank the consequences of question so that the most pertinent outcomes show up. Here, we gather more XML documents. Followed by that, feature extraction occurs. Since the selected feature contains both relevant as well as irrelevant features it is essential to filter the irrelevant features. For the purpose of selecting, the relevant features probability-based feature selection method is used. Then for clustering the relevant features on the basis of keywords weighted fuzzy c means clustering algorithm is used. In order to assess the XML data quality, optimal neural network (ONN) classifier is utilized. In this ONN classifier in order to select the optimal weights, whale optimization algorithm is used. Thus, the web pages are effectively ranked. The efficiency of the proposed method is assessed using clustering and classification accuracy, RMSE, and search time. The proposed method is implemented in JAVA.																	1432-7643	1433-7479				FEB	2020	24	3					2139	2150		10.1007/s00500-019-04045-9													
J								FuzzyCIE: fuzzy colour image enhancement for low-exposure images	SOFT COMPUTING										Image enhancement; Histogram equalization; Structural similarity index; Feature similarity index; Low-exposure colour image; Fuzzy histogram	BI-HISTOGRAM EQUALIZATION; CONTRAST; ALGORITHM; MODEL	Colour image enhancement not only is of high importance in consumer electronics, but also plays significant role in medical imaging, remotely sensed imaging, etc. Moreover, low-exposure colour images inherently lack sufficient image details which are exclusively necessary for workings in these domains. To address this less explored problem, a novel enhancement algorithm involving estimation of the fuzzy histogram with thresholding based on the computed effect of exposure value has been proposed. The algorithm operates on the lightness (L*) component of the input image in L*a*b* colour space, while preserving the colour-opponent dimensions (a* and b*) to maintain the natural outlook of the image. This technique has been experimentally demonstrated over a dataset consisting of images generated at different exposure levels. Quantitative and qualitative analysis of the relative performance of the proposed algorithm has been shown with respect to state-of-the-art enhancement algorithms over the L*a*b* space.																	1432-7643	1433-7479				FEB	2020	24	3					2151	2167		10.1007/s00500-019-04048-6													
J								A hybrid genetic algorithm for the degree-constrained minimum spanning tree problem	SOFT COMPUTING										Degree-constrained; Spanning tree; Steady-state genetic algorithm; Problem-specific crossover operator; Local search; Replacement strategy	NUMBER	Given an undirected, connected, edge-weighted graph G and a positive integer d, the degree-constrained minimum spanning tree (dc-MST) problem aims to find a minimum spanning tree T on G subject to the constraint that each vertex is either a leaf vertex or else has degree at most d in T, where d is a given positive integer. The dc-MST is NP-hard problem for d >= 2 and finds several real-world applications. This paper proposes a hybrid approach (HSSGA) combining a steady-state genetic algorithm and local search strategies for the this problem. An additional step (based on perturbation strategy at a regular interval of time) in the replacement strategy is applied in order to maintain diversity in the population throughout the search process. On a set of available 107 benchmark instances, computational results show the superiority of our proposed HSSGA in comparison with the state-of-the-art metaheuristic techniques.																	1432-7643	1433-7479				FEB	2020	24	3					2169	2186		10.1007/s00500-019-04051-x													
J								SCRM: self-correlated representation model for visual tracking	SOFT COMPUTING										Visual tracking; Self-correlated representation; Low-dimensional subspace; Particle filter	FEATURE-SELECTION; OBJECT TRACKING; SPARSITY	y Sparse representation (SR) as a seminal model for visual tracking explores the relationship between all candidates and the observed templates. Different from SR-based trackers, we propose a self-correlated representation model for robust visual tracking. Firstly, we learn a low-dimensional subspace representation from highly correlated templates to model the object, which aims at eliminating the redundant information and reducing the influence of noisy templates. Then, we represent the subspace by itself to learn the inner underlying features from subspace vectors. To further enhance model's discriminating power, a new observation model is developed by considering both error distribution and large outliers. Experiments are conducted on some challenging video clips and demonstrate the favorable performance of our tracking system compared to some state-of-the-art representation-based trackers.																	1432-7643	1433-7479				FEB	2020	24	3					2187	2199		10.1007/s00500-019-04052-w													
J								Peer-induced fairness capacitated vehicle routing scheduling using a hybrid optimization ACO-VNS algorithm	SOFT COMPUTING										Emergency relief distribution; Vehicle routing problem; Fairness; Variable neighbourhood search; Ant colony optimization	VARIABLE NEIGHBORHOOD SEARCH; LOGISTICS MANAGEMENT; EQUITY; TRANSPORTATION; ALLOCATION; MODELS	In this paper, we address the problem of delivering a given amount of goods in emergency relief distribution. This problem is considered to be a specific case of capacitated vehicle routing. As a novel issue, peer-induced fairness concern is aimed at securing more customers' needs by introducing the peer-induced fairness coefficient, which is the value of the population size divided by direct travel time. Thus, a peer-induced fairness capacitated vehicle routing scheduling model is proposed to handle the trade-off between timeliness and fairness in emergency material delivery. To solve the specific NP-hard capacitated vehicle routing problem, the properties of this problem are analysed, and an improved hybrid ACO-VNS algorithm based on ant colony optimization and variable neighbourhood search algorithm with five neighbourhood structures is accordingly presented. A comparison of the proposed algorithm with CPLEX and common optimization algorithms demonstrates that this method achieves better performance in a shorter time and is an efficient way to solve the vehicle routing scheduling problem in emergency relief distribution.																	1432-7643	1433-7479				FEB	2020	24	3					2201	2213		10.1007/s00500-019-04053-9													
J								Intuitionistic Fuzzy TOPSIS method for green supplier selection problem	SOFT COMPUTING										Green supplier selection (GSS); Multi-criteria decision-making (MCDM); Intuitionistic Fuzzy TOPSIS (IFTOPSIS)	GROUP DECISION-MAKING; MULTIOBJECTIVE GENETIC ALGORITHM; ORDER ALLOCATION; CHAIN; ENVIRONMENT; DISTANCES; FRAMEWORK; NETWORK; MODEL	One of the most important functions of supply chain management is to enhance competitive pressure. Competition conditions and customer perception have changed in favor of environmentalist attitude. Therefore, green supplier selection (GSS) has become an important issue. In this study, the problem of GSS aiming for lean, agile, environmentally sensitive, sustainability, and durability is addressed. The environmental criteria considered in GSS and classical supplier selection are different from each other in terms of carbon footprint, water consumption, environmental applications, and recycling applications. The Technique for Order Preference by Similarity to Ideal Solution (TOPSIS) method has been used in the problem of GSS by considering the multi-criteria decision-making (MCDM) method since MCDM is very effective in many aspects such as evaluating and selecting the classical and environmental criteria. Due to linguistic criteria and no possibility to measure all criteria, it is needed to consolidate the fuzzy approach with the TOPSIS method to reduce the effects of ambiguity and instability. The Intuitionistic Fuzzy TOPSIS method is used because this method makes evaluating decision-makers and criteria convenient. According to the criteria determined by the order of importance, the hybrid method resulting from combining the Intuitionistic Fuzzy Set and TOPSIS is very effective to select which supplier is more suitable among the alternatives and also this method can be integrated to similar problems.																	1432-7643	1433-7479				FEB	2020	24	3					2215	2228		10.1007/s00500-019-04054-8													
J								Design concept evaluation using soft sets based on acceptable and satisfactory levels: an integrated TOPSIS and Shannon entropy	SOFT COMPUTING										Soft sets; TOPSIS; Shannon entropy; Concept evaluation; Acceptable and satisfactory levels; Preferences	DECISION-MAKING; PRODUCT DEVELOPMENT; CONCEPT SELECTION; SYSTEM; ALTERNATIVES; CUSTOMER; METHODOLOGY; INDEX; VIKOR; AHP	Among several phases of new product development, concept selection is the most crucial activity and it gives perfection to further progress of a product. Customer's ideas and linguistic requirements are often substantial in concept specifications to assess quantitative criteria, which gives satisfaction for a product to progress in the markets. This work aggregates concept selection on design parameters values by merging acceptable- and satisfactory-level needs of the customers. A promising framework is developed based on soft sets, TOPSIS and the Shannon entropy. Customer's preferences on incorporating design values are identified based on acceptable- and satisfactory-level needs, and these preferences are weighted through Shannon entropy. By performing AND operation on the soft set of level requirements of one customer with the soft set of requirements of another customer, several weighted tables of soft sets are obtained on the pair of design parameters values. To obtain the best concept on different levels of requirements, TOPSIS is performed which provides several integrated evaluations. An illustration is considered for the demonstration of the method, brings the best concept for two customers which is acceptable for both of the customers, satisfactory for both the customers and vise-versa. Finally, the comparisons are presented with recent major existing methods.																	1432-7643	1433-7479				FEB	2020	24	3					2229	2263		10.1007/s00500-019-04055-7													
J								A novel parallel object-tracking behavior algorithm based on dynamics for data clustering	SOFT COMPUTING										Parallel; Object tracking; Clustering; Parameters self-learning; Energy	PARTICLE SWARM OPTIMIZATION; EVOLUTIONARY; INTELLIGENCE	Recently, many evolutionary algorithms (EAs) have been used to solve clustering problem. However, compared to K-means which is a simple and fast clustering algorithm, these EA-based clustering algorithms take too much computation time. In addition, the parameters of most EAs are fixed or dynamical adjustment by a simple method on different datasets, and it will cause that the performance of these algorithms is good on some datasets but bad on others. In order to overcome these disadvantages, a novel parallel object-tracking behavior algorithm (POTBA) based on dynamics is proposed in this paper. The proposed algorithm consists of three different models which are parallel object-tracking model, parameters self-learning model and energy model, respectively. First, the parallel object-tracking model is designed to accelerate the computation speed and avoid local minima. Second, the parameters of POTBA are self-adjusted by the parameters self-learning model. Third, the energy model is introduced to depict energy changes of POTBA during the evolutionary process. The correctness and convergence properties of POTBA are analyzed theoretically. Moreover, the effectiveness and parallelism of POTBA are evaluated through several standard datasets, and the experimental results demonstrate that POTBA exhibits superior overall performance than five other state-of-the-art algorithms. In the aspect of search performance, the results of POTBA are better than other comparison algorithms on most used datasets. In the aspect of time performance, the time overhead of POTBA is significantly reduced through parallel computing. When the number of processors increases to 32, the computation time of POTBA is less or close to K-means which is the fastest comparison algorithm.																	1432-7643	1433-7479				FEB	2020	24	3					2265	2285		10.1007/s00500-019-04058-4													
J								A novel interval-valued intuitionistic trapezoidal fuzzy combinative distance-based assessment (CODAS) method	SOFT COMPUTING										MADM; Fuzzy sets; IVITrFS; CODAS method	GROUP DECISION-MAKING; AGGREGATION OPERATORS; NUMBERS; SELECTION; RANKING; SETS	Multiple Attribute Decision Making (MADM) problems have received great attention from many researchers over the past decades. Many useful models and methods have been developed and applied in diverse fields. The objective of this paper is to integrate newly developed COmbinative Distance-based ASsessment (CODAS) method and Interval-Valued Intuitionistic Trapezoidal Fuzzy Set (IVITrFS) to cope with MADM problems. In the proposed method, IVITrFS is used considering membership and non-membership degrees of elements to handle more complex and flexible data than the ordinary fuzzy sets and their extensions. In addition, there has been no work extending CODAS method with IVITrFS to solve MADM problems in the literature. To illustrate applicability and effectiveness of the proposed method, a numerical example is employed for the selection of the most suitable investment project. A sensitivity analysis is applied to examine the stability and validity of the proposed approach. Then, the obtained results of the proposed method are compared with the existing methods to confirm the efficiency and reliability of the proposed method. Accordingly, the proposed IVITrFS-CODAS method is superior to CODAS, ordinary fuzzy CODAS and interval-valued Atanassov intuitionistic fuzzy CODAS (IVAIF-CODAS) methods since IVITrFS-CODAS deals with the hesitancy and fuzziness of human thinking better in decision-making process and provides larger domain for decision makers by assigning membership and non-membership scores in the interval between 0 (non-membership) and 1 (full membership). Finally, concluding remarks are presented at the end of the study.																	1432-7643	1433-7479				FEB	2020	24	3					2287	2300		10.1007/s00500-019-04059-3													
J								Tauberian theorems for ((N)over-bar, p, q) summable double sequences of fuzzy numbers	SOFT COMPUTING										Double sequences of fuzzy numbers; Weighted mean method; Convergence in Pringsheim's sense; Slow oscillation; Tauberian theorems		In this paper, we define the weighted mean method ((N) over bar, p, q) of double sequences of fuzzy numbers and give necessary and sufficient Tauberian conditions under which convergence in Pringsheim's sense of a double sequence of fuzzy numbers follows from its ((N) over bar, p, q) summability. These conditions are weaker than the weighted analogues of Landau's conditions and Schmidt's slow oscillation condition in some senses for two-dimensional case.																	1432-7643	1433-7479				FEB	2020	24	3					2301	2310		10.1007/s00500-019-04060-w													
J								A robust 2D-Cochlear transform-based palmprint recognition	SOFT COMPUTING										Biometrics; Palmprint; Cochlear transform; ROI extraction; Feature extraction; Robustness	IDENTIFICATION; VERIFICATION; FUSION; PRINT; FACE	In this paper, a noise-robust palmprint recognition system is discussed with a novel feature extraction technique: two-dimensional Cochlear transform (2D-CT) based on the textural analysis of image sample. Orthogonality of 2D-CT is proved which shows the high robustness of the proposed 2D-CT to noise. To validate the proposed feature extraction technique, palmprint recognition has been tested on both left and right palm of IITD database of 230 persons, CASIA palmprint database of 312 persons, polyU palmprint database of 386 persons and achieved high accuracy. The proposed 2D-CT method is compared with discriminative and robust competitive code, double orientation code, competitive coding, ordinal coding, Gabor transform, Gaussian membership-based features, absolute average deviation and mean features. Further, K-nearest neighbor is used to validate the matching stage. The results show superiority of the proposed method over other feature extraction methods.																	1432-7643	1433-7479				FEB	2020	24	3					2311	2328		10.1007/s00500-019-04062-8													
J								Weighted belief function of sensor data fusion in engine fault diagnosis	SOFT COMPUTING										Dempster-Shafer evidence theory; Fault diagnosis; Information fusion; Basic probability assignment; Belief function; Weighted factor; Weighted basic probability assignment	DECISION-MAKING METHOD; EVIDENTIAL NETWORK; RELIABILITY-ANALYSIS; UNCERTAINTY; INFORMATION; FRAMEWORK; EVIDENCES	Fault diagnosis (the process of finding out whether system or equipment is in fault and where the corresponding fault is by using various inspection and testing method) on the engine is a typical information fusion (the process of integrating multiple data sources to produce more consistent, accurate, and useful information than that provided by any individual data source) problem where the information can be obtained from engine vibration, temperature, pressure, etc. Due to the efficiency of data fusion, Dempster-Shafer evidence theory is widely used in fault diagnosis. One key step to using evidence theory is to obtain the so-called basic probability assignment (BPA), or belief function. In this article, a new mathematical framework is presented to determine weighted BPA (WBPA). This WBPA function is obtained by weighting the distance between sample data and empirical data. With the assumption that the empirical data are normally distributed, the weighting factor can be determined. Then, the WBPA can be combined with D-S evidence theory to determine the status of the engine. Finally, a case in fault diagnosis and comparison with Song and Jiang (Adv Mech Eng 8(10):1-16, 2016) method illustrate the efficiency of the proposed method.																	1432-7643	1433-7479				FEB	2020	24	3					2329	2339		10.1007/s00500-019-04063-7													
J								Gas chimney and hydrocarbon detection using combined BBO and artificial neural network with hybrid seismic attributes	SOFT COMPUTING										Gas chimneys; Hydrocarbon reservoirs; Seismic attributes; Biogeography-based optimization; Supervised neural network	BIOGEOGRAPHY BASED OPTIMIZATION; FEATURE-SELECTION; MIGRATION	The exact interpretation of structure of the Earth is essential to avoid drilling of dry holes or locating hydrocarbon reservoirs in faulty regions. The economic approaches are required to locate the reservoirs without damaging the tectonic plates of the Earth. The identification of gas chimneys provides an impressive approach to indirectly interpret the hydrocarbon reservoirs as the chimneys form the migration pathway for gas from reservoirs, and it looks like a gas cloud in the seismic data. The proposed method using biogeography-based optimization (BBO) with supervised neural networks enables the effective interpretation of chimneys and hydrocarbons. Seismic attributes are measured on the pre-processed seismic data, where the attributes play an important role in providing qualitative information on structure of the Earth. Continuity, instantaneous and amplitude attributes are measured to provide the details on the areas of discontinuity, brightspot regions with high amplitude and low frequency. The calculated attributes are picked from selected locations in the preprocessed seismic data, and the hybrid combination of attributes enables effective, economic chimney identification and provides indications to locate the reservoirs. The redundant and highly correlated features are eliminated using BBO which in turn improves the classification accuracy. BBO algorithm retains the best solution in each generation, and the fitness of the algorithm is verified with supervised neural networks. The proposed algorithm is applied on F3 block dataset, and BBO selects 20 predominant features among 75 features. The optimal solution after 50 generations enables the classification of chimneys through multilayer feed-forward supervised neural network. The results indicate that the chimney classification accuracy is improved by 90%, and the mean square error is minimized with BBO as a feature selection algorithm compared to other evolutionary algorithms.																	1432-7643	1433-7479				FEB	2020	24	3					2341	2354		10.1007/s00500-019-04064-6													
J								A novel WASPAS approach for multi-criteria physician selection problem with intuitionistic fuzzy type-2 sets	SOFT COMPUTING										Divergence measure; Entropy measure; Multi-criteria decision making; Intuitionistic fuzzy type-2 set; Pythagorean fuzzy set; WASPAS	PYTHAGOREAN MEMBERSHIP GRADES; DECISION-MAKING; INFORMATION MEASURES; PATIENTS CHOOSE; TOPSIS; EXTENSION; NUMBERS; SWARA	Due to innovative and practical technology, the selection of a right physician is an important issue for patients. However, uncertainty and vagueness frequently arise during the process of selecting physicians. Intuitionistic fuzzy type-2 sets (IFT2Ss) (recently named as Pythagorean fuzzy sets) provide an important tool to handle the uncertainty arises in real-life decision-making problems. This paper presents an extended weighted aggregated sum product assessment (WASPAS) method based on novel information measures (entropy and divergence measures) and operators under IFT2Ss context. In the proposed WASPAS method, entropy and divergence measure-based formula is developed to find the criteria weights. For this, several intuitionistic fuzzy entropy and divergence measures are developed for IFT2Ss. To increase the stability of the proposed methodology, the criteria's weights are calculated in the form of objective and subjective weights. Further, to reveal the applicability and effectiveness of proposed method, an uncertain multi-criteria decision-making problem of physician selection is executed with intuitionistic fuzzy information of second type. Finally, the validity of the proposed method is implemented by comparison with existing methods and sensitivity analysis and also proves that the proposed method is valid and feasible in the physician selection processes with information given in intuitionistic fuzzy type-2 numbers (IFT2Ns).																	1432-7643	1433-7479				FEB	2020	24	3					2355	2367		10.1007/s00500-019-04065-5													
J								A new fallback beetle antennae search algorithm for path planning of mobile robots with collision-free capability	SOFT COMPUTING										Fallback beetle antennae search (FBAS); Path planning; Obstacle avoidance; Mobile robots; Optimization	OBSTACLE-AVOIDANCE; SCHEME; MOTION; MANIPULATORS; MODELS	With the development of technology, mobile robots are becoming more and more common in industrial production and daily life. Various rules are set to ensure that mobile robots can move without collision. This paper proposes a novel intelligent optimization algorithm, named fallback beetle antennae search algorithm. Based on the analysis of biological habits, when the creature enters blind alley during the foraging process, it will retreat a distance and then restart the search process. We introduce a fallback mechanism in the traditional beetle antenna search algorithm. In addition, the proposed algorithm possesses the characteristic of low time complexity. It can plan a collision-free path in a short period of time. Moreover, the effectiveness and superiority of the algorithm are verified by simulations in different types of environments and comparisons with existing path planning algorithms.																	1432-7643	1433-7479				FEB	2020	24	3					2369	2380		10.1007/s00500-019-04067-3													
J								Active constraint spectral clustering based on Hessian matrix	SOFT COMPUTING										Spectral clustering; Active constraint; Hessian matrix; Pairwise constraint; Laplacian matrix	ALGORITHM	Applying the pairwise constraint algorithm to spectral clustering has become a hot topic in data mining research in recent years. In this paper, a clustering algorithm is proposed, called an active constraint spectral clustering based on Hessian matrix (ACSCHM); this algorithm not only use Hessian matrix instead of Laplacian matrix to free the parameter but also use an active query function to dynamically select constraint pairs and use these constraints to tune and optimize data points. In this paper, we used active query strategy to replace the previous random query strategy, which overcame the instability of the clustering results brought by the random query and enhanced the robustness of the algorithm. The unique parameter in the Hessian matrix was obtained by the spectral radius of the matrix, and the parameter selection problem in the original spectral clustering algorithm was also solved. Experiments on multiple UCI data sets can prove the effectiveness of this algorithm.																	1432-7643	1433-7479				FEB	2020	24	3					2381	2390		10.1007/s00500-019-04069-1													
J								A Novel High Voltage Dielectric Test System Based on Resonant Circuits Using the Magnetically Controllable Inductance	ADVANCES IN ELECTRICAL AND COMPUTER ENGINEERING										high-voltage techniques; resonance; insulation testing; magnetic variables control; EMTDC		Dielectric tests such as high voltage withstand test is important to verify whether the electrical devices are in reliable working condition or not. This paper presents a novel high voltage dielectric test system up to 160kV AC based on resonant circuits using the magnetically controllable inductance. Firstly, the working principle and the design of the magnetically controllable inductance are introduced. In addition, a finite-element model is completed and analyzed to verify the designs of the magnetically controllable inductance. Next, resonant circuits and the control scheme of the dielectric test system are described. Finally, the proposed testing system is simulated using PSCAD/EMTDC. The test system can perform series or parallel resonance AC tests by adjusting the inductance in the resonant circuit to meet the various requirements of the equipment to be examined. Compared with the conventional high-voltage test systems, the proposed test system has the advantages of compact structure, stable output voltage, and strong adaptability of the test method.																	1582-7445	1844-7600				FEB	2020	20	1					3	10		10.4316/AECE.2020.01001													
J								Artificial Immunity Based Wound Healing Algorithm for Power Loss Optimization in Smart Grids	ADVANCES IN ELECTRICAL AND COMPUTER ENGINEERING										smart grids; load flow; optimization methods; power system analysis computing; power system simulation	PARTICLE SWARM OPTIMIZATION; REACTIVE POWER; DISPATCH	In this study, a human immune system based wound healing algorithm is mentioned to optimize power losses in the smart grids. The smart grids are a concept that uses communication and control techniques to increase the efficiency of today's electrical systems, provide bidirectional communication and allow instant monitoring of the grid. The wound healing algorithm is computationally simulated in the event of a possible injury to the human body and there are very few publications on the proposed algorithm when the literature review is performed. Therefore, the proposed algorithm is capable of removing this gap in the literature. The codes are written in the Matlab GUI environment and applied to the IEEE 30-busbar system and power losses are tried to be optimized Simulation results show that the actual power loss is significantly reduced. The obtained results were compared with the results of other algorithms that are available in the literature. The proposed wound healing algorithm has given more optimum and superior solutions than the other algorithms compared in terms of calculation time and optimum power loss values and it was emphasized that it was a more effective method in providing the solution.																	1582-7445	1844-7600				FEB	2020	20	1					11	18		10.4316/AECE.2020.01002													
J								Visualizing Imitation of Typical Intense X-ray Radiation Processes	ADVANCES IN ELECTRICAL AND COMPUTER ENGINEERING										visualization; radiation imaging; X-rays; computer simulation; radiation effects	DYNAMICS	A visualizing imitation system of typical intense X-ray radiation process which uses the Light Emitting Diode (LED) light source, the Liquid Crystal Display (LCD) panel, and the Charge Coupled Device (CCD) camera is proposed. Both the radiation processes of spot and wire array are investigated. First, two imitation devices are developed. A LED light producer is designed to imitate the spot source. A microcontroller is utilized for its output control. A LCD panel and a raspberry pi circuit are employed for the radiation imitation of wire array. Second, two radiation imitation methods are proposed. An exponential attenuation function and a particle system-based simulation are proposed. Third, a CCD camera is used to observe the imitation devices above, and many image datasets can be created. Because the shutter response speed of normal camera cannot reach to the change speed of the actual radiation source, a time-lapse capture technique is developed. Finally, some image features, including the statistic and the geometric metrics, are computed to evaluate the imitation effect. Many experiment results have shown the effectiveness of propose method. This system can help radiation researcher understand the transient radiation processes and study the initial image processing algorithms for them to some extent.																	1582-7445	1844-7600				FEB	2020	20	1					19	26		10.4316/AECE.2020.01003													
J								A Novel Simulation Model for Pricing Different QoS Levels in IP Networks	ADVANCES IN ELECTRICAL AND COMPUTER ENGINEERING										decision making; IP networks; quality of service; programming; simulation		The selection of the appropriate pricing concept is one of the most important business decisions an Internet service provider (ISP) has to make. An efficient pricing concept implies that price reflects the quality of service (QoS) obtained by a service provider. In this paper, we propose four billing scenarios, each with applied user-centric QoS-based pricing concept, which is focused on users' demands, defined through QoS and price requirements, as well as delivered QoS. The main issue of this research is to analyse how much ISP's revenue will vary depending on the applied billing scenario. For this purpose, we propose a novel discrete event simulation model. The proposed simulation model is implemented using a newly created programming library based on the event scheduling strategy. This model can be helpful to an ISP in the process of decision making which billing scenario to choose in order to maximize its revenue. Output parameters obtained through the simulation analysis are ISP's revenue and service prices for different billing scenarios.																	1582-7445	1844-7600				FEB	2020	20	1					27	34		10.4316/AECE.2020.01004													
J								A New Approach for Estimating Insulation Condition of Field Transformers Using FRA	ADVANCES IN ELECTRICAL AND COMPUTER ENGINEERING										frequency response; insulation; insulation testing; power transformers	FREQUENCY-RESPONSE; DIAGNOSTICS	Frequency response analysis (FRA) is a tool for evaluating mechanical integrity of transformer's core and winding. However, several studies have reported that FRA is also sensitive to other parameters such as temperature, moisture content and presence of oil. Since some of these parameters are critical in the insulation degradation process, it is interesting to further investigate the possibility of using FRA for insulation assessment. In this paper, the insulation conditions of three field transformers are investigated using FRA. This is performed after statistical analysis on the response suggested no mechanical damage on the transformers. The responses are further analyzed to determine the percentage of change of winding capacitance. This is achieved by comparing resonance frequencies between the responses. The percentage of change represents the amount of degradation in the insulation. This is because as the insulation condition degrades, the response is shifted towards lower frequencies. This new investigation demonstrates the applicability of FRA to estimate the insulation degradation.																	1582-7445	1844-7600				FEB	2020	20	1					35	42		10.4316/AECE.2020.01005													
J								Coarse-to-fine Method for Vision-based Pedestrian Traffic Light Detection	ADVANCES IN ELECTRICAL AND COMPUTER ENGINEERING										gaussian mixture model; multi-layer neural network; boosting; object detection; computer vision	NEURAL-NETWORK; RECOGNITION	Pedestrian traffic light detection is an important technique of the navigation system for the visually impaired during road crossing. In this paper, a three-stage coarse-to-fine method for pedestrian traffic light detection is proposed. The proposed method is mainly divided into two processes, the training process and the detection process. In the training process, the Gaussian mixture model (GMM) is adopted to determine the parameters of the filter on stage I. The classifier on stage II is trained by a modified convolutional neural network (CNN) to capture features in each channel of the CIELAB color space. The classifier on stage III is trained by the adaptive boosting (AdaBoost) algorithm with Haar features. In the detection process, firstly the board filter is adopted to generate candidate regions of pedestrian traffic lights. Secondly, these candidate regions are detected in multiple scales by the CNN-based classifier with fixed size. Finally the AdaBoost-based classifier is adopted for refinement detection. Testing results verify the effectiveness of the proposed method.																	1582-7445	1844-7600				FEB	2020	20	1					43	48		10.4316/AECE.2020.01006													
J								An Improved Analytical Methodology for Joint Distribution in Probabilistic Load Flow	ADVANCES IN ELECTRICAL AND COMPUTER ENGINEERING										gaussian mixture model; maximum likelihood estimation; genetic algorithm; density function; distribution	SMALL-SIGNAL STABILITY; POWER-SYSTEMS; PENETRATION	This paper presents a novel analytical method based on improved Gaussian mixture model (GMM) to solve the probabilistic load flow problem. The proposed method accounts for the uncertainty introduced due to increasing percentages of renewable generation. First, the joint probability density function of several wind farms outputs is derived by using the improved GMM with the estimated parameters obtained by genetic algorithm (GA) in this paper, which could improve the accuracy of the probabilistic model. Next, the analytical expressions between the output power of wind farms and line power of power system are deduced by linearizing load flow equations. And, the joint probability density function and joint cumulative distribution function of line power are obtained from linear load equation and joint probability density function of wind output power. Finally, the proposed method, Monte Carlo simulation (MCS) and traditional GMM based methods are all tested on a modified IEEE 39-bus system and a modified IEEE 118-bus system with multiple wind farms, which demonstrates the feasibility of the proposed method.																	1582-7445	1844-7600				FEB	2020	20	1					49	56		10.4316/AECE.2020.01007													
J								Solid State Transformer for Connecting Consumers to the Medium Voltage Network	ADVANCES IN ELECTRICAL AND COMPUTER ENGINEERING										micro-grids; converter; power flow; SST electronic transformer; low-voltage stabilization	CONVERTER	In this paper the authors describe and analyze an innovative solution for the development of an electronic transformer (Solid State Transformer - SST) with the voltage 10.0/0.230 kV. The transformer is designed to provide direct power to low-voltage consumers from the medium voltage network with the 50 Hz frequency. The proposed transformer permits bidirectional energy exchange. In order to stabilize the low voltage output, an original method has been adopted to manage the parameters of the control pulses for the transistors of the SST inverter. The primary winding of the high frequency transformer consists of 16 coils connected separately by means of two transistors. The design simplification of the transformer leads to the increase of energy efficiency indicators of the transformer and helps to reduce the high harmonics of the voltage and current in the power distribution network with a positive impact on power quality. It becomes possible to use this equipment to connect renewable energy sources, for example, the so-called micro-grids, to centralized power network of medium voltage.																	1582-7445	1844-7600				FEB	2020	20	1					57	62		10.4316/AECE.2020.01008													
J								Hardware Real-time Event Management with Support of RISC-V Architecture for FPGA-Based Reconfigurable Embedded Systems	ADVANCES IN ELECTRICAL AND COMPUTER ENGINEERING										pipeline processing; field programmable gate arrays; architecture; operating systems; scheduling		Task context switching, unitary management of events, synchronization and communication mechanisms are significant problems for each real-time operating system. For real-time systems, another overhead factor is the processor's time to execute the routine of treating external asynchronous interrupts. The main objective of this paper is to describe, implement, and validate the preemptive scheduler module as part of the hardware accelerated real-time operating system, using the RISC-V instruction set and Verilog HDL. The new architecture contains the hardware structure used for static and dynamic scheduling of the tasks, real-time management of the events, and also defines a method used to attach interrupts to tasks. In order to accomplish this objective, it was necessary to structure CPU modules so as to ensure easy adaptation to other implementations (MIPS coprocessor, ARM or RISC-V).																	1582-7445	1844-7600				FEB	2020	20	1					63	70		10.4316/AECE.2020.01009													
J								Improved Edge Refinement Filter with Entropy Feedback Measurement for Retrieving Region of Interest and Blind Image Deconvolution	ADVANCES IN ELECTRICAL AND COMPUTER ENGINEERING										image restoration; image edge detection; deconvolution; filtering; image enhancement		This study proposes an improved edge refinement filter with entropy feedback measurement for locating an optimal region of interest (ROI) in blurry images. This technique is inspired by He et al.'s algorithm and enhanced by introducing a suitable filter to obtain smooth unwanted pixels whilst retaining important and significant edges. This approach led to an accurate retrieval of ROI and a considerably precise image restoration within a blind deconvolution framework. Results show that the proposed method is more competitive than existing techniques and achieves better performance in terms of peak signal-to-noise ratio, kernel similarity index and error ratio.																	1582-7445	1844-7600				FEB	2020	20	1					71	82		10.4316/AECE.2020.01010													
J								Exploiting the Inherent Connectivity of Urban Mobile Backbones Using the P-DSDV Routing Protocol	ADVANCES IN ELECTRICAL AND COMPUTER ENGINEERING										computer simulation; network topology; quality of service; routing protocols; vehicular ad hoc networks		Vehicular ad hoc networks (VANETs) are mobile networks where the communication is established among vehicles (V2V) and/or roadside units (V2I). In these networks, the main challenges of the communication are related to problems of connectivity, and the consequent worsening of the routing protocol's performance by the starting of a new route discovery procedure. Many studies claim that the use of fixed infrastructure with classic routing protocols may provide connectivity and allow the use of VANETs. However, high deployment and maintenance costs in these networks make them unpractical most of the times. In many big cities, public transport buses travel through exclusive lanes with relatively regular schedules. This fact can be used to establish a cheap and reliable wireless communication infrastructure (called MOB-NET). This paper proposes the P-DSDV, a proactive routing protocol which prioritizes the buses of MOB-NET. The P-DSDV considers a route selection metric which takes into account the characteristics of the mobile nodes. Simulation results indicate the benefits of the pair P-DSDVAVIOB-NET in networks with low connectivity (density <= 60 vehicles/km(2)). The average gains obtained were 85% in packet delivery rate and 60% in throughput.																	1582-7445	1844-7600				FEB	2020	20	1					83	90		10.4316/AECE.2020.01011													
J								Investigation on Performance of Controllers for Three Level PFC Converter for Wide Operating Range	ADVANCES IN ELECTRICAL AND COMPUTER ENGINEERING										fuzzy control; PI control; sliding mode control; switching converter; system performance	SLIDING-MODE CONTROL; FUZZY-LOGIC	Single phase single-stage three level AC-DC power factor correction converters are commonly used as switch mode power supplies in telecommunication systems to provide effective power conversion with good operating performance. This paper deals with the comparative analysis of the converter response with four different control techniques, namely proportional integral and derivative controller (PID), fuzzy Logic controller (FLC), sliding mode controller (SMC), and average current controller (ACC) for wide operating conditions. Necessary analytical equations are derived to observe the dynamic behavior of the converter. Simulations are done with MATLAB/Simulink, and the responses of the PID, FLC, SMC, and ACC are compared for line and load regulations. It is observed that the average current control technique yields better transient and dynamic response than the other three control techniques. The effective performance of the ACC is validated through experimentation.																	1582-7445	1844-7600				FEB	2020	20	1					91	98		10.4316/AECE.2020.01012													
J								Design Time Temperature Reduction in Mixed Polarity Dual Reed-Muller Network: a NSGA-II Based Approach	ADVANCES IN ELECTRICAL AND COMPUTER ENGINEERING										genetic algorithms; logic design; Pareto optimization; power dissipation; thermal analysis		Proposed work addresses the existing thermal problem of OR-XNOR based circuit by introducing design time thermal management technique at the logic level. The approach is used to reduce the peak temperature by eliminating local hotspots. In proposed thermal-aware synthesis, non-dominated sorting genetic algorithm-II (NSGA-II) based meta-heuristic search algorithm is used to select a suitable input polarity of Mixed Polarity Dual Reed-Muller Expansion (MPDRM) to reduce the power and power-density by optimizing the area sharing. A parallel tabular technique is used for input polarity conversion from Product-of-Sum (POS) to MPDRM function. Finally, the optimized solutions are implemented in the physical design level to obtain the actual values of area, power, and temperature. MCNC benchmark suit is considered for performance evaluation. A comparative study of the proposed approach with existing state-of-art algorithms such as fixed and mixed polarity Reed-Muller network is reported. A significant reduction in area occupancy, power dissipation, and peak temperature generation are reported.																	1582-7445	1844-7600				FEB	2020	20	1					99	104		10.4316/AECE.2020.01013													
J								Effects of risk attitudes and investment spillover on supplier encroachment	SOFT COMPUTING										Encroachment; Investments; Spillover effects; Confidence level; Uncertainty environment	PRICE-COMPETITION; CHAIN; CHANNEL; LEADERSHIP; RETAILERS; CONTRACT; COORDINATION; REVENUE; IMPACT; COST	With the development of e-commerce, a growing number of suppliers have begun to initially establish their own direct channels, competing with their retail channels. However, while this encroachment endows the suppliers with an efficient method to control downstream competition and total production output directly, it may hurt the retailer due to the loss of monopoly in the retail market. This inconsistency presents a difficulty in reaching equilibrium. In this paper, we focus on the combined effects of the risk attitudes and upstream production investment of supply chain members on supplier encroachment and verify the existence of "win-win" results for both supplier and retailer. We find that, while the two parties cannot simultaneously benefit from supplier encroachment in the absence of upstream investment, they can obtain a Pareto improvement from it in the presence of upstream investment and spillover effect. Regarding risk attitudes, we find that both the supplier and the retailer can reach agreement on the supplier encroachment in the case of a moderate confidence level. In other words, the not too risk-loving and not too risk-averse supply chain members are more likely to obtain a Pareto improvement.																	1432-7643	1433-7479				FEB	2020	24	4			SI		2395	2416		10.1007/s00500-018-03677-7													
J								Uncertain population model	SOFT COMPUTING										Uncertainty theory; Uncertain differential equation; Population model; Logistic model	VALUATION; OPTION; GROWTH	Considering that the population size is always influenced by various uncertain factors in varying environment, we present some new types of uncertain population models: uncertain population growth model and uncertain logistic population growth model which are described by uncertain differential equations. And some properties of these uncertain population models are discussed within the framework of uncertainty theory.																	1432-7643	1433-7479				FEB	2020	24	4			SI		2417	2423		10.1007/s00500-018-03678-6													
J								Uncertain pursuit-evasion game	SOFT COMPUTING										Uncertain differential equation; Differential game; Pursuit-evasion game; Riccati equation; Target interception problem	DIFFERENTIAL-GAMES	Pursuit-evasion game deals with the situation in which a pursuer tries to catch an evader. Taking into account the subjectivity of the players' strategies and the fact that the noise of system state does not obey the statistical regularity, this paper employs an uncertain differential equation to describe the dynamics of the pursuit-evasion system, and introduces an uncertain pursuit-evasion game. Within the framework of uncertain differential game theory, a solution for the uncertain pursuit-evasion game is derived via the corresponding Riccati equation. At last, as an application, a target interception problem is proposed.																	1432-7643	1433-7479				FEB	2020	24	4			SI		2425	2429		10.1007/s00500-018-03689-3													
J								Uncertain random shortest path problem	SOFT COMPUTING										Shortest path problem; Chance theory; Uncertain random variable; Uncertain random network	ENTROPY; MODEL	The shortest path is an important problem in network optimization theory. This paper considers the shortest path problem under the situation where weights of edges in a network include both uncertainty and randomness and focuses on the case that the weights of edges are expressed by uncertain random variables. Some optimization models based on chance theory are proposed in order to find the shortest path which fully reflects uncertain and random information. This paper proposes also an intelligent algorithm to calculate the shortest path for an uncertain random network. A numerical example is given to illustrate its effectiveness.																	1432-7643	1433-7479				FEB	2020	24	4			SI		2431	2440		10.1007/s00500-018-03714-5													
J								A novel uncertain bimatrix game with Hurwicz criterion	SOFT COMPUTING										Uncertainty theory; Uncertain game; Nash equilibrium; Hurwicz criterion	COALITIONAL GAME; FUZZY PAYOFFS	In an uncertain bimatrix game, different uncertain equilibrium strategies have been proposed based on different criterions, such as expected value criterion, optimistic value criterion and uncertain measure criterion. This paper further presents an uncertain bimatrix game with Hurwicz criterion and defines a new solution concept Hurwicz Nash equilibrium. Furthermore, its existence theorem is also proved, and a sufficient and necessary condition is presented to find the Hurwicz Nash equilibrium. Finally, an example is provided for illustrating the usefulness of Hurwicz Nash equilibrium.																	1432-7643	1433-7479				FEB	2020	24	4			SI		2441	2446		10.1007/s00500-018-03715-4													
J								A comparison of milestone contract and royalty contract under critical value criterion in R&D alliance	SOFT COMPUTING										Milestone contract; Royalty contract; Critical value criterion; Information asymmetry; R&D alliance	ADVERSE SELECTION; SUPPLY CHAIN; RISK; AGENCY; COMPENSATION; INCENTIVES; PROJECT; IMPACTS; DESIGN; MODELS	This paper attempts to better understand contract type and risk attitude in R&D alliance under information asymmetry, that is, how the marketer designs and chooses optimal contract between royalty contract and milestone contract and how the innovator's risk attitude and asymmetric information affect the marketer's optimal contract strategies and profits. We use principal-agent models to formulate the marketer's contracting problem under asymmetric information about the innovator's innovation expertise and unobservable efforts. We find that, compared to the case under full information in both contracting structures, the marketer should distort the commission rate upwards under dual asymmetric information when the innovator is risk averse or downwards to lower innovation-expertise and risk-loving innovator; nevertheless, the marketer should offer the first-best contract. Furthermore, investigating the impacts of information asymmetry on the marketer's profits under two information structures, we find that dual asymmetric information harms the marketer's profit, especially when the innovator's effort marginal efficiency is higher. Finally, by comparing the marketer's profits in two types of contracts, we give the specific regions that milestone contract or royalty contract benefits the marketer better under different information structures.																	1432-7643	1433-7479				FEB	2020	24	4			SI		2447	2462		10.1007/s00500-018-03727-0													
J								Benchmarking framework for command and control mission planning under uncertain environment	SOFT COMPUTING										Operational System of Systems (SoS); Command and control; Adaptive planning	NORMATIVE DESIGN; ORGANIZATIONAL DESIGN; CONGRUENT	As the core of the military information system, the command and control (C2) mission planning suffers from the high complexity, environmental uncertainty. To address this problem, many studies highlight the agility and resilience of C2-organizations and propose many solutions. However, there is no benchmark to compare these models and methods. In order to understand such organization's dynamic and emergence behaviors, this paper presents a benchmark framework of C2 decision-making under uncertainty environment. This is a basic case on multi-force joint operation. We present an optimization model and a horizon partition algorithm aimed to plan an optimal organizational structure with higher operational flexibility, low cost and high performance. Finally, we explore the main traditional models on the benchmark case. The result shows the proposed model is competitive under uncertain environment.																	1432-7643	1433-7479				FEB	2020	24	4			SI		2463	2478		10.1007/s00500-018-03732-3													
J								Static uncertain behavioral game with application to investment problem	SOFT COMPUTING										Behavioral game; Uncertainty theory; Nash equilibrium; Investment problem	ALLOCATION	Uncertain game considers situations in which payoffs are characterized by uncertain variables. This paper goes further by taking into account players' behaviors. For uncertain game with normal form, we define a new spectrum of uncertain behavioral game. Then, with the frame work of behavioral game theory and uncertainty theory, the expected Nash equilibrium is proposed. A necessary condition is provided in order to find the expected Nash equilibrium. Finally, an example is provided for illustrating purpose.																	1432-7643	1433-7479				FEB	2020	24	4			SI		2479	2485		10.1007/s00500-018-03737-y													
J								Internationalization strategy, social responsibility pressure and enterprise value	SOFT COMPUTING										Internationalization strategy; Social responsibility pressure; Enterprise value; Chinese enterprises	DIVERSIFICATION	Recent years has seen Chinese enterprises competing to broaden the scope of business through the internationalization strategy. The proliferation of the strategy is attributable to the favorable environment created by the Chinese government with the increasingly stringent requirements on corporate social responsibility. The external pressure of social responsibility has also stimulated the fulfillment of social responsibility within the enterprises. Against this backdrop, this paper constructs a mathematical model with the constraint of external social responsibility pressure and applies the model to examine the effect of the internationalization strategy of Chinese enterprises on enterprise value. The research finds that: First, the internationalization strategy enhances the enterprise value; the stakeholders will jointly promote the strategy once they are aware of its significance. Second, with the increase in the external pressure of social responsibility, the enterprises will promote enterprise value through the implementation of social responsibility. Third, the fulfillment of corporate social responsibility contributes to the proliferation of internationalization strategy and further enhances the long-term enterprise value. These modeling conclusions are validated through an empirical test. This research enriches the exploration into the internationalization strategy of enterprises in emerging markets and pioneers the study on the fulfillment of corporate social responsibility from the external angle.																	1432-7643	1433-7479				FEB	2020	24	4			SI		2487	2494		10.1007/s00500-018-3425-1													
J								Tail value-at-risk in uncertain random environment	SOFT COMPUTING										Risk analysis; Uncertainty theory; Chance theory; Tail value-at-risk		Chance theory is a rational tool to be used in the systems which contain not only uncertainty but also randomness. In this paper, the concept of tail value-at-risk in uncertain random risk analysis is proposed and some theorems are provided for its calculation. Moreover, the tail value-at-risk is applied as the right-tail in the parallel system, series system, standby system, k-out-of-n system and structural system.																	1432-7643	1433-7479				FEB	2020	24	4			SI		2495	2502		10.1007/s00500-018-3492-3													
J								Sustainability efficiency evaluation of seaports in China: an uncertain data envelopment analysis approach	SOFT COMPUTING										Seaport efficiency; Environmental sustainability; Social sustainability; Sustainability efficiency; Uncertain DEA model; Sensitivity and stability analysis	PORT; GAME; DEA; MODELS	Sustainability is regarded as achieving economic, environmental, and social dimensions simultaneously that support an organization for long-term competitiveness. Port sustainability has attracted increasing attention because it is related to the issues of climate change and public health and safety. Therefore, it is urgent to measure the sustainability of ports. However, some variables (for instance, air pollutants and the neighboring relationship with surrounding communities) cannot be measured precisely by collecting quantitative data. This led us to select 23 seaports of China and use uncertain variables and uncertain data envelopment analysis model to measure their sustainability efficiency. Moreover, we captured the quantity to be improved of each output. The results show that 14 seaports such as Shanghai Port and Qingdao Port are deemed to be inefficient in terms of their sustainability. And our results can identify whether economic, environmental, or social dimensions contribute to the sustainability inefficiency of each seaport. On the basis of the results, we point out the managerial implications and put forward measures toward enhancing the efficiency of seaports with respect to these two dimensions.																	1432-7643	1433-7479				FEB	2020	24	4			SI		2503	2514		10.1007/s00500-018-3559-1													
J								A comprehensive model for fuzzy multi-objective portfolio selection based on DEA cross-efficiency model	SOFT COMPUTING										Fuzzy portfolio selection; Data envelopment analysis; Cross-efficiency evaluation; Sharpe ratio; Multi-objective firefly algorithm	OPTIMIZATION MODEL; MUTUAL FUNDS; PERFORMANCE; VARIANCE; ALGORITHMS; RISK	In this paper, we discuss the fuzzy portfolio selection problems in multi-objective frameworks. A comprehensive model for multi-objective portfolio selection in fuzzy environment is proposed by incorporating mean-semivariance model and data envelopment analysis cross-efficiency model. In the proposed model, the cross-efficiency model is formulated within the framework of Sharpe ratio; bounds on holdings, and cardinality constraints are also considered. The nonlinear constrained multi-objective portfolio optimization problem cannot be efficiently solved by using traditional approaches. Thus, a multi-objective firefly algorithm is developed to solve the relevant model. Finally, an example verifies the validity of the proposed approaches.																	1432-7643	1433-7479				FEB	2020	24	4			SI		2515	2526		10.1007/s00500-018-3595-x													
J								Estimation of dynamic mixed double factors model in high-dimensional panel data	SOFT COMPUTING										Panel data; Dynamic mixed double factor model; Identification; GMM estimation; Cross section and Time series correlation	NUMBER; COMPONENTS; ARBITRAGE	This paper endeavors to develop some dimension reduction techniques in panel data analysis when the numbers of individuals and indicators are very large. We use principal component analysis method to represent a large number of indicators via minority common factors in the factor models. We propose the dynamic mixed double factor model (DMDFM for short) to reflect cross section and time series correlation with the interactive factor structure. DMDFM not only reduces the dimension of indicators but also deals with the time series and cross section mixed effect. Different from other models, mixed factor models have two styles of common factors. The regressors factors reflect common trend and the dimension reducing, while the error components factors reflect difference and weak correlation of individuals. The results of Monte Carlo simulation show that generalized method of moments estimators have good properties of unbiasedness and consistency. Simulation results also show that the DMDFM can improve the prediction power of the models effectively.																	1432-7643	1433-7479				FEB	2020	24	4			SI		2527	2541		10.1007/s00500-018-3603-1													
J								Uncertain Gompertz regression model with imprecise observations	SOFT COMPUTING										Regression analysis; Uncertainty theory; Uncertain variable; Residual; Confidence interval		Regression is widely applied in many fields. Regardless of the types of regression, we often assume that the observations are precise. However, in real-life circumstances, this assumption can only be met sometimes, which means the traditional regression methods can result in significant imprecise or biased predictions. Consequently, uncertain regression models might provide more accurate and meaningful results under these circumstances. In this article, we provide the residual analysis of uncertain Gompertz regression model, as well as the corresponding forecast value and confidence interval. Finally, we give a numerical example of uncertain Gompertz regression model.																	1432-7643	1433-7479				FEB	2020	24	4			SI		2543	2549		10.1007/s00500-018-3611-1													
J								The impact of online referral on brand market strategies with consumer search and spillover effect	SOFT COMPUTING										Store brand; Online referral; Consumer search; Spillover effect; Uncertain information	STORE-BRAND; SUPPLY CHAIN; RECRUITMENT PROBLEM; CHANNEL; MANUFACTURER; MODEL; PREFERENCES; SERVICE; ENTRY; TOO	This paper studies how online referral affects the brand market management and investigates the spillover conditions from national brand to store brand market. We develop the game models without and with online referral, and derive the equilibrium strategies under the uniform pricing strategy and the differential pricing strategy. The results show that as the brand awareness increases, the presence of online referral would reduce the national brand's selling price, market demand and profit, and raise the store brand's selling price and market demand. Moreover, the differential pricing strategy could be better for the store brand than the uniform pricing strategy. Under the differential pricing strategy, the infomediary decides a higher referral commission only when the brand awareness is lower. In addition, the results also demonstrate that as the brand awareness increases, the spillover condition would be reduced under the differential pricing strategy and show a first declining and then rising trend under the uniform pricing strategy. To mitigate the spillover effect, the national brand should reduce the consumer's search cost in the national brand market.																	1432-7643	1433-7479				FEB	2020	24	4			SI		2551	2565		10.1007/s00500-018-3661-4													
J								Price discrimination based on purchase behavior and service cost in competitive channels	SOFT COMPUTING										Price discrimination; Behavior-based pricing; Consumer cost-based pricing; Bricks and clicks channels; Competition	REWARD PROGRAMS; CUSTOMER; PRODUCT; PREFERENCES; DEMAND; MARKET	y With the recent emergence of cloud computing and big data technologies, collection of consumers' information is widespread. Retailers use consumers' purchase history and consumptive habits data to price discriminate between current and new, high-cost and low-cost consumers. We investigate behavior-based pricing (BBP) and consumers cost-based pricing (CCP) simultaneously in a competitive two-period market in which bricks and clicks retailers sell products to high-cost-type and low-cost-type consumers during two periods. We examine how the price discrimination affects the channel members' prices, market shares and profits. We find that dual price discriminations (BBP and CCP) decrease the service cost advantage retailer's profit, but increase the service cost disadvantage retailer's profit if the consumer's travel cost is low. Compared the market shares of retailers, it is interesting that a cost advantage retailer serves more type-H consumers under the case of BBP and CCP than other cases. In addition, our results illustrate that cost disadvantage retailers prefer to reward the current consumers in the second period. Additionally, we find that consumers may benefit from price discrimination that they face a lower price in the case of BBP and CCP than other cases under certain conditions. Even more egregious, the current type-H consumers served by the cost advantage retailer enjoy a lower price than the type-L consumers served by the competitor.																	1432-7643	1433-7479				FEB	2020	24	4			SI		2567	2588		10.1007/s00500-019-03760-7													
J								Green investment in a supply chain based on price and quality competition	SOFT COMPUTING										Green supply chain management; Price and quality-based competition; Green investment; Interaction between quality level and green level	CONSUMER ENVIRONMENTAL AWARENESS; CHANNEL; DIFFERENTIATION; COORDINATION; INFORMATION; RETAILERS; IMPACTS	Along with the significant improvement of environmental consciousness, consumers not only consider the price and quality level of products, but also pay more attention to their green level. In order to strengthen the competitive advantage, the manufacturers should consider the green level of products in addition to their price and the quality level. In this paper, we investigate the green investment of two competing manufacturers in a supply chain based on price and quality competition and analyze the effect of green investment on the quality level of the product. The research shows that the manufacturer is willing to make a green investment with a relatively low value of green sensitivity regardless of whether the manufacturer's rival makes a green investment. Further, we find that the profit of the manufacturer who makes a green investment is greater than the profit of the manufacturer who does not invest regardless of the market size. When both competing manufacturers make green investments, the profit of the manufacturer who is in a large potential market is higher than that of the manufacturer who is in a small potential market. While in a same potential market, the profits of the two competing manufacturers are the same. Finally, we conclude that the green investment counterintuitively will not always improve the quality level of the products.																	1432-7643	1433-7479				FEB	2020	24	4			SI		2589	2608		10.1007/s00500-019-03777-y													
J								A credibilistic failure indicator for modeling structural reliability design optimization	SOFT COMPUTING										Structural reliability; Credibilistic failure indicator; Limit-state function; Trapezoidal distribution	FUZZY; SYSTEMS	Structural reliability design optimization under epistemic uncertainty has attracted the attention of many researchers, which plays a pivotal role both in theory and engineering application. However, many traditional fuzzy reliability indicators are formulated by fuzzy measure without self-duality. For this reason, we reconsider structural system with fuzzy parameters, and a new credibilistic failure indicator (CFI) is presented based on self-dual credibility measure, which provides the exact expression of structural failure degree under fuzzy environment. Then, for the structure with fuzzy trapezoidal parameters, the explicit expressions of the CFI formulations are presented under fuzzy linear limit-state function and nonlinear limit-state function. Furthermore, CFI-based design optimization is formulated to obtain the optimal structural design under the given reliability level. Meanwhile, one theorem on the reliability constraint is provided to facilitate us to obtain the equivalent deterministic constraint of the reliability constraint. Finally, two illustrative examples are performed to demonstrate the efficiency of the proposed CFI formulation and the corresponding computational methods.																	1432-7643	1433-7479				FEB	2020	24	4			SI		2609	2615		10.1007/s00500-019-03781-2													
J								Green supply chain analysis under cost sharing contract with uncertain information based on confidence level	SOFT COMPUTING										Green supply chains; Channel coordination; Cost sharing contract; Uncertainty theory; Confidence level	COORDINATION; PERFORMANCE; FRAMEWORK	In this paper, we study supply chain coordination issues arising out of green supply chain consisting of a manufacturer and a retailer under cost sharing contract with uncertain information. Instead of expected utility maximization, we present an alternative decision rule based on confidence level, that is, both the manufacturer's and the retailer's aims are to maximize the potential incomes under their confidence levels. First, we obtain the equilibrium values for the decentralized and the centralized channel cases under the given confidence levels, then compare the equilibrium values between the decentralized channel case and the centralized channel case to motivate cost sharing contract framework. Second, we consider the retailer participates in the green channel and obtain that the manufacturer and the retailer incur higher profits in the cost sharing contract case than the decentralized supply chain case. Third, we propose a cost sharing contract between the players who bargain on the cost sharing parameter, and the contract benefits the manufacturer significantly through sharing of costs with the retailer.																	1432-7643	1433-7479				FEB	2020	24	4			SI		2617	2635		10.1007/s00500-019-03801-1													
J								Roman domination problem with uncertain positioning and deployment costs	SOFT COMPUTING										Roman-dominating set problem; Positioning and deployment costs; Integer linear programming; Uncertainty theory; Uncertain variable	SET	In a connected simple graph, the weighted Roman domination problem is considered at which the cost of positioning at each vertex is imposed in addition to the costs of potential deployments from a vertex to some of its neighboring vertices. Proper decision in practice is prone to a high degree of indeterminacy, mostly raised by unpredictable events that do not obey the rules and prerequisites of the probability theory. In this study, we model this problem with such assumptions in the context of the uncertainty theory initiated by Liu (Uncertainty theory. Studies in fuzziness and soft computing, Springer, Berlin, 2007). Two different optimization models are presented, and a concrete example is provided for illustrative purposes. Weaknesses of the probability theory and fuzzy theory in dealing with this problem are also mentioned in detail.																	1432-7643	1433-7479				FEB	2020	24	4			SI		2637	2645		10.1007/s00500-019-03811-z													
J								A transportation planning problem with transfer costs in uncertain environment	SOFT COMPUTING										Transportation problem; Transfer cost; Uncertainty theory; Uncertain variable	PROGRAMMING-MODELS	As a generalization of existing uncertain transportation models, this paper proposes a new uncertain transportation model with transfer costs, of which the demands and the transportation costs as well as the transfer costs are uncertain variables. The model is presented in a form with expected-value objective and chance constraints. Based on the operational laws of uncertain variables, the presented model is transformed into an equivalent crisp model. After that, a numerical experiment is performed to illustrate the application of the model.																	1432-7643	1433-7479				FEB	2020	24	4			SI		2647	2653		10.1007/s00500-019-03813-x													
J								Uncertain revised regression analysis with responses of logarithmic, square root and reciprocal transformations	SOFT COMPUTING										Uncertain revised regression models; Uncertain revised asymptotic regression models; Uncertain revised Michaelis-Menten kinetics regression models	TESTING STATISTICAL HYPOTHESES; FUZZY	Regression models are often called for to quantify relationships between the explanatory variables and the response variable. Mathematically, data should be collected and recorded as their true values, which turns out to be unrealistic for the real world. In this paper, we introduce uncertain variables to characterize such imprecise data, apply the most useful logarithmic, square root or reciprocal transformation to alleviate possible nonlinearity problems and estimate the disturbance terms for the obtained uncertain regression models, followed by confidence interval estimations and point predictions. For each type of models being proposed, namely the uncertain revised regression models, uncertain revised asymptotic regression models and uncertain revised Michaelis-Menten kinetics regression models, we give a numerical example, respectively, to illustrate our approach.																	1432-7643	1433-7479				FEB	2020	24	4			SI		2655	2670		10.1007/s00500-019-03821-x													
J								Green fresh product cost sharing contracts considering freshness-keeping effort	SOFT COMPUTING										Green supply chain; Green product; Freshness-keeping effort; Greenness improvement level	SUPPLY CHAIN; PERISHABLE PRODUCTS; MODEL; COORDINATION; DECISIONS; POLICIES	Nowadays, along with increased public demands on the high-quality green fresh product, the downstream retailer has to spend in the packaging and cold chain transportation and the upstream farmer needs to invest more in green fresh products producing. In this paper, we investigate a green fresh product supply chain problem, in which the retailer transports and sells green fresh products to the ultimate consumer while the farmer produces green fresh product through spending a greenness improvement investment and sells green fresh products to the retailer. Since the fresh product is perishable, the retailer needs to make a costly freshness-keeping effort. Obviously, the freshness-keeping effort, the price, and the greenness improvement level will affect the demand. Thus, to demonstrate the game structure between the retailer and the farmer, a decentralized model without cost sharing, a decentralized Stackelberg cost sharing model, and a Nash bargaining model with cost sharing are formulated. Results show that: (1) The equilibrium decisions under Stackelberg model with cost sharing are larger than that of the Nash bargaining model with cost sharing, while equilibrium decisions in the decentralized model without cost sharing are the least. (2) Both greenness improvement levels in Stackelberg cost sharing contract and Nash bargaining are greater than that in decentralized model without cost sharing. (3) The retailer's profits in Stackelberg cost sharing contract and Nash bargaining are larger than that in decentralized case without cost sharing, while the farmer's profits in Stackelberg cost sharing contract and Nash bargaining are larger than in decentralized model without cost sharing in certain conditions. Meanwhile, a numerical example is given to illustrate the results we obtained in the theoretical analysis process.																	1432-7643	1433-7479				FEB	2020	24	4			SI		2671	2691		10.1007/s00500-019-03828-4													
J								Incentive contracts of knowledge investment for cooperative innovation in project-based supply chain with double moral hazard	SOFT COMPUTING										Incentive contract; Knowledge investment; Cooperative innovation; Project-based supply chain; Double moral hazard	ORGANIZATIONS; PERFORMANCE	The degree of knowledge investment of partners is the key to success for cooperative innovation in project-based supply chain. The intangibility and unverifiability of their knowledge investment lead to the double moral hazard which will hinder the smooth progress of cooperative innovation. In order to stimulate the knowledge investment of partners for cooperative innovation in project-based supply chain, this paper has designed formal contract and relational contract of knowledge investment with principal-agent theory, and then the incentive effects of contracts are analyzed. We find that the formal contract cannot motivate their knowledge investment effectively; the degree of knowledge investment and the revenue of participants for cooperative innovation under the relational contract are not less than that under the formal contract for all discount rate; the incentive effect of the relational contract is getting more obvious as the discount rate increases. When the discount rate reaches a certain threshold value, the optimal degree of knowledge investment and revenue of participants for cooperative innovation can be achieved through the relational contract. At last, the effectiveness of the conclusions is verified through numerical example.																	1432-7643	1433-7479				FEB	2020	24	4			SI		2693	2702		10.1007/s00500-019-03894-8													
J								Bi-level programming problem in the supply chain and its solution algorithm	SOFT COMPUTING										Decentralized supply chain; Bi-level linear programming; Budget constraint; Particle swarm optimization algorithm	PARTICLE SWARM OPTIMIZATION; GENETIC ALGORITHM; DISTRIBUTION-SYSTEM; HYBRID; MODEL; DECOMPOSITION; STRATEGIES; FRAMEWORK; SELECTION; DESIGN	Enterprise-wide supply chain planning problems naturally exhibit a multi-level decision network structure, where the upper level of a hierarchy may have his objective function and decision space partly determined by other levels. In addition, each planner's control instruments may allow him to influence the policies at other levels and thereby to improve his own objective function. As a tool, bi-level programming is applied for modeling decentralized decisions in which two decision makers make decisions successively. In this paper, we specifically address bi-level decision-making problems with budget constraint as an attractive feature in the context of enterprise-wide supply chain. We first describe the typical bi-level linear programming problem (BLLPP) and its optimal solution to the penalty function problem, and then, a cooperative decision-making problem in supply chain is modeled as BLLPP. A particle swarm optimization-based computational algorithm is designed to solve the problem, and the numerical example is presented to illustrate the proposed framework.																	1432-7643	1433-7479				FEB	2020	24	4			SI		2703	2714		10.1007/s00500-019-03930-7													
J								Uncertain Johnson-Schumacher growthmodel with imprecise observations and k-fold cross-validation test	SOFT COMPUTING										Regression analysis; Johnson-Schumacher growth model; Uncertainty theory; Uncertain variable; k-fold cross-validation		Regression is a powerful tool to study how the response variables vary due to changes of explanatory variables. Unlike traditional statistics or mathematics where data are assumed fairly accurate, we notice that the real-world data are messy and obscure; thus, the uncertainty theory seems more appropriate. In this paper, we focus on the residual analysis of the Johnson-Schumacher growth model, with parameter estimation performed by the least squares method, followed by the prediction intervals for new explanatory variables. We also propose a k-fold cross-validation method for model selection with imprecise observations. A numerical example illustrates that our approach will achieve better prediction accuracy.																	1432-7643	1433-7479				FEB	2020	24	4			SI		2715	2720		10.1007/s00500-019-04090-4													
J								Analytic solution of uncertain autoregressive model based on principle of least squares	SOFT COMPUTING										Uncertain autoregressive model; Uncertain variable; Principle of least squares	DEA MODEL; GAME	Uncertain time series is a method to predict future values based on previously uncertain observed values, which is firstly proposed by Yang and Liu (Fuzzy Optim Decis Mak, 2019. https:// doi.org/10.1007/s10700-018-9298-z). This paper continues to study a special uncertain time series-uncertain autoregressive model, and gives an analytic solution of uncertain autoregressive model based on the principle of least squares. Moreover, this paper proves another equivalent form to calculate the unknown parameters of uncertain autoregressive model via uncertainty distribution and also analyzes the disturbance term via uncertainty distribution.																	1432-7643	1433-7479				FEB	2020	24	4			SI		2721	2726		10.1007/s00500-019-04128-7													
J								A simple differential evolution with time-varying strategy for continuous optimization	SOFT COMPUTING										Differential evolution; Time-varying strategy; Evolutionary algorithm; Continuous optimization	MUTATION; ALGORITHM	We propose a novel simple variant of differential evolution (DE) algorithm and call it TVDE because it is a time-varying strategy-based DE algorithm. In our TVDE, three functions with time-varying characteristics are applied to create a new mutation operator and automatically tune the values of two key control parameters (scaling factor and crossover rate) during the evolutionary process. To verify its availability, the proposed TVDE has been tested on the CEC 2014 benchmark sets and four real-life problems and compared to seven state-of-the-art DE variants. The experimental results indicate that the proposed TVDE algorithm obtains the best overall performance among the eight DE algorithms.																	1432-7643	1433-7479				FEB	2020	24	4			SI		2727	2747		10.1007/s00500-019-04159-0													
J								A distributionally robust optimization model for designing humanitarian relief network with resource reallocation	SOFT COMPUTING										Humanitarian relief; Network design; Resource reallocation; Distributionally robust optimization; Ambiguity set	LAST-MILE DISTRIBUTION; SUPPLY-CHAIN; FACILITY LOCATION; LOGISTICS NETWORK; DISASTER; DEMAND; TRANSPORTATION; CONSTRAINTS	Since natural disasters often cause the loss of lives and property, it is necessary to design a reasonable relief network to distribute relief supplies after a disaster. The shortage of relief supplies and timely transportation are the main difficulties in the whole relief network. To overcome these difficulties, we introduce a three-level humanitarian relief network design problem with resource reallocation of relief supplies. Based on the existing relief network system before a disaster, this problem determines the positions of candidate local distribution centers and points of distribution, while considering the relief distribution in the network under an uncertain post-disaster environment. For the uncertain variables, we consider a distributionally robust model with mean absolute semi-deviation (MASD) as a risk measure taken into the transportation time function. Under the partial probability distribution information of the uncertainties, we deduce a tractable framework of the distributionally robust model. Specifically, we derive the worse case form of the MASD objective under an ambiguity set and the safe approximations of the chance constraints under a box+ellipsoid+generalized budget perturbation set. Finally, we demonstrate the efficacy of the model by a real case in Anambra flood.																	1432-7643	1433-7479				FEB	2020	24	4			SI		2749	2767		10.1007/s00500-019-04362-z													
J								Multi-period multi-scenario optimal design for closed-loop supply chain network of hazardous products with consideration of facility expansion	SOFT COMPUTING										Closed-loop supply chain network design; Hazardous products; Multiple periods; Facility expansion	LOGISTICS NETWORK; MODEL; LOCATION; INVENTORY; ALGORITHM; DEMAND; ROBUST	Increased concern for the environment has led to urgent need to design the closed-loop supply chain network of hazardous products economically and ecologically. In this paper, we focus on the optimal design problem of multi-period closed-loop supply chain network of hazardous products considering uncertain demands and returns, expandable facility capacities and social acceptable risk simultaneously. In each period, the built facilities have the ability to expand within a certain scope. The problem is formulated as a mixed-integer nonlinear programming model, which can determine the number, location and expansion scale of the facilities and the forward and reverse logistics quantities between the facilities in each period simultaneously. The goal is to minimize the expected cost over the multi-period planning horizon, including the facilities building, operating and expansion costs and the costs related to manufacturing, collection, processing, recovery and transportation. In order to solve the proposed model, two classes of dummy variables are introduced to equivalently transform it into a mixed-integer linear programming, which can be optimally solved by LINGO. A case study is presented to illustrate the validity of the proposed model. The dynamic design with expansion strategy addressed in this paper is compared with two different strategies of static design and dynamic design without expansion. The results highlight that the dynamic design with expansion strategy has the advantages in saving expenses and raising the average expected collection rate of hazardous wastes.																	1432-7643	1433-7479				FEB	2020	24	4			SI		2769	2780		10.1007/s00500-019-04435-z													
J								A hybrid combinatorial approach to a two-stage stochastic portfolio optimization model with uncertain asset prices	SOFT COMPUTING										Hybrid algorithm; Combinatorial approach; Stochastic programming; Population-based incremental learning; Local search; Learning inheritance; Portfolio optimization problem	INTEGER PROGRAMMING APPROACH; VALUE-AT-RISK; CONDITIONAL VALUE; ROBUST OPTIMIZATION; TRANSACTION COSTS; SELECTION; STOCK; RETURNS; SENSITIVITY; INVESTMENT	Portfolio optimization is one of the most important problems in the finance field. The traditional Markowitz mean-variance model is often unrealistic since it relies on the perfect market information. In this work, we propose a two-stage stochastic portfolio optimization model with a comprehensive set of real-world trading constraints to address this issue. Our model incorporates the market uncertainty in terms of future asset price scenarios based on asset return distributions stemming from the real market data. Compared with existing models, our model is more reliable since it encompasses real-world trading constraints and it adopts CVaR as the risk measure. Furthermore, our model is more practical because it could help investors to design their future investment strategies based on their future asset price expectations. In order to solve the proposed stochastic model, we develop a hybrid combinatorial approach, which integrates a hybrid algorithm and a linear programming (LP) solver for the problem with a large number of scenarios. The comparison of the computational results obtained with three different metaheuristic algorithms and with our hybrid approach shows the effectiveness of the latter. The superiority of our model is mainly embedded in solution quality. The results demonstrate that our model is capable of solving complex portfolio optimization problems with tremendous scenarios while maintaining high solution quality in a reasonable amount of time and it has outstanding practical investment implications, such as effective portfolio constructions.																	1432-7643	1433-7479				FEB	2020	24	4			SI		2809	2831		10.1007/s00500-019-04517-y													
J								The stability analysis for uncertain heat equations based on p-th moment	SOFT COMPUTING										Uncertain process; Uncertain heat equation; Stability in p-th moment	UNIQUENESS THEOREM; EXISTENCE	Stability in p-th moment plays a vital role in uncertain heat equation (UHE). However, little is known about the definition and properties of stability in p-th moment for UHE. This paper fills this gap and advances the concept of stability in p-th moment for UHE. Based on Markov inequality, this study shows the relationship between stability in p-th moment and stability in measure, that is if an UHE is stable in p-th moment, then it is stable in measure, but not vice versa. Our analysis further reveals that if the coefficients of UHE satisfy strong Lipschitz condition, and meanwhile, the Lipschitz coefficients meet some integral constraints, then the UHE is stable in p-th moment. This study provides a strong theoretically foundation for understanding the stability in p-th moment of UHE.																	1432-7643	1433-7479				FEB	2020	24	4			SI		2833	2839		10.1007/s00500-019-04529-8													
J								Human performance modeling and its uncertainty factors affecting decision making: a survery	SOFT COMPUTING										Human factors; Human performance modeling; Uncertainty decision making; Cognitive model; Integrated models of HPM; Military applications	LEVEL; METAANALYSIS; INFORMATION; JUDGMENT; OPERATOR; TIME	This paper introduces the background and connotation of human performance modeling (HPM), HPM models, and the application of artificial intelligence algorithms in HPM. It deeply analyzes the connotation and uncertainty of each model and finally puts forward its military application. The aim is to provide relevant researchers in the field with an in-depth understanding of domain knowledge and related uncertainties and to indicate future research directions. The first part is a general overview of human factors engineering, where the definition, origin, research field, importance, and general problems of HPM are elaborated. The composition of the man-machine system and its corresponding relationship with the observe-orient-decide-act loop are described. The second part reviews the models of perception, cognition, understanding, and decision making. Among them, models of cognition consist of visual search, visual sampling, mental workload, and goals, operators, methods, and selection rules; models of action consist of Hick-Hyman law, Fitts's law, and manual control theory. The third part is a review of the source and importance of the integrated models and focuses on the principles, composition, and successful application cases of the three models, namely SAINT, IMPRINT, and ACT-R. The fourth part is a review of the application of the algorithms and models in the fields of artificial intelligence, deep learning, and data mining in analyzing multivariate datasets in HPM. In addition, future HPM military applications are presented.																	1432-7643	1433-7479				FEB	2020	24	4			SI		2851	2871		10.1007/s00500-019-04659-z													
J								Global well-posedness for the nonlinear damped wave equation with logarithmic type nonlinearity	SOFT COMPUTING										Damped nonlinear wave equations; Logarithmic nonlinearity; Global existence; Asymptotic behavior	TIME BLOW-UP; POTENTIAL WELLS; CAUCHY-PROBLEM; DECAY-RATE; EXISTENCE	The initial boundary value problem for the nonlinear wave equations with damping and logarithmic nonlinearity is investigated in this paper. By making use of modified potential well theory and the technique of Logarithmic-Sobolev inequality, we establish global existence as well as asymptotic behavior of solution, under the assumption that the initial energy is small. Moreover, we obtain an exponential decay which is much faster than the decay in polynomial nonlinear case of Gazzola and Squassina (Ann I H Poincare AN 23:185-207, 2006). These results generalize and extend work in application of potential well theory to wave equations.																	1432-7643	1433-7479				FEB	2020	24	4			SI		2873	2885		10.1007/s00500-019-04660-6													
J								A spatial oligopolistic electricity model under uncertain demands	SOFT COMPUTING										Uncertainty theory; Variational inequality; Oligopoly; Cournot competition; Electricity market	SUPPLY FUNCTION EQUILIBRIA	This paper mainly investigates a single-period spatial oligopolistic model of electricity generators under uncertain demands. We maximize the beta-optimistic value of each generator's uncertain profit function in this game. Emphasis is first put on computing the beta-optimistic value of the uncertain profit function through the inverse uncertainty distribution of the uncertain variable in the inverse demand function. We later focus on transforming the single-period spatial oligopolistic game, which is a generalized Nash equilibrium (short for GNE) problem, into a variational inequality problem. Then the model is applied to simulate the transmission price of the electricity system of America, which reveals that our model is effective.																	1432-7643	1433-7479				FEB	2020	24	4			SI		2887	2895		10.1007/s00500-019-04665-1													
J								A new framework for reliable control placement in software-defined networks based on multi-criteria clustering approach	SOFT COMPUTING										Software-defined networks; Clustering; Multi-criteria decision-making; Controller placement problem; Controller placement framework; Non-dominated sorting moth flame controller placement optimizer	SDN; ALGORITHMS; MODEL	In large-scale software-defined networks (SDNs), multiple controllers are deployed. Each controller has a logically centralized vision of the network that is used to manage a set of switches. In SDN, a challenge known as controller placement problem arises which is very important to specify the number of controllers that are needed and where they should be deployed. These specifications affect the performance of the network. Meanwhile, the assignment of switches to the controllers plays key role in the quality of solution in this problem. However, recent studies focus more on simply assigning switches to their closest controllers based on propagation delay between controller and switches. In this paper, a new controller placement framework is designed which considers both control plane architecture and relation between control and data planes. This framework is considered as a multi-objective optimization model with two objective functions to minimize the flow setup time and inter-controller latency. Furthermore, we propose a new model for flow setup time function that considers all affected metrics in the placement. To solve the framework, a multi-objective algorithm called non-dominated sorting moth flame controller placement optimizer is designed. To this end, we adapt the best-worst multi-criteria decision-making method considering three metrics, namely hop count, propagation latency and link utilization, to assign switches to controllers. A heuristic approach is also used to assign a path between switch and its controller using above three metrics and path reliability. We run theses three models iteratively to find the best location for controllers, the best switch assignment to controller and also find the best route in the network. We compare our proposed framework with other models using expected path loss and link load balancing metrics. Our performance evaluations on real wide area network topologies show the efficiency of the proposed framework.																	1432-7643	1433-7479				FEB	2020	24	4			SI		2897	2916		10.1007/s00500-019-04070-8													
J								Analysis of semi-asynchronous multi-objective evolutionary algorithm with different asynchronies	SOFT COMPUTING										Asynchronous evolutionary algorithm; Multi-objective optimization; Asynchrony; NSGA-II; NSGA-III	NONDOMINATED SORTING APPROACH; OPTIMIZATION ALGORITHM; PARALLELIZATION	This paper proposes a novel master-slave parallel evolutionary algorithm (EA) approach with different asynchrony and provides its detailed analyses on multi-objective optimization problems. We express the proposed EA with different asynchrony as a semi-asynchronous EA. A semi-asynchronous EA generates new solutions whenever evaluations of the predefined number of solutions complete, unlike a conventional synchronous EA waits for evaluations of all solutions to generate the next population. To establish a semi-asynchronous EA, this paper introduces an asynchrony parameter that is used to decide how many solutions are waited to generate new solutions. We conduct an experiment to verify the effectiveness of the proposed semi-asynchronous EA on benchmark problems with the several variances of the evaluation time. In the experiment, we apply a semi-asynchronous EA to NSGA-II and NSGA-III, which are well-known multi-objective EAs. The semi-asynchronous NSGA-IIs and the semi-asynchronous NSGA-IIIs with different asynchronies are compared on multi-objective optimization benchmark problems. The experimental result reveals that semi-asynchronous approaches with an appropriate asynchrony have possibility to outperform the asynchronous and the synchronous ones. Additionally, detailed analysis reveals that an appropriate asynchrony may vary not only depends on a target problem but also depends on the degree of the evolution process.																	1432-7643	1433-7479				FEB	2020	24	4			SI		2917	2939		10.1007/s00500-019-04071-7													
J								A multi-start ILS-RVND algorithm with adaptive solution acceptance for the CVRP	SOFT COMPUTING										Capacitated vehicle routing problem; Iterated local search; Random variable neighborhood descent; Adaptive acceptance function; Multi-start; Hybrid metaheuristic	VEHICLE-ROUTING PROBLEM; ITERATED LOCAL SEARCH; OPTIMIZATION; DELIVERY	This study proposes a novel hybrid algorithm based on Iterated Local Search (ILS) and Random Variable Neighborhood Descent (RVND) metaheuristics for the purpose of solving the Capacitated Vehicle Routing Problem (CVRP). The main contribution of this work is that two new search rules have been developed for multi-starting and adaptive acceptance strategies, and applied together to enhance the power of the algorithm. A comprehensive experimental work has been conducted on two common CVRP benchmarks. Computational results demonstrate that both multi-start and adaptive acceptance strategies provide a significant improvement on the performance of pure ILS-RVND hybrid. Experimental work also shows that our algorithm is highly effective in solving CVRP and comparable with the state of the art.																	1432-7643	1433-7479				FEB	2020	24	4			SI		2941	2953		10.1007/s00500-019-04072-6													
J								Dense adaptive cascade forest: a self-adaptive deep ensemble for classification problems	SOFT COMPUTING										Deep forest; Ensemble; Deep learning; Boosting; Dense connectivity	NEURAL-NETWORKS; ALGORITHM; GAME; GO	Recent researches have shown that deep forest ensemble achieves a considerable increase in classification accuracy compared with the general ensemble learning methods, especially when the training set is small. In this paper, we take advantage of deep forest ensemble and introduce the dense adaptive cascade forest (daForest). Our model has a better performance than the original cascade forest with three major features: First, we apply SAMME.R boosting algorithm to improve the performance of the model. It guarantees the improvement as the number of layers increases. Second, our model connects each layer to the subsequent ones in a feed-forward fashion, which enhances the capability of the model to resist performance degeneration. Third, we add a hyper-parameter optimization layer before the first classification layer, making our model spend less time to set up and find the optimal hyper-parameters. Experimental results show that daForest performs significantly well and, in some cases, even outperforms neural networks and achieves state-of-the-art results.																	1432-7643	1433-7479				FEB	2020	24	4			SI		2955	2968		10.1007/s00500-019-04073-5													
J								Teaching-learning-based optimisation algorithm and its application in capturing critical slip surface in slope stability analysis	SOFT COMPUTING										Teaching-learning-based optimisation; Landslide; Slope stability analysis; Critical failure surface; Swarm intelligence	CRITICAL FAILURE SURFACE; LIMIT EQUILIBRIUM; DESIGN; LOCATION; SEARCH	The identification of ideal values for algorithm-specific parameters required for the functioning of metaheuristic approaches at their optimum performance is a difficult task. This paper presents the application of a recently proposed teaching-learning-based optimisation (TLBO) algorithm to determine the lowest factor of safety (FS) along a critical slip surface for soil slope. TLBO is a nature-inspired search algorithm based on the teaching-learning phenomenon of a classroom. Four benchmark slopes are reanalysed to test the performance of the TLBO approach. The results indicate that the present technique can detect the critical failure surface and can be easily implemented by practitioners without fine-tuning the parameters that affect the convergence of results. Statistical analyses indicate a drastic decrease in uncertainty and the number of function evaluations in the estimation of the FS over previous approaches.																	1432-7643	1433-7479				FEB	2020	24	4			SI		2969	2982		10.1007/s00500-019-04075-3													
J								A minimum entropy deconvolution-enhanced convolutional neural networks for fault diagnosis of axial piston pumps	SOFT COMPUTING										Axial piston pumps; Minimum entropy deconvolution; Convolutional neural network; Fault classification	ROLLING ELEMENT BEARING; STOCHASTIC RESONANCE; WAVELET PACKET; IDENTIFICATION; REGRESSION; SYSTEM	Condition monitoring of piston pumps has great significance to ensure the reliability and security of hydraulic systems. However, the complex working conditions of the integrated electromechanical systems make the fault mechanism unclear which is difficult for fault diagnosis by feature matching techniques. In this paper, a novel minimum entropy deconvolution (MED)-based convolutional neural network (CNN) is presented to classify faults in axial piston pumps. Firstly, the collected raw signals are preprocessed using the MED technique. Then, the filtered signals are used to construct training samples and testing samples. Finally, the constructed samples are fed into the CNN to classify the multi-faults of axial piston pumps. With the convolution and subsampling operations, the present model can automatically obtain data features via iterative learning processes, which is suitable for the unknown fault mechanism problems. The learned features are visualized by t-distributed stochastic neighbor embedding technique. A benchmark data simulation of mechanical transmission systems and an experimental data investigation of an axial piston pump are performed to manifest the superiority of the present method by comparing with the traditional CNN.																	1432-7643	1433-7479				FEB	2020	24	4			SI		2983	2997		10.1007/s00500-019-04076-2													
J								Multi-objective optimal power flow solutions using a constraint handling technique of evolutionary algorithms	SOFT COMPUTING										Multi-objective optimal power flow; Cost; Emission; MOEA/D algorithm; Superiority of feasible solutions (SF)	IMPERIALIST COMPETITIVE ALGORITHM; OPTIMIZATION; EMISSION; MOEA/D; COST; DECOMPOSITION; LOSSES; WIND	In power systems, optimal power flow (OPF) is a complex and constrained optimization problem in which quite often multiple and conflicting objectives are required to be optimized. The traditional way of dealing with multi-objective OPF (MOOPF) is the weighted sum method which converts the multi-objective OPF into a single-objective problem and provides a single solution from the set of Pareto solutions. This paper presents MOOPF study applying multi-objective evolutionary algorithm based on decomposition (MOEA/D) where a set of non-dominated solutions (Pareto solutions) can be obtained in a single run of the algorithm. OPF is formulated with two or more objectives among fuel (generation) cost, emission, power loss and voltage deviation. The other important aspect in OPF problem is about satisfying power system constraints. As the search process adopted by evolutionary algorithms is unconstrained, for a constrained optimization problem like OPF, static penalty function approach has been extensively employed to discard infeasible solutions. This approach requires selection of a suitable penalty coefficient, largely done by trial-and-error, and an improper selection may often lead to violation of system constraints. In this paper, an effective constraint handling method, superiority of feasible solutions (SF), is used in conjunction with MOEA/D to handle network constraints in MOOPF study. The algorithm MOEA/D-SF is applied to standard IEEE 30-bus and IEEE 57-bus test systems. Simulation results are analyzed, especially for constraint violation and compared with recently reported results on OPF.																	1432-7643	1433-7479				FEB	2020	24	4			SI		2999	3023		10.1007/s00500-019-04077-1													
J								Fuzzy descriptive evaluation system: real, complete and fair evaluation of students	SOFT COMPUTING										Fuzzy logic; Descriptive evaluation of students; Linguistic variables; Fuzzy descriptive evaluation system (FDES)	BACK RELAXATION	In recent years, descriptive evaluation has been introduced as a new model for educational evaluation of Iranian students. The current descriptive evaluation method is based on four-valued logic. Assessing all students with only four values is led to a lack of relative justice and creation of unrealistic equality. Also, the complexity of the evaluation process in the current method increases teacher error's likelihood. As a suitable solution, in this paper, a fuzzy descriptive evaluation system has been proposed. The proposed method is based on fuzzy logic, which is an infinite-valued logic, and it can perform approximate reasoning on natural language propositions. By the proposed fuzzy system, student assessment is performed over the school year with infinite values instead of four values. In order to eliminate the diversity of assigned values to students, at the end of the school year, the calculated values for each student will be rounded to the nearest value of the four standard values of the current descriptive evaluation method. It can be implemented in an appropriate smartphone application, which makes it much easier for teachers to assess the educational process of students. In this paper, the evaluation process of the elementary third-grade mathematics course in Iran during the period from the beginning of the MEHR (the seventh month of Iran) to the end of BAHMAN (the eleventh month of Iran) is examined by the proposed system. To evaluate the validity of this system, the proposed method has been simulated in MATLAB software.																	1432-7643	1433-7479				FEB	2020	24	4			SI		3025	3035		10.1007/s00500-019-04078-0													
J								Multi-Objective Stochastic Fractal Search: a powerful algorithm for solving complex multi-objective optimization problems	SOFT COMPUTING										Multi-Objective Stochastic Fractal Search; Multi-objective optimization; Metaheuristic algorithm; Multi-criteria optimization; Engineering optimization	WATER CYCLE ALGORITHM; HEURISTIC OPTIMIZATION; MULTIPLE OBJECTIVES; DESIGN; MODEL; EMISSION; FLOW	Stochastic Fractal Search (SFS) is a novel and powerful metaheuristic algorithm. This paper presents a Multi-Objective Stochastic Fractal Search (MOSFS) for the first time, to solve complex multi-objective optimization problems. The presented algorithm uses an external archive to collect efficient Pareto optimal solutions during the optimization process. Using dominance rules, leader selection and grid mechanisms, MOSFS precisely approximates the true Pareto optimal front. The MOSFS is implemented on nine multi-objective benchmark functions (CEC 2009) with multimodal, convex, discrete and non-convex optimal Pareto fronts. Performance of the proposed algorithm is compared to well-known algorithms. In addition, different performance measures are considered to evaluate the convergence and coverage abilities of the algorithms including Inverted Generational Distance, Maximum Spread and Spacing. Furthermore, statistical analyses are utilized to determine the superior algorithm. The results revealed that the MOSFS performs significantly better than other algorithms in both convergence and coverage and it is able to approximate true Pareto front precisely. In the end, MOSFS is implemented to solve a real-world engineering design problem called welded beam design problem and efficiency of the algorithm is compared to recently developed algorithms. The results of simulations and the Wilcoxon rank-sum test showed that the MOSFS is able to provide the most promising Pareto front for the problem considering various performance metrics at a 95% confidence level.																	1432-7643	1433-7479				FEB	2020	24	4			SI		3037	3066		10.1007/s00500-019-04080-6													
J								Optimization of Many Objective Pickup and Delivery Problem with Delay Time of Vehicle Using Memetic Decomposition Based Evolutionary Algorithm	INTERNATIONAL JOURNAL ON ARTIFICIAL INTELLIGENCE TOOLS										Vehicle routing problem; pickup and delivery problem; many-objective problem; optimization; evolutionary computations; memetic I-DBEA	MULTIOBJECTIVE OPTIMIZATION; GENETIC ALGORITHM; PARETO	The pickup and delivery problem (PDP) is a very common and important problem, which has a large number of real-world applications in logistics and transportation. In PDP, customers send transportation requests to pick up an object from one place and deliver it to another place. This problem is under the focus of researchers since the last two decades with multiple variations. In the literature, different variations of PDP with different number of objectives and constraints have been considered. Depending on the number of objectives, multi and many-objective evolutionary algorithms have been applied to solve the problem and to study the conflicts between objectives. In this paper, PDP is formulated as a many-objective pickup and delivery problem (MaOPDP) with delay time of vehicle having six criteria to be optimized. To the best of our knowledge, this variation of PDP has not been considered in the literature. To solve the problem, this paper proposes a memetic I-DBEA (Improved Decomposition Based Evolutionary Algorithm), which is basically the modification of an existing many-objective evolutionary algorithm called I-DBEA. To demonstrate the superiority of our approach, a set of experiments have been conducted on a variety of small, medium and large-scale problems. The quality of the results obtained by the proposed approach is compared with five existing multi and many-objective evolutionary algorithms using three different multi-objective evaluation measures such as hypervolume (HV), inverted generational distance (IGD) and generational distance (GD). The experimental results demonstrate that the proposed algorithm has significant advantages over several state-of-the-art algorithms in terms of the quality of the obtained solutions.																	0218-2130	1793-6349				FEB	2020	29	1							2050003	10.1142/S0218213020500037													
J								Density-based Approach with Dual Optimization for Tracking Community Structure of Increasing Social Networks	INTERNATIONAL JOURNAL ON ARTIFICIAL INTELLIGENCE TOOLS										Community detection; social networks; tracking community structure; density		The rapid evolution of social networks in recent years has focused the attention of researchers to find adequate solutions for the management of these networks. For this purpose, several efficient algorithms dedicated to the tracking and the rapid detection of the community structure have been proposed. In this paper, we propose a novel density-based approach with dual optimization for tracking community structure of increasing social networks. These networks are part of dynamic networks evolving by adding nodes with their links. The local optimization of the density makes it possible to reduce the resolution limit problem generated by the optimization of the modularity. The presented algorithm is incremental with a relatively low algorithmic complexity, making it efficient and faster. To demonstrate the effectiveness of our method, we test it on social networks of the real world. The experimental results show the performance and efficiency of our algorithm measured in terms of modularity density, modularity, normalized mutual information, number of communities discovered, running time and stability of communities.																	0218-2130	1793-6349				FEB	2020	29	1							2050002	10.1142/S0218213020500025													
J								Drug-target Interaction Prediction by Metapath2vec Node Embedding in Heterogeneous Network of Interactions	INTERNATIONAL JOURNAL ON ARTIFICIAL INTELLIGENCE TOOLS										Drug-target interaction prediction; representation learning; metapath2vec; node embedding in heterogeneous network	INFORMATION	Drug discovery is a complicated, time-consuming and expensive process. The cost for each new molecular entity (NME) is estimated at $1.8 billion.(1) Furthermore, for a new drug to be FDA approved it often takes nearly a decade and approximately 20 new drugs being approved by the US Food and Drug Administration (FDA) each year.(2) Accurately predicting drug-target interactions (DTIs) by computational methods is an important area of drug research, which brings about a broad prospect for fast and low-risk drug development. By accurate prediction of drugs and targets interactions scientists can scale-down huge experimental space and reduce the costs and help to faster drug development as well as predicting the side effects and potential function of new drugs. Many approaches have been taken by researchers to solve DTI problem and enhance the accuracy of methods. State-of-the-art approaches are based on various techniques, such as deep learning methods-like stacked auto-encoder-, matrix factorization, network inference, and ensemble methods. In this work, we have taken a new approach based on node embedding in a heterogeneous interaction network to obtain the representation of each node in the interaction network and then use a binary classifier such as logistic regression to solve this prominent problem in the pharmaceutical industry. Most introduced network-based methods use a homogeneous network of interactions as their input data whereas in the real word problem there exist other informative networks to help to enhance the prediction and by considering the homogeneous networks we lose some precious network information. Hence, in this work, we have tried to work on the heterogeneous network and have improved the accuracy of methods in comparison to baseline methods.																	0218-2130	1793-6349				FEB	2020	29	1							2050001	10.1142/S0218213020500013													
J								Dynamic Hybrid Graph Matching for Unsupervised Video-based Person Re-identification	INTERNATIONAL JOURNAL ON ARTIFICIAL INTELLIGENCE TOOLS										Person re-identification; dynamic hybrid graph matching; unsupervised learning		Taking videos as nodes in a graph, graph matching is an effective technique for unsupervised video-based person re-identification (re-ID). However, most of existing methods are sensitive to noisy training data and mainly only focus on visual content relations between query and gallery videos, which may introduce large amount of false positives. To enhance the robustness to training data and alleviate the visual ambiguity, a Dynamic Hybrid Graph Matching (DHGM) method is proposed, which jointly considers both content and context information for person re-ID in an iterative manner. The content relations between video nodes are obtained by metric learning, based on which the context relation is acquired by encoding the bidirectional feature of each probe node relative to its graph neighbors. The model is iteratively updated during the process of graph construction for promoted distance measurement and further better matching performance. Experimental results on the PRID 2011 and iLIDS-VID datasets demonstrate the superiority of the DHGM.																	0218-2130	1793-6349				FEB	2020	29	1							2050004	10.1142/S0218213020500049													
J								A survey on semi-supervised learning	MACHINE LEARNING										Semi-supervised learning; Machine learning; Classification	UNLABELED DATA; RANDOM FOREST; MANIFOLD REGULARIZATION; CLASSIFICATION; ROBUST; ALGORITHM; MACHINE; SOFTWARE; GRAPH	Semi-supervised learning is the branch of machine learning concerned with using labelled as well as unlabelled data to perform certain learning tasks. Conceptually situated between supervised and unsupervised learning, it permits harnessing the large amounts of unlabelled data available in many use cases in combination with typically smaller sets of labelled data. In recent years, research in this area has followed the general trends observed in machine learning, with much attention directed at neural network-based models and generative learning. The literature on the topic has also expanded in volume and scope, now encompassing a broad spectrum of theory, algorithms and applications. However, no recent surveys exist to collect and organize this knowledge, impeding the ability of researchers and engineers alike to utilize it. Filling this void, we present an up-to-date overview of semi-supervised learning methods, covering earlier work as well as more recent advances. We focus primarily on semi-supervised classification, where the large majority of semi-supervised learning research takes place. Our survey aims to provide researchers and practitioners new to the field as well as more advanced readers with a solid understanding of the main approaches and algorithms developed over the past two decades, with an emphasis on the most prominent and currently relevant work. Furthermore, we propose a new taxonomy of semi-supervised classification algorithms, which sheds light on the different conceptual and methodological approaches for incorporating unlabelled data into the training process. Lastly, we show how the fundamental assumptions underlying most semi-supervised learning algorithms are closely connected to each other, and how they relate to the well-known semi-supervised clustering assumption.																	0885-6125	1573-0565				FEB	2020	109	2					373	440		10.1007/s10994-019-05855-6													
J								Skeleton Marching-based Parallel Vascular Geometry Reconstruction Using Implicit Functions	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Vascular geometric reconstruction; implicit modelling; parallel computing; high-performance; high-accuracy	3D; REPRESENTATION	Fast high-precision patient-specific vascular tissue and geometric structure reconstruction is an essential task for vascular tissue engineering and computer-aided minimally invasive vascular disease diagnosis and surgery. In this paper, we present an effective vascular geometry reconstruction technique by representing a highly complicated geometric structure of a vascular system as an implicit function. By implicit geometric modelling, we are able to reduce the complexity and level of difficulty of this geometric reconstruction task and turn it into a parallel process of reconstructing a set of simple short tubular-like vascular sections, thanks to the easy-blending nature of implicit geometries on combining implicitly modelled geometric forms. The basic idea behind our technique is to consider this extremely difficult task as a process of team exploration of an unknown environment like a cave. Based on this idea, we developed a parallel vascular modelling technique, called Skeleton Marching, for fast vascular geometric reconstruction. With the proposed technique, we first extract the vascular skeleton system from a given volumetric medical image. A set of sub-regions of a volumetric image containing a vascular segment is then identified by marching along the extracted skeleton tree. A localised segmentation method is then applied to each of these sub-image blocks to extract a point cloud from the surface of the short simple blood vessel segment contained in the image block. These small point clouds are then fitted with a set of implicit surfaces in a parallel manner. A high-precision geometric vascular tree is then reconstructed by blending together these simple tubular-shaped implicit surfaces using the shape-preserving blending operations. Experimental results show the time required for reconstructing a vascular system can be greatly reduced by the proposed parallel technique.																	1476-8186	1751-8520				FEB	2020	17	1			SI		30	43		10.1007/s11633-019-1189-4													
J								Output Feedback Stabilization for MIMO Semi-linear Stochastic Systems with Transient Optimisation	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Multi-input-multi-output (MIMO) stochastic systems; output feedback stabilisation; block backstepping; randomness attenuation; probabilistic decoupling; mutual information	MINIMUM ENTROPY CONTROL; NONLINEAR-SYSTEMS; FAULT-DETECTION; CONTROLLER-DESIGN; TRACKING	This paper investigates the stabilisation problem and consider transient optimisation for a class of the multi-input-multi-output (MIMO) semi-linear stochastic systems. A control algorithm is presented via an m-block backstepping controller design where the closed-loop system has been stabilized in a probabilistic sense and the transient performance is optimisable by optimised by searching the design parameters under the given criterion. In particular, the transient randomness and the probabilistic decoupling will be investigated as case studies. Note that the presented control algorithm can be potentially extended as a framework based on the various performance criteria. To evaluate the effectiveness of this proposed control framework, a numerical example is given with simulation results. In summary, the key contributions of this paper are stated as follows: 1) one block backstepping-based output feedback control design is developed to stabilize the dynamic MIMO semi-linear stochastic systems using a linear estimator; 2) the randomness and probabilistic couplings of the system outputs have been minimized based on the optimisation of the design parameters of the controller; 3) a control framework with transient performance enhancement of multi-variable semi-linear stochastic systems has been discussed.																	1476-8186	1751-8520				FEB	2020	17	1			SI		83	95		10.1007/s11633-019-1193-8													
J								Expression Analysis Based on Face Regions in Real-world Conditions	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Facial emotion analysis; face areas; class activation map; confusion matrix; concerned area	TEXTURE CLASSIFICATION; SCALE	Facial emotion recognition is an essential and important aspect of the field of human-machine interaction. Past research on facial emotion recognition focuses on the laboratory environment. However, it faces many challenges in real-world conditions, i.e., illumination changes, large pose variations and partial or full occlusions. Those challenges lead to different face areas with different degrees of sharpness and completeness. Inspired by this fact, we focus on the authenticity of predictions generated by different <emotion, region> pairs. For example, if only the mouth areas are available and the emotion classifier predicts happiness, then there is a question of how to judge the authenticity of predictions. This problem can be converted into the contribution of different face areas to different emotions. In this paper, we divide the whole face into six areas: nose areas, mouth areas, eyes areas, nose to mouth areas, nose to eyes areas and mouth to eyes areas. To obtain more convincing results, our experiments are conducted on three different databases: facial expression recognition + ( FER+), real-world affective faces database (RAF-DB) and expression in-the-wild (ExpW) dataset. Through analysis of the classification accuracy, the confusion matrix and the class activation map (CAM), we can establish convincing results. To sum up, the contributions of this paper lie in two areas: 1) We visualize concerned areas of human faces in emotion recognition; 2) We analyze the contribution of different face areas to different emotions in real-world conditions through experimental analysis. Our findings can be combined with findings in psychology to promote the understanding of emotional expressions.																	1476-8186	1751-8520				FEB	2020	17	1			SI		96	107		10.1007/s11633-019-1176-9													
J								Accurate Classification of EEG Signals Using Neural Networks Trained by Hybrid Population-physic-based Algorithm	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Brain-computer interface (BCI); classification; electroencephalography (EEG); gravitational search algorithm (GSA); multi-layer perceptron neural network (MLP-NN); particle swarm optimization	BIOGEOGRAPHY-BASED OPTIMIZER; BRAIN-COMPUTER INTERFACE; SINGLE-TRIAL EEG; MOTOR IMAGERY; EXTRACTION	A brain-computer interface (BCI) system is one of the most effective ways that translates brain signals into output commands. Different imagery activities can be classified based on the changes in mu and beta rhythms and their spatial distributions. Multi-layer perceptron neural networks (MLP-NNs) are commonly used for classification. Training such MLP-NNs has great importance in a way that has attracted many researchers to this field recently. Conventional methods for training NNs, such as gradient descent and recursive methods, have some disadvantages including low accuracy, slow convergence speed and trapping in local minimums. In this paper, in order to overcome these issues, the MLP-NN trained by a hybrid population-physics-based algorithm, the combination of particle swarm optimization and gravitational search algorithm (PSOGSA), is proposed for our classification problem. To show the advantages of using PSOGSA that trains NNs, this algorithm is compared with other meta-heuristic algorithms such as particle swarm optimization (PSO), gravitational search algorithm (GSA) and new versions of PSO. The metrics that are discussed in this paper are the speed of convergence and classification accuracy metrics. The results show that the proposed algorithm in most subjects of encephalography (EEG) dataset has very better or acceptable performance compared to others.																	1476-8186	1751-8520				FEB	2020	17	1			SI		108	122		10.1007/s11633-018-1158-3													
J								Image Encryption Application of Chaotic Sequences Incorporating Quantum Keys	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Logistic chaotic system; quantum key; nonlinear operation; sequence performance; image encryption algorithm	ALGORITHM; SCHEME; MAP	This paper proposes an image encryption algorithm LQBPNN (logistic quantum and back propagation neural network) based on chaotic sequences incorporating quantum keys. Firstly, the improved one-dimensional logistic chaotic sequence is used as the basic key sequence. After the quantum key is introduced, the quantum key is incorporated into the chaotic sequence by nonlinear operation. Then the pixel confused process is completed by the neural network. Finally, two sets of different mixed secret key sequences are used to perform two rounds of diffusion encryption on the confusing image. The experimental results show that the randomness and uniformity of the key sequence are effectively enhanced. The algorithm has a secret key space greater than 2(182). The adjacent pixel correlation of the encrypted image is close to 0, and the information entropy is close to 8. The ciphertext image can resist several common attacks such as typical attacks, statistical analysis attacks and differential attacks.																	1476-8186	1751-8520				FEB	2020	17	1			SI		123	138		10.1007/s11633-019-1173-z													
J								An Operator-based Nonlinear Vibration Control System Using a Flexible Arm with Shape Memory Alloy	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Operator theory; nonlinear control; stability; right coprime factorization; shape memory alloy; hysteresis characteristics	COMPOSITE BEAMS; DESIGN; TRACKING	In the past, arms used in the fields of industry and robotics have been designed not to vibrate by increasing their mass and stiffness. The weight of arms has tended to be reduced to improve speed of operation, and decrease the cost of their production. Since the weight saving makes the arms lose their stiffness and therefore vibrate more easily, the vibration suppression control is needed for realizing the above purpose. Incidentally, the use of various smart materials in actuators has grown. In particular, a shape memory alloy (SMA) is applied widely and has several advantages: light weight, large displacement by temperature change, and large force to mass ratio. However, the SMA actuators possess hysteresis nonlinearity between their own temperature and displacement obtained by the temperature. The hysteretic behavior of the SMA actuators affects their control performance. In previous research, an operator-based control system including a hysteresis compensator has been proposed. The vibration of a flexible arm is dealt with as the controlled object; one end of the arm is clamped and the other end is free. The effectiveness of the hysteresis compensator has been confirmed by simulations and experiments. Nevertheless, the feedback signal of the previous designed system has increased exponentially. It is difficult to use the system in the long-term because of the phenomenon. Additionally, the SMA actuator generates and radiates heat because electric current passing through the SMA actuator provides heat, and strain on the SMA actuator is generated. With long-time use of the SMA actuator, the environmental temperature around the SMA actuator varies through radiation of the heat. There exists a risk that the ambient temperature change dealt with as disturbance affects the temperature and strain of the SMA actuator. In this research, a design method of the operator-based control system is proposed considering the long-term use of the system. In the method, the hysteresis characteristics of the SMA actuator and the temperature change around the actuator are considered. The effectiveness of the proposed method is verified by simulations and experiments.																	1476-8186	1751-8520				FEB	2020	17	1			SI		139	150		10.1007/s11633-018-1149-4													
J								Parkinsonism Differently Affects the Single Neuronal Activity in the Primary and Supplementary Motor Areas in Monkeys: An Investigation in Linear and Nonlinear Domains	INTERNATIONAL JOURNAL OF NEURAL SYSTEMS										Brain; movement disorders; neurophysiology; primate; sensorimotor; Parkinson's disease	GLOBUS-PALLIDUS NEURONS; DEEP BRAIN-STIMULATION; BASAL GANGLIA; OSCILLATORY ACTIVITY; SUBTHALAMIC NUCLEUS; ACTIVITY PATTERNS; CEREBRAL-CORTEX; FIRING RATES; MEDIAL WALL; DISEASE	The changes in neuronal firing activity in the primary motor cortex (M1) and supplementary motor area (SMA) were compared in monkeys rendered parkinsonian by treatment with 1-methyl- 4-phenyl-1,2,3,6-tetrahydropyridine. The neuronal dynamic was characterized using mathematical tools defined in different frameworks (rate, oscillations or complex patterns). Then, and for each cortical area, multivariate and discriminate analyses were further performed on these features to identify those important to differentiate between the normal and the pathological neuronal activity. Our results show a different order in the importance of the features to discriminate the pathological state in each cortical area which suggests that the M1 and the SMA exhibit dissimilarities in their neuronal alterations induced by parkinsonism. Our findings highlight the need for multiple mathematical frameworks to best characterize the pathological neuronal activity related to parkinsonism. Future translational studies are warranted to investigate the causal relationships between cortical region-specificities, dominant pathological hallmarks and symptoms.																	0129-0657	1793-6462				FEB	2020	30	2							2050010	10.1142/S0129065720500100													
J								Scaling Analysis of Phase Fluctuations of Brain Networks in Dynamic Constrained Object Manipulation	INTERNATIONAL JOURNAL OF NEURAL SYSTEMS										EEG; phase characterization; functional brain networks; motor control; dynamic constraints	WAVELET-CHAOS METHODOLOGY; EEG-BASED DIAGNOSIS; SPECTRUM; SYNCHRONIZATION; FRACTALITY; PERFORMANCE; STATE	In this study, we investigated the dynamic properties of oscillatory activities in the scalp electroencephalographs (EEGs) of 20 participants involved in a novel dynamic manipulating task using a physical interface and a virtual feedback. The complexity of such a task a rises from the unexpected relationship between the magnitude of the motion and the feedback. The characterization of complex patterns arising from EEG is an important problem in identifying different mental intentions. We proposed a scaling analysis of phase fluctuation in the scalp EEG to discriminate the network states related to different EEG patterns, which correspond to manipulating the task with right or left movement intention. These intentions are generated while the participant is engaged in such a complex task. The phase characterization method was used to calculate the instantaneous phase from the operational EEG. Then, functional brain networks (FBNs) of 20 subjects based on the task-related EEG were constructed by phase synchronization. The degree features representing the structures and scaling components of brain networks are sensitive to the EEG patterns with left or right motor intention. The correlation between features and mental intentions was investigated by discriminant analysis. For 20 subjects, the average accuracy of state detection is 0.8541 +/- 0.0398, and the average mean-squared error (MSE) is 0.6036 +/- 0.1226. The brain state depicted by the results is related to high awareness, the phase characterization is of the effectiveness in EEG processing and FBN construction and the difference of control intentions can be explored by the phase characterization method. This finding may be relevant to understanding some neuronal mechanisms underlying the attention and some applications of closed-loop control for the safety operation of tools.																	0129-0657	1793-6462				FEB	2020	30	2							2050002	10.1142/S0129065720500021													
J								Controllability of Networks of Multiple Coupled Neural Populations: An Analytical Method for Neuromodulation's Feasibility	INTERNATIONAL JOURNAL OF NEURAL SYSTEMS										Neuromodulation's feasibility; controllability; complex network; the lumped-parameter model	TRANSCRANIAL MAGNETIC STIMULATION; DEEP BRAIN-STIMULATION; KALMAN FILTER CONTROL; SMALL-WORLD; FUNCTIONAL CONNECTIVITY; MATHEMATICAL-MODEL; SYNCHRONIZATION; GRAPH; COMPLEXITY; DYNAMICS	Neuromodulation plays a vital role in the prevention and treatment of neurological and psychiatric disorders. Neuromodulation's feasibility is a long-standing issue because it provides the necessity for neuromodulation to realize the desired purpose. A controllability analysis of neural dynamics is necessary to ensure neuromodulation's feasibility. Here, we present such a theoretical method by using the concept of controllability from the control theory that neuromodulation's feasibility can be studied smoothly. Firstly, networks of multiple coupled neural populations with different topologies are established to mathematically model complicated neural dynamics. Secondly, an analytical method composed of a linearization method, the Kalman controllable rank condition and a controllability index is applied to analyze the controllability of the established network models. Finally, the relationship between network dynamics or topological characteristic parameters and controllability is studied by using the analytical method. The proposed method provides a new idea for the study of neuromodulation's feasibility, and the results are expected to guide us to better modulate neurodynamics by optimizing network dynamics and network topology.																	0129-0657	1793-6462				FEB	2020	30	2							2050001	10.1142/S012906572050001X													
J								Small World Index in Default Mode Network Predicts Progression from Mild Cognitive Impairment to Dementia	INTERNATIONAL JOURNAL OF NEURAL SYSTEMS										EEG; graph theory; default mode network; mild cognitive impairment; neuropsychological test; Alzheimer's disease; dementia	GRAPH-THEORETICAL ANALYSIS; GAMMA-BAND ACTIVITY; DEEP BRAIN-STIMULATION; FUNCTIONAL CONNECTIVITY; ALZHEIMERS-DISEASE; ELECTROMAGNETIC TOMOGRAPHY; CORTICAL CONNECTIVITY; MEMORY PERFORMANCE; THETA OSCILLATIONS; ANTERIOR THALAMUS	Aim of this study was to explore the EEG functional connectivity in amnesic mild cognitive impairments (MCI) subjects with multidomain impairment in order to characterize the Default Mode Network (DMN) in converted MCI (cMCI), which converted to Alzheimer's disease (AD), compared to stable MCI (sMCI) subjects. A total of 59 MCI subjects were recruited and divided-after appropriate follow-up-into cMCI or sMCI. They were further divided in MCI with linguistic domain (LD) impairment and in MCI with executive domain (ED) impairment. Small World (SW) index was measured as index of balance between integration and segregation brain processes. SW, computed restricting to nodes of DMN regions for all frequency bands, evaluated how they differ between MCI subgroups assessed through clinical and neuropsychological four-years follow-up. In addition, SW evaluated how this pattern differs between MCI with LD and MCI with ED. Results showed that SW index significantly decreased in gamma band in cMCI compared to sMCI. In cMCI with LD impairment, the SW index significantly decreased in delta band, while in cMCI with ED impairment the SW index decreased in delta and gamma bands and increased in alpha1 band. We propose that the DMN functional alterations in cognitive impairment could reflect an abnormal flow of brain information processing during resting state possibly associated to a status of pre-dementia.																	0129-0657	1793-6462				FEB	2020	30	2							2050004	10.1142/S0129065720500045													
J								Non-Canonical Microstate Becomes Salient in High Density EEG During Propofol-Induced Altered States of Consciousness	INTERNATIONAL JOURNAL OF NEURAL SYSTEMS										Altered states of consciousness; high-density EEG; microstate; anesthesia; multivariate empirical mode decomposition	EMPIRICAL MODE DECOMPOSITION; SPONTANEOUS BRAIN ACTIVITY; WAVELET-CHAOS METHODOLOGY; FUNCTIONAL CONNECTIVITY; GENERAL-ANESTHESIA; MECHANISMS; NETWORK; SYNCHRONIZATION; DIAGNOSIS; SPECTRUM	Dynamically assessing the level of consciousness is still challenging during anesthesia. With the help of Electroencephalography (EEG), the human brain electric activity can be noninvasively measured at high temporal resolution. Several typical quasi-stable states are introduced to represent the oscillation of the global scalp electric field. These so-called microstates reflect spatiotemporal dynamics of coherent neural activities and capture the switch of brain states within the millisecond range. In this study, the microstates of high-density EEG were extracted and investigated during propofol-induced transition of consciousness. To analyze microstates on the frequency domain, a novel microstate-wise spectral analysis was proposed by the means of multivariate empirical mode decomposition and Hilbert-Huang transform. During the transition of consciousness, a map with a posterior central maximum denoted as microstate F appeared and became salient. The current results indicated that the coverage, occurrence, and power of microstate F significantly increased in moderate sedation. The results also demonstrated that the transition of brain state from rest to sedation was accompanied by significant increase in mean energy of all frequency bands in microstate F. Combined with studies on the possible cortical sources of microstates, the findings reveal that non-canonical microstate F is highly associated with propofol-induced altered states of consciousness. The results may also support the inference that this distinct topography can be derived from canonical microstate C (anterior-posterior orientation). Finally, this study further develops pertinent methodology and extends possible applications of the EEG microstate during propofol-induced anesthesia.																	0129-0657	1793-6462				FEB	2020	30	2							2050005	10.1142/S0129065720500057													
J								Head mouse control system for people with disabilities	EXPERT SYSTEMS										computer vision; convolutional neural network; deep learning; disabled people	CONVOLUTIONAL NEURAL-NETWORK; FACE RECOGNITION; COMPUTER MOUSE; EYE-TRACKING; INTERFACE; EMOTION	In this paper, a human-machine interface for disabled people with spinal cord injuries is proposed. The designed human-machine interface is an assistive system that uses head movements and blinking for mouse control. In the proposed system, by moving one's head, the user moves the mouse pointer to the required coordinates and then blinks to send commands. The considered head mouse control is based on image processing including facial recognition, in particular, the recognition of the eyes, mouth, and nose. The proposed recognition system is based on the convolutional neural network, which uses the low-quality images that are captured by a computer's camera. The convolutional neural network (CNN) includes convolutional layers, a pooling layer, and a fully connected network. The CNN transforms the head movements to the actual coordinates of the mouse. The designed system allows people with disabilities to control a mouse pointer with head movements and to control mouse buttons with blinks. The results of the experiments demonstrate that this system is robust and accurate. This invention allows people with disabilities to freely control mouse cursors and mouse buttons without wearing any equipment.																	0266-4720	1468-0394				FEB	2020	37	1			SI				e12398	10.1111/exsy.12398													
J								A knowledge construction methodology to automate case-based learning using clinical documents	EXPERT SYSTEMS										case-based learning; clinical case; controlled natural language; declarative knowledge; knowledge engineering; ontological model	SYSTEM; PHYSIOLOGY	The case-based learning (CBL) approach has gained attention in medical education as an alternative to traditional learning methodology. However, current CBL systems do not facilitate and provide computer-based domain knowledge to medical students for solving real-world clinical cases during CBL practice. To automate CBL, clinical documents are beneficial for constructing domain knowledge. In the literature, most systems and methodologies require a knowledge engineer to construct machine-readable knowledge. Keeping in view these facts, we present a knowledge construction methodology (KCM-CD) to construct domain knowledge ontology (i.e., structured declarative knowledge) from unstructured text in a systematic way using artificial intelligence techniques, with minimum intervention from a knowledge engineer. To utilize the strength of humans and computers, and to realize the KCM-CD methodology, an interactive case-based learning system(iCBLS) was developed. Finally, the developed ontological model was evaluated to evaluate the quality of domain knowledge in terms of coherence measure. The results showed that the overall domain model has positive coherence values, indicating that all words in each branch of the domain ontology are correlated with each other and the quality of the developed model is acceptable.																	0266-4720	1468-0394				FEB	2020	37	1			SI				e12401	10.1111/exsy.12401													
J								AUDIT: AnomaloUs data Detection and Isolation approach for mobile healThcare systems	EXPERT SYSTEMS										anomaly detection; data accuracy; dimension reduction; mobile health care systems; reduced complexities; spatio-temporal correlation	NETWORKS	Mobile health care systems highly depend on collected physiological data through medical sensors to provide high-quality care services. However, inaccurate physiological data from sensors pose a major challenge for health care providers when making decisions, whereas an erroneous decision can affect the user's life. We propose, in this paper, an anomalous data detection and isolation approach for mobile health care systems. Our approach, called AUDIT, detects inaccurate measurements in real time and distinguishes between faults or errors and health events. To do so, we propose reduced time and space complexities algorithms based on dimension reduction within the context of resource constraints. Furthermore, a decision algorithm is proposed while exploring the spatio-temporal correlation between physiological attributes. First, we describe our approach. Then, we give its implementation details. Finally, to demonstrate the effectiveness of our approach, we show different experiments related to its detection performances and its time and space complexities.																	0266-4720	1468-0394				FEB	2020	37	1			SI				e12390	10.1111/exsy.12390													
J								Improving privacy in health care with an ontology-based provenance management system	EXPERT SYSTEMS										health care; knowledge representation; ontology; privacy; provenance; Semantic Web	SEMANTIC WEB; MODEL	Provenanc refers to the origin of information. Therefore, provenance is the metadata that record the history of data. As provenance is the derivation history of an object starting from its original source, the provenance information is used to analyse processes that are performed on an object and to track by whom these processes are performed. Thus, provenance shows the trustworthiness and quality of data. In a provenance management system in order to verify the trustworthy of provenance information, security needs must be also fulfilled. In this work, an ontology-based privacy-aware provenance management model is proposed. The proposed model is based on the Open Provenance Model, which is a common model for provenance. The proposed model aims to detect privacy violations, to reduce privacy risks by using permissions and prohibitions, and also to query the provenance data. The proposed model is implemented with Semantic Web technologies and demonstrated for the health care domain in order to preserve patients' privacy. Also, an infectious disease ontology and a vaccination ontology are integrated to the system in order to track the patients' vaccination history, to improve the quality of medical processes, the reliability of medical data, and the decision making in the health care domain.																	0266-4720	1468-0394				FEB	2020	37	1			SI				e12427	10.1111/exsy.12427													
J								Converness: Ontology-driven conversational awareness and context understanding in multimodal dialogue systems	EXPERT SYSTEMS										conversational awareness; context fusion; defeasible rules; DL reasoning; ontologies	ARGUMENTATION; FUSION; OWL	Dialogue-based systems often consist of several components, such as communication analysis, dialogue management, domain reasoning, and language generation. In this paper, we present Converness, an ontology-driven, rule-based framework to facilitate domain reasoning for conversational awareness in multimodal dialogue-based agents. Converness uses Web Ontology Language 2 (OWL 2) ontologies to capture and combine the conversational modalities of the domain, for example, deictic gestures and spoken utterances, fuelling conversational topic understanding, and interpretation using description logics and rules. At the same time, defeasible rules are used to couple domain and user-centred knowledge to further assist the interaction with end users, facilitating advanced conflict resolution and personalised context disambiguation. We illustrate the capabilities of the framework through its integration into a multimodal dialogue-based agent that serves as an intelligent interface between users (elderly, caregivers, and health experts) and an ambient assistive living platform in real home settings.																	0266-4720	1468-0394				FEB	2020	37	1			SI				e12378	10.1111/exsy.12378													
J								Predicting and reducing "hospital-acquired infections" using a knowledge-based e-surveillance system	EXPERT SYSTEMS										e-health surveillance systems; health care and simulation; hospital-acquired infection; infection control; knowledge discovery rules	CARE-ASSOCIATED INFECTIONS; BLOOD-STREAM INFECTIONS; CENTRAL-LINE; ELECTRONIC SURVEILLANCE; STRATEGIES	The use of automated computer methods when detecting hospital-acquired infections (HAIs) enhances the validity of the surveillance in an effective manner. This is because manual infection control systems used by hospitals are time consuming and are often restricted to intensive care units. This paper proposes a new knowledge-based electronic surveillance system to predict and reduce HAIs. The system can gather patient-associated data from hospital databanks to automatically predict patient injury based on the standard central line-associated bloodstream infection algorithm for HAI detection rules. The application of the proposed electronic system reduces the number of central lines associated with infection of the bloodstream and reduces the length of stay for patient treatment and thus reduces costs. The proposed system has several advantages: (a) It is a web-based system that collects actual data from patients from several IT sources, which will help collect patient data safely and quickly, thereby predicting HAIs effectively. (b) It has an integrated simulator to generate patient records, providing the ability to train medical personnel and nurses to enhance their skills. (c) It is a multimedia-based system to improve patient health reporting. (d) It assists policymakers in reviewing and approving control plans and policies to reduce and prevent hospital injuries. (e) The investigational results of the system showed an enhancement value equal to 87%.																	0266-4720	1468-0394				FEB	2020	37	1			SI				e12402	10.1111/exsy.12402													
J								Onyx: A new Canvas-based tool for visualizing biomedical and health ontologies	EXPERT SYSTEMS										biomedical and health ontologies; e-health; ontology visualization; OntoQA; OOPS! pitfall scanner; OWL; Semantic Web		Ontologies provide formal, machine-readable, and human-interpretable representations of domain knowledge. Therefore, ontologies have come into question with the development of Semantic Web technologies. People who want to use ontologies need an understanding of the ontology, but this understanding is very difficult to attain if the ontology user lacks the background knowledge necessary to comprehend the ontology or if the ontology is very large. Thus, software tools that facilitate the understanding of ontologies are needed. Ontology visualization is an important research area because visualization can help in the development, exploration, verification, and comprehension of ontologies. This paper introduces the design of a new ontology visualization tool, which differs from traditional visualization tools by providing important metrics and analytics about ontology concepts and warning the ontology developer about potential ontology design errors. The tool, called Onyx, also has advantages in terms of speed and readability. Thus, Onyx offers a suitable environment for the representation of large ontologies, especially those used in biomedical and health information systems and those that contain many terms. It is clear that these additional functionalities will increase the value of traditional ontology visualization tools during ontology exploration and evaluation.																	0266-4720	1468-0394				FEB	2020	37	1			SI				e12380	10.1111/exsy.12380													
J								A secure mHealth application for attention deficit and hyperactivity disorder	EXPERT SYSTEMS										ADHD; gamification; mobile application; mHealth; security	WORKING-MEMORY; CHILDREN; HEALTH; GAMIFICATION; PERFORMANCE; PRIVACY; ADHD	Nowadays, many people have smartphones, the fact that encourages the development of new tools to address different problems. One of its consequences is the recent growth of mHealth, a term that refers to the practice of medicine based on the use of mobile devices for medical and health purposes. This work describes a new mHealth tool to improve memory and cognitive abilities through gamification and serious games. In particular, a mobile application here is proposed to help children that suffer from attention deficit hyperactivity disorder (ADHD). This application integrates the four profiles involved in this disorder: children, parents, teachers, and medical staff. With it, parents can discover if their children suffer the disorder, and children can improve their cognitive abilities through games of different types. Besides, the security aspect of the proposal is emphasized to highlight its importance in mHealth. Thus, the developed tool includes various cryptographic mechanisms to protect the confidentiality of communications and the authenticity of users and data.																	0266-4720	1468-0394				FEB	2020	37	1			SI				e12431	10.1111/exsy.12431													
J								A hybrid knowledge and ensemble classification approach for prediction of venous thromboembolism	EXPERT SYSTEMS										clinical decision support system; clinical text processing; ensemble classifier; semantic extraction; sentiment analysis; venous thromboembolism	CLINICAL DECISION-SUPPORT; MACHINE-LEARNING METHODS; SYSTEM	Clinical narratives such as progress summaries, lab reports, surgical reports, and other narrative texts contain key biomarkers about a patient's health. Evidence-based preventive medicine needs accurate semantic and sentiment analysis to extract and classify medical features as the input to appropriate machine learning classifiers. However, the traditional approach of using single classifiers is limited by the need for dimensionality reduction techniques, statistical feature correlation, a faster learning rate, and the lack of consideration of the semantic relations among features. Hence, extracting semantic and sentiment-based features from clinical text and combining multiple classifiers to create an ensemble intelligent system overcomes many limitations and provides a more robust prediction outcome. The selection of an appropriate approach and its interparameter dependency becomes key for the success of the ensemble method. This paper proposes a hybrid knowledge and ensemble learning framework for prediction of venous thromboembolism (VTE) diagnosis consisting of the following components: a VTE ontology, semantic extraction and sentiment assessment of risk factor framework, and an ensemble classifier. Therefore, a component-based analysis approach was adopted for evaluation using a data set of 250 clinical narratives where knowledge and ensemble achieved the following results with and without semantic extraction and sentiment assessment of risk factor, respectively: a precision of 81.8% and 62.9%, a recall of 81.8% and 57.6%, an F measure of 81.8% and 53.8%, and a receiving operating characteristic of 80.1% and 58.5% in identifying cases of VTE.																	0266-4720	1468-0394				FEB	2020	37	1			SI				e12388	10.1111/exsy.12388													
J								Architecture and implementation of a smart-pregnancy monitoring system using web-based application	EXPERT SYSTEMS										big data; e-health; pregnancy monitoring		Today, people use web-based technologies to meet their information needs, socialise, communicate, and deal with formal and informal processes. At the same time, mobile versions of these applications provide people with great convenience in daily life. These applications include blood-pressure monitors, blood-glucose monitors, body-analysis scales, pulse oximeters, and activity and sleep trackers. Many of these products sync directly with a free mobile app that makes monitoring, viewing, storing, and sharing of health vitals simple and comprehensive. The data collected from the user is stored in a cloud-based application, then trained by intelligent algorithms that use machine learning for health aims so that the user can instantly see his or her status and development. In this study, the aim was to construct a cloud-based application specific to women for monitoring pregnancy. In the web-based application working with membership logic, members can access machine learning assisted calculators of the baby percentile, period tracker, pregnancy calendar, and baby vaccination schedule. Moreover, they can access augmented/virtual-reality-assisted visual training.																	0266-4720	1468-0394				FEB	2020	37	1			SI				e12379	10.1111/exsy.12379													
J								Frame Logic-based specification and discovery of semantic web services with application to medical appointments	EXPERT SYSTEMS										discovery; frame logic; intelligent agent; logical inference; matching; medical appointment; semantic web services	SEARCH	Matching web services and client requirements in the form of goals is a significant challenge in the discovery of semantic web services. The most common but unsatisfactory approach to matching is set-based, where both the client and web services declare what objects they require and what objects they can provide. Matching then becomes the simple task of comparing sets of objects. This approach is inadequate because it says nothing about the functionality required by the client or the functionality provided by the web service. As an alternative, we use the Frame Logic as implemented in Flora-2 to specify web service capabilities and client requirements, including their preconditions, postconditions, and ontologies, implement a logic-based discovery agent using Flora-2, demonstrate its usefulness in a medical appointment making scenario, and show its efficiency both theoretically and by benchmarking. The result is an expressive yet concise representation scheme for semantic web services, and a practical, efficient, powerful, and fully implemented matching engine based purely on logical inference for web service discovery, with direct applicability to Web Service Modeling Ontology and Web Service Modeling Language, because both are based on Frame Logic.																	0266-4720	1468-0394				FEB	2020	37	1			SI				e12387	10.1111/exsy.12387													
J								On the use of ear and profile faces for distinguishing identical twins and nontwins	EXPERT SYSTEMS										ear recognition; face recognition; identical twins; information fusion; texture descriptors	FEATURE-EXTRACTION METHOD; K-NEAREST NEIGHBORS; WAVELET ENTROPY; FUSION	This study aims to measure the efficiency of ear and profile face in distinguishing identical twins under identification and verification modes. In addition, to distinguish identical twins by ear and profile face separately, we propose to fuse these traits with all possible binary combinations of left ear, left profile face, right ear, and right profile face. Fusion is implemented by score-level fusion and decision-level fusion techniques in the proposed method. Additionally, feature-level fusion is used for comparison. All experiments in this paper are also implemented on nontwins individuals, and the recognition performance of twins and nontwins are compared. Local binary patterns, local phase quantization, and binarized statistical image features approaches are used as texture-based descriptors for feature extraction process. Images under controlled and uncontrolled lighting are tested. Ear and profile images from ND-TWINS-2009-2010 dataset are used in the experiments. The experimental results show that the proposed method is more accurate and reliable than using ear or profile face images separately. The performance of the proposed method for recognizing identical twins as recognition rate is 100% and 99.45%, and equal error rates are 0.54% and 1.63% in controlled and uncontrolled illumination conditions, respectively.																	0266-4720	1468-0394				FEB	2020	37	1			SI				e12389	10.1111/exsy.12389													
J								Wearable ECG signal processing for automated cardiac arrhythmia classification using CFASE-based feature selection	EXPERT SYSTEMS										cardiac arrhythmia; classification; combined frequency analysis and Shannon entropy (CFASE); information gain (IG); wearable electrocardiograms (ECG)	WAVELET; DECOMPOSITION	Classification of electrocardiogram (ECG) signals is obligatory for the automatic diagnosis of cardiovascular disease. With the recent advancement of low-cost wearable ECG device, it becomes more feasible to utilize ECG for cardiac arrhythmia classification in daily life. In this paper, we propose a lightweight approach to classify five types of cardiac arrhythmia, namely, normal beat (N), atrial premature contraction (A), premature ventricular contraction (V), left bundle branch block beat (L), and right bundle branch block beat (R). The combined method of frequency analysis and Shannon entropy is applied to extract appropriate statistical features. Information gain criterion is employed to select features that the results show that 10 highly effective features can obtain performance measures comparable to those obtained by using the complete features. The selected features are then fed to the input of Random Forest, K-Nearest Neighbour, and J48 for classification. To evaluate classification performance, tenfold cross validation is used to verify the effectiveness of our method. Experimental results show that Random Forest classifier demonstrates significant performance with the highest sensitivity of 98.1%, the specificity of 99.5%, the precision of 98.1%, and the accuracy of 98.08%, outperforming other representative approaches for automated cardiac arrhythmia classification.																	0266-4720	1468-0394				FEB	2020	37	1			SI				e12432	10.1111/exsy.12432													
J								A Dirichlet process biterm-based mixture model for short text stream clustering	APPLIED INTELLIGENCE										Data mining; Stream clustering; Topic modeling		Short text stream clustering has become an important problem for mining textual data in diverse social media platforms (e.g., Twitter). However, most of the existing clustering methods (e.g., LDA and PLSA) are developed based on the assumption of a static corpus of long texts, while little attention has been given to short text streams. Different from the long texts, the clustering of short texts is more challenging since their word co-occurrence pattern easily suffers from a sparsity problem. In this paper, we propose a Dirichlet process biterm-based mixture model (DP-BMM), which can deal with the topic drift problem and the sparsity problem in short text stream clustering. The major advantages of DP-BMM include (1) DP-BMM explicitly exploits the word-pairs constructed from each document to enhance the word co-occurrence pattern in short texts; (2) DP-BMM can deal with the topic drift problem of short text streams naturally. Moreover, we further propose an improved algorithm of DP-BMM with forgetting property called DP-BMM-FP, which can efficiently delete biterms of outdated documents by deleting clusters of outdated batches. To perform inference, we adopt an online Gibbs sampling method for parameter estimation. Our extensive experimental results on real-world datasets show that DP-BMM and DP-BMM-FP can achieve a better performance than the state-of-the-art methods in terms of NMI metrics.																	0924-669X	1573-7497				MAY	2020	50	5					1609	1619		10.1007/s10489-019-01606-1		FEB 2020											
J								Progressive residual networks for image super-resolution	APPLIED INTELLIGENCE										Image super-resolution; Progressive residual network; Multi-scale features; Residual learning; Deep convolutional neural networks		The recent advances in deep convolutional neural networks (DCNNs) have convincingly demonstrated high-capability reconstruction for single image super-resolution (SR). However, it is a big challenge for most DCNNs-based SR models when the scaling factor increases. In this paper, we propose a novel Progressive Residual Network (PRNet) to integrate hierarchical and scale features for single image SR, which works well for both small and large scaling factors. Specifically, we introduce a Progressive Residual Module (PRM) to extract local multi-scale features through dense connected up-sampling convolution layers. Meanwhile, by embedding residual learning into each module, the relative information between high-resolution and low-resolution multi-scale features is fully exploited to boost reconstruction performance. Finally, the scale-specific features are fused to the reconstruction module for restoring the high-quality image. Extensive quantitative and qualitative evaluations on benchmark datasets illustrate that our PRNet achieves superior performance and in particular obtains new state-of-the-art results for large scaling factors such as 4 x and 8 x.																	0924-669X	1573-7497				MAY	2020	50	5					1620	1632		10.1007/s10489-019-01548-8		FEB 2020											
J								An FPGA-based design for real-time super-resolution reconstruction	JOURNAL OF REAL-TIME IMAGE PROCESSING										FPGA; Super-resolution; Real-time texture analysis; Spatial interpolation; Real-time implementation	SINGLE-IMAGE SUPERRESOLUTION; CLASSIFICATION; MAXIMUM	For several decades, the camera spatial resolution is gradually increasing with the CMOS technology evolution. The image sensors provide more and more pixels, generating new constraints for suitable optics. As an alternative, promising solutions propose super-resolution (SR) techniques to reconstruct high-resolution images or video without modifying the sensor architecture. However, most of the SR implementations are far from reaching real-time performance on a low-budget hardware platform. Moreover, convincing state-of-the-art studies reveal that artifacts can be observed in highly textured areas of the image. In this paper, we propose a local adaptive spatial super resolution (LASSR) method to fix this limitation. LASSR is a two-step SR method including a machine learning-based texture analysis and a fast interpolation method that performs a pixel-by-pixel SR. Multiple evaluations of our method are also provided using standard image metrics for quantitative evaluation and also a psycho-visual assessment for a perceptual evaluation. A first FPGA-based implementation of the proposed method is then presented. It enables high-quality 2-4 k super-resolution videos to be performed at 16 fps, using only 13% of the FPGA capacity, opening the way to reach more than 60 fps by executing several parallel instances of the LASSR code on the FPGA.																	1861-8200	1861-8219															10.1007/s11554-020-00944-5		FEB 2020											
J								A novel nature-inspired meta-heuristic algorithm for optimization: bear smell search algorithm	SOFT COMPUTING										Nature-inspired algorithm; Bear smell search algorithm; Benchmark functions; Meta-heuristic algorithm; Bear's sense of smell	BEE MATING OPTIMIZATION; POWER-SYSTEM; ELECTRICITY PRICE; HYBRID ALGORITHM; ROBUST DESIGN; DISPATCH	In the recent years, the optimization problems show that they are a big challenge for engineering regarding the fast growth of new nature-inspired optimization algorithms. Therefore, this paper presents a novel nature-inspired meta-heuristic algorithm for optimization which is called as bear smell search algorithm (BSSA) that takes into account the powerful global and local search operators. The proposed algorithm imitates both dynamic behaviors of bear based on sense of smell mechanism and the way bear moves in the search of food in thousand miles farther. Among all animals, bears have inconceivable sense of smell due to their huge olfactory bulbs that manage the sense of different odors. Since the olfactory bulb is a neural model of the vertebrate forebrain, it can make a strong exploration and exploitation for optimization. According to the odors value, bear moves the next location. Therefore, this paper mathematically models these structures. To demonstrate and evaluate the BSSA ability, numerous types of benchmark functions and four engineering problems are employed to compare the obtained results of BSSA with other available optimization methods with several analyzed indices such as pair-wise test, Wilcoxon rank and statistical analysis. The numerical results revealed that proposed BSSA presents competitive and greater results compared to other optimization algorithms.																	1432-7643	1433-7479				SEP	2020	24	17					13003	13035		10.1007/s00500-020-04721-1		FEB 2020											
J								Malicious User Nodes Detection by Web Mining Based Artificial Intelligence Technique	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										ANN-Artificial Neural Network; SNS - Social Networking Sites; Sentiment Analysis; Protege tool		Recently, Social Networking is a significant function in human life. It is a foremost communication medium in the middle of persons and associations. Generally, Online Social Media Sites (OSMS) is used with the intention of developing and categorizing illegal behavior in social networking. Therefore, we decided to identify the malicious consumer nodes by the help of ANN through evaluating the response of the chat discussion/remark situation. At last, the 'Protege' tool is used to illustrate the exchange of messages from the distrustful node for upcoming large-scale indication. The anticipated method is executed in the functioning platform of JAVA among Big Data Analytics with Hadoop.																	0218-4885	1793-6411				FEB	2020	28	1					1	24		10.1142/S0218488520500014													
J								A New Efficient Algorithm Based on Multi-Classifiers Model for Classification	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										Data mining; ensemble learning; imbalance classification; classification rule	NEGATIVE ASSOCIATION RULES; SELECTION	Classification is one of the most important problems in data mining and machine learning. The quality and quantity of classification rules are two factors to influence the accuracy of classification. In this paper, we propose a new algorithm to enhance the final classification accuracy, called CMCM (Classification based on Multiple Classifier Models), which consists of two classification models. Model1 centers on the improvement of quality. The optimal attribute values are obtained as the first item of a classification rule from both the items and their complements. While in Model2, quantity is taken into consideration, so it constructs two candidate sets and uses the one-versus-many strategy to generate several rules at one time. The experiment results demonstrate that CMCM can achieve higher classification accuracy than the proposed classification approaches. CMCM can extract sufficient high-quality rules for imbalanced data. Meanwhile, it can also obtain sufficient latent information for classification.																	0218-4885	1793-6411				FEB	2020	28	1					25	46		10.1142/S0218488520500026													
J								Semantic Provenance Based Trustworthy Users Classification on Book-Based Social Network using Fuzzy Decision Tree	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										Social network; provenance; trust; fuzzy logic; fuzzy decision tree; fuzzy rules	TRUST; MODELS	As web-based social network allows anyone to post the content without any restriction, the trustworthiness of the content creator plays an important role before using the content. An effective way to find the trustworthiness is, by analyzing the web resources related to the content creator. Therefore the trustworthiness is assessed using the provenance based ontological model called W7 model. Since it is a real time data, the computed trust for each reviewer using the ontological model is uncertain and vague. An appropriate way to classify such data is using the fuzzy logic with gradual trust level. As the computed trust data are feature-based and non-symbolic, the classification ambiguity need to be reduced greatly. This is achieved with the fuzzy decision tree approach, which is a fusion of fuzzy sets with decision tree. The truth of the rule is crucial in trustworthy user classification, as highly truthful rules really increase the credibility of the user in their domain. Therefore, in the proposed model, degree of truth is used as a pruning criteria that classifies the users with minimum number of fuzzy evidence or knowledge. This paper proposes a semantic provenance based gradual trust model to classify the trustworthy reviewers in a book-based social networks using fuzzy decision tree approach. Performance analysis of the proposed model in the terms of classifier accuracy, precision, recall, the number of rules generated and its time complexity are discussed. The analysis shows that the proposed learning model outperforms other classification models. This method is also applied to other data sets and the performance of the classifier is assessed.																	0218-4885	1793-6411				FEB	2020	28	1					47	77		10.1142/S0218488520500038													
J								Decentralized Incremental Fuzzy Reinforcement Learning for Multi-Agent Systems	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										multi-agent systems; decentralized partially observable Markov decision processes; planning under uncertainty; fuzzy inference systems; reinforcement learning		We present a new incremental fuzzy reinforcement learning algorithm to find a sub-optimal policy for infinite-horizon Decentralized Partially Observable Markov Decision Processes (Dec-POMDPs). The algorithm addresses the high computational complexity of solving large Dec-POMDPs by generating a compact fuzzy rule-base for each agent. In our method, each agent uses its own fuzzy rule-base to make the decisions. The fuzzy rules in these rule-bases are incrementally created and tuned according to experiences of the agents. Reinforcement learning is used to tune the behavior of each agent in such a way that maximum global reward is achieved. In addition, we propose a method to construct the initial rule-base for each agent using the solution of the underlying MDP. This drastically improves the performance of the algorithm in comparison with random initialization of the rule-base. We assess the performance of our proposed method using several benchmark problems in comparison with some state-of-the-art methods. Experimental results show that our algorithm achieves better or similar reward when compared with other methods. However, from the runtime point of view, our method is superior to all previous methods. Using a compact fuzzy rule-base not only decreases the amount of memory used but also significantly speeds up the learning phase.																	0218-4885	1793-6411				FEB	2020	28	1					79	98		10.1142/S021848852050004X													
J								Modeling Opinion Formation in Social Networks: A Probabilistic Fuzzy Approach	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										Prediction; social networks analysis; telegram analysis; fuzzy logic; probabilistic fuzzy theory	RUMOR SPREADING MODEL; DYNAMICS; DISEASES	Opinion formation in social networks is an interesting dynamical process from the perspective of system modeling due to its large scale as well as the variety of structural and parametric uncertainties that it entails. This paper proposes a probabilistic fuzzy opinion formation model for predicting the opinions of communities in the social networks. In this regard, the opinions of a group of individuals about a given topic in a Telegram pilot group, as a popular social network, are collected and presented in the framework of the probabilistic fuzzy model. Based on the obtained data, the parameters of the model are extracted, and the model is tuned. Finally, the variations of the actual opinions throughout time are compared with the model predictions. The numerical results in this study show that, with appropriately tuned parameters, the model successfully represents the opinion formation process, with an average error that approaches zero.																	0218-4885	1793-6411				FEB	2020	28	1					99	116		10.1142/S0218488520500051													
J								Double Frontier Two-Stage Fuzzy Data Envelopment Analysis	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										Data envelopment analysis; double frontier analysis; fuzzy data; optimistic and pessimistic efficiencies; series system	DECISION-MAKING UNITS; DEA EFFICIENCY ANALYSIS; INTERVAL DEA; PERFORMANCE-MEASUREMENT; UNDESIRABLE OUTPUTS; SHARED RESOURCES; MISSING VALUES; IMPRECISE DATA; NETWORK DEA; MODELS	Data envelopment analysis (DEA) is a mathematical programming approach with widespread applications in productivity and efficiency analysis. Compared with traditional DEA models, two-stage DEA models show the performance of each process and make available more information for decision making. In an article by Kao and Liu,(1)( )models were proposed for combining a two-stage process to achieve overall fuzzy efficiency measures. Their method follows the simple geometric average approach and uses the product of two efficiencies. The present article applies a different angle for efficiency analysis in the two-stage fuzzy DEA. We suggest that the overall efficiency score of a decision-making unit (DMU) is defined as total weight of stage efficiencies, not as the simple product of their efficiency. Moreover, the proposed fuzzy DEA models are different from the model by Kao and Liu(1) for fuzzy data in that our models are linear without the need for additional changes in variables and use the same set of constraints to measure the efficiency of DMUs with fuzzy input and output data. While the models by Kao and Liu(1) are a nonlinear optimization problem that need additional changes in variables, and use different sets of constraints to measure fuzzy efficiencies. Additionally, our proposed approach evaluates the performance of DMUs from both optimistic and pessimistic viewpoints. Finally, using the proposed approach, the Taiwanese non-life insurance company problem will be investigated.																	0218-4885	1793-6411				FEB	2020	28	1					117	152		10.1142/S0218488520500063													
J								Deep Learning of Empirical Mean Curve Decomposition-Wavelet Decomposed EEG Signal for Emotion Recognition	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										Emotional state recognition; recognition; EEG signal; DWT; EMCD; DBN model	FEATURE-EXTRACTION; SELECTION; ENTROPY	Recently, the emotional state recognition of humans via Electroencephalogram (EEG) is one of the emerging topics that grasp the attention of researchers too. This EEG based recognition is normally an effective model for many of the real-time applications, especially for disabled people. A number of researchers are in progress to make the recognition model more effective in terms of accurate emotion recognition. However, it is not so satisfactory in the precise accurate progressing. Hence this paper intends to recognize the human emotional states or affects through EEG signals by adopting advanced features and classifier models. In the first stage of recognition procedure, this paper exploits 2501 (EMCD) and Wavelet Transformation to represent the EEG signal in low dimension as well as descriptive. By EMCD, the EEG redundancy can be neglected, and the significant information can be extracted. The classification processes using the extracted features with the aid of a classifier named Deep Belief Network (DBN). The performance of the proposed Wavelet-EMCD (WE) approach is analyzed in terms of measures such as Accuracy, Sensitivity, Specificity, Precision, False positive rate (FPR), False negative rate (FNR), Negative Predictive Value (NPV), False Discovery Rate (FDR), FlScore and Mathews correlation coefficient (MCC) and proven the superiority of proposed work in recognizing the emotions more accurately.																	0218-4885	1793-6411				FEB	2020	28	1					153	177		10.1142/S0218488520500075													
J								Grid quorum-based spatial coverage for IoT smart agriculture monitoring using enhanced multi-verse optimizer	NEURAL COMPUTING & APPLICATIONS										IoT; Wireless sensor networks; Area coverage; Multi-verse optimizer; Smart agriculture; East Oweinat; Metaheuristic; Algorithm	SENSOR NETWORK; DEPLOYMENT; INTERNET; THINGS; ALGORITHM; ANOVA; POWER	Wireless sensor networks (WSN) are the backbone in various modern Internet of Things (IoT) smart applications ranging from automated control, surveillance, forest fire detection, etc. One of the most important applications is the smart agriculture. The deployment of WSN in agricultural processes can predict crop yield, soil temperature, air quality, water level, crop price, and the appropriate time for market delivery which will help to increase productivity. In this paper, an enhanced metaheuristic algorithm called multi-verse optimizer with overlapping detection phase (DMVO) is introduced for optimizing the area coverage percentage of WSN. The proposed algorithm is tested on many datasets with different criterions and is compared with other algorithms including the original MVO, particle swarm optimization, and flower pollination algorithm. The experimental results are analyzed with one-way ANOVA test. In addition, DMVO is applied to IoT smart agriculture in East Oweinat area in Egypt and compared with Krill Herd algorithm. In addition, the experimental results are analyzed with Wilcoxon signed-rank test. The experimental results and the statistical analysis prove the prosperity and consistency of the proposed algorithm.																	0941-0643	1433-3058				FEB	2020	32	3			SI		607	624		10.1007/s00521-018-3807-4													
J								Decision-level fusion scheme for nasopharyngeal carcinoma identification using machine learning techniques	NEURAL COMPUTING & APPLICATIONS										Nasopharyngeal carcinoma; Machine learning techniques; Feature-based decision-level fusion; Endoscopic images; Local binary patterns; Support vector machines; k-nearest neighbors' algorithm; Artificial neural network	NEURAL-NETWORK; CLASSIFICATION; SEGMENTATION; PREDICTION; FEATURES	Making an accurate diagnosis of nasopharyngeal carcinoma (NPC) disease is a challenging task that involves many parties such as radiology specialists often times need to delineate NPC boundaries on various tumor-bearing endoscopic images. It is a tedious and time-consuming operation exceedingly based on doctors and experience of radiologist. NPC has complex and irregular structures which makes it difficult to diagnose even by an expert physician. However, the diagnosis accuracy results of such methods are still insignificant and need improvement in order to manifest robust solution. The study aim is to develop and propose a new automatic classification of NPC tumor using machine learning techniques and feature-based decision-level fusion scheme from endoscopic images. We have implemented the fusion of the three image texture-based schemes (local binary patterns, the first-order statistics histogram properties, and histogram of gray scale) at the decision level and tested the performance of this scheme using the same experimental setup in the previous section for simple score-level fusion, but for comparison, We used the classifiers methods which are support vector machines (SVM), k-nearest neighbors' algorithm, and artificial neural network (ANN). The results demonstrate that the majority rule for decision-based fusion is outperformed considerably by the single best performing feature scheme (FFGF) for the SVM classifier, but for the ANN and KNN classifier it is significantly outperformed by each of the components features. The classifiers approaches were listed a high accuracy of 94.07%, the sensitivity of 92.05%, and specificity of 93.07%.																	0941-0643	1433-3058				FEB	2020	32	3			SI		625	638		10.1007/s00521-018-3882-6													
J								Securing e-health records using keyless signature infrastructure blockchain technology in the cloud	NEURAL COMPUTING & APPLICATIONS										Electronic health record (EHR); e-Health; Keyless signature infrastructure (KSI); Timestamped algorithm; Merkle tree; Blockchain	ENABLING TECHNOLOGIES; INTERNET; THINGS	Health record maintenance and sharing are one of the essential tasks in the healthcare system. In this system, loss of confidentiality leads to a passive impact on the security of health record whereas loss of integrity leads can have a serious impact such as loss of a patient's life. Therefore, it is of prime importance to secure electronic health records. Health records are represented by Fast Healthcare Interoperability Resources standards and managed by Health Level Seven International Healthcare Standards Organization. Centralized storage of health data is attractive to cyber-attacks and constant viewing of patient records is challenging. Therefore, it is necessary to design a system using the cloud that helps to ensure authentication and that also provides integrity to health records. The keyless signature infrastructure used in the proposed system for ensuring the secrecy of digital signatures also ensures aspects of authentication. Furthermore, data integrity is managed by the proposed blockchain technology. The performance of the proposed framework is evaluated by comparing the parameters like average time, size, and cost of data storage and retrieval of the blockchain technology with conventional data storage techniques. The results show that the response time of the proposed system with the blockchain technology is almost 50% shorter than the conventional techniques. Also they express the cost of storage is about 20% less for the system with blockchain in comparison with the existing techniques.																	0941-0643	1433-3058				FEB	2020	32	3			SI		639	647		10.1007/s00521-018-3915-1													
J								A streak detection approach for comprehensive two-dimensional gas chromatography based on image analysis	NEURAL COMPUTING & APPLICATIONS										Two-dimensional chromatography; Streak detection; Steerable Gaussian filtering; Marker-controlled watershed algorithm; Image analysis	PEAK-DETECTION; ALGORITHM; GC	Comprehensive two-dimensional gas chromatography (GC x GC) can separate thousands of different compounds, and is used for many important applications such as petrochemical processing and environmental monitoring, etc. GC x GC generates rich and complex information, which requires automated processing for rapid chemical identification and classification. A challenge is to remove unwanted streaks that may affect the quantification and identification of analytes. It is difficult to detect streaks because of complex backgrounds, low-contrast data, and variable shapes, scales, and orientations of streaks in GC x GC data. This paper proposes a new approach to detect streaks effectively based on image analysis techniques. By adopting a pseudo-log function and preprocessing methods to compress the original data and enhance the low-contrast data, we employ steerable Gaussian filtering to delineate streak regions based on the specific orientations of streaks. A marker-controlled watershed algorithm is then used to segment the streaks, and highly discriminating characteristics are used to identify candidate regions and reject false streaks. In the end, with a diverse data set generated from gas chromatograph, experiments are carried out and the results demonstrate that our streak detection approach is effective and robust with respect to changes in streak patterns, even in variable chromatographic conditions. The proposed object detection method effective in complex backgrounds and low-contrast conditions is also helpful for object detection in other scenes.																	0941-0643	1433-3058				FEB	2020	32	3			SI		649	663		10.1007/s00521-018-3917-z													
J								Alcoholism identification via convolutional neural network based on parametric ReLU, dropout, and batch normalization	NEURAL COMPUTING & APPLICATIONS										Alcoholism; Deep learning; Parametric rectified linear unit; Dropout; Batch normalization; Convolutional neural network; Deep neural network		Alcoholism changes the structure of brain. Several somatic marker hypothesis network-related regions are known to be damaged in chronic alcoholism. Neuroimaging approach can help us better understanding the impairment discovered in alcohol-dependent subjects. In this research, we recruited subjects from participating hospitals. In total, 188 abstinent long-term chronic alcoholic participants (95 men and 93 women) and 191 non-alcoholic control participants (95 men and 96 women) were enrolled in our experiment via computerized diagnostic interview schedule version IV and medical history interview employed to determine whether the applicants can be enrolled or excluded. The Siemens Verio Tim 3.0 T MR scanner (Siemens Medical Solutions, Erlangen, Germany) was employed to scan the subjects. Then, we proposed a 10-layer convolutional neural network for the diagnosis based on imaging, including three advanced techniques: parametric rectified linear unit (PReLU); batch normalization; and dropout. The structure of network is fine-tuned. The results show that our method secured a sensitivity of 97.73 +/- 1.04%, a specificity of 97.69 +/- 0.87%, and an accuracy of 97.71 +/- 0.68%. We observed the PReLU gives better performance than ordinary ReLU, clipped ReLU, and leaky ReLU. The batch normalization and dropout gained enhanced performance as batch normalization overcame the internal covariate shift and dropout got over the overfitting. The results of our proposed 10-layer CNN model show its performance better than seven state-of-the-art approaches.																	0941-0643	1433-3058				FEB	2020	32	3			SI		665	680		10.1007/s00521-018-3924-0													
J								A-COA: an adaptive cuckoo optimization algorithm for continuous and combinatorial optimization	NEURAL COMPUTING & APPLICATIONS										Cuckoo optimization algorithm (COA); Metaheuristics; Multiprocessor task scheduling problem (MTSP); Numerical benchmark functions; Combinatorial optimization	TASK; SEARCH	Cuckoo optimization algorithm (COA) is inspired from the special and exotic lifestyle of a bird family called the cuckoo and her amazing and unique behavior in egg laying and breeding. Just like any other population-based swarm intelligence metaheuristic algorithms, the basic COA starts with a set of randomly generated solutions called "habitats." Actually, the habitats can be the current locations of either the mature cuckoos or their eggs. In an iterative manner, cuckoos lay their eggs around their habitats inside the other birds' nests by mimicking their eggs' color, pattern, and size, and this is a kind of parasitic brooding behavior. Some hosts may discriminate the strange eggs and throw them out while the others not. The survival competition between cuckoos and their hosts, and migration of cuckoos in swarm are two main underlying motivations to introduce the COA. In this paper, an adaptive cuckoo optimization algorithm named A-COA is proposed in which three novelties in egg-laying and migration phases are applied. These modifications have made the basic algorithm more efficient with faster convergence to solve continuous and discrete optimization problems. A comprehensive comparison study of A-COA versus not only the basic COA but also some other conventional metaheuristics like GA, PSO, ABC, and TLBO has been made on a variety of unimodal and multimodal numerical benchmark functions with different characteristics, and the results show an overall 25.85% of improvement in terms of performance with a faster convergence speed compared to the basic COA, where the statistical Wilcoxon rank-sum test certifies our conclusions. In addition, a discretized version of A-COA and its application to the multiprocessor task scheduling problem as a complex combinatorial optimization problem are investigated where the proposed A-COA is very competitive with not only the strongest conventional heuristics, for example, MCP, ETF, and DLS, but also the basic COA and the newly proposed ACO-based approach.																	0941-0643	1433-3058				FEB	2020	32	3			SI		681	705		10.1007/s00521-018-3928-9													
J								An enhanced diabetic retinopathy detection and classification approach using deep convolutional neural network	NEURAL COMPUTING & APPLICATIONS										Diabetic retinopathy; Image processing; Deep learning; Convolutional neural network	ARTIFICIAL-INTELLIGENCE; OPTIMIZATION; SYSTEM; DIAGNOSIS; FEATURES; HEALTH	The objective of this study is to propose an alternative, hybrid solution method for diagnosing diabetic retinopathy from retinal fundus images. In detail, the hybrid method is based on using both image processing and deep learning for improved results. In medical image processing, reliable diabetic retinopathy detection from digital fundus images is known as an open problem and needs alternative solutions to be developed. In this context, manual interpretation of retinal fundus images requires the magnitude of work, expertise, and over-processing time. So, doctors need support from imaging and computer vision systems and the next step is widely associated with use of intelligent diagnosis systems. The solution method proposed in this study includes employment of image processing with histogram equalization, and the contrast limited adaptive histogram equalization techniques. Next, the diagnosis is performed by the classification of a convolutional neural network. The method was validated using 400 retinal fundus images within the MESSIDOR database, and average values for different performance evaluation parameters were obtained as accuracy 97%, sensitivity (recall) 94%, specificity 98%, precision 94%, FScore 94%, and GMean 95%. In addition to those results, a general comparison of with some previously carried out studies has also shown that the introduced method is efficient and successful enough at diagnosing diabetic retinopathy from retinal fundus images. By employing the related image processing techniques and deep learning for diagnosing diabetic retinopathy, the proposed method and the research results are valuable contributions to the associated literature.																	0941-0643	1433-3058				FEB	2020	32	3			SI		707	721		10.1007/s00521-018-03974-0													
J								An adaptive QoS computation for medical data processing in intelligent healthcare applications	NEURAL COMPUTING & APPLICATIONS										Adaptive; QoS computation; Medical data processing; QoS-QoE correlation; Intelligent healthcare applications	BIG DATA; INTERNET; SCHEMES; SERVICE; THINGS; MODEL	Efficient computation of quality of service (QoS) during medical data processing through intelligent measurement methods is one of the mandatory requirements of the medial healthcare world. However, emergency medical services often involve transmission of critical data, thus having stringent requirements for network quality of service (QoS). This paper contributes in three distinct ways. First, it proposes the novel adaptive QoS computation algorithm (AQCA) for fair and efficient monitoring of the performance indicators, i.e., transmission power, duty cycle and route selection during medical data processing in healthcare applications. Second, framework of QoS computation in medical applications is proposed at physical, medium access control (MAC) and network layers. Third, QoS computation mechanism with proposed AQCA and quality of experience (QoE) is developed. Besides, proper examination of QoS computation for medical healthcare application is evaluated with 4-10 inches large-screen user terminal (UT) devices (for example, LCD panel size, resolution, etc.). These devices are based on high visualization, battery lifetime and power optimization for ECG service in emergency condition. These UT devices are used to achieve highest level of satisfaction in terms, i.e., less power drain, extended battery lifetime and optimal route selection. QoS parameters with estimation of QoE perception identify the degree of influence of each QoS parameters on the medical data processing is analyzed. The experimental results indicate that QoS is computed at physical, MAC and network layers with transmission power (- 15 dBm), delay (100 ms), jitter (40 ms), throughput (200 Bytes), duty cycle (10%) and route selection (optimal). Thus it can be said that proposed AQCA is the potential candidate for QoS computation than Baseline for medical healthcare applications.																	0941-0643	1433-3058				FEB	2020	32	3			SI		723	734		10.1007/s00521-018-3931-1													
J								IoT-based 3D convolution for video salient object detection	NEURAL COMPUTING & APPLICATIONS										Internet of Things; Salient object detection; Video processing; Deep learning	SEGMENTATION	The video salient object detection (SOD) is the first step for the devices in the Internet of Things (IoT) to understand the environment around them. The video SOD needs the objects' motion information in contiguous video frames as well as spatial contrast information from a single video frame. A large number of IoT devices' computing power is not sufficient to support the existing SOD methods' expensive computational complexity in emotion estimation, because they might have low hardware configurations (e.g., surveillance camera, and smartphone). In order to model the objects' motion information efficiently for SOD, we propose an end-to-end video SOD algorithm with an efficient representation of the objects' motion information. This algorithm contains two major parts: a 3D convolution-based X-shape structure that directly represents the motion information in successive video frames efficiently, and 2D densely connected convolutional neural networks (DenseNet) with pyramid structure to extract the rich spatial contrast information in a single video frame. Our method not only can maintain a small number of parameters as the 2D convolutional neural network but also represents spatiotemporal information uniformly that enables it can be trained end-to-end. We evaluate our proposed method on four benchmark datasets. The results show that our method achieves state-of-the-art performance compared with the other five methods.																	0941-0643	1433-3058				FEB	2020	32	3			SI		735	746		10.1007/s00521-018-03971-3													
J								Evaluation of artificial intelligence techniques for the classification of different activities of daily living and falls	NEURAL COMPUTING & APPLICATIONS										Artificial intelligence; Classification; Fall detection; Activities of daily living		Automatic detection of falls is extremely important, especially in the remote monitoring of elderly people, and will become more and more critical in the future, given the constant increase in the number of older adults. Within this framework, this paper deals with the task of evaluating several artificial intelligence techniques to automatically distinguish between different activities of daily living (ADLs) and different types of falls. To do this, UniMiB SHAR, a publicly available data set containing instances of nine different ADLs and of eight kinds of falls, is considered. We take into account five different classes of classification algorithms, namely tree-based, discriminant-based, support vector machines, K-nearest neighbors, and ensemble mechanisms, and we consider several representatives for each of these classes. These are all the classes contained in the Classification Learner app contained in MATLAB, which serves as the computational basis for our experiments. As a result, we apply 22 different classification algorithms coming from artificial intelligence under a fivefold cross-validation learning strategy, with the aim to individuate which the most suitable is for this data set. The numerical results show that the algorithm with the highest classification accuracy is the ensemble based on subspace as the ensemble method and on KNN as learner type. This shows an accuracy equal to 86.0%. Its results are better than those in the other papers in the literature that face this specific 17-class problem.																	0941-0643	1433-3058				FEB	2020	32	3			SI		747	758		10.1007/s00521-018-03973-1													
J								Learning visual representations with optimum-path forest and its applications to Barrett's esophagus and adenocarcinoma diagnosis	NEURAL COMPUTING & APPLICATIONS										Barrett's esophagus; Optimum-path forest; Machine learning; Adenocarcinoma; Image processing	CAPSULE ENDOSCOPY; CLASSIFICATION	Considering the increase in the number of the Barrett's esophagus (BE) in the last decade, and its expected continuous increase, methods that can provide an early diagnosis of dysplasia in BE-diagnosed patients may provide a high probability of cancer remission. The limitations related to traditional methods of BE detection and management encourage the creation of computer-aided tools to assist in this problem. In this work, we introduce the unsupervised Optimum-Path Forest (OPF) classifier for learning visual dictionaries in the context of Barrett's esophagus (BE) and automatic adenocarcinoma diagnosis. The proposed approach was validated in two datasets (MICCAI 2015 and Augsburg) using three different feature extractors (SIFT, SURF, and the not yet applied to the BE context A-KAZE), as well as five supervised classifiers, including two variants of the OPF, Support Vector Machines with Radial Basis Function and Linear kernels, and a Bayesian classifier. Concerning MICCAI 2015 dataset, the best results were obtained using unsupervised OPF for dictionary generation using supervised OPF for classification purposes and using SURF feature extractor with accuracy nearly to 78% for distinguishing BE patients from adenocarcinoma ones. Regarding the Augsburg dataset, the most accurate results were also obtained using both OPF classifiers but with A-KAZE as the feature extractor with accuracy close to 73%. The combination of feature extraction and bag-of-visual-words techniques showed results that outperformed others obtained recently in the literature, as well as we highlight new advances in the related research area. Reinforcing the significance of this work, to the best of our knowledge, this is the first one that aimed at addressing computer-aided BE identification using bag-of-visual-words and OPF classifiers, being the application of unsupervised technique in the BE feature calculation the major contribution of this work. It is also proposed a new BE and adenocarcinoma description using the A-KAZE features, not yet applied in the literature.																	0941-0643	1433-3058				FEB	2020	32	3			SI		759	775		10.1007/s00521-018-03982-0													
J								Automatic detection of lung cancer from biomedical data set using discrete AdaBoost optimized ensemble learning generalized neural networks	NEURAL COMPUTING & APPLICATIONS										Computer-aided diagnosis; Neural computing; Biomedical; ELVIRA Biomedical Data Set Repository; Minimum repetition and Wolf heuristic features; Discrete AdaBoost optimized ensemble learning generalized neural networks	MUTUAL INFORMATION; CLASSIFICATION; DIAGNOSIS; SELECTION; SEGMENTATION; FOREST	Today, most of the people are affected by lung cancer, mainly because of the genetic changes of the tissues in the lungs. Other factors such as smoking, alcohol, and exposure to dangerous gases can also be considered the contributory causes of lung cancer. Due to the serious consequences of lung cancer, the medical associations have been striving to diagnose cancer in its early stage of growth by applying the computer-aided diagnosis process. Although the CAD system at healthcare centers is able to diagnose lung cancer during its early stage of growth, the accuracy of cancer detection is difficult to achieve, mainly because of the overfitting of lung cancer features and the dimensionality of the feature set. Thus, this paper introduces the effective and optimized neural computing and soft computing techniques to minimize the difficulties and issues in the feature set. Initially, lung biomedical data were collected from the ELVIRA Biomedical Data Set Repository. The noise present in the data was eliminated by applying the bin smoothing normalization process. The minimum repetition and Wolf heuristic features were subsequently selected to minimize the dimensionality and complexity of the features. The selected lung features were analyzed using discrete AdaBoost optimized ensemble learning generalized neural networks, which successfully analyzed the biomedical lung data and classified the normal and abnormal features with great effectiveness. The efficiency of the system was then evaluated using MATLAB experimental setup in terms of error rate, precision, recall, G-mean, F-measure, and prediction rate.																	0941-0643	1433-3058				FEB	2020	32	3			SI		777	790		10.1007/s00521-018-03972-2													
J								Analyzing genetic diseases using multimedia processing techniques associative decision tree-based learning and Hopfield dynamic neural networks from medical images	NEURAL COMPUTING & APPLICATIONS										Medical image; Multimedia tool; Genetic diseases; Artificial bee colony; Associative decision tree-based learning; Greedy forward selection; Scatter search; Hopfield dynamic neural networks	FEATURE SUBSET-SELECTION; CLASSIFICATION; DIAGNOSIS	Genetic diseases are the most common next-generation diseases because of the improper mutation of the genes and DNA. These genetic diseases are failed to predict with an accurate manner in the beginning stage by using the particular genes and related information. So, the genetic diseases are identified in the medical systems by utilizing the hybridization of multimedia techniques such as big data and related soft computing techniques.Initially, the genetic disease-related medical images are collected from healthcare sectors, and from the genetic image, various genetic data are collected from the large amount of datasets in which the major challenge is too high dimensionality that increases the complexity of the genetic disease prediction system. So, in this paper the complexity of the system is reduced by using the associative decision tree-based learning and Hopfield dynamic neural networks (HDNN). After collecting the data from the various resources, the immune clonal selection algorithm approach is used to remove inconsistent data and minimize the dimensionality of data. The selected features are trained by the proposed associative decision tree approach which helps to compare with the testing features using the HDNN that successfully recognize the genetic disease-based features effectively. The excellence of the system is measured with the aid of the experimental outcomes that are corresponding to the forecasting methods such as greedy algorithm, rough set method and artificial bee colony, and the comparison is made with the avail of the accuracy, sensitivity and specificity.																	0941-0643	1433-3058				FEB	2020	32	3			SI		791	803		10.1007/s00521-018-04004-9													
J								Fuzzy rank correlation-based segmentation method and deep neural network for bone cancer identification	NEURAL COMPUTING & APPLICATIONS										Deep neural networks; Intuitionistic fuzzy rank correlation; Bone malignant; Swear symptoms; Swelling; Bones weaken risk; Median filter	DIAGNOSIS; CLASSIFICATION	Bone malignant tumors are one of the important health problems because tumors are formed due to the affectedness of the healthiest bone tissues. This serious bone cancer has been identified with the help of the different risk factors such as chills, swear symptoms, swelling, bones weaken risk, and night swears symptoms. These symptoms are not easy to predict in beginning stage with accurate manner. So, automatic bone cancer detection system has been developed to predict the cancer in earlier sate. Initially, the bone images are collected from patient, and noise in the images is eliminated using median filter. After eliminating the noise, affected tumor part is detected by applying the intuitionistic fuzzy rank correlation. From the detected intuitionistic fuzzy-based clustered images, different statistical features are extracted. The derived features are processed by applying the deep neural networks layers that successfully examines each features using Levenberg-Marquardt learning algorithm. The successful learning process predicts bone cancer-related features with accurate manner (99.1%). Finally, excellence of bone cancer prediction system is analyzed using MATLAB-based experimental setup and performance metrics such as F1 score, accuracy, error rate and so on.																	0941-0643	1433-3058				FEB	2020	32	3			SI		805	815		10.1007/s00521-018-04005-8													
J								Classification of stroke disease using machine learning algorithms	NEURAL COMPUTING & APPLICATIONS										Stroke; Tagging; Maximum entropy; Data pre-processing; Classification; Machine learning	ISCHEMIC-STROKE; RISK-FACTORS; PREDICTION; INTERSTROKE; COUNTRIES	This paper presents a prototype to classify stroke that combines text mining tools and machine learning algorithms. Machine learning can be portrayed as a significant tracker in areas like surveillance, medicine, data management with the aid of suitably trained machine learning algorithms. Data mining techniques applied in this work give an overall review about the tracking of information with respect to semantic as well as syntactic perspectives. The proposed idea is to mine patients' symptoms from the case sheets and train the system with the acquired data. In the data collection phase, the case sheets of 507 patients were collected from Sugam Multispecialty Hospital, Kumbakonam, Tamil Nadu, India. Next, the case sheets were mined using tagging and maximum entropy methodologies, and the proposed stemmer extracts the common and unique set of attributes to classify the strokes. Then, the processed data were fed into various machine learning algorithms such as artificial neural networks, support vector machine, boosting and bagging and random forests. Among these algorithms, artificial neural networks trained with a stochastic gradient descent algorithm outperformed the other algorithms with a higher classification accuracy of 95% and a smaller standard deviation of 14.69.																	0941-0643	1433-3058				FEB	2020	32	3			SI		817	828		10.1007/s00521-019-04041-y													
J								CMDP-based intelligent transmission for wireless body area network in remote health monitoring	NEURAL COMPUTING & APPLICATIONS										Constrained Markov decision processes (CMDP); Intelligent adaptive learning algorithm; Joint intra; and beyond-WBAN; Remote health monitoring; Wireless body area network	MARKOV DECISION-PROCESSES; TECHNOLOGIES; INTERNET; POLICIES; CLOUD	Remote health monitoring is one kind of E-health service, which transfer the users' physiological data to the medical data center for analysis or diagnosis. Wireless body area network (WBAN) is a promising technology to achieve physiological information acquiring and delivering and thus has been widely adopted in remote health-monitoring applications. For WBAN, energy consumption is the major concern which has been addressed in many researches. Different from existing works, this work studies a joint scheduling and admission control problem with objective of optimizing the energy efficiency of both intra- and beyond-WBAN link. The problem is formulated as constrained Markov decision processes, and the relative value iteration and Lagrange multiplier approach are used to derive the optimal intelligent algorithm. Simulation results show the proposed algorithm is capable of, in comparison with greedy scheme, achieving nearly 100% throughput improvement in various power consumption budgets. Moreover, the proposed algorithm can achieve up to 5.5x power consumption saving for sensor node in comparison with other scheduling algorithms.																	0941-0643	1433-3058				FEB	2020	32	3			SI		829	837		10.1007/s00521-019-04034-x													
J								Refining Parkinson's neurological disorder identification through deep transfer learning	NEURAL COMPUTING & APPLICATIONS										Parkinson disease; Handwriting analysis; Neurodegenerative disorder	DISEASE; DIAGNOSIS; TREMOR; MICROGRAPHIA; MARKER; GAIT	Parkinson's disease (PD), a multi-system neurodegenerative disorder which affects the brain slowly, is characterized by symptoms such as muscle stiffness, tremor in the limbs and impaired balance, all of which tend to worsen with the passage of time. Available treatments target its symptoms, aiming to improve the quality of life. However, automatic diagnosis at early stages is still a challenging medicine-related task to date, since a patient may have an identical behavior to that of a healthy individual at the very early stage of the disease. Parkinson's disease detection through handwriting data is a significant classification problem for identification of PD at the infancy stage. In this paper, a PD identification is realized with help of handwriting images that help as one of the earliest indicators for PD. For this purpose, we proposed a deep convolutional neural network classifier with transfer learning and data augmentation techniques to improve the identification. Two approaches like freeze and fine-tuning of transfer learning are investigated using ImageNet and MNIST dataset as source task independently. A trained network achieved 98.28% accuracy using fine-tuning-based approach using ImageNet and PaHaW dataset. Experimental results on benchmark dataset reveal that the proposed approach provides better detection of Parkinson's disease as compared to state-of-the-art work.																	0941-0643	1433-3058				FEB	2020	32	3			SI		839	854		10.1007/s00521-019-04069-0													
J								Diagnosis of the Hypopnea syndrome in the early stage	NEURAL COMPUTING & APPLICATIONS										Hypopnea syndrome; Respiration sensor; Early warning; Biomedical engineering; Machine learning	OBSTRUCTIVE SLEEP-APNEA; VALIDATION	Hypopnea syndrome is a chronic respiratory disease that is characterized by repetitive episodes of breathing disruptions during sleep. Hypopnea syndrome is a systemic disease that manifests respiratory problems; however, more than 80% of Hypopnea syndrome patients remain undiagnosed due to complicated polysomnography. Objective assessment of breathing patterns of an individual can provide useful insight into the respiratory function unearthing severity of Hypopnea syndrome. This paper explores a novel approach to detect incognito Hypopnea syndrome as well as provide a contactless alternative to traditional medical tests. The proposed method is based on S-Band sensing technique (including a spectrum analyzer, vector network analyzer, antennas, software-defined radio, RF generator, etc.), peak detection algorithm and Sine function fitting for the observation of breathing patterns and characterization of normal or disruptive breathing patterns for Hypopnea syndrome detection. The proposed system observes the human subject and changes in the channel frequency response caused by Hypopnea syndrome utilizing a wireless link between two monopole antennas, placed 3 m apart. Commercial respiratory sensors were used to verify the experimental results. By comparing the results, it is found that for both cases, the pause time is more than 10 s with 14 peaks. The experimental results show that this technique has the potential to open up new clinical opportunities for contactless and accurate Hypopnea syndrome monitoring in a patient-friendly and flexible environment.																	0941-0643	1433-3058				FEB	2020	32	3			SI		855	866		10.1007/s00521-019-04037-8													
J								Placement delivery array design for the coded caching scheme in medical data sharing	NEURAL COMPUTING & APPLICATIONS										Caching architecture of medical data; Coded caching; Placement delivery array; Combination networks; MDS codes	KEY MANAGEMENT SCHEME; FUNDAMENTAL LIMITS; RECORDS; INTERNET; IMAGE; CLOUD	A coded caching scheme for efficient storage and access of medical data is discussed in this paper. Being an inspiring technology for improving the availability and sharing of medical data, the coded caching scheme for medical data is not only convenient for the medical staffs to grasp the patients' health conditions and treatment history in time, but also effective for the patients to obtain improved quality of medical services. In this paper, the coded caching scheme for the combination network which is the model for the medical data sharing is studied. In the combination network, the server communicates with the users via multiple relays, and the relays as well as the users have cache memories. Using the maximum distance separable codes and placement delivery array (PDA) algorithm, the coded placement phase and delivery phase for combination networks are designed. In addition, for combination networks that satisfy the resolvability property, we extend the PDA algorithm to the case where the users' priority is considered. The proposed scheme greatly reduces the subpacketization level with slightly increasing the transmission rate. The users with higher priority can get the requested content faster. We demonstrate that relays with cache memories can reduce the data transmission in peak time. The gap between the upper bound and lower bound of the transmission rate is gradually decreased with the increase in the users' memories. The proposed scheme will shorten the latency time of the system and improve system efficiency for storage and access of medical data.																	0941-0643	1433-3058				FEB	2020	32	3			SI		867	878		10.1007/s00521-019-04042-x													
J								A novel modular RBF neural network based on a brain-like partition method	NEURAL COMPUTING & APPLICATIONS										Modular neural network; Brain-like partition; Radial basis function (RBF) network; Second-order algorithm	EXTREME LEARNING-MACHINE; ALGORITHM; DESIGN; OPTIMIZATION	In this study, a modular design methodology inherited from cognitive neuroscience and neurophysiology is proposed to develop artificial neural networks, aiming to realize the powerful capability of brain-divide and conquer-when tackling complex problems. First, a density-based brain-like partition method is developed to construct the modular architecture, with a highly connected center in each sub-network as the human brain. The whole task is also divided into different sub-tasks at this stage. Then, a compact radial basis function (RBF) network with fast learning speed and desirable generalization performance is applied as the sub-network to solve the corresponding task. On the one hand, the modular structure helps to improve the ability of neural networks on complex problems by implementing divide and conquer. On the other hand, sub-networks with considerable ability could guarantee the parsimonious and generalization of the entire neural network. Finally, the novel modular RBF (NM-RBF) network is evaluated through multiple benchmark numerical experiments, and results demonstrate that the NM-RBF network is capable of constructing a relative compact architecture during a short learning process with achievable satisfactory generalization performance, showing its effectiveness and outperformance.																	0941-0643	1433-3058				FEB	2020	32	3			SI		899	911		10.1007/s00521-018-3763-z													
J								Text sentiment analysis based on CBOW model and deep learning in big data environment	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Network big data; Text sentiment analysis; CBOW language model; Convolutional neural network; Dropout strategy		For the issues that the accurate and rapid sentiment analysis of comment texts in the network big data environment, a text sentiment analysis method combining Bag of Words (CBOW) language model and deep learning is proposed. First, a vector representation of text is constructed by a CBOW language model based on feedforward neural networks. Then, the Convolutional Neural Network (CNN) is trained through the labeled training set to capture the semantic features of the text. Finally, the Dropout strategy is introduced in the Softmax classifier of traditional CNN, which can effectively prevent the model from over-fitting and has better classification ability. Experimental results on COAE2014 and IMDB datasets show that this method can accurately determine the emotional category of the text and is robust, the accuracy on the two datasets reached 90.5% and 87.2%, respectively.																	1868-5137	1868-5145				FEB	2020	11	2			SI		451	458		10.1007/s12652-018-1095-6													
J								Distributed-observer-based fault diagnosis and fault-tolerant control for time-varying discrete interconnected systems	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Discrete interconnected systems; Fault diagnosis; Fault-tolerant control; Linear matrix inequality	OUTPUT-FEEDBACK CONTROL; INVARIANCE	In this paper, we study distributed-observer-based fault diagnosis and propose a fault-tolerant control approach for a class of discrete interconnected systems. The distributed fault observers are designed to estimate faults based on the improved fast adaptive fault estimation (FAFE) algorithm. As a result of the improved FAFE algorithm, the constraints which are necessary to the general FAFE algorithm can be reduced while the fault estimation accuracy can be maintained. Based on the online fault estimates, the distributed output feedback controllers are developed to accommodate faults. To solve the observers and controllers, the corresponding algorithms are proposed. Finally, various fault situations are considered in detail in a simulation, and the results verify the accuracy of the theory and method.																	1868-5137	1868-5145				FEB	2020	11	2			SI		459	482		10.1007/s12652-018-1130-7													
J								Research on optimization of multi stage yard crane scheduling based on genetic algorithm	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Yard crane scheduling; Multi stage; Genetic algorithm; Dig box coefficient; Optimization		Railway container yard is an important node in container transportation system, plays a very important role in the global logistics integration, improve the railway container freight yard handling equipment operation scheduling level, speed up the internal connection efficiency, reasonable co-ordination of container truck railway container freight yard handling equipment resources configuration can significantly improve the overall efficiency of railway container freight yard, reduce comprehensive operation cost. Study on railway container freight yard yard crane scheduling problem, the problem is to train a known time and external conditions of each container truck into time a container under the proposed "dig box coefficient" concept for decision making for yard crane storing containers, container sequence of the target position, work process, according to the problem, established the mathematical model, the design of multi stage genetic algorithm.																	1868-5137	1868-5145				FEB	2020	11	2			SI		483	494		10.1007/s12652-018-0918-9													
J								An improved apriori algorithm based on support weight matrix for data mining in transaction database	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Data mining; Apriori algorithm; Weight matrix; Support and confidence; k-Itemset		Data mining is a process to discover hidden information or knowledge automatically from huge database. In order to reduce the number of scanning databases and reflect the importance of different items and transaction so as to extract more valuable information, an improved Apriori algorithm is proposed in this paper, which is to build the 0-1 transaction matrix by scanning transaction database for getting the weighted support and confidence. The items and transactions is weighted to reflect the importance in the transaction database. The experiment results, both qualitative and quantitative, have shown that our improved algorithm shortens the running time and reduces the memory requirement and the number of I/O operations. Meanwhile, the support for rare items tends to increase, while the support for other items decreases slightly, thus the hidden and valuable items can be effectively extracted.																	1868-5137	1868-5145				FEB	2020	11	2			SI		495	501		10.1007/s12652-019-01222-4													
J								An edge computing offloading mechanism for mobile peer sensing and network load weak balancing in 5G network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										5G network; Edge computing; Offloading; Mobile peer sensing; Network load balancing	ARCHITECTURE	In large-scale applications, 5G networks are faced with various challenges such as large-scale mobile terminals, dynamic topology and resource management. To solve these problems, we introduced the edge computing, peer entity perception, network load balancing and calculation offload into 5G networks, and proposed a real-time and efficient edge computing offload mechanism. On the one hand, the mechanism, through organic distribution and deep fusion of mobile terminals and cloud computing servers, rationally allocate storage, computing and network services, could achieve a feasible division of labor and collaboration between the quality of the network service and the quality of the user experience. On the other hand, the mechanism can satisfy the needs of massive mobile terminals and peer entities to access 5G networks by introducing peer to peer entities. In addition, the mechanism removes the mismatch features and isomerism between the mobile terminal of the limited 5G network resources and computing power and the high-performance server in real time through mobile peer sensing. Thus, a balance between user experience requirements and network service quality is sought. Simulation experiments and mathematical analysis results demonstrate the advantages of the proposed algorithm in terms of real-time performance, resource management level and computational efficiency.																	1868-5137	1868-5145				FEB	2020	11	2			SI		503	510		10.1007/s12652-018-0970-5													
J								Frame rate up-conversion algorithm based on adaptive-agent motion compensation combined with semantic feature analysis	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Frame rate up-conversion; Motion compensation; Semantic analysis; Feature matching		In this paper, a novel video frame rate up-conversion (FRUC) technique based on adaptive motion compensation is presented by combining the semantic feature analysis for image sequences. Firstly, the image is divided into the sub-patch with the same size and their feature areextracted. And, semantic feature analysis is adopted to generate a more accurate motion vector field from the previous frame to the following frame, and then we select these suitable classification algorithms for patial-temporal patches to divide the frame into moving areas, static areas and fault areas. By using the semantic-based adaptive interpolation, we developed a hierarchical refinement strategy to adaptively correct these motion vector so as to get the interpolation frame. Experimental results show that the performance of our method is better than those of the popular I-FRUC, B-FRUC, F-FRUC and A-FRUC methods in both objective and subjective quality, which has relative advantage for engineering applications.																	1868-5137	1868-5145				FEB	2020	11	2			SI		511	518		10.1007/s12652-018-0974-1													
J								Information fusion for wireless sensor network based on mass deep auto-encoder learning and adaptive weighted D-S evidence synthesis	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										D-S evidence theory; Information fusion; Auto-encoder learning; Wireless sensor network; Sink node; Deep learning; Feature classification	HELP	Since many information fusion algorithms fail to effectively identify the boundary threshold value, this paper integrates D-S evidence theory with stacked auto-encoder, and proposes an information fusion algorithm for wireless sensor network based on mass deep auto-encoder learning and adaptive weighted D-S evidence synthesis. Firstly, feature extraction and classification model is designed in our proposed algorithm to extract and classify the data features of nodes in each cluster, and then these features in the same class are sent to sink node. Finally, the fused feature information from different sensor nodes can be obtained through adaptive weighted D-S evidence theory. Experimental results show that the performance of our proposed method is better than those of the popular comparison methods in both objective and subjective quality, which has relative advantages for engineering applications.																	1868-5137	1868-5145				FEB	2020	11	2			SI		519	526		10.1007/s12652-018-0999-5													
J								Online model-learning algorithm from samples and trajectories	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Model learning; Planning; Multiple-step prediction; Continuous space; MDPs		Learning of the value function and the policy for continuous MDPs is non-trial due to the difficulty in collecting enough data. Model learning can use the collected data effectively, to learn a model and then use the learned model for planning, so as to accelerate the learning of the value function and the policy. Most of the existing works about model learning only concern the improvement of the single-step or multiple-step prediction, while the combination of them may be a better choice. Therefore, we propose an online algorithm where the samples for learning the model are both from the samples and from the trajectories, called Online-ML-ST. Other than the existing work, the trajectories collected in the interaction with the environment are not only used to learn the model offline, but also to learn the model, the value function and the policy online. The experiments are implemented in two typical continuous benchmarks such as the Pole Balancing and Inverted Pendulum, and the result shows that Online-ML-ST outperforms the other three typical methods in learning rate and convergence rate.																	1868-5137	1868-5145				FEB	2020	11	2			SI		527	537		10.1007/s12652-018-1133-4													
J								Research on a new automatic generation algorithm of concept map based on text analysis and association rules mining	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Concept map; Educational data mining; Automatic generation; Text analysis; Text classification; Association rules mining	CONSTRUCTING CONCEPT MAPS; SYSTEMS	As an important knowledge visualization tool, concept map has become a research hotspot in educational data mining. Traditional concept map generation algorithms are difficult to generate concept maps quickly because of their strong reliance on experts' experience. A hybrid TA-ARM algorithm for automatic generation of concept map based on text analysis and association rule mining is proposed. The TA-ARM algorithm fully considers the association rules between concepts, uses the text classification algorithm in text analysis technology instead of manually classify the questions into concepts, and combines the association rule mining method to generate concept maps. The experimental result shows that the TA-ARM algorithm can automatically and rapidly generate the concept map, which not only reduces the impact of outside experts, but can also dynamically adjusts the concept map based on the parameters such as the threshold of confidence between test questions. The concept map generated by the TA-ARM algorithm expresses the association rules between the concepts and the degree of closeness through the associated pairs and relevant degree, and can clearly show the structural associations between concepts. The contrast experiment shows that the quality of the concept map automatically generated by the TA-ARM has a high quality and can visualize the associations between concepts and provide optimization and guidance for knowledge visualization.																	1868-5137	1868-5145				FEB	2020	11	2			SI		539	551		10.1007/s12652-018-0934-9													
J								An alternative approach to determine cycle length of roadway excavation in coal mines	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Roadway excavation; Length of each cycle excavation; Immediate roof; Finite difference method	TUNNEL	Mine excavation rate is a fundamental parameter that controls production and efficiency in underground mines. The length of each cycle excavation (LCE) is vital to decrease the cycle numbers and improve the excavation rate. This paper presents a new approach to determine the LCE for the excavation of roadways in coal mines. Initially, a roof stability model of roadway is established using finite difference method (FDM) to obtain the relationship between principal stresses in the roof of the excavation and the LCE. Finally, specific geological conditions are taken into consideration to determine the better suited LCE for a given roadway. The latter assumes a simple beam-problem, in which a beam fixed at two ends and subjected to uniformly distributed loads. The proposed approach thus combines mining stress and geological conditions to provide an indication of an optimal LCE for the roadways under consideration. The results show a safe increase in excavation rate could be achieved by implementing an LCE based on a 6 m advance with temporary support, followed by the installation of the permanent support using rock bolts installed at 0.8 m spacing.																	1868-5137	1868-5145				FEB	2020	11	2			SI		553	560		10.1007/s12652-018-0987-9													
J								Articulatory and acoustic analyses of Mandarin sentences with different emotions for speaking training of dysphonic disorders	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Dysphonic disorders; Articulatory analyses; Acoustics analyses; Emotional speech; Sentences; Electromagnetic articulatory	SPEECH; KINEMATICS; LEVEL	The aim of the current study was to analyze articulatory and acoustic feature of sentences in Mandarin speakers with different emotions; for articulatory features, the movements of lips and tongue, especially velocities of the lips and tongue, during speech production were analyzed; for acoustic features, formants, fundamental frequency, amplitude and speed were analyzed. 14 subjects with pure Mandarin accent were recruited in this experiment. The subjects were asked to express specified sentences under different emotions (anger, sadness, happiness and neutral), for subsequent articulatory and acoustic analyses. The result indicated that emotions influenced the motion of articulators (tongue and lips) obviously; and then, the motion range of tongue and lips with anger and happiness were larger than with sadness and neutral. Results had been discussed to discover the relations between acoustic and articulatory feature of sentences, similarities and difference of multi-syllables and vowels. This study can be the basement for constructing the functional relation between articulatory parameters and acoustic parameters of emotional speech in the future in order to help individuals with dysphonic disorders to do speaking training.																	1868-5137	1868-5145				FEB	2020	11	2			SI		561	571		10.1007/s12652-018-0942-9													
J								Security authentication technology based on dynamic Bayesian network in Internet of Things	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Internet of Things; Dynamic Bayesian network; Security authentication; Trusted transmission; Combined public key; Security information exchange		With the rapid development of the Internet of Things (IoT) technology in the information society, how to meet the urgent requirements of current users for trusted transmission services is a research hot-spots in the field of the IoT industry. In view of the research status on security authentication in IoT, a security authentication technology based on dynamic Bayesian network combined with trusted protocol is proposed in this paper. Through the introduction of the trusted measurement and the combined public key-based security authentication mechanism in the network, it enhances the security information exchange and considers the node credibility and the path reliability in the routing decision so as to choose a high secure and trusted path for information transmission in IoT. The evaluation results showed that our algorithm achieves a much better security performance than comparison algorithms in overhead and computational complexity for real time applications. In addition, our algorithm has also an adaptive capability and can quickly react to the denial of the service attack, which effectively suppress the threat of abnormal entity in the IoT.																	1868-5137	1868-5145				FEB	2020	11	2			SI		573	580		10.1007/s12652-018-0949-2													
J								Web cache intelligent replacement strategy combined with GDSF and SVM network re-accessed probability prediction	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Web cache; Replacement strategy; Greedy dual size frequency; Re-accessed probability prediction; Support vector machine		Web caching is used to solve the problem of network access delays and network congestion. The intelligent cache replacement strategy directly affects the cache hit rate. This paper proposed a web cache replacement strategy combining greedy dual size frequency (GDSF) algorithm and support vector machine (SVM) re-accessed probability prediction. In the traditional GDSF method, a new objective function is constructed by considering the network object type and object re-accessed probability. The object re-accessed probability is predicted by learning the historical access data through SVM classifier. The simulation results show that compared with the traditional LRU and GDSF schemes, the proposed strategy has a higher request hit rate and byte hit ratio. When the cache size is 16%, the HR and BHR values reached 0.623 and 0.522, respectively.																	1868-5137	1868-5145				FEB	2020	11	2			SI		581	587		10.1007/s12652-018-1109-4													
J								The optimal game model of energy consumption for nodes cooperation in WSN	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										WSN; Game theory; Nodes cooperation; Energy-saving efficiency	WIRELESS SENSOR NETWORKS	It is known that the energy of wireless sensor network nodes is very limited. In order to solve this problem, considering all the nodes in the cluster, we propose an optimal model of energy consumption for nodes cooperation based on game theory. The model establishes the payoff function of nodes' residual energy with the main energy consumption factor of the nodes, energy consumption of communication, as the argument. The nodes' coalitions are constructed through the combination of exhaustion and the sub-regions division. The Nash Equilibrium of cooperative game is solved by using the nodes income distribution method based on Shapley value. With the proposed method, the maximum residual energy of the nodes in the cluster is obtained. The experimental results show that, compared with the game model of non-cooperative, the optimal game model of energy consumption for nodes cooperation has obvious advantages in energy-saving efficiency. As the number of nodes increases, the energy-saving efficiency increases from 12.870 to 38.796%. This paper also verifies that the optimal game model of energy consumption for nodes cooperation has better stability.																	1868-5137	1868-5145				FEB	2020	11	2			SI		589	599		10.1007/s12652-018-1128-1													
J								Social recommendation algorithm based on stochastic gradient matrix decomposition in social network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Matrix decomposition; Recommendation system; Social network; Stochastic gradient		The revenue of an e-commerce system is affected directly by the prediction accuracy of recommendation system. Although recommendation systems have been comprehensively analyzed in the past decade, the study of social-based recommendation systems just started. In this paper, aiming at providing a general method for improving recommendation systems by incorporating social network information, we propose a social recommendation algorithm based on stochastic gradient matrix decomposition in social network so as to improve the prediction accuracy. This paper considered the social network as auxiliary information, and proposed a matrix factorization based on social recommendation algorithm, which systematically illustrate how to design a matrix factorization objective function with social regularization. It constructed a matrix with the social network and the user scoring matrix, and proposed a stochastic gradient descent algorithm for matrix factorization. The empirical analysis on two large datasets demonstrates our proposed algorithm has lower prediction error, and is obviously better than other state-of-the-art methods.																	1868-5137	1868-5145				FEB	2020	11	2			SI		601	608		10.1007/s12652-018-1167-7													
J								Multi crowd fast power control algorithm based on neighborhood opportunistic learning	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Multi intelligent fast power; Neighborhood set; Opportunistic learning; Crowd control	SPECTRUM ACCESS; INTELLIGENT; SWITCH; GAME	For improving the energy-saving power generation scheduling in the electric power industry energy saving and emission reduction capability and enhance the safety and economy of power energy structure, we based on neighborhood opportunity learning multi intelligent power quickly swarm intelligent control algorithm is presented in this paper. Firstly, the proposed algorithm is based on the characteristic value of each neighborhood characteristic value which is obtained by chance mining. The system can obtain the real-time sensing data of each neighborhood power station. This algorithm establishes a neighborhood modular architecture. The algorithm in the opportunity to automate the operation of the module information on the opportunity to learn. Secondly, based on the opportunity to learn and fast variation of the power vector, the algorithm is derived to control the power generation fast group. The algorithm of the multiple power generation control system through the opportunity to control and quickly set the mapping link. Finally, the experimental results show that the proposed algorithm has a significant advantage in real-time, reliability and cost of intelligent management and fast control of power generation to adapt to a variety of power generation devices.																	1868-5137	1868-5145				FEB	2020	11	2			SI		609	615		10.1007/s12652-018-0921-1													
J								Internet of things control mechanism based on controlled object information perception and multimode image signal processing	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Internet of things; Control mechanism; Information perception; Multimode image signal processing		With the scale increase of the internet of things (IOT) control system, the difficulty of information acquisition and image processing for controlled objects increases gradually. To solve the above problems, we proposed a stable and robust IOT control system based on the controlled object information sensing algorithm and multi-mode image signal processing. Firstly, a perceptual real-time control algorithm for the controlled objects information is indicated for controlling, monitoring and maintaining the moving objects at anywhere and anytime. Secondly, in the target location 3D area, the pixels of the controlled object are collected. These pixels are combined with the data characteristics of controlled objects to accurately describe the controlled targets. The multi-mode image signal of the controlled object was computed and processed in the multi-dimensional localization region of the image in real time. Finally, the data acquisition accuracy, signal processing accuracy and system execution efficiency of the proposed scheme were analyzed by mathematical analysis and field experiments. The results show that the proposed algorithm and the designed control mechanism of the IOT have good real-time performance, accuracy and robustness.																	1868-5137	1868-5145				FEB	2020	11	2			SI		617	622		10.1007/s12652-018-1119-2													
J								Bi-dimensional empirical mode decomposition (BEMD) and the stopping criterion based on the number and change of extreme points	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Bi-dimensional empirical mode decomposition; Stopping criterion; Extreme point; Self-adaption		In the bi-dimensional empirical mode decomposition, the determination of the stopping criterion is an important reason for obtaining a complete bi-dimensional intrinsic mode function. An excellent stopping criterion can adapt the image characteristics to stop the decomposition. This paper first analyzes the number and distribution of extreme points reflecting the physical state of the mean surface during the sieving process. Then, bi-dimensional empirical mode decomposition and the stopping criterion based on the number and change of extreme points is proposed. In order to verify the adaptability, effectiveness and reliability of the criterion, the relevant examples were subjected to bi-dimensional empirical mode decomposition experiments under the conditions of the traditional stopping criterion, the GRILL stopping criterion and the stopping criterion proposed in this paper. The results show that the intrinsic mode function obtained by the stop criterion proposed in this paper is more in line with the physical characteristics, more reflective of the original image feature information and with higher accuracy.																	1868-5137	1868-5145				FEB	2020	11	2			SI		623	633		10.1007/s12652-018-0955-4													
J								To strengthen the relationship and the long term trading orientation between the relationship quality and the B-SERVQUAL: focus on the logistics intelligent equipment manufacturing industry	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										B-SERVQUAL; Relationship quality; Relationship strength; Long-term trading orientation; Logistics intelligent equipment	CUSTOMER SATISFACTION; SERVICE QUALITY; TRUST; COMMITMENT; MODEL; DETERMINANTS; RETENTION; OUTCOMES; FIRM	The change in the existing business environment in the B2B enterprise is in response to customers' requests. As customers need convey, the first change in the business environment is the uncertainty of demand while allowing the customer's own industry stage strategy to turn into smooth progression, shorter product life cycles. The technological competition is really overwhelming. As a result, in this study, it is confirmed that the measurement items used in manufacturing industry can be applied to B2B companies, and further, it can contribute to the diffusion into service industries such as finance and public sectors. However, as a future suggestion, this study provides methodology approach according to the long-term and macroscopic dynamic concept applied with multiple techniques, unlike the existing correlation and the regression analysis in applying the methodologies between the service quality and the lower dimension (the economic quality, technical quality, processing quality, sympathy quality and the convenience quality).																	1868-5137	1868-5145				FEB	2020	11	2			SI		635	646		10.1007/s12652-018-1143-2													
J								Improved convolutional neural network combined with rough set theory for data aggregation algorithm	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Granular computing; Rough set theory; Information aggregation; Convolutional neural network; Wireless sensor network; Sink node; Deep learning	WIRELESS SENSOR NETWORKS; DATA FUSION	Data aggregation is a crucial method to relieve the energy consumption in wireless sensor networks (WSNs). However, how to perform data aggregation while preserving data fidelity and confidentiality is a challenging research task. Since many existing aggregation algorithms have large communication and computation overheads, this paper integrates rough set theory with an improved convolutional neural network, and proposes a novel information aggregation algorithm for wireless sensor network. Firstly, a feature extraction model is designed in our proposed algorithm and then trained in Sink node, where the rough set theory is adopted to effectively simplify information and cut down the tagged dimension. Once these data features from granular deep network are extracted by the cluster nodes, they will be sent to the Sink node by cluster heads, so as to reduce the quantity of data transmission and extend the network lifetime. Qualitative and quantitative simulation results show that compared with existing data aggregation algorithms, the energy consumption of our proposed granular CNN model can decrease obviously and the accuracy of the data aggregation can be effectively improved.																	1868-5137	1868-5145				FEB	2020	11	2			SI		647	654		10.1007/s12652-018-1068-9													
J								Intelligent task allocation method based on improved QPSO in multi-agent system	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Multi-Agent system; Task allocation; Improved quantum particle swarm optimization; Population diversity; Load balancing		To improve the task execution efficiency of multi-Agent system (MAS), an intelligent task allocation method based on improved quantum particle swarm optimization (QPSO) algorithm is proposed. Firstly, the task allocation of MAS system is modeled, and the objective function is constructed by considering the ability and load of Agent. Then, the traditional QPSO algorithm is improved by incorporating chaotic mapping, Gaussian distribution mutation operator and dynamic inertia weighting technology to enhance the diversity of the population and make it have stronger search ability. Finally, the improved QPSO algorithm is used to optimize the task allocation model and get the best allocation scheme. Simulation results show that this method can shorten the task completion time and balance the system load.																	1868-5137	1868-5145				FEB	2020	11	2			SI		655	662		10.1007/s12652-019-01242-0													
J								Fermatean fuzzy sets	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Fermatean fuzzy set; Euclidean distance; Multi-criteria decision making; TOPSIS	PYTHAGOREAN MEMBERSHIP GRADES; CRITERIA DECISION-MAKING; TOPSIS METHOD; EXTENSION; FUZZINESS; NEGATION	In this paper, we propose Fermatean fuzzy sets. We compare Fermatean fuzzy sets with Pythagorean fuzzy sets and intuitionistic fuzzy sets. We focus on complement operator of Fermatean fuzzy sets. We find out the fundamental set of operations for the Fermatean fuzzy sets. We define score function and accuracy function for ranking of Fermatean fuzzy sets. In addition, we also study Euclidean distance between two Fermatean fuzzy sets. Later, we establish a Fermatean fuzzy TOPSIS method to fix multiple criteria decision-making problem. Ultimately, an interpretative example is stated in details to justify the elaborated method and to illustrate its viability and usefulness.																	1868-5137	1868-5145				FEB	2020	11	2			SI		663	674		10.1007/s12652-019-01377-0													
J								A comparative analysis of pooling strategies for convolutional neural network based Hindi ASR	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										CNN; Max-pooling; Pooling; Speech recognition	PERFORMANCE EVALUATION; SPEECH; SYSTEM	State-of-the-art speech recognition is witnessing its golden era as convolutional neural network (CNN) becomes the leader in this domain. CNN based acoustic models have been shown significant improvement in speech recognition tasks. This improvement is achieved due to the special components of CNN, i.e., local filters, weight sharing, and pooling. However, lack of core understanding renders this powerful model as a black-box machine. Although, CNN is performing well in speech recognition still further investigation will help in achieving better recognition rate. Pooling is a very important component of CNN that reduces the dimensionality of the feature-map and offers compact feature representation. Various pooling methods like max pooling, average pooling, stochastic pooling, mixed pooling, Lp pooling, multi-scale orderless pooling, and spectral pooling have their own advantages and disadvantages. In this paper, we deeply explore the state-of-the-art pooling for speech recognition tasks. This paper also helps to investigate that which pooling method performs well in which condition. This work explores different pooling methods for different architectures on Hindi speech dataset. The experimental results show that max pooling performs well when tested for clean speech and stochastic pooling works well in the noisy environment.																	1868-5137	1868-5145				FEB	2020	11	2			SI		675	691		10.1007/s12652-019-01325-y													
J								Smartphone-based respiratory rate estimation using photoplethysmographic imaging and discrete wavelet transform	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Smartphone; Respiratory rate; Photoplethysmographic; Discrete wavelet transform	SIGNALS; FREQUENCY; ALGORITHM	Respiratory rate is a key vital sign that needs daily monitoring for hospital patients in general and those with respiratory conditions in particular. Moreover, it is a predictor of major heart conditions. Yet, studies have shown that it is widely neglected in hospital care due partially to the discomfort caused by the required equipment. In this paper, we propose a smartphone-based method for accurate measurement of the respiratory rate using the video of the skin surface as recorded by the smartphone built-in camera in the presence of the flash light. From this input, we use frame averaging to extract a photoplethysmographic signal of the red, green, and blue channels. Next, we apply discrete wavelet transform on the best representative photoplethysmographic signal for respiratory signal extraction and estimate the rate. Fifteen subjects participated in the testing and evaluation. The maximum absolute error was 0.67 breaths/min, whereas the root mean square error was 0.366 breaths/min. The average percentage error and average percentage accuracy using our approach were 2.2%, 97.8% respectively. A comparison with other works in the literature reveal a superior performance in terms of accuracy, ease of use, and cost.																	1868-5137	1868-5145				FEB	2020	11	2			SI		693	703		10.1007/s12652-019-01339-6													
J								Modeling of individual differences in driver behavior	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Driver agent; Driving behavior; Car-following; Lane change; Modeling; Simulation; Personality profile; Individual differences	CAR-FOLLOWING MODELS; DRIVING BEHAVIOR; PERSONALITY	Computational transportation is a scientific discipline which uses traffic flow simulation for intervention design and analysis. Realistic traffic flow simulation depends on realistic computational modeling of individual agents such as drivers. Whereas, realistic agent model relies on realistic modeling of microscopic driver behaviors. IDM and MOBIL are considered de-facto car-following and lane change models respectively. All the prominent microscopic models have been developed with engineering perspective i.e. to reproduce perfect driving behavior. Whereas human driving behavior exhibit individual difference and is prone to risks and errors. This study focuses on development of a personality-based model of driver behavior. The parameters that have been modeled to represent driving preferences have been identified. In their existing forms, model parameters could be assigned arbitrary values from a prescribed range to define different driver profiles. This way, theoretically, infinite driver profiles could be created, many of which does not exist in real. Whereas, literature of traffic psychology suggests that there are few prevalent classes of drivers which exhibit certain behavioral patterns. These classes are characterized with the help of human personality. In proposed study, a relationship between personality traits and model parameters have been modeled. This enhancement may reproduce individual differences in driving behaviors.																	1868-5137	1868-5145				FEB	2020	11	2			SI		705	718		10.1007/s12652-019-01313-2													
J								Feature selection method based on hybrid data transformation and binary binomial cuckoo search	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Feature selection; Cuckoo search; Data transformation; Principal component analysis; Fast independent component analysis	INDEPENDENT COMPONENT ANALYSIS; PARTICLE SWARM OPTIMIZATION; DIFFERENTIAL EVOLUTION; ANT COLONY; ALGORITHM; CLASSIFICATION; REDUCTION	Feature selection is one of the key components of data mining and machine learning domain that selects the best subset of features with respect to target data by removing irrelevant data. However, it is a complex task to select optimal set of features from a dataset using traditional feature selection methods, as for n number of features, 2n feature subsets are possible. Therefore, this paper introduces a novel metaheuristics-based feature selection method based binomial cuckoo search. Generally, metaheuristics-based feature selection methods suffer with stability issue since they select different set of features in different runs. Hence, to deal with stability issue, a hybrid data transformation method based on principal component analysis and fast independent component analysis has also been introduced. The proposed hybrid data transformation method first transforms the original data thereafter proposed binary binomial cuckoo search method is used to elect the best subset of features. The proposed feature selection method maximizes the classification accuracy and minimizes the number of selected features. The performance of the proposed method has been tested on the fourteen feature selection benchmark datasets taken from UCI repository and compared with other latest state-of- the art approaches including binary cuckoo search, binary bat algorithm, binary gravitational search algorithm, binary whale optimization with simulated annealing, and binary grey wolf optimization. Further, statistical analysis has also been carried out to validate the efficacy of the proposed method.																	1868-5137	1868-5145				FEB	2020	11	2			SI		719	738		10.1007/s12652-019-01330-1													
J								A new multi-criteria decision-making method based on intuitionistic fuzzy information and its application to fault detection in a machine	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Intuitionistic fuzzy set; Intuitionistic fuzzy information measure; MCDM; TOPSIS	MULTIPLE REFERENCE POINTS; ENTROPY; SETS; WEIGHTS; ALPHA	Intuitionistic Fuzzy Sets (IFSs) introduced by Atanassov are well suitable to deal with hesitancy and vagueness. In this communication, a new bi-parametric exponential information measure based on IFSs is introduced. Besides establishing its validity, some of its major properties are also discussed. Further, a new multi-criteria decision-making method based on the proposed IF measure and weighted correlation coefficients is introduced. The proposed method is utilized in detecting the fault in a machine that is not working properly through a numerical example.																	1868-5137	1868-5145				FEB	2020	11	2			SI		739	753		10.1007/s12652-019-01322-1													
J								An integrated framework for blockchain inspired fog communications and computing in internet of vehicles	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Distributed consensus; Blockchain paradigm; Fog communications and computing; Internet of vehicles	AD HOC NETWORKS; TRANSMISSION; SECURITY; MODEL	In this paper, the novel fog communications and computing paradigm is addressed by presenting an integrated system architecture, that is applied to achieve a full context awareness for vehicular networks and, consequently, to react on traffic anomalous conditions. In particular, we propose to adopt a specific co-designed approach involving application and networks layers. For the latter one, as no infrastructure usually exists, effective routing protocols are needed to guarantee a certain level of reliability of the information collected from individual vehicles. As a consequence, we investigated classical epidemic flooding based, network coding inspired and chord protocols. Besides, we resort to blockchain principle to design a distributed consensus sensing application. The system has been tested by resorting to OMNeT++ framework for its modularity, high fidelity and flexibility. Performance analysis has been conducted over realistic scenarios in terms of consensus making overhead, latency and scalability, pointing out the better trade-off allowing the overlay P2P network formation and the complete context awareness achieved by the vehicles community.																	1868-5137	1868-5145				FEB	2020	11	2			SI		755	762		10.1007/s12652-019-01476-y													
J								Hybrid flower pollination and pattern search algorithm optimized sliding mode controller for deregulated AGC system	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Output feedback sliding mode controller (OFSMC); flower pollination algorithm (FPA); Pattern search (PS); Generation rate constraint (GRC); Governor dead band (GDB)	LOAD-FREQUENCY CONTROL; AUTOMATIC-GENERATION CONTROL; VARIABLE-STRUCTURE CONTROLLER; POWER-SYSTEM; PID CONTROLLER; DESIGN; LFC; SIMULATION	A new evolutionary optimisation technique, namely hybrid flower pollination and pattern search (hFPA-PS) technique is developed to tune output feedback sliding mode controller (OFSMC)for a multi-sources interconnected deregulated automatic generation control (AGC) system. The developed algorithm is justified considering popular benchmark functions. The developed algorithm is applied first time for the AGC system, to tune parameters of different classical controllers. The supremacy of recommended approach is established by competing the dynamic system performances with FPA, fruit fly optimization (FOA) and particle swarm optimization (PSO) technique for different power scenario for different classical controllers. Dynamic response of the system with hFPA-PS optimized OFSMC is established to be better than the classical controllers. The dynamic response of the system is analysed in the presence of generation rate constraint (GRC), governor dead band (GDB) and time delay. Additionally, the supremacy of recommended approach is analysed with sensitivity analysis, under uncertainty of system parameters.																	1868-5137	1868-5145				FEB	2020	11	2			SI		763	776		10.1007/s12652-019-01348-5													
J								Pareto set based optimized routing in opportunistic network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Opportunistic networks; Delay-tolerant-network; Prophet; Hibop; A*OR; HBPR; EHBPR; GAER; Energy; ONE simulator	PROTOCOL	In opportunistic networks (OppNets), messages are transmitted between nodes in an opportunistic fashion following a store-carry-and-forward approach and a routing protocol, with the hope to reach their destinations. In existing routing protocols for OppNets, different parameters such node's delivery predictability, node's mobility, distance with respect to destination, node's energy consumption, contact duration between nodes, to name a few, are considered in the selection of the best suitable next hop to carry the message toward the destination. This paper proposes a novel routing protocol that uses three parameters, namely the contact duration, the node's residual energy and the signal- to-noise-plus-interference-ratio (SINR) simultaneously to decide on the selection of the best next forwarder for a message. Simulation results using the Opportunistic Network Environment (ONE) simulator are presented, demonstrating the effectiveness of the proposed technique.																	1868-5137	1868-5145				FEB	2020	11	2			SI		777	797		10.1007/s12652-019-01337-8													
J								Evaluation of heterogeneous uncertain information fusion	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Probability distribution; Possibility distribution; Gini index; Specificity measure; Possibility-probability transformation	PROBABILITY; TRANSFORMATIONS; RULES	In this paper our motivation is to provide evaluation approaches for information fusion where both possibilistic uncertainty and probabilistic uncertainty occurs. For effective utilization of such diverse data, fusion is used to assist decision-making. However it is necessary to evaluate the results in order to assess their value. Our innovation is the use of information and specificity measures to provide assessments of the fusion results. In particular we investigate transformation-based approaches to the problem of combining possibility and probability distributions. We consider a total heterogeneous fusion process as having three phases in general. Phase 1 is a transformation phase used to produce homogenous data representations. Specifically we explore two transformations-probability to possibility and vice versa. Phase 2 consists of specific aggregation functions operating on the homogenous formatted data. For aggregation functions we representatively cover a range of possible functions by using min, max and average. Phase 3 consists of the applicable assessment measures on the fusion results. Two examples of the complete approach for representative probability and possibility distributions are worked out in full detail and evaluation techniques are used to compare the various results. Finally, general evaluative comparisons of the approaches are given based on extreme bounding cases of completely certain and uncertain probability and possibility distributions. Our contribution then has been to provide approaches to understand which aggregation functions from the min, avg, max spectrum and which transformations would be most useful for the fusion result.																	1868-5137	1868-5145				FEB	2020	11	2			SI		799	811		10.1007/s12652-019-01320-3													
J								CatchPhish: detection of phishing websites by inspecting URLs	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										URL; Phishing; Anti-phishing; TF-IDF; Hostname; Random forest	MALICIOUS URLS	There exists many anti-phishing techniques which use source code-based features and third party services to detect the phishing sites. These techniques have some limitations and one of them is that they fail to handle drive-by-downloads. They also use third-party services for the detection of phishing URLs which delay the classification process. Hence, in this paper, we propose a light-weight application, CatchPhish which predicts the URL legitimacy without visiting the website. The proposed technique uses hostname, full URL, Term Frequency-Inverse Document Frequency (TF-IDF) features and phish-hinted words from the suspicious URL for the classification using the Random forest classifier. The proposed model with only TF-IDF features on our dataset achieved an accuracy of 93.25%. Experiment with TF-IDF and hand-crafted features achieved a significant accuracy of 94.26% on our dataset and an accuracy of 98.25%, 97.49% on benchmark datasets which is much better than the existing baseline models.																	1868-5137	1868-5145				FEB	2020	11	2			SI		813	825		10.1007/s12652-019-01311-4													
J								Simulation study on pedestrian strategy choice based on direction fuzzy visual field	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Evolutionary game; Direction fuzzy visual field; Dynamic equilibrium; Cooperation strategy; Competition strategy	HUMAN PERSONALITY; EVACUATION; MODEL; FLOW	In the process of evacuation of pedestrian flow, the dynamic strategy choosing among pedestrians greatly influence evacuation efficiency. To address the problem of this dynamic evolutionary game, the direction fuzzy visual field of a moving pedestrian is defined, which fully takes into account the differences between pedestrians in terms of visual field selection. Based on the direction fuzzy visual field, this paper constructs a dynamic evolutionary game model and analyzes the formation and dynamic development of pedestrian flow strategy choosing. It presents the payoff matrix in which heterogeneous pedestrians adjust their strategy according to different payoff. The study results show that the model can effectively demonstrate macroscopic self-organization phenomena of pedestrian flow. The dynamic game equilibrium is related to the radius of the direction fuzzy visual field, the cost of strategic adjustment, the pedestrian density, and the system scale. The radius of the direction fuzzy visual field is bigger, the pedestrians can have more information, and they more likely to adopt competition strategy. There is a critical value for pedestrian density. When pedestrian density is greater than this critical value, the pedestrians will feel crowded and tend to adopt competition strategy. The cost of strategic adjustment is smaller, the pedestrians are more likely to adopt cooperation strategy, a few competitors will gather near the walls. When the cost of strategic adjustment is bigger, the pedestrians tend to adopt competition strategy, a small number of cooperators are gathering near the walls. Therefore, the walls play a very important role in pedestrian flow evacuation. The cellular system scale is bigger, pedestrians are more likely to cooperate.																	1868-5137	1868-5145				FEB	2020	11	2			SI		827	843		10.1007/s12652-019-01389-w													
J								A reckoning algorithm for the prediction of arriving passengers for subway station networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Arrival passenger estimation; Passenger flow distribution; Travel time chains (TTCs); Reckoning algorithm; Subway	CAPACITY; DESIGN; MODEL; FLOW	Knowing the volume of arriving passengers (APs) is fundamental for optimizing their paths through subway stations and evacuating them under emergency conditions. To predict AP volume online, we first analyze arrival and departure parameters and discuss the relationships among various parameters to determine the train a passenger will most likely take. Interconnecting stations and transfer paths among stations are considered direct connections and direct transfer connections, respectively, to define and construct traveling route sets. Then, travel time chains (TTCs) of transfer and nontransfer passengers are constructed to illustrate the possible routes and time costs between the origin and destination (O/D) stations of passengers. Furthermore, based on TTCs, train capacities and the inbound and outbound times of passengers accessed from an automated fare collection system, we predict the AP volumes at specified stations using a stage-by-stage reckoning algorithm in real time. Finally, to validate the model and the algorithm, we estimate the AP volume for the Beijing Subway network.																	1868-5137	1868-5145				FEB	2020	11	2			SI		845	864		10.1007/s12652-019-01198-1													
J								Public-key encryption with keyword search: a generic construction secure against online and offline keyword guessing attacks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Searchable encryption; Public-key encryption; Online and offline keyword guessing attacks	IDENTITY-BASED ENCRYPTION; SCHEME; LATTICE	In public-key setting, the problem of searching for keywords in encrypted data is handled by the notion of public-key encryption with keyword search (PEKS). An important challenge in designing secure PEKS schemes is providing resistance against variants of an attack known as the keyword guessing attack (KGA). Basically, by KGA, an adversary is able to determine the searched keyword through using the data communicated in the search process. Security against offline KGA performed by both inside/outside adversaries is well-studied in the literature. However, this is not true about the online version. In this paper, we employ a technique called ciphertext re-randomization to propose a generic construction for designing PEKS schemes which are secure against both online and offline KGAs performed by outsiders. We show that compared to existing literature, our construction is more efficient in terms of computational and communication costs.																	1868-5137	1868-5145				FEB	2020	11	2			SI		879	890		10.1007/s12652-019-01254-w													
J								Community detection in dynamic signed network: an intimacy evolutionary clustering algorithm	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Signed networks; Similarity; Neighbor; Dynamic evolution; Consensus	SYNCHRONIZATION	The tremendous development of community detection in dynamic networks have been witnessed in recent years. In this paper, intimacy evolutionary clustering algorithm is proposed to detect community structure in dynamic networks. Firstly, the time weighted similarity matrix is utilized and calculated to grasp time variation during the community evolution. Secondly, the differential equations are adopted to learn the intimacy evolutionary behaviors. During the interactions, intimacy between two nodes would be updated based on the iteration model. Nodes with higher intimacy would gather into the same cluster and nodes with lower intimacy would get away, then the community structure would be formed in dynamic networks. The extensive experiments are conducted on both real-world and synthetic signed networks to show the efficiency of detection performance. Moreover, the presented method achieves better detection performance compared with several better algorithms in terms of detection accuracy.																	1868-5137	1868-5145				FEB	2020	11	2			SI		891	900		10.1007/s12652-019-01215-3													
J								FIS and hybrid ABC-PSO based optimal capacitor placement and sizing for radial distribution networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Capacitor placement; Fuzzy inference system (FIS); Hybrid ABC-PSO algorithm; Loss sensitivity factor (LSF); Optimal capacitor sizing; Radial distribution network (RDN); Sensitive nodes	LOAD-FLOW SOLUTION; DISTRIBUTION-SYSTEMS	Electric power generated from the power stations can be distributed to the consumers using different networks. Among those the radial distribution network is the attractive one. Power loss occurred in this network can be reduced and the voltage profile can be improved by placing optimal sized capacitors. There are various algorithms and techniques, which have been used previously to inspect the situation where the capacitors are needed to be placed at suitable nodes with optimal sized. This paper proposes another near approach, which will decide the most appropriate nodes on the essential feeders, laterals and sublaterals of any radial distribution network for ideal capacitor integration in order to enhance power loss reduction and also to enhance the voltage profile utilizing loss sensitivity factor (LSF) strategy and hybrid ABC-PSO calculation. The established LSF approach is utilized here to locate the most appropriate nodes and the optimal capacitor size can be settled with the hybrid ABC-PSO calculation. Capacitor size is an exceedingly nonlinear issue and henceforth fuzzy inference system (FIS) technique is chosen as the most suitable transports for the capacitor position. The sizes of the capacitors relating to least genuine power misfortune are resolved. The proposed technique has been implemented on IEEE 69-node and 34-node radial distribution networks.																	1868-5137	1868-5145				FEB	2020	11	2			SI		901	916		10.1007/s12652-019-01216-2													
J								Locality preserving projection least squares twin support vector machine for pattern classification	PATTERN ANALYSIS AND APPLICATIONS										Pattern classification; Locality preserving projection; Least squares; Twin support vector machine; Projection twin support vector machine	DIMENSIONALITY REDUCTION; REGULARIZATION	During the last few years, multiple surface classification algorithms, such as twin support vector machine (TWSVM), least squares twin support vector machine (LSTSVM) and least squares projection twin support vector machine (LSPTSVM), have attracted much attention. However, these algorithms did not consider the local geometrical structure information of training samples. To alleviate this problem, in this paper, a locality preserving projection least squares twin support vector machine (LPPLSTSVM) is presented by introducing the basic idea of the locality preserving projection into LSPTSVM. This method not only inherits the ability of TWSVM, LSTSVM and LSPTSVM for pattern classification, but also fully considers the local geometrical structure between samples and shows the local underlying discriminatory information. Experimental results conducted on both synthetic and real-world datasets illustrate the effectiveness of the proposed LPPLSTSVM method.																	1433-7541	1433-755X				FEB	2020	23	1					1	13		10.1007/s10044-018-0728-x													
J								Recognition of mixture control chart patterns based on fusion feature reduction and fireworks algorithm-optimized MSVM	PATTERN ANALYSIS AND APPLICATIONS										Control chart patterns recognition; Multiclass support vector machines; Fusion feature reduction; Fireworks algorithm; Parameters optimization	IDENTIFICATION; CLASSIFICATION; DECOMPOSITION; SELECTION; KPCA	Unnatural control chart patterns (CCPs) can be associated with the quality problems of the production process. It is quite critical to detect and identify these patterns effectively based on process data. Various machine learning techniques to CCPs recognition have been studied on the process only suffer from basic CCPs of unnatural patterns. Practical production process data may be the combination of two or more basic patterns simultaneously in reality. This paper proposes a mixture CCPs recognition method based on fusion feature reduction (FFR) and fireworks algorithm-optimized multiclass support vector machine (MSVM). FFR algorithm consists of three main sub-networks: statistical and shape features, features fusion and kernel principal component analysis feature dimensionality reduction, which make the features more effective. In MSVM classifier algorithm, the kernel function parameters play a very significant role in mixture CCPs recognition accuracy. Therefore, fireworks algorithm is proposed to select the two-dimensional parameters of the classifier. The results of the proposed algorithm are benchmarked with popular genetic algorithm and particle swarm optimization methods. Simulation results demonstrate that the proposed method can gain the higher recognition accuracy and significantly reduce the running time.																	1433-7541	1433-755X				FEB	2020	23	1					15	26		10.1007/s10044-018-0748-6													
J								Using semantic context for multiple concepts detection in still images	PATTERN ANALYSIS AND APPLICATIONS										Multimedia semantic indexing; Multiple concepts detection; Concepts pairs; Triplets of concepts; Learned descriptors; Deep learning; Semantic context; Pascal VOC; Multi-concept; Bi-concepts; Tri-concepts	DESCRIPTOR; FEATURES; COLOR; OPTIMIZATION; RETRIEVAL	Multimedia documents indexing systems performances have been improved significantly in recent years, especially after the involvement of deep learning approaches. However, this progress remains insufficient with the evolution of users' needs that become complex in terms of semantics and the number of words that compose their queries. So, it is important to think about indexing images by a group of concepts simultaneously (multi-concepts) and not just single ones. This would allow systems to better respond to queries composed of several terms. This task is much more difficult than indexing images by single concepts. Multi-concepts detection in images has been little dealt in the state of the art compared to the detection of visual single concepts. On the other hand, the use of context has proved its effectiveness in the field of multimedia semantic indexing. In this work, we propose two approaches that consider the semantic context for multi-concepts detection in still images. We tested and evaluated our proposal on the international standard corpus Pascal VOC for the detection of concepts pairs and triplets of concepts. Our contributions have shown that context is useful and improves multi-concepts detection in images. The combination of the use of semantic context and deep learning-based features yielded much better results than those of the state of the art. This difference in performance is estimated by a relative gain on mean average precision reaching + 70% for concepts pairs and + 34% for the case of triplets of concepts.																	1433-7541	1433-755X				FEB	2020	23	1					27	44		10.1007/s10044-018-0761-9													
J								Design of a frequency spectrum-based versatile two-dimensional arbitrary shape filter bank: application to contact lens detection	PATTERN ANALYSIS AND APPLICATIONS										Filter bank; Frequency spectrum; Feature extraction; Eigenfilter; Contact lens detection	IRIS RECOGNITION; WAVELETS	A filter bank (FB) is an integral part of any image processing system. The designing of a FB generally involves modifying an existing FB or focusing on a particular property of the filter bank. Such FBs limit their use to a particular image. Through our work, we have devised a unique and novel approach for designing a two-dimensional arbitrary shape filter bank (2-D ASFB). This FB is inherently 2-D and eliminates the need for transforming a one-dimensional FB into 2-D. Its arbitrary nature expands its application to any image as compared to regular-shaped FBs currently in use. The novelty of the design lies in the fact that the designed FB can match the frequency spectrum of any image by reducing the error function between the frequency spectrum and the desired filter response of the FB. The error function has been minimized using the eigenfilter approach. After designing the low-pass analysis filter, perfect reconstruction constraint has been used to get a low-pass synthesis filter. In this paper, we have demonstrated the use of the 2-D ASFB specifically for contact lens detection (CLD). The proposed CLD system focuses on feature extraction using the 2-D ASFB. The support vector machine classifier is the same as in the existing systems. The results show improved correct classification rate as compared to the existing systems for IIITD and ND2013 contact lens database. This 2-D ASFB overcomes limitations posed by the existing filter banks with respect to separability, directionality, orthogonality, and shape. This FB can be effectively applied to any feature extraction application such as pattern recognition, biometrics, medical image processing.																	1433-7541	1433-755X				FEB	2020	23	1					45	58		10.1007/s10044-018-0764-6													
J								Automatic computing of number of clusters for color image segmentation employing fuzzy c-means by extracting chromaticity features of colors	PATTERN ANALYSIS AND APPLICATIONS										Competitive neural networks; Color classification; Image segmentation; Color spaces	UNSUPERVISED SEGMENTATION; ALGORITHM; SPACE; REGION; MODEL	In this paper we introduce a method for color image segmentation by computing automatically the number of clusters the data, pixels, are divided into using fuzzy c-means. In several works the number of clusters is defined by the user. In other ones the number of clusters is computed by obtaining the number of dominant colors, which is determined with unsupervised neural networks (NN) trained with the image's colors; the number of dominant colors is defined by the number of the most activated neurons. The drawbacks with this approach are as follows: (1) The NN must be trained every time a new image is given and (2) despite employing different color spaces, the intensity data of colors are used, so the undesired effects of non-uniform illumination may affect computing the number of dominant colors. Our proposal consists in processing the images with an unsupervised NN trained previously with chromaticity samples of different colors; the number of the neurons with the highest activation occurrences defines the number of clusters the image is segmented. By training the NN with chromatic data of colors it can be employed to process any image without training it again, and our approach is, to some extent, robust to non-uniform illumination. We perform experiments with the images of the Berkeley segmentation database, using competitive NN and self-organizing maps; we compute and compare the quantitative evaluation of the segmented images obtained with related works using the probabilistic random index and variation of information metrics.																	1433-7541	1433-755X				FEB	2020	23	1					59	84		10.1007/s10044-018-0729-9													
J								An image segmentation method based on Mumford-Shah model with mask factor and neighborhood factor	PATTERN ANALYSIS AND APPLICATIONS										Image segmentation; Mumford-Shah model; Mask factor; Neighborhood factor; Level set method	ALGORITHMS	A novel image segmentation model is proposed to improve the stability of existing segmentation methods. In the proposed model, we introduce two factors into the Mumford-Shah model, including mask factor and neighborhood factor. Firstly, the mask factor can express the image more accurately. Therefore, the new segmentation model can more realistically reflect the structure of the image. Moreover, neighborhood factor is used to constrain the evolution of the initial contour. Then the segmentation model is converted into an equivalent form by a level set function. At last, the model can be solved in a simple way based on partial differential equations and extreme values. The experimental results show the proposed method could generate accurate segmentation results, and the segmentation results are not sensitive to initial contour and external disturbances, such as noise and blurring.																	1433-7541	1433-755X				FEB	2020	23	1					85	94		10.1007/s10044-018-0730-3													
J								Towards instance-dependent label noise-tolerant classification: a probabilistic approach	PATTERN ANALYSIS AND APPLICATIONS										Instance-dependent label noise; Classification; Logistic regression	DISCRIMINANT-ANALYSIS; CANCER	Learning from labelled data is becoming more and more challenging due to inherent imperfection of training labels. Existing label noise-tolerant learning machines were primarily designed to tackle class-conditional noise which occurs at random, independently from input instances. However, relatively less attention was given to a more general type of label noise which is influenced by input features. In this paper, we try to address the problem of learning a classifier in the presence of instance-dependent label noise by developing a novel label noise model which is expected to capture the variation of label noise rate within a class. This is accomplished by adopting a probability density function of a mixture of Gaussians to approximate the label flipping probabilities. Experimental results demonstrate the effectiveness of the proposed method over existing approaches.																	1433-7541	1433-755X				FEB	2020	23	1					95	111		10.1007/s10044-018-0750-z													
J								Rare association rule mining from incremental databases	PATTERN ANALYSIS AND APPLICATIONS										Rare pattern; Association rule; Rare association rule; Incremental mining	FREQUENT PATTERNS; GENERATION; ALGORITHM; TREE	Rare association rule mining is an imperative field of data mining that attempts to identify rare correlations among the items in a database. Although numerous attempts pertaining to rare association rule mining can be found in the literature, there are still certain issues that need utmost attention. The most prominent one among them is the rare association rule mining from incremental databases. The existing rare association rule mining techniques are capable of operating only on static databases, assuming that the entire database to be operated on is available during the outset of the mining process. Inclusion of new records, however, may lead to the generation of some new interesting rules from the current set of data, invalidating the previously extracted significant rare association rules. Executing the entire mining process from scratch for the newly arrived set of data could be a tedious affair. With a view to resolve the issue of incremental rare association rule mining, this study presents a single-pass tree-based approach for extracting rare association rules when new data are inserted into the original database. The proposed approach is capable of generating the complete set of frequent and rare patterns without rescanning the updated database and reconstructing the entire tree structure when new transactions are added to the existent database. Experimental evaluation has been carried out on several benchmark real and synthetic datasets to analyze the efficiency of the proposed approach. Furthermore, to assess its applicability in real-world applications, experimental analysis has been performed on a real geological dataset where earthquake records are incrementally being added on an annual basis. Comparative performance analysis demonstrates the preeminence of proposed approach over existing frequent and rare association rule mining techniques.																	1433-7541	1433-755X				FEB	2020	23	1					113	134		10.1007/s10044-018-0759-3													
J								Iterative scheme-inspired network for impulse noise removal	PATTERN ANALYSIS AND APPLICATIONS										Impulse noise removal; Deep learning; Augmented Lagrangian; Supervised learning	SPARSE REPRESENTATION; REDUNDANT REPRESENTATIONS; IMAGE; REGULARIZATION; FILTER	This paper presents a supervised data-driven algorithm for impulse noise removal via iterative scheme-inspired network (IIN). IIN is defined over a data flow graph, which is derived from the iterative procedures in Alternating Direction Method of Multipliers (ADMM) algorithm for optimizing the L1-guided variational model. In the training phase, the L1-minimization is reformulated into an augmented Lagrangian scheme through adding a new auxiliary variable. In the testing phase, it has computational overhead similar to ADMM but uses optimized parameters learned from the training data for restoration task. Experimental results demonstrate that the newly proposed method can obtain very significantly superior performance than current state-of-the-art variational and dictionary learning-based approaches for salt-and-pepper noise removal.																	1433-7541	1433-755X				FEB	2020	23	1					135	145		10.1007/s10044-018-0762-8													
J								Understanding temporal structure for video captioning	PATTERN ANALYSIS AND APPLICATIONS										Video captioning; Deep learning; Attention models; Hierarchical neural networks		Recent research in convolutional and recurrent neural networks has fueled incredible advances in video understanding. We propose a video captioning framework that achieves the performance and quality necessary to be deployed in distributed surveillance systems. Our method combines an efficient hierarchical architecture with novel attention mechanisms at both the local and global levels. By shifting focus to different spatiotemporal locations, attention mechanisms correlate sequential outputs with activation maps, offering a clever way to adaptively combine multiple frames and locations of video. As soft attention mixing weights are solved via back-propagation, the number of weights or input frames needs to be known in advance. To remove this restriction, our video understanding framework combines continuous attention mechanisms over a family of Gaussian distributions. Our efficient multistream hierarchical model combines a recurrent architecture with a soft hierarchy layer using both equally spaced and dynamically localized boundary cuts. As opposed to costly volumetric attention approaches, we use video attributes to steer temporal attention. Our fully learnable end-to-end approach helps predict salient temporal regions of action/objects in the video. We demonstrate state-of-the-art captioning results on the popular MSVD, MSR-VTT and M-VAD video datasets and compare several variants of the algorithm suitable for real-time applications. By adjusting the frame rate, we show a single computer can generate effective video captions for 100 simultaneous cameras. We additionally perform studies to show how bit rate compression modifies captioning results.																	1433-7541	1433-755X				FEB	2020	23	1					147	159		10.1007/s10044-018-00770-3													
J								A recursive partitioning approach for subgroup identification in brain-behaviour correlation analysis	PATTERN ANALYSIS AND APPLICATIONS										Subgroup identification; Recursive partitioning; Brain-behaviour correlation; Partial correlation; Unbiased variable selection	RISK DECISION-MAKING; SPLIT SELECTION; TREES; TASK	In neural correlates studies, the goal is to understand the brain-behaviour relationship characterized by correlation between brain activation responses and human behaviour measures. Such correlation depends on subject-related covariates such as age and gender, so it is necessary to identify subgroups within the population that have different brain-behaviour correlations. The subgrouping is made by manual specification in current practice, which is inefficient and may ignore potential covariates whose effects are unknown in the literature. This study proposes a recursive partitioning approach, called correlation tree, for automatic subgroup identification in brain-behaviour correlation analysis. In constructing a correlation tree, the split variable at each node is selected through an unbiased variable selection method based on partial correlation test, and then, the optimal cutpoint of the selected split variable is determined through exhaustive search under an objective function. Three types of meaningful objective functions are considered to meet various practical needs. Results of simulation and application to real data from optical brain imaging demonstrate effectiveness of the proposed approach.																	1433-7541	1433-755X				FEB	2020	23	1					161	177		10.1007/s10044-018-00775-y													
J								Enhancement of speech signal using diminished empirical mean curve decomposition-based adaptive Wiener filtering	PATTERN ANALYSIS AND APPLICATIONS										Speech signal; Enhancement; STFT; D-EMCD; Wiener filtering	NOISE; MODEL	During the last few decades, speech signal enhancement has been one of the wide-spreading research topics. Numerous algorithms are being proposed to enhance the perceptibility and the quality of speech signal. These algorithms are often formulated to recover the clear signal from the signals that are ruined by noise. Usually, short-time Fourier transform and wavelet transform are widely used to process the speech signal. This paper attempts to overcome the regular drawbacks of the speech enhancement algorithms. As the frequency domain has good noise-removing ability, the short-time Fourier domain is also aimed to enhance the speech. Additionally, this paper introduces a decomposition model, named diminished empirical mean curve decomposition, to adaptively tune the Wiener filtering process and to accomplish effective speech enhancement. The performances of the proposed method and the conventional methods are compared, and it is observed that the proposed method is superior to the conventional methods.																	1433-7541	1433-755X				FEB	2020	23	1					179	198		10.1007/s10044-018-00768-x													
J								A novel nearest interest point classifier for offline Tamil handwritten character recognition	PATTERN ANALYSIS AND APPLICATIONS										Handwritten character recognition; Pattern recognition; Classification; Variable length; High dimensional data; Speeded up robust features	NEIGHBOR; ONLINE	Handwritten character recognition is the most widely used branch of study in image pattern recognition. Tamil, the official language of Tamil Nadu in South India, Sri Lanka, Singapore and Malaysia, has a script which contains many loops and compound characters, with small differences between character classes. Most of the research on offline Tamil handwritten character recognition system was done only on few character classes as it is very difficult to distinguish between minute dissimilarities of large character classes. It is important to design a complete recognition system that can process all character classes of Tamil and distinguish natural variability between inter-class images. Unlike conventional machine learning approaches for pattern recognition problems, we have proposed a nearest interest point classifier, which can choose sufficient and necessary subset of features from a variable length high dimensional feature vector. Since this is a practical problem, in this work, a study on image to image matching is included through feature analysis without using machine learning approaches. The proposed algorithm gave a good recognition accuracy for all the character classes on the standard database available for Tamil, HP Labs offline Tamil handwritten character database. Our proposed classifier produced a recognition accuracy of 90.2% while including the whole dataset. The method has been compared with the standard classifiers and has been proved to be a state-of-the-art performance in recognition of accuracy over the previous results given in the literature.																	1433-7541	1433-755X				FEB	2020	23	1					199	212		10.1007/s10044-018-00776-x													
J								Hybrid minutiae and edge corners feature points for increased fingerprint recognition performance	PATTERN ANALYSIS AND APPLICATIONS										Minutiae; Corners; Ridges; Edges; Fingerprints; Fingerprint recognition; Fingerprint verification	ENHANCEMENT; ALGORITHM	In general, most fingerprint recognition systems are based on the minutiae feature points. When matching two fingerprint images, the goal in most recognition systems is to find the optimal transformation model that aligns their feature points in order to find among them the number of matched or aligned points and then generate a matching score. A major problem in feature extraction stage is that when the fingerprint image is of a poor quality due to skin conditions and sensor noise, that leads to many broken ridges in the image caused by cutline. In this case, the extraction of minutiae leads to a lot of spurious points and the performance of the system will degrade. Usually, image enhancement techniques are applied as preprocessing step to overcome this problem. In this work, we propose to use corner points on fingerprint ridges as new features in addition to the ridges minutiae in order to improve the recognition performance. Every ridge is decomposed into several straight edges (SEs). A straight edge is defined as a straight link of ridge points. On a ridge, the head of the first straight edge and the tail of the last one are two minutia and the intersections of the SEs are the ridge corners. Thus, we propose to use a ridge as primitive rather than individual points for matching. This primitive is a structure consisting of groups of both feature points which are minutiae and corners belonging to the same ridge. Based on this primitive, an intelligent matching technique is introduced using sets of feature points on the same primitive. As a result, the recognition performance is increased since it is based on ridge primitive matching rather than individual minutiae matching. Finally, our experimental results compared with those obtained by other well-known techniques in the literature demonstrate the effectiveness and efficiency of our proposed algorithm.																	1433-7541	1433-755X				FEB	2020	23	1					213	224		10.1007/s10044-018-00766-z													
