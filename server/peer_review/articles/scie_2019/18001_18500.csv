PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	RP	EM	RI	OI	FU	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	D2	EA	PG	WC	SC	GA	UT	PM	OA	HC	HP	DA
J								Mobile phone selection based on a novel quality function deployment approach	SOFT COMPUTING										Quality function deployment; New product development; TOPSIS; Interval type-2 fuzzy number; Mobile phone selection	HESITANT FUZZY; PRODUCT DEVELOPMENT; QFD; MODEL; METHODOLOGY; INTEGRATION	Fuzzy quality function deployment (QFD) approach has been extensively implemented to transform customer requirements (CRs) into products or services because fuzzy numbers provide to obtain more accurately the judgments of experts in vagueness environment. This study proposes to use interval type-2 fuzzy (IT2F) numbers in the improving of fuzzy QFD method. The developed IT2F number-based QFD approach utilizes IT2F sets to define the correlations among CRs; the relations between CRs and design requirements (DRs); the correlations among DRs; the weights of DRs. There is no paper about integrating QFD approach and IT2F set in the literature. IT2F numbers include more accurately the judgments of the experts to express the vagueness of the applications. In addition, TOPSIS (technique for order performance by similarity to ideal solution) approach based on interval type-2 trapezoidal fuzzy (IT2TrF) is utilized to select the best mobile phone. Finally, mobile phone selection implementation is handled to indicate the efficiency of the proposed method.																	1432-7643	1433-7479				OCT	2020	24	20					15447	15461		10.1007/s00500-020-04876-x		MAR 2020											
J								Electrical Resistivity Inversion Based on a Hybrid CCSFLA-MSVR Method	NEURAL PROCESSING LETTERS										Electrical resistivity imaging; Multi-output support vector regression; Shuffled frog leaping algorithm; Inversion	FROG-LEAPING ALGORITHM; ARTIFICIAL NEURAL-NETWORKS; SOUNDING DATA; OPTIMIZATION; PREDICTION; REDUCTION; WAVELET	2D electrical resistivity inversion is a complicated nonlinear optimization problem, which is high-dimensional and non-convex. Using traditional neural networks to solve resistivity inversion problem is cost effective but suffers from trapping in local minima. In order to solve the above problem, a multi-output support vector regression (MSVR) nonlinear inversion method with limited ERI learning samples is researched in this paper, which considers the combined fitting errors of all outputs. Moreover, a Cauchy random and chaotic oscillation shuffled frog leaping algorithm is applied to optimize the RBF kernel widths and penalty coefficients of MSVR for improving the inversion accuracy and the computational efficiency. The key issues of data sets generation, data preprocessing and inversion flowchart are analyzed. The experimental results based on the synthetic and field examples demonstrated that the proposed algorithm is accurate, efficient and can be applied in practical engineering applications.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2871	2890		10.1007/s11063-020-10229-4		MAR 2020											
J								Coherent point drift peak alignment algorithms using distance and similarity measures for two-dimensional gas chromatography mass spectrometry data	JOURNAL OF CHEMOMETRICS										GC x GC - MS; MS similarity; peak alignment; point matching algorithm	COMPOUND IDENTIFICATION; CHEMOMETRIC ANALYSIS	The peak alignment is a vital preprocessing step before downstream analysis, such as biomarker discovery and pathway analysis, for two-dimensional gas chromatography mass spectrometry (2DGCMS)-based metabolomics data. Due to uncontrollable experimental conditions, for example, the differences in temperature or pressure, matrix effects on samples, and stationary phase degradation, a shift of retention times among samples inevitably occurs during 2DGCMS experiments, making it difficult to align peaks. Various peak alignment algorithms have been developed to correct retention time shifts for homogeneous, heterogeneous, or both types of mass spectrometry data. However, almost all existing algorithms have been focused on a local alignment and are suffering from low accuracy especially when aligning dense biological data with many peaks. We have developed four global peak alignment (GPA) algorithms using coherent point drift (CPD) point matching algorithms: retention time-based CPD-GPA (RT), prior CPD-GPA (P), mixture CPD-GPA (M), and prior mixture CPD-GPA (P + M). Method RT performs the peak alignment based only on the retention time distance, while methods P, M, and P + M carry out the peak alignment using both the retention time distance and mass spectral similarity. Method P incorporates the mass spectral similarity through prior information, and Methods M and P + M use the mixture distance measure. Four developed algorithms are applied to homogeneous and heterogeneous spiked-in data as well as two real biological data and compared with three existing algorithms, mSPA, SWPA, and BiPACE-2D. The results show that our CPD-GPA algorithms perform better than all existing algorithms in terms of F1 score.																	0886-9383	1099-128X				AUG	2020	34	8							e3236	10.1002/cem.3236		MAR 2020											
J								Decouple co-adaptation: Classifier randomization for person re-identification	NEUROCOMPUTING										Person re-identification; Convolutional neural networks; Image retrieval; Representation learning	NETWORK	The Person Re-identification (ReID) task aims to match persons across cameras in a surveillance system. In the past few years, many researches are devoted to ReID and its performance has gained significant improvement. ReID models are usually trained as a joint framework comprising a person feature extractor and a classifier. However, there exists co-adaptation between the feature extractor and the classifier, which prevents the feature extractor from making effective and sufficient optimization and results in inferior retrieval performance. In this paper, we propose a very simple and effective training method, called DeAda, to decouple this co-adaptation. Our main motivation is to construct a series of weak classifiers during training by randomization of parameters, so that optimization on the feature extractor could be strengthened in the training stage. DeAda is easy, effective, and efficient, and could serve as a plug-and-play optimization tool for ReID models, without additional memory and time cost. We also analyze the theoretical property of DeAda and show that it could produce identical features for the same person under some simple assumptions. We demonstrate its effectiveness on three public ReID datasets: Market1501, DukeMTMC-reID and CUHK03 over different ReID models. With DeAda optimization, we finally obtain state-of-the-art results on all the three datasets. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 28	2020	383						1	9		10.1016/j.neucom.2019.11.093													
J								SA-Net: A deep spectral analysis network for image clustering	NEUROCOMPUTING										Image clustering; Spectral analysis network; Deep representation learning	CUTS	Although supervised deep representation learning has attracted enormous attentions across areas of pattern recognition and computer vision, little progress has been made towards unsupervised deep representation learning for image clustering. In this paper, we propose a deep spectral analysis network for unsupervised representation learning and image clustering. While spectral analysis is established with solid theoretical foundations and has been widely applied to unsupervised data mining, its essential weakness lies in the fact that it is difficult to construct a proper affinity matrix and determine the involving Laplacian matrix for a given dataset. In this paper, we propose a SA-Net to overcome these weaknesses and achieve improved image clustering by extending the spectral analysis procedure into a deep learning framework with multiple layers. The SA-Net has the capability to learn deep representations and reveal deep correlations among data samples. Compared with the existing spectral analysis, the SA-Net achieves two advantages: (i) Given the fact that one spectral analysis procedure can only deal with one subset of the given dataset, our proposed SA-Net elegantly integrates multiple parallel and consecutive spectral analysis procedures together to enable interactive learning across different units towards a coordinated clustering model; (ii) Our SA-Net can identify the local similarities among different images at patch level and hence achieves a higher level of robustness against occlusions. Extensive experiments on a number of popular datasets support that our proposed SA-Net outperforms 11 benchmarks across a number of image clustering applications. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 28	2020	383						10	23		10.1016/j.neucom.2019.11.078													
J								Perceptual image hashing based on frequency dominant neighborhood structure applied to Tor domains recognition	NEUROCOMPUTING										Perceptual hashing; Deep web; Tor; DCT; F-DNS; Image classification	RING PARTITION; ROBUST; SECURE; ANGLE	Tor (The Onion Router) is one of the most famous anonymous networks in the Deep Web. It provides a wide range of legal and illegal hidden services to the user. Recognizing such illicit domains is a challenging task for Cyber Security and Law Enforcement Agencies. However, doing it manually, based only on the officers' experience, is slow and prone to errors. Therefore, in this paper, we propose an automatic method based on perceptual hashing to recognize services on the Tor network only by means of their snapshots. Firstly, we introduce and make publicly available DUSI-2K (Darknet Usage Service Images-2K), an image dataset which contains snapshots from active Tor service domains. We also present a new, efficient, robust and discriminative image hashing method, named F-DNS, built by incorporating the Dominant Neighborhood Structure (DNS) map and the Global Neighborhood Structure (GNS) texture energy map extracted from the discrete cosine transform of the image. In order to evaluate the efficiency of our hashing method, we carry out intra- and inter- tests using images from some state-of-the-art datasets subject to various content-preserving operations. The high correlation coefficient values that our method obtains, indicates that F-DNS performs better than other state-of-the-art methods, especially in the case of rotation. Additionally, we assess F-DNS for recognizing the category of Tor domains based on their snapshots using the DUSI-2K dataset. We compare its performance with three typical image classification methods, i.e. Bag of Visual Words (BoVW) and features obtained from ResNet50 and Inception-ResNet-v2. F-DNS outperforms all of them, with an accuracy of 98.75%, against 31.39%, 82.70% and 85.19%, respectively. Fine-tuning ResNet50 and Inception-Resnet-v2 for DUSI-2K does not improve the result, attaining 37.12% and 79.15%, respectively. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 28	2020	383						24	38		10.1016/j.neucom.2019.11.065													
J								Multi-stream attentive generative adversarial network for dynamic scene deblurring	NEUROCOMPUTING										Attentive guidance module; Multi-stream; Attention map; Multi-scale fusion; Gate structure; Dynamic scene deblurring		Image deblurring methods based on deep learning have been proved to be effective for dynamic scene deblurring. Nevertheless, the most existing methods usually fail to deal with severe and complex dynamic scene blurs. In this paper, we propose a novel multi-stream attentive generative adversarial network (MSA-GAN) for solving the problems. Firstly, we design a novel attentive guidance module for generating an attention map which can guide to pay more attention on the severe blur regions or objects and their edge or texture structures in the feature extraction and deblurring process. Secondly, we propose the multi-stream and multi-scale feature extraction strategy and design an attentive and multi-scale feature extraction module, in which we design a multi-scale residual block for extracting the multi-scale features and capturing more information of complex blurs. Thirdly, we propose a multi-scale feature fusion strategy and the strategy is implemented in the multi-stream and multi-scale feature fusion module through the designed gate structure, which can adaptively learn and fuse multi-scale features from different streams. Finally, The extensive experiments are performed on the GoPro dynamic scene dataset and real image data, and the experimental results both quantitatively and qualitatively have demonstrated that the proposed method outperform the recent state-of-the-art methods. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 28	2020	383						39	56		10.1016/j.neucom.2019.11.063													
J								Deep attention user-based collaborative filtering for recommendation	NEUROCOMPUTING										User-based collaborative filtering; Pairwise interaction; Attention network; Deep learning; Recommender systems		The user-based collaborative filtering (UCF) model has been widely used in industry for recommender systems. UCF predicts a user's interest in an item based on rating information from similar user profiles. A neural network UCF model can learn effectively the high-order relations between users and items, but it cannot distinguish the importance of users in learning process. To mine the complex relationships between users and items, we incorporate a Deep+Shadow pattern to improve learning features effectively, namely as DeepUCF. Firstly, we define historical users, that is, users from the historical data of interactions with an item. The target user and historical users are calculated to capture complex process of user's interacted item. Secondly, we integrate a shallow linear model to effectively solve single pair interaction problems. Finally, DeepUCF construct of a pair of user relations (interactive users with a history of items) for the input, and joint linear and nonlinear models to build relationships between users. More importantly, DeepUCF+a add an attention network to distinguish the historical user importance of items, which make DeepUCF more expressive. Experiments on real datasets show that DeepUCF and DeepUCF+a can effectively capture users' complex high-order relationships, and achieve better performance. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 28	2020	383						57	68		10.1016/j.neucom.2019.09.050													
J								A dynamical-structure neural network model specified for representing logical relations with inhibitory links and fewer neurons	NEUROCOMPUTING										Brain-inspired computing; Logical representation; Dynamical neural network structure; Inhibitory link; Adaptivity; Rule library	P SYSTEMS	To make ANNs have the ability of logical processing in order to fulfil the urgent requirement that computers can automatically judge according to numerous specific conditions, researches have been carried out to design novel neural network models for representing logical relations. Recently, a new ANN model for representing logical relations is proposed. In the model, six components are designed to simulate the operations of logic gates. The work provides a novel way for constructing logical relations running in a neural-like manner. However, the components are still complex and indirect for the representation since more extra neurons and links are needed to simulate logic gates. In order to represent logical relations more directly, this paper defines new neurons and multiple kinds of links to represent logic gates directly, and they can be combined to represent complex logical relations in a simpler neural network structures with fewer neurons. Additionally, this ANN model can dynamically create links on demand instead of the fixed full connections. It can constantly adjust its network structure when getting the data continuously. It can be used for the establishment of the rule library of the intelligent information system in the form of the neural network structure. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 28	2020	383						69	80		10.1016/j.neucom.2019.11.037													
J								Employing the Friedrichs' inequality to ensure global exponential stability of delayed reaction-diffusion neural networks with nonlinear boundary conditions	NEUROCOMPUTING										Diffusion; Friedrichs' Inequality; Eigenvalue; Nonlinear boundary condition; Exponential stability	TIME-VARYING DELAYS; ROBUST STABILITY; MIXED DELAYS; SYNCHRONIZATION	By employing the Friedrichs' inequality and M-matrices, we have obtained two sets of sufficient conditions to ensure global exponential stability of the reaction-diffusion Hopfield networks with S-type distributed delays and nonlinear boundary conditions. Our work demonstrates that both diffusion effects, boundary conditions and shapes of spatial regions have played critical roles in determining the global exponential stability of the network system. Comparing with the previous work, our method can further provide how to find the more accurate and larger convergence rate. We also present several examples and simulations of the network models defined respectively on the cylinder, unit ball, cube and line segment so as to show the significance and application of our theory. The theoretical result and the idea here can be applied in considering system control and synchronization with or without random terms. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 28	2020	383						81	94		10.1016/j.neucom.2019.11.091													
J								MLTL: A multi-label approach for the Tomek Link undersampling algorithm	NEUROCOMPUTING										Multi-label learning; Resampling techniques; Dataset imbalanceness	CLASSIFICATION; ENSEMBLE	A large variety of problems are multi-labeled, which made the Multi-Label Classification field become an active topic in the machine learning community. However, real world problems tend to be imbalanced, meaning that some classes may have more samples than others. Learning from imbalanced datasets is a challenging task and for that has attracted the attention of researchers that have proposed some resampling algorithms to address this problem. This work presents two main contributions: A new resampling algorithm for multi-label classification problems named MLTL - Multi-Label Tomek Link, which is based on the standard Tomek Link resampling algorithm; A multi-label imbalanceness API for the Mulan framework. Results in seven well-known datasets showed that MLTL is a competitive technique when compared to other multi-label resampling methods from the literature. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 28	2020	383						95	105		10.1016/j.neucom.2019.11.076													
J								Designing non-linear minimax and related discriminants by disjoint tangent configurations applied to RBF networks	NEUROCOMPUTING										Binary classification; Minimax; Non-linear discriminants; Single-hidden layer feedforward networks; Radial basis functions		Non-linear classification machines seldom are trained under criteria that are usual and useful for linear discriminants, such as minimax, Fisher's, and other similar criteria. The reason is the learning difficulties that transformation-trainable machines suffer when applying such criteria. However, the possibility of using non-linear machines whose transformations are pre-designed merits attention. In this contribution, we propose and study an efficient and potentially effective option: Applying Disjoint Tangent Configurations (DTC), a formulation that includes discriminants such as Fisher's, Bayes for normal distributions, Minimax Probabilistic Decision Hyperplane (MPDH), and others, to the output of a Radial Basis Function (RBF) network which has been previously designed with a moderate number of nodes to reduce the computational load, but with a high quality centroid selection algorithm, Frequency Sensitive Competitive Learning (FSCL), which allows to obtain networks with high representation capabilities. Experiments demonstrate that this approach leads to good performance results with acceptable computational efforts. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 28	2020	383						106	112		10.1016/j.neucom.2019.12.016													
J								Keywords extraction with deep neural network model	NEUROCOMPUTING										Keyword extraction; Deep learning; LSTM; Attention mechanism; Two-stage training		Keywords can express the main content of an article or a sentence. Keywords extraction is a critical issue in many Natural Language Processing (NLP) applications and can improve the performance of many NLP systems. The traditional methods of keywords extraction are based on machine learning or graph model. The performance of these methods is influenced by the feature selection and the manually defined rules. In recent years, with the emergence of deep learning technology, learning features automatically with the deep learning algorithm can improve the performance of many tasks. In this paper, we propose a deep neural network model for the task of keywords extraction. We make two extensions on the basis of traditional LSTM model. First, to better utilize both the historic and following contextual information of the given target word, we propose a target center-based LSTM model (TC-LSTM), which learns to encode the target word by considering its contextual information. Second, on the basis of TC-LSTM model, we apply the self-attention mechanism, which enables our model has an ability to focus on informative parts of the associated text. In addition, we also introduce a two-stage training method, which takes advantage of large-scale pseudo training data. Experimental results show the advantage of our method, our model beats all the baseline systems all across the board. And also, the two-stage training method is of great significance for improving the effectiveness of the model. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 28	2020	383						113	121		10.1016/j.neucom.2019.11.083													
J								Multistage attention network for multivariate time series prediction	NEUROCOMPUTING										Attention mechanism; Multivariate time series prediction; Long short-term memory		The deep learning model has been used to predict the variation rule of the target series of multivariate time series data. Based on the attention mechanism, the influence information of multiple non-predictive time series on target series in different time stages is processed as the same weight in the previous studies. However, on real-world datasets, multiple non-predictive time series has different influence (such as different mutation information) on target series in different time stages. Therefore, a new multistage attention network is designed to capture the different influence. The model is mainly composed of the influential attention mechanism and temporal attention mechanism. In the influential attention mechanism, the same and different time stage attention mechanisms are used to capture the influence information of different non-predictive time series on the target series over time. In the temporal attention mechanism, the variation law of data can be captured over time. Besides, the prediction performance of proposed model on two different real-world multivariate time series datasets is comprehensively evaluated. The results show that, the prediction performance of the proposed model beat all baseline models and SOTA models. In a word, the multistage attention network model can effectively learn the information of the influence of different non-predictive time series on the target series in different time stages in the historical data. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 28	2020	383						122	137		10.1016/j.neucom.2019.11.060													
J								Synchronization of coupled memristive neural networks with actuator saturation and switching topology	NEUROCOMPUTING										Coupled memristive neural networks; Reliable control; Non-fragile control; Switching topology	EVENT-TRIGGERED CONTROL; SYSTEMS; TIME	This study inspects the synchronization problem for a class of coupled memristive neural networks against parameter uncertainties, actuator saturation and switching topology, where the saturation effect is incorporated in the control design. The main objective of this study is to originate a non-fragile reliable controller with saturation effect such that the state trajectories of the proposed coupled memristive neural networks are initiated to synchronize asymptotically to the leader node under direct communication topology. In particular, a novel set of sufficient conditions is derived in conjunction with graph theory and the Lyapunov's direct method for obtaining the asymptotic synchronization of the addressed coupled memristive neural networks. After that, the explicit form of the controller gains are obtained by using linear matrix inequalities. Eventually, two numerical examples are presented to verify the feasibility and effectiveness of the proposed synchronization criteria. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 28	2020	383						138	150		10.1016/j.neucom.2019.11.034													
J								Regularized matrix completion with partial side information	NEUROCOMPUTING										Matrix completion; Side information; Prior subspace information; Regularization	LOW-RANK MATRIX	Side information has been shown useful for improving the performance of matrix completion applications. However, in most cases, only partial side information of either the column or row space is available. In this work, we propose a novel regularization based model to incorporate partial side information in matrix completion. We provide theoretical guarantees to ensure the success of the proposed model. It is proved that the proposed model achieves the state-of-the-art sample complexity when the given partial side information is exact, and an error bound for inexact partial side information is also provided. Moreover, we provide a deterministic rule for the selection of regularization parameter. We conduct extensive experiments on both synthetic and real-world data-sets. Experimental results show that our model succeeds to incorporate partial side information, and outperforms the state-of-the-art models on most data-sets. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 28	2020	383						151	164		10.1016/j.neucom.2019.12.021													
J								BiLSTM with Multi-Polarity Orthogonal Attention for Implicit Sentiment Analysis	NEUROCOMPUTING										Implicit Sentiment Analysis; Multi-polarity Attention; Orthogonal Attention		Sentiment analysis has been a popular field in natural language processing. Sentiments can be expressed explicitly or implicitly. Most current studies on sentiment analysis focus on the identification of explicit sentiments. However, implicit sentiment analysis has become one of the most difficult tasks in sentiment analysis due to the absence of explicit sentiment words. In this article, we propose a BiLSTM model with multi-polarity orthogonal attention for implicit sentiment analysis. Compared to the traditional single attention model, the difference between the words and the sentiment orientation can be identified by using multi-polarity attention. This difference can be regarded as a significant feature for implicit sentiment analysis. Moreover, an orthogonal restriction mechanism is adopted to ensure that the discriminatory performance can be maintained during optimization. The experimental results on the SMP2019 implicit sentiment analysis dataset and two explicit sentiment analysis datasets demonstrate that our model more accurately captures the characteristic differences among sentiment polarities. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 28	2020	383						165	173		10.1016/j.neucom.2019.11.054													
J								Scale-aware spatial pyramid pooling with both encoder-mask and scale-attention for semantic segmentation	NEUROCOMPUTING										Scene understanding; Semantic segmentation; Encoder-decoder; Scale selection; Attention	FEATURES	This paper focuses on semantic segmentation of scenes by capturing the appropriate scale, rich detail and contextual dependencies in a feature representation. Semantic segmentation is a pixel-level classification task and has made steady progress on the basis of fully convolutional networks (FCNs). However, we find there is still room for improvement in the following aspects. The first is that the pixel itself has not enough information for semantic prediction, it needs to look around to determine which category it belongs to. However, the fixed size of the receptive field defined by the network is not suitable for all pixels when an image contains objects with various scales. The second is that the extracted scale-aware features do not handle sharper object boundaries due to low-resolution. The final aspect is regarding the ability to model long-range dependencies. In order to solve the above challenges, in this paper we propose three modules: Scale-aware spatial pyramid pool module, Encoder mask module and Scale-attention module (SSPP-ES). Extensive experiments on the Cityscapes and ADE20K benchmarks demonstrate the effectiveness of our approach for semantic segmentation. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 28	2020	383						174	182		10.1016/j.neucom.2019.11.042													
J								Consensusability of discrete-time linear multi-agent systems with multiple inputs	NEUROCOMPUTING										Consensusability; Multi-agent systems (MASs)	TRACKING CONTROL; NONLINEAR-SYSTEMS; SYNCHRONIZATION; NETWORKS; GRAPHS	In this paper, consensusability of the discrete-time linear multi-agent systems (MASs) with multiple inputs are considered. Both the leaderless and leader-following consensus are taken into account, and the associated communication graphs can be undirected or directed. Using an appropriately designed weighted dynamic distributed protocol, it is proved that consensusability of the MASs completely hinges on consensusability of the designed distributed dynamic compensators. New consensusability conditions are derived which are inequalities associated with the maximum absolute unstable eigenvalue of the agent's dynamics (rather than the product of all absolute unstable eigenvalues in literatures) and the eigen-ratio of the communication graph. Simulation examples are given to demonstrate the validity of the theoretical results. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 28	2020	383						183	193		10.1016/j.neucom.2019.11.040													
J								Exploiting background divergence and foreground compactness for salient object detection	NEUROCOMPUTING										Background divergence; Deep features; Foreground compactness; Manifold ranking; Salient object detection	REGION DETECTION; CONTRAST; MODEL; INFORMATION; ALGORITHM; RANKING	In this paper, we propose an efficient and discriminative saliency method that takes advantage of background divergence and foreground compactness. Concretely, a graph is first constructed by introducing the concept of virtual node to effectively enhance the distinction between nodes along object boundaries and the similarity among object regions. A reasonable edge weight is defined by incorporating low-level features as well as deep features extracted from deep networks to measure the relationship between different regions. To remove incorrect outputs, two computational mechanisms are then developed to extract reliable background seeds and compact foreground regions, respectively. The saliency value of a node is calculated by fully considering the relationship between the corresponding node and the virtual background (foreground) node. As a result, two types of saliency maps are obtained and integrated into a uniform map. In order to achieve significant performance improvement consistently, we propose a robust saliency optimization mechanism, which subtly combine suppressed/active (SA) nodes and mid-level structure information based on manifold ranking. Extensive experimental results demonstrate that the proposed algorithm performs favorably against the state-of-art saliency detection methods in terms of different evaluation metrics on several benchmark datasets. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 28	2020	383						194	211		10.1016/j.neucom.2019.09.096													
J								Polar coordinate sampling-based segmentation of overlapping cervical cells using attention U-Net and random walk	NEUROCOMPUTING										Cervical cell segmentation; Attention U-Net; Polar coordinate sampling; Random walk; Overlapping	CYTOPLASM SEGMENTATION; CLASSIFICATION; CYTOLOGY; NUCLEUS	Segmentation of nuclei and cytoplasm inside the cellular clumps in cervical smear images is a difficult task because of the poor contrast and unpredictable shape of cytoplasm. This article addresses a new framework based on Attention U-Net (ATT U-Net) network and graph-based Random Walk (RW) to extract both nucleus and cytoplasm of each individual cell within an image of overlapping cervical cells. The proposed approach starts by separating nuclei from the cellular clumps through ATT U-Net architecture. Then, we remove fake nuclei that are usually much smaller than real nuclei. For each nucleus, a polar coordinate sampling matrix is generated. Each element in this matrix is realized by converting the image pixel from Cartesian coordinates to polar coordinates. After that, converted images would serve as the input of ATT U-Net for predicting cytoplasm. And finally, Graph-based RW is applied to extract the contour of cytoplasm. Because the features of cytoplasm boundaries in predicted maps are so obvious that the segmentation of every individual cell, including overlapping area, worked well under RW. We evaluate our framework on ISBI 2014 Challenge Dataset. The results reveal that our approach improves the performance on extracting individual cell from heavy overlapping cell clumps. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 28	2020	383						212	223		10.1016/j.neucom.2019.12.036													
J								Meta-Learning based prototype-relation network for few-shot classification	NEUROCOMPUTING										Few-shot learning; Classification; Meta-learning; Prototype		Pattern recognition has made great progress under large amount of labeled data, while performs poorly on a very few examples obtained, named few-shot classification, where a classifier can identify new classes not encountered during training. In this paper, a simple framework named Prototype-Relation Network is presented for the few-shot classification. Moreover, a novel loss function compared with prototype networks is proposed which takes both inter-class and intra-class distance into account. During meta-learning, the model is optimized by end-to-end episodes, each of which is to imitate the test few-shot setting. The trained model is used to classify new classes by computing min distance between query images and the prototype of each class. Extensive experimental results demonstrate that our proposed meta-learning model is competitive and effective, which achieves the state-of-the-art performance on Omniglot and miniImageNet datasets. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 28	2020	383						224	234		10.1016/j.neucom.2019.12.034													
J								Intelligent cross-machine fault diagnosis approach with deep auto-encoder and domain adaptation	NEUROCOMPUTING										Deep learning; Fault diagnosis; Model generalization; Auto-encoder; Rolling bearing	NEURAL-NETWORK	Recently, due to the rising industrial demands for intelligent machinery fault diagnosis with strong generalization, transfer learning techniques have been used to enhance adaptability of data-driven approaches. Particularly, the domain shift problem where training and testing data are sampled from different operating conditions of the same machine is well addressed. However, it is still difficult to prepare sufficient labeled data on the tested machine. Therefore, the idea of transferring fault diagnosis knowledge learned from one machine to different but related machines is motivated, and that is realized through a deep learning-based method in this paper. Features of different equipments are first projected into the same subspace using an auto-encoder structure, and cross-machine adaptation algorithm is adopted for knowledge generalization, where the distribution discrepancy between data from different machines is minimized. Experiments on three rolling bearing datasets are implemented to validate the proposed method. The results suggest it is feasible to transfer fault diagnosis knowledge across different machines, and the proposed method offers a novel and promising approach for knowledge generalization. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 28	2020	383						235	247		10.1016/j.neucom.2019.12.033													
J								Self-paced and auto-weighted multi-view clustering	NEUROCOMPUTING										Self-paced learning; Multi-view clustering; Soft weighting	K-MEANS	Multi-view clustering (MVC) methods are effective approaches to enhance clustering performance by exploiting complementary information from multiple views. One main disadvantage of most existing MVC methods is that the corresponding optimization problems are non-convex and thus local optimal solutions are usually obtained. To alleviate this issue, we propose a novel multi-view clustering method equipped with self-paced learning, which first learns the MVC model with easy examples and then progressively considers complex ones from each view. In addition, a soft weighting scheme of self-paced learning is designed to further reduce the negative impact from outliers and noises. Furthermore, to consider the importance of different views, we develop an auto-weighted technique to automatically assign weights to views. The proposed model only needs the number of clusters as input and can be easily solved by an alternating optimization paradigm. Experimental results on various benchmark data sets demonstrate the effectiveness of the proposed model. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 28	2020	383						248	256		10.1016/j.neucom.2019.11.104													
J								Identification of membrane protein types via multivariate information fusion with Hilbert-Schmidt Independence Criterion	NEUROCOMPUTING										Membrane protein; Position specific scoring matrix; Multiple kernel learning; Multivariate information fusion; Feature extraction	SUPPORT VECTOR MACHINES; AMINO-ACID-COMPOSITION; PHYSICOCHEMICAL PROPERTIES; SCORING MATRIX; GENERAL-FORM; WEB SERVER; PREDICTION; ENSEMBLE; EVOLUTIONARY; ATTRIBUTES	Membrane proteins perform a variety of functions vital to the survival of organisms, such as oxidoreductase, transferase or hydrolase. If the type of membrane protein can be detected, the function of protein can be quickly determined. Many existing computational methods not only use the autocorrelation function on the hydrophobicity index of amino acids, but also consider the evolutionary conservatism information of the primary protein sequences. In this study, we employ Average Blocks (AN/Block), Discrete Wavelet Transform (DWT), Discrete Cosine Transform (DCT), Histogram of Oriented Gradient (HOG) and Pseudo-PSSM (PsePSSM) to extract evolution characteristics from Position-Specific Score Matrix (PSSM). Then, we construct five kernels from above five corresponding feature sets. Finally, we propose a novel Multiple Kernel Support Vector Machine (MKSVM) classifier based on Hilbert Schmidt Independence Criterion (HSIC) to integrate five kernels for identifying membrane proteins. For the performance evaluation, our method is tested on four benchmark datasets of membrane proteins. The comparative results demonstrate that our prediction model achieves the best performance among all existing outstanding approaches. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 28	2020	383						257	269		10.1016/j.neucom.2019.11.103													
J								A weakly supervised framework for abnormal behavior detection and localization in crowded scenes	NEUROCOMPUTING										Faster regional convolutional neural network; Abnormal behavior detection and localization; Multiple instance learning	VIDEO ANOMALY DETECTION; EVENT DETECTION; NEURAL-NETWORKS	In this paper, a weakly supervised framework is proposed for Abnormal Behavior Detection and Localization (ABDL) in the scenes. First, the objects in the scene such as pedestrians, vehicles, etc. are detected using the Faster Regional Conventional Neural Network (Faster R-CNN); then, the object behavior is described by a Histogram of Large Scale Optical Flow (HLSOF) descriptor; finally, the Multiple Instance Support Vector Machine (MISVM) is trained and then used to identify the testing behaviors as normal or abnormal. Summarily, the proposed approach has three main advantages: (1) Benefit from the Faster R-CNN, our approach can analyze the behavior at object-wise, which makes our approach has good generality and high computational efficiency; (2) The HLSOF descriptor can characterize the object behavior efficiently, and is insensitive to the variations of the size of objects; (3) As a weakly supervised learning framework, the MISVM only requires the labels at the bag level rather than instance level, which makes our approach has high accuracy as the supervised approaches but not requires completely labeled training samples, only the frame-level label is required. Experimental results analysis on different datasets validates the effectiveness of our approach. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 28	2020	383						270	281		10.1016/j.neucom.2019.11.087													
J								Generation of topic evolution graphs from short text streams	NEUROCOMPUTING										Topic evolution graph; Topic model; Short text mining; Word embedding	MODEL	Topic evolution mining on short texts is an important research topic in natural language processing. Existing methods have been focused either on the topic evolution of normal documents or on the evolution of topics along a timeline. In this paper, we aim to generate topic evolutionary graphs from short texts, which not only capture the main topic timeline, but also reveal the correlations between related subtopics. Firstly, we propose an Encoder-only Transformer Language Model (ETLM) to quantify the relationship between words. Then we propose a novel topic model, referred as weighted Conditional random field regularized Correlated Topic Model (CCTM), which leverages semantic correlations to discover meaningful topics and topic correlations. Finally, topic evolutionary graphs are generated by an Online version of CCTM (OCCTM) to capture the evolutionary patterns of main topics and related subtopics. Experimental results on real-world datasets demonstrate our method outperforms baselines on quality of topics and presents motivated patterns for topic evolution mining. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 28	2020	383						282	294		10.1016/j.neucom.2019.11.077													
J								Flow-guided feature enhancement network for video-based person re-identification	NEUROCOMPUTING										Video person re-identification; Optical flow; Feature enhancement	ATTENTION	Video-based person re-identification associates sequences of the same person among surveillance camera network. Most existing works explore motion and inter-frame information on the features corrupted by spatial noises such as occlusion, blur, posture changes, etc, leading to degraded representation and matching performance. Enhancing features of each frame guarantees a more robust and discriminative final feature representation. In this paper, we propose a novel flow-guided feature enhancement network that leverages flow information to enhance low-level features. Specifically, it improves per-frame features by aggregating with the warped feature under the guidance of optical flow and the enhanced feature of previous frame in spatial attention mechanism. Then, a part-based loss is directly employed on the enhanced features to supervise the aggregation process, which can exert full capability of the network. Experiments on three widely used benchmark datasets: iLIDS-VID, PRID-2011 and MARS, demonstrate that the proposed model achieves superior performance and outperforms most of the recent state-of-the-art methods. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 28	2020	383						295	302		10.1016/j.neucom.2019.11.050													
J								Data, depth, and design: Learning reliable models for skin lesion analysis	NEUROCOMPUTING										Skin lesion analysis; Deep learning; Experimental design; Model parameterization; Cross dataset	CLASSIFICATION	Deep learning fostered a leap ahead in automated skin lesion analysis in the last two years. Those models, however, are expensive to train and difficult to parameterize. Objective: We investigate methodological issues for designing and evaluating deep learning models for skin lesion analysis. We explore ten choices faced by researchers: use of transfer learning, model architecture, train dataset, image resolution, type of data augmentation, input normalization, use of segmentation, duration of training, additional use of Support Vector Machines, and test data augmentation. Methods: We perform two full factorial experiments, for five different test datasets, resulting in 2560 exhaustive trials in our main experiment, and 1280 trials in our assessment of transfer learning. We analyze both with multi-way analyses of variance (ANOVA). We use the exhaustive trials to simulate sequential decisions and ensembles, with and without the use of privileged information from the test set. Results main experiment: Amount of train data has disproportionate influence, explaining almost half the variation in performance. Of the other factors, test data augmentation and input resolution are the most influential. Deeper models, when combined, with extra data, also help. - transfer experiment: Transfer learning is critical, its absence brings huge performance penalties. - simulations: Ensembles of models are the best option to provide reliable results with limited resources, without using privileged information and sacrificing methodological rigor. Conclusions and Significance: Advancing research on automated skin lesion analysis requires curating larger public datasets. Indirect use of privileged information from the test set to design the models is a subtle, but frequent methodological mistake that leads to overoptimistic results. Ensembles of models are a cost-effective alternative to the expensive full-factorial and to the unstable sequential designs. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 28	2020	383						303	313		10.1016/j.neucom.2019.12.003													
J								Online singular value decomposition of time-varying matrix via zeroing neural dynamics	NEUROCOMPUTING										Singular value decomposition (SVD); Zeroing neural dynamics (ZND); Permutation matrix; Kronecker product; Dimensional reduction technique; Zhang et al discretization (ZeaD)	DISCRETE WAVELET TRANSFORM; NONLINEAR OPTIMIZATION; ROBOT MANIPULATORS; MODELS; SVD; PSEUDOINVERSION; NETWORK; PATTERN	In this paper, the problem of online singular value decomposition (SVD) for time-varying matrix is proposed, analyzed and investigated. In order to solve this complex and difficult problem in real time, we consider to transform it into an equation system firstly. Then, by applying zeroing neural dynamics (ZND) method and a dimensional reduction technique, a continuous-time SVD (CTSVD) model is proposed. Besides, a high-precision eight-instant Zhang et al discretization (ZeaD) formula with theoretical analysis is proposed and studied. Furthermore, by using this new ZeaD formula to discretize the CTSVD model, an eight-instant discrete-time SVD (EIDTSVD) model is thus proposed. Moreover, three other discrete-time SVD (DTSVD) models termed Euler-type DTSVD (ETDTSVD) model, four-instant DTSVD (FIDTSVD) model and six-instant DTSVD (SIDTSVD) model are derived and proposed, respectively, for the purpose of comparison. Finally, numerical experiments and results further substantiate the great effectiveness, accuracy and superiority of the proposed CTSVD and EIDTSVD models. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 28	2020	383						314	323		10.1016/j.neucom.2019.11.036													
J								Active one-shot learning by a deep Q-network strategy	NEUROCOMPUTING										One-shot learning; A deep Q-network strategy; Active-learning set-up		One-shot learning has recently attracted growing attention to produce models which can classify significant events from a few or even no labeled examples. In this paper, we introduce a deep Q-network strategy into one-shot learning (OL-DQN) to design a more intelligent learner to infer whether to label a sample automatically or request the true label for the active-learning set-up. Then we conducted experiments in the ALOI dataset for the classification of objects recorded under various imaging circumstances and a dataset for handwriting recognition composed of both characters and digits to have a performance evaluation and application analysis of the proposed model respectively, and the obtained results demonstrate that our model can achieve a better trade-off between prediction accuracy and the need of label requests compared with a purely supervised task, a prior work AOL, and a conventional active learning algorithm QBC. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 28	2020	383						324	335		10.1016/j.neucom.2019.11.017													
J								Improving aspect-based sentiment analysis via aligning aspect embedding	NEUROCOMPUTING										Aspect embedding; Sentiment analysis; Representation learning		Aspect-Based Sentiment Analysis (ABSA) is a fine-grained sentiment analysis task, which aims to predict sentiment polarities of given aspects or target terms in text. ABSA contains two subtasks: Aspect-Category Sentiment Analysis (ACSA) and Aspect-Term Sentiment Analysis (ATSA). Aspect embeddings have been extensively used for representing aspect-categories on ACSA task. Based on our observations, existing aspect embeddings cannot properly represent the relation between aspect-categories and aspect-terms. To address this limitation, this paper presents a learning method which trains aspect embeddings according to the relation between aspect-categories and aspect-terms. According to the cosine measure metric we proposed in this paper, the limitation is successfully alleviated in the aspect embeddings which are trained by our method. The trained aspect embeddings can be used as initialization in existing models to solve ACSA task. We conduct experiments on SemEval datasets for ACSA task, and the results indicate that our pre-trained aspect embeddings are capable of improving the performance of sentiment analysis. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 28	2020	383						336	347		10.1016/j.neucom.2019.12.035													
J								Detection of incipient faults in EMU braking system based on data domain description and variable control limit	NEUROCOMPUTING										Incipient fault; Electric multiple unit (EMU); Braking system; Local reachability density (LRD); Variable control limit	COMPONENT STATISTICAL-ANALYSIS; DIAGNOSIS	The performance of braking system strongly affects the safe operation of electric multiple unit (EMU). During the practical operation, it's of great significance to detect incipient faults in braking system. Since the braking process is a typical non-Gaussian and multi-stage process, it's difficult to detect these incipient faults. Particularly, the usually occurred overlap of normal and faulty samples during braking process makes the detection more difficult. In this paper, a novel method based on data domain description and variable control limit (VCL) is developed for detecting incipient faults in EMU braking system. The local reachability density (LRD) weighted support vector data description with negative samples (NSVDD) is introduced for offline modeling to get more accurate domain description, while the Gaussian kernel trick is utilized to obtain hypersphere with soft boundary. With the presence of sample overlap, the VCL strategy is adopted for online fault detection, which effectively reduces false alarm rate (FAR) and increases fault detection rate (FDR) simultaneously. A case study of three kinds of incipient faults in EMU braking system fully demonstrates the effectiveness of the proposed method. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 28	2020	383						348	358		10.1016/j.neucom.2019.12.029													
J								Guaranteed-performance consensus for descriptor nonlinear multi-agent systems based on distributed nonlinear consensus protocol	NEUROCOMPUTING										Descriptor nonlinear multi-agent systems; Distributed nonlinear control protocol; Guaranteed-performance cost; Leader-following consensus	ADMISSIBLE CONSENSUS; NETWORKED SYSTEMS; SYNCHRONIZATION	In this paper, guaranteed-performance consensus (GPC) for descriptor nonlinear multi-agent systems (DNMASs) with a leader is studied. The interactions among followers are bidirectional for leader-following DNMASs. Firstly, one designs a novel distributed nonlinear consensus protocol based on state feedback to reach consensus for DNMASs. The performance function is constructed by state errors among agents, which is time integration of quadratic function. Secondly, not only are sufficient conditions presented for guaranteed-performance consensus to ensure the scalability of DNMASs based on the Riccati inequality, but also an upper bound of the cost function is derived. It is shown that the guaranteed-performance costs are dependent on initial states of agents. Moreover, the conclusions are extended to achieve the leaderless GPC. Finally, simulation examples are presented to demonstrate the effectiveness of theoretical results. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				MAR 28	2020	383						359	367		10.1016/j.neucom.2019.12.028													
J								Local matrix approximation via automatic anchor selection and asymmetric neighbor inclusion based on feature divergence measure	NEUROCOMPUTING										Recommender system; Matrix factorization; Local matrix construction; Automatic anchor selection; Asymmetric neighbor inclusion	RECOMMENDATION	Due to the ever-increasing users and items, the global matrix factorization (MF) over a large yet sparse user-item rating matrix no longer suffices for accurate rating approximations. This paper studies the problem of how to obtain many local matrices for improving the global rating approximation via averaging local MF results. In our scheme, we propose to use a feature divergence measure to compute the asymmetric distances in between users and items. To obtain local matrix approximation, we first propose to use a density-based clustering algorithm to automatically select a number of user-item pairs as anchors for self-adapting different datasets. Next for each anchor, we recruit its neighboring users and items with small feature divergences to the anchor, thus forming each local matrix. A kernel-based approach is then applied for local matrix approximation and a weighted averaging is used to obtain the global matrix rating approximations. Our experiments on the three real-world datasets show that the proposed solution can outperform the state-of-the-art schemes in terms of lower approximation errors and higher coverage ratios. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 28	2020	383						368	379		10.1016/j.neucom.2019.12.025													
J								Robust sequential subspace clustering via l(1)-norm temporal graph	NEUROCOMPUTING										Sequential data; Sparse subspace clustering (SSC); Low rank representation (LRR); Proximal gradient; l(2,1) norm	LOW-RANK; FACE RECOGNITION; SEGMENTATION; CUTS	Subspace clustering (SC) has been widely applied to segment data drawn from multiple subspaces. However, for sequential data, a main challenge in subspace clustering is to exploit temporal information. In this paper, we propose a novel robust sequential subspace clustering approach with a l(1)-norm temporal graph. The l(1)-norm temporal graph is designed to encode the temporal information underlying in sequential data. By using the l(1) norm, it can enforce well temporal similarity of neighboring frames with a sample-dependent weight, and mitigate the effect of noises and outliers on subspace clustering because large errors mixed in the real data can be suppressed. Under assumption of data self-expression, our clustering model is put forward by further integrating the classical Sparse Subspace Clustering and the l(1)-norm Temporal Graph (SSC-L1TG). To solve the proposed model, we introduce a new efficient proximity algorithm. At each iteration, the sub-problem is solved by proximal minimization with closed-form solution. In contrast to the alternating direction method of multipliers (ADMM) employed in most existing clustering approaches without convergence guarantee, the proposed SSC-L1TG is guaranteed to converge to the desired optimal solution. Experimental results on both synthetic and real data demonstrate the efficacy of our method and its superior performance over the state-of-the-art methods. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 28	2020	383						380	395		10.1016/j.neucom.2019.12.019													
J								Deep feature selection using a teacher-student network	NEUROCOMPUTING										Feature-selection; Deep learning; Teacher-student	UNSUPERVISED FEATURE-SELECTION; RELEVANCE	High-dimensional data in many machine learning applications leads to computational and analytical complexities. Feature selection provides an effective way for solving these problems by removing irrelevant and redundant features, thus reducing model complexity and improving accuracy and generalization capability of the model. In this paper, we present a novel teacher-student feature selection (TSFS) method in which a 'teacher' (a deep neural network or a complicated dimension reduction method) is first employed to learn the best representation of data in low dimension. Then a 'student' network (a simple neural network) is used to perform feature selection by minimizing the reconstruction error of low dimensional representation. Although the teacher-student scheme is not new, to the best of our knowledge, it is the first time that this scheme is employed for feature selection. The proposed TSFS can be used for both supervised and unsupervised feature selection. This method is evaluated on different datasets and is compared with state-of-the-art existing feature selection methods. The results show that TSFS performs better in terms of classification and clustering accuracies and reconstruction error. Moreover, experimental evaluations demonstrate a low degree of sensitivity to parameter selection in the proposed method. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 28	2020	383						396	408		10.1016/j.neucom.2019.12.017													
J								Zeroing neural network methods for solving the Yang-Baxter-like matrix equation	NEUROCOMPUTING										Zeroing neural network; Yang-Baxter-like matrix equation; Idempotent matrix equation; Nonlinear activation function; Convergence analysis	VARYING SYLVESTER EQUATION; OPTIMAL DIVIDEND PROBLEM; FINITE-TIME CONVERGENCE; PARAMETER-ESTIMATION; ITERATIVE IDENTIFICATION; ESTIMATION ALGORITHM; NONLINEAR-SYSTEMS; MODEL; STABILITY; DYNAMICS	By solving the idempotent matrix equation Y-2 = Y and using the zeroing neural network model, an indirect numerical solution for the Yang-Baxter-like matrix equation is established. By defining the error function and using the zeroing neural network, a direct method is suggested for solving the time-invariant and time-varying Yang-Baxter-like matrix equations. The asymptotic convergence and the fixed-time convergence are discussed. Four numerical examples are offered to illustrate the efficacy of the suggested methods. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 28	2020	383						409	418		10.1016/j.neucom.2019.11.101													
J								Joint extraction of entities and relations based on character graph convolutional network and Multi-Head Self-Attention Mechanism	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Tagging scheme; Character Graph Convolutional Network (CGCN); Multi-Head Self-Attention Mechanism (MS); densely Connected Convolutional Network (Dense Net)		The traditional method of extracting entities and relations not only disregards the dependency between the two subtasks of entities and relations but also facilitates the cumulative propagation of errors. To solve these problems, a method - called MSBD - of joint extraction of entities and relations based on the Character Graph Convolutional Network (CGCN) and Multi-Head Self-Attention Mechanism (MS) is proposed. First, a new tagging scheme is used to tag the positions of entities and relations in the text. Second, to depict the hierarchical structure information between the entities and the relations in the text and the internal structure information of the entity, the CGCN is designed to obtain the character vector of the text. MS is introduced into the coding framework of Bidirectional Long Short-Term Memory (BiLSTM) to capture the relative position information between two entities in the text and represent the subspace information. The Dense Connected Convolutional Network (Dense Net) is embedded in the decoding framework to enhance the reuse and transmission of key information and achieve joint extraction of entities and relations in the text. The experimental results show that the P, R and F values are significantly improved for extracting entities and relations.																	0952-813X	1362-3079															10.1080/0952813X.2020.1744198		MAR 2020											
J								Deep convolutional neural networks with transfer learning for automated brain image classification	MACHINE VISION AND APPLICATIONS										Transfer learning; Overfit; Classification; MR images	SUPPORT VECTOR MACHINE; FEATURE-EXTRACTION; GLIOMA DETECTION; WAVELET ENTROPY; TUMOR; MRI; SEGMENTATION; OPTIMIZATION; TEXTURE; HYBRIDIZATION	MR brain image categorization has been an active research domain from the last decade. Several techniques have been devised in the past for MR image categorization, starting from classical to the deep learning methods like convolutional neural networks (CNNs). Classical machine learning methods need handcrafted features to perform classification. The CNNs, on the other hand, perform classification by extracting image features directly from raw images via tuning the parameters of the convolutional and pooling layer. The features extracted by CNN strongly depend on the size of the training dataset. If the training dataset is small, CNN tends to overfit after several epochs. So, deep CNNs (DCNNs) with transfer learning have evolved. The prime objective of the present work is to explore the capability of different pre-trained DCNN models with transfer learning for pathological brain image classification. Various pre-trained DCNNs, namely Alexnet, Resnet50, GoogLeNet, VGG-16, Resnet101, VGG-19, Inceptionv3, and InceptionResNetV2, were used in the present study. The last few layers of these models were replaced to accommodate new image categories for our application. These models were extensively evaluated on data from Harvard, clinical, and benchmark Figshare repository. The dataset was then partitioned in the ratio 60:40 for training and testing. The validation on the test set reveals that the pre-trained Alexnet with transfer learning exhibited the best performance in less time compared to other proposed models. The proposed method is more generic as it does not need any handcrafted features and can achieve an accuracy value of 100%, 94%, and 95.92% for three datasets. Other performance measures used in the study include sensitivity, specificity, precision, false positive rate, error, F-score, Mathew correlation coefficient, and area under the curve. The results are compared with both the traditional machine learning methods and those using CNN.																	0932-8092	1432-1769				MAR 27	2020	31	3							20	10.1007/s00138-020-01069-2													
J								Singular value decomposition-based virtual representation for face recognition	MACHINE VISION AND APPLICATIONS										Singular value decomposition; Virtual sample; Face recognition; Collaborative representation classification	SPARSE REPRESENTATION; COLLABORATIVE REPRESENTATION; CLASSIFICATION METHOD; BIMODAL BIOMETRICS; LINEAR-REGRESSION; IMAGE; ALGORITHM	In the field of face recognition, a key issue is whether there are a sufficient number of face training samples with valid information. Due to the complexity of human face images, face recognition is easy to be affected by the external environment such as light intensity, gesture expression, hairstyle, and occlusion. Therefore, it is difficult to obtain enough effective samples in practical applications. In this paper, we propose a new algorithm that generates virtual images by utilizing the information of the test sample via singular value decomposition. The virtual images not only extend the training sample set but also can better adapt to the test sample. In addition, we use the weighted score fusion scheme to calculate the ultimate result, which can better take advantages of data from different sources including original images and virtual images. Experimental results on the Extended Yale_B, AR, GT, ORL, and FERET face databases prove that our algorithm can obtain satisfactory performance.																	0932-8092	1432-1769				MAR 27	2020	31	3							19	10.1007/s00138-020-01067-4													
J								Approximate kernel partial least squares	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Kernel partial least squares (KPLS); Random Fourier features; Randomized kernel partial least squares (RKPLS); Matrix approximate	LINEAR-REGRESSION	As an extension of partial least squares (PLS), kernel partial least squares (KPLS) is an very important methods to find nonlinear patterns from data. However, the application of KPLS to large-scale problems remains a big challenge, due to storage and computation issues in the number of examples. To address this limitation, we utilize randomness to design scalable new variants of the kernel matrix to solve KPLS. Specifically, we consider the spectral properties of low-rank kernel matrices constructed as sums of random feature dot-products and present a new method called randomized kernel partial least squares (RKPLS) to approximate KPLS. RKPLS can alleviate the computation requirements of approximate KPLS with linear space and computation in the sample size. Theoretical analysis and experimental results show that the solution of our algorithm converges to exact kernel matrix in expectation.																	1012-2443	1573-7470				SEP	2020	88	9					973	986		10.1007/s10472-020-09694-3		MAR 2020											
J								Deep Multicameral Decoding for Localizing Unoccluded Object Instances from a Single RGB Image	INTERNATIONAL JOURNAL OF COMPUTER VISION										Instance boundary and occlusion detection; Fully convolutional encoder-decoder networks; Synthetic data; Domain adaptation		Occlusion-aware instance-sensitive segmentation is a complex task generally split into region-based segmentations, by approximating instances as their bounding box. We address the showcase scenario of dense homogeneous layouts in which this approximation does not hold. In this scenario, outlining unoccluded instances by decoding a deep encoder becomes difficult, due to the translation invariance of convolutional layers and the lack of complexity in the decoder. We therefore propose a multicameral design composed of subtask-specific lightweight decoder and encoder-decoder units, coupled in cascade to encourage subtask-specific feature reuse and enforce a learning path within the decoding process. Furthermore, the state-of-the-art datasets for occlusion-aware instance segmentation contain real images with few instances and occlusions mostly due to objects occluding the background, unlike dense object layouts. We thus also introduce a synthetic dataset of dense homogeneous object layouts, namely Mikado, which extensibly contains more instances and inter-instance occlusions per image than these public datasets. Our extensive experiments on Mikado and public datasets show that ordinal multiscale units within the decoding process prove more effective than state-of-the-art design patterns for capturing position-sensitive representations. We also show that Mikado is plausible with respect to real-world problems, in the sense that it enables the learning of performance-enhancing representations transferable to real images, while drastically reducing the need of hand-made annotations for finetuning. The proposed dataset will be made publicly available.																	0920-5691	1573-1405				MAY	2020	128	5					1331	1359		10.1007/s11263-020-01323-0		MAR 2020											
J								Multi-step medical image segmentation based on reinforcement learning	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Reinforcement learning; Deep deterministic policy gradient; Image segmentation; Multi-step manner	DEEP; EXTRACTION	Image segmentation technology has made a remarkable effect in medical image analysis and processing, which is used to help physicians get a more accurate diagnosis. Manual segmentation of the medical image requires a lot of effort by professionals, which is also a subjective task. Therefore, developing an advanced segmentation method is an essential demand. We propose an end-to-end segmentation method for medical images, which mimics physicians delineating a region of interest (ROI) on the medical image in a multi-step manner. This multi-step operation improves the performance from a coarse result to a fine result progressively. In this paper, the segmentation process is formulated as a Markov decision process and solved by a deep reinforcement learning (DRL) algorithm, which trains an agent for segmenting ROI in images. The agent performs a serial action to delineate the ROI. We define the action as a set of continuous parameters. Then, we adopted a DRL algorithm called deep deterministic policy gradient to learn the segmentation model in continuous action space. The experimental result shows that the proposed method has 7.24% improved to the state-of-the-art method on three prostate MR data sets and has 3.52% improved on one retinal fundus image data set.																	1868-5137	1868-5145															10.1007/s12652-020-01905-3		MAR 2020											
J								PALM-CSS: a high accuracy and intelligent machine learning based cooperative spectrum sensing methodology in cognitive health care networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Co-operative spectrum sensing; High-speed multi-layer ELM; Prediction; Classification; Health care networks	RADIO NETWORKS	Spectrum sensing is the most crucial importance in cognitive radios. We propose a novel machine-learning algorithm for spectrum sensing in cognitive radio networks, which plays an essential role in medical data transmission. In this regard, high-speed pre-emptive decision-based multi-layer extreme learning machines are implemented for co-operative spectrums sensing in CR health care networks. For a radio channel, different vectors such as energy levels, distance, Channel ID, sensor values are determined at CR devices and are considered as a feature vector and thus used to feed into the proposed classifier for the determination of the availability of the channel. The classifier further categorizes the parameters such as user identification i.e., primary and secondary users, availability of channels, and the most crucial predictive decision of the available channels. The proposed PALM-CSS consists of two major phases, such as classification and prediction. Before the online classification and prediction, datasets are generated, and these datasets are used for the training of the proposed classifier. The proposed classifier uses the principle of high-speed priority-based multi-layer extreme learning machines for the classification and prediction. The experimental testbed has designed based Multicore CoxtexM-3 boards for implementing the real-time cognitive scenario and various performance parameters such as prediction accuracy, training and testing time, Receiver operating characteristics, and accuracy of detection. Furthermore, the proposed algorithms has also compared with the other existing machine learning algorithm such as artificial neural networks, support vector machines, K-nearest neighbor, Naive Bayes and ensemble machine learning algorithms in which the proposed algorithm outperforms the other existing algorithms and finds its more suitable for cognitive health care networks.																	1868-5137	1868-5145															10.1007/s12652-020-01859-6		MAR 2020											
J								Automated detection of defects with low semantic information in X-ray images based on deep learning	JOURNAL OF INTELLIGENT MANUFACTURING										Defect detection; Casting parts; Deep learning; X-ray image; Computer vision	COMPUTER VISION; CLASSIFICATION	Nondestructive testing using X-ray imaging has been widely adopted in the defect detection of casting parts for quality management. Deep learning has been proved to be an effective way to detect defects in X-ray images. In this work, Feature Pyramid Network (FPN) which has been utilized broadly in many applications is adopted as our baseline. In FPN, there mainly exits two issues: firstly, down sampling operation in Convolutional Neural Network is often utilized to enhance the perception field, causing the loss of location information in feature maps, and secondly, there exists feature imbalance in feature maps and proposals. DetNet and Path Aggregation Network are adopted to solve the two shortages. To further improve the recall rate, soft Non-Maximum Suppression (soft-NMS) is adopted to remain more proposals that have high classification confidence. Defects in X-ray images of casting parts are provided with low semantic information, causing the different instances between detection results and annotations in the same area. We propose soft Intersection Over Union (soft-IOU) criterion which could evaluate several results or ground truths in the near area, making it more accurate to evaluate detection results. The experimental results demonstrate that the three proposed strategies have better performance than the baseline for our dataset.																	0956-5515	1572-8145															10.1007/s10845-020-01566-1		MAR 2020											
J								Visual Positioning of Distant Wall-Climbing Robots Using Convolutional Neural Networks	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Wall-climbing robot; Localization; CNN	MOBILE ROBOTS; LOCALIZATION; DESIGN	Detection of visual markers, such as circular markers or quick response codes, is a commonly used approach to the positioning of wall-climbing robots. However, when the camera is far from the wall-climbing robot (e.g., 20 m), these markers become extremely blurred and difficult to detect. In this paper, a convolutional neural network-based positioning scheme comprised of a global bounding box detector and local wheel detector is proposed. The light-weight local wheel detector can quickly and accurately detect the four wheel points of a distant wall-climbing robot, and the detected wheel points can be used for calculating its position and direction angle. Our wheel detector has a single-frame processing time of 72.2 ms on a CPU and 7.1 ms on a GPU, where the latter meets the real-time positioning requirements of the wall-climbing robot. We also developed an efficient cost function for wheel matching between video frames. Simulation results and multiple test videos confirmed that the proposed cost function can match wheels between video frames perfectly. The high performance of this positioning system indicates that it may be used in a variety of industrial applications.																	0921-0296	1573-0409				JUN	2020	98	3-4					603	613		10.1007/s10846-019-01096-w		MAR 2020											
J								Optimum Cuts in Graphs by General Fuzzy Connectedness with Local Band Constraints	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Boundary band constraint; Hedgehog shape prior; Image foresting transform; Graph-cut segmentation	IMAGE FORESTING TRANSFORM; SEGMENTATION; ALGORITHM	The goal of this work is to describe an efficient algorithm for finding a binary segmentation of an image such that the indicated object satisfies a novel high-level prior, called local band, LB, constraint; the returned segmentation is optimal, with respect to an appropriate graph-cut measure, among all segmentations satisfying the given LB constraint. The new algorithm has two stages: expanding the number of edges of a standard edge-weighted graph of an image; applying to this new weighted graph an algorithm known as an oriented image foresting transform, OIFT. In our theoretical investigation, we prove that OIFT algorithm belongs to a class of general fuzzy connectedness algorithms and so has several good theoretical properties, like robustness for seed placement. The extension of the graph constructed in the first stage ensures, as we prove, that the resulted object indeed satisfies the given LB constraint. We also notice that this graph construction is flexible enough to allow combining it with other high-level constraints. Finally, we experimentally demonstrate that the LB constraint gives competitive results as compared to geodesic star convexity, boundary band, and hedgehog shape prior, all implemented within OIFT framework and applied to various scenarios involving natural and medical images.																	0924-9907	1573-7683				JUN	2020	62	5			SI		659	672		10.1007/s10851-020-00953-w		MAR 2020											
J								Cyber security incidents analysis and classification in a case study of Korean enterprises	KNOWLEDGE AND INFORMATION SYSTEMS										Cyber security; Security data sets; Data mining; Machine learning; Text analysis		The increasing amount and complexity of Cyber security attacks in recent years have made text analysis and data mining techniques an important factor in discovering features of such attacks and detecting future security threats. In this paper, we report on the results of a recent case study that involved the analysis of a community data set collected from five small and medium companies in Korea. The data set represents Cyber security incidents and response actions. We investigated in the study the kind of problems concerned with the prediction of response actions to future incidents from features of past incidents. Our analysis is based on text mining methods, such as n-gram and bag-of-words, as well as on machine learning algorithms for the classification of incidents and their response actions. Based on the results of the study, we also suggest an experience-sharing model, which we use to demonstrate how companies may share their trained classifiers without the sharing of their individual data sets in a collaborative environment.																	0219-1377	0219-3116				JUL	2020	62	7					2917	2935		10.1007/s10115-020-01452-5		MAR 2020											
J								Feature extraction from null and non-null spaces of kernel local discriminant embedding	KNOWLEDGE AND INFORMATION SYSTEMS										Kernel methods; Local discriminant embedding; Nonlinear distribution; Manifold learning; Face recognition	GENERAL FRAMEWORK; LDA	Extracting discriminative features and reducing the dimensionality of data are two main objectives of manifold learning. Among different techniques, nonlinear manifold learning methods have been proposed in order to extract features from data which are not linearly distributed. Kernel trick is one of the famous nonlinear techniques which helps to project the data without an explicit mapping which can be used in combination with different linear techniques (e.g., Linear discriminant analysis and local discriminant embedding (LDE)). In this paper, we propose a Two Subspace-based Kernel Local Discriminant Embedding (TSKLDE) method which extract features from both non-null and null space of the within-class locality preserving scatter matrix of LDE in the kernel space. We evaluated the proposed algorithm using three publicly available face databases. The obtained results demonstrate that the use of both features in TSKLDE leads to more noise tolerant features compared to other kernel methods and to higher discriminant ability than many existing manifold learning techniques.																	0219-1377	0219-3116				AUG	2020	62	8					3217	3238		10.1007/s10115-020-01457-0		MAR 2020											
J								A support vector regression model for the prediction of total polyaromatic hydrocarbons in soil: an artificial intelligent system for mapping environmental pollution	NEURAL COMPUTING & APPLICATIONS										Support vector regression model; Total polyaromatic hydrocarbon (TPAH) and total petroleum hydrocarbons (TPH); Artificial intelligence; Pollution mapping	SUPERCONDUCTING TRANSITION-TEMPERATURE; NEURAL-NETWORKS; EXTRACTION; METABOLITE; CARBON; PAHS	The significance of total polyaromatic hydrocarbons (TPAH) determination in assessing the carcinogenicity of environmental samples for measuring the level of environmental pollution cannot be overemphasized. Despite the environmental danger of TPAH, its laboratory quantification is laborious, which consumes appreciable time and other valuable resources. This research work develops a computational intelligence-based model for the first time, which directly estimates and quantifies the level of TPAH of any environmental solid samples using total petroleum hydrocarbons descriptor that can be easily determined experimentally. The hyperparameters of the developed support vector regression (SVR)-based model are optimized using manual search (MS) approach and genetic algorithm (GA) search approach with Gaussian and polynomial kernel functions. Experimental validation of the developed model was carried out using samples obtained from the marine sediments of Arabian Gulf Sea. The future generalization and predictive strength of the developed models were assessed using correlation coefficient (CC), root-mean-square error, mean absolute error and mean absolute percentage deviation (MAPD). GA-SVR-Gaussian performs better than MS-SVR and GA-SVR-poly with performance enhancement of 63.89% and 536.32%, respectively, on the basis of MAPD as a performance-measuring parameter, while MS-SVR model performs better than GA-SVR-poly with performance improvement of 288.25% using MAPD to evaluate the model performance. The estimation accuracy and generalization strength of the developed models indicate the potential of the models in measuring the level of environmental pollution of oil-spilled area without experimental stress, while experimental precision is preserved.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14899	14908		10.1007/s00521-020-04845-3		MAR 2020											
J								Fault classification in three-phase motors based on vibration signal analysis and artificial neural networks	NEURAL COMPUTING & APPLICATIONS										Predictive maintenance; Vibration analysis; FFT; Artificial neural networks; Damage classification	IDENTIFICATION	Competition in the industrial environment is increasingly intense, so it is of utmost importance that organizations keep their assets in operation as much as possible (in order to produce more). In this context, there is a need for predictive maintenance, a technique that detects the health of assets in real time, allowing failures to be diagnosed before they can interrupt the operation of the assets, avoiding high financial losses. This study uses a sixteen-motor experimental setup with four different known operating conditions. The vibration signal of these motors, through signal analysis, both in time and frequency domains, is performed to evaluate the types and severities of the defects. An artificial neural network (ANN) is used to classify these defects. Considering the vibration analysis, mechanical faults can be identified quickly and conveniently. For the development of the ANN, it was necessary to perform a preprocessing of the vibration signal (response in time) due to the data size, which overwhelms the network. Thus, statistical data were used to extract key information from the vibration signal. Finally, the neural network created based on this study's methodology presents extremely reliable results, allowing a quick and robust diagnosis of the motor operating condition.																	0941-0643	1433-3058				SEP	2020	32	18			SI		15171	15189		10.1007/s00521-020-04868-w		MAR 2020											
J								Multi-sensor data fusion for accurate surface modeling	SOFT COMPUTING										Surface reconstruction; Data fusion; Gaussian mixture model; Convex relaxation; Laser range sensor; Kinect (V1)	3D; RANGE; RECONSTRUCTION; REGISTRATION; RESOLUTION; CAMERAS; IMAGES; SENSOR	Multi-sensor data fusion is advantageous while fusing data from heterogeneous range sensors, for scanning a scene containing both fine and coarse details. This paper presents a new multi-sensor range data fusion method with the aim to increase the descriptive contents of the entire generated surface model. First, a new training framework of the scanned range dataset to solve the relaxed Gaussian mixture model-based method by applying the convex relaxation technique is presented. The classification of the range data is based on a trained statistical model. In the data fusion experiments, a laser range sensor and Kinect (V1) are used. Based on the segmentation criterion, the range data fusion is performed by integration of the finer regions range data obtained from a laser range sensor with the coarser regions of the Kinect range data. The fused range information overcomes the weaknesses of the respective range sensors, i.e., the laser scanner is accurate but takes time while the Kinect is fast but not very accurate. The surface model of the fused range dataset generates a highly accurate, realistic surface model of the scene. The experimental results demonstrate robustness of the proposed approach.																	1432-7643	1433-7479				OCT	2020	24	19					14449	14462		10.1007/s00500-020-04797-9		MAR 2020											
J								The probe for the weighted dual probabilistic linguistic correlation coefficient to invest an artificial intelligence project	SOFT COMPUTING										Dual probabilistic linguistic term sets; Weighted correlation coefficient; Distance measure; Closeness coefficient; Multi-attribute group decision making	GROUP DECISION-MAKING; HESITANT FUZZY-SETS; TERM SETS; PREFERENCE; VECTORS	As one of the burgeoning decision-making instruments, the integrity of dual probabilistic linguistic term sets (DPLTSs) is to express the decision information in terms of cognitive certainty and uncertainty. The superiority of correlation coefficient is to demonstrate the interrelationship of the variables. This paper aims to give full play to the advantages of the above two. Firstly, it defines the dual probabilistic linguistic correlation coefficient. Then, it is based on the proposed entropy for DPLTSs calculates the comprehensive weight vector. Moreover, combined with the proposed correlation coefficient, it further defines the weighted correlation coefficient as a measure for the application about artificial intelligence. Besides, it uses the dual probabilistic linguistic closeness coefficient as the reference to compare the pros and cons. Finally, a specific numeric simulation is utilized to demonstrate the feasibility of the two different measures.																	1432-7643	1433-7479				OCT	2020	24	20					15389	15408		10.1007/s00500-020-04873-0		MAR 2020											
J								A study of deep neural networks for human activity recognition	COMPUTATIONAL INTELLIGENCE										convolutional neural network; deep learning; human activity recognition; recurrent neural network	LEARNING APPROACH; BACKPROPAGATION; SENSORS	Human activity recognition and deep learning are two fields that have attracted attention in recent years. The former due to its relevance in many application domains, such as ambient assisted living or health monitoring, and the latter for its recent and excellent performance achievements in different domains of application such as image and speech recognition. In this article, an extensive analysis among the most suited deep learning architectures for activity recognition is conducted to compare its performance in terms of accuracy, speed, and memory requirements. In particular, convolutional neural networks (CNN), long short-term memory networks (LSTM), bidirectional LSTM (biLSTM), gated recurrent unit networks (GRU), and deep belief networks (DBN) have been tested on a total of 10 publicly available datasets, with different sensors, sets of activities, and sampling rates. All tests have been designed under a multimodal approach to take advantage of synchronized raw sensor' signals. Results show that CNNs are efficient at capturing local temporal dependencies of activity signals, as well as at identifying correlations among sensors. Their performance in activity classification is comparable with, and in most cases better than, the performance of recurrent models. Their faster response and lower memory footprint make them the architecture of choice for wearable and IoT devices.																	0824-7935	1467-8640				AUG	2020	36	3					1113	1139		10.1111/coin.12318		MAR 2020											
J								Varied channels region proposal and classification network for wildlife image classification under complex environment	IET IMAGE PROCESSING										feature extraction; object recognition; object detection; learning (artificial intelligence); neural nets; image segmentation; image classification; convolution; cameras; varied channels region proposal; classification network; wildlife image classification; deep convolutional neural network; automatic wildlife animal classification; camera trapped images; different aims; background images; region proposal component; region candidates; classification component; animals; potential animal regions; low contrast animal images; object detection network; faster region convolutional neural network		A varied channels region proposal and classification network (VCRPCN) is developed based on a deep convolutional neural network (DCNN) and the characteristics of the animals appearing for automatic wildlife animal classification in camera trapped images, the architecture of the network is improved by feeding different channels into different components of the network to accomplish different aims, i.e. the animal images and their background images are employed in the region proposal component to extract region candidates for the animal's location, and the animal images combined with the region candidates are fed into the classification component to identify their categories. This novel architecture considers changes to the image due to the animals' appearances, and identifies potential animal regions in images and extracts their local features to describe and classify them. Five hundred low contrast animal images have been collected. All images have low contrast due to being acquired during the night. Cross-validation is employed to statistically measure the performance of the proposed algorithm. The experimental results demonstrate that in comparison with the well-known object detection network, faster R-CNN, the proposed VCRPCN achieved higher accuracy with the same dataset and training configuration with an average accuracy improvement of 21%.																	1751-9659	1751-9667				MAR 27	2020	14	4					585	591		10.1049/iet-ipr.2019.1042													
J								Robust retinal optic disc and optic cup segmentation via stationary wavelet transform and maximum vessel pixel sum	IET IMAGE PROCESSING										wavelet transforms; eye; diseases; biomedical optical imaging; image segmentation; blood vessels; medical image processing; irreversible blindness; cup-disc ratio; glaucoma; ROI; maximum vessel pixel sum; partial cup edges; cup boundary; central retinal blood vessels; OC segmentation; robust retinal optic disc; optic cup segmentation; fundus image; inverse polar transform; horizontal coefficients; sixth level decomposition Daubechies stationary wavelet transform	MATHEMATICAL MORPHOLOGY; DIABETIC-RETINOPATHY; AUTOMATIC DETECTION; FUNDUS IMAGES; NERVE HEAD; GLAUCOMA; PREVALENCE; PEOPLE	Glaucoma leads to irreversible blindness and its diagnosis relies heavily on cup to disc ratio. This ratio can be calculated by segmenting the optic disc (OD) and optic cup (OC) from the fundus image. However, the segmentation of OD and OC is a complex process and should be carried out with utmost accuracy to screen the risk of glaucoma. In order to circumvent this complexity, this study presents two novel algorithms to segment the OD and OC boundaries, respectively by creating an automated region of interest (ROI). The first algorithm uses the inverse polar transform to segment OD where the horizontal coefficients of sixth level decomposition Daubechies stationary wavelet transform of ROI is processed. The second algorithm uses maximum vessel pixel sum to extract the complete OC region by extending the partial cup edges to the nasal side of the cup boundary. This approach covers the region under central retinal blood vessels also which were missing in earlier research. The proposed algorithms achieved an accuracy rate up to 99.70% for OD and 99.47% for OC segmentation, respectively even under severe retinal pathological conditions.																	1751-9659	1751-9667				MAR 27	2020	14	4					592	602		10.1049/iet-ipr.2019.0845													
J								Context-based ensemble classification for the detection of architectural distortion in a digitised mammogram	IET IMAGE PROCESSING										image classification; mammography; learning (artificial intelligence); medical image processing; radiology; context-based ensemble classification; architectural distortion; digitised mammogram; computer-aided detection; particular region; surrounding regions; inference; multiple radiologists; test RoI; surrounding RoIs; context-based ensemble classifier		The problem of computer-aided detection of architectural distortion (AD) in a digitised mammogram has been attempted in this manuscript. In examining a mammogram, the decision regarding a particular region of interest (RoI) is dependent on the appearance of the surrounding regions. However, in existing methods to detect AD the inference about an RoI is dependent on the appearance of this RoI alone. In addition, multiple radiologists infer the same mammogram in coming to a final decision about the mammogram. Contrary to popular ensemble classifiers like Adaboost and Random Forest, the authors propose an ensemble based method (imitating multiple radiologists by classifiers) for detecting AD such that the decision on a test RoI is dependent on the decisions of the surrounding RoIs in the proposed ensemble classifier. The proposed context-based ensemble classifier has been validated on two mammographic databases. The proposal shows promising results in both the databases.																	1751-9659	1751-9667				MAR 27	2020	14	4					603	614		10.1049/iet-ipr.2019.0639													
J								Detection and localisation of multiple brain tumours by object counting and elimination	IET IMAGE PROCESSING										medical image processing; image segmentation; cancer; tumours; biomedical MRI; object detection; brain; time-consuming manual segmentation; brain image scans; rigorous image processing; automated image segmentation method; evaluation process; object detection; multiple binary images; tumour parameters; magnetic resonance imaging images; object counting; brain cancer; medical knowledge; laboratory test; medical images; treatment protocols; multiple brain tumour localisation; multiple brain tumour detection	SKULL STRIPPING PROBLEM; SEGMENTATION	Brain cancer is a major health problem that affects the lives of many people. Clinicians depend on their medical knowledge to analyse laboratory test results and clinical information extracted manually from medical images, to identify the essential characteristics of tumours such as size, shape, and location. The accurate diagnosis of brain tumours is very essential for deciding the most appropriate treatment protocols and procedures. Apart from being tedious, slow and time-consuming manual segmentation of brain image scans is also prone to human error. In order to enhance the accuracy and reliability of the diagnosis of brain tumours, rigorous image processing and segmentation is required. In this study the authors present an automated image segmentation method for detecting and localising multiple brain tumours of different sizes and intensities by using a technique of object counting coupled with an evaluation process. The object counting technique were carried out using multiple binary images with different threshold values, and the evaluation process was based on assessment criteria of specific tumour parameters. Different magnetic resonance images with multiple tumours were used for testing the proposed method. The results indicated that the proposed method is effective in detecting and localising multiple true tumours in the brain.																	1751-9659	1751-9667				MAR 27	2020	14	4					615	620		10.1049/iet-ipr.2019.0071													
J								Deep convolution network for dense crowd counting	IET IMAGE PROCESSING										learning (artificial intelligence); feature extraction; object detection; convolutional neural nets; dense crowd counting; deep learning framework; MCNN; CP-CNN; convolutional neutral network; transposed convolutional layers; high-quality density map; two-branch CNN; deep convolution network; feature extraction; density map; stacked pooling; ShanghaiTech PartA dataset; ShanghaiTech PartB dataset		Estimating the total number of people in a crowded situation is a challenging task due to numerous occlusions and perspective changes existing in crowd images. To address this issue, the authors have proposed a new deep learning framework for accurate and efficient crowd counting here. Inspired by multi-column convolutional neural network (MCNN) and contextual pyramid convolutional neural network (CP-CNN), the authors use a combination of a two branches, convolutional neutral network (CNN) and transposed convolutional layers, to generate a high-quality density map. The two-branch CNN for feature extraction generates a density map that is only a quarter of size of the original image Then a set of transposed convolutional layers and convolutional layers are combined with the network to make up for the detail loss of the density map conducted by stacked pooling. Compared with MCNN and CP-CNN, the authors' approach employs fewer branches and simpler architecture. Experimental result shows that their approach achieves MAE 80.7 and MSE 131.2 in ShanghaiTech PartA dataset, MAE 15.6 and MSE 26.8 in ShanghaiTech PartB dataset, and MAE Average 7.1 in WorldExpo'10 dataset.																	1751-9659	1751-9667				MAR 27	2020	14	4					621	627		10.1049/iet-ipr.2019.0435													
J								Reference data preparation for complex satellite image segmentation	IET IMAGE PROCESSING										image segmentation; data preparation; HRS images; eye-tracking data; CTA data; satellite image segmentation; high-resolution satellite images; segmentation methods; visual analysis; Gestalt principles; Gibson theory; psycho-visual-based approach; concurrent think-aloud data	EYE-MOVEMENTS; BUILDING DETECTION; SALIENCY; PERCEPTION; ATTENTION; FEATURES; REGION	The high spatial resolution satellite images covering multiple objects of the urban area are visually complex in nature. This visual complexity causes ambiguity during segmentation of such images when the targets are unknown. In such case, reference data are required to assess the segmentation methods. Due to excellence of humans in visual analysis, the presented work has attempted for a psycho-visual approach to prepare the reference segmented complex HRS images. The reference data have has prepared through the correlation of the quantified eye-tracking data (metrics) and corresponding concurrent think-aloud (CTA) data for each of the created segments. Segments get updated based on Gestalt principles and Gibson's theory. Those segments having the best correlation between metrics and CTA data have been opted as the final output. The results suggest that the functional grouping of objects while preferring perceptual grouping for segments drawing conforms the most to the participants' verbal response. The final results have been compared with the existing notion of full segmentation used for complex images. The comparison also proffers the superiority of the proposed segmentation over full segmentation to be used as reference. In the future, a large number of images may be used to prepare better reference data.																	1751-9659	1751-9667				MAR 27	2020	14	4					628	637		10.1049/iet-ipr.2019.0234													
J								Effective key-frame extraction approach using TSTBTC-BBA	IET IMAGE PROCESSING										mean square error methods; video coding; object detection; image retrieval; video signal processing; image colour analysis; feature extraction; video retrieval; image segmentation; content-based retrieval; key-frame extraction approach; video summarisation; video processing; content-based video analysis; proficient video; effective key-frame extraction; input video database; TSTBTC algorithm; red-green-blue colour space; LUV colour space; YCbCr colour space; YUV colour space; TSTBTC-BBA algorithm; sorted ternary block truncation coding		Video summarisation and key-frame extraction play a vital role in video processing and content-based video analysis. Key-frame extraction facilitates rapid browsing and proficient video ordering in numerous applications such as multimedia, gaming etc. The application of 'effective key-frame extraction' can be used in the field of object detection and tracking, and it is found to be helpful in providing some sort of signal to visually impaired persons. In this study, the advantages of Thepade's sorted ternary block truncation coding (TSTBTC) along with binary bat algorithm (BBA) are used to overcome the limitations by extracting an effective key frame from the videos. Static images in the form of frames are extracted from input video database and processed through TSTBTC algorithm for calculating similarity measures amongst two consecutive frames. The input data is processed using four colour spaces, namely red-green-blue colour space, Kekre's LUV colour space, YCbCr colour space, YUV colour space and BBA are used to optimise the threshold value. Furthermore, five similarity measures indices are used to calculate the number of frames, and the result obtained shows that TSTBTC-BBA algorithm deployed in YUV colour space provides better results compared with other existing techniques regarding precision, F-measure and colour spaces.																	1751-9659	1751-9667				MAR 27	2020	14	4					638	647		10.1049/iet-ipr.2018.6361													
J								Image quality assessment via spatial-transformed domains multi-feature fusion	IET IMAGE PROCESSING										regression analysis; feature extraction; gradient methods; image fusion; transforms; vectors; random forests; learning (artificial intelligence); spatial-transformed domains multifeature fusion; image processing; subjective methods; image quality assessment tasks; edge contour information; mask scale; cross-database operation capability; gradient information operators; feature extraction; 12-dimensional feature vector generation; random forest regression technique; FVC-G	VISUAL SALIENCY; SIMILARITY	The basis of image processing is to evaluate and monitor image quality using algorithms rather than subjective methods. Conventional gradient operators have been popularly used in previous image quality assessment tasks to reflect the edge contour of an image, while there are some obvious defects in terms of the selection of mask scale and direction. Some improved versions are also less than ideal since they fail to consider the gradient information of the same pixel in different directions at the same time. The authors adopt a powerful gradient operator that can simultaneously capture edge information in all four directions at the same pixel point with more relevant values being considered instead of selecting the maximum in these four directions. Furthermore, four complementary types of features extracted from the spatial and transform domains are considered. A set of 12-dimensional feature vectors is generated for each image by multi-feature fusion. Ultimately, random forest regression technique is employed to train their model and then map the distortion effects to the prediction scores. The experimental results show that the proposed FVC-G has better overall performance, more powerful cross-database operation capability, and higher visual consistency than other advanced methods.																	1751-9659	1751-9667				MAR 27	2020	14	4					648	657		10.1049/iet-ipr.2018.6417													
J								Medical fusion framework using discrete fractional wavelets and non-subsampled directional filter banks	IET IMAGE PROCESSING										neurophysiology; single photon emission computed tomography; medical image processing; entropy; image fusion; channel bank filters; brain; biomedical MRI; feature extraction; wavelet transforms; neuro diagnosis; brain image; intermodal transmission; medical image fusion; vital salient information; colour deficiencies; salient image elements; Grey Wolf optimisation algorithm; neurological multimodal magnetic resonance imaging; diagnostic information; superior image quality; medical fusion framework; nonsubsampled directional filter banks; single photon emission computed tomography brain image; discrete fractional wavelet transform; Shannon entropy	ADAPTIVE HISTOGRAM EQUALIZATION; IMAGES; PET; TRANSFORM; MODEL; CT	Image fusion in neuro diagnosis is intimidating due to its complexity. The heterogeneous natures of the original brain images make intermodal transmission difficult during fusion. Medical image fusion using complementary modalities results in loss of vital salient information. Poor fusion, colour deficiencies result due to similar processing for both the modalities. A dual technique is proposed using discrete fractional wavelet transform (FRWT) and non-subsampled directional filter banks for better extraction of salient image elements for improved diagnosis. The sparsity character of the coefficients FRWT is controlled by optimising the parity operator using Grey Wolf optimisation algorithm. Four sets of neurological multimodal magnetic resonance imaging and single photon emission computed tomography (CT) brain images are used from benchmark database for validation. The objective evaluation has been conducted using five metrics. The main values obtained from objective metrics based on the proposed technique are 6.3213 for Shannon entropy, mutual information is computed to be 2.7582, fusion factor is 1.9095, standard deviation is 0.1310, and edge strength is 0.76122 indicating improved diagnostic information and superior image quality. Subjective evaluation by a medico validates the findings with finer visual output and enhanced contrast in comparison with recent and state-of-the-art methods.																	1751-9659	1751-9667				MAR 27	2020	14	4					658	667		10.1049/iet-ipr.2019.0948													
J								Low light image enhancement with adaptive sigmoid transfer function	IET IMAGE PROCESSING										image enhancement; computer vision; neural nets; transfer functions; adaptive sigmoid transfer function; visually pleasant images; computer vision applications; low light images; visual aesthetics; high computational complexity; visual quality; computationally efficient low light image enhancement framework; sigmoid activation function; contrast-enhanced images	HISTOGRAM EQUALIZATION; CONTRAST ENHANCEMENT; ALGORITHM; RETINEX	Low light image enhancement algorithms intent to produce visually pleasant images and target to extract valuable information for computer vision applications. The task of improving the quality of low light images is a challenging one. The existing methods for quality improvement undeniably annoy the visual aesthetics and suffer the major drawback of high computational complexity and less efficiency. To improve the visual quality and lower the distortions, a simple and computationally efficient low light image enhancement framework is presented in this study. To achieve this, an adaptive sigmoid transfer function (ASTF) is used and is derived from the sigmoid activation function of neural networks. By combining ASTF with a Laplacian operator, colour and contrast-enhanced images are obtained. Experiments show the effectiveness of the proposed method with state-of-the-art methods.																	1751-9659	1751-9667				MAR 27	2020	14	4					668	678		10.1049/iet-ipr.2019.0781													
J								New design of adaptive Gabor wavelet filter bank for medical image retrieval	IET IMAGE PROCESSING										maximum likelihood estimation; Gabor filters; wavelet transforms; medical image processing; image segmentation; image texture; image retrieval; feature extraction; adaptive Gabor; medical image retrieval; texture feature; feature descriptor; Gabor wavelet basis; dominant directional features; Gabor filters; feature vector; heterogeneous medical images; average retrieval rate	BINARY FEATURE DESCRIPTOR; PATTERNS; FEATURES; MRI	Gabor wavelet is widely used in the analysis of texture feature. This study presents a novel feature descriptor based on the design of adaptive Gabor wavelet filter-bank for medical image retrieval. The design of proposed Gabor wavelet provides flexibility to extract the dominant directional features from medical images. First, peaks in the spectrum of medical image are analysed to determine the dominant directions present in the image. With these dominant directions, a bank of Gabor-filters is designed to extract the directional features effectively. Next, feature vector is derived by computing the energy and standard deviation from the Gabor filtered coefficients at a particular scale and orientation. The use of maximum likelihood estimation (MLE) is suggested to measure the similarity between the feature vectors of heterogeneous medical images. The performance of the proposed method is evaluated on three different publicly available databases namely NEMA, OASIS and EXACT09. The performance in terms of average retrieval precision (ARP), average retrieval rate (ARR) and computational time are compared with well-known existing methods. It is observed from experimental results that the proposed approach achieved ARP of 85.32, 85.24 and 77% and ARR of 31.33, 14.05 and 23.78% for NEMA, OASIS and EXACT09 databases respectively.																	1751-9659	1751-9667				MAR 27	2020	14	4					679	687		10.1049/iet-ipr.2019.1024													
J								Efficient inception V2 based deep convolutional neural network for real-time hand action recognition	IET IMAGE PROCESSING										convolution; neural nets; gesture recognition; image classification; feature extraction; learning (artificial intelligence); AP; Faster R-CNN Inception V2 model; real-time hand gesture recognition system; Efficient inception V2; real-time hand action recognition; effective network; accurate deep convolutional neural network; faster region-based convolutional neural network; standard data sets; NUS hand posture data set-II; IoU value; higher precision; SSD Inception V2 model; MITI-HD 160	GESTURE RECOGNITION	The most effective and accurate deep convolutional neural network (faster region-based convolutional neural network (Faster R-CNN) Inception V2 model, single shot detector (SSD) Inception V2 model) based architectures for real-time hand gesture recognition is proposed. The proposed models are tested on standard data sets (NUS hand posture data set-II, Senz-3D) and custom-developed (MITI hand data set (MITI-HD)) data set. The performance metrics are analysed for intersection over union (IoU) ranges between 0.5 and 0.95. IoU value of 0.5 resulted in higher precision compared to other IoU values considered (0.5:0.95, 0.75). It is observed that the Faster R-CNN Inception V2 model resulted in higher precision (0.990 for AP(all), IoU = 0.5) compared to SSD Inception V2 model (0.984 for (all)) for MITI-HD 160. The computation time of Faster R-CNN Inception V2 is higher compared to SSD Inception V2 model and also resulted in less number of mispredictions. Increasing the size of samples (MITI-HD 300) resulted in improvement of AP(all) = 0.991. Improvement in large (APlarge) and medium (APmedium) size detections are not significant when compared to small (APsmall) detections. It is concluded that the Faster R-CNN Inception V2 model is highly suitable for real-time hand gesture recognition system under unconstrained environments.																	1751-9659	1751-9667				MAR 27	2020	14	4					688	696		10.1049/iet-ipr.2019.0985													
J								Wavelet transform modulus maxima-based robust logo watermarking	IET IMAGE PROCESSING										watermarking; image watermarking; random noise; wavelet transforms; copyright; image coding; geometrical attacks; geometric attacks; 1-bit watermark; distortion parameter detection capabilities; modulus maxima-based robust logo watermarking; digital image watermarking; digital images; novel blind logo image watermarking technique; RGB images; error correction capabilities; human visual system; different watermarks; different sub-bands; high capacity multibit watermark	SPREAD-SPECTRUM WATERMARKING; IMAGE WATERMARKING	Digital image watermarking is used to protect the copyright of digital images. In this study, a novel blind logo image watermarking technique for RGB images is proposed. The proposed technique exploits the error correction capabilities of the human visual system. It embeds two different watermarks in the wavelet/multiwavelet domains. The two watermarks are embedded in different sub-bands, are orthogonal, and serve different purposes. One is a high capacity multi-bit watermark used to embed the logo, and the other is a 1-bit watermark which is used for the detection and reversal of geometrical attacks. The two watermarks are both embedded using a spread spectrum approach, based on a pseudo-random noise sequence and a unique secret key. Robustness against geometric attacks such as rotation, scaling, and translation (RST) is achieved by embedding the 1-bit watermark in the wavelet transform modulus maximacoefficients of the wavelet transform. The experimental results show that the proposed watermarking technique has better distortion parameter detection capabilities and compares favourably against existing techniques in terms of robustness against geometrical attacks such as RST.																	1751-9659	1751-9667				MAR 27	2020	14	4					697	708		10.1049/iet-ipr.2018.5868													
J								Semi-supervised convolutional generative adversarial network for hyperspectral image classification	IET IMAGE PROCESSING										neural nets; pattern classification; geophysical image processing; learning (artificial intelligence); image classification; semisupervised generative adversarial training strategy; deep residual network; labelled samples; widely used hyperspectral images; classification performance; training samples; hyperspectral image classification; insufficient annotated samples; semisupervised convolutional generative adversarial network classification model; generative adversarial framework; adversarial game; generator captures data distribution; fake samples; three-dimensional convolutional neural network; fake cube samples; trained discriminator	SPECTRAL-SPATIAL CLASSIFICATION; NEURAL-NETWORKS	To solve the problem of insufficient annotated samples in hyperspectral image classification, the semi-supervised convolutional generative adversarial network classification model is proposed in this study. The generative adversarial framework constructs an adversarial game, where the generator captures data distribution and generates fake samples, while the discriminator determines whether the input comes from generated or training data. In the proposed method, a deep three-dimensional (3D) convolutional neural network is used to generate the so-called fake cube samples and another 3D deep residual network is designed to discriminate the inputs. Furthermore, the generated samples, labelled and unlabelled samples are put into the discriminator for joint training, and the trained discriminator can determine the authenticity of the sample and the class label. This semi-supervised generative adversarial training strategy can effectively improve the generalisation capability of the deep residual network where the labelled samples are limited. Three widely used hyperspectral images are utilised to evaluate the classification performance of the proposed method: Indian Pines, Pavia University, and Salinas-A. The classification results reveal that the proposed model can improve the classification performance and achieve competitive results compared with the state-of-art methods, especially when there are few training samples.																	1751-9659	1751-9667				MAR 27	2020	14	4					709	719		10.1049/iet-ipr.2019.0869													
J								Wavelet-based deep learning for skin lesion classification	IET IMAGE PROCESSING										image classification; telemedicine; medical image processing; wavelet transforms; learning (artificial intelligence); cancer; skin; biomedical optical imaging; wavelet-based deep learning; skin lesion classification; skin lesions; malignant forms; benign forms; benign skin lesion types; malignant types; skin cancer; malignant melanoma; seborrhoeic keratosis lesions; skin images; vertical wavelet coefficients; deep learning models; approximate coefficients; sequential wavelet transformation; approximation coefficients; transfer learning-based ResNet-18; model images; skin lesion detection		Skin lesions can be in malignant or benign forms. Benign skin lesion types are not deadly; however, malignant types of skin lesions can be fatal. Lethal forms are known as skin cancer. These types require urgent clinical treatment. Fast detection and diagnosis of malignant types of skin lesions might prevent life-threatening scenarios. This work presents two methods for the automatic classification of malignant melanoma and seborrhoeic keratosis lesions. The first method builds on modelling skin images together with wavelet coefficients. Approximate, horizontal, and vertical wavelet coefficients are obtained using the wavelet transform, and then deep learning (DL) models are generated for each of the representations and skin images. The second method builds on modelling skin images together with three approximate coefficients. This method utilises a sequential wavelet transformation to produce approximation coefficients. Then DL models are generated for each of the representations and skin images. Transfer learning-based ResNet-18 and ResNet-50 DL models provide model images and wavelet coefficients. Then skin lesion detection is achieved by fusing model output probabilities. Both proposed models outperform the methods only based on image data and other previously proposed methods.																	1751-9659	1751-9667				MAR 27	2020	14	4					720	726		10.1049/iet-ipr.2019.0553													
J								Novel visual tracking approach via ant lion optimiser	IET IMAGE PROCESSING										convergence; search problems; particle swarm optimisation; object tracking; filtering theory; abrupt motion tracking; visual tracking benchmarks; ant lion optimiser; nature-inspired swarm intelligence optimisation algorithm; high exploitation; convergence speed; adaptive boundary shrinking mechanism; search space; sensitivity; tracking performance; track efficiency; ALO-based visual tracking framework; adaptive boundary shrinking elitism	KERNEL CORRELATION FILTER; ABRUPT MOTION TRACKING; OBJECT TRACKING	Ant lion optimiser (ALO) is a new nature-inspired swarm intelligence optimisation algorithm that mimics the hunting mechanism of antlions in nature. ALO has been proved to have the merits of high exploitation and convergence speed benefiting from adaptive boundary shrinking mechanism and elitism. In this work, visual tracking is expressed as searching for object in whole search space by interaction between antlions and ants. A novel ALO-based visual tracking framework is proposed and the adaptation and sensitivity of the parameters in ALO are discussed to improve tracking performance. In addition, considering that ALO tracker needs a lot of iteration consumption, kernel correlation filter with deep feature is integrated into the ALO tracking framework (ALOKCF) to improve track efficiency. Extensive experimental results prove that the ALO tracker is very competitive compared to other trackers, especially for abrupt motion tracking. At the same time, two visual tracking benchmarks are used to verify ALOKCF tracker achieves state-of-the-art performance.																	1751-9659	1751-9667				MAR 27	2020	14	4					727	735		10.1049/iet-ipr.2018.5702													
J								Segmentation-based MAP despeckling of medical ultrasound images in shearlet domain based on normal inverse Gaussian distribution	IET IMAGE PROCESSING										image denoising; wavelet transforms; biomedical ultrasonics; medical image processing; speckle; radar imaging; transforms; Gaussian distribution; Bayes methods; kidney; image segmentation; denoising results; segmentation-based MAP despeckling; medical ultrasound images; shearlet domain; normal inverse Gaussian distribution; speckle noise; undesirable phenomenon; granular pattern; diagnostic capability; clinical US images; speckle denoising method; detail band shearlet coefficients; log-transformed; detail shearlet subband; image local statistics; heterogeneity viz; homogeneous regions; strongly heterogeneous regions; denoised images; prior distributions maximum; posteriori estimation; strongly heterogeneous coefficients; MAP expression; noise-free synthetic kidney; 60 real US images	SPECKLE NOISE-REDUCTION; WAVELET DOMAIN; BAYESIAN PROCESSOR; SHRINKAGE; THRESHOLD; REMOVAL	Speckle noise is an undesirable phenomenon that exhibits granular pattern which reduces the diagnostic capability of clinical ultrasound (US) images. In this study, a new speckle denoising method based on modelling of detail band shearlet coefficients of log-transformed US images is presented. In each detail shearlet subband, coefficients corresponding to signal and speckle noise are modelled as normal inverse Gaussian and Gaussian priors, respectively. These coefficients, based on image local statistics are segmented into different regions of heterogeneity viz. homogeneous, heterogeneous and strongly heterogeneous regions, respectively, so as to control over smoothing of denoised images. Then, using the prior distributions maximum a posteriori (MAP) estimation is performed over all regions of detail bands except those regions that represent strongly heterogeneous coefficients. For better performance, an adaptive weight function is also used in the MAP expression which reduces the loss of feature information. Experimentation is done on noise-free synthetic kidney and foetus US images and a set of 60 real US images. The results are presented for objective and subjective quality assessment of the proposed method and five other methods for speckle denoising. The potential of the proposed method in comparison to other methods can easily be ascertained from the obtained denoising results.																	1751-9659	1751-9667				MAR 27	2020	14	4					736	746		10.1049/iet-ipr.2018.6347													
J								Normalised gamma transformation-based contrast-limited adaptive histogram equalisation with colour correction for sand-dust image enhancement	IET IMAGE PROCESSING										dust; image colour analysis; sand; image enhancement; computer vision; image segmentation; normalised gamma transformation-based contrast-limited adaptive histogram equalisation; sand-dust image enhancement; sand-dust weather; serious colour cast; CLAHE; Lab colour space; image contrast enhancement; image colour correction; colour deviation; input sand-dust images; red colour space; blue colour space; unbalanced contrast; normalised gamma correction function; grey-world-based colour correction method; testing images; contrast restoration; colour fidelity; green colour space	QUALITY ASSESSMENT; RESTORATION; VISIBILITY; WEATHER; WORLD; RETINEX	Images captured in the sand-dust weather often suffer from serious colour cast and poor contrast, and this has serious implications for outdoor computer vision systems. To address these problems, a normalised gamma transformation-based contrast-limited adaptive histogram equalisation (CLAHE) with colour correction in Lab colour space for sand-dust image enhancement is proposed in this study. This method consists of image contrast enhancement and image colour correction. To avoid producing new colour deviation, the input sand-dust images are first transformed from red, green, and blue colour space into Lab colour space. Then, the contrast of the lightness component (L channel) of the sand-dust image is enhanced using CLAHE. To avoid unbalanced contrast, as well as to reduce the overincreased brightness caused by CLAHE, a normalised gamma correction function is introduced to CLAHE. After that, the a and b chromatic components are recovered by a grey-world-based colour correction method. Experiments on real sand-dust images demonstrate that the proposed method can obtain the highest percentage of new visible edges for all testing images. The contrast restoration exhibits good colour fidelity and proper brightness.																	1751-9659	1751-9667				MAR 27	2020	14	4					747	756		10.1049/iet-ipr.2019.0992													
J								Semantic combined network for zero-shot scene parsing	IET IMAGE PROCESSING										object recognition; unsupervised learning; learning (artificial intelligence); natural language processing; object detection; zero-shot scene parsing; image-based scene parsing; training set; discrete labels; meaningless labels; target domains; semantic combined network; SCN; scene parsing model; semantic embeddings; traditional fully supervised scene parsing methods; generalised ZSSP settings; state-of-the-art scenes; traditional fully supervised setting; original network models		Recently, image-based scene parsing has attracted increasing attention due to its wide application. However, conventional models can only be valid on images with the same domain of the training set and are typically trained using discrete and meaningless labels. Inspired by the traditional zero-shot learning methods which employ auxiliary side information to bridge the source and target domains, the authors propose a novel framework called semantic combined network (SCN), which aims at learning a scene parsing model only from the images of the seen classes while targeting on the unseen ones. In addition, with the assistance of semantic embeddings of classes, the proposed SCN can further improve the performances of traditional fully supervised scene parsing methods. Extensive experiments are conducted on the data set Cityscapes, and the results show that the proposed SCN can perform well on both zero-shot scene parsing (ZSSP) and generalised ZSSP settings based on several state-of-the-art scenes parsing architectures. Furthermore, the authors test the proposed model under the traditional fully supervised setting and the results show that the proposed SCN can also significantly improve the performances of the original network models.																	1751-9659	1751-9667				MAR 27	2020	14	4					757	765		10.1049/iet-ipr.2019.0870													
J								Variable step-size matching pursuit based on oblique projection for compressed sensing	IET IMAGE PROCESSING										compressed sensing; image reconstruction; signal reconstruction; computational complexity; greedy algorithms; iterative methods; matrix algebra; reconstruction efficiency; available matching pursuit algorithms; compressed sensing; compressive sensing; sparse signal reconstruction; greedy algorithms; satisfactory reconstruction performance; target signal; high-computational expenses; low-reconstruction accuracy; signal sparsity; novel variable step-size matching pursuit; VSMPOP algorithm; initial sparsity; restricted isometry property criterion; oblique projection test; sensing matrix; sparsity level; variable step size; computational complexity	SIGNAL RECOVERY; ALGORITHM; RECONSTRUCTION	The development of compressive sensing has focused on sparse signal reconstruction in recent years. Most existing greedy algorithms achieve satisfactory reconstruction performance only when the sparsity of the target signal has been known as prior information. Moreover, some greedy algorithms always involve either high-computational expenses or low-reconstruction accuracy caused by the process of adaptive adjustment of signal sparsity. To address these concerns, a novel variable step-size matching pursuit based on oblique projection (VSMPOP) for compressed sensing is proposed. The proposed VSMPOP algorithm estimates the initial sparsity based on the restricted isometry property criterion. The algorithm creates a support set of the target signal after a preliminary test and oblique projection test between the sensing matrix and the residual. VSMPOP realises a similar approach to the sparsity level with a variable step size. The experimental results demonstrated that the proposed VSMPOP algorithm provides superior performance in terms of computational complexity and reconstruction efficiency compared with most of the available matching pursuit algorithms.																	1751-9659	1751-9667				MAR 27	2020	14	4					766	773		10.1049/iet-ipr.2019.0916													
J								No-reference quality assessment for live broadcasting videos in temporal and spatial domains	IET IMAGE PROCESSING										video databases; feature extraction; neural nets; video signal processing; video streaming; image sequences; entropy; backpropagation; temporal domains; spatial domains; intensity mutation; advanced live broadcasting video delivery systems; live broadcasting video database; frame freezing; entropy-based feature extraction; global jitter; phase coherence; abnormal exposure ratio; adaptive threshold; backpropagation neural network; quality prediction model	MULTIPLY-DISTORTED IMAGES; INDEX; STATISTICS; FRAMEWORK	Nowadays, live broadcasting video has become increasingly popular and high-quality live broadcasting video is highly needed. In practice, live broadcasting videos usually undergo several processing stages, which inevitably introduce multiple distortions, e. g. frame freezing and intensity mutation, causing the degraded quality of experience. However, little work has been done to the quality evaluation of live broadcasting videos, which may hinder the further development of more advanced live broadcasting video delivery systems. Motivated by this, this study presents a no-reference quality evaluation model for live broadcasting videos (LBVQA) in temporal and spatial domains. In the temporal domain, statistic features are extracted to measure the frame freezing and intensity mutation, and the entropy-based feature is extracted to describe the global jitter. In the spatial domain, blurring is measured based on phase coherence, and abnormal exposure ratio is calculated based on an adaptive threshold. Finally, all features are fed into a backpropagation neural network to train the quality prediction model. Experimental results on the Live Broadcasting Video Database demonstrate the advantages of the proposed metric over the state-of-the-art image and video quality metrics.																	1751-9659	1751-9667				MAR 27	2020	14	4					774	781		10.1049/iet-ipr.2019.1195													
J								Learning a minimum similarity projection and lowest correlation representation for image classification	IET IMAGE PROCESSING										feature extraction; face recognition; image representation; learning (artificial intelligence); image classification; matrix algebra; minimum similarity projection; lowest correlation representation; representation learning; image classification problem; classification performance; image classification method; discriminative representation; low-dimensional space; feature projection; feature representation; projection matrix; representation results; learned projection; visual classification tasks; scene classification	FACE RECOGNITION	The projection and representation learning is an attractive tool for image classification problem due to its effectiveness and efficiency of extracting interior structure for data. However, the complexity and diversity of real data lead to the decline of classification performance. A novel image classification method is proposed by learning a minimum similarity projection and lowest correlation representation. This method attempts to produce a discriminative representation on a low-dimensional space for the data, which takes two steps: feature projection and feature representation. By learning a projection matrix, the feature projection aims to map the samples into a low-dimensional space which jointly minimises the similar within-class difference and maximises the dissimilar cross-class difference. A discriminative representation for the data on the new space is generated by using the de-correlated effect to the representation results of all classes. Therefore, the learned projection and representation simultaneously demonstrate discriminative properties in the learning of both steps. The extensive experiments conducted on different visual classification tasks consist of face recognition, object categorisation, and scene classification that the proposed method performs superior performance for image classification.																	1751-9659	1751-9667				MAR 27	2020	14	4					782	788		10.1049/iet-ipr.2019.0683													
J								Application of optimal FrFT order for improving the azimuth resolution of range Doppler imaging algorithm	IET IMAGE PROCESSING										Fourier transforms; radar imaging; synthetic aperture radar; spaceborne radar; Doppler radar; image resolution; airborne radar; airborne SAR; fractional Fourier transform; high-resolution RD azimuth imaging algorithm; synthetic aperture radar imaging; range Doppler imaging algorithm; SAR azimuth signals; optimal FrFT order; azimuth resolution; spaceborne SAR measurement data; azimuth imaging parameters		The traditional range Doppler (RD) imaging algorithm has become the most intuitive and classical method in synthetic aperture radar (SAR) imaging processing because of its easy implementation and high processing efficiency. However, owing to the poor quality of the image produced by RD algorithm, this traditional imaging method is growing increasingly unable to meet practical needs. In an effort to solve the problem of poor azimuth imaging quality, this study proposes a high-resolution RD azimuth imaging algorithm. The expression of the optimal order of the SAR azimuth signals using fractional Fourier transform (FrFT) is derived in detail herein. A theoretical analysis shows that this optimal order is unique and depends on the azimuth imaging parameters. The experimental results based on airborne SAR simulation data and spaceborne SAR measurement data indicate that the azimuth resolution of the proposed algorithm is improved by 31.6 or 46.8% as against that of the traditional RD algorithm, whereas the peak side lobe ratio (PSLR) and integrated side lobe ratio (ISLR) values obtained via the proposed algorithm are approximately equivalent to those obtained via the traditional RD algorithm.																	1751-9659	1751-9667				MAR 27	2020	14	4					789	793		10.1049/iet-ipr.2019.1065													
J								Image contrast enhancement with brightness preservation using an optimal gamma and logarithmic approach	IET IMAGE PROCESSING										image enhancement; brightness; image contrast enhancement; brightness preservation; optimal gamma; logarithmic approach; dark images; traditional histogram equalisation; visually pleasing image; input image; gamma value; histogram spikes; over-enhancement; noise artefacts effect; logarithmic transformation; low-intensity values; dark input values; intensity values	HISTOGRAM EQUALIZATION; RESONANCE; ERROR	In this study, a new enhancement framework is proposed for low contrast and dark images where traditional histogram equalisation (HE), gamma and logarithmic transformation are incorporated to achieve a visually pleasing image. Before the operation of HE on the input image, gamma and logarithmic transformation are performed in order to preserve the fine details of the image. A new gamma value of the proposed algorithm helps to restrain histogram spikes to avoid over-enhancement and noise artefacts effect. After that, a novel logarithmic transformation is used to map a narrow range of low-intensity values in the input image to a wider range of output levels. Thus, the dark input values are spread out into the higher intensity values, which improve the overall contrast and brightness of the image. The proposed method is compared with various state-of-the-art techniques. The large dataset has been used to check the feasibility of the technique. The subjective and objective analysis shows that the proposed algorithm outperforms most of the existing contrast-enhancement algorithms and the results are natural-looking, good contrast images with almost no artefacts.																	1751-9659	1751-9667				MAR 27	2020	14	4					794	805		10.1049/iet-ipr.2019.0921													
J								Self-building technologies	AI & SOCIETY										Selfhood; Technology; Introspection; Self-representation; Artificial intelligence; Cognitive enhancement; Extended mind		On the basis of two thought experiments, I argue that self-building technologies are possible given our current level of technological progress. We could already use technology to make us instantiate selfhood in a more perfect, complete manner. I then examine possible extensions of this thesis, regarding more radical self-building technologies which might become available in a distant future. I also discuss objections and reservations one might have about this view.																	0951-5666	1435-5655															10.1007/s00146-020-00962-8		MAR 2020											
J								A novel classification method based on ICGOA-KELM for fault diagnosis of rolling bearing	APPLIED INTELLIGENCE										Rolling bearing; Fault diagnosis; ICGOA; KELM; VMD; MPE	GRASSHOPPER OPTIMIZATION ALGORITHM; MULTISCALE PERMUTATION ENTROPY; EXTREME LEARNING-MACHINE; EMPIRICAL MODE DECOMPOSITION; APPROXIMATE ENTROPY; SIGNALS; SCHEME	A novel classification method based on ICGOA-KELM is presented in this paper. In ICGOA-KELM, an improved circle chaotic map with grasshopper optimization algorithm (ICGOA) is designed to optimize the parameters of Kernel extreme learning machine (KELM) to improve the stability and accuracy of fault classification for rolling bearing based on parameter modification of circle chaotic map. Grasshopper optimization algorithm (GOA) is a new heuristic optimization algorithm, which has strong global searching ability. However, it still may fall into local optimization in some cases. In this paper, the vibration signals of rolling bearing are preprocessed by using Variational Modal Decomposition (VMD). Then Multi-scale Permutation Entropy (MPE) is utilized to extracted features of intrinsic mode functions (IMFs) decomposed by VMD. In addition, KPCA is adopted to select the salient features with high contribution rates to remove redundant and irrelevant features. Finally, the salient features are fed into ICGOA-KELM to fulfill fault classification. Therefore, a new fault detection and classification method based on VMD, MPE, KPCA and ICGOA-KELM is proposed. This method is applied to the fault classification of rolling bearing and the identification of different damage fault degrees. Experiments verify that the proposed method is more effective than CGOA-KELM for fault diagnosis of rolling bearing.																	0924-669X	1573-7497				SEP	2020	50	9					2833	2847		10.1007/s10489-020-01684-6		MAR 2020											
J								Merging of appearance-based place knowledge among multiple robots	AUTONOMOUS ROBOTS										Place recognition; Multi-robot; Unsupervised learning	LARGE-SCALE; SLAM; MAPS	If robots can merge the appearance-based place knowledge of other robots with their own, they can relate to these places even if they have not previously visited them. We have investigated this problem using robots with compatible visual sensing capabilities and with each robot having its individual long-term place memory. Here, each place refers to a spatial region as defined by a collection of appearances and in the place memory, the knowledge is organized in a tree hierarchy. In the proposed merging approach, the hierarchical organization plays a key role-as it corresponds to a nested sequence of hyperspheres in the appearance space. The merging proceeds by considering the extent of overlap of the respective nested hyperspheres-starting with the largest covering hypersphere. Thus, differing from related work, knowledge is merged in as large chunks as possible while the hierarchical structure is preserved accordingly. As such, the merging scales better as the extent of knowledge to be merged increases. This is demonstrated in an extensive set of multirobot experiments where robots share their knowledge and then use their merged knowledge when visiting these places.																	0929-5593	1573-7527				JUL	2020	44	6					1009	1027		10.1007/s10514-020-09911-2		MAR 2020											
J								An approach for optimal-secure multi-path routing and intrusion detection in MANET	EVOLUTIONARY INTELLIGENCE										Mobile ad-hoc network; Multipath routing; Optimization; Naive Bayes; Intrusion detection; Bird swarm optimization; Whale optimization algorithm	PROTOCOL; CRYPTOGRAPHY	Mobile ad-hoc network (MANET) is dynamic in nature that is susceptible to energy and security constraints. Among most of the existing techniques, energy optimization was a hectic challenge, which is addressed effectively using the routing protocols. Accordingly, this paper proposes an effective multipath routing protocol in MANET based on an optimization algorithm. The energy and the security crisis in the MANET are addressed effectively using the cluster head (CH) selection and intrusion detection strategies namely, fuzzy clustering and fuzzy Naive Bayes (fuzzy NB). Then, the multipath routing progresses using the secure nodes based on the routing protocol, Bird swarm-whale optimization algorithm (BSWOA), which is the integration of bird swarm optimization (BSA) in whale optimization algorithm (WOA). The selection of the optimal routes is based on fitness factors, such as connectivity, energy, trust, and throughput. The analysis of the methods is done using the attacks, such as flooding, blackhole, and selective packet drop based on the performance metrics. The proposed BSWOA acquired the maximal energy, throughput, detection rate, and a minimal delay of 9.48 Joule, 0.676 bps, 69.9%, and 0.00372 ms in the presence of the attack.																	1864-5909	1864-5917															10.1007/s12065-020-00388-7		MAR 2020											
J								Query expansion for patent retrieval using a modified stellar-mass black hole optimization	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Intelligence query expansion; Black hole; Text retrieval; Patent retrieval; Information retrieval	INFORMATION-RETRIEVAL; RELEVANCE FEEDBACK	Query expansion is an approach, which plays a critical role on the web-based search and retrieval methods. It's primary objective is to expedite the performance of the retrieval process in retrieving information by merging alternate word forms in the original query. The related word forms are retrievable from the document available in databases, which stores documents with a high density. A typical method for query expansion involves clustering of related documents and extracting closely related expressions using these clusters. However, traditional clustering poses a few problems to query expansion. Biclustering appears to be more effective in query expansion than traditional clustering. Recently a nature-inspired algorithm, namely the stellar mass black hole optimization has been introduced, to carry out the process of biclustering. Modification of the SBO has been performed to propose MSBO. Preliminary results have shown that the proposed modified stellar mass black hole (MSBO) method effectively clusters identical expressions and consequently the detected biclustering algorithms were effective in generating suitable words for expanding a given query. The MSBO has been tested on particular sets of documents with the objective of identifying groups of relevant categories of text that form the biclusters. Thus the proposed MSBO proves to be an effective query expansion tool. Based on the analysis of the obtained results, and on the comparisons performed with the existing methods, it has been found that MSBO is effective in generating results that are encouraging.																	1868-5137	1868-5145															10.1007/s12652-020-01894-3		MAR 2020											
J								Monitoring the quality of water in shrimp ponds and forecasting of dissolved oxygen using Fuzzy C means clustering based radial basis function neural networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING											PREDICTION; MODEL	The success of shrimp farming lies in the proper monitoring of water quality in the pond. Of all the water quality parameters the dissolved oxygen plays a vital role in the proper growth of shrimps and influences mortality to a great extent. Traditional methods of sensing the water quality especially dissolved oxygen is done in laboratories far away from the actual pond site and it is often time consuming. Hence an accurate forecasting model which predicts the changes in the DO content is quintessential to decrease mortality, yield export quality shrimps and reduce operational costs. Water quality datasets are collected by using devices fabricated with wireless water quality sensors which floats in the ponds. Fuzzy C means was chosen as the clustering method based on the dataset collected and radial basis functions neural networks were constructed with accurate number of hidden neurons to predict the changes in the dissolved oxygen. Root Mean Square Error (RMSE), Mean Absolute Error (MAE), Correlation Coefficient (R) and Willmott index of agreement (WIA) were used as the evaluation parameters in comparison with the Multilayer Perceptron based Backpropagation Neural Network and Standard Radial Basis Function Neural Network for prediction of the DO content. The results show that the proposed method is effective with reduced errors and correlation coefficient close to unity. This accurate prediction of the dissolved oxygen helps the farmers to take corrective action when required and decrease the operational costs and produce export quality shrimps.																	1868-5137	1868-5145															10.1007/s12652-020-01900-8		MAR 2020											
J								Robust ambulance base allocation strategy with social media and traffic congestion information	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Ambulance base allocation strategy; Social media; Traffic congestion	COVERING LOCATION MODEL; SIMULATION-MODELS; RELOCATION	At present, traffic congestion has become a major problem in many metropolitan areas globally, affecting the economy and social conditions in these areas. Of special concern, the response time of ambulances has increased, causing the death or disability of patients during emergencies. Various ambulance allocation strategies have been developed to locate a base that can achieve coverage of the demand point within a prescribed time or distance frame. However, real-time ambulance deployment is required to determine the number of ambulance and their bases. In particular, the dynamic relocation of an ambulance base is complicated, and each relocation does not guarantee that the next period will change again, thus increasing the workloads of the ambulance crew and potentially reducing their capability of responding to an emergency call. Furthermore, these models only considered covering all demand points but lacked the ability to consider uncertain factors, such as traffic congestion, patient conditions, public events, and population movement. Therefore, this study focused on formulating a covering model based on traffic congestion from web-based services and social media analysis, using the Markov-chain traffic speed assignment to allocate ambulance bases and trade-off the number of ambulance facilities between the current period and next period while considering the number of ambulance vehicles. Moreover, the proposed model was demonstrated using a case study of Bangkok emergency medical services. According to the results, obtained through collecting data on social media and traffic speed, the average traveling time of an ambulance can be improved by more than 70% with the trade-off between different periods if several emergency calls are received.																	1868-5137	1868-5145															10.1007/s12652-020-01889-0		MAR 2020											
J								Two-stage parallel speed-scaling machine scheduling under time-of-use tariffs	JOURNAL OF INTELLIGENT MANUFACTURING										Two-stage parallel machine scheduling; Time-of-use tariffs; Speed-scaling; Tabu search-greedy insertion hybrid algorithm; Electricity costs	PERMUTATION FLOW-SHOP; SINGLE-MACHINE; ENERGY-CONSUMPTION; GENETIC ALGORITHM; DEMAND RESPONSE; ELECTRICITY; MINIMIZE	As one of the demand-side programs, time-of-use (TOU) tariffs brings opportunities of maintaining power grid stability for electricity providers and chances of energy conservation for manufacturers, but it also brings challenge for enterprises to optimize scheduling schemes. This paper studies a two-stage parallel machine scheduling problem under TOU to minimize total electricity costs. The two-stage parallel machine system is composed of identical parallel speed-scaling machines at stage 1 and unrelated parallel machines at stage 2. The key issues lie in assigning a group of jobs to a set of parallel machines at each stage and choosing the appropriate processing speed for all jobs at stage 1, and then determining the interval of processing time for jobs on each selected machine. To solve this problem, a new continuous-time mixed-integer linear programming model is formulated. According to the characteristics of this model, a tabu search-greedy insertion hybrid (TS-GIH) algorithm is designed, which realizes job-machine assignment based on load balancing principle, job insertion with greedy mechanism as well as movement and speed adjustment strategies to find more suitable positions for jobs. The effectiveness of the proposed TS-GIH is demonstrated by comparing with CLPEX and improved genetic algorithm (IGA) through real-life and randomly generated instances. The results show that TS-GIH can realize the trade-off between computation time and solution quality. Compared with CLPEX, the computation time of TS-GIH is significantly less, and the solution quality is much better than IGA.																	0956-5515	1572-8145															10.1007/s10845-020-01561-6		MAR 2020											
J								Pneumatically Actuated Self-Healing Bionic Crawling Soft Robot	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Crawling soft robot; Bionic earthworm; Self-healing; Pneumatically actuated; Polydimethylsiloxane	POLYMER; WIRES; SHAPE	The soft robot is composed almost entirely of flexible materials, making it highly flexible, complex environment adaptable and safe human-computer interaction. However, the soft robot is exposed to sharp objects during work, which is ease to cause damage and reduces the service life of the soft robot. In this study, we proposed a bionic earthworm crawling robot with self-healing ability, which is achieved by implanting self-healing silicone elastomer (PDMS-TFB) at key parts with Ecoflex00-30 silica gel as the main body. The PDMS-TFB (Polydimethylsiloxane- Triformylbenzene) elastomer has a high tensile property (maximum strain up to 2400%), which makes it possible to cope with the large deformations that occur during the movement of the soft robot. The soft robot uses the self-healing behavior of materials to make them recyclable. The self-repairing speed of the material can be accelerated by heating, and the performance of the soft robot after healing is almost completely recovered. Actuated by air, the earthworm robot will expand axially after being aerated (10 kPa). When inflate and deflate in designed sequence for each chamber, maximum crawling speed of the soft robot can reach 150 mm/min, meanwhile, it can stably crawl on smooth planes with different angles (maximum 45 degrees) owning to the suction cup structures. This novel strategy provides a solution to greatly improve the service life of soft robots, making them useful in wide applications in scientific research, disaster relief etc.																	0921-0296	1573-0409				NOV	2020	100	2					445	454		10.1007/s10846-020-01187-z		MAR 2020											
J								Simple supervised dissimilarity measure: Bolstering iForest-induced similarity with class information without learning	KNOWLEDGE AND INFORMATION SYSTEMS										Distance metric learning; Supervised dissimilarity measure; Data-dependent dissimilarity; Class entropy; Isolation forest		Existing distance metric learning methods require optimisation to learn a feature space to transform data-this makes them computationally expensive in large datasets. In classification tasks, they make use of class information to learn an appropriate feature space. In this paper, we present a simple supervised dissimilarity measure which does not require learning or optimisation. It uses class information to measure dissimilarity of two data instances in the input space directly. It is a supervised version of an existing data-dependent dissimilarity measure called me. Our empirical results in k-NN and LVQ classification tasks show that the proposed simple supervised dissimilarity measure generally produces predictive accuracy better than or at least as good as existing state-of-the-art supervised and unsupervised dissimilarity measures.																	0219-1377	0219-3116				AUG	2020	62	8					3203	3216		10.1007/s10115-020-01454-3		MAR 2020											
J								The Abstraction/Representation Account of Computation and Subjective Experience	MINDS AND MACHINES										Computationalism; Implementation; Mental representation; Mental modeling		I examine the abstraction/representation theory of computation put forward by Horsman et al., connecting it to the broader notion of modeling, and in particular, model-based explanation, as considered by Rosen. I argue that the 'representational entities' it depends on cannot themselves be computational, and that, in particular, their representational capacities cannot be realized by computational means, and must remain explanatorily opaque to them. I then propose that representation might be realized by subjective experience (qualia), through being the bearer of the structure of abstract objects that are represented.																	0924-6495	1572-8641				JUN	2020	30	2					259	299		10.1007/s11023-020-09522-x		MAR 2020											
J								Deep Convolutional Generalized Classifier Neural Network	NEURAL PROCESSING LETTERS										Generalized classifier neural network; Image classification; Deep convolutional neural network; Kernel-based deep structures		Up to date technological implementations of deep convolutional neural networks are at the forefront of many issues, such as autonomous device control, effective image and pattern recognition solutions. Deep neural networks generally utilize a hybrid topology of a feature extractor containing convolutional layers followed by a fully connected classifier network. The characteristic and quality of the produced features differ according to the deep learning structure. In order to get high performance, it is necessary to choose an effective topology. In this study, a novel topology based hybrid structure named as Deep Convolutional Generalized Classifier Neural Network and its learning algoritm are introduced. This novel structure allows the deep learning network to extract features with the desired characteristics. This ensures high performance classification, even for relatively small deep learning networks. This has led to many novelties such as principal feature analysis, better learning ability, one-pass learning for classifier part, new error computation and backpropagation approach for filter weights. Two experiment sets were performed to measure the performance of DC-GCNN. In the first experiment set, DC-GCNN was compared with clasical approach on 10 different datasets. DC-GCNN performed better up to 44.45% for precision, 39.69% for recall and 42.57% for F1-score. In the second experiment set, DC-GCNN's performance was compared with alternative methods on larger datasets. Proposed structure performed better than alternative deep learning based classifier structures on CIFAR-10 and MNIST datasets with 89.12% and 99.28% accuracy values.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2839	2854		10.1007/s11063-020-10233-8		MAR 2020											
J								Optimizing Deep Feedforward Neural Network Architecture: A Tabu Search Based Approach	NEURAL PROCESSING LETTERS										Tabu search (TS); Deep feedforward neural network (DFNN); Hidden layer; Hidden neurons; Optimization; Architecture	OPTIMIZATION METHODOLOGY; GENETIC ALGORITHM; PARAMETERS; WEIGHTS; DESIGN	The optimal architecture of a deep feedforward neural network (DFNN) is essential for its better accuracy and faster convergence. Also, the training of DFNN becomes tedious as the depth of the network increases. The DFNN can be tweaked using several parameters, such as the number of hidden layers, the number of hidden neurons at each hidden layer, and the number of connections between layers. The optimal architecture of DFNN is usually set using a trial-and-error process, which is an exponential combinatorial problem and a tedious task. To address this problem, we need an algorithm that can automatically design an optimal architecture with improved generalization ability. This work aims to propose a new methodology that can simultaneously optimize the number of hidden layers and their respective neurons for DFNN. This work combines the advantages of Tabu search and Gradient descent with a momentum backpropagation training algorithm. The proposed approach has been tested on four different classification benchmark datasets, which show better generalization ability of the optimized networks.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2855	2870		10.1007/s11063-020-10234-7		MAR 2020											
J								Minimal Solvers for Rectifying from Radially-Distorted Scales and Change of Scales	INTERNATIONAL JOURNAL OF COMPUTER VISION										Rectification; Radial lens distortion; Minimal solvers; Repeated patterns; Symmetry; Local features		This paper introduces the first minimal solvers that jointly estimate lens distortion and affine rectification from the image of rigidly-transformed coplanar features. The solvers work on scenes without straight lines and, in general, relax strong assumptions about scene content made by the state of the art. The proposed solvers use the affine invariant that coplanar repeats have the same scale in rectified space. The solvers are separated into two groups that differ by how the equal scale invariant of rectified space is used to place constraints on the lens undistortion and rectification parameters. We demonstrate a principled approach for generating stable minimal solvers by the Grobner basis method, which is accomplished by sampling feasible monomial bases to maximize numerical stability. Synthetic and real-image experiments confirm that the proposed solvers demonstrate superior robustness to noise compared to the state of the art. Accurate rectifications on imagery taken with narrow to fisheye field-of-view lenses demonstrate the wide applicability of the proposed method. The method is fully automatic.																	0920-5691	1573-1405				APR	2020	128	4			SI		950	968		10.1007/s11263-019-01216-x		MAR 2020											
J								Enhanced Balanced Min Cut	INTERNATIONAL JOURNAL OF COMPUTER VISION										Clustering; Spectral clustering; Normalized cut	ALGORITHM	Spectral clustering is a hot topic and many spectral clustering algorithms have been proposed. These algorithms usually solve the discrete cluster indicator matrix by relaxing the original problems, obtaining the continuous solution and finally obtaining a discrete solution that is close to the continuous solution. However, such methods often result in a non-optimal solution to the original problem since the different steps solve different problems. In this paper, we propose a novel spectral clustering method, named as Enhanced Balanced Min Cut (EBMC). In the new method, a new normalized cut model is proposed, in which a set of balance parameters are learned to capture the differences among different clusters. An iterative method with proved convergence is used to effectively solve the new model without eigendecomposition. Theoretical analysis reveals the connection between EBMC and the classical normalized cut. Extensive experimental results show the effectiveness and efficiency of our approach in comparison with the state-of-the-art methods.																	0920-5691	1573-1405				JUL	2020	128	7					1982	1995		10.1007/s11263-020-01320-3		MAR 2020											
J								Application of fuzzy logic-based MPPT technique for harvesting the heat energy dissipated by the wind generator stator windings to power single-phase AC grid systems	NEURAL COMPUTING & APPLICATIONS										Energy harvesting system; Thermoelectric generator; DC-DC boost converter; Fuzzy logic-based MPPT technique; PI controller; Inverter; Single-phase AC grid	THERMOELECTRIC GENERATOR; PERFORMANCE; OPTIMIZATION; ELECTRICITY; CONTROLLER; DESIGN	The proposed research work investigates a thermoelectric energy harvester system for generating electricity from waste heat dissipated through the wind generator stator windings to feed a single-phase AC grid. Since the wind velocity is changing at every instant, a dynamic analysis is carried out under varying temperature conditions. A single thermoelectric module (TEM) can generate low power in the range of a few watts. To increase the power, the TEMs can be connected in square series-parallel configuration, as it has the benefit of non-varying internal resistance value. To operate the TEMs at maximum power under varying temperature conditions, the maximum power point tracking (MPPT) needs to be carried out to match the internal resistance of the TEM array with the load resistance. A fuzzy logic-based MPPT technique is employed during this work, in view that it is adaptive, robust and respond rapidly under varying temperature conditions. The change in accuracy of fuzzy logic-based MPPT controller in terms of maximum power point is found to be 95.18% to 99.24%. To feed the generated power to the single-phase AC grid, a DC-DC boost converter with controller and inverter is essential. A proportional integral (PI) controller is simple to implement and can provide an inverter with a constant DC voltage. A sine pulse width modulation (SPWM) inverter is employed in this work that has the capability of producing an appropriate voltage and frequency for interconnecting the system to AC grid. The entire system was developed and analysed using MATLAB-Simulink.																	0941-0643	1433-3058				SEP	2020	32	18			SI		15155	15170		10.1007/s00521-020-04865-z		MAR 2020											
J								Real-time camera pose estimation for sports fields	MACHINE VISION AND APPLICATIONS										Camera Pose Estimation; Camera Registration; Keypoints detection; Augmented Reality; Sport		Given an image sequence featuring a portion of a sports field filmed by a moving and uncalibrated camera, such as the one of the smartphones, our goal is to compute automatically in real time the focal length and extrinsic camera parameters for each image in the sequence without using a priori knowledges of the position and orientation of the camera.To this end, we propose a novel framework that combines accurate localization and robust identification of specific keypoints in the image by using a fully convolutional deep architecture.Our algorithm exploits both the field lines and the players' image locations, assuming their ground plane positions to be given, to achieve accuracy and robustness that is beyond the current state of the art.We will demonstrate its effectiveness on challenging soccer, basketball, and volleyball benchmark datasets.																	0932-8092	1432-1769				MAR 25	2020	31	3							16	10.1007/s00138-020-01064-7													
J								Object detection based on semi-supervised domain adaptation for imbalanced domain resources	MACHINE VISION AND APPLICATIONS										Semi-supervised learning; Object detection; Cross-domain adaptation; Generative adversarial network; Self-placed learning		On specified scenarios, models trained on specific datasets (source domain) can generalize well to novel scenes (target domain) via knowledge transfer. However, these source detectors might not be perfectly aligned with a low target resource due to the imbalanced and inconsistent domain shift involved. In this paper, we propose a semi-supervised detector that adapts the domain shifts on both appearance and semantic levels. Based on this, two components are introduced as appearance adaptation networks with instance and batch normalization, and semantic adaptation networks where an adversarial transferring procedure is embedded by re-weighting the discriminator loss to improve the feature alignments between the two domains with imbalanced scales. Furthermore, a self-paced training procedure is performed to re-train the detector by alternately generating pseudo-labels in the target domain from easy to hard. In our experiments, an empirical analysis of the proposed framework is conducted by evaluating performance in various datasets such as Cityscapes and VOC0712, and the results verify the higher accuracy and effectiveness of the proposed detector in comparison with state-of-the-art detectors.																	0932-8092	1432-1769				MAR 25	2020	31	3							18	10.1007/s00138-020-01068-3													
J								ColpoNet for automated cervical cancer screening using colposcopy images	MACHINE VISION AND APPLICATIONS										Cervical cancer; Colposcopy; Deep learning; Deep convolutional neural network (CNN)	COMPUTER-AIDED-DIAGNOSIS; LESIONS; CLASSIFICATION	Cervical cancer is one among the trivial forms of cancer that counts for 6.6% of all females cancers with an estimated 570,000 new cases in 2018. The mortality rate due to cervical cancer is approximately 90% in low or middle income countries due to lack of suitable pre-screening procedures and experienced medical staff. Colposcopy images or cervigrams, are the images that capture the cervical region, are considered as the gold standard by the medical experts for the identification and evaluation of cervical cancer. The visual assessment of cervigrams for recognizing cancer suffers from high inter- or intra-variations especially among less or unskilled medical experts. However, this method is dependent on colposcopists' observation and it is more time consuming, tedious and laborious task which calls for development of computer-aided method for diagnosis of cervical cancer. With the technological advancements, deep learning has been commonly employed for providing automated solutions for disease diagnosis due to its self-learning capability. This paper presents a deep-learning-based method for cervix cancer classification using colposcopy images. The architecture of the proposed method namely, ColpoNet, has been motivated by the DenseNet model because it is computationally more efficient as compared to other models. Further, the method has been tested and validated on the dataset released by the National Cancer Institute and it has been compared with other deep-learning models namely AlexNet, VGG16, ResNet50, LeNet and GoogleNet to check scope of its applicability. The experimental analysis revealed that ColpoNet achieved an accuracy of 81.353% and shows the highest performance rate as compared to other state-of-the-art deep techniques. Such classification system can be deployed in clinics to enhance the early detection of cervical cancer in less developed countries.																	0932-8092	1432-1769				MAR 25	2020	31	3							15	10.1007/s00138-020-01063-8													
J								User-interactive salient object detection using YOLOv2, lazy snapping, and gabor filters	MACHINE VISION AND APPLICATIONS										Gabor filters; Lazy snapping; Salient object; YOLOv2	CONTRAST	Salient object detection is the process of locating prominent objects in an image. In this field, deep learning methods are providing outstanding results. One way of finding salient objects is to first obtain a bounding box for the prominent object in the image and then use the bounding box to form the actual shape of the salient object. In this work, we find an object bounding box using YOLOv2 network. Next, we apply boundary correction to the bounding box predicted by the deep network. In the third step, we segment the image using a set of Gabor filters. Then, we select the matching segment from the first-level boundary correction. On the matching segment, we apply second-level boundary correction. Usually, in salient object detection, the end-user plays no role in selecting the salient object. In this work, we provide the user with a choice to improvise on the salient object detected at the first level. If the user is not satisfied with first-level boundary correction, he/she can choose for second-level boundary correction. The method provides a benefit over the existing methods as most of the saliency map results are static, and pure deep learning methods have blurred edges. By using this procedure, neat object edges are obtained. The algorithm is tested on three datasets against four state-of-the-art methods. The algorithm is evaluated based on F-measure. The proposed model achieves 0.86, 0.7904, and 0.745 F-measure for ASD, ECSSD, and PASCAL-S dataset, respectively.																	0932-8092	1432-1769				MAR 25	2020	31	3							17	10.1007/s00138-020-01065-6													
J								Comprehensive learning cuckoo search with chaos-lambda method for solving economic dispatch problems	APPLIED INTELLIGENCE										Economic dispatch; Swarm intelligence; Cuckoo search; Power systems	PARTICLE SWARM OPTIMIZATION; FIREFLY ALGORITHM; HARMONY SEARCH; OPERATION	Economic dispatch (ED) is an important part in the economic operation of power systems. It is an NP-hard problem with multiple practical constraints. This paper proposes a novel approach that combines a swarm intelligence algorithm with a constraint-handling mechanism to solve the ED problem. First, we design a comprehensive learning cuckoo search algorithm with two strengthen strategies. A comprehensive learning strategy is designed to give the algorithm advanced learning ability in high-dimensional and multi-modal environment and thus enhance the search ability. A duplicate elimination strategy is utilized as an elite strategy to improve the evolving efficiency of the algorithm. Then, we propose a constraint-based population generation method named chaos-lambda method to reduce the searching complexity, and a solution repair method to repair unfeasible solutions that violate the constraints. The proposed approach is tested on 5 systems with different benchmarks and compared with the state-of-the-art algorithms. Our approach achieves the best performance on every test.																	0924-669X	1573-7497				SEP	2020	50	9					2779	2799		10.1007/s10489-020-01654-y		MAR 2020											
J								Path planning of UAV for oilfield inspections in a three-dimensional dynamic environment with moving obstacles based on an improved pigeon-inspired optimization algorithm	APPLIED INTELLIGENCE										Oilfield inspection; Path planning; UAV; Pigeon-inspired optimization; Fruit fly optimization algorithm	PARTICLE SWARM OPTIMIZATION; DIFFERENTIAL EVOLUTION; SEARCH	In recent years, uninhabited aerial vehicles (UAV) have been used for oilfield inspections in many enterprises which can realize oilfield inspections by fewer workers. Path planning is one indispensable element in oilfield inspections through UAVs and it is also a complicated optimal problem. Now, although many researches have been focused on it, they are mainly discussed based on two-dimension planes. In practices, oilfields are complex three-dimensional spaces with many targeted points and moving obstacles between the starting and the ending point, which bring current methods some difficulties. In order to solve this problem, a three-dimensional environment model for oilfields is established for the first time, which includes: a static oil-well equipment, moving obstacles, and so on. Then, a cost function is defined to evaluate the best path, which includes: total length, average height, total time, and total electricity consumption. Finally, an improved pigeon-inspired optimization algorithm is proposed to solve problems about path planning in a three-dimensional dynamic environment of oilfields, which is named PIOFOA. In the PIOFOA, a pigeon-inspired optimization (PIO) algorithm is used to optimize the initial path and a fruit fly optimization algorithm (FOA) is used to continue local optimizations, so as to search the best path after movements of obstacles. Compared with some other methods, simulation results show that the proposed PIOFOA method is more effective.																	0924-669X	1573-7497				SEP	2020	50	9					2800	2817		10.1007/s10489-020-01650-2		MAR 2020											
J								A hybrid model of convolutional neural networks and deep regression forests for crowd counting	APPLIED INTELLIGENCE										Crowd counting; Label distribution learning; MCNN; Deep decision forest	IMAGE	Real-time monitoring variation of crowd via video surveillance plays a significant role in the new generation of technology in a smart city. We propose a crowd counting algorithm based on deep regression forest, named CountForest. First of all, according to the correlation among frames, the crowd counting problem is transformed into a label-distribution-learning problem. Then we combine convolutional neural networks(CNN) and deep regression forest to make a hybrid model. CNN is introduced for the task of feature learning and deep decision forest is extended to address label distribution learning problem in crowd counting. Thereinto, the proposed network replaces its softmax layer with the aforementioned probabilistic decision forest in order to better establish a mapping relationship between image features and crowds' number so as to implement an end-to-end hybrid model for crowd counting problem. Our method demonstrated in the final experiments not only attains the high accuracy in crowd counting but has comparable robustness and instantaneity in selected public datasets as well.																	0924-669X	1573-7497				SEP	2020	50	9					2818	2832		10.1007/s10489-020-01688-2		MAR 2020											
J								Formation control of unmanned micro aerial vehicles for straitened environments	AUTONOMOUS ROBOTS										Micro aerial vehicles; Motion planning; Virtual leader migration; Formation control	MODEL-PREDICTIVE CONTROL; NAVIGATION; FLIGHT; QUADROTOR; POSITION; DESIGN	This paper presents a novel approach for control and motion planning of formations of multiple unmanned micro aerial vehicles (multi-rotor helicopters, in the literature also often called unmanned aerial vehicles-UAVs or unmanned aerial system-UAS) in cluttered GPS-denied on straitened environments. The proposed method enables us to autonomously design complex maneuvers of a compact Micro Aerial Vehicles (MAV) team in a virtual-leader-follower scheme. The results of the motion planning approach and the required stability of the formation are achieved by migrating the virtual leader along with the hull surrounding the formation. This enables us to suddenly change the formation motion in all directions, independently from the current orientation of the formation, and therefore to fully exploit the maneuverability of small multi-rotor helicopters. The proposed method was verified and its performance has been statistically evaluated in numerous simulations and experiments with a fleet of MAVs.																	0929-5593	1573-7527				JUL	2020	44	6					991	1008		10.1007/s10514-020-09913-0		MAR 2020											
J								Design of an effectual node balancing cluster with partitioner algorithm using Markov decision process	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Controller placement; SD-WAN; Latency; Multi controller; Network partitioner; Energy efficiency	CONTROLLER PLACEMENT PROBLEM; SOFTWARE; MODEL	In recent times, the foremost challenging task in implementing software defined networking is to choose appropriate locations for controllers to reduce latency amongst switches and controllers in wide area networks. Based on the existing investigations, most of the studies concentrate on reduction of packet propagation latency, however propagation latency is an essential contributor of overall latency amongst switches and associated controllers. In this research work, investigation is carried out on most significant contributor of latency, such as end-to-end latency and queuing based latency of controllers. So as to reduce end-to-end delay, the network partitioning idea is anticipated and an efficient node balancing cluster with partitioner algorithm using Markov decision process (NB-PAMDP) is modeled for network partitioning. NB-PAMDP can ensure that every partition is competent to reduce maximum amount of end-to-end delay amongst switches and controllers. In order to diminish queuing delay of controllers, an appropriate multiple controller placement are then designed for sub networks. Extensive simulation was carried out in NS-3 simulator under a network topology from Internet topology. Results demonstrate that the anticipated model can resourcefully diminish maximum latency or delay amongst switches and corresponding controllers.																	1868-5137	1868-5145															10.1007/s12652-020-01891-6		MAR 2020											
J								The impact of virtual reality technology on tourists' experience: a textual data analysis	SOFT COMPUTING										Virtual reality (VR); Immersive virtual reality (IVR); Past View; Tourist experience quality; Correspondence analysis	BEHAVIORAL INTENTIONS; VISITORS EXPERIENCE; DESTINATION IMAGE; CREATIVE TOURISM; PERCEIVED VALUE; MANAGEMENT; SATISFACTION; INFORMATION; QUALITY; DESIGN	This paper is focused on the study of the quality of experience lived by tourists when visiting a cultural heritage destination by using a tourist product based on a virtual tour. The research is justified by the increased demand by tourists to have a memorable experience in a destination as well as the growing offer on virtual reality and augmented reality technologies applied to the tourism sector. The database consists of online comments extracted from those who visited two well-known tourism destinations in Spain, Seville and Barcelona, where the immersive virtual reality technology named Past View is currently used. A total of 119 online comments on the tourists' experience after the use of the Past View smart glasses and posted in the e-WOM community Trip Advisor were valid for the analysis. Using a correspondence analysis of textual data, the results shed light about how virtual reality technologies influence on tourists' quality of experience. The findings drawn from the empirical analysis provide destination marketing organizations suitable and useful information to promote the destination and therefore encouraging entrepreneurs to innovate in tourism sector to attend the desire of tourists to have a memorable tourist experience.																	1432-7643	1433-7479				SEP	2020	24	18			SI		13879	13892		10.1007/s00500-020-04883-y		MAR 2020											
J								Learning multi-agent communication with double attentional deep reinforcement learning	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Learning to communicate; Multi-agent reinforcement learning; Attentional deep reinforcement learning; Large-scale communication	DECENTRALIZED CONTROL; COMPLEXITY; SYSTEMS	Communication is a critical factor for the big multi-agent world to stay organized and productive. Recently, Deep Reinforcement Learning (DRL) has been adopted to learn the communication among multiple intelligent agents. However, in terms of the DRL setting, the increasing number of communication messages introduces two problems: (1) there are usually some redundant messages; (2) even in the case that all messages are necessary, how to process a large number of messages in an efficient way remains a big challenge. In this paper, we propose a DRL method named Double Attentional Actor-Critic Message Processor (DAACMP) to jointly address these two problems. Specifically, DAACMP adopts two attention mechanisms. The first one is embedded in the actor part, such that it can select the important messages from all communication messages adaptively. The other one is embedded in the critic part so that all important messages can be processed efficiently. We evaluate DAACMP on three multi-agent tasks with seven different settings. Results show that DAACMP not only outperforms several state-of-the-art methods but also achieves better scalability in all tasks. Furthermore, we conduct experiments to reveal some insights about the proposed attention mechanisms and the learned policies.																	1387-2532	1573-7454				MAR 25	2020	34	1							32	10.1007/s10458-020-09455-w													
J								What do you really want to do? Towards a Theory of Intentions for Human-Robot Collaboration	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Knowledge representation and reasoning; Theory of intentions; Non-monotonic logical reasoning; Probabilistic reasoning; Human-robot collaboration	POLICIES	The architecture described in this paper encodes a theory of intentions based on the key principles of non-procrastination, persistence, and automatically limiting reasoning to relevant knowledge and observations. The architecture reasons with transition diagrams of any given domain at two different resolutions, with the fine-resolution description defined as a refinement of, and hence tightly-coupled to, a coarse-resolution description. For any given goal, nonmonotonic logical reasoning with the coarse-resolution description computes an activity, i.e., a plan, comprising a sequence of abstract actions to be executed to achieve the goal. Each abstract action is implemented as a sequence of concrete actions by automatically zooming to and reasoning with the part of the fine-resolution transition diagram relevant to the current coarse-resolution transition and the goal. Each concrete action in this sequence is executed using probabilistic models of the uncertainty in sensing and actuation, and the corresponding fine-resolution outcomes are used to infer coarse-resolution observations that are added to the coarse-resolution history. The architecture's capabilities are evaluated in the context of a simulated robot assisting humans in an office domain, on a physical robot (Baxter) manipulating tabletop objects, and on a wheeled robot (Turtlebot) moving objects to particular places or people. The experimental results indicate improvements in reliability and computational efficiency compared with an architecture that does not include the theory of intentions, and an architecture that does not include zooming for fine-resolution reasoning.																	1012-2443	1573-7470															10.1007/s10472-019-09672-4		MAR 2020											
J								Benchmarking state-of-the-art symbolic regression algorithms	GENETIC PROGRAMMING AND EVOLVABLE MACHINES										Symbolic regression; Genetic programming; Linear regression; Comparative study	REGULARIZATION; PERFORMANCE; MODELS	Symbolic regression (SR) is a powerful method for building predictive models from data without assuming any model structure. Traditionally, genetic programming (GP) was used as the SR engine. However, for these purely evolutionary methods it was quite hard to even accommodate the function to the range of the data and the training was consequently inefficient and slow. Recently, several SR algorithms emerged which employ multiple linear regression. This allows the algorithms to create models with relatively small error right from the beginning of the search. Such algorithms are claimed to be by orders of magnitude faster than SR algorithms based on classic GP. However, a systematic comparison of these algorithms on a common set of problems is still missing and there is no basis on which to decide which algorithm to use. In this paper we conceptually and experimentally compare several representatives of such algorithms: GPTIPS, FFX, and EFS. We also include GSGP-Red, which is an enhanced version of geometric semantic genetic programming, an important algorithm in the field of SR. They are applied as off-the-shelf, ready-to-use techniques, mostly using their default settings. The methods are compared on several synthetic SR benchmark problems as well as real-world ones ranging from civil engineering to aerodynamics and acoustics. Their performance is also related to the performance of three conventional machine learning algorithms: multiple regression, random forests and support vector regression. The results suggest that across all the problems, the algorithms have comparable performance. We provide basic recommendations to the user regarding the choice of the algorithm.																	1389-2576	1573-7632															10.1007/s10710-020-09387-0		MAR 2020											
J								A Survey of Deep Facial Attribute Analysis	INTERNATIONAL JOURNAL OF COMPUTER VISION										Deep neural networks; Deep facial attribute analysis; Facial attribute estimation; Facial attribute manipulation	NEURAL-NETWORK; FACE	Facial attribute analysis has received considerable attention when deep learning techniques made remarkable breakthroughs in this field over the past few years. Deep learning based facial attribute analysis consists of two basic sub-issues: facial attribute estimation (FAE), which recognizes whether facial attributes are present in given images, and facial attribute manipulation (FAM), which synthesizes or removes desired facial attributes. In this paper, we provide a comprehensive survey of deep facial attribute analysis from the perspectives of both estimation and manipulation. First, we summarize a general pipeline that deep facial attribute analysis follows, which comprises two stages: data preprocessing and model construction. Additionally, we introduce the underlying theories of this two-stage pipeline for both FAE and FAM. Second, the datasets and performance metrics commonly used in facial attribute analysis are presented. Third, we create a taxonomy of state-of-the-art methods and review deep FAE and FAM algorithms in detail. Furthermore, several additional facial attribute related issues are introduced, as well as relevant real-world applications. Finally, we discuss possible challenges and promising future research directions.																	0920-5691	1573-1405				SEP	2020	128	8-9			SI		2002	2034		10.1007/s11263-020-01308-z		MAR 2020											
J								Coronary Heart Disease Diagnosis Through Self-Organizing Map and Fuzzy Support Vector Machine with Incremental Updates	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Fuzzy support vector machine; Coronary heart disease; Self-organizing map; Incremental learning; Prediction accuracy	HYBRID INTELLIGENT SYSTEM; HOT DECK IMPUTATION; K-MEANS; PRINCIPAL COMPONENTS; NEURAL-NETWORKS; CANCER; SVM; CLASSIFICATION; PREDICTION; PROGRESSION	The trade-off between computation time and predictive accuracy is important in the design and implementation of clinical decision support systems. Machine learning techniques with incremental updates have proven its usefulness in analyzing large collection of medical datasets for diseases diagnosis. This research aims to develop a predictive method for heart disease diagnosis using machine learning techniques. To this end, the proposed method is developed by unsupervised and supervised learning techniques. In particular, this research relies on Principal Component Analysis (PCA), Self-Organizing Map, Fuzzy Support Vector Machine (Fuzzy SVM), and two imputation techniques for missing value imputation. Furthermore, we apply the incremental PCA and FSVM for incremental learning of the data to reduce the computation time of disease prediction. Our data analysis on two real-world datasets, Cleveland and Statlog, showed that the use of incremental Fuzzy SVM can significantly improve the accuracy of heart disease classification. The experimental results further revealed that the method is effective in reducing the computation time of disease diagnosis in relation to the non-incremental learning technique.																	1562-2479	2199-3211				JUN	2020	22	4					1376	1388		10.1007/s40815-020-00828-7		MAR 2020											
J								Study of correlation between the steels susceptibility to hydrogen embrittlement and hydrogen thermal desorption spectroscopy using artificial neural network	NEURAL COMPUTING & APPLICATIONS										Hydrogen embrittlement; Artificial neural network; Thermal desorption spectroscopy; Hydrogen sensitivity parameter	MECHANICAL-PROPERTIES; RETAINED AUSTENITE; BEHAVIOR; FRACTURE	Steels are the most used structural material in the world, and hydrogen content and localization within the microstructure play an important role in its properties, namely inducing some level of embrittlement. The characterization of the steels susceptibility to hydrogen embrittlement (HE) is a complex task requiring always a broad and multidisciplinary approach. The target of the present work is to introduce the artificial neural network (ANN) computing system to predict the hydrogen-induced mechanical properties degradation using the hydrogen thermal desorption spectroscopy (TDS) data of the studied steel. Hydrogen sensitivity parameter (HSP) calculated from the reduction of elongation to fracture caused by hydrogen was linked to the corresponding hydrogen thermal desorption spectra measured for austenitic, ferritic, and ferritic-martensitic steel grades. Correlation between the TDS input data and HSP output data was studied using two ANN models. A correlation of 98% was obtained between the experimentally measured HSP values and HSP values predicted using the developed densely connected layers ANN model. The performance of the developed ANN models is good even for never-before-seen steels. The ANN-coupled system based on the TDS is a powerful tool in steels characterization especially in the analysis of the steels susceptibility to HE.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14995	15006		10.1007/s00521-020-04853-3		MAR 2020											
J								Transforming view of medical images using deep learning	NEURAL COMPUTING & APPLICATIONS										Deep learning; Conditional generative adversarial network; CT scan; MRI; 2-Dimensional; 3-Dimensional	X-RAY; RECONSTRUCTION; FEMUR	Since the last decade, there is a significant change in the procedure of medical diagnosis and treatment. Specifically, when internal tissues, organs such as heart, lungs, brain, kidneys and bones are the target regions, a doctor recommends 'computerized tomography' scan and/or magnetic resonance imaging to get a clear picture of the damaged portion of an organ or a bone. This is important for correct examination of the medical deformities such as bone fracture, arthritis, and brain tumor. It ensures prescription of the best possible treatment. But 'computerized tomography' scan exposes a patient to high ionizing radiation. These rays make a person more prone to cancer. Magnetic resonance imaging requires a strong magnetic field. Thus, it becomes impractical for patients with implants in their body. Moreover, the high cost makes the above-stated techniques unaffordable for low economy class of society. The above-mentioned challenges of 'computerized tomography' scan and magnetic resonance imaging motivate researchers to focus on developing a technique for conversion of 2-dimensional view of medical images into their corresponding multiple views. In this manuscript, the authors design and develop a deep learning model that makes an effective use of conditional generative adversarial network, an extension of generative adversarial network for the transformation of 2-dimensional views of human bone into the corresponding multiple views at different angles. The model will prove useful for both doctors and patients.																	0941-0643	1433-3058				SEP	2020	32	18			SI		15043	15054		10.1007/s00521-020-04857-z		MAR 2020											
J								A multi-stage learning-based fuzzy cognitive maps for tobacco use	NEURAL COMPUTING & APPLICATIONS										Fuzzy cognitive map; Nonlinear Hebbian learning algorithm; Extended Great Deluge algorithm	ALGORITHM; SYSTEM; MANAGEMENT	Fuzzy cognitive map (FCM) is an important approach for modeling the behavior of dynamic systems. FCM's ability to represent casual relationships between the concepts (factors, attributes, etc.) has attracted the interest of researchers from different disciplines. The construction process of FCMs is mostly initialized with expert knowledge because FCMs can conveniently incorporate available information and expertise in the determination of vital parameters and relations of the system. However, their higher dependence on expert knowledge may significantly influence the reliability of the model due to the increase in subjectivity. In order to avoid weaknesses depending on expert knowledge, learning algorithms that search for the appropriate relationships between the concepts have been used with FCM studies. In this paper, a FCM analysis was performed for tobacco use to understand the cause-effect relationships between demographic characteristics of people (such as gender, age range, and residence type) and likelihood to tobacco use. In order to reduce the impact of external interventions (from experts), a multi-stage learning procedure was applied by integrating two different learning algorithms (nonlinear Hebbian learning algorithm and extended Great Deluge algorithm). The results showed that the multi-stage learning procedure increased the accuracy of the model and provided more reliable maps for the studied system. They also proved that the multi-stage learning procedures can help to reduce the dependency to expert knowledge and improve the robustness of the study.																	0941-0643	1433-3058				SEP	2020	32	18			SI		15101	15118		10.1007/s00521-020-04860-4		MAR 2020											
J								Covariances with OWA operators and Bonferroni means	SOFT COMPUTING										Variance; Covariance; Bonferroni means; OWA operator	ORDERED WEIGHTED AVERAGE; ATTRIBUTE DECISION-MAKING; AGGREGATION OPERATORS; DISTANCE MEASURES; MOVING AVERAGES; TIME-SERIES; FUZZY; VARIANCE; DYNAMICS	The covariance is a statistical technique that is widely used to measure the dispersion between two sets of elements. This work develops new covariance measures by using the ordered weighted average (OWA) operator and Bonferroni means. Thus, this work presents the Bonferroni covariance OWA operator. The main advantage of this approach is that the decision maker can underestimate or overestimate the covariance according to his or her attitudes. The article further generalizes this formulation by using generalized and quasi-arithmetic means to obtain a wide range of particular types of covariances, including the quadratic Bonferroni covariance and the cubic Bonferroni covariance. The paper also considers some other extensions by using induced aggregation operators in order to use complex reordering processes in the analysis. The work ends by studying the applicability of these new techniques to real-world problems and presents an illustrative example of a research and development (R&D) investment problem.																	1432-7643	1433-7479				OCT	2020	24	19					14999	15014		10.1007/s00500-020-04852-5		MAR 2020											
J								Law and software agents: Are they "Agents" by the way?	ARTIFICIAL INTELLIGENCE AND LAW										Software agent; Legal personality; Agency; Liability	LEGAL PERSONALITY	Using intelligent software agents in the world of e-commerce may give rise to many difficulties especially with regard to the validity of agent-based contracts and the attribution of liability for the actions of such agents. This paper thus critically examines the main approaches that have been advanced to deal with software agents, and proposes the gradual approach as a way of overcoming the difficulties of such agents by adopting different standards of responsibility depending whether the action is done autonomously by an unattended software, or whether it is done automatically by an attended software. Throughout this paper, it is argued that the introduction of "one size" regulation without sufficient consideration of the nature of software agents or the environments in which they communicate might lead to a divorce between the legal theory and technological practice. It is also concluded that it is incorrect to deal with software agents as if they were either legal persons or nothing without in any way accounting for the fact that there are various kinds of such agents endowed with different levels of autonomy, mobility, intelligence, and sophistication. However, this paper is not intended to provide the final answer to all problematic questions posed by the emergence of intelligent software agents, but is designed to provide some kind of temporary relief until such agents reach a more reliable and autonomous level whereby law begins to regard them, rather than their users, as the source of the relevant action.																	0924-8463	1572-8382															10.1007/s10506-020-09265-1		MAR 2020											
J								A Deep Multi-task Model for Dialogue Act Classification, Intent Detection and Slot Filling	COGNITIVE COMPUTATION										Multi-tasking; Dialogue act classification; Intent detection; Slot filling	RECURRENT NEURAL-NETWORKS; RECOGNITION; SEQUENCE; MACHINE	An essential component of any dialogue system is understanding the language which is known as spoken language understanding (SLU). Dialogue act classification (DAC), intent detection (ID) and slot filling (SF) are significant aspects of every dialogue system. In this paper, we propose a deep learning-based multi-task model that can perform DAC, ID and SF tasks together. We use a deep bi-directional recurrent neural network (RNN) with long short-term memory (LSTM) and gated recurrent unit (GRU) as the frameworks in our multi-task model. We use attention on the LSTM/GRU output for DAC and ID. The attention outputs are fed to individual task-specific dense layers for DAC and ID. The output of LSTM/GRU is fed to softmax layer for slot filling as well. Experiments on three datasets, i.e. ATIS, TRAINS and FRAMES, show that our proposed multi-task model performs better than the individual models as well as all the pipeline models. The experimental results prove that our attention-based multi-task model outperforms the state-of-the-art approaches for the SLU tasks. For DAC, in relation to the individual model, we achieve an improvement of more than 2% for all the datasets. Similarly, for ID, we get an improvement of 1% on the ATIS dataset, while for TRAINS and FRAMES dataset, there is a significant improvement of more than 3% compared to individual models. We also get a 0.8% enhancement for ATIS and a 4% enhancement for TRAINS and FRAMES dataset for SF with respect to individual models. Results obtained clearly show that our approach is better than existing methods. The validation of the obtained results is also demonstrated using statistical significance t tests.																	1866-9956	1866-9964															10.1007/s12559-020-09718-4		MAR 2020											
J								Optimized support vector neural network and contourlet transform for image steganography	EVOLUTIONARY INTELLIGENCE										Image steganography; Optimization; Embedding strength; Pixel-based prediction; Contourlet	STEGANALYSIS; WAVELET	Image steganography is one of the promising and popular techniques used to secure the sensitive information. Even though there are numerous steganography techniques for hiding the sensitive information, there are still a lot of challenges to the researchers regarding the effective hiding of the sensitive data. Thus, an effective pixel prediction-based image steganography method is proposed, which uses the error dependent SVNN classifier for effective pixel identification. The suitable pixels are effectively identified from the medical image using the SVNN classifier using the pixel features, such as edge information, pixel coverage, texture, wavelet energy, Gabor, and scattering features. Here, the SVNN is trained optimally using the GA or MS Algorithm based on the minimal error. Then, the CT is applied to the predicted pixel for embedding. Finally, the inverse CT is employed to extract the secret message from the embedded image. The experimentation of the proposed image steganography is performed using the BRATS database depending on the performance metrics, PSNR, SSIM, and correlation coefficient, which acquired 89.3253 dB, 1, and 1, for the image without noise and 48.5778 dB, 0.6123, and 0.9933, for the image affected by noise, respectively.																	1864-5909	1864-5917															10.1007/s12065-020-00387-8		MAR 2020											
J								Quadratic Form Optimization with Fuzzy Number Parameters: Multiobjective Approaches	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Optimization; Quadratic; Fuzzy program; Triangular fuzzy numbers; Karush-Kuhn-Tucker conditions; Multiobjective optimization problem; Sum of Objective Method		Problems in daily life can be modeled into mathematical forms, one of them is the optimization of the quadratic form where the objective functions are the quadratic function. There are many real problems involving variables that cannot be stated numerically so that fuzzy logic appears. The purpose of this research is to optimize the quadratic form with all its parameters in the form of a fuzzy number by using the method of a triangular fuzzy number. This research resulted in a new completion method by utilizing definitions and arithmetic on the triangular fuzzy numbers. The resulted method is by changing a single fuzzy quadratic program becomes a simple quadratic multiobjective program. By using Sum of Objective Method, the multiobjective problem can be changed into single optimization problem. This single optimization problem is then completed using Karush-Kuhn-Tucker method, resulting in three optimal values which will form optimal values in the form of triangular fuzzy numbers.																	1562-2479	2199-3211				JUN	2020	22	4					1191	1197		10.1007/s40815-020-00808-x		MAR 2020											
J								Gradient boosting in crowd ensembles for Q-learning using weight sharing	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Reinforcement Learning; Gradient Boosting; Convolutional Neural Network; Deep Q-learning; Weight Sharing; Ensemble Learning		Reinforcement learning (RL) is a double-edged sword: it frees the human trainer from having to provide voluminous supervised training data or from even knowing a solution. On the other hand, a common complaint about RL is that learning is slow. Deep Q-learning (DQN), a somewhat recent development, has allowed practitioners and scientists to solve tasks previously thought unsolvable by a reinforcement learning approach. However DQN has resulted in an explosion in the number of model parameters which has further exasperated the computational needs of Q-learning during training. In this work, an ensemble approach which improves the training time, in terms of the number of interactions with the training environment, is proposed. In the presented experiments, it is shown that the proposed approach improves stability of during training, results in improved average performance, results in more reliable training, and faster learning of features in convolutional layers.																	1868-8071	1868-808X				OCT	2020	11	10					2275	2287		10.1007/s13042-020-01115-5		MAR 2020											
J								A fuzzy spatial description logic for the semantic web	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Decision procedure; Semantic Web; Description logics; Fuzzy spatial reasoning	REGION CONNECTION CALCULUS; CONCRETE DOMAINS; ALGORITHM	Spatial information is a critical feature in a large number of application domains. Spatial information, however, is often not crisp but with the nature of imprecision and fuzziness. As the increasing requirements of spatial applications, there emerges many challenges regarding to the representation and reasoning of spatial knowledge. Description logic (DL) is a logical basis for representing knowledge and realizing reasoning tasks in the Semantic Web. Therefore, how to extend DL to achieve the goal of representing and reasoning fuzzy spatial knowledge needs to be settled. In this work, we study a fuzzy spatial extension of the well known fuzzy ALC DL to reason fuzzy spatial knowledge. First, we construct a fuzzy spatial concrete domain S which is comprised of fuzzy spatial regions and fuzzy RCC relationships. More importantly, we give the admissibility proof of fuzzy spatial concrete domain Then we extend fuzzy ALC is decidable in PSPACE-complete.																	1868-5137	1868-5145															10.1007/s12652-020-01864-9		MAR 2020											
J								Tube-Based Robust MPC Processor-in-the-Loop Validation for Fixed-Wing UAVs	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Robust MPC; Path planning; Mini UAV	ADAPTIVE-CONTROL; DESIGN; VEHICLES	Real systems, as Unmanned Aerial Vehicles (UAVs), are usually subject to environmental disturbances, which could compromise the mission accomplishment. For this reason, the main idea proposed in this research is the design of a robust controller, as autopilot control system candidate for a fixed-wing UAV. In detail, the inner loop of the autopilot system is designed with a tube-based robust model predictive control (TRMPC) scheme, able to handle additive noise. Moreover, the navigation outer loop is regulated by a proportional-integral-derivative controller. The proposed TRMPC is composed of two parts: (i) a linear nominal dynamics, evaluated online with an optimization problem, and (ii) a linear error dynamics,which includes a feedback gain matrix, evaluated offline. The key aspects of the proposed methodology are: (i) offline evaluation of the feedback gain matrix, and (ii) robustness to random, bounded disturbances. Moreover, a path-following algorithm is designated for the guidance task, which provides the reference heading angle as input to the control algorithm. Software-in-the-loop and processor-in-the-loop simulations have been performed to validate the proposed approach. The obtained performance have been evaluated in terms of tracking capabilities and computational load, assessing the real-time implementability compliance with the XMOS development board, selected as continuation of previous works.																	0921-0296	1573-0409				OCT	2020	100	1					239	258		10.1007/s10846-020-01172-6		MAR 2020											
J								Multi-facility-based improved closed-loop supply chain network for handling uncertain demands	SOFT COMPUTING										End-of-life; End-of-use; Closed-loop supply chain; Metaheuristics	PRODUCT RECOVERY; DESIGN; MODEL; LOGISTICS; TRANSPORTATION; OPTIMIZATION; METAHEURISTICS; ALGORITHMS; MANAGEMENT; INVENTORY	Globalization has enlightened market with both opportunity and risk by bringing a more connected business environment, which ensures more customers and new markets. In contrast, it also brought a larger extent of competitors. The more collaborative environment can help companies to focus on their core competence to simultaneously reduce cost and participate more profitably in their trade. Uncertainty is a major outcome of the globalization process; firms are developing new methods and strategies to deal with risk and take control of uncertainty factors. This work introduces a novel approach that could help in collecting the end-of-life and end-of-use products from the end-users. These collected products enter the value chain and help in reducing the overall cost of the supply chain. A mixed-integer linear programming model has been formulated to assess the overall cost of the supply chain for the presented study. Due to the NP-hardness of the problem, few well-known metaheuristics and hybrid approaches are proposed as solution techniques for the first time. The Taguchi method is used to obtain the best combinations of algorithm parameters. In addition, problem instances are generated to validate the proposed model for a real-world case. Finally, the effectiveness of the algorithms is compared by using different criteria.																	1432-7643	1433-7479				MAY	2020	24	10					7125	7147		10.1007/s00500-020-04868-x		MAR 2020											
J								Optimization insisted watermarking model: hybrid firefly and Jaya algorithm for video copyright protection	SOFT COMPUTING										Video watermarking; TU-JF algorithm; GLCM features; Deep belief network; Error measures	DIGITAL IMAGE WATERMARKING; NON-BLIND; SCHEME; ROBUST; DECOMPOSITION	In this digital era, illegitimate redistribution and protection of digital multimedia have to turn out to be the critical issue. Therefore, digital watermarking has been introduced for avoiding illegitimate works and also to ensure security and authentication as well. "Digital video watermarking is a method to hide some kind of data like audio, image, text into digital video sequences which is nothing but orders of successive still images." This paper intends to formulate a novel video watermarking framework that includes three stages, such as (i) optimal video frame prediction, (ii) watermark embedding process and (iii) watermark extraction process. Very first, the optimal frames are determined using a new hybrid algorithm termed trial-based update on Jaya plus firefly (TU-JF) algorithm, in such a way that the peak signal-to-noise ratio (PSNR) should be maximal. The frames are assigned with a label of one or zero, where label one denotes the frame with better PSNR (can select for embedding process) and label zero denotes the frame with reduced PSNR (cannot be used for embedding). Consequently, a data library is formed from the obtained results, where each frame of videos is determined with their gray-level co-occurrence matrix (GLCM) features and labels (can embed or not). As in the proposed model, the optimal frame prediction is carried out using deep belief network (DBN) framework; the obtained data are then trained in the model. The optimal frames could be predicted in an efficient manner while testing. The significant contribution of this work concerns the optimization of hidden neurons in the DBN framework, which helps to enhance prediction accuracy. At last, the "watermark embedding process" and "watermark extraction process" are done by which the image could be embedded within the optimal frames.																	1432-7643	1433-7479				OCT	2020	24	19					14809	14823		10.1007/s00500-020-04833-8		MAR 2020											
J								An efficient method for human hand gesture detection and recognition using deep learning convolutional neural networks	SOFT COMPUTING										Hand gesture; Recognition; Mask; Fingers; Segmentation	CLASSIFICATION	The physical movement of the human hand produces gestures, and hand gesture recognition leads to the advancement in automated vehicle movement system. In this paper, the human hand gestures are detected and recognized using convolutional neural networks (CNN) classification approach. This process flow consists of hand region of interest segmentation using mask image, fingers segmentation, normalization of segmented finger image and finger recognition using CNN classifier. The hand region of the image is segmented from the whole image using mask images. The adaptive histogram equalization method is used as enhancement method for improving the contrast of each pixel in an image. In this paper, connected component analysis algorithm is used in order to segment the finger tips from hand image. The segmented finger regions from hand image are given to the CNN classification algorithm which classifies the image into various classes. The proposed hand gesture detection and recognition methodology using CNN classification approach with enhancement technique stated in this paper achieves high performance with state-of-the-art methods.																	1432-7643	1433-7479				OCT	2020	24	20					15239	15248		10.1007/s00500-020-04860-5		MAR 2020											
J								An immune-based response particle swarm optimizer for knapsack problems in dynamic environments	SOFT COMPUTING										Dynamic knapsack problems; Particle swarm optimization; Immune response; External archive; Environmental reaction	ELITISM-BASED IMMIGRANTS; GENETIC ALGORITHMS; SCHEME; MEMORY; EVOLUTIONARY	This paper proposes a novel binary particle swarm optimization algorithm (called IRBPSO) to address high-dimensional knapsack problems in dynamic environments (DKPs). The IRBPSO integrates an immune-based response strategy into the basic binary particle swarm optimization algorithm for improving the quantity of evolutional particles in high-dimensional decision space. In order to enhance the convergence speed of the IRBPSO in the current environment, the particles with high fitness values are cloned and mutated. In addition, an external archive is designed to store the elite from the current generation. To maintain the diversity of elites in the external archive, the elite of current generation will replace the worst one in the external archive if and only if it differs from any of the existing particles in the external archive based on the Hamming distance measurement when the archive is due to update. In this way, the external archive can store diversiform elites for previous environments as much as possible, and so as to the stored elites are utilized to transfer historical information to new environment for assisting to solve the new optimization problem. Moreover, the environmental reaction scheme is also investigated in order to improve the ability of adapting to different kinds of dynamic environments. Experimental results on a series of DKPs with different randomly generated data sets indicate that the IRBPSO can faster track the changing environments and manifest superior statistical performance, when compared with peer optimization algorithms.																	1432-7643	1433-7479				OCT	2020	24	20					15409	15425		10.1007/s00500-020-04874-z		MAR 2020											
J								Legal document similarity:a multi-criteria decision-making perspective	PEERJ COMPUTER SCIENCE										Legal Information Retrieval; Concept Based Similarity; Multi-Dimensional Similarity; OWA; Concept interaction graph	DOMAIN	The vast volume of documents available in legal databases demands effective information retrieval approaches which take into consideration the intricacies of the legal domain. Relevant document retrieval is the backbone of the legal domain. The concept of relevance in the legal domain is very complex and multi-faceted. In this work, we propose a novel approach of concept based similarity estimation among court judgments. We use a graph-based method, to identify prominent concepts present in a judgment and extract sentences representative of these concepts. The sentences and concepts so mined are used to express/visualize likeness among concepts between a pair of documents from different perspectives. We also propose to aggregate the different levels of matching so obtained into one measure quantifying the level of similarity between a judgment pair. We employ the ordered weighted average (OWA) family of aggregation operators for obtaining the similarity value. The experimental results suggest that the proposed approach of concept based similarity is effective in the extraction of relevant legal documents and performs better than other competing techniques. Additionally, the proposed two-level abstraction of similarity enables informative visualization for deeper insights into case relevance.																	2376-5992					MAR 23	2020									e262	10.7717/peerj-cs.262													
J								Semi-supervised learning method based on predefined evenly-distributed class centroids	APPLIED INTELLIGENCE										Semi-supervised learning; Predefined class centroids; PEDCC-loss; Maximum mean discrepancy; Data augmentation		Compared to supervised learning, semi-supervised learning reduces the dependence of deep learning on a large number of labeled samples. In this work, we use a small number of labeled samples and perform data augmentation on unlabeled samples to achieve image classification. Our method constrains all samples to the predefined evenly-distributed class centroids (PEDCC) by the corresponding loss function. Specifically, the PEDCC-Loss for labeled samples, and the maximum mean discrepancy loss for unlabeled samples are used to make the feature distribution closer to the distribution of PEDCC. Our method ensures that the inter-class distance is large and the intra-class distance is small enough to make the classification boundaries between different classes clearer. Meanwhile, for unlabeled samples, we also use KL divergence to constrain the consistency of the network predictions between unlabeled and augmented samples. Our semi-supervised learning method achieves the state-of-the-art results, with 4000 labeled samples on CIFAR10 and 1000 labeled samples on SVHN, and the accuracy is 95.10% and 97.58% respectively. Code is available in https://github.com/sweetTT/semi-supervised-method-based-on-PEDCC. .																	0924-669X	1573-7497				SEP	2020	50	9					2770	2778		10.1007/s10489-020-01689-1		MAR 2020											
J								The long road from performing word sense disambiguation to successfully using it in information retrieval: An overview of the unsupervised approach	COMPUTATIONAL INTELLIGENCE										ambiguous query; information retrieval; Naive Bayes model; spectral clustering; unsupervised word sense disambiguation; word sense disambiguation	DISCRIMINATION; CONSTRUCTION; ALGORITHM; KNOWLEDGE	The issue of whether or not word sense disambiguation (WSD) can improve information retrieval (IR) results has been intensely debated over the years, with many inconclusive or contradictory results and a majority of skeptical opinions. All three classes of WSD methods (supervised, unsupervised, and knowledge-based) have been considered by the literature with respect to IR. We hereby survey the unsupervised approach which, although relatively rarely used, has provided positive results at a large scale. Unsupervised WSD has already made proof of its utility in IR and it is our belief that it still holds a promise for this field. The two main existing types of unsupervised methods for IR, which are of completely different natures, are presented, within the scientific context in which they were born, and are compared. Regardless of the gap in time between these central approaches, we are of the opinion that the unsupervised solution to the discussed problem remains the most significant for IR applications. By surveying what we consider the most promising existing approach to usage of WSD in IR, and by discussing its possible extensions, we hope to stimulate continuation of this line of research, possibly at an even more successful level.																	0824-7935	1467-8640				AUG	2020	36	3					1026	1062		10.1111/coin.12303		MAR 2020											
J								Learning non-convex abstract concepts with regulated activation networks A hybrid and evolving computational modeling approach	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Computational modeling; Hybrid models; Machine learning; Dynamic models; Abstract concepts; Non-convex models; Evolving topology	CONTEXT AVAILABILITY; CONCRETE; REPRESENTATIONS; OBJECTS; MIND	Perceivable objects are customarily termed as concepts and their representations (localist-distributed, modality-specific, or experience-dependent) are ingrained in our lives. Despite a considerable amount of computational modeling research focuses on concrete concepts, no comprehensible method for abstract concepts has hitherto been considered. concepts can be viewed as a blend of concrete concepts. We use this view in our proposed model, Regulated Activation Network (RAN), by learning representations of non-convex abstract concepts without supervision via a hybrid model that has an evolving topology. First, we describe the RAN's modeling process through a Toy-data problem yielding a performance of 98.5%(ca.) in a classification task. Second, RAN's model is used to infer psychological and physiological biomarkers from students' active and inactive states using sleep-detection data. The RAN's capability of performing classification is shown using five UCI benchmarks, with the best outcome of 96.5% (ca.) for Human Activity recognition data. We empirically demonstrate the proposed model using standard performance measures for classification and establish RAN's competency with five classifiers. We show that the RAN adeptly performs classification with a small amount of data and simulate cognitive functions like activation propagation and learning.																	1012-2443	1573-7470				DEC	2020	88	11-12					1207	1235		10.1007/s10472-020-09692-5		MAR 2020											
J								Utilizing the fuzzy IoT to reduce Green Harbor emissions	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										IoT; AIS; Fuzzy logic control; Data mining; Green Harbor	AIR-POLLUTION	The global trend toward "Green Harbors" has now become a vital topic. In the management of maritime pollution, government officials have paid increasing attention to air pollution from ship and port activities. Static and dynamic vessel data are provided by the Automatic Identification System (AIS). This is a telemetric system that automatically transmits information about a vessel to ports and ships in the vicinity; this is then combined with the Internet of Things (IoT). This study integrates the Marine Geography Information System (MGIS) and Fuzzy Division theory with the Spatial Data Mining (SDM) method to propose a system that calculates the optimal speed for any inbound container ship. This will not only reduce harbor air pollution, it will also ensure safe port entry and exit for all vessels. Seven algorithm blocks were used for the fuzzy logic control (FLC) inputs: Turning Capacity (TC), Crush Stop Capacity (CSC), Wind and Current Effect (WACE), Ship Operating Capacity Index (SOCI), Environment Influence (EI), Ship Maneuvering Capacity Index (SMCI), and Green Port Index (GPI); the fuzzy logic system produced one output, namely Optimum Speed (OS). This method provides the OS for the captain and thus the reference navigation speed for the Vessel Traffic Service (VTS). Hence, by limiting speed, the system can suppress air pollution, ensure navigation safety, and improve the efficiency of the port.																	1868-5137	1868-5145															10.1007/s12652-020-01844-z		MAR 2020											
J								Real time behavior based service specific secure routing for cloud centric IoT systems	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cloud systems; Edge nodes; IoT systems; Secure routing; ESM; SSM; SRM; TDSM		The problem of secure routing in cloud centric IoT systems has been well studied. Number of approaches has been discussed earlier to support the security performance in IoT systems. Still the performance of algorithms in security measures is below the expected level. To solve this issue, an efficient behavior analysis model is described in this article. The method monitors the behaviors of edge nodes in cloud, to fetch the information like number of service access, number of transmissions involved, and number of IoT systems, total bytes supported and so on. First, available ways to reach the service point is discovered through each edge nodes of the network, and estimate the edge support measure, service support measure, secure routing measure. The method evaluates the suitability of route for data transmission at each time session based on different factors. According to these factors, trusted data support measure is estimated to select the transmission route. The behavior based service specific secure routing algorithm improves the performance of security towards the growth of QoS in IoT systems.																	1868-5137	1868-5145															10.1007/s12652-020-01879-2		MAR 2020											
J								Dynamic analysis of seven-dimensional fractional-order chaotic system and its application in encrypted communication	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Chaotic system; Lyapunov exponent; 0-1 test; Complexity; Circuit implementation; Encrypted communication	SECURE COMMUNICATION; SYNCHRONIZATION; TIME; IMPLEMENTATION	This paper proposes a novel seven-dimensional fractional-order chaotic system applied to secure communication. First, a seven-dimensional fractional order system is constructed by selecting an arbitrary fractional order q. Then, the chaotic and non-chaotic dynamic behaviors of the system are analyzed by 0-1 Test, and the correctness is verified by phase diagram comparison and complexity analysis. Then, the corresponding fractional-order chaotic circuit is designed, and the phase image which is basically consistent with the experimental simulation results is obtained. Finally, the seven-dimensional fractional-order chaotic system is used to complete the signal encryption and decryption test to achieve secure communication. This indicates that the seven-dimensional fractional-order chaotic system proposed to this paper has more complex dynamic characteristics and can be used for secure communication.																	1868-5137	1868-5145															10.1007/s12652-020-01896-1		MAR 2020											
J								Subspace based noise addition for privacy preserved data mining on high dimensional continuous data	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Privacy preserving data mining; Noise addition; Data privacy	DIFFERENTIAL PRIVACY; PROTECTION; ALGORITHM	Clustering is an important data mining technique. Due to privacy concerns associated with data mining, privacy preservation algorithms have been developed in the last decade. Noise addition is a popular privacy preservation technique. However, in recent years a lot of applications have been dealing with high dimensional datasets. The present privacy-preserving algorithms perform noise addition in a univariate manner along each dimension. This approach does not work well with high dimensional continuous datasets because the distance measurements that contrast a point between similar and non-similar is less and data groups differently in different dimensions. Therefore information loss and data loss are very high, clusters identified are reduced drastically and sometimes even wrong clusters are identified. Data mining performed on such data is ineffective and sometimes invalid. This paper proposes a novel technique called subspace based noise addition (SBNA) that adds noise in subspaces. Dense and Non-dense subspaces are identified and noise addition is then performed separately in dense and non-dense subspaces. Noise is added such that points lying in dense and non-dense subspaces continue to lie in their respective subspaces even after privacy preservation. This approach reduces data loss, information loss and maximizes the identification of clusters. This ensures effective data mining. Experiments are performed on benchmark high dimensional continuous datasets from UCI Machine Learning Repository and the results are compared with the related works like SNA, NALT, and NANLT. SBNA provides an improvement of up to 80% in data utility, 90% in cluster identification and information measures.																	1868-5137	1868-5145															10.1007/s12652-020-01881-8		MAR 2020											
J								Digital twin-driven system for roller conveyor line: design and control	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Digital twin (DT); Cyber-physical system (CPS); Roller conveyor line (RCL); Smart manufacturing		Roller conveyor line (RCL) plays an important role in smart manufacturing workshop and modern logistics industry. RCL transfers the specified type of workpiece box to specified exports according to the personalized demands. It greatly improves the sorting speed and delivery speed of workpiece. However, the construction of RCL is very time-consuming and difficult because of the unintuitive design and complex control. In order to reduce the difficulty of constructing RCL and eliminate the gap between design and control, a research about cyber-physical system (CPS) for design and control driven by digital twin (DT) is conducted. In this research, three key enabling technologies of constructing five-dimensional DT for RCL are illustrated in detail as follows: (1) multi-scale modeling method of RCL; (2) extensible distributed communication framework; (3) fast mapping method of distributed controllers. Finally, this paper provides a case study of using the CPS driven by DT to design and control RCL. Experimental results show that the CPS in this paper can achieve the rapid design and distributed control of RCL.																	1868-5137	1868-5145															10.1007/s12652-020-01898-z		MAR 2020											
J								Conventional neural network for blind image blur correction using latent semantics	SOFT COMPUTING										Blur kernel estimation; Blind image de-blurring; Gaussian mixture model; Convolutional neural network; Auto-encoder		In this work, deep learning for enhancing the sharpness of blurred image is investigated. Initial pre-processing is blur image kernel estimation which is critical for blind image de-blurring. In prior investigation, handcrafted blur features are optimized for certain uniform blur, which is unrealistic for blind de-convolution. To deal with this crisis, initially this work attempts to carry out kernel matrix estimation using latent semantic analysis (KME-LSA) in dermatology image. In order to enhance the image sparseness, this work modelled an image descriptor based on Gaussian mixture model in auto-encoder (GMM-AE) as a primary layer in convolutional neural networks. The functionality of the proposed GMM-AE triggers the selection of efficient features for subsequent layers in CNN. The features extracted from the integrated trained GMM-AE in CNN can fine-tune the quality of blurred image. Datasets used are melanoma-based dermascope images. Pre-processing procedures are carried out by LSA-based kernel matrix estimation. The attained sharp image outcome is given to the proposed model for effective feature extraction and to attain improved blind image. The anticipated KME-LSA and GMM-AE in CNN estimates blur parameters with high accuracy. Experiment illustrates the efficacy of proposed method and the competitive outcomes are compared with state-of-the-art datasets. Simulation was carried out in MATLAB environment; performance metrics like MSE-227.6, PSNR-33.6762, SSIM-0.9755 and VIF-0.08162 are evaluated. The results show better trade-off than the prevailing techniques.																	1432-7643	1433-7479				OCT	2020	24	20					15223	15237		10.1007/s00500-020-04859-y		MAR 2020											
J								A novel hybrid model based on recurrent neural networks for stock market timing	SOFT COMPUTING										Stock market timing; RNN; LSTM; GRU; Classification; Deep learning	SUPPORT VECTOR MACHINE; RETURN PREDICTABILITY; TECHNICAL ANALYSIS; PREDICTION; LSTM; INDEX; IMAGE	Stock market timing is regarded as a challenging task of financial prediction. An accurate prediction of stock trend can yield great profits for investors. At present, recurrent neural networks (RNNs) have a good performance in stock market forecasting. However, there has been a relative lack of research in the stock market timing using RNNs. In this paper, a novel model named hybrid RNN model is proposed for stock market timing by incorporating multi-layer long short-term memory, multi-layer gated recurrent unit and one-layer ReLU layer. Moreover, based on five popular benchmark datasets from UCI Machine Learning Repository and six daily securities from Shanghai Stock Exchange, comparisons with 12 state-of-the-art models are conducted to verify the superiority of the proposed hybrid RNN model in terms of nine technical indicators. The findings from the experiment demonstrate that: (1) as opposed to 12 models, the average accuracy, MSE and AUC of hybrid RNN model (0.7406, 0.2592, 0.7368) are significantly better than other comparison models, and (2) the proposed hybrid RNN classification procedure can be considered as a feasible and effective tool for stock market timing.																	1432-7643	1433-7479				OCT	2020	24	20					15273	15290		10.1007/s00500-020-04862-3		MAR 2020											
J								Adversarial Confidence Learning for Medical Image Segmentation and Synthesis	INTERNATIONAL JOURNAL OF COMPUTER VISION										Adversarial confidence learning; Medical image analysis; Segmentation; Image synthesis	CT IMAGE; NETWORK; MRI	Generative adversarial networks (GAN) are widely used in medical image analysis tasks, such as medical image segmentation and synthesis. In these works, adversarial learning is directly applied to the original supervised segmentation (synthesis) networks. The usage of adversarial learning is effective in improving visual perception performance since adversarial learning works as realistic regularization for supervised generators. However, the quantitative performance often cannot improve as much as the qualitative performance, and it can even become worse in some cases. In this paper, we explore how we can take better advantage of adversarial learning in supervised segmentation (synthesis) models and propose an adversarial confidence learning framework to better model these problems. We analyze the roles of discriminator in the classic GANs and compare them with those in supervised adversarial systems. Based on this analysis, we propose adversarial confidence learning, i.e., besides the adversarial learning for emphasizing visual perception, we use the confidence information provided by the adversarial network to enhance the design of supervised segmentation (synthesis) network. In particular, we propose using a fully convolutional adversarial network for confidence learning to provide voxel-wise and region-wise confidence information for the segmentation (synthesis) network. With these settings, we propose a difficulty-aware attention mechanism to properly handle hard samples or regions by taking structural information into consideration so that we can better deal with the irregular distribution of medical data. Furthermore, we investigate the loss functions of various GANs and propose using the binary cross entropy loss to train the proposed adversarial system so that we can retain the unlimited modeling capacity of the discriminator. Experimental results on clinical and challenge datasets show that our proposed network can achieve state-of-the-art segmentation (synthesis) accuracy. Further analysis also indicates that adversarial confidence learning can both improve the visual perception performance and the quantitative performance.																	0920-5691	1573-1405				NOV	2020	128	10-11			SI		2494	2513		10.1007/s11263-020-01321-2		MAR 2020											
J								EEWC: energy-efficient weighted clustering method based on genetic algorithm for HWSNs	COMPLEX & INTELLIGENT SYSTEMS										Wireless sensor networks; Genetic algorithms; Clustering; Energy efficiency	PROTOCOL	Wireless sensor networks are widely used in monitoring and managing environmental factors like air quality, humidity, temperature, and pressure. The recent works show that clustering is an effective technique for increasing energy efficiency, traffic load balancing, prolonging the lifetime of the network and scalability of the sensor network. In this paper, a new energy-efficient clustering technique has been proposed based on a genetic algorithm with the newly defined objective function. The proposed clustering method modifies the steady-state phase of the LEACH protocol in a heterogeneous environment. The proposed objective function considers three main clustering parameters such as compactness, separation, and number of cluster heads for optimization. The simulation result shows that the proposed protocol is more effective in improving the performance of wireless sensor networks as compared to other state-of-the-art methods, namely SEP, IHCR, and ERP.																	2199-4536	2198-6053				JUL	2020	6	2					391	400		10.1007/s40747-020-00137-4		MAR 2020											
J								Analysis of asynchronous distributed multi-master parallel genetic algorithm optimization on CAN bus	EVOLVING SYSTEMS										CAN bus; Multi-master architecture; Parallel genetic algorithm; Busmaster simulator	EVOLUTIONARY ALGORITHMS; NEURAL-NETWORK; ENHANCEMENT	Industrial optimization problems are usually difficult to solve due to complexity and high number of constraints. Evolutionary algorithms are a conventional method to solve these problems. However, many industrial applications are real-time or we need to find a feasible optima solution in a limited time. Parallel genetic algorithm is a method to utilize properties of the genetic algorithm and parallel processing and implementation of a fast evolutionary algorithm. Controller Area Network (CAN) protocol is widely used in various industries such as automotive, medical, aerospace. In this paper, we implement a multiple-population coarse-grained parallel genetic algorithm on CAN bus to improve speed and performance of the conventional genetic algorithm which is asynchronous distributed multi-master. Evaluation criteria such as speed up, efficiency, serial fraction and reliability are calculated for the proposed parallel processing which is used for optimization problem of five benchmark functions. And finally, this structure is compared with the master-slave model. The proposed structure is created conditions for improving network reliability with very low cost of communication.																	1868-6478	1868-6486				DEC	2020	11	4					673	682		10.1007/s12530-020-09337-2		MAR 2020											
J								Development of novel and efficient approach for analyzing and monitoring the movement parameters for javelin athletes based on internet of things	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Javelin; Database; Cloud; Sensors; Release parameters; ROSOTO-JAV; IoT	IOT; AUTOMATION; SYSTEM	The javelin is one of the well-received track and field events, which are available from the origin of the human race. The people from the old civilization used the spear-like structure for hunting animals and a soldier to kill enemies in the Warfield used later spear. Due to the development of the human race, the skill of throwing spear was added to the sports category. It is named as Javelin, and all countries play it over the world. The ultimate goal of Javelin throw is to throw a javelin with exposed hands with the highest force so that it will achieve the maximum distance in a given area. Several factors need to be considered for playing this game effectively, they are physiological parameters, a movement (or) release parameters, and injury parameters of javelin athletes. In the proposed work, we design a device, which is used to measure the movement, or release parameters of a javelin by using various sensors and these measured parameters are sent to a cloud server with the help of Wi-Fi device for maintaining a database. The coach or athlete can view the release parameters of javelin such as the angle of release, the height of release and speed of release in real-time through the application called Real-Time Sports Training for Javelin Athletes (ROSOTO-JAV). By monitoring and analyzing, certain throw and provide auto feedback to the athlete along with their parameters to improve their performance in future using the IoT technology.																	1868-5137	1868-5145															10.1007/s12652-020-01863-w		MAR 2020											
J								A novel user interaction middleware component system for ubiquitous soft computing environment by using fuzzy agent computing system	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										User centric; Middleware; Semantic query; Functional dependency; Ubiquitous computing environment; Soft computing; Fuzzy agent system; Fuzzy logic	INTELLIGENCE	Ubiquitous Computing Environment (UCE) has become increasingly crucial for a component-based Middleware for Ubiquitous Soft Computing Environment by using Fuzzy Agent Computing System (USCE-FACS) that can fully exploit interactive service characteristics to support user-centric services in UCE. It has posed some challenges, such as high Memory Consumption and high Component Building Time to users who are hiring services in a heterogeneous environment. To control the UCE for the user's benefit, FACS works non-rudely in an online deep-rooted learning way to get familiar with the user behavior. The point of incorporation of previously mentioned advances is to make more extensive the connection between individuals and data innovation gear through the use of an undetectable system of UCE devices making dynamic computational-environments equipped for fulfilling the users' prerequisites. Due to a large number of available services without ontology and Metadata Repository (MR), it becomes time-consuming for end-users to find appropriate services to satisfy their various needs. To help end-users obtain their desired services, the USCE-FACS proposed, it reduces MC and CBT in a heterogeneous environment. This paper concentrated on the communication from users to devices to enable a general and quick access to accessible element and administrations gave by the UCE.																	1868-5137	1868-5145															10.1007/s12652-020-01893-4		MAR 2020											
J								Efficient column-oriented processing for mutual subspace skyline queries	SOFT COMPUTING										Subspace; Mutual skyline queries; Algorithm; Spatial database		A mutual skyline query will enable some new applications, such as marketing analysis, task allocation, and personalized matching. Algorithms for efficient processing of this query have been recently proposed in the literature. Those approaches use the R-tree indexes and apply a series of pruning criteria toward efficient processing. However, they are characterized by several limitations: (1) they cannot process different interests on attributes for skyline and reverse skyline, (2) they require a multidimensional index, which suffers from performance degradation, especially in high-dimensional space, and (3) they do not support vertically decomposed data that is a natural and intuitive choice for the parallel queries. To this end, we address aforementioned these problems and propose three efficient algorithms, i.e., index-based mutual subspace skyline, optimized index-based MSS, and parallel mutual subspace skyline, using the column-oriented processing that is more suitable for subspace and parallel skyline. Extensive experimental results show that our proposed algorithms are effective and efficient.																	1432-7643	1433-7479				OCT	2020	24	20					15427	15445		10.1007/s00500-020-04875-y		MAR 2020											
J								LTR-expand: query expansion model based on learning to rank association rules	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Formal Concept Analysis (FCA); Association Rule (AR); Generic basis; Learning to rank; SVM rank; Query Expansion (QE)	RELEVANCE FEEDBACK	Query Expansion (QE) is widely applied to improve the retrieval performance of ad-hoc search, using different techniques and several data sources to find expansion terms. In Information Retrieval literature, selecting expansion terms remains a challenging task that relies on the extraction of term relationships. In this paper, we propose a new learning to rank-based query expansion model. The main idea behind is that, given a query and the set of its related ARs, our model ranks these ARs according to their relevance score regarding to this query and then selects the most suitable ones to be used in the QE process. Experiments are conducted on three test collections, namely: CLEF2003, TREC-Robust and TREC-Microblog, including long, hard and short queries. Results showed that the retrieval performance can be significantly improved when the ARs ranking method is used compared to other state of the art expansion models, especially for hard and long queries.																	0925-9902	1573-7675				OCT	2020	55	2					261	286		10.1007/s10844-020-00596-8		MAR 2020											
J								Improving named entity recognition in noisy user-generated text with local distance neighbor feature	NEUROCOMPUTING										Named entity recognition; Gazetteer; Text mining; Darknet; Hidden services		Recognizing infrequent or emerging named entities in a user-generated text is a challenging task, especially when informal or slang text is used. Some recent works propose to use a gazetteer to solve this problem, but this solution is not general because the gazetteer is task-specific and its maintenance is costly. In this paper, we overcome this drawback by presenting Local Distance Neighbor (LDN), a novel feature that substitutes the gazetteer and makes that the model obtains state-of-the-art results. LDN captures an initial guess for each input token based on the categories of its neighboring tokens within an embedding space. We evaluated the proposed network on the W-NUT-2017 dataset, and we obtained the state-of-the-art F1 score for the Group, Person, and Product categories. We employed our new feature together with the model proposed by Aguilar et al. to recognize named entities in the Tor Darknet related to suspicious activities associated with weapons and drug selling. After increasing the samples of the W-NUT-2017 dataset with 851 manually annotated entries, we repeated our evaluation in this extended version of the dataset, achieving entity and surface F1 scores of 52.96% and 50.57%, respectively. Furthermore, we demonstrate that our proposal can be useful for Law Enforcement Agencies in mining the textual information in the Tor hidden services, being especially adequate for the Group, Person, and Product categories. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 21	2020	382						1	11		10.1016/j.neucom.2019.11.072													
J								Power scheduling optimization under single-valued neutrosophic uncertainty	NEUROCOMPUTING										Power scheduling optimization; Fuzzy uncertainty; Single-valued neutrosophic set; State transition algorithm; Decision making method	STATE TRANSITION ALGORITHM; MULTIOBJECTIVE OPTIMIZATION; DESIGN OPTIMIZATION; TOPSIS; SYSTEM	Optimization problems with improper expression of constraints exist widely in practical engineering. In order to achieve a reasonable degree of constraints satisfaction, this paper investigates a single-valued neutrosophic optimization method to deal with system uncertainty. Firstly, an equivalent model based on single-valued neutrosophic entropy is proposed to transform the original problem into a crisp multi-objective optimization problem. The Pareto-front of the optimization problem is then obtained by a multi-objective state transition algorithm. Finally, the best solution is determined by a multi-criteria decision making method. A practical example of a zinc electrowinning process is used to illustrate the effectiveness and advantage of the developed new optimization approach, which provides a more cost-effective solution to decrease the electricity utility charge and satisfy the daily output production requirements. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 21	2020	382						12	20		10.1016/j.neucom.2019.11.089													
J								Relative-output-based consensus for nonlinear multi-agent systems with unknown measurement sensitivities	NEUROCOMPUTING										Nonlinear multi-agent systems; Consensus; Output feedback; Measurement sensitivities	FEEDBACK STABILIZATION; NETWORKS; SYNCHRONIZATION; DESIGN; AGENTS	In this paper, the consensus problem is investigated for a class of nonlinear multi-agent systems in lower triangular form through dynamic output feedback control. Each agent can only access to the uncertain relative output information from other agents under directed communication. The presence of unknown measurement sensitivities causes the relative output can not be used to construct the observer for each agent directly. For the sake of overcoming this difficulty, a dual-domination gain method is applied to dominate the nonlinear terms and the unknown measurement sensitivities. Then, a new dynamic output feedback consensus prtocol is presented to drive the multi-agent systems to consensus. The upper bound of the unknown measurements sensitivities is also given. At last, we provide two numerical simulation examples to verify the effectiveness of our proposed methods. (C) 2019 The Authors. Published by Elsevier B.V.																	0925-2312	1872-8286				MAR 21	2020	382						21	31		10.1016/j.neucom.2019.11.082													
J								PCFNet: Deep neural network with predefined convolutional filters	NEUROCOMPUTING										Traditional image filters; Domain knowledge; Image classification; Convolutional neural networks		Convolutional neural network (CNN) is a common framework that has been widely applied in many computer vision tasks. However, dealing with many real-world problems, especially problems of medical image analysis, requires the addition of some other task-specific information, which we call as domain knowledge. In this study, we propose a new framework named as Predefined Convolutional Filters Network (PCFNet), which replaces the kernel in the first layer convolution of conventional CNN with some learnable predefined filters. This framework is a modified version of the conventional CNN and, when using proper image filters, outperforms conventional CNN in solving many specific problems, such as medical image classification and handwritten digit recognition problems. As expected, this framework achieves state-of-the-art results on publicly available medical image (IDRiD) dataset and USPS dataset. In addition, PCFNet has fewer learnable parameters in the first layer convolution, meaning less training data will be required, and this effectiveness has been validated by experimental results on CIFAR10/100. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 21	2020	382						32	39		10.1016/j.neucom.2019.11.075													
J								MFC: Initialization method for multi-label feature selection based on conditional mutual information	NEUROCOMPUTING										Multi-label feature selection; Evolutionary algorithm; Mutual information	PARTICLE SWARM OPTIMIZATION; HYBRID GENETIC ALGORITHM; CLASSIFICATION; SEARCH	Feature selection is widely used in multi-label classification because of its simplicity, efficiency, and accuracy. Specifically, evolutionary algorithm (EA)-based multi-label feature selection methods have produced good potential. However, in conventional EA-based approaches, the initial population is usually generated at random, because knowledge about the input dataset is not usually available. In this study, we propose a method for generating the initial population of an EA-based multi-label feature selection method considering dependencies between features and labels. To the best of our knowledge, this is the first initialization method described for EA-based multi-label feature selection. The initial population generated by the proposed method can be easily applied to the EA-based feature selection method because it is independent of EA-based feature selection algorithms. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 21	2020	382						40	51		10.1016/j.neucom.2019.11.071													
J								A novel deep neural network based approach for sparse code multiple access	NEUROCOMPUTING										Sparse code multiple access; Non-orthogonal multiple access; Machine learning; Dense code multiple access	MASSIVE MIMO; DESIGN	Sparse code multiple access (SCMA) has been one of the non-orthogonal multiple access (NOMA) schemes aiming to support high spectral efficiency and ubiquitous access requirements for 5G communication networks. Conventional SCMA approaches are confronting challenges in designing low-complexity high-accuracy decoding algorithm and constructing optimum codebooks. Fortunately, the recent spotlighted deep learning technologies are of significant potentials in solving many communication engineering problems. Inspired by this, we propose and train a deep neural network (DNN) called DL-SCMA to learn to decode SCMA modulated signals corrupted by additive white Gaussian noise (AWGN). An autoencoder called AE-SCMA is established and trained to generate optimal SCMA codewords and reconstruct original bits. Furthermore, by manipulating the mapping vectors, an autoencoder is able to generalize SCMA, thus a dense code multiple access (DCMA) scheme is proposed. Simulations show that the DNN SCMA decoder significantly outperforms the conventional message passing algorithm (MPA) in terms of bit error rate (BER), symbol error rate (SER) and computational complexity, and AE-SCMA also demonstrates better performances via constructing better SCMA codebooks. The performance of deep learning aided DCMA is superior to the SCMA. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 21	2020	382						52	63		10.1016/j.neucom.2019.11.066													
J								Multi-scale multi-patch person re-identification with exclusivity regularized softmax	NEUROCOMPUTING										Person re-identification; Deep learning; Exclusivity regularized softmax	NETWORK	Discriminative feature learning is critical for person re-identification. To obtain abundant visual information from the input person image, we first propose a novel network that extracts multi-scale patch-level deep features. Then, we propose an improved softmax loss function for learning more compact and more discriminative feature vectors. Specifically, we integrate feature pyramid blocks and region-level global average pooling functions into the feature extraction network, introduce the well-established normalization techniques in face recognition algorithms into person re-ID, and penalize the redundancy in feature vectors by minimizing the l(1,2) norm of the weight matrix in the softmax layer. Experiments on three large-scale datasets under the standard settings show the effectiveness of the proposed method. Moreover, we report our cross-domain re-ID results by training re-ID models on source datasets and testing them on other target datasets. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 21	2020	382						64	70		10.1016/j.neucom.2019.11.062													
J								Decentralized composite suboptimal control for a class of two-time-scale interconnected networks with unknown slow dynamics	NEUROCOMPUTING										Two-time-scale interconnected networks; Decentralized control; Composite suboptimal control; Adaptive dynamic programming	QUADRATIC TRACKING CONTROL; ADAPTIVE OPTIMAL-CONTROL; NEAR-OPTIMAL CONTROL; SYSTEMS; SYNCHRONIZATION; STABILIZATION	This paper presents a decentralized composite suboptimal control strategy to solve the optimal control problem for a class of two-time-scale networks, which consist of m lower-level subsystems interconnected through an upper-level main system with unknown dynamics. Firstly, the original optimal control problem is reformulated into the optimal control problems of separated subsystems on different time-scales by means of singular perturbation theory. Secondly, a decentralized composite suboptimal control is designed by combing model-based fast controller and data-based slow controller. Thirdly, the asymptotic stability of the closed-loop overall system and the sub-optimality of the proposed scheme are rigorously proved. As a consequence, the ill-conditioned numerical issues and high dimensionality associated with the full-order model are eliminated in controller design. Finally, a numerical simulation is provided to illustrate the effectiveness of the theoretical developments. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				MAR 21	2020	382						71	79		10.1016/j.neucom.2019.11.057													
J								Inspecting adversarial examples using the fisher information	NEUROCOMPUTING										Deep Learning; Adversarial Examples; Fisher information; Explainability	MODELS	Adversarial examples are constructed by slightly perturbing a correctly processed input to a trained neural network such that the network produces an incorrect result. This work proposes the usage of the Fisher information for the detection of such adversarial attacks. We discuss various quantities whose computation scales well with the network size, study their behavior on adversarial examples and show how they can highlight the importance of single input neurons, thereby providing a visual tool for further analyzing the behavior of a neural network. The potential of our methods is demonstrated by applications to the MNIST, CIFARI0 and Fruits-360 datasets and through comparison to concurring methods. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 21	2020	382						80	86		10.1016/j.neucom.2019.11.052													
J								Adversarial attacks on Faster R-CNN object detector	NEUROCOMPUTING										Adversarial attack; Object detection; White-box attack; Black-box attack		Adversarial attacks have stimulated research interests in the field of deep learning security. However, most of existing adversarial attack methods are developed on classification. In this paper, we use Projected Gradient Descent (PGD), the strongest first-order attack method on classification, to produce adversarial examples on the total loss of Faster R-CNN object detector. Compared with the state-of-the-art Dense Adversary Generation (DAG) method, our attack is more efficient and more powerful in both white-box and black-box attack settings, and is applicable in a variety of neural network architectures. On Pascal VOC2007, under white-box attack, DAG has 5.92% mAP on Faster R-CNN with VGG16 backbone using 41.42 iterations on average, while our method achieves 0.90% using only 4 iterations. We also analyze the difference of attacks between classification and detection, and find that in addition to misclassification, adversarial examples on detection also lead to mis-localization. Besides, we validate the adversarial effectiveness of both Region Proposal Network (RPN) and Fast R-CNN loss, the components of the total loss. Our research will provide inspiration for further efforts in adversarial attacks on other vision tasks. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 21	2020	382						87	95		10.1016/j.neucom.2019.11.051													
J								Data-based stable value iteration optimal control for unknown discrete-time systems with time delays	NEUROCOMPUTING										Adaptive dynamic programming; Data-based control; Multiple time delays; Stable value iteration; Optimal control	HORIZON OPTIMAL-CONTROL; OPTIMAL-CONTROL DESIGN; NONLINEAR-SYSTEMS	In this study, a novel data-based stable value iteration (SVI) optimal control scheme is presented in order to tackle with the linear discrete-time (DT) system with multiple time delays. Due to the difficulty in acquiring the knowledge of system dynamics, the optimal control strategies could be computed with only history input and output database by employing an estimator on the basis of adaptive dynamic programming (ADP) technology. By analyzing features of time delay systems, a homologous equivalent notion of delay-free systems is proposed so that the optimal control policy against systems with multiple time delays could be designed indirectly. Moreover, four equivalent conditions are deduced between the two related systems. The convergence of the rising SVI algorithm with discount factor is discussed according to optimal control principles. The proposed SVI algorithm is proved to converge to optimal values with proper discount factor. In the end, two numerical examples are given, and simulation results illustrate that the presented data-based SVI method is effective. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 21	2020	382						96	105		10.1016/j.neucom.2019.11.047													
J								A low-cost and high-speed hardware implementation of spiking neural network	NEUROCOMPUTING										Spiking neural network; Neurons; Hardware implementation; Speed-up; Leaky-Integrate-Fire; Tempotron supervised learning rules		Spiking neural network (SNN) is a neuromorphic system based on the information process and store procedure of biological neurons. In this paper, a low-cost and high-speed implementation for a spiking neural network based on FPGA is proposed. The LIF (Leaky-Integrate-Fire) neuron model and tempotron supervised learning rules are used to construct the SNN which can be applied to the classification of pictures. A combined circuit instead of lookup table implementation method is proposed to realize the complex computing of kernel function in LIF neuron model. In addition, this work replaces the multiplication operations in the weights training with the arithmetic shift, which can speed up the training efficiency and reduce the consumption of computing resources. Experimental results based on Vertix-7 FPGA shows that the classification accuracy is approximately 96% and the average time for classifying a sample is 0.576us at the maximum frequency 178 MHz which achieves approximately 908,578 speedup compared with the software implementation on Matlab. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 21	2020	382						106	115		10.1016/j.neucom.2019.11.045													
J								Wavelet-based residual attention network for image super-resolution	NEUROCOMPUTING										Super-resolution; Wavelet transform; Multi-kernel convolution; Channel attention; Spatial attention	LOW-RANK; REPRESENTATION; INTERPOLATION	Image super-resolution (SR) is a fundamental technique in the field of image processing and computer vision. Recently, deep learning has witnessed remarkable progress in many super-resolution approaches. However, we observe that most studies focus on designing deeper and wider architectures to improve the quality of image SR at the cost of computational burden and speed. Few researches adopt lightweight but effective modules to improve the efficiency of SR without compromising its performance. In this paper, we propose the Wavelet-based residual attention network (WRAN) for image SR. Specifically, the input and label of our network are four coefficients generated by the two-dimensional (2D) Wavelet transform, which reduces the training difficulty of our network by explicitly separating low-frequency and high-frequency details into four channels. We propose the multi-kernel convolutional layers as basic modules in our network, which can adaptively aggregate features from various sized receptive fields. We adopt the residual attention block (RAB) that contains channel attention and spatial attention modules. Thus, our method can focus on more crucial underlying patterns in both channel and spatial dimensions in a lightweight manner. Extensive experiments validate that our WRAN is computationally efficient and demonstrate competitive results against state-of-the-art SR methods. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 21	2020	382						116	126		10.1016/j.neucom.2019.11.044													
J								Learning from sensory predictions for autonomous and adaptive exploration of object shape with a tactile robot	NEUROCOMPUTING										Active and adaptive perception; Sensorimotor control; Autonomous tactile exploration; Bayesian inference	BAYESIAN INTEGRATION; TOUCH; INFORMATION	Humans use information from sensory predictions, together with current observations, for the optimal exploration and recognition of their surrounding environment. In this work, two novel adaptive perception strategies are proposed for accurate and fast exploration of object shape with a robotic tactile sensor. These strategies called (1) adaptive weighted prior and (2) adaptive weighted posterior, combine tactile sensory predictions and current sensor observations to autonomously adapt the accuracy and speed of active Bayesian perception in object exploration tasks. Sensory predictions, obtained from a forward model, use a novel Predicted Information Gain method. These predictions are used by the tactile robot to analyse 'what would have happened' if certain decisions 'would have been made' at previous decision times. The accuracy of predictions is evaluated and controlled by a confidence parameter, to ensure that the adaptive perception strategies rely more on predictions when they are accurate, and more on current sensory observations otherwise. This work is systematically validated with the recognition of angle and position data extracted from the exploration of object shape, using a biomimetic tactile sensor and a robotic platform. The exploration task implements the contour following procedure used by humans to extract object shape with the sense of touch. The validation process is performed with the adaptive weighted strategies and active perception alone. The adaptive approach achieved higher angle accuracy (2.8 deg) over active perception (5 deg). The position accuracy was similar for all perception methods (0.18 mm). The reaction time or number of tactile contacts, needed by the tactile robot to make a decision, was improved by the adaptive perception (1 tap) over active perception (5 taps). The results show that the adaptive perception strategies can enable future robots to adapt their performance, while improving the trade-off between accuracy and reaction time, for tactile exploration, interaction and recognition tasks. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 21	2020	382						127	139		10.1016/j.neucom.2019.10.114													
J								DNA computing inspired deep networks design	NEUROCOMPUTING										Deep neural networks; DNA computing; Automatic architecture design; Image classification	NEURAL-NETWORK; EVOLUTIONARY; OPTIMIZATION; COMPUTATION	Deep neural networks have gained state-of-the-art results in many applications, such as pattern recognition and computer vision. However, most of the deep neural networks are designed manually by researchers. This architecture design process is generally time consuming and needs much expertise. Hence, automatic neural network design becomes an important issue. In this paper, we propose a novel method, called DNA computing inspired networks design (DNAND), to automatically learn high performance deep networks. In DNAND, we use DNA strands to represent blocks of a model, and these DNA strands are reacted to construct the overall networks according to the base pairing principle. We also present the killing strategy, with which we stop training "bad" models if they fail to reach the specific accuracy threshold on the validation set, so as to reduce the computational cost and accelerate the learning process. Extensive experiments on image classification and detection data sets demonstrate the effectiveness of the proposed method. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 21	2020	382						140	147		10.1016/j.neucom.2019.11.098													
J								CascadeGAN: A category-supervised cascading generative adversarial network for clothes translation from the human body to tiled images	NEUROCOMPUTING										Tiled clothing generation; Image-to-image translation; Clothing retrieval; Generative adversarial network		With the popularity and development of the Internet, purchasing items that are similar to those that appear in videos or on fashion websites has gradually become an established trend in online commerce. Currently, determining how to accurately locate these similar items in a huge e-commerce database poses a major challenge. When processing images that contain clothes worn on the human body, traditional methods usually detect the clothes in these images first, add them to a database for clothing retrieval, and finally identify a few pieces of clothing with a high degree of similarity for recommendation. Generative adversarial networks (GANs) have been widely and successfully utilized in the field of image-to-image translation. In this paper, GAN is used to transfer human body images into tiled clothing images, which can be directly used for clothing retrieval. To achieve this, a category-supervised GAN under a cascading structure is proposed. For model training, a large-scale dataset was compiled that contains 39,521 image pairs. Experimental results demonstrate that the tiled clothing images generated by our proposed method deliver higher quality, as well as performance superiority, for clothing retrieval in comparison to other existing methods. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 21	2020	382						148	161		10.1016/j.neucom.2019.11.085													
J								Local manifold sparse model for image classification	NEUROCOMPUTING										Image classification; Sparse representation; Manifold structure; Local information	DIMENSIONALITY REDUCTION; LOW-RANK; FACE RECOGNITION; REPRESENTATION; ROBUST	How to discriminate the types of images is very important for image understanding. A large number of classifiers have been designed for the automatic classification of image. In recent years, sparse representation has widely used in the field of image classification. However, most of the sparse classification methods are based on the sparse reconstruction which ignores the intrinsic structure of data. Therefore, in this paper, we proposed a local manifold sparse classifier (LMSC) on the basis of the sparse manifold assumption and the local structure of data. The proposed method uses the sparse manifold assumption and the local neighbors of data to construct a sparse representation model. Then, we can obtain the sparse coefficients with the proposed sparse model. Finally, we can calculate the probability of the unknown image in each class and assign the class label of the maximization probability to this image. LMSC can reveal the intrinsic structure of data and improve the accuracy of image classification. Experiments on two handwritten digit image data sets (MNIST and Semeion) and a hyperspectral remote sensing image data set (Pavia university) show that the proposed method can achieve better representation and classification accuracy compared to some state-of-the-art classification methods. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 21	2020	382						162	173		10.1016/j.neucom.2019.11.084													
J								Knowledge graph based natural language generation with adapted pointer-generator networks	NEUROCOMPUTING										Knowledge graph; Natural language generation; Supervised attention; Seq2Seq neural networks; Coverage loss		Pointer-generator networks have recently shown superior performance in Natural Language Generation (NLG) tasks, such as automatic generating descriptions for entities in Knowledge Graph (KG). In the absence of the introductory description about an entity we intend to know, the natural language text automatically generated by neural network model can greatly help people to better understand the entity. Entities in KG always have multiple property fields and corresponding values. The target generated description sentences should consist of these slot types and slot values in a consistent and high coverage manner. In order to cover the facts in an input KG, pointer-generator networks always copy certain segments from the input sequences via softmax pointing while generating novel words through the generator. But the challenges that when and where to integrate the copied information with the generated one, how to prevent duplicate generation and information loss are hard to conquer. In this paper, the KG2TEXT model based on adapted pointer-generator networks is proposed. In it, a varied coverage loss function is devised to cover attribute-value pairs as many as possible when generating natural language descriptions for entities in KG. Secondly, an attention mechanism named supervised attention mechanism is added to the model which aims to guide the soft switch transformation process (to generate or to copy). With empirical experiments on two kinds of realistic datasets, the KG2TEXT model achieves promising results and outperforms the state-of-the-art approaches. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				MAR 21	2020	382						174	187		10.1016/j.neucom.2019.11.079													
J								Improved sequence generation model for multi-label classification via CNN and initialized fully connection	NEUROCOMPUTING										Multi-label classification; CNN; LSTM; Text classification	NEURAL-NETWORKS	In multi-label text classification, considering the correlation between labels is an important yet challenging task due to the combination possibility in the label space increasing exponentially. In recent years, neural network models have been widely applied and gradually achieved satisfactory performance in this field. However, existing methods either not model the fully internal correlations among labels or not capture the local and global semantic information of text simultaneously, which somewhat affects the classification results finally. In this paper, we implement a novel model for multi-label classification based on sequence-to-sequence learning, in which two different neural network modules are employed, named encoder and decoder respectively. The encoder uses the convolutional neural network to extract the high-level local sequential semantic, which is combined with the word vector to generate the final text representation through the recurrent neuron network and attention mechanism. The decoder, besides using a recurrent neural network to capture the global label correlation, employs an initialized fully connection layer to capture the correlation between any two different labels. When trained on RCV1-v2, AAPD and Ren-CECps datasets, the proposed model outperforms previous work in main evaluation metrics of hamming loss and micro-F1 score. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 21	2020	382						188	195		10.1016/j.neucom.2019.11.074													
J								Regularized nonnegative matrix factorization with adaptive local structure learning	NEUROCOMPUTING										Graph regularization; Adaptive local structure learning; Nonnegative matrix factorization; Clustering; Similarity measure	ALGORITHMS	Due to the effectiveness of Nonnegative Matrix Factorization (NMF) and its graph regularized extensions, these methods have been received much attention from various researchers. Generally, these methods are performed in two separate steps including Laplacian graph construction and the subsequent matrix decomposition.However, the similarity measurement for Laplacian graph is challenging since it's often affected by several factors such as the neighborhood size, choice of similarity metric, etc. As a result, the learned graph may be not suitable, let alone the subsequent matrix decomposition. In this paper, we propose adaptive graph regularized NMF. Different from existing methods, the similarity matrix is automatically learned from the data. The proposed model can simultaneously performs matrix decomposition and similarity learning. By balancing the interactions between both of the two subtasks in our model, each subtask is improved iteratively based on the result of another. Experimental results on benchmark data sets illustrate the effectiveness of our model. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 21	2020	382						196	209		10.1016/j.neucom.2019.11.070													
J								Crowd counting with crowd attention convolutional neural network	NEUROCOMPUTING										Convolutional neural network; Crowd counting; Confidence map; Density map		Crowd counting is a challenging problem due to the scene complexity and scale variation. Although deep learning has achieved great improvement in crowd counting, scene complexity affects the judgement of these methods and they usually regard some objects as people mistakenly; causing potentially enormous errors in the crowd counting result. To address the problem, we propose a novel end-to-end model called Crowd Attention Convolutional Neural Network (CAT-CNN). Our CAT-CNN can adaptively assess the importance of a human head at each pixel location by automatically encoding a confidence map. With the guidance of the confidence map, the position of human head in estimated density map gets more attention to encode the final density map, which can avoid enormous misjudgements effectively. The crowd count can be obtained by integrating the final density map. To encode a highly refined density map, the total crowd count of each image is classified in a designed classification task and we first explicitly map the prior of the population-level category to feature maps. To verify the efficiency of our proposed method, extensive experiments are conducted on three highly challenging datasets. Results establish the superiority of our method over many state-of-the-art methods. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 21	2020	382						210	220		10.1016/j.neucom.2019.11.064													
J								Event-triggering based adaptive neural tracking control for a class of pure-feedback systems with finite-time prescribed performance	NEUROCOMPUTING										Pure-feedback nonlinear systems; Event-triggered control; Prescribed performance; RBF neural network	FAULT-TOLERANT CONTROL; NONLINEAR-SYSTEMS; PREDICTIVE CONTROL; NETWORK	In this paper, a finite-time tracking control problem is considered for a class of pure-feedback nonlinear systems with event-triggered strategy. The implicit function theorem and the mean value theorem are used to transform the pure-feedback nonlinear systems into strict feedback nonlinear systems. The neural network is adopted to approximate the unknown function and the tracking error is limited to a pre-given boundary by prescribed performance at a finite time. In addition, an improved event-triggered control strategy is proposed to obtain a larger threshold, and also the proposed controller can avoid the Zeno-behavior. Based on Lyapunov stability theory, the adaptive neural network controller can ensure that all the signals in the closed-loop system are uniformly ultimately bounded. Finally, the feasibility of this control scheme is proved by simulation. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 21	2020	382						221	232		10.1016/j.neucom.2019.11.055													
J								Noise-tolerant Z-type neural dynamics for online solving time-varying inverse square root problems: A control-based approach	NEUROCOMPUTING										Modified Z-type models (MZTMs); Time-varying inverse square root problems (TVISRPs); Online solution; Nonlinear control system; Control viewpoint	MODELS; NETWORK; DESIGN	Assuming that the solving process is free of measurement noises, the zeroing neural dynamics (Z-type) based on different zeroing dynamics for online solving time-varying inverse square root problems (TVISRPs) are revisited from the perspective of control technique and unified into a control framework. However, noises are ubiquitous and unavoidable in the application of the real-time system. Therefore, the modified Z-type models (MZTMs) with different measurement noises are needed for online solving TVISRPs. In this study, the MZTMs are first developed, analyzed and verified for online solution of the TVISRPs with different measurement noises. Furthermore, theoretical analyses infer that the MZTMs globally and exponentially converge to the theoretical solution. Compared with the traditional Z-type neural dynamics model (ZTNDM) and gradient neural dynamics model (GNDM), two illustrative examples are described and investigated to substantiate the efficiency and superiority of the developed MZTMs. Finally, a systematic method is proposed via exploring control framework to construct MZTMs for online solving TVISRPs with great robustness and accuracy. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 21	2020	382						233	248		10.1016/j.neucom.2019.11.035													
J								Adaptive event-triggered synchronization control for complex networks with quantization and cyber-attacks	NEUROCOMPUTING										Complex networks; Adaptive event-triggered scheme; Synchronization control; Quantization; Cyber-attacks	STATE ESTIMATION; MULTIAGENT SYSTEMS; DYNAMICAL NETWORKS; DECEPTION ATTACKS; SENSOR NETWORKS; NEURAL-NETWORKS; DELAYS	This paper investigates the adaptive event-triggered synchronization control problem for a class of complex networks with quantization and cyber-attacks. For reducing the burden of communication and computation, an adaptive event-triggered scheme and a logarithmic quantizer are both introduced. Firstly, considering the effect of adaptive event-triggered sampling, quantization and cyber-attacks, a novel synchronization error model is established. By using Lyapunov functional method, some sufficient conditions are derived to guarantee the asymptotic stability of the augmented synchronization error system. Moreover, the co-design of the controller and adaptive event-triggered scheme are also solved in terms of solving the solution of linear matrix inequality(LMI). Finally, a numerical example is exploited to show the effectiveness of the proposed method. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 21	2020	382						249	258		10.1016/j.neucom.2019.11.096													
J								Hard sample mining makes person re-identification more efficient and accurate	NEUROCOMPUTING										Person re-identification; Hard sample mining; Deep learning		In recent years, the field of person re-identification has made significant advances riding on the wave of deep learning. However, owing to the fact that there are much more easy examples than those meaningful hard examples in a dataset, the training tends to stagnate quickly and the model may suffer from over-fitting, which leads to some error matching of models especially for some hard samples during the test process. Therefore, the hard sample mining method is fateful to optimize the model and improve the learning efficiency. In this paper, an Adaptive Hard Sample Mining algorithm is proposed for training a robust person re-identification model. No need for hand-picking the images in the batch or designing the loss function for both positive and negative pairs, we can briefly calculate the hard level by comparing the prediction result with the true label of the sample. Meanwhile, taking into account the change in the number of samples required for the model during training process, an adaptive threshold of hard level can make the algorithm not only stay in step with training process harmoniously but also alleviate the under-fitting and over-fitting problem simultaneously. Besides, the designed network to implement the approach is very efficient and has good generalization performance that can be combined with various existing models readily. Experimental results on Market-1501, DukeMTMC-reID and CUHK03 datasets clearly demonstrate the effectiveness of the proposed algorithm. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 21	2020	382						259	267		10.1016/j.neucom.2019.11.094													
J								A sigmoid attractiveness based improved firefly algorithm and its applications in IIR filter design	CONNECTION SCIENCE										Firefly algorithm; evolutionary algorithms; function optimisation; IIR filter design	OPTIMIZATION ALGORITHM; PARAMETER-ESTIMATION; SYSTEM; SWARM; MODEL	Recently, a novel population-based optimisation algorithm, namely firefly algorithm (FA), which mimics the flashing and attraction behaviour of fireflies, has shown promising performance in solving global optimisation problems. However, the preliminary studies have shown that FA often gets stuck in local optima. In this paper, we investigate the reasons why the FA suffers from getting stuck in local optima; and then propose an improved firefly algorithm (IFA). These improvements are twofold: first, a sigmoid-based attractiveness is employed to reformulate its definition and strengthen its local refinement ability; second, a dynamic step parameter tuning strategy is designed to adjust the random search intensity and narrow the search space iteratively to strengthen its global search ability. The empirical results indicate IFA can well balance between the global exploration and the local exploitation, and provides the best solutions, at least the competitive results, for most of 12 global optimisation problems over other FA variants. Besides, by employing IFA to solve well-known infinite impulse response filter design problems, we evaluate the effectiveness and efficiency of IFA. The experimental results and comparisons show that IFA performs better than, at least as competent again, other meta-heuristics in terms of the solution accuracy, solution robustness, and convergence rate.																	0954-0091	1360-0494															10.1080/09540091.2020.1742660		MAR 2020											
J								Autonomous Data Density pruning fuzzy neural network for Optical Interconnection Network	EVOLVING SYSTEMS										Fuzzy neural networks; Autonomous data density; Optical interconnection network	EXTREME LEARNING-MACHINE; CLASSIFICATION; REGRESSION; MODEL	Traditionally, fuzzy neural networks have parametric clustering methods based on equally spaced membership functions to fuzzify inputs of the model. In this sense, it produces an excessive number calculations for the parameters' definition of the network architecture, which may be a problem especially for real-time large-scale tasks. Therefore, this paper proposes a new model that uses a non-parametric technique for the fuzzification process. The proposed model uses an autonomous data density approach in a pruned fuzzy neural network, wich favours the compactness of the model. The performance of the proposed approach is evaluated through the usage of databases related to the Optical Interconnection Network. Finally, binary patterns classification tests for the identification of temporal distribution (asynchronous or client-server) were performed and compared with state-of-the-art fuzzy neural-based and traditional machine learning approaches. Results demonstrated that the proposed model is an efficient tool for these challenging classification tasks.																	1868-6478	1868-6486															10.1007/s12530-020-09336-3		MAR 2020											
J								A multi-agent complex network algorithm for multi-objective optimization	APPLIED INTELLIGENCE										Multi-objective optimization; Cellular genetic algorithm; Complex network; Private archive; "Local-global" genetic operator	PARTICLE SWARM OPTIMIZATION; CELLULAR GENETIC ALGORITHM; EVOLUTIONARY ALGORITHM	To deal with the multi-objective optimization problems (MOPs), this study proposes a new Multi-Objective Multi-Agent Complex Network Optimization Algorithm called MOMCNA based on the idea of Cellular genetic algorithm (CGA) and the Multi-agent complex network. Compared with the traditional CGA for multi objective problem, the individuals in the population of MOMCNA have more features of intelligent agent, the new form of neighborhood for the population, private archive for individuals, the new strategy of "local-global" genetic operator and the chaotic mutation are proposed in the new algorithm to balance the convergence and diversity of the algorithm. Seventeen unconstrained multi-objective optimization problems and seven many-objective problems are introduced and tested to evaluate the new algorithm, in addition, the classical traffic assignment problem based on different system optimum principle is also established to evaluate the new algorithm. The comparison between MOMCNA and other classical algorithms shows that the proposed MOMCNA proves to be competitive in dealing with multi-objective and many-objective optimization problems and the structure of the complex network made up of population also has effect on algorithm's performance.																	0924-669X	1573-7497				SEP	2020	50	9					2690	2717		10.1007/s10489-020-01666-8		MAR 2020											
J								A BOA-based adaptive strategy with multi-party perspective for automated multilateral negotiations	APPLIED INTELLIGENCE										Multilateral negotiations; Bidding strategy; Opponent modeling; BOA framework	GROUP DECISION-MAKING; AGENTS COMPETITION; OPPONENT; MODEL	Determining an effective strategy for intelligent agents in multilateral negotiations is a more complicated problem than in bilateral negotiations. In order to achieve an optimal and beneficial agreement the agent needs to consider the behavior and desired utility of more than one opponent, determine a concession tactic based on a smaller agreement space, and use a computationally efficient mechanism for generating optimal offers. However, a mere extension of bilateral negotiation strategies cannot be effective in multilateral negotiations because the nature of most bilateral negotiation strategies is based on interaction with only one opponent and tracking a single behavior during the negotiation process. In this paper, we propose an adaptive approach based on a multi-party perspective to determine multilateral negotiation strategy. The proposed approach applies the BOA framework (Bidding, Opponent model, and Acceptance) and dynamically models the opponents' preference profiles. In order to estimate the obtainable utility from opponents and help find a good offer, the agent uses an ensemble model made by individual frequency-based opponent models and a different level of attention to each party's behavior. The proposed approach also implements a bidding strategy which applies the opponents' desirable utility to adapt the agent's concession tactic and produce appropriate offers. The results of experimental evaluations on various negotiation scenarios against the state of the art multilateral negotiation strategies show that our proposed strategy can provide superior performance in both individual utility and social welfare and lead to more optimal and fairer agreements.																	0924-669X	1573-7497				SEP	2020	50	9					2718	2748		10.1007/s10489-020-01646-y		MAR 2020											
J								An Intelligent Programmed Genetic Algorithm with advanced deterministic diversity creating operator using objective surface visualization	EVOLUTIONARY INTELLIGENCE										Evolutionary algorithm; Diversity; Multimodal; Visualized interactive GA; Intelligent programmed GA	VISUAL EXPLORATION EVALUATION; OPTIMIZATION	This paper presents a new fast Intelligent Programmed Genetic Algorithm (IPGA) based evolutionary optimization algorithm which requires lesser number of objective function evaluation for reaching optima. The proposed algorithm, apart from using probabilistic genetic operator, i.e. crossover and mutation, also uses a deterministic diversity creating operator for generating new solution in the current population. This is done by first projecting objective surface from higher dimension to lower dimension for visualization purpose and then deterministically generates new solution using some predefined rules in the region with higher objective function value. As the newly generated solution is in lower-dimensional space, these solutions are again projected back to higher dimensional space and then the objective function is evaluated at that point. The proposed IPGA is tested on three different categories of standard test functions viz. Unimodal function (2 Test Function), Unrotated Multimodal function (6 Test Function) and Rotated Multimodal function (5 Test Function). Simulation results were compared with that obtained using Binary Coded GA, Real Coded GA, recently proposed GA with Differential Evolution crossover operator (GA-DEx) and another success-history-based adaptive GA with aging mechanism (GA-aDEx(SPS)) in terms of mean and standard deviation of the objective function, average number of objective function evaluation required to reach optima and algorithmic complexity. Simulation results clearly demonstrate better performance of the proposed IPGA when compared with other variants of GAs.																	1864-5909	1864-5917				DEC	2020	13	4					705	723		10.1007/s12065-020-00385-w		MAR 2020											
J								Pix2Shape: Towards Unsupervised Learning of 3D Scenes from Images Using a View-Based Representation	INTERNATIONAL JOURNAL OF COMPUTER VISION										Computer vision; Differentiable rendering; 3D understanding; Adversarial training		We infer and generate three-dimensional (3D) scene information from a single input image and without supervision. This problem is under-explored, with most prior work relying on supervision from, e.g., 3D ground-truth, multiple images of a scene, image silhouettes or key-points. We propose Pix2Shape, an approach to solve this problem with four component: (i) an encoder that infers the latent 3D representation from an image, (ii) a decoder that generates an explicit 2.5D surfel-based reconstruction of a scene-from the latent code-(iii) a differentiable renderer that synthesizes a 2D image from the surfel representation, and (iv) a critic network trained to discriminate between images generated by the decoder-renderer and those from a training distribution. Pix2Shape can generate complex 3D scenes that scale with the view-dependent on-screen resolution, unlike representations that capture world-space resolution, i.e., voxels or meshes. We show that Pix2Shape learns a consistent scene representation in its encoded latent space, and that the decoder can then be applied to this latent representation in order to synthesize the scene from a novel viewpoint. We evaluate Pix2Shape with experiments on the ShapeNet dataset as well as on a novel benchmark we developed - called 3D-IQTT-to evaluate models based on their ability to enable 3d spatial reasoning. Qualitative and quantitative evaluation demonstrate Pix2Shape's ability to solve scene reconstruction, generation and understanding tasks.																	0920-5691	1573-1405				NOV	2020	128	10-11			SI		2478	2493		10.1007/s11263-020-01322-1		MAR 2020											
J								Smartphone based human activity monitoring and recognition using ML and DL: a comprehensive survey	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Human activity monitoring and recognition; Healthcare; Machine learning; Smartphone; Sensors	DEEP LEARNING APPROACH; MOBILE; INTERVENTIONS; ENSEMBLE; MACHINE	Human activity monitoring and recognition (HAMR) based on smartphone sensor data is a field that promotes a lot of observation in current era due to its notable desire in various Ambient Intelligent applications such as healthcare, sports, surveillance, and remote health monitoring. In this context, many research works have unveiled incredible results using various smartphone sensors such as accelerometer, gyroscope, magnetometer, digital compass, microphone, GPS and camera. The waveform of sensor motion is quite different in several smartphone placements even for the identical activity. This makes it challenging to apprehend varied completely different activities with high precision. Due to the difference in behavioral habits, gender and age, the movement patterns of various individuals vary greatly, which boosts the problem of dividing boundaries of various activities. In HAMR, the main computational tasks are quantitative analysis of human motion and its automatic classification. These cause the inception of Machine Learning (ML) and Deep Learning (DL) techniques to automatically recognize various human activity signals collected using smartphone sensors. This paper presents a comprehensive survey of smartphone sensor based human activity monitoring and recognition using various ML and DL techniques to address the above mentioned challenges. This study unveils the "research gaps in the field of HAMR, to provide the future research directions in HAMR.																	1868-5137	1868-5145															10.1007/s12652-020-01899-y		MAR 2020											
J								Detecting outlying variables in multigroup data: A comparison of different loading similarity coefficients	JOURNAL OF CHEMOMETRICS										component similarity; multivariate multigroup data; PCA; SCA; Tucker's congruence	SIMULTANEOUS COMPONENT ANALYSIS; CONGRUENCE COEFFICIENTS; ROTATION; MULTIBLOCK; MODELS; NUMBER; COMMON	Multivariate multigroup data are collected in many fields of science, where the so-called groups pertain to, for instance, experimental groups or countries the participants are nested in. To summarize the main information in such data, principal component analysis (PCA) is highly popular. PCA reduces the variables to a few components that are linear combinations of the original variables. Researchers usually assume those components to be the same across the groups and aim to apply a simultaneous component analysis. To investigate whether this assumption is reasonable, one often analyzes the groups separately and computes a similarity index between the group-specific component loadings of the variables. In many cases, however, most variables have highly similar loadings across the groups, but a few variables, which we will call "outlying variables," behave differently, indicating that a simultaneous analysis is not warranted. In such cases, the outlying variables should be removed before proceeding with the simultaneous analysis. To do so, the variables are ranked according to their relative outlyingness. Although some procedures have been proposed that yield such an outlyingness ranking, they might not be optimal, because they all rely on the same choice of similarity coefficient without evaluating other alternatives. In this paper, we give an overview of other options and report extensive simulations in which we investigate how this choice affects the correctness of the outlyingness ranking. We also illustrate the added value of the outlying variable approach by means of sensometric data on different bread samples.																	0886-9383	1099-128X														e3233	10.1002/cem.3233		MAR 2020											
J								Lexifield: a system for the automatic building of lexicons by semantic expansion of short word lists	KNOWLEDGE AND INFORMATION SYSTEMS										Semantic lexicon; Language resource; Unsupervised automatic system		We present Lexifield, a fully automatic language-independent system for building domain-specific lexicons from a short list of terms defining the domain. Lexifield relies on a pre-trained word embedding model, a definition dictionary and a dictionary of synonyms. To evaluate this system, four lexicons have been generated: one lexicon in French for the topic "son" ("sound") and three lexicons in English for the topics "sound", "taste" and "odour". As compared to other word embedding-based systems and a state-of-the-art sensorial lexicon, Sensicon, our system achieves better precision and recall on reference lists extracted from manually created resources such as Roget's Thesaurus.																	0219-1377	0219-3116				AUG	2020	62	8					3181	3201		10.1007/s10115-020-01451-6		MAR 2020											
J								Deep Learning Based Application for Indoor Scene Recognition	NEURAL PROCESSING LETTERS										Indoor scene recognition; Visually impaired people (VIP); Deep convolutional neural network (DCNN); Deep learning		Recognizing indoor scene and objects and estimating their poses present a wide range of applications in robotic field. This task becomes more challenging especially in cluttered environments like the indoor scenery. Scaling up convnets presents a key component in achieving better accuracy results of deep convolutional neural networks. In this paper, we make use of the rethinked efficient neural networks and we fine-tune them in order to develop a new application used for indoor object and scene recognition system. This new application will be especially dedicated for blind and visually impaired persons to explore new indoor environments and to fully integrate in daily life. The proposed indoor object and scene recognition system achieves new state-of-the-art results in MIT 67 indoor dataset and in scene 15 dataset. We obtained 95.60% and 97% respectively as a recognition rate.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2827	2837		10.1007/s11063-020-10231-w		MAR 2020											
J								Optimization of feedback bits using firefly algorithm for interference reduction in LTE femtocell networks	SOFT COMPUTING										Femtocell; Firefly algorithm; Modified dirty paper coding	BLOCK DIAGONALIZATION	Femtocells are the feasible solutions to extend the network coverage of indoor users and to enhance the network capacity in long-term evolution advanced (LTE-A)-based 5G networks. However, the femtocell base station shares the same frequency spectrum of microcell base station in unplanned manner. Hence, interference mitigation is a crucial problem in densely deployed femtocell environment and it is more severe with the deployment of femtocells in LTE-A network. In this paper, a modified dirty paper coding is proposed for interference mitigation along with the optimization of feedback bits using natural inspired meta-heuristic firefly algorithm. The proposed meta-heuristic algorithm reduces the interference by periodically unicasting the channel state information. Since the bandwidth of feedback system is limited, it is optimized in such a way that it does not affect the performance of the system. As compared to the conventional zero-forcing pre-coding, the proposed modified dirty paper coding along with firefly algorithm scheme offers improved sum rate of 70% and 64% with increase in the number of feedback bits and number of users, respectively.																	1432-7643	1433-7479				OCT	2020	24	20					15361	15371		10.1007/s00500-020-04871-2		MAR 2020											
J								Utility function for intelligent access web selection using the normalized fuzzy fractional entropy	SOFT COMPUTING										Utility function; Wireless heterogeneous; Measurement; Fractional calculus; Fractional fuzzy entropy		An intelligent access web selection must have an illustration of the favorites for making decisions. For this, we will use the theory of a utility function describing which situations are better for the costumers. Obviously, such utility is qualified and achieved all properties. We propose new single-pattern and multi-pattern utility procedures to best capture the user agreement and sensitivity facing up to a package of access web features. Mathematical propositions and numerical analysis slow down the ability and the practicality of our proposed simulations. For this purpose, we utilize a new methodology which is based on the concept of fractional entropy. This concept has a dominant effect on information theory.																	1432-7643	1433-7479															10.1007/s00500-020-04858-z		MAR 2020											
J								Semantic image segmentation with shared decomposition convolution and boundary reinforcement structure	APPLIED INTELLIGENCE										Convolutional neural networks; Semantic image segmentation; Shared decomposed convolution; Boundary reinforcement		Deep convolutional neural networks (DCNNs) have shown excellent performances in the field of computer vision. In this paper, we propose a new semantic image segmentation model, and the two hallmarks of our architecture are the usage of shared decomposition convolution (SDC) operation and boundary reinforcement (BR) structure. SDC operation can extract dense features and increase correlation of features in the same group, which can relieve the grid artifact problem. BR structure combines the spatial information from different layers in DCNNs to enhance the spatial resolution and enrich target boundary position information simultaneously. The simulation results show that the proposed model can achieve 94.6% segmentation accuracy and 76.3% mIOU on PASCAL VOC 2012 database respectively, which verifies the effectiveness of the proposed model.																	0924-669X	1573-7497				SEP	2020	50	9					2676	2689		10.1007/s10489-020-01671-x		MAR 2020											
J								Sentiment analysis: dynamic and temporal clustering of product reviews	APPLIED INTELLIGENCE										Sentiment analysis; Temporal clustering; Unsupervised learning; Ensemble learning; Contextual analysis; k-means algorithm	ENSEMBLE; STREAMS	The increased availability of online reviews requires a relevant solution to draw chronological insights from review streams. This paper introduces temporal sentiment analysis by adopting the automatic contextual analysis and ensemble clustering (ACAEC) algorithm. ACAEC is a clustering algorithm which utilizes contextual analysis and a clustering ensemble learning. We propose chronological sentiment analysis using window sequential clustering (WSC) and segregated window clustering (SWC). WSC is a dynamic analysis, whereas SWC is solely based on the temporal characteristic of reviews. ACAEC is the base learning algorithm of WSC and SWC. ACAEC's ensemble approach is enhanced using an additional weight scheme and an additional learner to improve WSC's outcome. To understand the produced sentiment pattern, an unsupervised review selection is introduced which is based on review polarity. We also introduce consistency, a free-label measure to assess the algorithm's performance. For this study, new sets of reviews are introduced, these being four airlines and an Australian property agent. In terms of accuracy and stability, the proposed methods are effective in processing a review series. Experiments show that the average accuracy rates of SWC and WSC reach 87.54% and 83.87%, respectively. In addition, it is robust against the so-called imbalanced windows problem. The suggested solutions are unsupervised i. e. domain-independent and suitable for the analysis of a large review series.																	0924-669X	1573-7497															10.1007/s10489-020-01668-6		MAR 2020											
J								Big tech and societal sustainability: an ethical framework	AI & SOCIETY										Big tech; Ethical criteria; Cognition; Social impacts; Centralized power; User activism		Sustainability is typically viewed as consisting of three forces, economic, social, and ecological, in tension with one another. In this paper, we address the dangers posed to societal sustainability. The concern being addressed is the very survival of societies where the rights of individuals, personal and collective freedoms, an independent judiciary and media, and democracy, despite its messiness, are highly valued. We argue that, as a result of various technological innovations, a range of dysfunctional impacts are threatening social and political stability. For instance, robotics and automation are replacing human labor and decision-making in a range of industries; search engines, monetized through advertising, have access to, and track, our interests and preferences; social media, in connecting us to one another often know more about us than we ourselves do, enabling them to profit in ways which may not coincide with our well-being; online retailers have not only acquired the ability to track and predict our buying choices, but also they can squeeze vendors based on their outsize bargaining power; and, in general, virtual technologies have changed both the way we think and our sense of self. With the rising deployment of the Internet of Things, and developments in machine learning and artificial intelligence, the threats to individual freedoms and rights, societal cohesion and harmony, employment and economic well-being, and trust in democracy are being ratcheted up. This paper lauds the benefits and addresses the harm wrought by the high tech giants in Information and Communication Technologies (ICTs). The search for rapidly growing revenues (and shareholder returns and stock prices) drives firms to accelerate product innovation without fully investigating the entire gamut of their impacts. As greater wealth accrues to the leaders of tech firms, inequalities within firms and societies are widening, creating social tensions and political ferment. We explore the ethical nature of the challenge employing a simple utilitarian calculus, complemented by approaches rooted in rights, justice, and the common good. Various options to address the challenges posed by ICTs are considered and evaluated. We argue that regulation may do little more than slow down the damage to society, particularly since societal values and political preferences vary internationally. Firms need to establish ethical standards, imbuing the upholders of these standards with sufficient authority, while creating a culture of morality. User involvement and activism, and shareholders' concerns for the sustainability of societies on whose continued prosperity they depend, are imperative to humanity's ability to decide the future direction of technology.																	0951-5666	1435-5655															10.1007/s00146-020-00956-6		MAR 2020											
J								Artificial intelligence, transparency, and public decision-making Why explanations are key when trying to produce perceived legitimacy	AI & SOCIETY										Artificial intelligence; Transparency; Public decision-making; Perceived legitimacy; Explainability; Framework	QUALITY; SCIENCE; MODEL	The increasing use of Artificial Intelligence (AI) for making decisions in public affairs has sparked a lively debate on the benefits and potential harms of self-learning technologies, ranging from the hopes of fully informed and objectively taken decisions to fear for the destruction of mankind. To prevent the negative outcomes and to achieve accountable systems, many have argued that we need to open up the "black box" of AI decision-making and make it more transparent. Whereas this debate has primarily focused on how transparency can secure high-quality, fair, and reliable decisions, far less attention has been devoted to the role of transparency when it comes to how the general public come to perceive AI decision-making as legitimate and worthy of acceptance. Since relying on coercion is not only normatively problematic but also costly and highly inefficient, perceived legitimacy is fundamental to the democratic system. This paper discusses how transparency in and about AI decision-making can affect the public's perception of the legitimacy of decisions and decision-makers and produce a framework for analyzing these questions. We argue that a limited form of transparency that focuses on providing justifications for decisions has the potential to provide sufficient ground for perceived legitimacy without producing the harms full transparency would bring.																	0951-5666	1435-5655															10.1007/s00146-020-00960-w		MAR 2020											
J								An adaptive hybrid algorithm for social networks to choose groups with independent members	EVOLUTIONARY INTELLIGENCE										Committee; Independence; Social network; Adaptive selection mechanism; Hybrid algorithm	COMMUNITY DETECTION; OPTIMIZATION	Choosing a committee with independent members in social networks can be named as a problem in group selection and independence in the committee is considered as the main criterion of this selection. Independence is calculated based on the social distance between group members. Although there are many solutions to solve the problem of group selection in social networks, such as selection of the target set or community detection, just one solution has been proposed to choose committee members based on their independence as a measure of group performance. In this paper, a new adaptive hybrid algorithm is proposed to select the best committee members to maximize the independence of the committees. This algorithm is a combination of particle swarm optimization algorithm with two local search algorithms. The goal of this work is to combine the exploration and the exploitation to improve the efficiency of the proposed algorithm and obtain the optimal solution. Additionally, to combine local search algorithms with particle swarm optimization, an effective selection mechanism is used to select a suitable local search algorithm to combine with particle swarm optimization during the search process. The results of experimental simulation are compared with the well-known and successful metaheuristic algorithms. This comparison shows that the proposed method improves the group independence by at least 21%.																	1864-5909	1864-5917				DEC	2020	13	4					695	703		10.1007/s12065-020-00384-x		MAR 2020											
J								Control the Diversity of Population with Mutation Strategy and Fuzzy Inference System for Differential Evolution Algorithm	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										DE; Diversity control; Fuzzy inference system; Mutation strategy; Entropy	GLOBAL OPTIMIZATION	This paper displays how to use fuzzy inference system (FIS) to control the individual uniform diversity for differential evolution algorithm (DE). DE solves nonlinear optimization problems, and a successful control mechanism for population diversity enhances the performance of DE. This study proposed a control mechanism that contains a novel mutation strategy and FIS because FIS is suitable for consecutive and hard classified inputs. The proposed control mechanism does not fix the target vector and controls the ratio of mutating toward the whole best individual by FIS. The FIS decides the F values for this novel mutation strategy. The experiments compared the winner of each evaluated functions among four uniform diversity goals (UDGs) with conventional strategies. From experimental results, the proposed method finds superior solutions to conventional mutation strategies at least 11 out of 15 evaluated functions in 10, 30, and 50 dimensions. Furthermore, not only the diversity curves confirm the control ability of FIS, but also different paths of convergence curves indicate the fast convergence and mitigation of evolutionary stagnation.																	1562-2479	2199-3211				SEP	2020	22	6			SI		1979	1992		10.1007/s40815-020-00823-y		MAR 2020											
J								Implementation of key predistribution scheme in WSN based on binary Goppa codes and Reed Solomon codes with enhanced connectivity and resiliency	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Goppa codes; Reed Solomon codes; Connectivity key; Communication key	WIRELESS SENSOR NETWORKS; PRE-DISTRIBUTION	Establishing secure transmissions of messages among the resource limited sensor nodes in wireless sensor network (WSN) is a critical issue and requires secret keys to be established among the communicating nodes. Key predistribution is the most commonly used technique and whereby secret keys are preloaded to the sensor nodes before their deployment into a hostile region. A WSN can be structured or unstructured, and sensor nodes may be deployed in an ad-hoc manner or pre-planned manner into the target field. As sensor nodes are low-cost electronic devices equipped with data processing, limited storage, communication and computation power, connectivity, and resiliency are the major focus in designing key predistribution scheme (KPS) for WSNs. Furthermore, we also expect the KPS to be scalable, enabling insertion of a set of new nodes in WSN at any point of time without altering the key setup of the already existing nodes. Combinatorial design is a widely used mathematical tool for the KPS. However, most of the KPS developed by using combinatorial design are not scalable. In this article, rather than using combinatorial techniques, we employ a code-based approach and design a new method for key predistribution by building a communication model and a connectivity model. We exploit the Reed Solomon code to establish our communication model, integrate the binary Goppa code to derive our connectivity model, and skillfully blend these two models to construct our code-based KPS. A C implementation of our KPS confirms the significant performance gain over the existing similar works. Additionally, nodes in our KPS are all self-dependent for communication and do not rely on cluster heads. Furthermore, we have control over the choice of the parameters in the underlying codes which makes our KPS flexible. To be specific, prior knowledge of additional node deployment increases the scalability of our connectivity model. By suitably choosing the parameters of the Goppa code at prior, we can accommodate extra nodes. More interestingly, our communication model is scalable without any previous knowledge of deployment.																	1868-5137	1868-5145															10.1007/s12652-020-01869-4		MAR 2020											
J								Biomimetic Energy-Based Humanoid Gait Design	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Humanoid gait; Energy-exchange; Energy-based control; Biomimetic bipedal gait	MECHANICAL ENERGY; MOTION; WALKING	One of the challenges facing humanoid robots is the design of a more human-like gait. In this paper, we propose a new paradigm for gait design for humanoids that is founded in the field of Kinesiology and is based on energy-exchange between potential and kinetic energies. Additionally, we propose an energy-based controller, which not only maintains the desired gait but is also more efficient than current controllers in terms of energy expenditure and joint motor torque exertion. Experiments were performed in simulation on Webots and on an actual humanoid platform, the Nao. Results indicate an improvement in mechanical energy consumption by 10% in simulations, and 1.8% on the Nao. Qualitatively, the proposed gait yielded motions that are more human-like.																	0921-0296	1573-0409				OCT	2020	100	1					203	221		10.1007/s10846-020-01179-z		MAR 2020											
J								Cooperative Multiple Task Assignment of Heterogeneous UAVs Using a Modified Genetic Algorithm with Multi-type-gene Chromosome Encoding Strategy	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Unmanned aerial vehicles; Cooperative task assignment; Genetic algorithm; Multi-type-gene chromosome encoding; Dubins car model		The cooperative multiple task assignment problem (CMTAP) of heterogeneous fixed-wing unmanned aerial vehicles (UAVs) performing the Suppression of Enemy Air Defense (SEAD) mission against multiple ground stationary targets is studied in this paper. The CMTAP is a NP-hard combinatorial optimization problem, which faces many challenges like problem scale, heterogeneity of UAVs (different capability and maneuverability), task coupling and task precedence constraints. To address this issue, we proposed a modified genetic algorithm (GA) with multi-type-gene chromosome encoding strategy. Firstly, the multi-type-gene encoding scheme is raised to generate feasible chromosomes that satisfy the UAV capability, task coupling and task precedence constraints. Then, Dubins car model is adopted to calculate the mission execution time (objective function of CMTAP model) of each chromosome, and make each chromosome conform to the UAV maneuverability constraint. To balance the searching ability of algorithm and the diversity of population, we raise the modified crossover operator and multiple mutation operators according to the multi-type-gene chromosome encoding. The simulation results demonstrate that the modified GA has better optimization performance compared with random search method, ant colony optimization method and particle search optimization method.																	0921-0296	1573-0409				NOV	2020	100	2					615	627		10.1007/s10846-020-01184-2		MAR 2020											
J								Blind Hexapod Locomotion in Complex Terrain with Gait Adaptation Using Deep Reinforcement Learning and Classification	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Deep; reinforcement; learning; locomotion; hexapod; neural	ROBOT	We present a scalable two-level architecture for Hexapod locomotion through complex terrain without the use of exteroceptive sensors. Our approach assumes that the target complex terrain can be modeled by N discrete terrain distributions which capture individual difficulties of the target terrain. Expert policies (physical locomotion controllers) modeled by Artificial Neural Networks are trained independently in these individual scenarios using Deep Reinforcement Learning. These policies are then autonomously multiplexed during inference using a Recurrent Neural Network terrain classifier conditioned on the state history, giving an adaptive gait appropriate for the current terrain. We perform several tests to assess policy robustness by changing various parameters, such as contact, friction and actuator properties. We also show experiments of goal-based positional control of such a system and a way of selecting several gait criteria during deployment, giving us a complete solution for blind Hexapod locomotion in a practical setting. The Hexapod platform and all our experiments are modeled in the MuJoCo [1] physics simulator. Demonstrations are available in the supplementary video.																	0921-0296	1573-0409				SEP	2020	99	3-4			SI		659	671		10.1007/s10846-020-01162-8		MAR 2020											
J								Modelling and Motion Analysis of a Pill-Sized Hybrid Capsule Robot	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Hybrid capsule robot; Capsule endoscopy; In-vivo diagnosis; Legged mode; Legless mode; Anchoring mode; Modelling; Medical robot		This paper presents a miniature hybrid capsule robot for minimally invasive in-vivo interventions such as capsule endoscopy within the GI (gastrointestinal) tract. It proposes new modes of operation for the hybrid robot namely hybrid mode and anchoring mode. The hybrid mode assists the robot to open an occlusion or to widen a narrowing. The anchoring mode enables the robot to stay in a specific place overcoming external disturbances (e.g. peristalsis) for a better and prolonged observation. The modelling of the legged, hybrid and anchoring modes are presented and analysed. Simulation results show robot propulsions in various modes. The hybrid capsule robot consisting four operating modes is more effective for the locomotion and observation within GI tract when compared to the locomotion consisting a single mean of locomotion as the hybrid robot can switch among the operating modes to suit the situation/task.																	0921-0296	1573-0409															10.1007/s10846-020-01167-3		MAR 2020											
J								2D Geometric Moment Invariants from the Point of View of the Classical Invariant Theory	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Pattern recognition; Feature extraction; Geometric moment invariants; Rotation group; Classical invariant theory; Kravchuk polynomials	PATTERN-RECOGNITION	The aim of this paper is to clear up the question of the connection between the geometric moment invariants and the invariant theory, considering a problem of describing the 2D geometric moment invariants as a problem of the classical invariant theory. We give a precise statement of the problem of computation of the 2D geometric invariant moments, introducing the notions of the algebras of simultaneous 2D geometric moment invariants, and prove that they are isomorphic to the algebras of joint SO(2)-invariants of several binary forms. Also, to simplify the calculating of the invariants, we proceed from an action of Lie group SO(2) to an action of its Lie algebra so2 Though the 2D geometric moments are not as effective as the orthogonal ones are, the author hopes that the results will be useful to the researchers in the fields of image analysis and pattern recognition.																	0924-9907	1573-7683				OCT	2020	62	8					1062	1075		10.1007/s10851-020-00954-9		MAR 2020											
J								Bio-inspired smart vision sensor: toward a reconfigurable hardware modeling of the hierarchical processing in the brain	JOURNAL OF REAL-TIME IMAGE PROCESSING										Biological vision; Pixel-level processing; Reconfigurability; Predictive coding; Attention module; Smart image sensor; FPGA; ASIC	VISUAL-CORTEX; IMAGE SENSOR; PARALLEL; MECHANISM; CHIP; V4	Biological vision systems inspire processing methods in computer vision applications. This paper employs the insights of vision systems in hardware and presents a pixel-parallel, reconfigurable, and layer-based hierarchical architecture for smart image sensors. The architecture aims to bring computation close to the sensor to achieve high acceleration for different machine vision applications while consuming low power. We logically divide the image into multiple regions and perform pixel-level and region-level processing after removing spatiotemporal redundancy. Those processors use bio-inspired algorithms to activate the regions with region of interest of a scene. The hierarchical processing breaks the traditional sequential image processing and introduces parallelism for machine vision applications. Also, we make the hardware design reconfigurable even after fabrication to make the hardware reusable for different applications. Simulation results show that the area overhead and power penalty for adding reconfigurable features stay in an acceptable range. We emphasize to maximize the operating speed and obtain 800 MHz. Besides, the design saves 84.01% and 96.91% dynamic power at the first and second stages of the hierarchy by removing redundant information. Furthermore, the sequential deployment of high-level reasoning only on the selected regions of the image becomes computationally inexpensive to execute a complex task in real time.																	1861-8200	1861-8219															10.1007/s11554-020-00960-5		MAR 2020											
J								Parallel hashing-based matching for real-time aerial image mosaicing	JOURNAL OF REAL-TIME IMAGE PROCESSING										Image stitching; Feature matching; CUDA; Binary descriptors		This paper presents a GPU-based real-time approach for generating high-definition (HD) aerial image mosaics. The cumbersome process of registering HD images is addressed by a parallel scheme that rapidly matches binary features. The proposed feature matcher takes advantage of the fast ORB (oriented FAST and rotated BRIEF) descriptor and its attainable arrangement into hash tables. By exploiting the best functionalities of binary descriptors and hashing-based data structures, the process of creating HD mosaics is accelerated. On average, real-time performance of 14.5 ms is achieved in a frame-to-frame process, for input images of 2.7 K resolution (2704 x 1521). For evaluation purposes in terms of robustness and speed, we selected two image registration methods for comparison. The first method uses the feature extractor and matcher modules of the well-known ORB-SLAM. The second comparison is carried out against the standard KNN-based matcher of OpenCV. The experiments were conducted under different conditions and scenarios, and the proposed approach exhibits a speed-up of 10.5 times compared to ORB-SLAM-based approach and 36.5 times compared to the OpenCV matcher. Therefore, this research widens the range of applications for aerial mosaicing, since the proposed system is capable of creating high-detail panoramas of large sites while acquiring data.																	1861-8200	1861-8219															10.1007/s11554-020-00959-y		MAR 2020											
J								Breadth search strategies for finding minimal reducts: towards hardware implementation	NEURAL COMPUTING & APPLICATIONS										Attribute reduction; Minimal reducts; Breadth search strategy; Hardware implementation; Field programmable gate arrays	COVERING DECISION SYSTEMS; ROUGH SET REDUCTS; ATTRIBUTE REDUCTION; KNOWLEDGE REDUCTION; FEATURE-SELECTION; DESIGN; APPROXIMATION; ACCELERATOR; ALGORITHM	Attribute reduction, being a complex problem in data mining, has attracted many researchers. The importance of this issue rises due to ever-growing data to be mined. Together with data growth, a need for speeding up computations increases. The contribution of this paper is twofold: (1) investigation of breadth search strategies for finding minimal reducts in order to emerge the most promising method for processing large data sets; (2) development and implementation of the first hardware approach to finding minimal reducts in order to speed up time-consuming computations. Experimental research showed that for software implementation blind breadth search strategy is in general faster than frequency-based breadth search strategy not only in finding all minimal reducts but also in finding one of them. An inverse situation was observed for hardware implementation. In the future work, the implemented tool is to be used as a fundamental module in a system to be built for processing large data sets.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14801	14816		10.1007/s00521-020-04833-7		MAR 2020											
J								Human body flexibility fitness test based on image edge detection and feature point extraction	SOFT COMPUTING										Physical flexibility fitness; Human body anteflexion angle; Edge detection; Feature point extraction	REACH TEST; HAMSTRING FLEXIBILITY; VALIDITY	This paper proposes a human flexibility fitness detection algorithm based on edge detection and feature point extraction. This algorithm first improves on the deficiency of the classical Canny operator. Specifically, a hybrid filter is used instead of the original Gaussian filter to improve filtering performance. Next, the templates in the 45 degrees and 135 degrees directions are added based on the original gradient calculation templates, and Otsu algorithm is used to achieve threshold segmentation to obtain the final edge information. Then, based on the obtained edge information, a human body feature point extraction algorithm for calculating the anteflexion angle is proposed, and the feature points of the shoulder, hip, and leg of the person are extracted, and the angle formed by these points is calculated. Size is used to achieve human body flexibility and fitness testing. In order to verify the effectiveness of the edge detection algorithm proposed in this paper, experiments are performed to compare with other algorithms, and the results show that the results of our algorithm are more accurate.																	1432-7643	1433-7479				JUN	2020	24	12			SI		8673	8683		10.1007/s00500-020-04869-w		MAR 2020											
J								Enhanced Salp Swarm Algorithm based on random walk and its application to training feedforward neural networks	SOFT COMPUTING										Metaheuristic search; Salp Swarm Algorithm; Random walk; Optimization techniques; Simulated annealing	GREY WOLF OPTIMIZER	Salp Swarm Algorithm (SSA) is a new type of metaheuristic and has shown superiority over other well-known algorithms such as Particle Swarm Optimization and Grey Wolf Optimizer in solving challenging optimization problems. Despite its superior performance, SSA still has problems such as insufficient convergence speed. Moreover, its local optima avoidance ability is not as good as those evolutionary algorithms using crossover operators. In this paper, we propose a modified Salp Swarm Algorithm (m-SSA) which improves the exploitation and exploration of SSA by integrating random walk strategy and especially enhances exploration by adding a new controlling parameter. In addition, a simulated annealing-type acceptance criterion is adopted to accept the fittest follower position as the new best leader position. The performance of the proposed algorithm is benchmarked on a set of classical functions and CEC2014 test suite. The proposed algorithm (m-SSA) outperforms SSA significantly on most test functions. When compared with other state-of-the-art metaheuristics, it also presents very competitive results. Besides, we apply the proposed algorithm on training feedforward neural networks (FNNs) and the results prove the effectiveness and efficiency of m-SSA.																	1432-7643	1433-7479				OCT	2020	24	19					14791	14807		10.1007/s00500-020-04832-9		MAR 2020											
J								Frugal innovation in supply chain cooperation considering e-retailer's platform value	SOFT COMPUTING										Supply chain management; Frugal innovation; Metcalfe's Law	REVENUE-SHARING CONTRACT; PRODUCT DESIGN; METCALFES LAW; COORDINATION; DECISIONS	E-retailers have recently paid close attention to frugal innovation in their supply chains. However, there are few studies on frugal innovation considering the e-retailer's platform value raised by the increase in the platform user scale. Motivated by industrial practice, we consider a supply chain consisting of an e-retailer and a manufacturer in the context of frugal innovation, where the platform value of the retailer is characterized by Metcalfe's law. We use game models to investigate the frugal product price and optimal degree of frugality decisions for the centralized, decentralized, retailer-led revenue-sharing contract and bargaining revenue-sharing contract scenarios. Our results indicate that the bargaining revenue-sharing contract can improve the frugal degree of the development-intensive frugal product (DIFP), while the frugal degree of the marginal cost-intensive frugal product (MIFP) cannot be improved by cooperation in the supply chain. We then compare the supply chain profit, the platform value of the retailer and the profit of the manufacturer in different scenarios. The results show that the centralized scenario is the optimal scenario where the platform retailer implements vertical integration strategy. Otherwise, establishing strategic partnership through the bargaining revenue-sharing contract is suboptimal for both the platform retailer and the manufacturer.																	1432-7643	1433-7479				OCT	2020	24	20					15373	15387		10.1007/s00500-020-04872-1		MAR 2020											
J								A hybrid grasshopper and new cat swarm optimization algorithm for feature selection and optimization of multi-layer perceptron	SOFT COMPUTING										Simple matching distance; Binary grasshopper optimization algorithm; New cat swarm optimization algorithm; Feature selection; Multi-layer perceptron	NEURAL-NETWORKS; SEARCH	The classification accuracy of a multi-layer perceptron (MLP) depends on the selection of relevant features from the data set, its architecture, connection weights and the transfer functions. Generating an optimal value of all these parameters together is a complex task. Metaheuristic algorithms are popular choice among researchers to solve complex optimization problems. This paper presents a hybrid metaheuristic algorithm simple matching-grasshopper new cat swarm optimization algorithm (SM-GNCSOA) that optimizes all the four components simultaneously. SM-GNCSOA uses grasshopper optimization algorithm, a new variant of binary grasshopper optimization algorithm called simple matching-binary grasshopper optimization algorithm and a new variant of cat swarm optimization algorithm called new cat swarm optimization algorithm to generate an optimal MLP. Features play a vital role in determining the classification accuracy of a classifier. Here, we propose a new feature penalty function and use it in SM-GNCSOA to prevent underfitting or overfitting due to the selected number of features. To evaluate the performance of SM-GNCSOA, different variants of SM-GNCSOA are proposed and their classification accuracies are compared with SM-GNCSOA on ten classification data sets. The results show that SM-GNCSOA gives better results on most of the data sets due to its capability to balance exploration and exploitation and to avoid local minima.																	1432-7643	1433-7479				OCT	2020	24	20					15463	15489		10.1007/s00500-020-04877-w		MAR 2020											
J								Single layer Chebyshev neural network model with regression-based weights for solving nonlinear ordinary differential equations	EVOLUTIONARY INTELLIGENCE										Chebyshev neural network; Regression based weights; Error back propagation algorithm; Ordinary differential equations	SYSTEM-IDENTIFICATION; NUMERICAL-SOLUTION; ALGORITHM; FREQUENCIES	In this investigation, a novel single layer Functional Link Neural Network namely, Chebyshev artificial neural network (ChANN) model with regression-based weights has been developed to handle ordinary differential equations. In ChANN, the hidden layer is removed by an artificial expansion block of the input patterns by using Chebyshev polynomials. Thus the technique is more effectual than the multilayer ANN. Initial weights from the input layer to the output layer are taken by a regression-based model. Here, feed-forward structure and back-propagation algorithm of the unsupervised version have been utilized to make the error values minimal. Numerical examples and comparisons with other methods exhibit the superior behavior of this technique.																	1864-5909	1864-5917				DEC	2020	13	4					687	694		10.1007/s12065-020-00383-y		MAR 2020											
J								Fuzzy logic and sub-clustering approaches to predict main cutting force in high-pressure jet assisted turning	JOURNAL OF INTELLIGENT MANUFACTURING										Fuzzy logic; Sub-clustering; Reduction rule; Pressure of the jet; Main cutting force; Inconel 718	SURFACE-ROUGHNESS; MULTIOBJECTIVE OPTIMIZATION; STAINLESS-STEEL; NEURAL-NETWORKS; INCONEL 718; TOOL LIFE; TEMPERATURE; MODEL; PERFORMANCE; PARAMETERS	Due to the complexity of the high-pressure jet assisted turning, knowledge, and prediction of the cutting forces are essential for the planning of machining operations for maximum productivity and quality. However, it is well known that during processing using this procedure there are difficulties in collecting data. It is required to establish an adequate model that would make it possible to predict the cutting force based on the input parameters. During machining to avoid difficulties in acquisition data, two models have developed based on fuzzy logic that will allow indirect monitoring of the cutting force. This research uses the improved fuzzy logic methods for modeling, whereby it can make predictions of the main cutting force according to the different input parameters. The contribution of this work reflected through the application of two innovative methods based on reducing the number of rules, which leads to better interpretability of models. First is the Mamdani with rule reduction method, and second is the Sugeno sub-clustering method based on the identification of the model structure, it comes down to finding the required number of rules by forming specific clusters. Both approaches differ by reducing the number of rules without affecting the accuracy of the models. The ability to predict the model determined by applying different statistical parameters. It concluded that Mamdani and Sugeno models give an approximate quality of the prediction. The resulting models also have an acceptable error to predict data that did not participate in their creation. Furthermore, obtained models can be used at the generalization stage where the cutting force information is required and where direct measurement is not possible.																	0956-5515	1572-8145															10.1007/s10845-020-01555-4		MAR 2020											
J								Multiobjective optimal power flow using interior search algorithm: A case study on a real-time electrical network	COMPUTATIONAL INTELLIGENCE										interior search algorithm; losses; optimal power flow; pollutant emission; production cost; voltage deviation and Tamil Nadu power system	OPTIMIZATION ALGORITHM; FIREFLY ALGORITHM; NONSMOOTH; DISPATCH; COST; EMISSION; LOSSES	Optimal power flow (OPF) is a vital concern in an electrical network. In consequence of the intricacy of the power systems, the conventional formulations are not adequate for current situation. Hence, in this study, the multiobjective OPF (MOOPF) problem has been modeled to diminish the production cost, environmental emission, and losses and to enhance the voltage stability and voltage profile simultaneously. This study proposes the application of interior search algorithm (ISA) for resolving MOOPF problem. The simulations have been carried out on three various test systems such as IEEE 30-bus system, IEEE 57-bus system, and Tamil Nadu Generation and Distribution Corporation Limited, as a real part of 62 bus Indian utility system (IUS) to infer the efficacy of ISA in solving the OPF problems. The simulation results have been compared with other techniques. The comparison shows that ISA is used in resolving MOOPF problems.																	0824-7935	1467-8640				AUG	2020	36	3					1078	1096		10.1111/coin.12312		MAR 2020											
J								A general-purpose distributed pattern mining system	APPLIED INTELLIGENCE										Pattern mining; Decomposition; Distributed computing; Heterogeneous architecture	FREQUENT ITEMSETS; PARALLEL; ALGORITHM; DISCOVERY	This paper explores five pattern mining problems and proposes a new distributed framework called DT-DPM: Decomposition Transaction for Distributed Pattern Mining. DT-DPM addresses the limitations of the existing pattern mining problems by reducing the enumeration search space. Thus, it derives the relevant patterns by studying the different correlation among the transactions. It first decomposes the set of transactions into several clusters of different sizes, and then explores heterogeneous architectures, including MapReduce, single CPU, and multi CPU, based on the densities of each subset of transactions. To evaluate the DT-DPM framework, extensive experiments were carried out by solving five pattern mining problems (FIM: Frequent Itemset Mining, WIM: Weighted Itemset Mining, UIM: Uncertain Itemset Mining, HUIM: High Utility Itemset Mining, and SPM: Sequential Pattern Mining). Experimental results reveal that by using DT-DPM, the scalability of the pattern mining algorithms was improved on large databases. Results also reveal that DT-DPM outperforms the baseline parallel pattern mining algorithms on big databases.																	0924-669X	1573-7497				SEP	2020	50	9					2647	2662		10.1007/s10489-020-01664-w		MAR 2020											
J								Collaborative filtering recommendation algorithm based on interval-valued fuzzy numbers	APPLIED INTELLIGENCE										Collaborative filtering; Interval-valued triangular fuzzy number; Confidence interval; Signed distance	DECISION-MAKING; MODEL; SIMILARITY; ACCURACY	Most collaborative filtering recommendation algorithms use crisp ratings to represent the users' preferences. However, users' preferences are subjective and changeable, crisp ratings can't measure the uncertainty of users' preferences effectively. In order to solve this problem, this paper proposes the interval-valued triangular fuzzy rating model. This model replaces crisp ratings with interval-valued triangular fuzzy numbers on the basis of users' rating statistics information, which can measure the users' preferences in a more reasonable way. Based on this model, the collaborative filtering recommendation algorithm based on interval-valued fuzzy numbers is designed. The algorithm calculates the users' similarity by the interval-valued triangular fuzzy numbers, and takes the ambiguity of ratings into consideration in the prediction stage. Our experiments prove that, compared with other fuzzy and traditional algorithms, our algorithm can increase the prediction precision and rank accuracy effectively with a little time cost, and has an obvious advantage when implemented in a sparse dataset which has more users than items. Thus our method has strong effectiveness and practicability.																	0924-669X	1573-7497				SEP	2020	50	9					2663	2675		10.1007/s10489-020-01661-z		MAR 2020											
J								Guided sampling for large graphs	DATA MINING AND KNOWLEDGE DISCOVERY										Big graphs; Graph sampling; Social networks	RANDOM-WALKS; NETWORKS	Large real-world graphs claim lots of resources in terms of memory and computational power to study them and this makes their full analysis extremely challenging. In order to understand the structure and properties of these graphs, we intend to extract a small representative subgraph from a big graph while preserving its topology and characteristics. In this work, we aim at producing good samples with sample size as low as 0.1% while maintaining the structure and some of the key properties of a network. We exploit the fact that average values of degree and clustering coefficient of a graph can be estimated accurately and efficiently. We use the estimated values to guide the sampling process and extract tiny samples that preserve the properties of the graph and closely approximate their distributions in the original graph. The distinguishing feature of our work is that we apply traversal based sampling that utilizes only the local information of nodes as opposed to the global information of the network and this makes our approach a practical choice for crawling online networks. We evaluate the effectiveness of our sampling technique using real-world datasets and show that it surpasses the existing methods.																	1384-5810	1573-756X				JUL	2020	34	4					905	948		10.1007/s10618-020-00683-y		MAR 2020											
J								Fuzzy Cognitive Mapping Analysis to Recommend Machine Learning-Based Effort Estimation Technique for Web Applications	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Effort estimation; Fuzzy cognitive maps; Machine learning; MSE; Fuzzy numbers	SOFTWARE-DEVELOPMENT EFFORT; NEURAL-NETWORK; SYSTEM; ISSUES; MAPS	Effort estimation is a fairly researched field in the area of software engineering. Algorithmic and non-algorithmic methods are the two popular ways of estimating software development efforts. Various machine learning techniques are also being used to determine project efforts based on the historical project-related dataset. These techniques consume an array of project characteristics to estimate the project cost. The selection of the right technique to correctly determine the project cost is a significant challenge that the software industry is facing. This paper presents a fuzzy cognitive mapping (FCM) approach to recommend the best machine learning-based software estimation technique for Web applications. FCM shows synergistic interactions between system variables, and this property is used in the context of Web application estimation for suggesting an estimation technique based on the Web project configuration. To counter the ambiguity in defining abstract relationships between system variables, this article also proposes to incorporate fuzzy numbers. The current analysis involves using five different estimation techniques on 125 student project records. The mean square error (MSE) was taken as a performance metric to declare the supremacy of one estimation technique over others. The experimental results show that the selection of an effort estimation technique should not ignore the presence of project characteristics in the input vector. The achievement of this work is that the proposed technique is capable of recommending the suitable most Web estimation model based on project credentials for a specific Web project; it refrains from suggesting an estimation model optimum for the most project configurations. The FCM approach on software estimation technique recommendation results in a probability of success equals to 70%.																	1562-2479	2199-3211				JUN	2020	22	4					1212	1223		10.1007/s40815-020-00815-y		MAR 2020											
J								Fuzzy Preprocessing and Clustering Analysis Method of Underwater Multiple Targets in Forward Looking Sonar Image for AUV Tracking	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										AUV; Multiple underwater targets; Fuzzy preprocessing; Clustering analysis		Marine sonar image noise absorption under water can result in poor image quality, low image resolution and blurred target contour. This paper proposes a systematic framework including underwater image fuzzy preprocessing method, and clustering based on the principal component analysis (PCA) of underwater targets. Underwater images were denoised adopting fast median filtering algorithm with mean acceleration. The fuzzing mathematics were not only developed with the aim of improving the visual quality of underwater sonar images, but also involved in the sonar image segmentation for extracting the target area from the suspicious area. Both the geometric and statistical features were treated as features in PCA algorithm. As demonstrated by the experimental results, visual quality of the sonar image was improved, multi-target threshold segmentation was achieved and multiple targets could be analyzed and clustered for stably tracking AUV.																	1562-2479	2199-3211				JUN	2020	22	4					1261	1276		10.1007/s40815-020-00832-x		MAR 2020											
J								Hand Gesture Recognition in Complex Background Based on Convolutional Pose Machine and Fuzzy Gaussian Mixture Models	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Human-computer interaction; Hand gesture recognition; Convolutional pose machine; Fuzzy Gaussian Mixture Models	TRACKING; SEGMENTATION	Hand gesture is one of the most intuitive and natural ways for human to communicate with computers, and it has been widely adopted in many human-computer interaction applications. However, it is still a challenging problem when confronted with complex background, illumination variation and occlusion in real-world scenarios. In this paper, a two-stage hand gesture recognition method is proposed to tackle these problems. At the first stage, hand pose estimation is developed to locate the hand keypoints using the convolutional pose machine, which can effectively localize hand keypoints even in a complex background. At the second stage, the Fuzzy Gaussian mixture models (FGMMs) are tailored to reject the nongesture patterns and classify the gestures based on the estimated hand keypoints. Extensive experiments are conducted to evaluate the performance of the proposed method, and the result demonstrates that the proposed algorithm is effective, robust, and satisfactory in real-time scenarios.																	1562-2479	2199-3211				JUN	2020	22	4					1330	1341		10.1007/s40815-020-00825-w		MAR 2020											
J								Formal concept analysis of multi-scale formal context	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Multi-scale formal context; Granular computing; Multi-scale formal decision context; Optimal scale selection	RELATIONAL CONCEPT ANALYSIS; OPTIMAL SCALE SELECTION; GRANULATION	The case that attributes have a single value is considered in the classical formal concept. However, real-life data involve multi-level attribute values, which means that attributes have different values on different scales. In order to deal with multi-level attributes, we introduce the concept of multi-scale formal context in this paper. In this context, with the change of scale, objects owned by each attribute change monotonously. Thus, under the multi-scale formal context, the change of formal concept is discussed. Moreover, this paper defines the consistent multi-scale formal decision context and discusses the optimal scale selection problem of the consistent multi-scale decision context.																	1868-5137	1868-5145															10.1007/s12652-020-01867-6		MAR 2020											
J								A miniaturized circular maze shaped antenna for implantable health care applications	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Circular maze; Implantable antenna; Biocompatible; Polyamide	PATCH ANTENNA; BIOMEDICAL TELEMETRY; DESIGN; BIOTELEMETRY	In this work, a novel miniaturized circular maze shaped implantable antenna has been proposed for medical applications to be operated in medical band. The biocompatible polyamide substrate (epsilon(r) = 4.3 and tan delta = 0.004) with 0.05 mm thickness has been used as both substrate and superstrate. The proposed antenna is featured with very good miniaturization with the dimensions of 7 x 7 x 0.1 mm(3) by employing circular maze shaped structure in radiator. The proposed antenna shows maximum gain of - 23 dBi in a skin phantom model simulation at the frequency of 2.45 GHz. A prototype of antenna has been fabricated and the measurements are conducted in a pork slab. The maximum specific absorption rate has been evaluated for health safety consideration. The obtained margins are in the safe boundary and they satisfy IEEE C95.1-1999 and C95.1-2005 safety rules. The proposed antenna shows noticeable performance improvement compared to the recently proposed antennas.																	1868-5137	1868-5145															10.1007/s12652-020-01884-5		MAR 2020											
J								Frequent mining analysis using pattern mining utility incremental algorithm based on relational query process	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Frequent mining; Candidate model; High utility analysis; Pattern mining; Machine learning	PRODUCT; HELPFULNESS; DESIGN	The problem of frequent pattern mining has been well studied and there exist numerous techniques in identifying a subset of pattern from large pattern set which represent the frequency of items. However, they suffer to identify the unique pattern which has higher frequency and importance throughout the data set. To handle this issue and to identify the optimal pattern in a relational database, an incremental utility based pattern mining algorithm is presented. The proposed frequent pattern utility incremental algorithm (FPUIA) uses the unstructured data and uses time series machine learning (TSL) approach to perform frequent analysis. The model is designed to identify the recurrent itemset and the pattern set is selected based on the support and confidence measures. Initially, frequent patterns are selected based on the minimum support and confidence where the next level pattern are generated based on the frequency of patterns in the selected set, which are measured iteratively. The proposed system produces high supportive measure to finding the relevance of frequent items from the real dataset as well as increasing the overall performance.																	1868-5137	1868-5145															10.1007/s12652-020-01880-9		MAR 2020											
J								Hardware implementation of fast bilateral filter and canny edge detector using Raspberry Pi for telemedicine applications	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Raspberry Pi; Bilateral filter; Edge detection; Segmentation; Telemedicine	SPIHT IMAGE COMPRESSION	The role of preprocessing and segmentation are vital in image processing and computer vision. The medical images are prone to noise and the filtering algorithms are used for noise removal. In this paper, the fast bilateral filter is employed for noise removal and it has good edge preservation capacity. The segmentation algorithms are used to extract the region of interest and edge detection is a classical algorithm for tracing the contours of objects in an image. The canny edge detector is efficient when compared with the conventional edge detectors. The fast bilateral filter is proposed in this paper has the computation complexity of O(1) per pixel, while the classical bilateral filter has the computation complexity of O(W) operations per pixel, where W is the kernel size. The algorithms were implemented in Raspberry Pi using Open CV software package. The algorithms were tested on real time medical images.																	1868-5137	1868-5145															10.1007/s12652-020-01871-w		MAR 2020											
J								Fast and Consistent Matching for Landmark-Based Place Recognition	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Place recognition; CNN; Landmark extraction; Markov random field; Spatial information	LOOP-CLOSURE DETECTION; LOCALIZATION; WORDS; BAGS	Severe condition changes make visual place recognition a difficult task for robotics applications. Landmark-based methods that combine local image regions and features from a convolution neural network (CNN) have shown state-of-the-art performance. This paper focuses on improving the performance of landmark-based methods for place recognition. To save time in landmark matching, a novel method is presented to extract landmarks based directly on the lower layer of CNN features instead of using object proposals or superpixels. The centers of landmarks are projected onto the feature maps of the higher layer of the CNN to generate compact descriptors. We then extract potential images through similarities computed by direct matching. In the refinement stage, we exploit the Markov random field (MRF), combined with Bayes filters, to perform robust matchings. This stage integrates the spatial information of landmarks and introduces a new method for similarity measurements. We present extensive experiments using challenging datasets and compare the results to state-of-the-art methods to evaluate our approach. The results show that our approach ensures a remarkable performance under severe condition changes and significantly improves efficiency.																	0921-0296	1573-0409				JUL	2020	99	1					115	128		10.1007/s10846-019-01108-9		MAR 2020											
J								Real-time automated video highlight generation with dual-stream hierarchical growing self-organizing maps	JOURNAL OF REAL-TIME IMAGE PROCESSING										Unsupervised learning; Video highlights; Skimming-based video highlights; GSOM; Optical flow	DISTANCE	Video has rapidly become one of the most common sources of visual information transfer. The number of videos uploaded to YouTube in a single day is estimated to take over 82 years to watch. Automated tools and techniques for analyzing and understanding video content, thus, have become an essential requirement. This paper addresses the problem of video highlight generation for large video files. We propose a novel skimming-based unsupervised video highlight generation method utilizing statistical image processing and data clustering, which process frame-level static and dynamic features of input video in two streams. The dynamic feature stream is represented by computing a dense optical flow for each consecutive frame, providing instantaneous velocity information for every pixel, which is then characterized by a per-frame orientation histogram, weighted by the norm, with orientations quantized. To process multi-scene videos, we utilize the divisive hierarchical clustering capability of growing self-organizing map (GSOM) using a dual-step top-down hierarchical approach in which the first level consists of clustering of spatial and temporal features of the video and in the second level, each parent cluster is hierarchically subdivided into child clusters using GSOM. The video highlight generation process is conducted real time by evaluating segments of video snippets based on a pre-defined time interval. We demonstrate the accuracy, robustness and the quality of highlights generated using a qualitative analysis conducted using 1625 human experts on highlights generated from two datasets. Further, we conduct a runtime analysis to demonstrate the efficient processing capability of the proposed method, to be used in real-time settings.																	1861-8200	1861-8219															10.1007/s11554-020-00957-0		MAR 2020											
J								A survey of state-of-the-art approaches for emotion recognition in text	KNOWLEDGE AND INFORMATION SYSTEMS										Human emotion; Emotion recognition in text; Explicit emotion; Implicit emotion	CIRCUMPLEX MODEL; APPRAISAL; CLASSIFICATION; EXTRACTION	Emotion recognition in text is an important natural language processing (NLP) task whose solution can benefit several applications in different fields, including data mining, e-learning, information filtering systems, human-computer interaction, and psychology. Explicit emotion recognition in text is the most addressed problem in the literature. The solution to this problem is mainly based on identifying keywords. Implicit emotion recognition is the most challenging problem to solve because such emotion is typically hidden within the text, and thus, its solution requires an understanding of the context. There are four main approaches for implicit emotion recognition in text: rule-based approaches, classical learning-based approaches, deep learning approaches, and hybrid approaches. In this paper, we critically survey the state-of-the-art research for explicit and implicit emotion recognition in text. We present the different approaches found in the literature, detail their main features, discuss their advantages and limitations, and compare them within tables. This study shows that hybrid approaches and learning-based approaches that utilize traditional text representation with distributed word representation outperform the other approaches on benchmark corpora. This paper also identifies the sets of features that lead to the best-performing approaches; highlights the impacts of simple NLP tasks, such as part-of-speech tagging and parsing, on the performances of these approaches; and indicates some open problems.																	0219-1377	0219-3116				AUG	2020	62	8					2937	2987		10.1007/s10115-020-01449-0		MAR 2020											
J								Interval-valued intuitionistic fuzzy TODIM method based on Schweizer-Sklar power aggregation operators and their applications to group decision making	SOFT COMPUTING										Soft computing; Fuzzy TODIM; Schweizer-Sklar power aggregation operators; Psychological behaviour; Group decision making	SUPPLIER SELECTION; EXTENDED TODIM; PROSPECT-THEORY; ALGORITHM; NUMBERS; SETS	The present work proposes a novel integrated group decision making framework for decision making in IVIF setting employing Schweizer-Sklar t-conorm and t-norm (SSTT) aggregation operators, power average (PA) operators and TODIM (an acronym in Portuguese of interactive and multiple attribute decision making) methods. The SSTT aggregation operators make the aggregation process more flexible and the PA operators relive the decision making process from unreasonable data arising out due to biased evaluations. The TODIM method, on the other hand, ranks the alternatives by giving due consideration to the psychological behaviour of the expert towards risk. The extension of PA and SSTT operators to IVIF give rise to the weighted averaging and weighted geometric operators and are referred to as IVIFSSPAWA and IVIFSSPAWG operators. These operators, on integration with the TODIM method, give rise to two group decision making frameworks and are referred to as IVIFSSPWA fuzzy TODIM and IVIFSSPWG fuzzy TODIM approaches. The applicability of the proposed methods has been demonstrated through five illustrative examples from different domains: material selection, personnel selection, supplier selection, facility evaluation and technology evaluation. A comprehensive evaluation of the proposed methods with the considered illustrative examples aided to reveal the strength and weaknesses of the proposed approaches. The comprehensive analysis shows that the proposed IVIFSSPWA fuzzy TODIM and IVIFSSPWG fuzzy TODIM methods are superior to the benchmarks considered in the present work and also other MCDM methods without SSTT and PA aggregation operators and those ignoring the criticality of risk attitude of the experts. The proposed methodology, therefore, enhances the way of dealing with fuzzy information and in a way provides an improvisation of current studies.																	1432-7643	1433-7479				SEP	2020	24	18			SI		14091	14133		10.1007/s00500-020-04783-1		MAR 2020											
J								Research and implementation of multi-object tracking based on vision DSP	JOURNAL OF REAL-TIME IMAGE PROCESSING										KCF; Detection; Tracking; MOT; Hungary algorithm; Vision DSP	REAL-TIME	This paper applies a pragmatic approach to study the real-time performance effect of software design methods for multiple object tracking (MOT) based on vision digital signal processing (vision DSP). The MOT system in the paper combines target detection, the Hungarian algorithm and the Kernel correlation filter (KCF) tracker. In addition, the MOT system needs to support multiway video streams, so higher speed and storage requirements are necessary for target tracking. Therefore, we carried out some studies on how to improve tracker speed performance and reduce system resource consumption under limited system resources. In the paper, we achieved the goal in two respects. Regarding the data processing, we studied how to efficiently process tracking data by utilizing the parallel characteristics of iDMA (integrated direct memory access) and a DSP core; and regarding the data storage, we proposed a time-sharing strategy to solve the DSP local memory (data RAM) usage issue for multiple tracking objects. In addition, regarding the software design, we propose a new strategy, which includes two levels of parallel computations: the frame-level parallel computations and the tracking object-level parallel computations. The experimental results show that the KCF tracking algorithm based on vision DSP achieves not only the desired real-time tracking speed but also the expected goal of system resource utilization. Our research methods also provide a reference for algorithm embedded applications in the field of computer vision.																	1861-8200	1861-8219															10.1007/s11554-020-00958-z		MAR 2020											
J								A computational formulation of distribution reducts in probabilistic rough set models	SOFT COMPUTING										Distribution reduct; Probabilistic rough set model; Conceptual formulation; Computational formulation; Attribute reduction	ATTRIBUTE REDUCTION; KNOWLEDGE GRANULATION; INFORMATION ENTROPY; 3-WAY DECISIONS; CO-ENTROPY; APPROXIMATIONS; PARTITIONS; BIREDUCTS; COVERINGS	Conceptual and computational formulations are the two sides of the theory of rough sets. Conceptual formulation focuses on the meaning and interpretation of the concepts. Computational formulation focuses on procedures and algorithms for constructing these notions. In probabilistic rough set models, a distribution reduct is defined as a minimal subset of attributes that preserves the probabilistic lower or upper approximations of all decision classes. The definition is a conceptual formulation that provides an essential understanding of distribution reducts, but it does not directly give a computationally efficient method. In this paper, we study the computational formulation of distribution reducts in probabilistic rough set models by constructing monotonic measures, resulting in a more efficient computational method. We first construct two monotonic measures called the probabilistic low and upper approximation distribution measures, respectively, from which the computational formulation of distribution reducts can be obtained. We then propose the granularity-based probabilistic low and upper approximation distribution measures to evaluate the significance of attributes more effectively. On this basis, we develop two algorithms for finding distribution reducts based on addition-deletion method and deletion method, respectively. Finally, the experimental results show the effectiveness of the proposed measures.																	1432-7643	1433-7479				MAY	2020	24	10					7093	7111		10.1007/s00500-020-04849-0		MAR 2020											
J								A hybrid multiverse optimisation algorithm based on differential evolution and adaptive mutation	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Multiverse optimisation algorithm; meta-heuristic algorithm; differential evolution; adaptive mutation; optimisation performance	INSPIRED ALGORITHM; VERSE OPTIMIZER; COLONY	Multiverse optimisation (MVO) algorithm is an excellent meta-heuristic algorithm based on laws of physics. However, it is easy to fall into local optimum when solving complex multimodal optimisation problems with high dimensions. The global optimisation performance of the algorithm is still unsatisfactory. In this paper, we propose a hybrid multiverse optimisation (DE-SMVO) algorithm. First, in order to increase the ability of information exchange and global exploration, a differential mutation strategy is introduced to improve the search equations of MVO based on the differential evolution (DE) theory, enabling the algorithm to explore more unknown space. In addition, the adaptive mutation is carried out on the current global optimal universe in each iteration process, and excellent mutation universe is retained, which enhance the exploitation ability and improve the convergence accuracy of MVO algorithm. In order to investigate the performance of the proposed DE-SMVO algorithm, it is evaluated on 23 benchmark functions in this paper. The experimental results prove that the optimisation performance is significantly better than that of the MVO algorithm, and its performance is also superior compared with nine state-of-the-art meta-heuristic optimisation algorithms.																	0952-813X	1362-3079															10.1080/0952813X.2020.1735532		MAR 2020											
J								Cognitive Template-Clustering Improved LineMod for Efficient Multi-object Pose Estimation	COGNITIVE COMPUTATION										Muller-Lyer illusion; Cognitive template-clustering; Brain-inspired computation; LineMod; 6D pose estimation		Various types of theoretical algorithms have been proposed for 6D pose estimation, e.g., the point pair method, template matching method, Hough forest method, and deep learning method. However, they are still far from the performance of our natural biological systems, which can undertake 6D pose estimation of multi-objects efficiently, especially with severe occlusion. With the inspiration of the Muller-Lyer illusion in the biological visual system, in this paper, we propose a cognitive template-clustering improved LineMod (CT-LineMod) model. The model uses a 7D cognitive feature vector to replace standard 3D spatial points in the clustering procedure of Patch-LineMod, in which the cognitive distance of different 3D spatial points will be further influenced by the additional 4D information related with direction and magnitude of features in the Muller-Lyer illusion. The 7D vector will be dimensionally reduced into the 3D vector by the gradient-descent method, and then further clustered by K-means to aggregately match templates and automatically eliminate superfluous clusters, which makes the template matching possible on both holistic and part-based scales. The model has been verified on the standard Doumanoglou dataset and demonstrates a state-of-the-art performance, which shows the accuracy and efficiency of the proposed model on cognitive feature distance measurement and template selection on multiple pose estimation under severe occlusion. The powerful feature representation in the biological visual system also includes characteristics of the Muller-Lyer illusion, which, to some extent, will provide guidance towards a biologically plausible algorithm for efficient 6D pose estimation under severe occlusion.																	1866-9956	1866-9964				JUL	2020	12	4					834	843		10.1007/s12559-020-09717-5		MAR 2020											
J								CG-Art: demystifying the anthropocentric bias of artistic creativity	CONNECTION SCIENCE										Art; creativity; computer; algorithm; CG-Art	COMPUTATIONAL CREATIVITY	The following aesthetic discussion examines in a philosophical-scientific way the relationship between computation and artistic creativity. Currently, there is a criticism about the possible artistic creativity that an algorithm could have. Supporting the above, the term computer-generated art (CG-Art) defined by Margaret Boden would seem to have no exponents yet. Moreover, it has been pointed out that, rather than a matter of primitive technological development, CG-Art would have in its very foundations the inability to exist. This, because art is considered as one of the most unique and exclusive human manifestations of our species. On the contrary, I propose that the denial of CG-Art has an anthropocentric bias. For this, I use the recent studies that, from the cognitive sciences, have been carried out on artistic creativity. In this way, I intend to convince the reader that behind the denial of the creative artistic capacity to the machines, a negationist mysticism of the current scientific advances necessarily lies.																	0954-0091	1360-0494															10.1080/09540091.2020.1741514		MAR 2020											
J								Optimal trained artificial neural network for Telugu speaker diarization	EVOLUTIONARY INTELLIGENCE										Speaker diarization; Feature extraction; Neural network; Lion algorithm; Artificial Bee Colony	RECOGNITION; LANGUAGE; GMM; MACHINES; SPEECH	Speaker indexing or diarization is the process of automatically partitioning the conversation involving multiple speakers into homogeneous segments and grouping together all the segments that correspond to the same speaker. So far, certain works have been done under this aspect; still, the need of accurate partitioning process gets lagged under certain criteria. With this in mind, this paper aims to introduce a new speaker indexing or diarization model (Telugu language) that initially involves Mel Frequency Cepstral coefficient based feature extraction. Subsequently, a new Optimized Artificial Neural Network (ANN) is introduced for clustering process. The novelty behind the clustering process is: the training of ANN takes place through optimization logic that updates the weight of ANN by a hybrid concept of Artificial Bee Colony (ABC) and Lion Algorithm (LA). Thereby, the proposed model is named as ANN-ABC-LA model. Finally, the performance of the proposed ANN-ABC-LA model is compared over the state-of-the-art models with respect to different performance measures.																	1864-5909	1864-5917				DEC	2020	13	4					631	648		10.1007/s12065-020-00378-9		MAR 2020											
J								Is There Anything New to Say About SIFT Matching?	INTERNATIONAL JOURNAL OF COMPUTER VISION										SIFT; sGLOH2; Quantization; Binary descriptors; Symmetric matching; Hierarchical cascade filtering; Deep descriptors; Keypoint patch orientation; Approximated overlap error	DESCRIPTORS; FEATURES; REPRESENTATION; PERFORMANCE; TEXTURE; WORLD	SIFT is a classical hand-crafted, histogram-based descriptor that has deeply influenced research on image matching for more than a decade. In this paper, a critical review of the aspects that affect SIFT matching performance is carried out, and novel descriptor design strategies are introduced and individually evaluated. These encompass quantization, binarization and hierarchical cascade filtering as means to reduce data storage and increase matching efficiency, with no significant loss of accuracy. An original contextual matching strategy based on a symmetrical variant of the usual nearest-neighbor ratio is discussed as well, that can increase the discriminative power of any descriptor. The paper then undertakes a comprehensive experimental evaluation of state-of-the-art hand-crafted and data-driven descriptors, also including the most recent deep descriptors. Comparisons are carried out according to several performance parameters, among which accuracy and space-time efficiency. Results are provided for both planar and non-planar scenes, the latter being evaluated with a new benchmark based on the concept of approximated patch overlap. Experimental evidence shows that, despite their age, SIFT and other hand-crafted descriptors, once enhanced through the proposed strategies, are ready to meet the future image matching challenges. We also believe that the lessons learned from this work will inspire the design of better hand-crafted and data-driven descriptors.																	0920-5691	1573-1405				JUL	2020	128	7					1847	1866		10.1007/s11263-020-01297-z		MAR 2020											
J								Semi-online Multi-people Tracking by Re-identification	INTERNATIONAL JOURNAL OF COMPUTER VISION										Multi-object tracking; Semi-online methods; Combinatory optimization; Deep learning	MULTIOBJECT TRACKING; MULTITARGET TRACKING; ENERGY MINIMIZATION	In this paper, we propose a novel semi-online approach to tracking multiple people. In contrast to conventional offline approaches that take the whole image sequence as input, our semi-online approach tracks people in a frame-by-frame manner by exploring the time, space and multi-camera relationship of detection hypotheses in the near future frames. We cast the multi-people tracking task as a re-identification problem, and explicitly account for objects' appearance changes and longer-term associations. We model our approach using a Multi-Label Markov Random Field, and introduce a fast alpha-expansion algorithm to solve it efficiently. To our best knowledge, this is the first semi-online approach achieved by re-identification. It yields very promising tracking results especially in challenging cases, such as scenarios of the crowded streets where pedestrians frequently occlude each other, scenes captured with moving cameras where objects may disappear and reappear randomly, and videos under changing illuminations wherein the appearances of objects are influenced.																	0920-5691	1573-1405				JUL	2020	128	7					1937	1955		10.1007/s11263-020-01314-1		MAR 2020											
J								Learning Multifunctional Binary Codes for Personalized Image Retrieval	INTERNATIONAL JOURNAL OF COMPUTER VISION										Image retrieval; Multi-task learning; Hashing	DEEP; REPRESENTATION; NETWORK	Due to the highly complex semantic information of images, even with the same query image, the expected content-based image retrieval results could be very different and personalized in different scenarios. However, most existing hashing methods only preserve one single type of semantic similarity, making them incapable of addressing such realistic retrieval tasks. To deal with this problem, we propose a unified hashing framework to encode multiple types of information into the binary codes by exploiting convolutional networks (CNNs). Specifically, we assume that typical retrieval tasks are generally defined in two aspects, i.e. high-level semantics (e.g. object categories) and visual attributes (e.g. object shape and color). To this end, our Dual Purpose Hashing model is trained to jointly preserve two kinds of similarities characterizing the two aspects respectively. Moreover, since images with both category and attribute labels are scarce, our model is carefully designed to leverage the abundant partially labelled data as training inputs to alleviate the risk of overfitting. With such a framework, the binary codes of new-coming images can be readily obtained by quantizing the outputs of a specific CNN layer, and different retrieval tasks can be achieved by using the binary codes in different ways. Experiments on two large-scale datasets show that our method achieves comparable or even better performance than those state-of-the-art methods specifically designed for each individual retrieval task while being more compact than the compared methods.																	0920-5691	1573-1405				SEP	2020	128	8-9			SI		2223	2242		10.1007/s11263-020-01315-0		MAR 2020											
J								CR-Net: A Deep Classification-Regression Network for Multimodal Apparent Personality Analysis	INTERNATIONAL JOURNAL OF COMPUTER VISION										Personality traits; Multimodal data; Convolutional neural networks; Classification-regression network; Bell Loss function	BIMODAL REGRESSION	First impressions strongly influence social interactions, having a high impact in the personal and professional life. In this paper, we present a deep Classification-Regression Network (CR-Net) for analyzing the Big Five personality problem and further assisting on job interview recommendation in a first impressions setup. The setup is based on the ChaLearn First Impressions dataset, including multimodal data with video, audio, and text converted from the corresponding audio data, where each person is talking in front of a camera. In order to give a comprehensive prediction, we analyze the videos from both the entire scene (including the person's motions and background) and the face of the person. Our CR-Net first performs personality trait classification and applies a regression later, which can obtain accurate predictions for both personality traits and interview recommendation. Furthermore, we present a new loss function called Bell Loss to address inaccurate predictions caused by the regression-to-the-mean problem. Extensive experiments on the First Impressions dataset show the effectiveness of our proposed network, outperforming the state-of-the-art.																	0920-5691	1573-1405				DEC	2020	128	12					2763	2780		10.1007/s11263-020-01309-y		MAR 2020											
J								Emotional characterization of children through a learning environment using learning analytics and AR-Sandbox	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Augmented reality; AR-Sandbox; Technology-enhanced learning; Learning analytics; Emotiv EPOC; Clustering; DBSCAN; Time series analysis; Emotional metrics	AUGMENTED REALITY; USER INTERFACES; MOTIVATION; FRAMEWORK; EDUCATION; STUDENTS; DESIGN; LEVEL	Identifying emotions experienced by students in a learning environment contributes to measuring the impact when technologies such as augmented reality (AR) are implemented in the educational field. The most frequent methods for collecting emotional metrics are questionnaires, surveys, and observations, but each of these processes lacks objectivity and veracity. For this reason, this study proposes, develops and tests a learning analytics scheme, based on the density-based spatial clustering of applications with noise algorithm, as a clustering technique with time series analysis, using the brain- computer interface device, Emotiv EPOC, as a way to collect emotional metrics. The above, in order to perform emotional behavior characterization by using AR in a learning environment through AR-Sandbox. The proposed method shows a clear inclination in the tendency of each emotion in each cluster, allowing classification of children during their interaction with the immersive environment, as well as the ability to distinguish each group of students.																	1868-5137	1868-5145															10.1007/s12652-020-01887-2		MAR 2020											
J								Ensemble approach for short term load forecasting in wind energy system using hybrid algorithm	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										STLF; Load forecasting; Hybrid algorithm; MAPE; RMSE; Artificial Neural Network (ANN)	SUPPORT VECTOR REGRESSION; ECHO STATE NETWORK; METAHEURISTIC ALGORITHM; SEARCH ALGORITHM; NEURAL-NETWORKS; MODEL; POWER; SPEED; MULTISTEP; SCHEME	The uncertainty problem in the resources is essential to mitigate and to improve the system operation in order to attain the load forecasting. Sometimes, the wind power saturation level is high on the grid side; therefore wind power prediction is essential to improve the efficiency, safety, economic and stable operation of the electrical grids. In the wind energy system, balancing supply and load demands is being considered as a challenging task and it can be compensated by means of wind power forecasting. This paper depicts an ensemble approach for short term load forecasting (STLF) by means of the hybrid algorithm in the wind energy system. The hybrid algorithm is a grouping of the Deep Neural Network (DNN) and Chicken Swarm Optimization (CSO). Initially, 24 h load data of the wind energy system is collected from the New England ISO for training the DNN network thereby analyzing the load forecasting. During the training period, the training error rate is minimized with the help of the CSO algorithm. After the training period, there arises a testing period that recognizes future loads by means of the proposed hybrid algorithm. Based on the above consideration, the load forecasting problem in the wind energy system is achieved. The efficacy of the proposed method is expressed by computing the statistical measures in terms of Root Mean Square Error (RMSE) and Mean Absolute Percentage Error (MAPE), respectively. The proposed method will be implemented in the Matlab/Simulink and its performances were compared with the existing methods such as DNN and ANN, respectively.																	1868-5137	1868-5145															10.1007/s12652-020-01866-7		MAR 2020											
J								Route optimization for efficient data transmission using time desire strategy in wireless network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Routing; Route; Spatial path; Time window; Desire strategy; Wireless network	AD-HOC NETWORKS; ALGORITHMS; PROTOCOL	The spatial routing has been breaking down for some issues of traffic systems, and there are numerous methodologies has been communicated about for the effect of route selection and travel time expectation, but suffer from the exactness and time complicated issues. We propose a novel approach which performs spatial routing of routes utilizing which a single path will be chosen. The proposed technique keeps up log about the traffic design at each time window for every base station focuses. We assess the traffic design at every connection point at each time outline utilizing which the route traffic factor will be assumed for each route accessible for different goal from a beginning stage. The strategy keeps up various data about the routing system like the quantity of connection focuses, some concerns at every connection point and the separation between the links concentrates, and the amount of nodes goes through the network. Every one of these components is utilized to process the traffic factor at specific traffic channel any period. Given listed traffic factor, we prepare the movement time in every way at various time window to choose the single route to achieve any destination in the network. The proposed approach has created productive outcomes in route choice and travel time expectation.																	1868-5137	1868-5145															10.1007/s12652-020-01819-0		MAR 2020											
J								On defending against label flipping attacks on malware detection systems	NEURAL COMPUTING & APPLICATIONS										Adversarial machine learning (AML); Semi-supervised defense (SSD); Malware detection; Adversarial example; Label flipping attacks; Deep learning		Label manipulation attacks are a subclass of data poisoning attacks in adversarial machine learning used against different applications, such as malware detection. These types of attacks represent a serious threat to detection systems in environments having high noise rate or uncertainty, such as complex networks and Internet of Thing (IoT). Recent work in the literature has suggested using the K-nearest neighboring algorithm to defend against such attacks. However, such an approach can suffer from low to miss-classification rate accuracy. In this paper, we design an architecture to tackle the Android malware detection problem in IoT systems. We develop an attack mechanism based on silhouette clustering method, modified for mobile Android platforms. We proposed two convolutional neural network-type deep learning algorithms against this Silhouette Clustering-based Label Flipping Attack. We show the effectiveness of these two defense algorithms-label-based semi-supervised defense and clustering-based semi-supervised defense-in correcting labels being attacked. We evaluate the performance of the proposed algorithms by varying the various machine learning parameters on three Android datasets: Drebin, Contagio, and Genome and three types of features: API, intent, and permission. Our evaluation shows that using random forest feature selection and varying ratios of features can result in an improvement of up to 19% accuracy when compared with the state-of-the-art method in the literature.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14781	14800		10.1007/s00521-020-04831-9		MAR 2020											
J								Cloud customers service selection scheme based on improved conventional cat swarm optimization	NEURAL COMPUTING & APPLICATIONS										Cloud computing; Scheduling; Cat swarm optimization; Pareto dominance	SCHEDULING ALGORITHM; DIFFERENTIAL EVOLUTION; FUZZY; TIME; ALLOCATION; WORKFLOWS; QUALITY; TASKS	With growing demand on resources situated at the cloud datacenters, the need for customers' resource selection techniques becomes paramount in dealing with the concerns of resource inefficiency. Techniques such as metaheuristics are promising than the heuristics, most especially when handling large scheduling request. However, addressing certain limitations attributed to the metaheuristic such as slow convergence speed and imbalance between its local and global search could enable it become even more promising for customers service selection. In this work, we propose a cloud customers service selection scheme called Dynamic Multi-Objective Orthogonal Taguchi-Cat (DMOOTC). In the proposed scheme, avoidance of local entrapment is achieved by not only increasing its convergence speed, but balancing between its local and global search through the incorporation of Taguchi orthogonal approach. To enable the scheme to meet customers' expectations, Pareto dominant strategy is incorporated providing better options for customers in selecting their service preferences. The implementation of our proposed scheme with that of the benchmarked schemes is carried out on CloudSim simulator tool. With two scheduling scenarios under consideration, simulation results show for the first scenario, our proposed DMOOTC scheme provides better service choices with minimum total execution time and cost (with up to 42.87%, 35.47%, 25.49% and 38.62%, 35.32%, 25.56% reduction) and achieves 21.64%, 18.97% and 13.19% improvement for the second scenario in terms of execution time compared to that of the benchmarked schemes. Similarly, statistical results based on 95% confidence interval for the whole scheduling scheme also show that our proposed scheme can be much more reliable than the benchmarked scheme. This is an indication that the proposed DMOOTC can meet customers' expectations while providing guaranteed performance of the whole cloud computing environment.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14817	14838		10.1007/s00521-020-04834-6		MAR 2020											
J								Solution of heterogeneous multi-attribute case-based decision making problems by using method based on TODIM	SOFT COMPUTING										Case-based decision theory; TODIM; Heterogeneous multi-attribute case-based decision making; Case similarity; Solution similarity	INTUITIONISTIC FUZZY; CASE RETRIEVAL; SUPPLIER SELECTION; PROSPECT-THEORY; VALUES	Heterogeneous multi-attribute case-based decision making (HMCBDM) is a type of complex and important problem encountered in many decision applications, and such problems involve real numbers, interval numbers, triangular fuzzy numbers, linguistic variables, and intuitionistic fuzzy numbers. However, heterogeneous multi-attributes are not fully considered in case-based decision making. Moreover, in the decision making process, the psychological behavior of the decision maker must be considered. To this end, this paper proposes a novel method for HMCBDM problems based on the TODIM (an acronym in Portuguese for interactive and multi-criteria decision making) method. First, the problem similarity and solution similarity are formulated. Next, a similar case set is constructed according to the problem and solution similarities. Further, the utility of the result of the historical solution is calculated. In addition, a ranking method based on TODIM is proposed, in which the problem similarity, solution similarity, and utility of the result of the historical solution are considered, and subsequently, the suitable solution(s) is obtained. Finally, a case study considering a gas explosion and pertaining to decision making in emergency situations is described to illustrate the use of the proposed method.																	1432-7643	1433-7479				MAY	2020	24	10					7081	7091		10.1007/s00500-020-04844-5		MAR 2020											
J								On the analysis of hyper-parameter space for a genetic programming system with iterated F-Race	SOFT COMPUTING										Hyper-parameter optimization; Iterated F-Race; Genetic programming		Evolutionary algorithms (EAs) have been with us for several decades and are highly popular given that they have proved competitive in the face of challenging problems' features such as deceptiveness, multiple local optima, among other characteristics. However, it is necessary to define multiple hyper-parameter values to have a working EA, which is a drawback for many practitioners. In the case of genetic programming (GP), an EA for the evolution of models and programs, hyper-parameter optimization has been extensively studied only recently. This work builds on recent findings and explores the hyper-parameter space of a specific GP system called neat-GP that controls model size. This is conducted using two large sets of symbolic regression benchmark problems to evaluate system performance, while hyper-parameter optimization is carried out using three variants of the iterated F-Race algorithm, for the first time applied to GP. From all the automatic parametrizations produced by optimization process, several findings are drawn. Automatic parametrizations do not outperform the manual configuration in many cases, and overall, the differences are not substantial in terms of testing error. Moreover, finding parametrizations that produce highly accurate models that are also compact is not trivially done, at least if the hyper-parameter optimization process (F-Race) is only guided by predictive error. This work is intended to foster more research and scrutiny of hyper-parameters in EAs, in general, and GP, in particular.																	1432-7643	1433-7479				OCT	2020	24	19					14757	14770		10.1007/s00500-020-04829-4		MAR 2020											
J								Quantum-behaved particle swarm optimization with generalized space transformation search	SOFT COMPUTING										Opposition-based learning; Particle swarm optimization; Space transformation; Numerical optimization	HARMONY SEARCH; ALGORITHM	Nature-inspired algorithms have been proved to be very powerful methods for complex numerical optimization problems. Quantum-behaved particle swarm optimization (QPSO) is a typical member of nature-inspired algorithms, and it is a simple and effective population-based technique used in numerical optimization. Despite its efficiency and wide use, QPSO suffers from premature convergence and poor balance between exploration and exploitation in solving complex optimization problems. To address these issues, a new evolutionary technique called generalized space transformation search is proposed, and then, we introduce an improved quantum-behaved particle swarm optimization algorithm combined with this new technique in this study. The proposed generalized space transformation search is based on opposition-based learning and generalized opposition-based learning, which can not only improve the exploitation of the current search space but also strengthen the exploration of the neighborhood of the current search space. The improved quantum-behaved particle swarm optimization algorithm employs generalized space transformation search for population initialization and generation jumping. A comprehensive set of 16 well-known unconstrained benchmark functions is employed for experimental verification. The contribution of the generalized space transformation search is empirically verified, and the influence of dimensionality is also investigated. Besides, the improved quantum-behaved particle swarm optimization algorithm is also compared with some typical extensions of QPSO and several competitive meta-heuristic algorithms. Such comparisons suggest that the improved quantum-behaved particle swarm optimization algorithm may lead to finding promising solutions compared to the other algorithms.																	1432-7643	1433-7479				OCT	2020	24	19					14981	14997		10.1007/s00500-020-04850-7		MAR 2020											
J								Quasi-closed-form solution and numerical method for currency option with uncertain volatility model	SOFT COMPUTING										Currency option pricing; Uncertain volatility model; Runge-Kutta-99 hybrid method; Uncertain differential equation; Uncertainty theory	MEAN REVERSION; STOCK MODEL; RATES	There exist some non-stochastic factors in the financial market, so the dynamics of the exchange rate highly depends on human uncertainty. This paper investigates the pricing problems of foreign currency options under the uncertain environment. First, we propose an currency model under the assumption that exchange rate, volatility, domestic interest rate and foreign interest rate are all driven by uncertain differential equations; especially, the exchange rate exhibits mean reversion. Since the analytical solutions of nested uncertain differential equations cannot always be obtained, we design a new numerical method, Runge-Kutta-99 hybrid method, for solving nested uncertain differential equations. The accuracy of the designed numerical method is investigated by comparison with the analytical solution. Subsequently, the quasi-closed-form solutions are derived for the prices of both European and American foreign currency options. Finally, in order to illustrate the rationality and the practicability of the proposed currency model, we design several numerical algorithms to calculate the option prices and analyze the price behaviors of foreign currency options across strike price and maturity.																	1432-7643	1433-7479				OCT	2020	24	19					15041	15057		10.1007/s00500-020-04854-3		MAR 2020											
J								Effective long short-term memory with fruit fly optimization algorithm for time series forecasting	SOFT COMPUTING										Long short-term memory; Fruit fly optimization algorithm; Time series forecasting	DIFFERENTIAL EVOLUTION ALGORITHM; NEURAL-NETWORK; HYBRID ARIMA; ANN MODEL; LSTM; DEMAND; PRICE; REGRESSION; PREDICTION; INDEX	A number of recent studies have adopted long short-term memory (LSTM) in extensive applications, such as handwriting recognition and time series prediction, with considerable success. However, the parameters of LSTM have greatly influenced its accuracy and performance. In this study, LSTM with fruit fly optimization algorithm (FOA), called FOA-LSTM, is designed to solve time series problems. As a novel intelligent algorithm, FOA is applied to decide on the optimal hyper-parameter of LSTM. Experiments under the NN3 time series, three comparative experiments and the monthly energy consumption of the USA are conducted to verify the effectiveness of the FOA-LSTM model. The results indicate that the symmetric mean absolute percentage error (SMAPE) is reduced by up to 11.44% in the last 11 monthly series in the NN3 dataset. Four comparative experiments and the real-life series verify further that the FOA-LSTM model obtains a better result compared with other forecasting models.																	1432-7643	1433-7479				OCT	2020	24	19					15059	15079		10.1007/s00500-020-04855-2		MAR 2020											
J								An improved particle swarm optimization with clone selection principle for dynamic economic emission dispatch	SOFT COMPUTING										Dynamic economic emission dispatch; Particle swarm optimization; Clonal selection principle; Strong constraints; Hybrid mutation	DIFFERENTIAL EVOLUTION; ALGORITHM; PSO	In this paper, an improved particle swarm optimization algorithm (PSOCS) that integrates with a clone selection (CS) principle of artificial immune system is proposed to solve dynamic economic emission dispatch (DEED) problem. Classical particle swarm optimization method is easy to fall into stagnation when no particle discovers a position that is better than its previous best position. To overcome the disadvantage, the CS mechanism is used to evolve the personal best swarm (i.e., P-best) at every generation. The fittest particles in P-best will be cloned independently and proportionally to their fitness. In order to force PSOCS jump out of stagnation, a hybrid mutation scheme (called R/1orCB/1) is developed to mutate the clones generated. A constrain-handling approach is utilized to repair infeasible solutions for enhancing the ability of adapting to the DEED problem with various strong constraints. In numerical experiments, the proposed PSOCS is applied to solve three test cases (5-unit, 10-unit, and 15-unit systems) with nonsmooth fuel cost and emission functions. Simulation results indicate that the PSOCS can find the high-quality solutions for the DEED problem, when compared with the most recent methods reported in the literature.																	1432-7643	1433-7479				OCT	2020	24	20					15249	15271		10.1007/s00500-020-04861-4		MAR 2020											
J								Enhancing artificial bee colony algorithm using refraction principle	SOFT COMPUTING										Evolutionary algorithms; Artificial bee colony algorithm; Opposition-based learning; Refraction principle	OPTIMIZATION ALGORITHM; DESIGN	The artificial bee colony algorithm (ABC), as one of the excellent intelligent optimization technologies, has presented very good optimization performance for many complex problems due to its simplicity and easiness of implementation. However, ABC has a very good performance at exploration relatively, but for some complex problems it still results in slower convergent speed and lower convergent accuracy in the later stage of algorithms. Meanwhile, ABC has relatively poor performance at exploitation. To overcome these drawbacks further, the enhancing ABC algorithm using refraction principle is proposed (EABC-RP) in this paper. In EABC-RP, on the one hand, in order to enhance its exploration further, the unified opposition-based learning (UOBL) based on refraction principle is employed to generate refraction solutions (new food sources) for employed bees, which helps to increase population diversity and guide search direction close to the global optimal solution. On the other hand, for exploitation, when ABC has fallen into the local optimal solution, the UOBL based on refraction principle is employed for mutation to increase the probability of jumping out of the local optimal solution for scout bees. A lot of experiments are conducted on 23 benchmark functions to verify the effectiveness of EABC-RP. The experimental results show that EABC-RP achieves higher solution accuracy and faster convergent speed in most cases and outperforms other ABC variants. In addition, EABC-RP is used to optimize finite impulse response (FIR) low-pass digital filter which obtains the better filtering performance, which validates the effectiveness of the EABC-RP algorithm further.																	1432-7643	1433-7479				OCT	2020	24	20					15291	15306		10.1007/s00500-020-04863-2		MAR 2020											
J								Global exponential stability of anti-periodic solutions for discontinuous Cohen-Grossberg neural networks with time-varying delays	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Anti-periodic solution; Cohen-Grossberg neural network; discontinuous activations; global exponential stability	CRITERIA; EXISTENCE	This paper presents a class of Cohen-Grossberg neural networks (CGNNs) with discontinuous activations and time-varying delays. Firstly, under the framework of Filippov solution, we derive some general sufficient conditions to guarantee the global existence of the solutions to the proposed CGNNs with discontinuous activations and time-varying delays. Then, by constructing the new Lyapunov-Krasovskii functional, some new sufficient criteria are given to ascertain the globally exponential stability of the anti-periodic solution for the considered CGNNs with discontinuous activations and time-varying delays. To the authors' knowledge, the results established in the paper are the only available results on the anti-periodic for the discontinuous CGNNs; some previously known results are extended and complemented. Finally, simulation results of two typical numerical examples are also delineated to demonstrate the effectiveness of our theoretical results.																	0952-813X	1362-3079															10.1080/0952813X.2020.1737244		MAR 2020											
J								Modelling material flow using the Milk run and Kanban systems in the automotive industry	EXPERT SYSTEMS										Kanban; material flow; Milk run; particle swarm optimization	VEHICLE-ROUTING PROBLEM; OPTIMIZATION; PARTS	Material flow management refers to the analysis and specific optimization of the inventory-production system. Material flow can be characterized as the organized flow of material in a production process with the required sequence determined by a technological procedure. TheMilk run systemassures the transportation of materials at the right time and in an optimal manner. It should be combined with the Kanban system to highlight when something is required in the production process. This paper presents biological swarm intelligence, in general, and a particular model, particle swarm optimization (PSO), for modelling material flow using aMilk run systemsupported by a Kanban system in the automotive industry. The aim of this study is to create a new model for the optimal number of trailers for one train and optimal number of containers in a tugger train system when the route time period has been defined. A new modified PSO approach for integrating inventory-production in a unique optimization model is used. The major modification to the original PSO is using the capacity of a container instead of a velocity component. Each new Kanban trigger is checked, and the total timing for theMilk rundelivery solution is calculated for the necessary raw material capacity for each shop floor.																	0266-4720	1468-0394														e12546	10.1111/exsy.12546		MAR 2020											
J								Robustness Through Simplicity: A Minimalist Gateway to Neurorobotic Flight	FRONTIERS IN NEUROROBOTICS										drones; miniature aerial vehicles; spiking neural network; PID control; flight simulator		In attempting to build neurorobotic systems based on flying animals, engineers have come to rely on existing firmware and simulation tools designed for miniature aerial vehicles (MAVs). Although they provide a valuable platform for the collection of data for Deep Learning and related AI approaches, such tools are deliberately designed to be general (supporting air, ground, and water vehicles) and feature-rich. The sheer amount of code required to support such broad capabilities can make it a daunting task to adapt these tools to building neurorobotic systems for flight. In this paper we present a complementary pair of simple, object-oriented software tools (multirotor flight-control firmware and simulation platform), each consisting of a core of a few thousand lines of C++ code, that we offer as a candidate solution to this challenge. By providing a minimalist application programming interface (API) for sensors and PID controllers, our software tools make it relatively painless for engineers to prototype neuromorphic approaches to MAV sensing and navigation. We conclude our discussion by presenting a simple PID controller we built using the popular Nengo neural simulator in conjunction with our flight-simulation platform.																	1662-5218					MAR 16	2020	14								16	10.3389/fnbot.2020.00016													
J								Implementation of adaptive integration method for free energy calculations in molecular systems	PEERJ COMPUTER SCIENCE										Adaptive integration; Monte Carlo; Free energy; Solvation; Protein; Biomolecule	PROTEIN-STRUCTURE; LAMBDA-DYNAMICS; FORCE-FIELD; BINDING; SIMULATION; SOLVATION; P53	Estimating free energy differences by computer simulation is useful for a wide variety of applications such as virtual screening for drug design and for understanding how amino acid mutations modify protein interactions. However, calculating free energy differences remains challenging and often requires extensive trial and error and very long simulation times in order to achieve converged results. Here, we present an implementation of the adaptive integration method (AIM). We tested our implementation on two molecular systems and compared results from AIM to those from a suite of other methods. The model systems tested here include calculating the solvation free energy of methane, and the free energy of mutating the peptide GAG to GVG. We show that AIM is more efficient than other tested methods for these systems, that is, AIM results converge to a higher level of accuracy and precision for a given simulation time.																	2376-5992					MAR 16	2020									e264	10.7717/peerj-cs.264													
J								A hermeneutics of scientific practices and the concept of "text"	AI & SOCIETY										Readable technologies; Material hermeneutics; Scientific practices; Interrelatedness of configured practices; Interpretive internalism; Heelan; Ihde		This paper discusses a version of the hermeneutic philosophy of science. Special focus is placed on the ways of reading theoretical objects in scientific inquiry. In implementing readable technologies, this reading succeeds in contextually visualizing the theoretical objects by means of various sorts of signs. A configuration of readable technology accomplishes a further step. The configuration textualizes the contextually produced signs. Textualizing the reading of theoretical objects interlaces the meaningful articulation and objectification of scientific domains. The horizon of possibilities for textualizing is constantly shifting in the process of normal-scientific inquiry, and the shifting horizon plays the role of a hermeneutic fore-structuring of the outcomes of textualizing. The paper explores the importance of "material hermeneutics" for the contextual reading of theoretical objects. The conclusion is drawn that the hermeneutic study of the entanglement of technological artifacts with the outcomes from reading-as-textualizing requires the introduction of ontic-ontological difference.																	0951-5666	1435-5655															10.1007/s00146-020-00955-7		MAR 2020											
J								Can a robot invigilator prevent cheating?	AI & SOCIETY										Nao; Robot; Cheating; Ethics; Virtuous robots; Educational robots	SELF-EFFICACY; TEACHER; PERCEPTIONS; RESPECT	One of the open questions in Educational robots is the role a robot should take in the classroom. The current focus in this area is on employing robots as a tool or in an assistive capacity such as the invigilator of an exam. With robots becoming commonplace in the classroom, inquiries will be raised regarding not only their suitability but also their ability to influence and control the morality and behaviour of the students via their presence. Therefore, as a means to test this cross-section of Educational robots with the underlying issue of morality and ethics we conducted an empirical study where the Nao robot invigilated an exam for a group of students. A between-subjects design (N = 56, 14 groups of 4 students) compared whether Nao was able to deter students from cheating and maintaining their discipline in comparison to a human invigilator or when there was no invigilator present. Our results showed that while explicit cheating rarely took place across all conditions, the students were significantly more talkative when they were invigilated by Nao. In conclusion, we discuss and speculate upon some of the ensuing implications towards not only the application of robots in education but also consequently the wider issue of the preservation of morality and ethics in a classroom in the presence of an agent.																	0951-5666	1435-5655															10.1007/s00146-020-00954-8		MAR 2020											
J								Characterization Of sampling patterns for low-tt-rank tensor retrieval	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Data completion; Tensor retrieval; Low-rank tensor completion; Tensor-train decomposition; Finite completability; Unique completability	COMPLETION; DECOMPOSITION; UNIQUENESS; FRAMEWORK; ALGORITHM; RECOVERY	In this paper, we analyze the fundamental conditions for low-rank tensor completion given the separation or tensor-train (TT) rank, i.e., ranks of TT unfoldings. We exploit the algebraic structure of the TT decomposition to obtain the deterministic necessary and sufficient conditions on the locations of the samples to ensure finite completability. Specifically, we propose an algebraic geometric analysis on the TT manifold that can incorporate the whole rank vector simultaneously in contrast to the existing approach based on the Grassmannian manifold that can only incorporate one rank component. Our proposed technique characterizes the algebraic independence of a set of polynomials defined based on the sampling pattern and the TT decomposition, which is instrumental to obtaining the deterministic condition on the sampling pattern for finite completability. In addition, based on the proposed analysis, assuming that the entries of the tensor are sampled independently with probability p, we derive a lower bound on the sampling probability p, or equivalently, the number of sampled entries that ensures finite completability with high probability. Moreover, we also provide the deterministic and probabilistic conditions for unique completability.																	1012-2443	1573-7470				AUG	2020	88	8					859	886		10.1007/s10472-020-09691-6		MAR 2020											
J								A hierarchical self-attentive neural extractive summarizer via reinforcement learning (HSASRL)	APPLIED INTELLIGENCE										Extractive summarization; Self-attention; Reinforcement learning; Long short-term memory (LSTM); Policy learning	ALGORITHMS	In recent years, deep neural extractive-based summarization approaches have achieved vast popularity over conventional approaches. However, previously proposed neural extractive-based models have issues that limit their performance. One of these issues is related to the architecture of the used neural network that skips some details about the document hierarchical structure. Moreover, these models are optimized to maximize the probabilities of the training data ground truth labels rather than the evaluation metric that actually measures the quality of the summarization; this way of optimization might neglect important information related to sentence ranking. To address these issues, we combined reinforcement and supervised learning to train a hierarchical self-attentive reinforced neural network-based summarization model to rank sentences according to their significance by directly optimizing the ROUGE evaluation metric. The proposed model employs a hierarchical self-attention mechanism to generate document and sentence embeddings that reflect the hierarchical structure of the document and give better feature representation. While reinforcement learning enables direct optimization with respect to evaluation metrics, the attention mechanism adds an extra source of information to direct the summary extraction. The model was evaluated on the basis of three well-known datasets, namely, CNN, Daily Mail, and their combined version CNN/Daily Mail. Experimental results showed that the model achieved higher ROUGE scores than state-of-the-art models for extractive summarization on the three datasets.																	0924-669X	1573-7497				SEP	2020	50	9					2633	2646		10.1007/s10489-020-01669-5		MAR 2020											
J								Computing exact P-values for community detection	DATA MINING AND KNOWLEDGE DISCOVERY										Community detection; Random graphs; Erdos-Renyi model; Statistical significance	NETWORKS	Community detection is one of the most important issues in modern network science. Although numerous community detection algorithms have been proposed during the past decades, how to assess the statistical significance of one single community analytically and exactly still remains an open problem. In this paper, we present an analytical solution to calculate the exact p-value of a single community with the Erdos-Renyi model. Meanwhile, we propose a local search method for finding statistically significant communities based on the p-value minimization. Experimental results on both real networks and simulated networks demonstrate that our method is able to effectively detect true communities from different types of networks.																	1384-5810	1573-756X				MAY	2020	34	3					833	869		10.1007/s10618-020-00681-0		MAR 2020											
J								ptype: probabilistic type inference	DATA MINING AND KNOWLEDGE DISCOVERY										Type inference; Robustness; Probabilistic finite-state machine	AUTOMATA	Type inference refers to the task of inferring the data type of a given column of data. Current approaches often fail when data contains missing data and anomalies, which are found commonly in real-world data sets. In this paper, we propose ptype, a probabilistic robust type inference method that allows us to detect such entries, and infer data types. We further show that the proposed method outperforms existing methods.																	1384-5810	1573-756X				MAY	2020	34	3					870	904		10.1007/s10618-020-00680-1		MAR 2020											
J								Stepping away from maximizers of concave quadratics in random line search	EVOLUTIONARY INTELLIGENCE										Optimization; Random lines; Concave; Quadratic; Differential evolution	GLOBAL OPTIMIZATION; ALGORITHMS	Random Lines (RL) search relies on finding a minimizer of a given cost function along randomly selected lines in the function domain. Once three points along each line are identified, a quadratic function passing through these points is determined and the minimum of the function is used whenever the function is convex. This paper proposes a two-step approach for handling concave cases: (1) starting from a point with the smallest function value and then (2) stepping in the direction away from the maximizer of the quadratic function. Promising numerical results comparing the improved RL method with other similar evolutionary methods are presented.																	1864-5909	1864-5917				DEC	2020	13	4					663	676		10.1007/s12065-020-00380-1		MAR 2020											
J								An overview on evolving systems and learning from stream data	EVOLVING SYSTEMS										Evolving intelligence; Fuzzy systems; Neural networks; Incremental machine learning; Online data stream; Adaptive systems	NEURAL-NETWORKS; ONLINE IDENTIFICATION; FUZZY MODELS; CLASSIFICATION; REGRESSION; ENSEMBLE; TREES; IMPLEMENTATION; CLASSIFIERS; RECOGNITION	Evolving systems unfolds from the interaction and cooperation between systems with adaptive structures, and recursive methods of machine learning. They construct models and derive decision patterns from stream data produced by dynamically changing environments. Different components that assemble the system structure can be chosen, being rules, trees, neurons, and nodes of graphs amongst the most prominent. Evolving systems relate mainly with time-varying environments, and processing of nonstationary data using computationally efficient recursive algorithms. They are particularly appropriate for online, real-time applications, and dynamically changing situations or operating conditions. This paper gives an overview of evolving systems with focus on system components, learning algorithms, and application examples. The purpose is to introduce the main ideas and some state-of-the-art methods of the area as well as to guide the reader to the essential literature, main methodological frameworks, and their foundations.																	1868-6478	1868-6486				JUN	2020	11	2			SI		181	198		10.1007/s12530-020-09334-5		MAR 2020											
J								Advertisement valid time triggered firefly and fruit-fly inspired approach for efficient cluster formation and standby CH selection in hierarchical wireless sensor network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										WSN; Hierarchical clustering; Fruit-fly; Firefly	ENERGY-EFFICIENT; OPTIMIZATION; ALGORITHM	A major concern in wireless sensor network (WSN) is energy efficiency as they utilize small sized batteries, which can neither be replaced nor be recharged. Hence, there are many research efforts in designing energy efficient hierarchical clustering model. Most of the hierarchical models have created improper clusters which induces increased communication overhead and quick depletion of energy in the network. In this paper, advertisement valid time triggered firefly and fruit-fly based bio-inspired algorithm is adapted in WSN for the efficient cluster formation and electing stand by CH to avoid unnecessary reelection process, respectively. These bio-inspired computational approaches ensure the formation of energy efficient hierarchical networking model. The firefly and fruit-fly based energy efficient routing protocol are implemented using the J-Sim tool and the effectiveness of algorithms are analyzed using the result parameters Viz. reelection, data delivery ratio, network lifetime and delay. Further, the results are compared with other variant Hierarchical routing approaches such as LEACH-FA, GA-ABC, and GWO. Our results demonstrate clear superiority of FA and FFA based clustering against other variant approaches.																	1868-5137	1868-5145															10.1007/s12652-020-01873-8		MAR 2020											
J								Gait recognition via random forests based on wearable inertial measurement unit	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Gait recognition; Random forests; Feature engineering; FPRF-GR	AUTHENTICATION	In recent years, gait detection has been widely used in medical rehabilitation, smart phone, criminal investigation, navigation and positioning and other fields. With the rapid development of micro-electro mechanical systems, inertial measurement unit (IMU) has been widely used in the field of gait recognition with many advantages, such as low cost, small size, and light weight. Therefore, this paper proposes a gait recognition algorithm based on IMU, which is named as FPRF-GR. Firstly, a fusion feature engineering operator is designed to eliminate redundant and defective features, which is mainly based on Fast Fourier Transform and principal component analysis. Then, in the design of classifier, in order to meet the requirements of gait recognition model for accuracy, generalization ability, speed, and noise resistance, this paper compares random forest (RF) and several commonly used classification algorithms, and finds that the model constructed by RF can meet the requirements. FPRF-GR builds the model based on RF, and uses the tenfold cross validation method to evaluate the model. Finally, this paper proposes an optimization scheme for the two parameters of decision tree number and sample number in RF. The results show that FPRF-GR can identify five gaits (walk, stationary, run, and up and down stairs) with the average accuracy of 98.2%.																	1868-5137	1868-5145															10.1007/s12652-020-01870-x		MAR 2020											
J								A Novel Superlinearly Convergent Trust Region-Sequential Quadratic Programming Approach for Optimal Gait of Bipedal Robots Via Nonlinear Model Predictive Control	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										TRSQP; Bipedal robots; NMPC; Global convergence; Nonlinear hybrid systems	EFFICIENT; WALKING; ALGORITHM; OPTIMIZATION; STABILITY	A type of trust region-sequential quadratic programming (TRSQP) approach with superlinearly convergent property is first proposed, investigated, and implemented on bipedal robots based on nonlinear model predictive control (NMPC). NMPC is utilized to predict the system behavior and optimize the control move in a receding horizon way, which will result in recursive efficiency and stability. Considering that the classical line search rules are expensive or hard in particular applications, the attempted trust region search is leveraged to avoid the drawbacks of the classical line search rules. Moreover, the feasible descent direction is contained in the trust region via a novelly truncated technique which avoids to recompute the quadratic programming subproblem (QPS) for the main search direction. Owing to some suitable conditions, the globally/superlinearly convergent performance and well-defined properties are analyzed and verified for the TRSQP. The main result is illustrated on a simple bipedal robot which is called as compass-like bipedal robot (CLBR) through numerical simulations and is used to generate dynamic locomotion via TRSQP and NMPC. Furthermore, to demonstrate the feasibility and superiority of a complex bipedal robot which is called as a high-dimensional bipedal robot, numerical simulations are conducted on the model of a three-link bipedal robot (TLBR) and a five-link robot (RABBIT). Furthermore, simulation results show that the TRSQP approach is effectiveness and superiority through comparing with the classical approaches, which included discrete mechanics and optimal control (DMOC) and hybrid zero dynamic (HZD), and control Lyapunov function-quadratic programming (CLF-QP) for the optimal gait of bipedal robot. In addition, to verify the robustness, the TLBR model with parameter's disturbance 1.5 times is investigated and analyzed via TRSQP with NMPC technique. Last,this study develops an interesting framework to exploiting control methods on bipedal robots through accurately and effectively solving nonlinear programming problems.																	0921-0296	1573-0409				NOV	2020	100	2					401	416		10.1007/s10846-020-01174-4		MAR 2020											
J								Discovering subjectively interesting multigraph patterns	MACHINE LEARNING										Multigraph; Subjective interestingness; Maximum entropy principle; Exploratory data mining	COMMUNITY STRUCTURE; NETWORKS	Over the past decade, network analysis has attracted substantial interest because of its potential to solve many real-world problems. This paper lays the conceptual foundation for an application in aviation, through focusing on the discovery of patterns in multigraphs (graphs in which multiple edges can be present between vertices). Our main contributions are twofold. Firstly, we propose a novel subjective interestingness measure for patterns in both undirected and directed multigraphs. Though this proposal is inspired by our previous related research for simple graphs (having only single edges), the properties of multigraphs make this transition challenging. Secondly, we propose a greedy algorithm for subjectively interesting pattern mining, and demonstrate its efficacy through several experiments on synthetic and real-world examples. We conclude with a case study in aviation, which demonstrates how the departure from an analyst's prior beliefs captured as subjectively interesting patterns could help improve an analyst's understanding of the data and problem at hand.																	0885-6125	1573-0565				AUG	2020	109	8					1669	1696		10.1007/s10994-020-05873-9		MAR 2020											
J								Wheat crop yield prediction using new activation functions in neural network	NEURAL COMPUTING & APPLICATIONS										Multilayer perceptron; Forecast; Activation function; Classification; Yield; Neural network; Agriculture		This research mainly based on multilayer perceptron (MLP) neural networks technique of data mining to forecast the wheat crop yield at the district level. There are many statistical and simulation models available, but the proposed algorithm with new activation function provides promising results in a shorter time with more accuracy. Sigmoid and hyperbolic tangent activation functions are widely used in the neural network. The activation functions play an important role in the neural network learning algorithm. The main objective of the proposed work is to develop an amended MLP neural network with new activation function and revised random weights and bias values for crop yield estimation by using the different weather parameter datasets. MLP model has been tested by existing activation functions and newly created activation functions with different cases including weights and bias values. In this research study, we evaluate the result of different activation functions and recommend some new simple activation functions, named DharaSig, DharaSigm and SHBSig, to improve the performance of neural networks and accurate results. Also, three new activation functions created with little variations in the DharaSig function named DharaSig1, DharaSig2 and DharaSig3. In this research study, variable numbers of hidden layers are tested with the variable number of neurons per hidden layer for the agriculture dataset. Variable values of momentum, seed and learning rate are also used in this study. Experiments show that newly created activation functions provide better results compared to 'sigmoid' default neural network activation function for agriculture datasets.																	0941-0643	1433-3058				SEP	2020	32	17					13941	13951		10.1007/s00521-020-04797-8		MAR 2020											
J								Evaluating the retest reproducibility of intrinsic connectivity network using multivariate correlation coefficient	NEURAL COMPUTING & APPLICATIONS										Intrinsic connectivity network; Reproducibility; Correlation coefficient; Sparse representation; fMRI	DEFAULT-MODE NETWORK; FUNCTIONAL CONNECTIVITY; SPARSE REPRESENTATION; FMRI; RELIABILITY; ACTIVATION; PATTERNS	Recently, the retest reproducibility of intrinsic connectivity networks (ICNs) has become an increasing concern in the fMRI research community. However, few indexes can be applied to directly quantify the similarity of three or more ICNs for evaluating the retest reproducibility of ICNs. To solve this problem, a multivariable correlation coefficient based on zero-mean normalization and intraclass correlation coefficient (Z-ICC) is proposed. After demonstrating the calculation method and performance analysis on theory, Z-ICC is adopted to evaluate the similarity of three ICNs from three ICN sets, which are inferred from the open retest resting-state fMRI dataset NYU_TRT with dual temporal and spatial sparse representation (DTSSR). The reproducible ICNs and quantization index of retest reproducibility are obtained by the calculated Z-ICC values and the accepted evaluation criterion. The experimental results and visual inspection show that Z-ICC can effectively identify the reproducible ICNs and quantify the retest reproducibility of ICNs. Eighteen (Z-ICC > 0.8) of the inferred twenty ICNs with DTSSR that are found to be reproducible are far more than the seven reproducible ICNs based on temporal concatenation group ICA (TC-GICA). Furthermore, the result of the one-tailed two-sample T test demonstrates that the Z-ICC values of the reproducible ICNs by DTSSR are significantly greater than those of TC-GICA, indicating that more reproducible group-level ICNs with higher retest reproducibility can be achieved with DTSSR.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14623	14638		10.1007/s00521-020-04816-8		MAR 2020											
J								A deep learning approach to evaluate intestinal fibrosis in magnetic resonance imaging models	NEURAL COMPUTING & APPLICATIONS										Severity scores of intestinal fibrosis; MRI model; Feature selection; Deep learning models	INFLAMMATORY-BOWEL-DISEASE; CROHNS-DISEASE; LESIONS	Fibrosis may be introduced as a severe complication of inflammatory bowel disease (IBD). This is a particular disorder causing luminal narrowing and stricture formation in the inflamed bowel wall of a patient denoting, possibly, need for surgery. Thus, the development of treatments reducing fibrosis is an urgent issue to be addressed in IBD. In this context, we require the finding and development of biomarkers of intestinal fibrosis. Potential candidates such as microRNAs, gene variants or fibrocytes have shown controversial results on heterogeneous sets of IBD patients. Magnetic resonance imaging (MRI) has been already successfully proven in the recognition of fibrosis. Nevertheless, while there are no numerical models capable of systematically reproducing experiments, the usage of MRI could not be considered a standard in the inflammatory domain. Hence, there is an importance of deploying new sequence combinations in MRI methods that enable learning reproducible models. In this work, we provide reproducible deep learning models of intestinal fibrosis severity scores based on MRI novel radiation-induced rat model of colitis that incorporates some unexplored sequences such as the flow-sensitive alternating inversion recovery or diffusion imaging. The results obtained return an 87.5% of success in the prediction of MRI scores with an associated mean-square error of 0.12. This approach offers practitioners a valuable tool to evaluate antifibrotic treatments under development and to extrapolate such noninvasive MRI scores model to patients with the aim of identifying early stages of fibrosis improving patients' management.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14865	14874		10.1007/s00521-020-04838-2		MAR 2020											
J								Artificial intelligence-based load optimization in cognitive Internet of Things	NEURAL COMPUTING & APPLICATIONS										Internet of Things; Cognitive engine; Cognitive radio; Artificial intelligence; Particle swarm optimization; Load optimization	IOT; ALGORITHM	The Internet of Things (IoT) comprises smart objects capable of sensing, processing, and transmitting application-specific data. These objects collect and transmit a huge amount of correlated and redundant data due to overlapped sensing regions, causing unnecessary exploitation of spectral bands and load balancing issues in the network. As a result, time-critical and delay-sensitive data experience a higher delay, lower throughput, and quality of service degradation. To circumvent these issues, in this paper, we propose a model that is energy efficient and is capable of maximizing the spectrum utilization with optimal Device-to-Gateway configuration. Initially, the network gateways perform spectrum sensing for available channels using an energy detection technique and forward them to a cognitive engine (CE). The CE assigns the best available channels in the licensed band to the network devices for communication. Each channel is divided into equal-length time slots for the timely delivery of critical data. In addition, the CE calculates the load on each gateway and uses particle swarm optimization algorithm for optimal load distribution among the network gateways. Our experimental results show that the proposed model is efficient for the resource-constrained IoT devices in terms of packet drop ratio, delay, and throughput of the network. Moreover, the proposed scheme also achieves optimal Device-to-Gateway configuration with efficient spectrum utilization in the licensed band.																	0941-0643	1433-3058				OCT	2020	32	20			SI		16179	16189		10.1007/s00521-020-04814-w		MAR 2020											
J								A novel equilibrium optimization algorithm for multi-thresholding image segmentation problems	NEURAL COMPUTING & APPLICATIONS										Image segmentation problem; Equilibrium optimization algorithm (EOA); Kapur's entropy	GENETIC ALGORITHM; ENTROPY	Image segmentation is considered a crucial step required for image analysis and research. Many techniques have been proposed to resolve the existing problems and improve the quality of research, such as region-based, threshold-based, edge-based, and feature-based clustering in the literature. The researchers have moved toward using the threshold technique due to the ease of use for image segmentation. To find the optimal threshold value for a grayscale image, we improved and used a novel meta-heuristic equilibrium algorithm to resolve this scientific problem. Additionally, our improved algorithm has the ability to enhance the accuracy of the segmented image for research analysis with a significant threshold level. The performance of our algorithm is compared with seven other algorithms like whale optimization algorithm, bat algorithm, sine-cosine algorithm, salp swarm algorithm, Harris hawks algorithm, crow search algorithm, and particle swarm optimization. Based on a set of well-known test images taken from Berkeley Segmentation Dataset, the performance evaluation of our algorithm and well-known algorithms described above has been conducted and compared. According to the independent results and analysis of each algorithm, our algorithm can outperform all other algorithms in fitness values, peak signal-to-noise ratio metric, structured similarity index metric, maximum absolute error, and signal-to-noise ratio. However, our algorithm cannot outperform some algorithms in standard deviation values and central processing unit time with the large threshold levels observed.																	0941-0643	1433-3058															10.1007/s00521-020-04820-y		MAR 2020											
J								A Metaphor Comprehension Method Based on Culture-Related Hierarchical Semantic Model	NEURAL PROCESSING LETTERS										Cultural semantics; Metaphor comprehension; Hierarchical model	LANGUAGE COMPREHENSION; EMBODIMENT; INFERENCES; TEXT	Usually a metaphor is encoded with rich cultural connotation, which signifies that culture plays a key factor in truly comprehending a metaphorically-used utterance. Given that, we developed a culture-related hierarchical semantic model to perform metaphor comprehension. Based on the character of a metaphor, to better represent context and background knowledge, we embedded word-level, attribute-level, perception-level, and context-level information into the model. Moreover, in the attribute-level, a culture mapping is developed to better use cultural information. We use a random walk algorithm to search for the most reasonable comprehension results. The model was tested in a nominal Chinese metaphor corpus. The results show the effectiveness of the model and demonstrate its advantages in understanding cultural metaphors.																	1370-4621	1573-773X				JUN	2020	51	3			SI		2807	2826		10.1007/s11063-020-10227-6		MAR 2020											
J								Flexible Discrete Multi-view Hashing with Collective Latent Feature Learning	NEURAL PROCESSING LETTERS											CORRENTROPY	Multi-view hashing has gained considerable research attention in efficient multimedia studies due to its promising performance on heterogeneous data from various sources. However, its application in discriminative hash codes learning remains challenging as it fails to efficiently capture preferable components from multiple representations. In this work, we propose a novel discriminative multi-view hashing framework, dubbed flexible discrete multi-view hashing, in conjunction with collective latent feature learning by combining multiple views of data and consistent hash codes learning by fusing visual features and flexible semantics. Specifically, an adaptive multi-view analysis dictionary learning model is developed to skillfully combine diverse representations into an established common latent feature space where the complementary properties of different views are well explored based on an automatic multi-view weighting strategy. Moreover, we introduce a collaborative learning scheme to jointly encode the visual and semantic embeddings into an aligned consistent Hamming space, which can effectively mitigate the visual-semantic gap. Particularly, we employ the correntropy induced regularization to improve the robustness of the formulated flexible semantics. An efficient learning algorithm is proposed to solve the optimization problem. Extensive experiments show the state-of-art performance of the proposed method on several benchmark datasets.																	1370-4621	1573-773X															10.1007/s11063-020-10221-y		MAR 2020											
J								Performance analysis intelligent-based advanced PSO algorithm and testing of real-time matrix converter electrical system	SOFT COMPUTING										Conventional matrix converter (CMC); Particle swarm optimization (PSO); Modified PSO; Modified hybrid PSO		The advanced improvements in power electronic switches make the development of power electronic converters. This drastic development results the matrix converter (MC) evaluation. MC is a bidirectional power flow device with single-stage conversion process with variable voltage and variable frequency. The major drawback of MC is harmonic content present due to switching of power electronic devices. To reduce the harmonics present in the MC, lots of research works were carried out, but it is limited to some extent; beyond the limit, the harmonics can be reduced in MC by introducing soft computational algorithm-based controller. The PSO-based controller was implemented in the literature with resistive and inductive loads. In this paper, PSO-, modified PSO-, modified hybrid PSO-based controllers were implemented with induction motor as load. This soft computing-based controller for MC effectively reduces the harmonic content by choosing the optimal switching pulses for each sampling. The simulation results were carried out on the MATLAB/Simulink interface. Both Simulink and hardware results of the MC strongly recommend the soft computing-based controller for MC to improve the behavior.																	1432-7643	1433-7479				SEP	2020	24	18			SI		14209	14220		10.1007/s00500-020-04789-9		MAR 2020											
J								Energy-efficient strategy for virtual machine consolidation in cloud environment	SOFT COMPUTING										Cloud computing; Data center; Live migration; Energy-efficient strategy; Energy consumption	DYNAMIC CONSOLIDATION; COMPUTING ENVIRONMENTS; CONSUMPTION; PERFORMANCE; AWARE; QOS; ALGORITHMS; QUALITY; POWER	An important issue of energy efficiency in cloud environment is to perform more jobs while consuming less amount of power. Virtual machine consolidation remains the most deployed strategy to manage both performance and energy consumption. Most of existing energy efficiency techniques save energy against the cost on performance degradation. Consolidation techniques leverage thresholds to detect overloaded and underloaded hosts that could be vacated to achieve optimal balance between host utilization and energy consumption. In this research, we propose an energy-efficient strategy (EES) to consolidate virtual machines in cloud environment with an aim of reducing the energy consumption while completing more tasks with the highest throughput. Our proposal makes use of the performance-to-power ratio to set upper thresholds for overload detection. In addition, EES considers the overall data center workload utilization to set lower thresholds, which can reduce the number of virtual machine migrations. The simulation results show that EES leads to energy-efficient workload consolidation with the minimal number of migrations and less energy consumption. The results conclude that EES saves energy consumption without compromising user's workload requirement.																	1432-7643	1433-7479				OCT	2020	24	19					14845	14859		10.1007/s00500-020-04839-2		MAR 2020											
J								Enhanced superposition determination for weighted superposition attraction algorithm	SOFT COMPUTING										WSA algorithm; Superposition principle; Performance enhancement; Functional optimization	PARTICLE SWARM OPTIMIZATION; HARMONY SEARCH ALGORITHM; ENGINEERING OPTIMIZATION; DESIGN OPTIMIZATION; EVOLUTIONARY OPTIMIZATION; INTELLIGENCE ALGORITHM; GLOBAL OPTIMIZATION; GENETIC ALGORITHM; SIMULATION; CHAOS	This paper argues the efficiency enhancement study of a recent meta-heuristic algorithm, WSA, by modifying one of its operators, superposition (target point) determination procedure. The original operator is based on the weighted vector summation and has some potential disadvantages with regard to domain of the decision variables such that determining a superposition out of the search space. Such potential disadvantages may cause WSA to behave as a random search and result in an unsatisfactory performance for some problems. In order to eliminate such potential disadvantages, we propose a new superposition determination procedure for the WSA algorithm. Thus, the mWSA algorithm will be able to behave more consistent during its search and its robustness will improve significantly in comparison to its original version. The mWSA algorithm is compared against the WSA algorithm and some other algorithms taken from the existing literature on both the constrained and unconstrained optimization problems. The experimental results clearly indicate that the mWSA algorithm is an improvement for the original WSA algorithm, and also prove that the mWSA algorithm is more robust and consistent search procedure in solving complex optimization problems.																	1432-7643	1433-7479				OCT	2020	24	19					15015	15040		10.1007/s00500-020-04853-4		MAR 2020											
J								Human-in-the-loop active learning via brain computer interface	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Deep learning; Human-In-The-Loop; Transfer Learning; Clustering; Brain Computer Interface; EEG; Active Learning	P300; REPRESENTATIONS; COMPONENT	This paper develops and examines an innovative methodology for training an artificial neural network to identify and tag target visual objects in a given database. While the field of Artificial Intelligence in general, and computer vision in particular, has greatly advanced in recent years, fast and efficient methods for tagging (i.e., labeling) visual targets are still lacking. Tagging data is important to train, as it allow to train supervised learning models. However, this is a tiresome task that often creates bottlenecks in academic and industrial research projects. In order to develop an algorithm that improves data tagging processes, this study utilizes the advantages of human cognition and machine learning by combining Brain Computer Interface, Human-In-The-Loop, and Deep Learning. Combining these three fields into one algorithm could enable the rapid annotation of large visual databases that have no prior references and cannot be described as a mathematical optimization function. Human-In-The-Loop is an increasingly researched area that refers to the integration of human feedback in computation processes. At present, computer-based deep learning can only be incorporated in the process of identifying and tagging target objects of interest if a predefined database exists - one that has already been defined by a human user. To reduce the scope of this timely and costly process, our algorithm uses machine learning techniques (i.e., active learning) to minimize the number of target objects a human user needs to identify before the computer can successfully carry out the task independently. In our method, users are connected to electroencephalograms electrodes and shown images using rapid serial visual presentation - a fast method for presenting users with images. Some images are target objects, while others are not. Based on users' brainwave activity when target objects are shown, the computer learns to identify and tag target objects - already in the learning stage (unlike naive uniform sampling methods that first require human input, and only then begin the learning stage). As such, our work is proof of concept for the effectiveness of involving humans in the computer's learning stage, i.e., human-in-the-loop as opposed to the traditional method of humans first tagging the data and the machines then learning and creating a model.																	1012-2443	1573-7470				DEC	2020	88	11-12					1191	1205		10.1007/s10472-020-09689-0		MAR 2020											
J								Efficient multi-dimensional web information discovery in wireless sensor network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Multidimensional data; Unified Cube; Octree based Hausdorff Distance (OHD); OHD aided Unified Cube (OAUC)	HAUSDORFF DISTANCE	The increasing availability of small-size sensor devices during the last few years and the large amount of data that they generate has led to the necessity for more efficient methods regarding data management. Here a model has been introduced that can be used for data gathering and information management in sensor networks which provides some advantages through the utilization of semantic web technologies. This work provides a multidimensional approach named an Octree-based Hausdorff Distance aided Unified Cube or OHD Aided Unified Cube (OAUC) which offers a generic representation for both warehoused data and Linked Object Data (LOD) at the conceptual level. A two-stage process is built using Unified Cube according to decision-makers' needs. Initially, the schemas published with specific modeling languages are transformed into a common conceptual representation called exportation cube. Then the associated data are combined using Hausdorff Distance (HD) to form a Unified Cube. The information discovery problem is formulated as a load balancing technique for multiple attributes, with the combined aim being to increase network lifetime, reduce hotspots and also to reduce query processing latency by introducing multi-resolution. The efficiency of the proposed OAUC is analyzed by the comparison of OAUC with existing VF-CAN indexing and storage schemes. OAUC provides better performance in Hausdorff Distance (HD) calculation that is proved by comparing it with ZHD and NAIVE algorithms.																	1868-5137	1868-5145															10.1007/s12652-020-01868-5		MAR 2020											
J								A social recommender system based on reliable implicit relationships	KNOWLEDGE-BASED SYSTEMS										Recommender system; Social information; Reliability; Implicit relationship; Dempster-Shafer theory	TRUST; CONFIDENCE	Recommender systems attempt to suggest information that is of potential interest to users helping them to quickly find information relevant to them. In addition to historical useritem interaction data, such as users ratings on items, social recommendation methods use social relationships between users to improve the accuracy of recommendations. However, the available social relationships are often extremely sparse. Therefore, incorporating implicit relationships into the recommendation process can be effective to improve the performance of social recommender systems, especially for those users whose explicit relationships are insufficient to make accurate recommendations. The existing approaches have not considered reliability of the implicit relationships. In this paper, a social recommender system is proposed based on reliable implicit relationships. To this end, Dempster-Shafer theory is used as a powerful mathematical tool to calculate the implicit relationships. Moreover, a new measure is introduced to evaluate the reliability of predictions, where unreliable predictions are recalculated using a neighborhood improvement mechanism. This mechanism uses a confidence measure between the users to identify ineffective users in the neighborhood set of a target user. Finally, new reliable ratings are calculated by removing the identified ineffective neighbors. Extensive experiments are conducted on three well-known datasets, and the results demonstrate that our approach achieves superior performance to the state-of-the-art recommendation methods. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 15	2020	192								105371	10.1016/j.knosys.2019.105371													
J								Characterizing and using gullibility, competence, and reciprocity in a very fast and robust trust and distrust inference algorithm for weighted signed social networks	KNOWLEDGE-BASED SYSTEMS										Online social network; Trust inference; Distrust; Trust metric; Social trait	USERS	Predicting trust is a classic problem in social networks analysis. Furthermore, while most early approaches ignore distrust, recent works seem to consider it as important, if not more important, than trust itself. In this paper, we present a novel approach to predict both trust and distrust in Weighted Signed Social Networks very efficiently and in a satisfyingly accurate and robust way. Therefore allowing people to have healthier online presence and interactions. Being a local metric that does not rely on trust propagation, the proposed approach does not suffer from some serious limitations like trust decay, opinions conflict, path dependence, and time complexity. Moreover, our experiments on four real-world datasets show that, in addition to its simplicity and extensibility, this algorithm is robust to network sparsity, and provides satisfyingly accurate and very fast predictions. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 15	2020	192								105345	10.1016/j.knosys.2019.105345													
J								A scalable saliency-based feature selection method with instance-level information	KNOWLEDGE-BASED SYSTEMS										Feature selection; Deep learning; Saliency		Classic feature selection techniques remove irrelevant or redundant features to achieve a subset of relevant features in compact models that are easier to interpret and so improve knowledge extraction. Most such techniques operate on the whole dataset, but are unable to provide the user with useful information when only instance-level information is required; in other words, classic feature selection algorithms do not identify the most relevant information in a sample. We have developed a novel feature selection method, called saliency-based feature selection (SFS), based on deep-learning saliency techniques. Our algorithm works under any architecture that is trained by using gradient descent techniques (Neural Networks, SVMs, ...), and can be used for classification or regression problems. Experimental results show our algorithm is robust, as it allows to transfer the feature ranking result between different architectures, achieving remarkable results. The versatility of our algorithm has been also demonstrated, as it can work either in big data environments as well as with small datasets. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 15	2020	192								105326	10.1016/j.knosys.2019.105326													
J								iBridge: Inferring bridge links that diffuse information across communities	KNOWLEDGE-BASED SYSTEMS										Bridge link prediction; Information diffusion; Weak ties; PU learning	COMPLEX NETWORKS; PREDICTION; SECURITY	While the accuracy of link prediction has been improved continuously, the utility of the inferred new links is rarely concerned especially when it comes to information diffusion. This paper defines the utility of links based on average shortest distance and more importantly defines a special type of links named bridge links based on community structure (overlapping or not) of the network. In sociology, bridge links are usually regarded as weak ties and play a more crucial role in information diffusion. Considering that the accuracy of previous link prediction methods is high in predicting strong ties but not much high in predicting weak ties, we propose a new link prediction method named iBridge, which aims to infer new bridge links using biased structural metrics in a PU (positive and unlabeled) learning framework. The experimental results in 3 real online social networks show that iBridge outperforms several comparative link prediction methods (based on supervised learning or PU learning) in inferring the bridge links and meantime, the overall performance of inferring bridge links and non-bridge links is not compromised, thus verifying its robustness in inferring all new links. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 15	2020	192								105249	10.1016/j.knosys.2019.105249													
J								A novel selective naive Bayes algorithm	KNOWLEDGE-BASED SYSTEMS										Naive Bayes; Attribute selection; Leave-one-out cross validation; Model selection	PROBABILITY-DISTRIBUTIONS; STATISTICAL COMPARISONS; WEIGHTING FILTER; DECISION TREE; CLASSIFIERS	Naive Bayes is one of the most popular data mining algorithms. Its efficiency comes from the assumption of attribute independence, although this might be violated in many real-world data sets. Many efforts have been done to mitigate the assumption, among which attribute selection is an important approach. However, conventional efforts to perform attribute selection in naive Bayes suffer from heavy computational overhead. This paper proposes an efficient selective naive Bayes algorithm, which adopts only some of the attributes to construct selective naive Bayes models. These models are built in such a way that each one is a trivial extension of another. The most predictive selective naive Bayes model can be selected by the measures of incremental leave-one-out cross validation. As a result, attributes can be selected by efficient model selection. Empirical results demonstrate that the selective naive Bayes shows superior classification accuracy, yet at the same time maintains the simplicity and efficiency. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 15	2020	192								105361	10.1016/j.knosys.2019.105361													
J								DMGAN: Discriminative Metric-based Generative Adversarial Networks	KNOWLEDGE-BASED SYSTEMS										Generative Adversarial Networks; Deep metric learning; Image generation; Weight adaption		With the proposed of Generative Adversarial Networks (GANs), the generative adversarial models have been extensively studied in recent years. Although probability-based methods have achieved remarkable results in image synthesis tasks, there are still some unsolved challenges that are difficult to overcome. In this paper, we propose a novel model, called Discriminative Metric-based Generative Adversarial Networks (DMGANs), for generating real-like samples from the perspective of deep metric learning. To be specific, the generator is trained to generate realistic samples by reducing the distance between real and generated samples. Instead of outputting probability, the discriminator in our model is conducted as a feature extractor, which is well constrained by introducing a combination of identity preserving loss and discriminative loss. Meanwhile, to reduce the identity preserving loss, we calculate the distance between samples and their corresponding center and update these centers during training to improve the stability of our model. In addition, a data-dependent strategy of weight adaption is proposed to further improve the quality of generated samples. Experiments on several datasets illustrate the potential of our model. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 15	2020	192								105370	10.1016/j.knosys.2019.105370													
J								Monotonic alignments for summarization	KNOWLEDGE-BASED SYSTEMS										Summarization; Monotonic; Alignment; Attention		Summarization is the task that creates a summary with the major points of the original document. Deep learning plays an important role in both abstractive and extractive summary generations. While a number of models show that combining the two gives good results, this paper focuses on a pure abstractive method to generate good summaries. Our model is a stacked RNN network with a monotonic alignment mechanism. Monotonic alignment has an advantage because it produces the context that is in the same sequence as the original document, at the same time eliminating repeating sequences. To obtain monotonic alignment, this paper proposes two energies that are calculated using only the previous alignment state. We use sub-word method to reduce the rate of producing OOVs(Out of Vocabulary). The dropout is used for generalization and the residual connection to overcome gradient vanishing. We experiment on CNN/daily new and Reddits dataset. Our method out-performs the previous models with monotonic alignment by 4 ROUGE-1 points and achieves the results comparable to state of the art. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 15	2020	192								105363	10.1016/j.knosys.2019.105363													
J								Multilingual aspect clustering for sentiment analysis	KNOWLEDGE-BASED SYSTEMS										Aspect-based sentiment analysis; Multilingual aspect clustering; Unsupervised learning; Word embeddings	NEWS	In the last few years, there has been growing interest in aspect-based sentiment analysis, which deals with extracting, clustering, and rating the overall opinion about the features of the entity being evaluated. Techniques for aspect extraction can produce an undesirably large number of aspects with many of those relating to the same product feature. Hence, aspect clustering becomes necessary. Current solutions for aspect clustering are monolingual, but in many practical situations, reviews for a given entity are available in several languages, calling for multilingual integration. In this article, we address the novel task of multilingual aspect clustering, which aims at grouping semantically related aspects extracted from reviews written in several languages. Our method is unsupervised and relies on the contextual information of the aspects, which is represented by word embeddings. This representation allied with a suitable similarity measure allows clustering related aspects. Our experiments on two datasets with five languages each showed that our unsupervised clustering technique achieves results that outperform monolingual baselines adapted to work with multilingual data. We also show the benefits of the multilingual approach compared to using languages in isolation. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 15	2020	192								105339	10.1016/j.knosys.2019.105339													
J								Semantic-enhanced discrete matrix factorization hashing for heterogeneous modal matching	KNOWLEDGE-BASED SYSTEMS										Discrete hashing; Discrete matrix factorization; Semantic enhanced; Heterogeneous modal matching	OPTIMIZATION	The simultaneous hash representation of heterogeneous modalities has shown its excellent performance in the multi-modal learning community. However, the binary discrete constraint of hash brings a great challenge to the common hash representation of heterogeneous modal data. To resolve the issue, this paper proposes a novel discrete hash method, referred to as Semantic enhanced Discrete Matrix Factorization Hashing (SDMFH). SDMFH directly extracts the common discrete hash representation of all modalities from the reconstructed semantic intermodal similarity graph, which makes the hash codes more discriminative. Meanwhile, the semantic labels are regressed to the extracted discrete hash book, so as to further strengthen the discriminating ability of the learned discrete hash book. Moreover, a linear embedding from the kernel space of the original data to the Hamming space is explored to generate the common hash codes of each modality to ensure the consistency of the hash codes of heterogeneous modalities. More importantly, we develop an efficient discrete iterative algorithm based on Stiefel manifold that can directly learn the discrete hash book in a closed form, thus both avoiding quantization loss caused by discrete relaxation and reducing computational complexity. Experimental results on three benchmark data sets show that SDMFH performs better than several state-of-the-art methods for heterogeneous modal matching tasks. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 15	2020	192								105381	10.1016/j.knosys.2019.105381													
J								Autoencoder based sample selection for self-taught learning	KNOWLEDGE-BASED SYSTEMS										Self-taught learning; Sample selection; Autoencoder; Domain mapping; Spectral graph analysis	SPARSE REGRESSION; CLASSIFICATION	Self-taught learning is a technique that uses a large number of unlabeled data as source samples to improve the task performance on target samples. Compared with other transfer learning techniques, self-taught learning can be applied to a broader set of scenarios due to the loose restrictions on the source data. However, knowledge transferred from source samples that are not sufficiently related to the target domain may negatively influence the target learner, which is referred to as negative transfer. In this paper, we propose a metric for the relevance between a source sample and the target samples. To be more specific, both source and target samples are reconstructed through a single-layer autoencoder with a linear relationship between source samples and reconstructed target samples being simultaneously enforced. An l(2.1)-norm sparsity constraint is imposed on the transformation matrix to identify source samples relevant to the target domain. Source domain samples that are deemed relevant are assigned pseudo-labels reflecting their relevance to target domain samples, and are combined with target samples in order to provide an expanded training set for classifier training. Local data structures are also preserved during source sample selection through spectral graph analysis. Promising results in extensive experiments show the advantages of the proposed approach. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 15	2020	192								105343	10.1016/j.knosys.2019.105343													
J								Predicting concentration levels of air pollutants by transfer learning and recurrent neural network	KNOWLEDGE-BASED SYSTEMS										Forecasting; Environment monitoring; Transfer learning; Recurrent neural network; Airborne particle matter	POLLUTION; MORTALITY	Air pollution (AP) poses a great threat to human health, and people are paying more attention than ever to its prediction. Accurate prediction of AP helps people to plan for their outdoor activities and aids protecting human health. In this paper, long-short term memory (LSTM) recurrent neural networks (RNNs) have been used to predict the future concentration of air pollutants (APS) in Macau. Additionally, meteorological data and data on the concentration of APS have been utilized. Moreover, in Macau, some air quality monitoring stations (AQMSs) have less observed data in quantity, and, at the same time, some AQMSs recorded less observed data of certain types of APS. Therefore, the transfer learning and pre-trained neural networks have been employed to assist AQMSs with less observed data to build a neural network with high prediction accuracy, The experimental sample covers a period longer than 12-year and includes daily measurements from several APS as well as other more classical meteorological values. Records from five stations, four out of them are AQMSs and the remaining one is an automatic weather station, have been prepared from the aforesaid period and eventually underwent to computational intelligence techniques to build and extract a prediction knowledge-based system. As shown by experimentation, LSTM RNNs initialized with transfer learning methods have higher prediction accuracy; it incurred shorter training time than randomly initialized recurrent neural networks. (C) 2020 Published by Elsevier B.V.																	0950-7051	1872-7409				MAR 15	2020	192								105622	10.1016/j.knosys.2020.105622													
J								Nearest and farthest spatial skyline queries under multiplicative weighted Euclidean distances	KNOWLEDGE-BASED SYSTEMS										Computer science; Decision-making support system; Nearest and farthest spatial skyline query; Weighted Euclidean distance; Graphics Processing Unit (GPU)	COMPUTATION; LOCATION; ALGORITHMS	Consider two point sets in the plane, a set of points of interest and a set of query points that is used to establish distance restrictions with respect to the set of points of interest. A nearest/farthest spatial skyline query retrieves the subset of desirable or relevant points of interest, called skyline points, such that no other point of interest is simultaneously closer to/farther from all the query points. The nearest/farthest top-k spatial skylines, are the best k nearest/farthest spatial skylines among the existent ones. All these queries find applications in decision-making support systems, facility location, crisis management and in trips or events planning. To take into account that each point of interest has a different importance, a weight is assigned to each of them and multiplicative weighted Euclidean distances are used. In this paper, we study, for the first time, the nearest and farthest spatial skyline queries when multiplicative weighted Euclidean distances are considered. We prove that most of the properties of the traditional non weighted nearest and farthest spatial skyline queries are no longer true under the weighted Euclidean distance and, consequently, the strategies used for solving non weighted spatial skyline queries are not usable in the weighted case. We present a sequential and a parallel algorithm, to be run on the CPU and on a Graphics Processing Unit, respectively, for solving nearest/farthest weighted spatial skyline queries and to extract the nearest/farthest top-k spatial skylines. We provide the time and space complexity analysis of both algorithms together with their theoretical comparison. We also have developed a simple interface to deal with weighted spatial skyline queries which allows to visualize and store in a file the obtained spatial skylines. Finally, we present and discuss experimental results obtained with the implementation of the proposed sequential and parallel algorithms. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 15	2020	192								105299	10.1016/j.knosys.2019.105299													
J								An efficient hybrid filter and evolutionary wrapper approach for sentiment analysis of various topics on Twitter	KNOWLEDGE-BASED SYSTEMS										Sentiment analysis; Support Vector Machine; SVM; Feature selection; Multi-Verse Optimizer; ReliefF; Social network	SUPPORT VECTOR MACHINE; FEATURE-SELECTION; CLASSIFICATION; ALGORITHM; SYSTEM	Sentiment Analysis is currently considered as one of the most attractive research topics in Natural Language Processing (NLP) field. The main objective of sentiment analysis is to identify the opinions and emotions of the users through written contents. While there are different studies that have approached this field using various techniques, it is still considered a challenging topic with many difficulties that are yet to be solved, such as having modern accents, slang words, spelling and grammatical mistakes, and other issues that cannot be overcome with traditional methods and sentiment lexicons. In this work, we propose a hybrid machine learning approach to enhance sentiment analysis; as we build a classification model based on three classes, which are positive, neutral, and negative emotions, using Support Vector Machines (SVM) classifier, while combining two feature selection techniques using the ReliefF and Multi-Verse Optimizer (MVO) algorithms. We also extract more than 6900 tweets from Twitter social network to test our work. Our hybrid method is compared against other classifiers and methods in terms of accuracy. Results show that our proposed method outperforms other techniques and classifiers, by obtaining better results in most of the datasets while reducing the number of features by up to 96.85% from the original feature set. We also categorize the extracted features into Objective, Subjective and Emoticon words to analyze them during the first and the final feature selection processes and find any existing relations. Very similar results are obtained by both feature selection techniques; due to a number of factors that are explained in this paper. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 15	2020	192								105353	10.1016/j.knosys.2019.105353													
J								jKarma: A highly-modular framework for pattern-based change detection on evolving data	KNOWLEDGE-BASED SYSTEMS										Change detection; Pattern mining; Evolving data; Software modularity		Pattern-based change detection (PBCD) describes a class of change detection algorithms for evolving data. Contrary to conventional solutions, PBCD seeks changes exhibited by the patterns over time and therefore works on an abstract form of the data, which prevents the search for changes on the raw data. Moreover, PBCD provides arguments on the validity of the results because patterns mirror changes occurred with any form of evidence. However, the existing solutions differ on data representation, pattern mining algorithm and change identification strategy, which we can deem as main modules of a general architecture, so that any PBCD task could be designed by accommodating custom implementations for those modules. This is what we propose in this paper through jKarma, a highly-modular framework written in Java for defining and performing PBCD. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 15	2020	192								105303	10.1016/j.knosys.2019.105303													
J								Attribute importance measurement method based on data coordination degree	KNOWLEDGE-BASED SYSTEMS										Data system; Core data set; Attribute importance; Decision coordination degree; Composite quantification	ROUGH SET; FEATURE-SELECTION; DECISION; REDUCTION; UNCERTAINTY; INFORMATION; MODEL	The increasing scale of data information cause the great amount of irrelevant attributes, which becomes a challenging issue for machine learning. Therefore, removing the redundancy through sorting the attributes with appropriate significance has attracted wide attention in academic and application. Taking the knowledge hidden in the data system as the carrier and the inclusion relationship between sets as the basis, this paper proposes the concept of decision coordination degree. Then a composite attribute importance measurement based on core data is established (BCD-AICM). Further the basic properties and features of BCD-AICM are discussed. Finally, the similarities and differences between BCD-AICM and the existing attribute importance measurement methods are discussed using eight UCI data sets. The theoretical analysis and experiments results show that the BCD-AICM has good interpretability and structural characteristics. This method enriches the existing related theories and has broad application prospects in the fields of fuzzy decision-making, knowledge acquisition, resource management, and artificial intelligence etc. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 15	2020	192								105359	10.1016/j.knosys.2019.105359													
J								Safety control modeling method based on Bayesian network transfer learning for the thickening process of gold hydrometallurgy	KNOWLEDGE-BASED SYSTEMS										Bayesian network; Transfer learning; Gold hydrometallurgy; Safety control; Expert knowledge	FAULT-DETECTION; IDENTIFICATION; CLASSIFICATION; ABNORMALITY; ALGORITHMS	When the data information of target domain is very limited, it is difficult to establish the accurate model to analyze the target problem. For the safety control modeling problem, this paper develops a new Bayesian network (BN) transfer learning strategy for the thickening process of gold hydrometallurgy. First of all, the safety control modeling problem in this process is analyzed deeply. When the data information of abnormality is insufficient, the safety control modeling problem is transformed into the BN transfer learning problem. Furthermore, the new BN transfer learning strategy is proposed, which includes the structure and parameters transfer learning methods. For the structure transfer learning, by integrating the common structural information of multiple sources and the useful information of target, the final structure of target is determined. For the parameters transfer learning, by distinguishing the similarity of multiple sources, the parameters of target are obtained by the fusion algorithm. Finally, the proposed method is verified by the Asia network and it is applied to establish the safety control model for the thickening process of gold hydrometallurgy. The simulation results demonstrate that the proposed method is effective and owns the better performances than the traditional modeling method. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 15	2020	192								105297	10.1016/j.knosys.2019.105297													
J								Three-domain fuzzy wavelet broad learning system for tremor estimation	KNOWLEDGE-BASED SYSTEMS										Three-domain fuzzy wavelet broad learning system (TDFW-BLS); Tremor; Tele-operation	PHYSIOLOGICAL TREMOR; MULTISTEP PREDICTION; NEURAL-NETWORKS; FILTER	This paper proposes a new three-domain fuzzy wavelet broad learning system (TDFW-BLS) for tremor estimation in tele-operation. In the feature layer of our novel method, feature nodes can be mapped by a three-domain fuzzy wavelet sub-systems (3DFWs) and the k-means method is applied to determine the parameters in the TDFWs. The architecture of the new proposed system maps the input of different dimensions to different groups feature nodes by 3DFWS. In the enhancement layer, feature nodes are mapped to enhancement nodes. Moreover, feature nodes and increment nodes can be concatenated into a matrix to map the total output of the novel system by the full connection layer. Finally, the semi-physical simulation experiment is designed to demonstrate the effectiveness of the novel TDFW-BLS. Meanwhile, it is compared with some existing methods and the results have shown superior performance. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 15	2020	192								105295	10.1016/j.knosys.2019.105295													
J								Dealing with class imbalance in classifier chains via random undersampling	KNOWLEDGE-BASED SYSTEMS										Multi-label learning; Class imbalance; Classifier chains; Undersampling	MULTI-LABEL; ALGORITHM	Class imbalance is an intrinsic characteristic of multi-label data. Most of the labels in multi-label data sets are associated with a small number of training examples, much smaller compared to the size of the data set. Class imbalance poses a key challenge that plagues most multi-label learning methods. Ensemble of Classifier Chains (ECC), one of the most prominent multi-label learning methods, is no exception to this rule, as each of the binary models it builds is trained from all positive and negative examples of a label. To make ECC resilient to class imbalance, we first couple it with random undersampling. We then present two extensions of this basic approach, where we build a varying number of binary models per label and construct chains of different sizes, in order to improve the exploitation of majority examples with approximately the same computational budget. Experimental results on 16 multi-label datasets demonstrate the effectiveness of the proposed approaches in a variety of evaluation metrics. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 15	2020	192								105292	10.1016/j.knosys.2019.105292													
J								Coverless steganography based on image retrieval of DenseNet features and DWT sequence mapping	KNOWLEDGE-BASED SYSTEMS										Coverless image steganography; Deep learning; DenseNet convolutional neural network; Image retrieval; Discrete wavelet transform	OPTIMIZATION; WATERMARKING; SIFT	Recently, researches have shown that coverless image steganography can resist the existing steganalysis tools effectively. On this basis, a novel coverless image steganography algorithm based on image retrieval of DenseNet features and DWT sequence mapping is proposed in this paper. Firstly, DenseNet convolutional neural network model in deep learning is used to extract the features of image datasets. Supervised learning is adopted to retrieve the image, and the retrieval results can be used as the information carrier. Secondly, the selected images are divided into 4 X 4 sub-blocks for block discrete wavelet transform. Then the DWT coefficient are calculated based on the low-frequency components after block transformation, and the coefficients between blocks are scanned according to the Zigzag scan, such that the robust feature sequences are generated. Finally, the secret information is divided into segments with the same length as the feature sequence, and an inverted index is established with feature sequence, the position of blocks, DWT coefficient and image path. The image with the same feature sequence as the secret information segment is selected through index as the carriers. The experimental results and analysis show that this method has better robust and security performance resisting most image attacks compared with the state-of-the-art methods. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 15	2020	192								105375	10.1016/j.knosys.2019.105375													
J								A knowledge-based heterogeneity characterization framework for 3D steam-assisted gravity drainage reservoirs	KNOWLEDGE-BASED SYSTEMS										SAGD; Deep learning; Convolutional neural network; Proxy model; Shale barriers; Heterogeneity modeling; Optimization	NUMERICAL-SIMULATION; NEURAL-NETWORK; HEAVY OIL; RECOVERY; CLASSIFICATION; OPTIMIZATION; CNN; ALGORITHM; ZONES	In typical field-scale steam-assisted gravity drainage (SAGD) projects, shale barriers would often act as flow barriers that may adversely impact the ensuing steam chamber development. Efficient inference and proper representation of such heterogeneities from production data remain challenging. A novel hybrid knowledge-based workflow for 3D SAGD heterogeneity inference is presented. A convolutional neural network (CNN) proxy model is integrated with the genetic algorithm (GA) to infer shale parameters. A total number of 1000 heterogeneous 3D cases are constructed and subjected to numerical simulation and the corresponding production data is recorded. A dataset is assembled from the simulation results corresponding to these 1000 cases to train a set of CNN-based proxy models: discrete wavelet transform (DWT) is applied to parameterize a 3D reservoir model and the corresponding production time-series data. A GA-based workflow is introduced to infer the unknown shale barrier arrangement from a given (known) production profile by searching for a set of shale barrier parameters that would minimize the difference between the known profile and the predictions. The proposed workflow would yield an ensemble of 3D models of shale barrier distribution that are consistent with the actual production histories. The proposed methodology is tested with cases involving both idealized and irregularly-shaped shale barrier configurations. The proposed hybrid characterization workflow provides a robust and computationally-efficient alternative for inferring uncertain 3D heterogeneous features and can be easily extended to solve other similar inverse problems in various engineering fields. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 15	2020	192								105327	10.1016/j.knosys.2019.105327													
J								An analysis on new hybrid parameter selection model performance over big data set	KNOWLEDGE-BASED SYSTEMS										Big data; Parameter selection; Analysis tool; Decision; Hybrid method	ROUGH SET; SOFT SETS; REDUCTION	Parameter selection or attribute selection is one of the crucial tasks in the data analysis process. Incorrect selection of the important attribute might generate imprecise or event for a wrong decision. It is an advantage if the decision-maker could select and apply the best model that helps in identifying the best-optimized attribute set - in the decision analysis process. Recently, many data scientists from various application areas are attracted to investigate and analyze the advantages and disadvantages of big data. One of the issues is, analyzing large volumes and variety of data in a big data environment is very challenging to the data scientists when there is a lack of a suitable model or no appropriate model to be implemented and used as a guideline. Hence, this paper proposes an alternative parameterization model that is able to generate the most optimized attribute set without requiring a high cost to learn, to use, and to maintain. The model is based on two integrated models that are combined with correlation-based feature selection, best-first search algorithm, soft set, and rough set theories which were compliments to each other as a parameter selection method. Experimental have shown that the proposed model has significantly shown as an alternative model in a big data analysis process. (C) 2020 The Authors. Published by Elsevier B.V.																	0950-7051	1872-7409				MAR 15	2020	192								105441	10.1016/j.knosys.2019.105441													
J								A novel approach for modeling positive vectors with inverted Dirichlet-based hidden Markov models	KNOWLEDGE-BASED SYSTEMS										Hidden Markov Models; Inverted Dirichlet; Texture categorization; Occupancy detection; Smart buildings; Facial expressions recognition	TEXTURE CLASSIFICATION; PROBABILISTIC FUNCTIONS; GENERALIZED DIRICHLET; OCCUPANCY DETECTION; ANOMALY DETECTION; SELECTION; INFORMATION; APPEARANCE; INFERENCE; MIXTURES	Hidden Markov Models (HMMs) are among the most remarkably powerful probabilistic models, that although been acknowledged for decades have recently made a huge resurgence in the machine learning field. Their ever-growing use to model diversified and heterogeneous data (image,video, audio, time series) in numerous important practical situations is the subject of all forms of perpetual extensions. This work presents what we believe to be the first integration of the Inverted Dirichlet (ID) Mixture Models into the framework of HMMs. The proposed method uses the inverted Dirichlet mixtures to model the emission probabilities also known as observation probabilities. This extension (IDHMM), is motivated by the proven capacity of these mixtures to deal with positive vectors and overcome mixture models' capability to take into account any ordering or temporal constraints relative to information. The complete inference and parameter estimation are detailed in this work. Applications in the context of image categorization and indoor occupancy detection demonstrate higher performance compared to the extensively used Gaussian mixture-based Hidden Markov Model (GHMM) and the Dirichlet mixture-based hidden Markov Model (DHMM). (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 15	2020	192								105335	10.1016/j.knosys.2019.105335													
J								MLACO: A multi-label feature selection algorithm based on ant colony optimization	KNOWLEDGE-BASED SYSTEMS										Multi-label feature selection; Ant colony optimization; Cosine similarity; Correlation similarity	GRAVITATIONAL SEARCH ALGORITHM	Nowadays, with emerge the multi-label datasets, the multi-label learning processes attracted interest and increasingly applied to different fields. In such learning processes, unlike single-label learning, instances have more than one class label simultaneously. Also, multi-label learning suffers from the curse of dimensionality, and thus, the feature selection becomes a difficult task. In this paper, we propose a novel multi-label relevance-redundancy feature selection method based on Ant colony optimization (ACO) for the first time, called MLACO. By introducing two unsupervised and supervised heuristic functions, MLACO tries to search in the features space to find the most promising features with the lowest redundancy (unsupervised) and highest relevancy with class labels (supervised) through several iterations. For speeding up the convergence of the algorithm, the normalized cosine similarity between features and class labels have been used as the initial pheromone of each ant. The proposed method does not take into account any learning algorithm, and it can be classified as a filter-based method. We compare the performance of the MLACO against five well-known and state-of-the-art feature selection methods using ML-KNN classifier. The experimental results on several frequently used datasets show the superiority of the MLACO in different multi-label evaluation measures criteria and runtime. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 15	2020	192								105285	10.1016/j.knosys.2019.105285													
J								Human-machine dialogue modelling with the fusion of word- and sentence-level emotions	KNOWLEDGE-BASED SYSTEMS										Emotional intelligence; Human-machine dialogue system; Dictionary matching; Machine learning	GENERATION	Emotion intelligence plays an important role in building a successful human-machine dialogue system. However, the extreme difficulty of capturing emotional information of social text, as well as the weakness of generative models for learning emotional expression, limits the performance of existing dialogue system. Combining dictionary matching and machine learning, this paper proposes a generative model, which fuses the word- and sentence-level emotions, to model the dialogue text and learn emotional expression. The model first obtains the emotional embedding of each word through dictionary matching, then concatenates the emotional word embedding with its traditional word embedding, and the finally formed vector is taken as the input of the encoder. In order to control the emotional feature of the generated response, our model employs a BernoulliNB-based classifier to extract the emotional feature of the post, which is used as the attribute of the original text, and subsequently adds it to the decoder. For further significantly improving the emotional expression of the response, the model leverages a discriminator to constrain the latent variable, which enables the latent variable to encode better the information of emotional feature of the post. With this model, we can generate the dialogue text that is consistent with the original emotion. Experimental results on our Twitter dataset demonstrate that our model outperforms several state-of-the-art methods in the emotion accuracy and quality of generated texts. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 15	2020	192								105319	10.1016/j.knosys.2019.105319													
J								Note on "Bipolar fuzzy graphs with applications"	KNOWLEDGE-BASED SYSTEMS										Bipolar fuzzy graph (BFG); Degree and order; Neighborly and highly irregular bipolar fuzzy graphs; Bipolar fuzzy digraph; Counterexample		In the paper, "Bipolar fuzzy graphs with applications" by Akram (2013), Definitions 3.2, 3.3. 3.4, 3.5, 4.3 and Theorem 3.16 contain some flaws. In this note, we show by examples that these definitions and theorem are not correct. The updated version of these definitions and theorem have been established in this paper with numerical examples. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 15	2020	192								105315	10.1016/j.knosys.2019.105315													
J								Predicting literature's early impact with sentiment analysis in Twitter	KNOWLEDGE-BASED SYSTEMS										Altmetrics; Twitter; Sentiment analysis; User category; Predicting citations	RESEARCH EXCELLENCE; TWEETS LINKING; ALTMETRICS; AGREEMENT	Traditional bibliometric techniques gauge the impact of research through quantitative indices based on the citations data. However, due to the lag time involved in the citation-based indices, it may take years to comprehend the full impact of an article. This paper seeks to measure the early impact of research articles through the sentiments expressed in tweets about them. We claim that cited articles in either positive or neutral tweets have a more significant impact than those not cited at all or cited in negative tweets. We used the SentiStrength tool and improved it by incorporating new opinion-bearing words into its sentiment lexicon pertaining to scientific domains. Then, we classified the sentiment of 6,482,260 tweets linked to 1,083,535 publications covered by Altmetric.com. Using positive and negative tweets as an independent variable, and the citation count as the dependent variable, linear regression analysis showed a weak positive prediction of high citation counts across 16 broad disciplines in Scopus. Introducing an additional indicator to the regression model, i.e. 'number of unique Twitter users', improved the adjusted R-squared value of regression analysis in several disciplines. Overall, an encouraging positive correlation between tweet sentiments and citation counts showed that Twitter-based opinion may be exploited as a complementary predictor of literatures early impact. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 15	2020	192								105383	10.1016/j.knosys.2019.105383													
J								Evolutionary multitasking fuzzy cognitive map learning	KNOWLEDGE-BASED SYSTEMS										Fuzzy cognitive maps; Evolutionary multitasking; Multiobjective optimization; Gene regulatory network reconstruction	TIME-SERIES; MEMETIC ALGORITHM; C-MEANS; PREDICTION; NETWORK; SHRINKAGE; SYSTEM	In real-world applications, there exist multiple fuzzy cognitive maps (FCMs) learning tasks with similar attributes that have to be optimized simultaneously, however, all existing algorithms were designed to learn single FCM without considering the valuable patterns that can share with each other. For the purpose of making use of similar structure patterns among different tasks, we introduce the evolutionary multitasking framework to learn different FCMs at one time by taking each FCM learning problem as a task. Most proposed evolutionary-based algorithms learn FCMs from time series by minimizing data error which evaluates the difference between generated response sequences and available response sequences, which did not take the sparsity of the weight matrix into consideration. To learn large-scale FCMs for each task, in this paper we adopt a decomposition strategy based multiobjective optimization algorithm considering both the measure error and sparsity of FCMs. Moreover, the memetic algorithm and LASSO initialization operator are incorporated into the multitasking framework to improve the performance and accelerate the convergence. Through the whole process, we find that multitasking optimization can not only learn various FCMs in a population but also improve the accuracy of similar tasks by taking the advantage of gene transfer for similar patterns. Extensive experiments on two-task FCM learning problems with varying number of nodes, densities and activation functions and the application for the problem of reconstructing gene regulatory networks have been conducted to illustrate that the proposal can learn large-scale FCMs with low errors in a fast convergence speed. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 15	2020	192								105294	10.1016/j.knosys.2019.105294													
J								A dynamic financial distress forecast model with multiple forecast results under unbalanced data environment	KNOWLEDGE-BASED SYSTEMS										Adaptive neighbor SMOTE; Chinese listed companies; Financial distress forecast; Random forest; Recursive ensemble approach	MINORITY OVERSAMPLING TECHNIQUE; RANDOM FORESTS; PREDICTION; ENSEMBLE; SMOTE	Corporate financial distress forecasts are important for companies, investors and regulatory authorities. However, as most financial distress forecast (FDF) models in previous studies were based on a single time dimension, they have tended to ignore the two key financial distress data characteristics, imbalanced data sets and concept drift of data stream. To overcome these problems, this study proposes a new dynamic financial distress forecasting (DFDF) approach, the Adaptive Neighbor SMOTE-Recursive Ensemble Approach (ANS-REA), that allows for multiple forecast results from unbalanced data streams. An empirical experiment was conducted on 373 financially distressed samples and 1119 matching normal Chinese listed companies from 2007 to 2017. With an overall average AUC, it was found that the Random Forest (RF) classifier outperformed other commonly used classifiers such as Support Vector Machine (SVM), Decision Tree (DT), baggingDT, oblique random forests (obRF), Kernel ridge regression (KRR) and Bayes in the classification of DFDF data. In addition, the proposed ANS-REA algorithm had better performance than SMOTE, ANS, Random Walk Over-Sampling Approach (RWO), Rapidly Converging Gibbs sampling Technique (racog), SMOTEboost, RUSboost, SMOTEbagging, wRACOG and Majority Weighted Minority Oversampling Technique (MWMOTE) methods in dealing with imbalanced data sets classification. Further, we found that the proposed model that combined the multiple forecast results is the effective way to solve the financial distress forecast problem. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 15	2020	192								105365	10.1016/j.knosys.2019.105365													
J								Automatic generation of meteorological briefing by event knowledge guided summarization model	KNOWLEDGE-BASED SYSTEMS										Meteorological domain; Fine-tuned BERT model; Event knowledge guided summarization; EKGS model; Briefing generation framework; Meteorological decision support platform	SOCIAL MEDIA; ENSO; NETWORK; TEXT	In recent years, frequent meteorological disasters have brought great suffering to people. The meteorological briefing is an effective way to realize the real-time perception of extreme meteorological events, which is of great significance for decision-makers to formulate plans and provide timely assistance. Traditional meteorological briefings primarily rely on physical sensors for data collection and are organized manually. However, such an approach has the disadvantages of rigid content, high cost, and poor real-time performance. As an emerging lightweight social sensor, social networks can respond to real world events in a timely and comprehensive manner, which also makes up for the shortcomings of the traditional methods. In this paper, we present an event knowledge guided summarization (EKGS) model to automatically summarize weibo posts in the meteorological domain. Our model consists of two modules: a summary generation module and an event knowledge guidance module. The event knowledge guidance module is used to guide and constrain the content generated by the summary generation module, so that it can generate the content with core knowledge of specific events, which are 14 types of extreme meteorological events defined by the China Meteorological Administration (CMA). Compared to other baseline models, our EKGS model achieves the best test results on all metrics. In addition, we construct an automatic meteorological briefing generation framework based on the EKGS model, which has been applied as an online service to the meteorological briefing overview module of the CMA Public Meteorological Service Center. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 15	2020	192								105379	10.1016/j.knosys.2019.105379													
J								Autonomous cell activation for energy saving in cloud-RANs based on dueling deep Q-network	KNOWLEDGE-BASED SYSTEMS										Cell activation; Deep reinforcement learning; Energy efficiency; Dueling deep Q-network; H-CRAN	USER ASSOCIATION; RESOURCE-ALLOCATION; MANAGEMENT	Heterogeneous cloud radio access network (H-CRAN) is a promising technology to help overcome the traffic density which in 5G communication networks. One of the main challenges that will arise in H-CRAN is how to minimize energy consumption. In this paper, a deep reinforcement learning method is used to minimize energy consumption. Firstly, we propose an autonomous cell activation framework and customized physical resource allocation schemes to balance energy consumption and QoS satisfaction in C-RANs. We formulate the cell activation problem as a Markov decision process(MDP). To solve the problem, we develop a dueling deep Q-network (DQN) based autonomous cell activation framework to ensure user QoS demand and minimized energy consumption with the minimum number of active RRHs under varying traffic demand. Simulation results illustrate the effectiveness of our proposed solution in minimized energy consumption in a network. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 15	2020	192								105347	10.1016/j.knosys.2019.105347													
J								Neighborhood multi-granulation rough sets-based attribute reduction using Lebesgue and entropy measures in incomplete neighborhood decision systems	KNOWLEDGE-BASED SYSTEMS										Neighborhood rough sets; Attribute reduction; Lebesgue measure; Pessimistic neighborhood entropy; Incomplete neighborhood decision systems	FEATURE-SELECTION; UNCERTAINTY MEASURES; GENE SELECTION; KNOWLEDGE; GRANULARITY; MODEL	For incomplete data with mixed numerical and symbolic attributes, attribute reduction based on neighborhood multi-granulation rough sets (NMRS) is an important method to improve the classification performance. However, most classical attribute reduction methods can only handle finite sets as to produce more attributes and lower classification accuracy. This paper proposes a novel NMRS-based attribute reduction method using Lebesgue and entropy measures in incomplete neighborhood decision systems. First, some concepts of optimistic and pessimistic NMRS models in incomplete neighborhood decision systems are given, respectively. Then, a Lebesgue measure is combined with NMRS to study neighborhood tolerance class-based uncertainty measures. To analyze the uncertainty, noise and redundancy of incomplete neighborhood decision systems in detail, some neighborhood multi-granulation entropy-based uncertainty measures are developed by integrating Lebesgue and entropy measures. Inspired by both algebraic view with information view in NMRS, the pessimistic neighborhood multi-granulation dependency joint entropy is proposed. What is more, the corresponding properties are further deduced and the relationships among these measures are discussed, which can help to investigate the uncertainty of incomplete neighborhood decision systems. Finally, the Fisher linear discriminant method is used to eliminate irrelevant attributes to significantly reduce computational complexity for high-dimensional datasets, and a heuristic attribute reduction algorithm with complexity analysis is designed to improve classification performance of incomplete and mixed datasets. Experimental results under seven UCI datasets and eight gene expression datasets illustrate that the proposed method is effective to select most relevant attributes with higher classification accuracy, as compared with representative algorithms. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 15	2020	192								105373	10.1016/j.knosys.2019.105373													
J								A probabilistic approach towards an unbiased semi-supervised cluster tree	KNOWLEDGE-BASED SYSTEMS										Semi-supervised learning; Cluster tree; Text classification	CLASSIFICATION; ENSEMBLE; MODELS	Conventionally, it is a prerequisite to acquire a good number of annotated data to train an accurate classifier. However, the acquisition of such dataset is usually infeasible due to the high annotation cost. Therefore, semi-supervised learning has emerged and attracts increasing research efforts in recent years. Essentially, semi-supervised learning is sensitive to the manner how the unlabeled data is sampled. However, the model performance might be seriously deteriorated if biased unlabeled data is sampled at the early stage. In this paper, an unbiased semi-supervised cluster tree is proposed which is learnt using only very few labeled data. Specifically, a K-means algorithm is adopted to build each level of this hierarchical tree in a decent top-down manner. The number of clusters is determined by the number of classes contained in the labeled data. The confidence error of the cluster tree is theoretically analyzed which is then used to prune the tree. Empirical studies on several datasets have demonstrated that the proposed semi-supervised cluster tree is superior to the state-of-the-art semi-supervised learning algorithms with respect to classification accuracy. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 15	2020	192								105306	10.1016/j.knosys.2019.105306													
J								Automatic sleep stages classification using optimize flexible analytic wavelet transform	KNOWLEDGE-BASED SYSTEMS										Sleep stages classification; EEG signal; Flexible analytic wavelet transform; Genetic algorithm; Ensemble classifier	DECISION-SUPPORT-SYSTEM; EEG SIGNALS; CHANNEL; FEATURES; IDENTIFICATION; DOMAIN; NETWORKS; ENTROPY; BRAIN; PHASE	Sleep stages classification avails the diagnosis and treatment of sleep-related disorders. The traditional visual inspection methods used by sleep-experts are time-consuming and error-prone. This framework proposes, an automatic sleep stages classification method based on optimize flexible analytic wavelet transform (OFAWT) for electroencephalogram (EEG) signals. In OFAWT, the parametric optimization is performed to obtain the most appropriate basis for the representation of EEG signals. The OFAWT parameters are selected by solving inequality constraints problem using the genetic algorithm. OFAWT decomposes EEG signal into band-limited basis or sub-bands (SBs). Time domain measures of SBs are used as features for the sleep stages EEG signals. The statistical significance of extracted features is assessed by multiple-comparison post hoc analysis of Kruskal-Wallis test, which ensures that reported features are statistically significant for the discrimination of sleep stages. The SB-wise features set is tested through the variants of decision tree, discriminant analysis, k-nearest neighbor, and ensemble classifiers for sleep stages classification. The ensemble classification model bagged-tree yields better classification accuracies for the classification of six to two sleep stages 96.03%, 96.39%, 96.48%, 97.56%, and 99.36%, respectively as compared to other existing methods. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 15	2020	192								105367	10.1016/j.knosys.2019.105367													
J								Soft large margin clustering for unsupervised domain adaptation	KNOWLEDGE-BASED SYSTEMS										Unsupervised domain adaptation; Domain shift; Soft large margin clustering; Cluster structure	UNIFIED FRAMEWORK; REGULARIZATION; ALGORITHM	Unsupervised domain adaptation (UDA) methods usually perform feature matching between domains by considering the domain shift. However, the cluster structure of data, which is one focus in traditional unsupervised learning, is not considered in those methods. In this paper, we attempt to explore such cluster structure in UDA. Specifically, a general transfer learning framework called Clustering for Domain Adaptation (DAC) has been proposed. DAC explores the cluster structure of target data with the help of source data. It seeks a domain-invariant classifier by simultaneously reducing the distribution shifts between domains and exploring the cluster structure for target instances. The optimization of DAC adopts the ADMM strategy, in which each iteration generates a closed-form solution. Empirical results demonstrate the effectiveness of DAC over several real datasets. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 15	2020	192								105344	10.1016/j.knosys.2019.105344													
J								A knowledge-light approach to personalised and open-ended human activity recognition	KNOWLEDGE-BASED SYSTEMS										Human activity recognition; Personalised HAR; Open-ended HAR; Zero-shot learning; Matching networks	CLASSIFICATION	Human Activity Recognition (HAR) is a core component of clinical decision support systems that rely on activity monitoring for self-management of chronic conditions such as Musculoskeletal Disorders. Deployment success of such applications in part depend on their ability to adapt to individual variations in human movement and to facilitate a range of human activity classes. Research in personalised HAR aims to learn models that are sensitive to the subtle nuances in human movement whilst Open-ended HAR learns models that can recognise activity classes out of the pre-defined set available at training. Current approaches to personalised HAR impose a data collection burden on the end user; whilst Open-ended HAR algorithms are heavily reliant on intermediary-level class descriptions. Instead of these "knowledge-intensive" HAR algorithms; in this article, we propose a "knowledge-light" method. Specifically, we show how by using a few seconds of raw sensor data, obtained through micro-interactions with the end-user, we can effectively personalise HAR models and transfer recognition functionality to new activities with zero re-training of the model after deployment. We introduce a Personalised Open-ended HAR algorithm, MNZ, a user context aware Matching Network architecture and evaluate on 3 HAR data sources. Performance results show up to 48.9% improvement with personalisation and up to 18.3% improvement compared to the most common "knowledge-intensive" Open-ended HAR algorithms. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 15	2020	192								105651	10.1016/j.knosys.2020.105651													
J								A local-gravitation-based method for the detection of outliers and boundary points	KNOWLEDGE-BASED SYSTEMS										Outlier detection; Boundary points; Local resultant force; Nearest neighbors; Data mining	ALGORITHMS	Detection of outliers and boundary points represents an effective, interesting and potentially valuable pattern, which may be more important than that of normal points. In order to detect outliers and boundary points, we propose a local-gravitation-based method in which each data point is viewed as an object with both mass and a local resultant force (LRF) generated by its neighbors. With the increase of neighbor, the LRF of outliers, boundary points and interior points varies at different rates. In this paper, the LRF changing rates of points with lower densities have higher scores, namely the changing rate of an outlier is greater than that of a boundary point and inner point. In other words, top-m ranked points can be identified as outliers, and the greater the LRF changing rate of a point is, the more likely it is a boundary point. The main advantage of our proposed method is that it does not depend on the choice of K value, which improves the detection performance. The experimental results on synthetic and real data sets show that the proposed method is better than the existing methods. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 15	2020	192								105331	10.1016/j.knosys.2019.105331													
J								Joint metric and feature representation learning for unsupervised domain adaptation	KNOWLEDGE-BASED SYSTEMS										Domain adaptation; Metric learning; Feature representation learning; JMFL	LOW-RANK; ALGORITHM	Domain adaptation algorithms leverage the knowledge from a well-labeled source domain to facilitate the learning of an unlabeled target domain, in which the source domain and the target domain are related but drawn from different data distributions. Existing domain adaptation approaches are either trying to explicitly mitigate the data distribution gaps by minimizing some distance metrics, or attempting to learn a new feature representation by revealing the shared factors and use the learned representation as a bridge of knowledge transfer. Recently, several researchers claim that jointly optimizing the distribution gaps and latent factors can learn a better transfer model. In this paper, therefore, we propose a novel approach which simultaneously mitigates the data distribution and learns a feature representation via a common objective. Specifically, we present joint metric and feature representation learning (JMFL) for unsupervised domain adaptation. JMFL, on the one hand, minimizes the domain discrepancy between the source domain and the target domain. On the other hand, JMFL reveals the shared underlying factors between the two domains to learn a new feature representation. We smoothly incorporate the two aspects into a unified objective and present a detailed optimization method. Extensive experiments on several open benchmarks verify that our approach achieves state-of-the-art results with significant improvements. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 15	2020	192								105222	10.1016/j.knosys.2019.105222													
J								Social image refinement and annotation via weakly-supervised variational auto-encoder	KNOWLEDGE-BASED SYSTEMS										Computer vision; Generative model; Variational auto-encoder; Image refinement; Image annotation	MATRIX; COMPLETION; RELEVANCE	The ever-increasing size of social images and their corresponding imperfect labels have made social image refinement and annotation a crucial problem in supervised learning. However, previous models based on nearest neighbors or matrix completion are limited when the social image set is huge and labels are highly sparse. Deep generative models utilize inference and generative networks to infer latent variables by introducing an observed data variable; they can handle imperfect data, capture noisy data, and fill in missing data variables. In this paper, we propose a new social image refinement and annotation model based on the weakly-supervised variational auto-encoder generative model. First, we formulate the social image refinement and annotation problem as a joint distribution of social images and labels in a probabilistic generative model. Secondly, we derive a new evidence lower bound object to handle imperfect labels. Thirdly, we design a new multi-layer neural network including inference and generative networks to optimize the new evidence lower bound efficiently. Finally, we perform a comparison of our model with other representative models on several real-world social image datasets. Experimental results on social image refinement and annotation tasks show that the proposed model is competitive or even better than existing state-of-the-arts. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 15	2020	192								105259	10.1016/j.knosys.2019.105259													
J								Superspreaders and superblockers based community evolution tracking in dynamic social networks	KNOWLEDGE-BASED SYSTEMS										Community detection; Community evolution tracking; Incremental clustering; Error accumulation; Evolutionary events identification; Dynamic social networks	OVERLAPPING COMMUNITIES; DISCOVERY	Detecting and tracking communities in dynamic social networks has been a grand multidisciplinary challenge. There are two key steps in tracking the community evolution of dynamic social networks, including dynamic community detection and evolutionary events identification. For dynamic community detection, incremental clustering has been used as one of the most efficient methods; however, incrementally detecting network communities may result in partition errors such that continuous error accumulation will cause a discrepancy between the computed community structure and the underlying ground-truth. For evolutionary events identification, core-node-based methods have been widely employed; however, they do not distinguish between the heterogeneous contributions of core nodes to different evolutionary events, thereby resulting in a reduced accuracy of evolutionary event identification. This paper introduces a novel two-stage method that circumvents both of these problems simultaneously. Firstly, we propose an error accumulation sensitive (EAS) incremental community detection method for dynamic social networks. In our novel EAS method, rather than updating the community structure partially, a dynamic network snapshot is totally re-partitioned once the error accumulation degree of incremental clustering exceeds a pre-defined threshold. Secondly, to identify different critical evolution events, we introduce a superspreaders and superblockers (SAS) based community evolution tracking method for dynamic social networks which utilizes the properties of superspreader and superblocker nodes, the two types of core nodes related to spreading outbreaks in social networks. Experiments conducted on artificial and real-world social networks demonstrate that our proposed method can both efficiently detect dynamic network communities and accurately identify all critical evolutionary events, outperforming a total of eight competing methods. Our two-stage EAS-SAS approach could thus represent a potential method of choice for many real-world applications to community discovery and community evolution tracking in dynamic social networks. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 15	2020	192								105377	10.1016/j.knosys.2019.105377													
J								UnLearnerMC: Unsupervised learning of dense depth and camera pose using mask and cooperative loss	KNOWLEDGE-BASED SYSTEMS										Deep learning; Depth estimation; Camera pose; Photometric loop consistency loss; Cooperative loss		In this paper, we propose an unsupervised learning framework, named UnLearnerMC, for jointly training monocular depth, camera pose, and segmentation from videos. Existing unsupervised methods typically exploit optical flow consistency to train the segmentation network and eliminate interference from moving objects. Our key point is to create SegMaskNet and corresponding training losses for distinguishing between moving objects and static scenes. Specifically, we eliminate misleading by exploiting boundary masks when moving objects are beyond the view boundary. The SegMaskNet constantly adjusts the allocation of static and moving object pixels via a cooperative loss to minimize the total loss during training. For the moving areas, we can use the re-estimated static depth and pose to eliminate interference from moving areas by using the proposed photometric loop consistency loss. Experiments on KITTI datasets show that UnLearnerMC achieves state-of-the-art results in single-view depth and camera ego-motion, which illustrate the benefits of our approach. (C) 2019 Published by Elsevier B.V.																	0950-7051	1872-7409				MAR 15	2020	192								105357	10.1016/j.knosys.2019.105357													
J								A novel sequential three-way decisions model based on penalty function	KNOWLEDGE-BASED SYSTEMS										Sequential three-way decisions; Classification accuracy; Penalty function; Cost parameter; Granular computing	PROBABILISTIC ROUGH SET; FEATURE-SELECTION; RISK DECISION; APPROXIMATIONS; GRANULATION; REDUCTION	In the sequential three-way decisions (S3WD) model, classification accuracy is an important issue. Based on the idea of minimum misclassification, loss functions have been used to calculate the thresholds. A lot of achievements have been achieved in the related research by using the idea of minimum misclassification. However, few researchers have focused on minimizing the misclassification by improving the classification precision in different granularity layers. In this paper, from the classification precision difference between two adjacent granularity layers, a new sequential three-way decisions model based on penalty function (S3WDPF) is proposed to improve the classification accuracy by modifying cost parameters. First, two types of negative benefit classification in the S3WD model are defined. Next, based on the idea of optimization, the penalty function is devised to optimize the cost parameters. Then, from the viewpoint of Bayesian minimum risk decision, a decision rule of mutually exclusive decision thresholds is designed, and the rules of three-way decisions are deduced. Further, the change rules of decision thresholds after modifying cost parameters are discussed 'in detail. Finally, the experimental results show that the performance of the S3WDPF model has improved classification accuracy compared with the current existing models. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 15	2020	192								105350	10.1016/j.knosys.2019.105350													
J								Learning robust word representation over a semantic manifold	KNOWLEDGE-BASED SYSTEMS										Distributed word embedding; Natural language processing; Manifold assumption	REGULARIZED MATRIX COMPLETION	The performance of the traditional Word2Vec model heavily depends on the quality and quantity of the corpus, which violates the way of the humans learn. To understand word meaning, human beings prefer a two-stage learning process. That is, reading a Linguist-compiled dictionary as well as doing reading comprehension. These two stages complement each other. Traditional Word2Vec is an analogy of reading comprehension. While the first stage, learning the semantic rules from a language dictionary, such as the knowledge of thesaurus and etymology, is usually ignored by existing methods. In this work, we propose a robust word embedding learning framework by imitating the two-stage human learning process. In particular, we construct a semantic manifold based on the thesaurus and etymology to approximate the first stage. Then, we regularize the second stage (Word2Vec model) with this semantic manifold. We train the proposed model on three corpora (Wikipedia, enwik9 and text8). The experimental results demonstrate that the proposed method learns much smoother vector representations. Also, the performance on learning word embedding is robust even when the method is trained with a very simple corpus. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				MAR 15	2020	192								105358	10.1016/j.knosys.2019.105358													
J								Performance comparison of sampling designs for quality and safety control of raw materials in bulk: A simulation study based on NIR spectral data and geostatistical analysis	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Near infrared spectroscopy; Geostatistics; Kriging; Real-time evaluation; In situ monitoring; Spatial analysis	NEAR-INFRARED SPECTROSCOPY	This study exploits the potential of near infrared (NIR) spectroscopy to deliver a measurement for each sampling point. Furthermore, it provides a protocol for the modelling of the spatial pattern of analytical constituents. On the basis of these two aspects, the methodology proposed in this work offers an opportunity to provide a real-time monitoring system to evaluate raw materials, easing and optimising the existing procedures for sampling and analysing products transported in bulk. In this paper, Processed Animal Proteins (PAPs) were selected as case study, and two types of quality/safety issues were tested in PAP lots induced by moisture and cross-contamination. A simulation study, based on geostatistical analysis and the use of a set of sampling protocols, made a qualitative analysis possible to compare the representation of the spatial surfaces produced by each design. Moreover, the Root Mean Square Error of Prediction (RMSEP), calculated from the differences between the analytical values and the geostatistical predictions at unsampled locations, was used to measure the performance in each case. Results show the high sensitivity of the process to the sampling plan used understood as the sampling design plus the sampling intensity. In general, a gradual decrease in the performance can be observed as the sampling intensity decreases, so that unlike for higher intensities, the too low ones resulted in oversmoothed surfaces which did not manage to represent the actual distribution. Overall, Stratified and Simple Random samplings achieved the best results in most cases. This indicated that an optimal balance between the design and the intensity of the sampling plan is imperative to perform this methodology.																	0169-7439	1873-3239				MAR 15	2020	198								103940	10.1016/j.chemolab.2020.103940													
J								Acid-base equilibrium of guttiferone-A in ethanol-water mixtures: Modeling and bootstrap-based evaluation of uncertainties	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Acidic dissociation constants; Guttiferone-A; Confidence interval; Bootstrap	SYMPHONIA-GLOBULIFERA; PH-MEASUREMENTS; BENZOPHENONES; CONSTITUENTS; ALGORITHM; CONSTANTS	Acid dissociation constants are important parameters for indicating the extent of dissociation at different pH values, which is directly reflected in the absorption and elimination of drugs. In this present study, the acid-base equilibrium of guttiferone-A - an important natural benzophenone with innumerable biological activities - was investigated. The pK(a) values were determined, via the spectrophotometric method, in six different ethanol-water mixtures, and a significant linear correlation for pK(a2) as a function of the alcohol percentage was found. A proposition was made for the order of the dissociation sites of the guttiferone-A, based on the value of pK(a) for the 7-epiclusianone. The modeling and the study of sensitivity enabled the molar absorptivity spectra as well as the regions of highest sensitivity for inversion of the constants to be obtained. Complementing this study, the bootstrap of residuals technique was evaluated for assessment of the confidence intervals for the fitted constants - the results were equivalent to those encountered via the parametric method.																	0169-7439	1873-3239				MAR 15	2020	198								103938	10.1016/j.chemolab.2020.103938													
J								On the potential and limitations of multivariate curve resolution in Mossbauer spectroscopic studies	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Mossbauer spectroscopy; Multivariate curve resolution; Data processing; Chemometrics	ALTERNATING LEAST-SQUARES; EVOLVING FACTOR-ANALYSIS; MIXTURE ANALYSIS; CHROMATOGRAPHY; CONSTRAINTS; AMBIGUITY; RANK	Traditional processing of Mossbauer spectroscopy measurements assumes a decomposition of the spectra into separate multiplets corresponding to particular non-equivalent states of the resonance atom. When the number of spectra is large (e.g. in kinetic, corrosion and phase transition studies), this procedure becomes time-consuming. Moreover, traditional processing assumes some hypotheses on the number of non-equivalent states and initial multiplet parameters. The results of the processing strongly depend on these hypotheses and may be quite subjective. In an attempt to circumvent this issue, we studied the potential of Multivariate curve resolution (MCR) to unravel mixed multiplets spectra into their individual contributions. The application of MCR to Mossbauer studies was found to be quite challenging due to 1) long acquisition times limiting the number of available samples, 2) presence of critical spectral overlaps and 3) occasional deviations from the ideal bilinear assumption. In this report, we show how these limitations can be circumvented under certain conditions.																	0169-7439	1873-3239				MAR 15	2020	198								103941	10.1016/j.chemolab.2020.103941													
J								Essential processing methods of hyperspectral images of agricultural and food products	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Hyperspectral image; Pre-processing method; Uneven illumination; Post-processing method; Distribution maps	WATER-HOLDING CAPACITY; APPLE FRUIT FIRMNESS; NONDESTRUCTIVE DETERMINATION; NOISE REMOVAL; QUALITY; CLASSIFICATION; PREDICTION; BEEF; TIME; IDENTIFICATION	Hyperspectral images integrate spatial and spectral details together. They can provide valuable information about both external physical and internal chemical characteristics of agricultural and food products rapidly and non-destructively. Despite rapid improvements in instruments and acquisition techniques, the collected high-quality hyperspectral images still contain much useless information, like uneven illumination, background, specular reflection, and bad pixels that need to be removed. That is, hyperspectral image preprocessing is necessary for almost each hyperspectral image to get pure images or pixels, or to reduce negative influences on the subsequent detection, classification, and prediction analysis. This manuscript will enumerate some possible solutions to deal with issues mentioned above before further image analyzing. The advantages and disadvantages of different methods when dealing with a specific problem are also discussed. Obtained clean images or pure signals can be used for further data analysis. Finally, post-processing of hyperspectral images can be carried out to enhance the classification result of images or to generate chemical images/distribution maps to show spatial component concentration distributions of non-homogeneous samples.																	0169-7439	1873-3239				MAR 15	2020	198								103936	10.1016/j.chemolab.2020.103936													
J								mdatools - R package for chemometrics	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										mdatools; R; Chemometrics; Software		The paper describes mdatools - R package, which implements mainly basic but also some advanced chemometric methods providing a unified interface and user experience. The package was created to give a low entry level for beginners, so they can start using the implemented methods without writing much of code. While progressing, though, users can also have direct access to all computed results thus extending the package functionality by writing own code on top.																	0169-7439	1873-3239				MAR 15	2020	198								103937	10.1016/j.chemolab.2020.103937													
J								A partition-based variable selection in partial least squares regression	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										K-means clustering; Partial least squares; Variable selection	DIMENSION REDUCTION; PLS-REGRESSION; MODEL	Partial least squares regression is one of the most popular modelling approaches for predicting spectral data and identifying key wavelengths when combining with many variable selection methods. But some traditional variable selection approaches often overlook the local or group information between the covariates. In this paper, a partition-based variable selection in partial least squares (PARPLS) method is proposed. It first uses the k-means algorithm to part the variable space and then estimates the coefficients in each group. Finally, these coefficients are sorted to select the important variables. The results on three near-infrared (NIR) spectroscopy datasets show that the PARPLS is able to obtain better prediction performance and more effective variables than its competitors.																	0169-7439	1873-3239				MAR 15	2020	198								103935	10.1016/j.chemolab.2020.103935													
J								Constructing response surface designs with orthogonal quadratic effects using cyclic generators	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Augmented-pair designs; Composite designs; Circulant matrices; Orthogonal quadratic effects; Plackett-Burman design		The central composite designs (CCDs [1]) and small composite designs (SCDs [2,3]) are designs for sequential experimentation for response surface optimization. The CCDs for fitting the second-order response surface require a 2-level factorial or a resolution V fraction at the first stage (screening stage). The SCDs developed for fitting the same model require many fewer runs at the first stage as they only require a resolution III* fraction. This paper introduces an algorithm which can augment a 2-level first-order design with additional 3-level runs to form a second-order design. This algorithm does not require the 2-level first-order design in stage I to be a resolution V or resolution III* fraction. These augmented runs are made up of circulant matrices. Since CCDs and SCDs are special cases of the designs constructed this way, we call the new designs generalized composite designs or GCDs. Like CCDs and SCDs, GCDs have orthogonal quadratic effects. GCDs can often be found with numbers of runs between those of SCDs and CCDs. This is useful because SCDs often have poorly estimated parameters and CCDs often require substantially more runs than required to fit a full quadratic model.																	0169-7439	1873-3239				MAR 15	2020	198								103918	10.1016/j.chemolab.2019.103918													
J								Transfer learning based on incorporating source knowledge using Gaussian process models for quick modeling of dynamic target processes	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Gaussian process model; Multi-step ahead prediction; Propagation of uncertainty; Transfer learning	BAYESIAN CALIBRATION; MIGRATION; DESIGN	To maintain optimum economic process performance, a good process model is the cornerstone of an optimal scheduling strategy and controller design. Up to now, approaches to dynamic modeling have already been studied, but the models they constructed are only valid in their corresponding operating conditions. As operating conditions switch fast during the production, the constructed model may lack the extrapolating capability and may not describe the process behaviors in the new operating condition properly. Especially in the case that only a small number of data can be collected from the new operating condition for the construction of the model; the performance of the model may not be guaranteed for online new data. In this paper, a dynamic transfer modeling approach based on the Gaussian process model (GPM) is proposed. It can quickly model the target process and get correct predictions, by transferring source model knowledge trained with a sufficient number of historical data to a target model with a small number of available target data. This can significantly reduce the amount of time waiting for getting the target process data and quickly achieve a good process model. The statistical approach leverages GPM to transfer the knowledge. GPM is introduced to capture the uncertainty that propagates from the source process to the target process. Thus, the multi-step ahead prediction of the target model can provide the mean prediction as well as probabilistic information for its prediction in the form of a predictive variance. Finally, CSTR and the real furnace system are used to demonstrate the features of the proposed method and the applicability to a real plant process.																	0169-7439	1873-3239				MAR 15	2020	198								103911	10.1016/j.chemolab.2019.103911													
J								Role-Value Maps and General Concept Inclusions in the Minimal Description Logic with Value Restrictions or Revisiting Old Skeletons in the DL Cupboard	KUNSTLICHE INTELLIGENZ										Description logic; Value restrictions; Role-value maps; FL0; Decidability and complexity		We investigate the impact that general concept inclusions and role-value maps have on the complexity and decidability of reasoning in the description logic FL0. On the one hand, we give a more direct proof for ExpTime-hardness of subsumption w.r. t. general concept inclusions in FL0. On the other hand, we determine restrictions on role-value maps that ensure decidability of subsumption, but we also show undecidability for the cases where these restrictions are not satisfied.																	0933-1875	1610-1987				SEP	2020	34	3			SI		291	301		10.1007/s13218-020-00651-0		MAR 2020											
J								Copy-move forgery detection using SURF feature extraction and SVM supervised learning technique	SOFT COMPUTING										Copy-move forgery detection; SURF; SVM; Object recognition; Feature matching		The goal of computer vision is to identify objects of interest from images. Copy-move forgery is the process of copying one or more parts of an image and moved into another part of an equivalent image. The detection of copy-move images has many limitations in recognition of objects. In this paper, the proposed work uses Speeded Up Robust Feature (SURF) feature extraction, and the specific object is recognized with the help of the support vector machine. When copy-move forgery was performed, some modifications were done to the image. For instance, turning, scaling, darkening, compression, and noise addition are applied to make effective impersonation forgeries. Here, feature matching process uses the image rotate function, which consists of bicubic and crop operations, and calculates the difference using the blend, scale and joint operation. The results show that forged images are extracted from a given set of test images. The test results exhibit that the proposed technique can get noteworthy and impressive results.																	1432-7643	1433-7479				OCT	2020	24	19					14429	14440		10.1007/s00500-020-04795-x		MAR 2020											
J								Improving the prediction rate of unusual behaviors of animal in a poultry using deep learning technique	SOFT COMPUTING										Behaviors attributes; Sequential model; SSD algorithm; YOLO algorithm		Poultry farms across the world house animals such as cows, sheep's, pigs and hen These farms are the places from where the market gets meat, eggs, wool and other animal products which are used in our daily lives. But sometimes the business of the farms is severely hit primarily due to poultry animals catching diseases or these animals succumbing to injuries caused due to fighting each other. The identification of such animals showing unusual characteristics or behavior is necessary. We through our paper use deep learning concepts to help the poultry owner to identify these unusual characteristics in a sheep. Some of the characteristics that our paper identifies are skinny, redness, non-aggression, aggression, beefy and foraging. Our paper makes use of a video, shot from a camera and the implementation of a sequential model as well as make use of the SSD algorithm to identify and characterize the sheep's depicted in the video. The sequential model is trained using the training dataset which contains static images of sheep's and the attribute/characteristics depicted by those sheep's that are stored in a CSV file. The testing dataset contains the images that are extracted from the input video as frames. The testing dataset is passed through the sequential model to get the characteristics/attributes depicted by the sheep in each frame and store those characteristics/attributes in a CSV file. The SSD algorithm is trained on identifying the various animals and not only does it display the name of the animal detected, it displays the confidence percentage of the animal as well, in our paper it is used for identifying sheep. The above stated algorithm also creates a border around the identified sheep such that it can be used for tracking purposes during the entirety of the video. The SSD algorithm also takes in the attributes depicted by these frames and displays them along the border which identifies the sheep. The accuracy is compared with YOLO algorithms and shows 3 to 6% improvement on prediction rate.																	1432-7643	1433-7479				OCT	2020	24	19					14491	14502		10.1007/s00500-020-04801-2		MAR 2020											
J								BiPhase adaptive learning-based neural network model for cloud datacenter workload forecasting	SOFT COMPUTING										Adaptive learning; Cloud computing; Differential evolution; Ring crossover; Heuristic crossover; Uniform crossover; Workload forecasting	PREDICTION; ALGORITHM; ANN	Cloud computing promises elasticity, flexibility and cost-effectiveness to satisfy service level agreement conditions. The cloud service providers should plan and provision the computing resources rapidly to ensure the availability of infrastructure to match the demands with closed proximity. The workload prediction has become critical as it can be helpful in managing the infrastructure effectively. In this paper, we present a workload forecasting framework based on neural network model with supervised learning technique. An improved and adaptive differential evolution algorithm is developed to improve the learning efficiency of predictive model. The algorithm is capable of optimizing the best suitable mutation operator and crossover operator. The prediction accuracy and convergence rate of the learning are observed to be improved due to its adaptive behavior in pattern learning from sampled data. The predictive model's performance is evaluated on four real-world data traces including Google cluster trace and NASA Kennedy Space Center logs. The results are compared with state-of-the-art methods, and improvements up to 91%, 97% and 97.2% are observed over self-adaptive differential evolution, backpropagation and average-based workload prediction techniques, respectively.																	1432-7643	1433-7479				OCT	2020	24	19					14593	14610		10.1007/s00500-020-04808-9		MAR 2020											
J								An efficient green computing fair resource allocation in cloud computing using modified deep reinforcement learning algorithm	SOFT COMPUTING										Deep reinforcement; Fair resource allocation; Cloud computing	MANAGEMENT; FRAMEWORK; STRATEGY	Cloud computing provides services and resources in the Internet, and many applications are self-service-supported, on-demand resource allocation-adapted. These dynamic networks allocated necessary resource to the users' need and they require proper resource allocation scheme. Since various resources are consumed by users if resource allocation is not proper, this leads the system to load imbalance nature. Using Internet-connected devices for storage and computation not only communicates the cloud resources but also connects the devices to network through various protocols. These changes make the network into a complex, dense, heterogeneous system. In this paper, a green computing fair resource allocation through deep reinforcement learning model is proposed to provide efficient resource allocation scheme to the users in the network. Conventional Q-learning model fails in dimensionality problem when the state space increases exponentially. The proposed model is combined with fair resource allocation with deep reinforcement learning to provide better allocation schemes compared to the conventional model.																	1432-7643	1433-7479				OCT	2020	24	19					14933	14942		10.1007/s00500-020-04846-3		MAR 2020											
J								Fast replica recovery and adaptive consistency preservation for edge cloud system	SOFT COMPUTING										Edge cloud; Replica creation; Fast replica recovery; Adaptive consistency preservation	FUZZY AGGREGATION OPERATORS; OPTIMIZATION; PLACEMENT; STRATEGY; SCHEME	Edge cloud extends the power of cloud computing to the edge of the devices that are closest to the demands of big connection, low latency and large bandwidth. However, there are still many challenges due to the dynamic, heterogeneous and real-time bandwidth of the node in the edge cloud environment. This paper studies dynamic replica creation, fast replica recovery and adaptive consistency preservation. Before solving the problem of data inconsistency caused by frequent updates of replicas, we consider the strategy of fast replica recovery. Firstly, the DRC-DS is based on the regional structure, considering the number of replicas and the creation location of the data process and copying the data information with high access frequency and long average response time. Moreover, the FRR-LB is based on the heat ranking of the replica, the source node and the target node are determined according to the double-cycle lookup structure. Besides, the ACP-IMP is proposed to solve the problem of replica consistency in the environment of high failure rate of network and node. The more certainty replica nodes are elected as the leaders of edge nodes, and the message transmission of reconfirmation is reduced.																	1432-7643	1433-7479				OCT	2020	24	19					14943	14964		10.1007/s00500-020-04847-2		MAR 2020											
J								Artificial intelligence applied to the production of high-added-value dinoflagellates toxins	AI & SOCIETY										Artificial intelligence; Biotoxins; Dinoflagellates; Drug discovery; Neural networks	NEURAL-NETWORKS; SAXITOXIN; PREDICTION; MICROALGAE; CHEMISTRY	Trade in high-value-added toxins for therapeutic and biological use is expanding. These toxins are generally derived from microalgae belonging to the dinoflagellate family. Due to the difficulties to grow these sensitive planktonic species and to the complexity of methods used to synthesize these molecules, which are generally complex chemical structures, biotoxin manufacturers called onartificial intelligence technologies. Manufacturing processes have been greatly improved through the development ofspecific learning neural networks, applied to each phases of biotoxin production: photo-bioreactors operating at optimal yied; new chemical synthesis research processes; toxin biosynthetic research pathways offering short-cut possibilities.																	0951-5666	1435-5655															10.1007/s00146-020-00959-3		MAR 2020											
J								Motion path planning of soccer training auxiliary robot based on genetic algorithm in fixed-point rotation environment	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Soccer training assisted robot; Path planning; Genetic algorithm	ASSISTED RADICAL CYSTECTOMY; RECONSTRUCTION; REGULARITY; IMPACT; POSE	Soccer training assisted robot system is a combination of robotics and artificial intelligence. Motion path planning is an important part of soccer training assisted robot decision system. Path planning aims to find an optimal path, complete dynamic and static obstacle avoidance, and thus can timely and quickly carry out the path planning of the soccer training assisted robot cooperation strategy. Aiming at the problem of soccer training auxiliary robot motion path planning, this paper proposes a global path planning method based on genetic algorithm, which avoids the problem of unsmooth path and large computation. Under the premise of ensuring the completeness of search, this method transforms the soccer training assisted robot path planning problem into the path search problem of the soccer training assisted robot system center. Under the premise of using the fixed-point rotation method on a large scale, the genetic algorithm is adopted for the case of some special narrow-channel environments, and the action area is determined by simple search. Finally, the planning path of each soccer training-assisted robot in the system is obtained. Finally, the simulation results verify the effectiveness of the algorithm.																	1868-5137	1868-5145															10.1007/s12652-020-01877-4		MAR 2020											
J								Stock market prediction using machine learning classifiers and social media, news	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Deep learning; Feature selection; Hybrid algorithm; Natural language processing; Predictive modeling; Sentiment analysis; Stock market prediction	SALP SWARM ALGORITHM; INTELLIGENCE; OPTIMIZATION; ENSEMBLE; RETURN	Accurate stock market prediction is of great interest to investors; however, stock markets are driven by volatile factors such as microblogs and news that make it hard to predict stock market index based on merely the historical data. The enormous stock market volatility emphasizes the need to effectively assess the role of external factors in stock prediction. Stock markets can be predicted using machine learning algorithms on information contained in social media and financial news, as this data can change investors' behavior. In this paper, we use algorithms on social media and financial news data to discover the impact of this data on stock market prediction accuracy for ten subsequent days. For improving performance and quality of predictions, feature selection and spam tweets reduction are performed on the data sets. Moreover, we perform experiments to find such stock markets that are difficult to predict and those that are more influenced by social media and financial news. We compare results of different algorithms to find a consistent classifier. Finally, for achieving maximum prediction accuracy, deep learning is used and some classifiers are ensembled. Our experimental results show that highest prediction accuracies of 80.53% and 75.16% are achieved using social media and financial news, respectively. We also show that New York and Red Hat stock markets are hard to predict, New York and IBM stocks are more influenced by social media, while London and Microsoft stocks by financial news. Random forest classifier is found to be consistent and highest accuracy of 83.22% is achieved by its ensemble.																	1868-5137	1868-5145															10.1007/s12652-020-01839-w		MAR 2020											
J								Human behavior sensing: challenges and approaches	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Activities of Daily Living Scale; Behavior sensing; Human-centric approaches; Thing-centric approaches	ACTIVITY RECOGNITION; TRACKING; USAGE; HOME	In recent years, Activities of Daily Living Scale (ADLs) is widely used to evaluate living abilities of the patients and the elderly. So, the study of behavior sensing has attracted more and more attention of researchers. Behavior sensing technology is of strong theoretical and practical value in the fields of smart home and virtual reality. Most of the currently proposed approaches for tracking indicators of ADLs are human-centric, which classify activities using physical information of the observed persons. Considering the privacy concerns of the human-centric approaches (e.g. images of home environment, private behavior), researchers have also proposed some thing-centric approaches, which use environmental information on things (e.g. the vibration of things) to infer human activity. In this paper, by considering the unified steps in both the human-centric approaches and the thing-centric approaches, we make a comprehensive survey on the challenges and proposed methods to do behavior sensing, which are signal collection, preprocessing, feature extraction, and activity recognition. Moreover, based on the latest research progress, we post a perspective from our standpoint, discussing future outlook and challenges of human behavior sensing.																	1868-5137	1868-5145															10.1007/s12652-020-01861-y		MAR 2020											
J								Software quality analysis based on cost and error using fuzzy combined COCOMO model	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Software life cycle; COCOMO model; Software quality model; Fuzzy logic; Cost analysis; Cost estimation; Error analysis	ROBUST	Software quality analysis and estimation is essential in developing a software to avoid faults and increase the reliability. Software quality model (SQM) is highly concerned with standard metrics to qualify the software modules to classify bug or no bug. By using these models, it is easy to identify the hurdles called as errors or faults Apriori to the development cycle. More likely the metrics will not follow the standard protocol in terms of size, performance, technology and the complexity involved. It will vary across the projects. Surprisingly there is no model-based architecture driven tool is available to intact the baseline estimates of the project based on the previous knowledge resource. In earlier research works, various quality assurance metrics are used for analysing the SQ. Also, there is no existing approaches can do earlier prediction of the faults/errors or reduced misclassification rates. But, the COCOMO (COnstructive COst MOdel) gives an approximate estimate in terms of the month constant will not be same for simulating the study. Hence By combining more than one model estimates COCOMO and Gaussian Membership Function software estimate relative error will be the best suite. A fuzzy-based analogy is obtained in the present study to select the nearest path from the history available to meet the project cost and time. Small or standard training sets were considered to deploy the estimate, and compare the performance with different estimators. From the experiment, it is concluded that the proposed fuzzy-COCOMO model outperforms than the existing approaches in terms of relative error.																	1868-5137	1868-5145															10.1007/s12652-020-01783-9		MAR 2020											
J								MAMHOA: a multi-agent meta-heuristic optimization algorithm with an approach for document summarization issues	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										NP-hard problems; Meta-heuristic algorithms; Multi-agent meta-heuristic optimization algorithm; Biogeography-based optimization; Extractive text summarization	BIOGEOGRAPHY-BASED OPTIMIZATION; MODELS	Today, given the increasing volume of information and the difficulty of using them for specific applications such as email, websites, news, etc., the use of automated information summarization algorithms has become more popular than traditional algorithms. Taking advantage of computer algorithms, these algorithms produce a summary of information while retaining its original meaning. However, given its semantic and structural properties as well as the variable comparative parameters of information, the summarization process is considered as an NP-hard problem. Therefore, to solve these problems it is better to use meta-heuristic algorithms, which are generally inspired by the behavior of nature. These meta-heuristic algorithms help to better solve the hard problems through producing optimum solutions. In this paper, we propose an optimization algorithm named multi-agent meta-heuristic optimization algorithm (MAMHOA) for extractive text summarization. MAMHOA is a combination of biogeography-based optimization (BBO) algorithm and multi-agent systems concepts to generate an optimum summary. Several computational tests are used to evaluate the effectiveness and efficiency of the proposed algorithm, which is compared to other algorithms provided in the literature. MAMHOA and other algorithms are tested on DUC2002 datasets and attained solutions are analyzed using ROUGE metrics. From the results obtained, it can be seen that the proposed algorithm is more effective and efficient than those mentioned in the literature i.e. Baseline and state-of-the-art methods for different ROUGE metrics.																	1868-5137	1868-5145															10.1007/s12652-020-01776-8		MAR 2020											
J								Sentiment analysis of student feedback using multi-head attention fusion model of word and context embedding for LSTM	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										LSTM; Deep learning; Glove; Cove; Multi-head attention		Classroom teaching becomes viable and efficient based on increase in participation of the student. This can be made possible by taking needed measure by finding the emotions of the students. Many researchers worked on emotion identification of students. Now-a-days sentiment analysis using deep learning models have gained good performance. Especially ensemble Long Short-Term Memory (LSTM) with attention layers gives more attention to the influence word on the emotion. In the proposed method, input sequences of sentences are processed parallel across multi-head attention layer with fine grained embeddings (Glove and Cove) and tested with different dropout rates to increase the accuracy. Later in this paper, the information from both deep multi-layers is fused and fed as input to the LSTM layer. In this paper, we conclude that the fusion of multiple layers accompanied with LSTM improves the result over a commonNatural Language Processingmethod.																	1868-5137	1868-5145															10.1007/s12652-020-01791-9		MAR 2020											
J								Estimation of tool wear and optimization of cutting parameters based on novel ANFIS-PSO method toward intelligent machining	JOURNAL OF INTELLIGENT MANUFACTURING										ANFIS; Tool wear; VCPSO; Multi-objective optimization	FUZZY INFERENCE SYSTEM; SURFACE-ROUGHNESS; ALGORITHM; SELECTION	Compacted graphite iron (CGI) plays an important role in contemporary manufacturing of automobile engine, and coated tool is the best choice for milling of CGI. But studies about the estimation of the wear of coated tool are still rare and incomplete. As tool wear is the main factor that affects the quality of machined surface, in this study, we proposed an intelligent model-adaptive neuro fuzzy inference system (ANFIS) to estimate the tool wear, and ANFIS was learned by the improved particle swarm optimization (PSO) algorithm. As the PSO algorithm is easy to fall into the local minimum, the vibration and communication particle swarm optimization (VCPSO) algorithm was proposed by introducing the self-random vibration and inter-particle communication mechanisms. Besides that, to obtain the optimal combination of milling parameters, the multi-objective optimization based on minimum cutting power, surface roughness and maximum material removal rate (MRR) was studied using VCPSO algorithm. The experimental results showed that the ANFIS learned by VCPSO algorithm (ANFIS-VCPSO) has better performance for the estimation of tool wear compared with other intelligent models. The VCPSO algorithm was tested using Benchmark functions, and the results showed VCPSO algorithm has the global optimization ability. Meantime, the best combinations of milling parameters under different tool wear status were obtained through VCPSO algorithm. The proposed ANFIS-VCPSO model as a new intelligent model can be applied for real-time tool wear monitoring, which can improve the machining efficiency and prolong tool life. In order to meet the requirements of green and intelligent manufacturing, the best combination of milling parameters was also obtained in this work.																	0956-5515	1572-8145															10.1007/s10845-020-01559-0		MAR 2020											
J								Optimal pricing and sourcing strategies in the presence of supply uncertainty and competition	JOURNAL OF INTELLIGENT MANUFACTURING										Supply risk; Price competition; Quantity competition; Revenue sharing contract; Dual sourcing	REVENUE-SHARING CONTRACTS; PROCUREMENT STRATEGIES; COMMON RETAILER; WHOLESALE PRICE; COORDINATION; MANAGEMENT; SELECTION; SINGLE; RISK	The implementation of cost leadership strategy can enable enterprises to obtain a lasting competitive advantage. In this paper, we construct a supply chain with two competing suppliers and two competing manufacturers. The suppliers act as the Stackelberg leaders, selling components to the follower manufacturers. The manufacturers use different sourcing strategies, one of which only uses the reliable supplier, while another adopts contingent dual sourcing. We derive the analytical form of the equilibrium solution of order quantities of manufacturers and wholesale prices of suppliers in different scenarios and compare the decision differences when reliable supplier stays in different game positions. We further investigate the impact of different cooperation contract between the manufacturer who adopts dual sourcing and unreliable supplier on the procurement and pricing strategies by numerical experiments. The results illustrate that reliable supplier acts as the Stackelberg leader are more beneficial to suppliers. Manufacturer with dual sourcing can work with the unreliable supplier by revenue sharing contract to achieve a win-win situation.																	0956-5515	1572-8145															10.1007/s10845-020-01557-2		MAR 2020											
J								Interconnection and Damping Assignment Passivity-Based Control Design Under Loss of Actuator Effectiveness	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										IDA-PBC; Loss of actuator effectiveness; Passive FTC; RUAV	FAULT-TOLERANT CONTROL; MECHANICAL SYSTEMS; IDA-PBC; SUBJECT	Due to the convenience in applications, interconnection and damping assignment passivity-based control (IDA-PBC) is applied widely to reformulate the nonlinear robust control as the total energy shaping. However, only few researches focus on the fault-tolerant control (FTC) method based on IDA-PBC, which limits its applications under actuator faults. To break this limitation, this paper improves the IDA-PBC with fault-tolerant ability, and the main contributions are to propose high-gain and adaptive IDA-PBC methods under loss of actuator effectiveness. The simulation and experiment results with a rotorcraft unmanned aerial vehicle (RUAV) are presented to illustrate the control effectiveness of the improved IDA-PBC methods.																	0921-0296	1573-0409				OCT	2020	100	1					29	45		10.1007/s10846-020-01170-8		MAR 2020											
J								H-infinity stability analysis and output feedback control for fuzzy stochastic networked control systems with time-varying communication delays and multipath packet dropouts	NEURAL COMPUTING & APPLICATIONS										Stability analysis; Lyapunov-Krasovskii functional; Takagi-Sugeno (T-S) fuzzy model; Linear matrix inequalities (LMIs); Time-varying delay	CHAOTIC NEURAL-NETWORKS; NONLINEAR-SYSTEMS; SYNCHRONIZATION; STABILIZATION; DESIGN; SENSOR	The H-infinity stability analysis and delay-dependent Takagi-Sugeno (T-S) fuzzy dynamic output feedback control are proposed for the T-S fuzzy discrete networked control systems with time-varying communication delay and multipath packet dropouts. T-S fuzzy model is employed to approximate the discrete networked control system with time-varying state delay and external disturbance. Stochastic system theory and Bernoulli probability distribution are employed to describe the time-varying communication delay and multipath packet dropouts. Delay-dependent T-S fuzzy dynamic output feedback controller is designed. The delay-dependent T-S fuzzy dynamic output feedback controller is employed to relax the design conditions and enhance the design flexibility. The delay-dependent Lyapunov-Krasovskii functional, stochastic system theory and Bernoulli probability distribution are introduced to guarantee the stochastic mean-square stability and prescribed H-infinity performance. Some slack matrices are introduced to reduce the computation complexity. Finally, simulation examples are presented to show the effectiveness and advantages of the proposed methods.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14733	14751		10.1007/s00521-020-04826-6		MAR 2020											
J								Development of riverbank erosion rate predictor for natural channels using NARX-QR Factorization model: a case study of Sg. Bernam, Selangor, Malaysia	NEURAL COMPUTING & APPLICATIONS										Riverbank erosion; Natural river; Dimensionless analysis; Sensitivity analysis; NARX; QR factorization	FLOW	This study presents a novel and comprehensive model development technique to predict the riverbank erosion rate for a natural channel using a Nonlinear AutoRegressive model with eXogenous inputs and QR factorization parameter estimation, known as the NARX-QR Factorization model. The model was developed based on a 12-month extensive field measurement at Sg. Bernam. This study established the governing factors and derived dependent and independent variables for riverbank erosion using dimensional analysis, based on the Buckingham PI theorem. Two functional relationships were derived from dimensional analysis incorporating the factors governing riverbank erosion. The functional relationships include parameters of hydraulic characteristics of the channel, riverbank geometry and soil characteristics. Parameter estimation was conducted using a linear least squares technique to quantify riverbank erosion rates. The significant independent variables and fourteen models with several numbers of hidden layers were set as the input parameters to the NARX-QR Factorization model. The model performance analysis shows that Models 1 and 9, developed based on the proposed NARX-QR Factorization model, have the highestR(2)at 75% and 91%, respectively. Model 1 performed the best with accuracies for training and testing datasets of 75% and 73%, respectively. Additionally, the scatter plot of Model 1 is uniformly distributed along the line of perfect agreement. Therefore, it is concluded that the NARX-QR Factorization model developed in this study performed well in estimating the riverbank erosion rate, particularly for a natural river similar to Sg. Bernam.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14839	14849		10.1007/s00521-020-04835-5		MAR 2020											
J								Binary atom search optimisation approaches for feature selection	CONNECTION SCIENCE										Feature selection; atom search optimisation; binary atom search optimisation; classification; binary optimisation	CLASSIFICATION; ALGORITHM; COLONY	Atom Search Optimisation (ASO) is a recently proposed metaheuristic algorithm that has proved to work effectively on several benchmark tests. In this paper, we propose the binary variants of atom search optimisation (BASO) for wrapper feature selection. In the proposed scheme, eight transfer functions from S-shaped and V-shaped families are used to convert the continuous ASO into the binary version. The proposed BASO approaches are employed to select a subset of significant features for efficient classification. Twenty-two well-known benchmark datasets acquired from the UCI machine learning repository are used for performance validation. In the experiment, the BASO with an optimal transfer function that contributes to the best classification performance is presented. The particle swarm optimisation (PSO), binary differential evolution (BDE), binary bat algorithm (BBA), binary flower pollination algorithm (BFPA), and binary salp swarm algorithm (BSSA) are used to evaluate the efficacy and efficiency of proposed approaches in feature selection. Our experimental results reveal the superiority of proposed BASO not only in high prediction accuracy but also in the minimal number of selected features.																	0954-0091	1360-0494															10.1080/09540091.2020.1741515		MAR 2020											
J								Multiclass discriminant analysis via adaptive weighted scheme	NEUROCOMPUTING										Multiclass discriminant analysis; Feature extraction; Class separation problem; Adaptive weighted scheme		Under homoscedastic Gaussian assumption, it is demonstrated that conventional LDA is formulated by maximizing the weighted arithmetic mean of the Kullback-Leibler (KL) divergences between different classes. However, the calculation of projection directions is dominated by class pairs with large KL divergence, which causes class pairs with small KL divergence overlapping in the low-dimensional subspace when the dimensionality of subspace is strictly lower than c - 1, wherein c is the class number. Therefore, the classification accuracy is degraded significantly when dealing with classification task. In this paper, we propose a novel method, namely Multiclass Discriminant Analysis via Adaptive Weighted Scheme (MDAAWS), to alleviate the problem mentioned above by assigning weights to all between-class pairs adaptively. Since the proposed problem is a challenging task, an iterative algorithm is exploited to solve it and the corresponding theoretical analysis is presented as well. Extensive experiments conducted on various data sets demonstrate the effectiveness of MDAAWS when compared with some state-of-the-art supervised dimensionality reduction methods. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 14	2020	381						1	9		10.1016/j.neucom.2019.10.070													
J								Neuropod: A real-time neuromorphic spiking CPG applied to robotics	NEUROCOMPUTING										Neurorobotics; Spinnaker; Central pattern generator; Spiking neural network; Neuromorphic hardware; FPGA	CENTRAL PATTERN GENERATOR; LOCOMOTION; CONTROLLER; MOTION; SENSOR	Initially, robots were developed with the aim of making our life easier, carrying out repetitive or dangerous tasks for humans. Although they were able to perform these tasks, the latest generation of robots are being designed to take a step further, by performing more complex tasks that have been carried out by smart animals or humans up to date. To this end, inspiration needs to be taken from biological examples. For instance, insects are able to optimally solve complex environment navigation problems, and many researchers have started to mimic how these insects behave. Recent interest in neuromorphic engineering has motivated us to present a real-time, neuromorphic, spike-based Central Pattern Generator of application in neurorobotics, using an arthropod-like robot. A Spiking Neural Network was designed and implemented on SpiNNaker. The network models a complex, online-change capable Central Pattern Generator which generates three gaits for a hexapod robot locomotion in real-time. Reconfigurable hardware was used to manage both the motors of the robot and the low-latency communication interface with the Spiking Neural Networks. Real-time measurements confirm the simulation results, and locomotion tests show that NeuroPod can perform the gaits without any balance loss or added delay. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 14	2020	381						10	19		10.1016/j.neucom.2019.11.007													
J								Deep clustering for weakly-supervised semantic segmentation in autonomous driving scenes	NEUROCOMPUTING										Weak supervision; Semantic segmentation; Deep clustering; Autonomous driving	FRAMEWORK; NETWORK	Weakly-supervised semantic segmentation (WSSS) using only tags can significantly ease the label costing, because full supervision needs pixel-level labeling. It is, however, a very challenging task because it is not straightforward to associate tags to visual appearance. Existing researches can only do tag-based WSSS on simple images, where only two or three tags exist in each image, and different images usually have different tags, such as the PASCAL VOC dataset. Therefore, it is easy to relate the tags to visual appearance and supervise the segmentation. However, real-world scenes are much more complex. Especially, the autonomous driving scenes usually contain nearly 20 tags in each image and those tags can repetitively appear from image to image, which means the existing simple image strategy does not work. In this paper, we propose to solve the problem by using region based deep clustering. The key idea is that, since each tagged object is repetitively appearing from image to image, it allows us to find the common appearance through region clustering, and particular deep neural network based clustering. Later, we relate the clustered region appearance to tags and utilize the tags to supervise the segmentation. Furthermore, regions found by clustering with weak supervision can be very noisy. We further propose a mechanic to improve and refine the supervision in an iterative manner. To our best knowledge, it is the first time that image tags weakly-supervised semantic segmentation can be applied in complex autonomous driving datasets with still images. Experimental results on the Cityscapes and CamVid datasets demonstrate the effectiveness of our method. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 14	2020	381						20	28		10.1016/j.neucom.2019.11.019													
J								ENAS oriented layer adaptive data scheduling strategy for resource limited hardware	NEUROCOMPUTING										Efficient neural architecture search; Resource limited hardware; Hardware deployment; Computing architecture; Data scheduling strategy		Efficient Neural Architecture Search (ENAS) is an effective solution for building deep Convolutional Neural Network (CNN) models automatically. However, it is confronted with challenges when concerning deploying the searched networks on embedded platforms under limited resources. The key issue is the mismatch between the traditional data scheduling and the irregularity of layers from ENAS searched networks, which results in remarkable bandwidth pressure increment, and further leads to performance degradation and power consumption increase. In this paper, three alternative data scheduling patterns are constructed for different layers from ENAS searched networks, and a layer adaptive data scheduling strategy is proposed according to the constrained resources given by embedded platforms. Additionally, an adaptive architecture is also presented to deploy the searched networks efficiently, providing 4-10x performance speedup and 2.5-6x power consumption saving. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 14	2020	381						29	39		10.1016/j.neucom.2019.11.005													
J								Diverse sample generation with multi-branch conditional generative adversarial network for remote sensing objects detection	NEUROCOMPUTING										Remote sensing; Deep learning; Object detection; Generative adversarial network; Data augmentation	FRAMEWORK	The remote sensing data is difficult to collect and lack of diversity, which extremely limits the performance of object detection on remote sensing images. In this paper, a multi-branch conditional generative adversarial network (MCGAN) is proposed to augment data for object detection in optical remote sensing images, which is the first GANs-based data augmentation framework proposed for this topic. We use MCGAN to generate the diverse objects based on the existing remote sensing datasets. The multi-branch dilated convolution and the classification branch are adopted into MCGAN to help the generator to generate the diverse and high-quality images. Meanwhile, an adaptive samples selection strategy based on the Faster R-CNN is proposed to select the samples for data augmentation from the objects generated by MCGAN, which can ensure the quality of new augmented training sets and improve the diversity of samples. Experiments based on NWPU VHR-10 and DOTA show that the objects generated by MCGAN have the higher quality compared with the objects generated by WGAN and LSGAN. And the mean average precision detected by the state-of-the-art object detection models used in the experiments has the satisfactory improvement after the MCGAN based data augmentation, which indicates that data augmentation by MCGAN can effectively improve the accuracy of remote sensing images object detection. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 14	2020	381						40	51		10.1016/j.neucom.2019.10.065													
J								Lagrange stability of delayed switched inertial neural networks	NEUROCOMPUTING										Lagrange stability; Switched neural networks; Inertial neural network; Time delay; Time scale	CONTINUOUS-TIME; SYNCHRONIZATION; PASSIFICATION; PASSIVITY	As an extension of single/multiple stability notion, Lagrange stability is considered here. A class of switched inertial neural networks (SINNs) is studied on both continuous-time and discrete-time domain. Two kinds of activation functions are evolved for the network. Through characteristic function approach, matrix measure strategy, and theory of time scales, Lagrange stability of delayed continuous-/discretetime SINNs with lurie-type and bounded-type activation functions are addressed, respectively. The results also show that the criteria corresponding to discrete-time network approach to those corresponding to continuous-time one as the graininess function tends to zero. Two examples are given to show the effectiveness of the main results. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 14	2020	381						52	60		10.1016/j.neucom.2019.10.052													
J								Deep learning in video multi-object tracking: A survey	NEUROCOMPUTING										Multiple object tracking; Deep learning; Video tracking; Convolutional neural networks; LSTM; Reinforcement learning	MULTIPLE OBJECT TRACKING; MULTITARGET TRACKING; HYPOTHESIS; CNN; NETWORKS; FEATURES; POINTS; MODEL	The problem of Multiple Object Tracking (MOT) consists in following the trajectory of different objects in a sequence, usually a video. In recent years, with the rise of Deep Learning, the algorithms that provide a solution to this problem have benefited from the representational power of deep models. This paper provides a comprehensive survey on works that employ Deep Learning models to solve the task of MOT on single-camera videos. Four main steps in MOT algorithms are identified, and an in-depth review of how Deep Learning was employed in each one of these stages is presented. A complete experimental comparison of the presented works on the three MOTChallenge datasets is also provided, identifying a number of similarities among the top-performing methods and presenting some possible future research directions. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 14	2020	381						61	88		10.1016/j.neucom.2019.11.023													
J								A neuromorphic SLAM architecture using gated-memristive synapses	NEUROCOMPUTING										SLAM; Gated-Memristors; Neuromorphic architecture; Associative learning	SIMULTANEOUS LOCALIZATION; VISUAL ODOMETRY; NAVIGATION; NEURON; CELLS; FILMS	Navigation in GPS-denied environments is a critical challenge for autonomous mobile platforms such as drones. The concept of simultaneous localization and mapping (SLAM) addresses this challenge through real-time mapping of the platform's surroundings as it explores its environment. The computational resources required for traditional SLAM implementations (e.g. graphical processing units) require large size, weight, and power overheads; making it infeasible to employ them in resource-constrained applications. This work proposes a self-learning hardware architecture utilizing a novel gated-memristive device to address the implementation of SLAM in an energy-efficient manner. The gated-memristive devices are implemented as electronic synapses in tandem with novel low-energy spiking neurons to create a spiking neural network (SNN). This work shows how the SNN allows for navigation through an environment via landmark association without needing GPS. In the simple environment in which the network exists, it can successfully determine a direction in which to navigate while only consuming 36 mu W of power and only needing to be exposed to each landmark within the environment for 1-2ms in order to remember that location. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 14	2020	381						89	104		10.1016/j.neucom.2019.09.098													
J								FBSN: A hybrid fine-grained neural network for biomedical event trigger identification	NEUROCOMPUTING										Biomedical event trigger identification; Fine-grained; Hybrid architecture; Bi-LSTM; SVM	EXTRACTION	Biomedical event extraction is one of the fundamental tasks in medical research and disease prevention. Event trigger usually signifies the occurrence of a biomedical event by adopting a word or a phrase. Meanwhile, the task of biomedical event trigger identification is a critical and prerequisite step for biomedical event extraction. The existing methods generally rely on the complex and unobtainable features engineering. To alleviate this problem, we propose a hybrid structure FBSN which consists of Fine-grained Bidirectional Long Short Term Memory (FBi-LSTM) and Support Vector Machine (SVM) to deal with the event trigger identification. The hybrid architecture makes the most of their advantages: FBi-LSTM is to mainly extract the higher level features by the fine-grained representations, and SVM is largely appropriate for small dataset for classifying the results of biomedical event trigger. After that, the popular dataset Multi Level Event Extraction (MLEE) is employed to verify our hybrid structure. Experimental results show that our method is able to achieve the state-of-the-art baseline approaches. Meanwhile, we also discuss the detailed experiments in trigger identification task. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 14	2020	381						105	112		10.1016/j.neucom.2019.09.042													
J								Pinning control for passivity and synchronization of coupled memristive reaction-diffusion neural networks with time-varying delay	NEUROCOMPUTING										Passivity; Coupled memristive reaction-diffusion neural networks; Pinning control; Synchronization; Time-varying delay	GLOBAL EXPONENTIAL SYNCHRONIZATION; STABILIZATION; CRITERIA	This paper studies the passivity and synchronization problems of delayed coupled memristive reaction-diffusion neural networks (CMRDNNs) by utilizing two pinning control schemes, respectively. Firstly, a nodes-based controller is added to the delayed coupled memristive reaction-diffusion neural network model, and some sufficient conditions are derived to ensure the passivity and synchronization of the presented network through constructing appropriate Lyapunov functionals. Additionally, with the help of an edges-based pinning adaptive strategy and inequality techniques, several passivity criteria are further established. Moreover, a similar pinning control method is employed in order to guarantee the synchronization of delayed CMRDNNs. Eventually, two examples are provided to confirm the validity of the obtained results. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 14	2020	381						113	129		10.1016/j.neucom.2019.09.103													
J								Lightweight adversarial network for salient object detection	NEUROCOMPUTING										Salient object detection; Lightweight model; Deep learning; Adversarial training; Multi-scale feature	DEEP	Recent advance on salient object detection benefits mostly from the revival of Convolutional Neural Networks (CNNs). However, with these CNN based models, the predicted saliency map is usually incomplete, that is, spatially inconsistent with the corresponding ground truth, because of the inherent complexity of the object and the inaccuracy of object boundary detection resulted from regular convolution and pooling operations. Besides, the breakthrough on saliency detection accuracy of current state-of-the-art deep models comes at the expense of high computational cost, which contradicts its role as a pretreatment procedure for other computer vision tasks. To alleviate these issues, we propose a lightweight adversarial network for salient object detection, which simultaneously improves the accuracy and efficiency by enforcing higher-order spatial consistency via adversarial training and lowering the computational cost through lightweight bottleneck blocks, respectively. Moreover, multi-scale contrast module is utilized to sufficiently capture contrast prior for visual saliency reasoning. Comprehensive experiments demonstrate that our method is superior to the state-of-the-art works on salient object detection in both accuracy and efficiency. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 14	2020	381						130	140		10.1016/j.neucom.2019.09.100													
J								Localization of radiance transformation for image dehazing in wavelet domain	NEUROCOMPUTING										Single image dehazing; Image dofogging; Image restoration; Image de-noising; Image enhancement; Air light estimation; Wavelet transformation	FAST SINGLE IMAGE; WEATHER; VISION; DECOMPOSITION; SEGMENTATION; RESTORATION; VISIBILITY; FRAMEWORK; ALGORITHM; CHANNEL	Hazy image observation is a serious phenomenon occurred by random scattering of atmospheric elements. This reduces the scene prominence in terms of color contrast and makes the vision system poor. To overcome this issue, dehazing of digital images is dealt with much attention in the field of computer vision. Dense scattering in hazy image results in loss of original spectral information of the object. It is required to remove the unwanted artifacts of weather during image acquisition process of the given object to improve the visual insight of an acquired image. It is found that most of the existing methods fail to retrieve the sufficient information from hazy images. To address such problem, a robust single image dehazing method is introduced to recover the haze-free image. This is performed by calculating the atmospheric light of the given hazy images by decomposing and retaining the high frequency sub bands using wavelet domain. The dense haze elimination is performed on approximated low frequency sub band of the given hazy image. On reconstruction process, well-refined dehaze image is obtained. Moreover, transmission maps are also produced as a byproduct of this method. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 14	2020	381						141	151		10.1016/j.neucom.2019.10.005													
J								Exponential stability analysis of quaternion-valued neural networks with proportional delays and linear threshold neurons: Continuous-time and discrete-time cases	NEUROCOMPUTING										Exponential stability; Continuous-time; Discrete-time; Quaternion-valued neural networks (QVNNs); Proportional delays; Linear threshold neurons	GLOBAL ASYMPTOTIC STABILITY; SYNCHRONIZATION; MULTISTABILITY	A class of quaternion-valued neural networks (QVNNs) with proportional delays and linear threshold neurons is proposed in this paper. First, by employing Halanay inequality technique and matrix measure method, the global exponential stability of continuous-time QVNNs with proportional delays and linear threshold neurons is studied, and some sufficient conditions are derived to guarantee global exponential stability of the studied continuous-time systems. Then, the discrete-time analogues of the continuous-time QVNNs with proportional delays and linear threshold neurons are formulated and investigated by using the semi-discretization method. The discrete-time analogues are equivalent to the considered continuous-time neural networks, and possess the convergence behaviors of the considered continuous-time systems without any limitation applied to the discretization step size. Finally, some numerical examples are presented to ensure the effectiveness and correctness of the theoretical results obtained. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				MAR 14	2020	381						152	166		10.1016/j.neucom.2019.09.051													
J								Towards zero-shot learning generalization via a cosine distance loss	NEUROCOMPUTING										Zero-shot learning; Cosine distance loss; Generalization evaluation; Data splits		With the knowledge learned from some labelled training images, zero-shot learning (ZSL) aims to recognize new visual concepts by leveraging some intermediate information for both seen and unseen classes. Despite the existence of various methods, few work tends to comprehensively evaluate the generalization ability towards some practical data, which is important for their popularization and application. In this paper, we illustrate that it is inadequate and unconvincing to evaluate the ZSL methods using the current fixed data split. 19 existing methods are investigated on two practical datasets using 5-fold cross-validation to evaluate their generalization performances. More specifically, to alleviate the hubness problem in the high-dimensional visual space, we propose a cosine distance-based objective function to learn the transformation from semantic to visual features. Extensive experiments on up to 9 practical subsets demonstrate that the proposed method significantly outperforms the other baseline approaches. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 14	2020	381						167	176		10.1016/j.neucom.2019.11.011													
J								A non-parametric softmax for improving neural attention in time-series forecasting	NEUROCOMPUTING										Attention; Activation function; Softmax; Time series forecasting	NETWORK; MODELS	Neural attention has become a key component in many deep learning applications, ranging from machine translation to time series forecasting. While many variations of attention have been developed over recent years, all share a common component in the application of a softmax function to normalize the attention weights, in order to transform them into valid mixing coefficients. In this paper, we aim to improve the modeling flexibility of a generic attention module by innovatively replacing this softmax operation with a learnable softmax, in which the normalizing functions are also adapted from the data. Specifically, our generalized softmax builds upon recent work in learning activation functions for deep networks, in particular the kernel activation function and its extensions. We describe the application of the proposed technique for the challenging case of time series forecasting with the dual-stage attention-based recurrent neural network (DA-RNN), an innovative model for predicting time series that employs two different attention modules for handling exogenous factors and long-term dependencies. A series of real-world benchmarks are used to show that simply plugging-in our generalized attention model can improve results on all datasets, even when keeping the number of trainable parameters in the model constant. To further evaluate the algorithm, we collect a novel dataset for predicting the Bitcoin closing exchange rate, a problem of high practical significance lately. Finally, to foster research in the topic, we also release both the dataset and our model as an open source extensible library. Over a baseline DA-RNN, our proposed model delivers an improvement of MAR ranging from 6% to 15% using our newly-released dataset. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 14	2020	381						177	185		10.1016/j.neucom.2019.10.084													
J								A semi-supervised Laplacian extreme learning machine and feature fusion with CNN for industrial superheat identification	NEUROCOMPUTING										Extreme learning machine (ELM); Semi-supervised learning; Laplacian regularization; SD classification	NETWORK	The superheat degree (SD) in industrial aluminum electrolysis cell is a critical index that can maintain the energy balance, improve the current efficiency and improve production. However, the existing SD identification is mainly relying on artificial experience and the accuracy of SD is far from satisfactory. Further, artificial costs and physical equipment are expensive and time-consuming. In this paper, we propose a deep soft sensor method for SD detection. First, CNN is utilized for flame hole image feature extraction. Second, a semi-supervised extreme learning machine (ELM) that integrates Laplacian regularization is further used for SD classification. The main contributions of the paper are: (1) The proposed CNN-LapsELM utilizes the CNN for flame hole image feature extraction and then ELM for further classification, which fully takes advantage of CNN's ability for complex feature extraction, ELM's excellent generalization ability, and high computation efficiency. (2) Both the labeled and unlabeled samples are utilized for the CNN-LapsELM training process. It fully leverages the information contained in unlabeled data. At the same time, Laplacian regularization is utilized for learning the manifold structure of hole image samples, so the performance of the proposed CNN-LapsELM are improved. (3) The proposed CNN-LapsELM algorithm improves the generalization ability and robustness. The comparison result demonstrates that the CNN-LapsELM is superior to the existing SD identification and the accuracy is 87%. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				MAR 14	2020	381						186	195		10.1016/j.neucom.2019.11.012													
J								Adaptive synchronization of memristor-based neural networks with discontinuous activations	NEUROCOMPUTING										Memristor-based neural networks; Discontinuous activations; Asymptotic synchronization; Adaptive control	FINITE-TIME SYNCHRONIZATION; EXPONENTIAL SYNCHRONIZATION; DELAYS; CRITERIA; SYSTEMS	This paper investigates the adaptive synchronization of memristor-based neural networks (MNNs) with discontinuous activations. First, by using a decoupling strategy, the extensively used models of MNNs are improved, which can describe the real dynamics more accurately. Moreover, we extend the asymptotic synchronization criteria for MNNs with the consideration of discontinuous activation functions. Additionally, a general adaptive controller is devised to synchronize the drive and response systems, and the sufficient stability conditions are established via Lyapunov functional method within the framework of differential inclusions. Finally, numerical simulations are conducted to show the effectiveness of the developed methods. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 14	2020	381						196	206		10.1016/j.neucom.2019.11.018													
J								k-Reciprocal nearest neighbors algorithm for one-class collaborative filtering	NEUROCOMPUTING										k-RNN; k-NN; Reciprocal neighbors; One-class collaborative filtering		In this paper, we study an important recommendation problem of exploiting users' one-class positive feedback such as "likes", which is usually termed as one-class collaborative filtering (OCCF). For modeling users' preferences beneath the observed one-class positive feedback, the similarity between two users is a key concept of constructing a neighborhood structure with like-minded users. With a well-constructed neighborhood, we can make a prediction of a user's preference to an un-interacted item by aggregating his or her neighboring users' tastes. However, the neighborhood constructed by a typical traditional method is usually asymmetric, meaning that a certain user may belong to the neighborhood of another user but the inverse is not necessarily true. Such an asymmetric structure may result in a less strong neighborhood, which is our major finding in this paper. As a response, we exploit the reciprocal neighborhood among users in order to construct a better neighborhood structure with more high-value users, which is expected to be less influenced by the active users. We then design a corresponding recommendation algorithm called k-reciprocal nearest neighbors algorithm (k-RNN). Extensive empirical studies on two large and public datasets show that our k-RNN performs significantly better than a closely related algorithm with the traditional asymmetric neighborhood and some competitive model-based recommendation methods. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 14	2020	381						207	216		10.1016/j.neucom.2019.10.112													
J								Distributed extremum seeking control of multi-agent systems with unknown dynamics for optimal resource allocation	NEUROCOMPUTING										Communication network; Consensus estimation; Distributed control; Extremum seeking control; Gradient estimation; Multi-agent systems; Real-time optimization; Resource allocation	GRADIENT-METHOD; STABILITY; NETWORKS	The paper considers a class of equality constrained resource allocation problems for dynamically coupled multi-agent systems. It is assumed that the mathematical structure of each agent's dynamics and its local cost function are unknown but depend on the entire resource allocation vector. A distributed dual-mode extremum seeking control is proposed. It is shown that the distributed approach decouples the local contribution of each agent locally while guaranteeing a solution of the network wide optimization problem subject to the resource allocation constraints. The agents operate over a communication network which enables the application of a dynamic consensus algorithm to generate local estimates of the total network cost. Locally, each agent implements a parameter estimation routine to estimate the gradient of the total cost with respect to the local action. Each agent uses its local gradient estimate to implement a dual mode extremum seeking controller that guarantees satisfaction of the resource allocation constraints. Two simulation examples are provided to demonstrate the effectiveness of the proposed technique. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 14	2020	381						217	226		10.1016/j.neucom.2019.11.086													
J								Vehicle re-identification in tunnel scenes via synergistically cascade forests	NEUROCOMPUTING										Tunnel surveillance; Tunnel vehicle re-identification; Extremely randomized forest; Random forest; Curriculum learning	LICENSE PLATE RECOGNITION; TRAFFIC ACCIDENTS; IMAGES	Nowadays, numerous cameras have been equipped in tunnels for monitoring the tunnel safety, such as detecting fire, vehicle stopping, crashes, and so forth. Nevertheless, safety events in tunnels may occur in the blind zones not covered by the multi-camera monitoring systems. Therefore, this paper opens the challenging problem, tunnel vehicle re-identification (abbr. tunnel vehicle Re-ID), to make a between-camera speculation. Different from the open road scenes focused by existing vehicle Re-ID methods, tunnel vehicle Re-ID is more challenging because of poor light condition, low resolution, frequent occlusion, severe motion blur, high between-vehicle similarity, and so on. To be specific, we propose a synergistically cascade forests (SCF) model which aims to gradually construct the linking relation between vehicle samples with an increasing of alternative layers of random forest and extremely randomized forest. Through the modeling of SCF, we can restrict the influence of little inter-variation of different vehicle identities and large intra-variation of the same identities. This paper constructs a new and challenging tunnel vehicle dataset (Tunnel-VReID), consisting of 1000 pairs of tunnel vehicle images. Extensive experiments on our Tunnel-VReID demonstrate that the proposed method can outperform current state-of-the-art methods. Besides, in order to prove the adaptation ability of SCF, we also verify the superiority of SCF on a large-scale vehicle Re-ID dataset, named as VehiclelD, collected in open road scenes. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 14	2020	381						227	239		10.1016/j.neucom.2019.11.069													
J								Deep semantic cross modal hashing with correlation alignment	NEUROCOMPUTING										Deep neural network; Cross modal hashing; Inter-modal similarity; Correlation alignment; Semantic embedding	ALGORITHM	Hashing has been extensively applied to cross modal retrieval due to its low storage and high efficiency. Deep hashing which can well extract features of multi-modal data has received increasing research attention recently. However, most of deep hashing for cross modal retrieval methods do not make full use of the semantic label information and do not fully mine correlation of heterogeneous data. In this paper, we propose a Deep Semantic cross modal hashing with Correlation Alignment (DSCA) method. In DSCA, we design two deep neural networks for image and text modality separately, and learn two hash functions. Firstly, we construct a new similarity for the multi-label data, which can well exploit the semantic information and improve the retrieval accuracy. Simultaneously, we preserve the inter-modal similarity of heterogeneous data features, which can exploit semantic correlation. Secondly, the distributions of heterogeneous data are aligned so as to mine the inter-modal correlation well. Thirdly, the semantic label information is embedded in the hash layer of the text network, which can make the learned hash matrix more stable and make the hash codes more discriminative. Experimental results demonstrate that DSCA outperforms the state-of-the-art methods. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 14	2020	381						240	251		10.1016/j.neucom.2019.11.061													
J								LRP-Based path relevances for global explanation of deep architectures	NEUROCOMPUTING										Explainable AI; Deep learning; Interpretable machine learning; Layer-wise relevant propagation		Understanding what Machine Learning models are doing is not always trivial. This is especially true for complex models such as Deep Neural Networks (DNN), which are the best-suited algorithms for modeling very complex and nonlinear relationships. But this need to understand has become a must since privacy regulations are hardening the industrial use of these models. There are different techniques to address the interpretability issues that Machine Learning models arises. This paper is focused on opening the so-called Deep Neural architectures black-box. This research extends the technique called Layer-wise Relevant Propagation (LRP) enhancing its properties to compute the most critical paths in different deep neural architectures using multicriteria analysis. We call this technique Ranked-LRP and it was tested on four different datasets and tasks, including classification and regression. The results show the worth of our proposal. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 14	2020	381						252	260		10.1016/j.neucom.2019.11.059													
J								Adaptive embedding gate for attention-based scene text recognition	NEUROCOMPUTING										Deep learning; Scene text recognition; Attention mechanism	NETWORK	Scene text recognition has attracted particular research interest because it is a very challenging problem and has various applications. The most cutting-edge methods are attentional encoder-decoder frameworks that learn the alignment between the input image and output sequences. In particular, the decoder recurrently outputs predictions, using the prediction of the previous step as a guidance for every time step. In this study, we point out that the inappropriate use of previous predictions in existing attentional decoders restricts the recognition performance and brings instability. To handle this problem, we propose a novel module, namely adaptive embedding gate (AEG). The proposed AEG focuses on introducing high-order character language models to attentional decoders by controlling the information transmission between adjacent characters. AEG is a flexible module and can be easily integrated into the state-of-the-art attentional decoders for scene text recognition. We evaluate its effectiveness as well as robustness on a number of standard benchmarks, including the IIIT5K, SVT, SVT-P, CUTE80, and ICDAR datasets. Experimental results demonstrate that AEG can significantly boost recognition performance and bring better robustness. (C) 2019 The Author(s). Published by Elsevier B.V.																	0925-2312	1872-8286				MAR 14	2020	381						261	271		10.1016/j.neucom.2019.11.049													
J								Self-supervised monocular image depth learning and confidence estimation	NEUROCOMPUTING										Monocular depth estimation; Deep convolutional neural networks; Confidence map	RECONSTRUCTION	We present a novel self-supervised framework for monocular image depth learning and confidence estimation. Our framework reduces the amount of ground truth annotation data required for training Convolutional Neural Networks (CNNs), which is often a challenging problem for the fast deployment of CNNs in many computer vision tasks. Our DepthNet adopts a novel fully differential patch-based cost function through the Zero-Mean Normalized Cross Correlation (ZNCC) to take multi-scale patches as matching and learning strategies. This approach greatly increases the accuracy and robustness of the depth learning. Whilst the proposed patch-based cost function naturally provides a 0-to-1 confidence, it is then used to self-supervise the training of a parallel network for confidence map learning and estimation by exploiting the fact that ZNCC is a normalized measure of similarity which can be approximated as the confidence of the depth estimation. Therefore, the proposed corresponding confidence map learning and estimation operate in a self-supervised manner and is a parallel network to the DepthNet. Evaluation on the KITTI depth prediction evaluation dataset and Make3D dataset show that our method outperforms the state-of-the-art results. Crown Copyright (C) 2019 Published by Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 14	2020	381						272	281		10.1016/j.neucom.2019.11.038													
J								Simultaneous identification, tracking control and disturbance rejection of uncertain nonlinear dynamics systems: A unified neural approach	NEUROCOMPUTING										Zhang neural networks (ZNN); Time-variant tracking control; Time-variant problems; Robustness; Identification	FINITE-TIME SOLUTION; REDUNDANT MANIPULATORS; ROBOT MANIPULATORS; KINEMATIC CONTROL; NETWORK; OPTIMIZATION; CONVERGENCE; STABILITY; CRITERIA; SUBJECT	Previous works of traditional zeroing neural networks (or termed Zhang neural networks, ZNN) show great success for solving specific time-variant problems of known systems in an ideal environment. However, it is still a challenging issue for the ZNN to effectively solve time-variant problems for uncertain systems without the prior knowledge. Simultaneously, the involvement of external disturbances in the neural network model makes it even hard for time-variant problem solving due to the intensively computational burden and low accuracy. In this paper, a unified neural approach of simultaneous identification, tracking control and disturbance rejection in the framework of the ZNN is proposed to address the time-variant tracking control of uncertain nonlinear dynamics systems (UNDS). The neural network model derived by the proposed approach captures hidden relations between inputs and outputs of the UNDS. The proposed model shows outstanding tracking performance even under the influences of uncertainties and disturbances. Then, the continuous-time model is discretized via Euler forward formula (EFF). The corresponding discrete algorithm and block diagram are also presented for the convenience of implementation. Theoretical analyses on the convergence property and discretization accuracy are presented to verify the performance of the neural network model. Finally, numerical studies, robot applications, performance comparisons and tests demonstrate the effectiveness and advantages of the proposed neural network model for the time-variant tracking control of UNDS. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				MAR 14	2020	381						282	297		10.1016/j.neucom.2019.11.031													
J								Multiobjective ResNet pruning by means of EMOAs for remote sensing scene classification	NEUROCOMPUTING										Random pruning; Evolutionary multiobjective optimization algorithms; ResNet-50; Remote sensing scene classification	EVOLUTIONARY ALGORITHMS; OPTIMIZATION	Convolutional neural networks have achieved remarkable success in the field of computer vision. However, due to their high storage and expensive computations, recently, there has been a lot of work focusing on reducing the complexity of convolutional neural networks. In this work, we propose a random filter pruning method by means of evolutionary multiobjective optimization algorithms to accelerate the Siamese ResNet-50 for remote sensing scene classification. We have conduct experiments on NWPU-RESISC45, UC Merced Land-Use and SIRI-WHU datasets for performance evaluation of the proposed method. The experimental results demonstrate that the classification performance of our pruned model has been improved while keeping a certain degree of sparsity of the model. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 14	2020	381						298	305		10.1016/j.neucom.2019.11.097													
J								Neuroevolutionary based convolutional neural network with adaptive activation functions	NEUROCOMPUTING										Convolutional neural networks; Neuroevolution; Adaptive activation function; Classification	DIAGNOSIS	Deep convolutional neural networks are one of the most successful types of neural networks widely used in image processing and pattern recognition. These networks involve many tunable parameters that influence network performance drastically. Among them, the proposed method of this paper focuses on the role of activation function in these networks, while the idea of adaptive activation functions is further developed by utilizing the neuroevolutionary technique. Considering several basic function to be combined in a non-linear manner, the proposed method attempts to construct an adaptive function by the help of Genetics Algorithm (GA) technique, while the selected basic functions by GA and the learned combination coefficients are adapted to the input data. As the network optimizer and the learning rate parameter are tightly related to the network activation functions, they are also included in the GA evolutionary process to be selected such that they are highly in coherence with the selected basic functions. Experiments done on the classification of CT brain images and the MNIST hand written digits dataset clearly confirm the efficiency of the proposed idea and the role of proper adaptive activation functions in extending the capabilities of neural networks. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 14	2020	381						306	313		10.1016/j.neucom.2019.11.090													
J								Weighted triple-sequence loss for video-based person re-identification	NEUROCOMPUTING										Deep learning; Person re-identification; Weighted triple-Sequence loss; Spatial transformed partial network		Person re-identification (re-id) task has attracted a lot of attention because of the excellent performance powered by the Convolutional Neural Network (CNN). However, video-based person re-id is still challenging and far to be solved. On the one hand, the sequence contains complementary information but also more noise information. On the other hand, the training for video is still mainly based on classifying a single frame via its person identity. The representation of the video is generated by aggregating each frame feature. All these will cause that the robustness of video feature is not adequate at the training stage, and the model is easy to be misled by noise information existing in frames. In order to alleviate the difficulty of training video-based re-id, we propose a novel loss named Weighted Triple-Sequence Loss (WTSL) to optimize the video-based feature and reduce the impact of outliers. Further more, we design a Spatial Transformed Partial Network (STPN) coordinated with jointly optimizing image-level and video-level features to generate more robust representation. Extensive experiments show that our algorithm outperforms the state-of-the-art results and achieves 82.2%, 95.2%, and 85.9% rank-1 accuracy on three popular video-based benchmarks: iLIDS-VID, PRID2011, and MARS, respectively. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 14	2020	381						314	321		10.1016/j.neucom.2019.11.088													
J								Hierarchical and overlapping social circle identification in ego networks based on link clustering	NEUROCOMPUTING										Ego networks; Hierarchical social circles; Overlapping social circles; Link clustering	COMMUNITY DETECTION; ORGANIZATION	As online social networks receive rising popularity, social circle identification has gradually attracted attention from researchers. However, the existing approaches are incapable of utilizing both structural and attributional information in an efficient way, which creates a space of improvement to performance. In this paper, HOSCIEN, as a novel solution to identifying hierarchical and overlapping social circles in ego networks, is developed. Rather than directly partitioning nodes into different groups, social circles are identified based on the link clustering. An efficient scheme is proposed to evaluate the similarity of links according to both structural and attributional information. Then a hierarchical clustering method is applied to construct a dendrogram of links. Two methods are suggested to perform calculations of proper cuts for the dendrogram, which leads to circles with varying granularity. Besides, a supervised classifier is trained to identify the category of a circle based on the structural and attributional features. The performance of HOSCIEN is assessed based on three benchmark datasets. As revealed by the results, HOSCIEN performs better than the state-of-the-art methods for all of the four evaluation metrics. Our method is also applied to real social networks by implementing a WeChat applet for classification of a user's friends into proper circles. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 14	2020	381						322	335		10.1016/j.neucom.2019.11.080													
J								ProLFA: Representative prototype selection for local feature aggregation	NEUROCOMPUTING										Prototype selection; Feature aggregation; Block coordinate descent; Domain-invariant projection	DISCRIMINATIVE DICTIONARY; IMAGE CLASSIFICATION; BAG	Given a set of hand-crafted local features, acquiring a global representation via aggregation is a promising technique to boost computational efficiency and improve task performance. Existing feature aggregation (FA) approaches, including Bag of Words and Fisher Vectors, usually fail to capture the desired information due to their pipeline mode. In this paper, we propose a generic formulation to provide a systematical solution (named ProLFA) to aggregate local descriptors. It is capable of producing compact yet interpretable representations by selecting representative prototypes from numerous descriptors, under relaxed exclusivity constraint. Meanwhile, to strengthen the discriminability of the aggregated representation, we rationally enforce the domain-invariant projection of bundled descriptors along a task-specific direction. Furthermore, ProLFA is also provided with a powerful generalization ability to deal flexibly with the semi-supervised and fully supervised scenarios in local feature aggregation. Experimental results on various descriptors and tasks demonstrate the proposed ProLFA is considerably superior over currently available alternatives about feature aggregation. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				MAR 14	2020	381						336	347		10.1016/j.neucom.2019.11.073													
J								PRF-RW: a progressive random forest-based random walk approach for interactive semi-automated pulmonary lobes segmentation	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Random forest; Lobes segmentation; Random walk; Semi-automated segmentation; Machine learning	CHEST X-RAYS; FISSURES; ANATOMY; DISEASE	The computational detection of lung lobes from computed tomography images is a challenging segmentation problem with important respiratory healthcare applications, including emphysema, chronic bronchitis, and asthma. This paper proposes a progressive random forest-based random walk approach for interactive semi-automated pulmonary lobes segmentation. First, our model performs automated segmentation of the lung lobes in a progressive random forest network, eliminating the need for prior segmentation of lungs, vessels, or airways. Then, an interactive lobes segmentation approach based on random walk mechanism is designed for improving auto-segmentation accuracy. Furthermore, we annotate a new dataset which contains 93 scans (57 men, 36 women; age range: 40-90 years) from the Central Hospital Affiliated with Shenyang Medical College (CHASMC). We evaluate the model on our annotated dataset, LIDC (https://wiki.cance rimagingarchive.net) and LOLA11 (http://lolall.com/) datasets. The proposed model achieved a Dice score of 0.906 +/- 0.106 for LIDC, 0.898 +/- 0.113 for LOLA11, and 0.921 +/- 0.101 for our dataset. Experimental results show the accuracy of the proposed approach, which consistently improves performance across different datasets by a maximum of 8.2% as compared to baselines model.																	1868-8071	1868-808X				OCT	2020	11	10					2221	2235		10.1007/s13042-020-01111-9		MAR 2020											
J								Discriminative low-rank projection for robust subspace learning	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Feature selection; Low-rank representation; Image classification; Small-class problem; Subspace learning	FACE-RECOGNITION; REPRESENTATION; FRAMEWORK; ALGORITHM; GRAPH	The robustness to outliers, noises, and corruptions has been paid more attention recently to increase the performance in linear feature extraction and image classification. As one of the most effective subspace learning methods, low-rank representation (LRR) can improve the robustness of an algorithm by exploring the global representative structure information among the samples. However, the traditional LRR cannot project the training samples into low-dimensional subspace with supervised information. Thus, in this paper, we integrate the properties of LRR with supervised dimensionality reduction techniques to obtain optimal low-rank subspace and discriminative projection at the same time. To achieve this goal, we proposed a novel model named Discriminative Low-Rank Projection (DLRP). Furthermore, DLRP can break the limitation of the small class problem which means the number of projections is bound by the number of classes. Our model can be solved by alternatively linearized alternating direction method with adaptive penalty and the singular value decomposition. Besides, the analyses of differences between DLRP and previous related models are shown. Extensive experiments conducted on various contaminated databases have confirmed the superiority of the proposed method.																	1868-8071	1868-808X				OCT	2020	11	10					2247	2260		10.1007/s13042-020-01113-7		MAR 2020											
J								Urban traffic flows forecasting by recurrent neural networks with spiral structures of layers	NEURAL COMPUTING & APPLICATIONS										Recurrent neural network; Associative processing; Forecasting; Traffic flow	PREDICTION; LSTM	The problem of neural network forecasting of processes with changing laws of their behavior and imperfection of time-series samples is considered on the example of analysis of urban traffic flows. The goal is to improve the accuracy of such forecasting. To achieve this goal, we analyze the applicability of self-learning recurrent neural networks with controlled elements and the spiral structure of layers. Based on the development and application of these neural networks, the new methods and the system implementing them are proposed. These methods, in contrast to known solutions, allow continuous training of neural networks and forecasting of processes. There is no need to interrupt training to perform forecasting. For forecasting, it is possible to continuously take into account the properties of the observed processes. In addition, improved controlling of associative recall of information from the memory of recurrent neural networks is provided to improve the accuracy of forecasting. The results of traffic flow forecasting are presented. The results are compared with estimates obtained using other methods. It is shown that the proposed methods have advantages in accuracy compared to the known solutions. The developed methods are recommended for use in advanced robotic and other intelligent systems.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14885	14897		10.1007/s00521-020-04843-5		MAR 2020											
J								Natural language understanding approaches based on joint task of intent detection and slot filling for IoT voice interaction	NEURAL COMPUTING & APPLICATIONS										Internet of Things; Artificial intelligence; Natural language understanding; Voice interaction; Intent detection and slot filling; Capsule network	RECURRENT NEURAL-NETWORKS	Internet of Things (IoT) based voice interaction system, as a new artificial intelligence application, provides a new human-computer interaction mode. The more intelligent and efficient communication approach poses greater challenges to the semantic understanding module in the system. Facing with the complex and diverse interactive scenarios in practical applications, the academia and the industry urgently need more powerful Natural Language Understanding (NLU) methods as support. Intent Detection and Slot Filling joint task, as one of the core sub-tasks in NLU, has been widely used in different human-computer interaction scenarios. In the current era of deep learning, the joint task of Intent Detection and Slot Filling has also changed from previous rule-based methods to deep learning-based methods. It is an important problem to explore how to realize the models of these tasks to be refined and targeted designed, and to make the Intent Detection task better serve the improvement of precision of Slot Filling task by connecting the before and after tasks. It has great significance for building a more humanized IoT voice interaction system. In this study, we designed two joint models to realize Intent Detection and Slot Filling joint task. For the Intent Detection type task, one is based on BiGRU-Att-CapsuleNet (hybrid-based model) and the other is based on the RCNN model. Both methods use the BiGRU-CRF model for the Slot Filling type task. The hybrid-based model can enhance the semantic capture capability of a single model. And by combining specialized models built independently for each task to achieve a complete joint task, it can be better to achieve optimal performance on each task. This study also carried out detailed comparative experiments of tasks and joint tasks on multiple datasets. Experiments show that the joint models have achieved competitive results in 7 typical datasets included in multiple scenarios in English and Chinese compared with other models.																	0941-0643	1433-3058				OCT	2020	32	20			SI		16149	16166		10.1007/s00521-020-04805-x		MAR 2020											
J								Proposed S-Algo plus data mining algorithm for web platforms course content and usage evaluation	SOFT COMPUTING										Data mining algorithms; Web mining in education; e-Learning course ranking; Course knowledge discovery and management; Learning analytics	AGGREGATION OPERATORS; STUDENTS	This paper suggests a novel data mining algorithm for the evaluation of e-learning courses from a Learning Management System. This new algorithm, which is called S-Algo+ (Superposition Algorithm), takes as input the course rankings and the suggestion results from any kind of ranking/hierarchical algorithms and evaluates the validity of a course ranking position. The ranking algorithms estimate the quantity and quality of the course content according to users' actions and interest. S-Algo+ generates an improved final ranking suggestion output, combining the best results of the source ranking algorithms using statistical and mathematic techniques. In this way, the researchers and course instructors can use more accurate results. The efficiency and applicability of the S-Algo+ algorithm was evaluated successfully with a cross-comparison quantitative and qualitative process in a case study at a Greek university. Our new proposed S-Algo+ algorithm may lead to both theoretical and practical advantages. It may also apply not only for course evaluation but for any kind of web application such as e-commerce.																	1432-7643	1433-7479				OCT	2020	24	19					14861	14883		10.1007/s00500-020-04841-8		MAR 2020											
J								Improving image thresholding by the type II fuzzy entropy and a hybrid optimization algorithm	SOFT COMPUTING										Type II fuzzy entropy; Paddy Field Algorithm; Plant Propagation Algorithm; Multilevel thresholding	ARTIFICIAL BEE COLONY; SEGMENTATION; OTSU	The segmentation of digital images is an open problem that has increasingly attracted the attention of researchers during the last years. Thresholding approaches are often used due to their independence from the resolution of the images and their speed. However, simple thresholding approaches usually generate low-quality images. To achieve a better balance between speed and quality, many criteria are used to select the thresholds that segment the image. The type II fuzzy entropy (TII-FE) was introduced to perform image thresholding by modeling the classes of an image as membership functions to avoid uncertainty on the selection of the thresholds leading to improvement regarding the quality of the segmented image. To maximize the TII-FE, an efficient optimizer should be used to converge quickly to the optimal. In this paper, a hybrid method based on the Paddy Field Algorithm (PFA) and the Plant Propagation Algorithm (PPA) with the disruption operator (HPFPPA-D) is presented for the maximization of the TII-FE. The hybridization of these algorithms is used to enhance the performance of each algorithm by introducing operators from other approaches. In this case, the PFA shows good exploitation features that are complemented by the exploration behavior of PPA and refined with the disruption operator. The synergy between those methods has led to an accurate methodology for TII-FE thresholding. The proposed HPFPPA-D for TII-FE is evaluated using a set of benchmark images regarding convergence and image quality. The results are compared against other state-of-the-art evolutionary algorithms providing evidence of a superior and significant performance.																	1432-7643	1433-7479				OCT	2020	24	19					14885	14905		10.1007/s00500-020-04842-7		MAR 2020											
J								A kernel principal component analysis-based approach for determining the spatial warning domain of dam safety	SOFT COMPUTING										Dam safety; Structural behavior identification; Spatial warning domain determination; Monitoring data; Kernel principal component analysis	STATISTICAL-ANALYSIS; DEFORMATION; IDENTIFICATION; WAVELET; KPCA	It is important to determine the warning value of structural behavior for evaluating the service safety, identifying the potential risk and preventing the failure of dam engineering. However, more attention was paid to determining the security warning value of a single observation point on deformation, seepage or stress. And the correlation between the adjacent points or among all points in one dam section is usually lack of consideration. In this paper, the monitoring data of multi-points are taken to determine the spatial warning domain of dam safety. The warning mode of abnormal structural behavior is changed from the single point into the linked multi-points. First, the kernel principal component analysis method is adopted to identify the inherent characteristics among observation points in a dam section. Second, considering the correlation among observation points, the implementation process is proposed to determine the spatial warning domain of dam safety. Finally, an actual concrete gravity dam is taken as an example. The proposed approach is used to determine the spatial warning domains of deformation and seepage. The results, which are obtained by the proposed approach, the traditional method and the qualitative analysis for monitoring data, are compared. It is indicated that the proposed multi-points correlation-based approach is feasible and superior to determine the spatial warning domain of dam safety.																	1432-7643	1433-7479				OCT	2020	24	19					14921	14931		10.1007/s00500-020-04845-4		MAR 2020											
J								R2R-CSES: proactive security data process using random round crypto security encryption standard in cloud environment	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cryptography; AES; Cloud security; Privacy standard file systems	COGNITIVE RADIO NETWORKS; ENERGY DETECTION; EFFICIENT	Cloud service provide the information security to protect the data using broad set of privacy policies and controls. Various security mechanisms concentrates the reality of key leakage issues to deal the security policy. The problem arise due to the leakage of data from attackers which leads the time factor of creating vulnerabilities and information loss. So information security in the cloud needs higher security crypto policy standards to make effective privacy protection. The cloud security service provider's intent the proactive security standard to make the higher security in cryptographic encryption techniques. To propose a standard two-phase implementation of Round-key and random key based crypto security encryption standard (R2R-CSES) for improving security system in a cloud environment. R2R provides the high-level implementation of security protection using key verification policy. This service makes the random encryption using proactive security encryption with round random crypto policy holds key applicable standards with cloud authenticator policy. The projection of our results estimates the advanced encryption standard with key substitution make effective security to prevent vulnerabilities. The optimization produces least time complexity with service access to provide the security mechanism in high standard.																	1868-5137	1868-5145															10.1007/s12652-020-01860-z		MAR 2020											
J								Robot target localization and interactive multi-mode motion trajectory tracking based on adaptive iterative learning	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Motion trajectory tracking; Target localization; Adaptive iterative learning; Interactive multimode	OPTIMIZATION	Due to factors such as nonlinearity, parameter and disturbance uncertainty, actuator failure and other factors in the robot control system, it is difficult to ensure high-precision trajectory tracking of the system based on the traditional system model-based algorithm. In this paper, a method of robot vision localization based on iterative Kalman particle filter is proposed, which realizes the global positioning of the robot and carries out experimental verification. The results show that the positioning accuracy and real-time performance of the robot mobile navigation can meet the requirements. The task of performing industrial robots is repetitive, and iterative learning control technology is introduced to adapt to the repeated dynamic characteristics of industrial robots when they repeatedly perform work tasks. An adaptive iterative learning interactive multi-mode trajectory tracking controller is proposed. According to the error and error derivative of the trajectory tracking, the learning amount is obtained, which is used to correct the output of the PD controller, thereby improving the trajectory tracking accuracy. The feasibility and effectiveness of the proposed algorithm are verified by simulation results.																	1868-5137	1868-5145															10.1007/s12652-020-01878-3		MAR 2020											
J								The development of a micro-pattern manufacturing method using rotating active tools with compensation of estimated errors and an LMS algorithm	JOURNAL OF INTELLIGENT MANUFACTURING										Micro-structuring; Rotating active tool; Runout compensation; LMS algorithm; Piezoelectric actuator; Gap sensor	FRICTION; TRIBOLOGY	In this paper, techniques for machining and micro-structuring dimples and grooves on the interior of cylinders using an active rotating tool are discussed. Microscopic dimples and grooves patterned on the inner surface of a cylinder act as lubrication and reduce friction. The active rotating tool presented here is equipped with a gap sensor that can measure the distance between the tool, workpiece, and machining tip so that micron-scale dimples and grooves can be patterned and connected to piezoelectric actuators. Electronic control and power connections are made to the external controller via a slip ring. Accurate measurements of the distance between the tool and workpiece were used to increase the lubrication effect by machining patterns with uniform size and depth. It is difficult to accurately measure errors in cylinders of various shapes using a single gap sensor; thus, we employed two gap sensors to ensure accurate assessment of cylinder shape, and a least mean square algorithm was implemented to compensate for the measured runout errors, which were tracked and compensated using the gap sensor. The method presented here reduces errors on the inner face of a cylinder, and produces a uniform pattern.																	0956-5515	1572-8145															10.1007/s10845-020-01558-1		MAR 2020											
J								Lion Algorithm-Optimized Long Short-Term Memory Network for Groundwater Level Forecasting in Udupi District, India	APPLIED COMPUTATIONAL INTELLIGENCE AND SOFT COMPUTING											MODEL	Groundwater is a precious natural resource. Groundwater level (GWL) forecasting is crucial in the field of water resource management. Measurement of GWL from observation-wells is the principle source of information about the aquifer and is critical to its evaluation. Most part of the Udupi district of Karnataka State in India consists of geological formations: lateritic terrain and gneissic complex. Due to the topographical ruggedness and inconsistency in rainfall, the GWL in Udupi region is declining continually and most of the open wells are drying-up during the summer. Hence, the current research aimed at developing a groundwater level forecasting model by using hybrid long short-term memory-lion algorithm (LSTM-LA). The historical GWL and rainfall data from an observation well from Udupi district, located in Karnataka state, India, were used to develop the model. The prediction accuracy of the hybrid LSTM-LA model was better than that of the feedforward neural network (FFNN) and the isolated LSTM models. The hybrid LSTM-LA-based forecasting model is promising for a larger dataset.																	1687-9724	1687-9732				MAR 13	2020	2020								8685724	10.1155/2020/8685724													
J								Light Structure from Pin Motion: Geometric Point Light Source Calibration	INTERNATIONAL JOURNAL OF COMPUTER VISION										Light source calibration; Photometric stereo; Structure from motion	PHOTOMETRIC STEREO; ILLUMINATION; CAMERA; IMAGE; REFLECTANCE; ORIENTATION; POSITION	We present a method for geometric point light source calibration. Unlike prior works that use Lambertian spheres, mirror spheres, or mirror planes, we use a calibration target consisting of a plane and small shadow casters at unknown positions above the plane. We show that shadow observations from a moving calibration target under a fixed light follow the principles of pinhole camera geometry and epipolar geometry, allowing joint recovery of the light position and 3D shadow caster positions, equivalent to how conventional structure from motion jointly recovers camera parameters and 3D feature positions from observed 2D features. Moreover, we devised a unified light model that works with nearby point lights as well as distant light in one common framework. Our evaluation shows that our method yields light estimates that are stable and more accurate than existing techniques while having a much simpler setup and requiring less manual labor.																	0920-5691	1573-1405				JUL	2020	128	7					1889	1912		10.1007/s11263-020-01312-3		MAR 2020											
J								The Open Images Dataset V4 Unified Image Classification, Object Detection, and Visual Relationship Detection at Scale	INTERNATIONAL JOURNAL OF COMPUTER VISION										Ground-truth dataset; Image classification; Object detection; Visual relationship detection		We present Open Images V4, a dataset of 9.2M images with unified annotations for image classification, object detection and visual relationship detection. The images have a Creative Commons Attribution license that allows to share and adapt the material, and they have been collected from Flickr without a predefined list of class names or tags, leading to natural class statistics and avoiding an initial design bias. Open Images V4 offers large scale across several dimensions: 30.1M image-level labels for 19.8k concepts, 15.4M bounding boxes for 600 object classes, and 375k visual relationship annotations involving 57 classes. For object detection in particular, we provide 15x more bounding boxes than the next largest datasets (15.4M boxes on 1.9M images). The images often show complex scenes with several objects (8 annotated objects per image on average). We annotated visual relationships between them, which support visual relationship detection, an emerging task that requires structured reasoning. We provide in-depth comprehensive statistics about the dataset, we validate the quality of the annotations, we study how the performance of several modern models evolves with increasing amounts of training data, and we demonstrate two applications made possible by having unified annotations of multiple types coexisting in the same images. We hope that the scale, quality, and variety of Open Images V4 will foster further research and innovation even beyond the areas of image classification, object detection, and visual relationship detection.																	0920-5691	1573-1405				JUL	2020	128	7					1956	1981		10.1007/s11263-020-01316-z		MAR 2020											
J								Medical diagnosis and treatment is NP-complete	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Medical diagnosis; artificial intelligence medicine; computational complexity; computer aided decision support; NP complete	NETWORKS	There is great interest in the pursuit of process-driven, algorithmic allocation of health-care resources, including computer-aided medical diagnosis and treatment (MDT). Little is understood regarding the computational complexity of MDT; however, and if determined, how relevant the complexity would be to automating MDT. We approach analysing the computational complexity of MDT in several ways: (1) proving that MDT is computationally intractable (NP-complete, NPC) by reducing the travelling salesperson (TSP) and set-cover (SETCOVER) problems to MDT, (2) showing, in contrast, an example of MDT in tractable form, and (3) showing that, leaving aside TSP and SETCOVER, there are computationally efficient, heuristic-driven search methods based on human diagnostician methods. Calculations show that the sparseness of actual symptom-treatments sets in the space of all possible sets is astronomical (e.g., 10(-204,678)). While the computational complexity of MDT is interesting theoretically, pragmatically uncovering actual human cognitive practices of MDT will accelerate the development of artificial intelligence to assist and eventually replace physicians.																	0952-813X	1362-3079															10.1080/0952813X.2020.1737581		MAR 2020											
J								A non-reduced order approach to stability analysis of delayed inertial genetic regulatory networks	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Inertial genetic regulatory network; exponential stability; non-reduced order approach	NICHOLSONS BLOWFLIES MODEL; NEURAL-NETWORKS; ROBUST STABILITY; EXPONENTIAL STABILITY; PERIODIC-SOLUTIONS; LIMIT-CYCLES; EXISTENCE; DYNAMICS; SYSTEMS; ATTRACTIVITY	In this paper, we present a non-reduced order approach to study the exponential stability of delayed inertial genetic regulatory networks. Sufficient conditions guaranteeing the networks to be exponentially stable are established by introducing a novel Lyapunov-Krasovskii functional. Compared with the existing ones in literature, the proposed methods significantly reduce the computational complexity. Moreover, the theoretical results are tested by a numerical example.																	0952-813X	1362-3079															10.1080/0952813X.2020.1735531		MAR 2020											
J								Enhanced route discovery using connected dominating set and 2-hop repair in wireless ad hoc networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Connected dominating set; Route discovery; Energy efficient; Multi point relay; Ad hoc networks; 2-Hop route repair		Efficient routing and broadcasting among a set of nodes play a critical role in wireless adhoc networks. for energy efficient routing, a connected dominating set (CDS) based virtual backbone is a promising approach. In the network, one-hop neighbors are selected as multi point relay (MPR) by each node to cover all its two-hop neighbors for the purpose of broadcasting and 2-hop repair. To improve the network lifetime, energy efficient MPR based CDS construction has been proposed by considering the node degree, energy level, node ID and velocity of the node. We also propose a route discovery protocol to relay route request messages, which makes use of the CDS nodes to obtain a stable routing path; 2-hop repair for route maintenance, which reduces the path damage (reduced control packets with lesser consumption of bandwidth) and broadcast storm problem. The simulation results show that the proposed protocol increases the network lifetime up to 30% than other works and effectively repair damaged routes by reducing control packets.																	1868-5137	1868-5145															10.1007/s12652-020-01799-1		MAR 2020											
J								Energy aware smartphone tasks offloading to the cloud using gray wolf optimization	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Communication cost; Energy consumption; Task offloading; Meta-heuristic algorithm	RESOURCE-ALLOCATION; MOBILE	In recent era, the task offloading to cloud servers from the smartphones is a promising strategy in improving the smartphones capability and its battery lifetime. The effectiveness of task offloading is found by considering the communication cost and energy consumption, which is considered as crucial factors. These two factors enables the smartphone devices to incur proper decisions whether to perform task offloading or not. In this paper, a heterogeneous framework is designed to improve the energy efficiency of mobile phones by considering various system parameters like local cloudlets, remote cloud servers, task and non-task offloading smartphone and radio access networks. The task offloading framework uses a meta-heuristic algorithm namely Gray Wolf Optimization (GWO) to schedule the task by optimizing the system parameters and this enables to make optimal decision on task offloading. The GWO does the scheduling of tasks in optimal way to enable restriction free communication between the mobile device and cloud server. This reduces the consumption of energy in mobile devices. The GWO task offloading framework is simulated using CloudSim simulation tool and the results are compared in terms of various parameters. The result shows that the GWO task offloading framework is efficient than conventional Online Code Offloading and Scheduling and Adaptive Partitioning and Dynamic Selective Offloading.																	1868-5137	1868-5145															10.1007/s12652-020-01756-y		MAR 2020											
J								Real-time image dehazing by superpixels segmentation and guidance filter	JOURNAL OF REAL-TIME IMAGE PROCESSING										Dehazing; Defogging; Real-time remote sensed images haze removal; Real-time underwater images enhancement; Statistical method of dark channel prior; Superpixels segmentation	CONTRAST ENHANCEMENT; PHYSICAL MODEL; VISIBILITY; ALGORITHM; RESTORATION; WEATHER; QUALITY; SYSTEM	Haze and fog had a great influence on the quality of images, and to eliminate this, dehazing and defogging are applied. For this purpose, an effective and automatic dehazing method is proposed. To dehaze a hazy image, we need to estimate two important parameters such as atmospheric light and transmission map. For atmospheric light estimation, the superpixels segmentation method is used to segment the input image. Then each superpixel intensities are summed and further compared with each superpixel individually to extract the maximum intense superpixel. Extracting the maximum intense superpixel from the outdoor hazy image automatically selects the hazy region (atmospheric light). Thus, we considered the individual channel intensities of the extracted maximum intense superpixel as an atmospheric light for our proposed algorithm. Secondly, on the basis of measured atmospheric light, an initial transmission map is estimated. The transmission map is further refined through a rolling guidance filter that preserves much of the image information such as textures, structures and edges in the final dehazed output. Finally, the haze-free image is produced by integrating the atmospheric light and refined transmission with the haze imaging model. Through detailed experimentation on several publicly available datasets, we showed that the proposed model achieved higher accuracy and can restore high-quality dehazed images as compared to the state-of-the-art models. The proposed model could be deployed as a real-time application for real-time image processing, real-time remote sensing images, real-time underwater images enhancement, video-guided transportation, outdoor surveillance, and auto-driver backed systems.																	1861-8200	1861-8219															10.1007/s11554-020-00953-4		MAR 2020											
J								Archimedean geometric Heronian mean aggregation operators based on dual hesitant fuzzy set and their application to multiple attribute decision making	SOFT COMPUTING										Dual hesitant fuzzy set; Archimedeant-norm andt-conorm; Geometric Heronian mean; Multiple attribute decision making	CORRELATION-COEFFICIENT; T-CONORM; DISTANCE	Fuzzy set, intuitionistic fuzzy set, hesitant fuzzy set can be regarded as a special case of dual hesitant fuzzy set. Therefore, dual hesitant fuzzy set is a more comprehensive set. Further, Archimedeant-norm andt-conorm provides generalized operational rules for dual hesitant fuzzy set. And geometric Heronian mean have advantages when considering the interrelationship of aggregation arguments. Thus, it is necessary to extend the geometric Heronian mean operator to the dual hesitant fuzzy environment based on Archimedeant-norm andt-conorm. Comprehensive above, in this paper, the dual hesitant fuzzy geometric Heronian mean operator and dual hesitant fuzzy geometric weighted Heronian mean operator based on Archimedeant-norm andt-conorm are developed. Their properties and special case are investigated. Moreover, a multiple attribute decision making method is proposed. The effectiveness of our method and the influence of parameters on multiple attribute decision making are studied by an example. The superiority of our method is illustrated by comparing with other existing methods.																	1432-7643	1433-7479				OCT	2020	24	19					14721	14733		10.1007/s00500-020-04819-6		MAR 2020											
J								Fuzzy logic-based decision-making system design for safe forklift truck speed: cast cobblestone production application	SOFT COMPUTING										Fuzzy logic application; Safely forklift speed; Forklift accident; Fuzzy decision-making; Safely logistics	INTELLIGENCE; METHODOLOGY; REGRESSION; ALGORITHM; SUPPORT; TESTS; MODEL	This paper presents a model in which "fuzzy logic multi-criteria decision-making method" is suggested to determine real-time forklift speed to reduce occupational accidents caused by operators. The model developed in the study uses the variables: the weight and height of the load carried by the forklift, the number of products on its pallet, the places with high risk of accident, and the wet-dry condition of the ground. In order to evaluate the performance of the suggested model, a data set comprises 128 different conditions in cast cobblestone production. Determined forklift speeds were compared with the forklift speeds determined by fuzzy logic using statistically analyses. Results showed that fuzzy logic model has a high accuracy and low error. Fuzzy logic modeling has proved to be a good way to decide the real-time speed of the forklifts being used in production without compromising occupational safety. Friedman test and Wilcoxon test have been used to estimate the significance of fuzzy logic method. The fuzzy logic results showed that our method achieved better results compared to beginner operator.																	1432-7643	1433-7479				OCT	2020	24	19					14907	14920		10.1007/s00500-020-04843-6		MAR 2020											
J								Evolving Toward Subject-Specific Gait Rehabilitation Through Single-Joint Resistive Force Interventions	FRONTIERS IN NEUROROBOTICS										gait rehabilitation; wearable robotics; cable-driven robots; subject-specific paradigm; single joint intervention	WALKING; DESIGN; TRIAL; OPTIMIZATION; INDIVIDUALS; ADAPTATION; ASSISTANCE; MOVEMENT; PELVIS; STROKE	Walking is one of the most relevant tasks that a person performs in their daily routine. Despite its mechanical complexities, any change in the external conditions that applies some external perturbation, or in the human musculoskeletal system that limits an individual's movement, entails a motor response that can either be compensatory or adaptive in nature. Incidentally, with aging or due to the occurrence of a neuro-musculoskeletal disorder, a combination of such changes including reduced sensory perception, muscle weakness, spasticity, etc. has been reported, and this can significantly degrade the human walking performance. Various studies in gait rehabilitation literature have identified a need for the development of better rehabilitation paradigms and have implied that an efficient human robot interaction is critical. Understanding how humans respond to a particular gait alteration can be beneficial in designing an effective rehabilitation paradigm. In this context, the current work investigates human locomotor adaptation to resistive alteration to the hip and ankle strategies of walking. A cable-driven robotic system, which does not add mobility constraints, was used to implement resistive force interventions within the hip and ankle joints separately through two experiments with eight healthy adult participants in each. In both cases, the intervention was applied during the push-off phase of walking, i.e., from pre-swing to terminal swing. The results showed that subjects in both groups adopted a compensatory response to the applied intervention and demonstrated intralimb and interlimb adaptation. Overall, the participants demonstrated a deviant gait implying lower limb musculoskeletal adjustments as if to compensate for a hip or ankle abnormality.																	1662-5218					MAR 12	2020	14								15	10.3389/fnbot.2020.00015													
J								Linear quadratic optimal control problem with fuzzy variables via neural network	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Optimal control problem with fuzzy variables; fuzzy linear system; perceptron neural network	CONTROLLABILITY; EQUATIONS	In this scientific research, a new technique to solve a linear quadratic optimal control problem with fuzzy variables is proposed. The problem by an efficient transformation reduces to a crisp problem, and the solution is obtained from the direct method. In fact, we use the approximation method with artificial neural network, and to obtain the solution, the perceptron neural network (PNN) is given. Finally, two numerical examples are presented to show the performance of the technique.																	0952-813X	1362-3079															10.1080/0952813X.2020.1737245		MAR 2020											
J								Face anti-spoofing by identity masking using random walk patterns and outlier detection	PATTERN ANALYSIS AND APPLICATIONS										Face anti-spoofing; Random scan; Auto-population; Planer spoofing; One-class SVM		Existing architectures used in face anti-spoofing tend to deploy registered spatial measurements to generate feature vectors for spoof detection. This means that the ordering or sequence in which specific statistics are computed cannot be changed, as one moves from one facial profile to another. While this arrangement works in a person-specific setting, it becomes a major drawback when single-sided training is done based on the natural face class alone. To mitigate subject identity linked content interference within the anti-spoofing frame, we propose a identity-independent architecture based on random correlated scans of natural face images. The same natural face image can be scanned multiple times through independent correlated random walks before deriving simple differential features on the 1D scanned vectors. This proposed frame tends to capture the pixel correlation statistics with minimal content interference and shows great promise, particularly when trained on natural face sets, using a one-class support vector machine and cross-validated on other databases.																	1433-7541	1433-755X				NOV	2020	23	4					1735	1754		10.1007/s10044-020-00875-8		MAR 2020											
J								Grasshopper optimization algorithm utilized Xilinx controller for maximum power generation in photovoltaic system	EVOLVING SYSTEMS										Photovoltaic array; Maximum power point tracking; Grasshopper optimization algorithm; Xilinx system generator controller; Voltage source inverter; Particle swarm optimization; Artificial bee colony	POINT TRACKING; MPPT; DESIGN; OPERATION	A novel Grasshopper Optimization (GOA) based Xilinx System Generator (XSG) controller is proposed for the purpose of evaluating the Maximum Power Point Tracking in the grid integrated Photovoltaic (PV) power generation system. The proposed controller functions with the assistance of the GOA algorithm and the XSG procedures. The innovation of the controller is intended to collect the maximum power produced from the PV array in accordance with the solar irradiance and temperature of the array. The proposed GOA algorithm based XSG controller is achieved by maximum power from the PV array, it is necessary to adjust the switching signal for the voltage source inverter (VSI). The proposed PV structure is elegantly designed in the mighty platform of the MATLAB/Simulink and the switching schemes are produced in accordance with the XSG controller. At last, the output response of the presented GOA based XSG controller is analyzed and compared with conventional techniques such as the Particle Swarm Optimization (PSO) and the Artificial Bee Colony (ABC) algorithms.																	1868-6478	1868-6486															10.1007/s12530-020-09333-6		MAR 2020											
J								A Control Scheme for Physical Human-Robot Interaction Coupled with an Environment of Unknown Stiffness	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Physical human-robot interaction (pHRI); Variable admittance control; Collaborative manipulator	IMPEDANCE CONTROL; MANIPULATORS; FORCE; MOTION; DESIGN; SYSTEM	Variable admittance control is commonly used for a collaborative robot to achieve the compliant or accurate cooperation according to human's intention. However, existing research seldom investigates such a human-robot collaboration coupled with an extra environment with unknown stiffness. If the end-effector that is guided by a human with various intended motion contacts the unknown environment, the interaction might become unstable. Additionally, current research for this physical human-robot-environment interaction use two force sensors to address the issue, and hence the cost of the robot is likely to increase and it reduces the flexibility to many applications. Therefore, in this paper, we address the issue of physical human-robot interaction coupled with an extra environment whose stiffness is unknown. To achieve this, the condition of robot admittance is rigorously proved in accordance with different human intended motion and environmental stiffness. Moreover, a variable admittance control scheme is proposed based on human intention, environmental force and environment stiffness using the combination of a force sensor and a force observer. Simulation and experiments are conducted to demonstrate the effectiveness of the proposed control scheme.																	0921-0296	1573-0409				OCT	2020	100	1					165	182		10.1007/s10846-020-01176-2		MAR 2020											
J								A genetic approach for the maximum network lifetime problem with additional operating time slot constraints	SOFT COMPUTING										Wireless sensor networks; Maximum network lifetime; Time slot; Genetic algorithm	WIRELESS SENSOR NETWORKS; COVERAGE	The maximum network lifetime problem is a well-known and challenging optimization problem which has been addressed successfully with several approaches in the last years. It essentially consists in finding an optimal schedule for sensors activities in a wireless sensor network (WSN) aiming at maximizing the total amount of time during which the WSN is able to perform its monitoring task. In this paper, we consider a new scenario in which, in order to monitor some locations in a geographical area, the sensors need to be active for a fixed amount of time, defined as operatingtime slot. For this new scenario, we derive an upper bound on the maximum lifetime and propose a genetic algorithm for finding a near-optimal node activity schedule. The performance evaluation results obtained on numerous benchmark instances show the effectiveness of the proposed approach.																	1432-7643	1433-7479				OCT	2020	24	19					14735	14741		10.1007/s00500-020-04821-y		MAR 2020											
J								Hybrid machine learning for predicting strength of sustainable concrete	SOFT COMPUTING										Artificial intelligence; Machine learning; Optimization; Lightweight foamed concrete; Sustainable concrete; Compressive strength	SUPPORT VECTOR MACHINES; GREY WOLF OPTIMIZATION; FOAMED CONCRETE; COMPRESSIVE STRENGTH; ARTIFICIAL-INTELLIGENCE; ALGORITHM; CEMENT	Foamed concrete material is a sustainable material which is widely used in the construction industry due to their sustainability. Accurate prediction of their compressive strength is vital for structural design. However, empirical methods are limited to consider simultaneously all influencing factors in predicting the compressive strength of foamed concrete materials. Thus, this study proposed a novel hybrid artificial intelligence (AI) model which couples the least squares support vector regression (LSSVR) with the grey wolf optimization (GWO) to consider effectively the influencing factors and improve the predictive accuracy in predicting the foamed concrete's compressive strength. Performance of the proposed model was evaluated using a real-world dataset. Comparison results confirm that the proposed GWO-LSSVR model was superior than the support vector regression, artificial neural networks, random forest, and M5Rules with the improvement rate of 144.2-284.0% in mean absolute percentage error (MAPE). Notably, the evaluation results show that the GWO-LSSVR model showed the good agreement between the actual and predicted values with the correlation coefficient of 0.991 and MAPE of 3.54%. Thus, the proposed AI model was suggested as an effective tool for designing foamed concrete materials.																	1432-7643	1433-7479				OCT	2020	24	19					14965	14980		10.1007/s00500-020-04848-1		MAR 2020											
J								Sequence recommendation based on deep learning	COMPUTATIONAL INTELLIGENCE										attention mechanism; deep learning; personalized commodity recommendation; recurrent neural networks		In order to solve the cold start problem of traditional recommendation algorithm, the sequence change of user interaction information and deep learning are gradually considered as a key feature of commodity recommendation system. However, most of the existing recommendation methods based on the sequence changes assume that all the interaction information of users is equally important for recommendation, which is not always applicable in real scenarios, because the interaction process of user items is full of randomness and contingency. In this article, we study how to reduce the randomness and contingency between session sequences, make full use of the association between session sequences in the interaction process of users by Deep Learning. In order to better simulate the change of session sequence in the real scene, we adopt sequence sampling methods to transform the single classification problem into sequence modeling problem. And attention mechanism is added to reduce the interference of the recommendation model in the sequence due to the contingency and randomness of the user in the shopping. Finally, through the verification of real data, the MRR@20 index of the improved model is 20% higher than the benchmark level.																	0824-7935	1467-8640															10.1111/coin.12307		MAR 2020											
J								A novel methodology for the development of an optimal agricultural crop field using Internet of Things	COMPUTATIONAL INTELLIGENCE										agriculture; Internet of Things; motor automation; soil parameters	WIRELESS; IRRIGATION; MODEL	The Internet of Things (IoT) plays a vital role in the entity sharing and minimizing the workload of the human beings in various aspects. Nowadays the term IoT is used in various fields such as health care, automobiles, industry, agriculture, and so on. Agriculture is the main source of food to whole world. There are various problems faced by the farmers in agriculture due to shortage and wastage of water and fertilizers. In this regard, an optimal IoT model has been developed and proposed to attain an effective crop field. The proposed IoT model will monitor, record temperature, and soil moisture values, which will be continuously analyzed to achieve optimal plant growth and yield. The motor will be connected to the IoT model which automatically switch on/off based on optimal threshold temperature and soil moisture content value. A novel irrigation algorithm named differential waterflow algorithm has been proposed and deployed in the proposed IoT model for the automatic usage of the motor in the field. The proposed IoT model provides a web interface to the user through the cloud storage, so that the farmer can control and monitor the system in remote. The proposed system will reduce the water consumption and will ensure the uniform water distribution to the crops through the Poisson distribution which results in increasing yield.																	0824-7935	1467-8640															10.1111/coin.12308		MAR 2020											
J								Integrated kitchen design and optimization based on the improved particle swarm intelligent algorithm	COMPUTATIONAL INTELLIGENCE										design; inertia weight; integrated kitchen; optimization; particle swarm intelligent algorithm		The layout and design of the integrated kitchen can affect the efficiency of people's cooking work greatly. An excellent integrated kitchen design requires each kitchen cabinet module to meet certain constraints and reach the highest work efficiency in a certain space. In this article, we proposed an improved particle swarm intelligence algorithm (IPSO, for short) method by initializing the population chaos, dynamically improving the inertia weight and adjusting the acceleration factor, and applied in the kitchen design and optimization. This method combines the mathematical intelligent algorithm with the integrated kitchen design for the first time, and further selects the optimal design scheme from the preliminary schemes according to the fitness curve of the kitchen mathematical model, which provides the theoretical basis for the refined design of kitchen products. The method can also be used in home design, interior design, and other related areas.																	0824-7935	1467-8640															10.1111/coin.12301		MAR 2020											
J								An approach to investigate the best location for the central node placement for energy efficient WBAN	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Energy efficiency; IEEE 802; 15; 6; Node placement; Path loss; Reflection coefficient; UWB; WBAN	BODY AREA NETWORKS; WIRELESS; TECHNOLOGIES	WBAN has been gaining wide interest as it enables continuous and real-time monitoring of applications like healthcare, sports, entertainment and military. IEEE 802.15.6 is the IEEE WBAN standard supporting low power wireless communication in and around the human body. On-body WBAN comprises of several nodes deployed on the human body to measure various physiological data and further forward it to the central node. For efficient working of WBAN, the central node which may act as the gateway is preferably placed on the body itself. In this work for the considered scenarios best location for the central node is computed using Reflection Coefficient (S-11) of an antenna positioned on the body and IEEE 802.15.6 CM3A path loss model between the communicating nodes. Considering a steady position of the body in the laboratory, computations have been carried out by using a standard lambda/4 microstrip monopole patch ultra-wideband antenna attached with a vector network analyzer (VNA). Various parameters are evaluated from the measured data and compared to investigate the effect of different node positions on WBAN performance and hence explore best suitable location for the central node.																	1868-5137	1868-5145															10.1007/s12652-020-01847-w		MAR 2020											
J								A ranking method based on possibility mean for multi-attribute decision making with single valued neutrosophic numbers	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Possibility mean; Single valued neutrosophic numbers; Trapezoidal neutrosophic numbers; Triangular neutrosophic numbers; Multi-attribute decision making	INTUITIONISTIC FUZZY-SETS; VARIANCE; OPERATOR	Single valued neutrosophic (SVN) set has a useful independent mathematical structure for expressing the incomplete, inconsistent and indeterminate information. Many researchers have studied decision making problems with SVN environment. This paper presented a new ranking method of SVN-numbers based on possibility theory for solving a multi-attribute decision making (MADM) problem. The first time we have defined the concept of possibility mean of SVN numbers and applied to multi-attribute decision making problem in which the attribute values are considered as SVN-numbers. Then, we propose the possibility mean of the truth-membership function, indeterminacy-membership function and falsity-membership function for single valued trapezoidal neutrosophic (SVTN) numbers and studied some desired properties of SVTN. Thus, we have developed a new ranking approach using the concept of weighted possibility mean, and applied to MADM problems. Finally, a numerical example is examined to show the applicability and an embodiment of the proposed method.																	1868-5137	1868-5145															10.1007/s12652-020-01853-y		MAR 2020											
J								Detecting disorders in retinal images using machine learning techniques	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Vessel classification; Optic disc; Image processing techniques; Classification; Machine learning techniques; AVR measurement; Exudates		The importance of the research is to detect disorders in retinal images using machine learning techniques. Eyes play an important role in our day to day life and are the most valuable gift we have in retinal eye research, the retinal vessel parameters and accurate AVR measurement is consider as the important issues in image processing techniques. Diseases like glaucoma, exudates, and diabetic retinopathy can be observed in earlier stages by using retinal images. Diabetic retinopathy also known as diabetic eye disease which affects up to 80% of all patients who have diabetes eventually leads to blindness. To diagnosis the various disorder in earlier stage by using the retinal image based on the methodology of image processing and machine learning techniques. The retinal image is used to detect the diabetes in early stages by evaluating all the retinal blood vessels together. The proposed novel algorithm known as multi-resolution curvelet Transform and normalized graph cut segmentation to detect the optic disc and blood vessels in the fundus images efficiently. In earlier stage of this research, the pre-processing of fundus image operation for image filtration and color contrast enhancement, and next is image segmentation for blood vessels achieved by image processing techniques such as thresholding, texture, and morphological operation and finally machine learning classification algorithm are executed using convolutional neural network. This construction results in a multi resolution, local, and directional image expansion using contour segments. Results indicate that neural network is better than the other techniques for vessels classification.																	1868-5137	1868-5145															10.1007/s12652-020-01841-2		MAR 2020											
J								Integration of a music generator and a song lyrics generator to create Spanish popular songs	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Computational creativity; Music generation; Lyrics generation	CONSTRUCTION; SYSTEMS	The automatic generation of music is an emerging field of research that has attracted wide attention in Computer Science. However, most works are centered in classical music. This work develops ETHNO-MUSIC, an intelligent system that generates melodies based on popular music. ETHNO-MUSIC generates melodies with Markov models, which learns from a corpus of Spanish popular music. Then, given the importance of the lyrics in this context, ETHNO-MUSIC was integrated with Tra-La-Lyrics, an existing system that generates lyrics following a melody, which has been specifically adapted to suit this purpose. Several experiments were carried out to evaluate the quality of the results, based on human opinions towards generated pieces of music and lyrics. Overall, results are positive. Briefly, they reflect that, on the one hand, the melodies transmit a feeling of Spanish popular music, and on the other hand, the text of the lyrics is related to the topics analyzed, and the rhythm follows the melodic aspects of the music.																	1868-5137	1868-5145															10.1007/s12652-020-01822-5		MAR 2020											
J								State estimation of T-S fuzzy Markovian generalized neural networks with reaction-diffusion terms: a time-varying nonfragile proportional retarded sampled-data control scheme	NEURAL COMPUTING & APPLICATIONS										T-S fuzzy model; Markovian GNNs; Reaction-diffusion terms; Nonfragile sampled-data control; Time-varying proportional retarded controller	DELAY-DEPENDENT STABILITY; DISSIPATIVITY ANALYSIS; EXPONENTIAL STABILITY; STOCHASTIC STABILITY; JUMP; DISCRETE; SYNCHRONIZATION; INTERVAL; PARAMETERS; CRITERIA	This paper focuses on the state estimation issue of T-S fuzzy Markovian generalized neural networks (GNNs) with reaction-diffusion terms. An estimator-based nonfragile time-varying proportional retarded sampled-data controller that permits norm-bounded indeterminacy and contains a time-varying delay is designed to guarantee the asymptotical stability of the error system. By establishing a novel Lyapunov-Krasovskii functional that involves positive indefinite items and discontinuous items, meanwhile, by combining the reciprocally convex combination method, Jenson's inequality and Wirtinger inequality, a less conservative stability criterion can be derived. Moreover, the principle for the number of selected variables in the process of deriving main results is also analyzed. Finally, two numerical examples are given to demonstrate the validity and advantages of the results proposed in this paper.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14639	14653		10.1007/s00521-020-04817-7		MAR 2020											
J								Constructing domain-dependent sentiment dictionary for sentiment analysis	NEURAL COMPUTING & APPLICATIONS										Lexicon sentiment dictionary; Neural network; Sentiment analysis; Aspect-level sentiment analysis	VALENCE; LEXICON	Sentiment dictionary is of great value to sentiment analysis, which is used widely in sentiment analysis compositionality. However, the sentiment polarity and intensity of the word may vary from one domain to another. In this paper, we introduce a novel approach to build domain-dependent sentiment dictionary,SentiDomain. We propose a weak supervised neural model that aims at learning a set of sentiment clusters embedding from the sentence global representation of the target domain. The model is trained on unlabeled data with weak supervision by reconstructing the input sentence representation from the resulting representation. Furthermore, we also propose an attention-based LSTM model to address aspect-level sentiment analysis task based on the sentiment score retrieved from the proposed dictionary. The key idea is to weight-down the non-sentiment parts among aspect-related information in a given sentence. Our extensive experiments on both English and Chinese benchmark datasets have shown that compared to the state-of-the-art alternatives, our proposals can effectively improve polarity detection.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14719	14732		10.1007/s00521-020-04824-8		MAR 2020											
J								Strategic voting in the lab: compromise and leader bias behavior	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Social choice; Voting; Agent societies; Coordination in multi-agent systems; Agreement technologies	SCORING RULES; MODEL; INFORMATION; ELECTIONS; CALCULUS; MOMENTUM; OUTCOMES; GAMES; VOTE	Plurality voting is perhaps the most commonly used way to aggregate the preferences of multiple voters. Yet, there is no consensus on how people vote strategically, even in very simple settings. The purpose of this paper is to provide a comprehensive study of people's voting behavior in various online settings under the plurality rule. We implemented voting games that replicate two common real-world voting scenarios in controlled experiments. In the first, a single voter votes once after seeing a pre-election poll. In the second game, a group of voters play an iterative game, and change their vote as the game progresses (as in online voting). The winning candidate in each game (and hence the subject's payment) is determined using the plurality rule. For each of these settings we generated hundreds of game instances, varying conditions such as the number of voters, subjects' preferences over candidates and the poll information that was made available to the subjects prior to voting. We show that people can be classified into several groups, one of which is not engaged in any strategic behavior, while the largest group demonstrates both a tendency for strategic compromise, and a bias toward voting for the leader in the poll. We provide a detailed analysis of this group behavior for both settings, and how it depends on the poll information. Our study has insight for multi-agent system designers in uncovering patterns that provide reasonable predictions of voters' behaviors, which may facilitate the design of agents that support people or act autonomously in voting systems.																	1387-2532	1573-7454				MAR 11	2020	34	1							31	10.1007/s10458-020-09446-x													
J								Experimental Validation of HeritageBot III, a Robotic Platform for Cultural Heritage	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Robot design; Mobile robotics; Prototypes; Experimental robotics; Application for cultural heritage	DRONES; DESIGN; VALLEY	Activity in Cultural Heritage frames aims to have a good monitoring of unknown cultural heritage also in an inaccessible site decreasing the cost of interventions. There are several robotics systems available both as rovers and drones. They are suited for inspection tasks in risky environments or for surveillance purposes provided of several sensors able to detect and monitor the area under examination, sometimes they can be also equipped with an end-effector for manipulation tasks or to collect objects. This paper introduces HeritageBot III (HBIII), a service robot for Cultural Heritage frames, which consists in a robotic platform with a modular design for both ground locomotion and flight capability. The proposed design is able to merge the advantages of drone and legged mobile robots in an innovative platform, able to navigate in most unknown environment.																	0921-0296	1573-0409				OCT	2020	100	1					223	237		10.1007/s10846-020-01180-6		MAR 2020											
J								Model-Based Reinforcement Learning Variable Impedance Control for Human-Robot Collaboration	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Human-robot collaboration; Machine learning; Industry 4; 0; Model-based reinforcement learning control; Variable impedance control	ADMITTANCE CONTROL; HUMAN ARM	Industry 4.0is takinghuman-robot collaborationat the center of the production environment. Collaborative robots enhance productivity and flexibility while reducing human's fatigue and the risk of injuries, exploiting advanced control methodologies. However, there is a lack of real-time model-based controllers accounting for the complex human-robot interaction dynamics. With this aim, this paper proposes aModel-Based Reinforcement Learning(MBRL) variable impedance controller to assist human operators in collaborative tasks. More in details, an ensemble of Artificial Neural Networks (ANNs) is used to learn a human-robot interaction dynamic model, capturing uncertainties. Such a learned model is kept updated during collaborative tasks execution. In addition, the learned model is used by aModel Predictive Controller(MPC) withCross-Entropy Method(CEM). The aim of the MPC+CEM is to online optimize the stiffness and dampingimpedance controlparameters minimizing the human effort (i.e, minimizing the human-robot interaction forces). The proposed approach has been validated through an experimental procedure. A lifting task has been considered as the reference validation application (weight of the manipulated part: 10 kg unknown to the robot controller). A KUKA LBR iiwa 14 R820 has been used as a test platform. Qualitative performance (i.e, questionnaire on perceived collaboration) have been evaluated. Achieved results have been compared with previous developed offline model-free optimized controllers and with the robot manual guidance controller. The proposed MBRL variable impedance controller shows improved human-robot collaboration. The proposed controller is capable to actively assist the human in the target task, compensating for the unknown part weight. The human-robot interaction dynamic model has been trained with a few initial experiments (30 initial experiments). In addition, the possibility to keep the learning of the human-robot interaction dynamics active allows accounting for the adaptation of human motor system.																	0921-0296	1573-0409				NOV	2020	100	2					417	433		10.1007/s10846-020-01183-3		MAR 2020											
J								Trajectory Planning of Quadrotor UAV with Maximum Payload and Minimum Oscillation of Suspended Load Using Optimal Control	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Quadrotor; Maximum payload; Minimum swing; Optimal control; Path planning; Suspended load	TRACKING CONTROL; GENERATION; MANIPULATORS; MAXIMIZATION; HELICOPTER	This paper focuses on the problem of transporting a cable-suspended load by a quadrotor UAV for safer flight and more efficiency. The dynamic model of a quadrotor coupled to the suspended load is derived using the Euler-Lagrange formulation. The optimal trajectory for carrying the maximum payload and minimum oscillation of swinging load will be obtained. The optimal cable length to increase the maximum payload capacity and reduce the maximum oscillation angle of swinging load is obtained. Also, the effect of load mass on the maximum oscillation angle of swinging load is studied. In this paper, the optimization procedure is based on the solution of the optimal control problem from the class of open loop with an indirect method. The application Pontryagin's Minimum Principle lead to deriving the optimality conditions and subsequently a two-point boundary value problem (TPBVP) which is solved by a numerical method. An appropriate algorithm is presented for calculating the maximum payload to move between two specified points. The main superiority of this method is that it can solve a wide range of optimal maneuvers for arbitrary initial and final configurations relevant to every considered cost function. Generating various optimal paths with different maximum payloads and oscillation angles by modifying the values of the penalty matrices. In order to verify the efficiency of the proposed method and the presented algorithm, a simulation study is performed for a quadrotor with a suspended load in maneuver between two specified points and various object function.																	0921-0296	1573-0409															10.1007/s10846-020-01166-4		MAR 2020											
J								Stress generation and non-intrusive measurement in virtual environments using eye tracking	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Virtual reality; Stress generation; Stress measurement; Virtual stressors; Eye tracking; VR application	METAANALYSIS; TASK	In real life, it is well understood how stress can be induced and how it is measured. While virtual reality (VR) applications can resemble such stress inducers, it is still an open question if and how stress can be measured in a non-intrusive way during VR exposure. Usually, the quality of VR applications is estimated by user acceptance in the form of presence. Presence itself describes the individual's acceptance of a virtual environment as real and is measured by specific questionnaires. Accordingly, it is expected that stress strongly affects this presence and thus also the quality assessment. Consequently, identifying the stress level of a VR user may enable content creators to engage users more immersively by adjusting the virtual environment to the measured stress. In this paper, we thus propose to use a commercially available eye tracking device to detect stress while users are exploring a virtual environment. We describe a user study in which a VR task was implemented to induce stress, while users' pupil diameter and pulse were measured and evaluated against a self-reported stress level. The results show a statistically significant correlation between self-reported stress and users' pupil dilation and pulse, indicating that stress measurements can indeed be conducted during the use of a head-mounted display. If this indication can be successfully proven in a larger scope, it will open up a new era of affective VR applications using individual and dynamic adjustments in the virtual environment.																	1868-5137	1868-5145															10.1007/s12652-020-01845-y		MAR 2020											
J								Cloud-based robotic system for crowd control in smart cities using hybrid intelligent generic algorithm	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Smart city; Cloud computing; Robotics; Energy; Storage	SERVICE	In recent years, cloud robotics in smart cities emerged with the technology of cloud computing and robotic enabled services such as ubiquitous computing based on internet resources, wireless sensor network, communication technologies, and large scale storage system for smart systems. The Internet of things is a complement to generate an enhanced system where robots offload the pervasive cloud to ensure the quality of the service from the data-intensive computation. In general the robots offloading is a crucial issue because of their skill learning, decision-making capabilities, a unique feature of mobility, and data collection. In this paper, a cloud-based robotic system for the crowd control system in smart cities using the hybrid intelligent generic algorithm has been proposed for smart crowd management. The cloud enhanced robots are employed to execute the essential task while taking into consideration of different complexity in a smart city. Furthermore, the integrated framework has been introduced to handling the task offloading and task completion through the robotics movement with minimum time and energy. The experimental results shows that the proposed system achieves powerful computation, storage, energy-efficient and reduce the cost with the help of cloud computing for smart city management.																	1868-5137	1868-5145															10.1007/s12652-020-01758-w		MAR 2020											
J								Antenna selection with improved group based particle swarm optimization (IGPSO) and joint adaptive beam forming for wideband millimeter wave communication	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Multiple-input multiple-output (MIMO); Path delay pre-compensation; Single-carrier (SC) transmission; Group sparse optimization (GSO); Millimeter wave communications; Lens antenna array	MIMO	In general, realization of large array communication for mm Wave systems is non-vital. Radio Traditional digital antenna array that comprises of one Radio Frequency (RF) chain for every antenna leads to high consumption of energy, complexity in signal processing, residue of noise. However, one major drawback of analogue beam forming lies in its inability to support spatial multiplexing. Minimum Variance Distortion less Response (MVDR) is enhanced to rectify this noise inference and also the variance/power of interference. The major aim of this work is to maximize the MVDR and minimum Signal-To-Interference-Plus-Noise Ratio (SINR through effective designing of beamforming and optimal selection of antenna. Antenna selection based on power is proposed at first step and beamforming vector is optimized after selection of antenna as like in single-user setup. In case of impossibility of separation of multi-path signal, performance is enhanced by an Improved Group based Particle Swarm Optimization (IGPSO) based antenna selection and Joint adaptive beamforming scheme is proposed. A Beamforming coefficient of the entire user's associated with an antenna is assigned to zero by MVDR and SINR in order to deactivate certain antenna. So, IGPSO is deployed in design of beamforming and joint selection of antenna. It has a non-convex constraint. Convex constraint is used for effectively approximate it. Experimental results shows that proposed design including array of lens antenna is able to produce better performance than benchmark schemes with reduced Bit Error Rate (BER) and improved spectral efficiently in terms of Spectrum efficiency and max min rate.																	1868-5137	1868-5145															10.1007/s12652-020-01828-z		MAR 2020											
J								An improved key term weightage algorithm for text summarization using local context information and fuzzy graph sentence score	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Data mining; Intelligent systems; NLP; Information retrieval; Fuzzy; Probabilistic model; Unsupervised learning		The process of text summarization is to identify the crux of the document. In the proposed work, summarization is done using three different algorithms. They are sentence based key term weightage, the two way local context information scoring (LCIS) and the fuzzy graph sentence scoring (FGSS). They are used to improve the weight of the key terms, identify LCIS and the centroid of the document by calculating FGSS score respectively. This intelligent system assigning sentence based weightage to the key terms is found to be effective. The present method is domain and language independent. It provides good harmonic mean in comparison with the earlier studies and does not require any training or testing.																	1868-5137	1868-5145															10.1007/s12652-020-01856-9		MAR 2020											
J								A novel hybrid GWO with WOA for global numerical optimization and solving pressure vessel design	NEURAL COMPUTING & APPLICATIONS										Whale optimization algorithm; Grey wolf optimization; Benchmark test functions; Nature-inspired algorithms; Engineering problem; Solving pressure vessel design	ALGORITHM	A recent metaheuristic algorithm, such as Whale optimization algorithm (WOA), was proposed. The idea of proposing this algorithm belongs to the hunting behavior of the humpback whale. However, WOA suffers from poor performance in the exploitation phase and stagnates in the local best solution. Grey wolf optimization (GWO) is a very competitive algorithm comparing to other common metaheuristic algorithms as it has a super performance in the exploitation phase, while it is tested on unimodal benchmark functions. Therefore, the aim of this paper is to hybridize GWO with WOA to overcome the problems. GWO can perform well in exploiting optimal solutions. In this paper, a hybridized WOA with GWO which is called WOAGWO is presented. The proposed hybridized model consists of two steps. Firstly, the hunting mechanism of GWO is embedded into the WOA exploitation phase with a new condition which is related to GWO. Secondly, a new technique is added to the exploration phase to improve the solution after each iteration. Experimentations are tested on three different standard test functions which are called benchmark functions: 23 common functions, 25 CEC2005 functions, and 10 CEC2019 functions. The proposed WOAGWO is also evaluated against original WOA, GWO, and three other commonly used algorithms. Results show that WOAGWO outperforms other algorithms depending on the Wilcoxon rank-sum test. Finally, WOAGWO is likewise applied to solve an engineering problem such as pressure vessel design. Then the results prove that WOAGWO achieves optimum solution which is better than WOA and fitness-dependent optimizer (FDO).																	0941-0643	1433-3058				SEP	2020	32	18			SI		14701	14718		10.1007/s00521-020-04823-9		MAR 2020											
J								Red deer algorithm (RDA): a new nature-inspired meta-heuristic	SOFT COMPUTING										Red deer algorithm; Meta-heuristics; Real-world applications; Multi-objective optimization	IMPERIALIST COMPETITIVE ALGORITHM; VARIABLE NEIGHBORHOOD SEARCH; VEHICLE-ROUTING PROBLEM; FIXED-CHARGE; GENETIC ALGORITHM; TABU SEARCH; TRANSPORTATION PROBLEM; MINIMIZING EARLINESS; TARDINESS PENALTIES; SCHEDULING PROBLEM	Nature has been considered as an inspiration of several recent meta-heuristic algorithms. This paper firstly studies and mimics the behavior of Scottish red deer in order to develop a new nature-inspired algorithm. The main inspiration of this meta-heuristic algorithm is to originate from an unusual mating behavior of Scottish red deer in a breading season. Similar to other population-based meta-heuristics, the red deer algorithm (RDA) starts with an initial population called red deers (RDs). They are divided into two types: hinds and male RDs. Besides, a harem is a group of female RDs. The general steps of this evolutionary algorithm are considered by the competition of male RDs to get the harem with more hinds via roaring and fighting behaviors. By solving 12 benchmark functions and important engineering as well as multi-objective optimization problems, the superiority of the proposed RDA shows in comparison with other well-known and recent meta-heuristics. [GRAPHICS]																	1432-7643	1433-7479				OCT	2020	24	19					14637	14665		10.1007/s00500-020-04812-z		MAR 2020											
J								Primary frequency regulation of a microgrid by deloaded tidal turbines	SOFT COMPUTING										Imperialist competitive algorithm; Tidal energy; Genetic algorithm; Primary frequency regulation; Particle swarm optimization	COMPARATIVE PERFORMANCE EVALUATION; GENERATION CONTROL; WIND GENERATORS; CONTROLLER; SPEED; PARTICIPATION; PITCH; AGC	In recent years, mindset of people is observed much more inclined towards the usage of renewable energy systems because of the environmentally friendly nature and the monetary advantages of fuel saving. However, since non-conventional sources are unpredictable in nature, consequently high penetration of these sources causes reliability and power quality issues. It inspires researchers to redefine or develop the frequency regulation strategies. This paper introduces a fractional-order control methodology to adapt the primary frequency regulation from deloaded tidal power generators on the basis of existing power constraints. It is ameliorated by inertia and damping control, primary frequency control, and supplementary frequency control of the system. The control processes are implemented through fractional controllers. The parameters of fractional controllers are tuned by imperialist competitive algorithm. To assess the effectiveness of the proposed controller, simulated outcomes are evaluated with conventional controller. Besides this, the ICA-based controller results are also compared with other algorithms such as particle swarm optimization and genetic algorithm. An improvement in performance index from 16.11 to 22.80% is reported with proposed control strategy.																	1432-7643	1433-7479				OCT	2020	24	19					14667	14685		10.1007/s00500-020-04813-y		MAR 2020											
J								Application of deep learning model based on image definition in real-time digital image fusion	JOURNAL OF REAL-TIME IMAGE PROCESSING										Deep learning model; PCNN; Digital image fusion; HVS; Image definition	OBJECT DETECTION; COLOR	This paper focuses on pulse coupled neural network (PCNN) and digital image fusion. Aiming at the existing problems, this paper proposes a real-time deep learning model with dual-channel PCNN fusion algorithm based on image definition. It will also be helpful to digital image forensics. With the integration of the orthogonal color space that conforms to HVS, this algorithm simplifies the traditional PCNN model to a parallel dual-channel adaptive PCNN structure. Also, it can realize the adaptive processing by defining the image definition to be beta, the coupled linking coefficient. As the dynamic threshold can be increased exponentially with this method, it can effectively solve the problems. The experimental result proves that our algorithm outperforms the traditional fusion algorithms according to the subjective visual effect or the objective assessment standard.																	1861-8200	1861-8219				JUN	2020	17	3					643	654		10.1007/s11554-020-00956-1		MAR 2020											
J								ResNet-Attention model for human authentication using ECG signals	EXPERT SYSTEMS										authentication; biometrics; convolutional neural network; DNN; ECG; end-to-end structure; ResNet	CONVOLUTION NEURAL-NETWORK; BIOMETRIC AUTHENTICATION; FEATURE-EXTRACTION; FINGERPRINT; FUSION	Authentication is the process of verifying the claimed identity of the user. Recently, traditional authentication methods such as passwords, tokens, and so on are no longer used for authentication as they are more prone to theft and different types of violations. Therefore, new authentication approaches based on biometric modalities such as heartbeat pattern obtained from electrocardiogram (ECG) signals are considered. Unlike other biometrics, ECG provides the assurance that the person is alive, and is considered as one of the most accurate recent methods for authentication. In this article, two end-to-end deep neural network models for ECG-based authentication are proposed. In the first model, a convolutional neural network (CNN) is developed and in the second model, a residual convolutional neural network (ResNet) with attention mechanism called ResNet-Attention is designed for human authentication. We have used 2-s duration ECG signals obtained from two ECG databases (Physikalisch-Technische Bundesanstalt [PTB] and Check Your Bio-signals Here initiative [CYBHi]) for authentication. Our proposed ResNet-Attention algorithm achieved an accuracy of 98.85 and 99.27% using PTB and CYBHi, respectively. The results obtained by our developed model show that the performance is better than existing algorithms and can be used in real-time authentication systems after the validation with more diverse ECG data.																	0266-4720	1468-0394														e12547	10.1111/exsy.12547		MAR 2020											
J								Development of Conversational Deliberative Agents Driven by Personality via Fuzzy Outranking Relations	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Conversational agent; Deliberative architecture; Personality; Fuzzy outranking model		Currently, there are many types of conversational agents whose goal is to emulate human behavior. These agents offer more believable conversations when their responses come from a deliberative process that mimics individuals' character. Conversational agents are mainly used for response selection linguistics and context situated strategies. These approaches usually build rules to find answers in dialogues; however, this is not the best alternative when the communicative intentions are not literal and context dependent. Deliberative Agents can solve these issues and improve their selection process through the integration of preferences and personality in their cognitive process. Hence, this work investigates how to drive the expression of dialogues of a Conversational Deliberative Agent (CDA) through personality and fuzzy outranking relations; for this, it proposes the characterization of context, and corpus through speech acts theory, and also a selection process based on fuzzy outranking relations to compare corpus phrases and context to choose the best response. The main contributions of this work are (1) the agent architecture that integrates preferences and personality of an individual in the response selection cognitive process; (2) a characterization model of speech acting through criteria based on belief, desires, and intentions to define a more human behavior expression; and (3) the use of fuzzy outranking relations to select phrases from a corpus to match dialogue intentions. An experimental design demonstrated the aptitude of the developed CDA to offer quality responses on a tutor application. Also, the results showed the capacity of speech acts to handle contexts in dialogues.																	1562-2479	2199-3211															10.1007/s40815-020-00817-w		MAR 2020											
J								Parallel Designs for Metaheuristics that Solve Portfolio Selection Problems Using Fuzzy Outranking Relations	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Outranking relations; Optimization; ACO; Parallel designs	EVOLUTIONARY MULTIOBJECTIVE OPTIMIZATION	In decision-making, the multiobjective portfolio selection problem (MPSP) consists of the selection of alternatives based on preferences of a particular decision-maker (DM). In real-world applications, MPSP has several conflicting criteria that DMs must consider to determine an appropriate solution. So far, only fuzzy outranking relations have been used in a relational system of preferences to guide the search process of genetic algorithms (NOSGAII), and ant colony optimization (NOACO) to approximate the region of interest (RoI) of MPSP involving DM's preferences. The NOSGAII and NOACO strategies are sequential, and they face a real challenge when solving high-dimensional instances which is a decrement in the computational efficiency due to the increment in the number of objectives. The present research proposes to use parallelism to tackle the efficiency situation in metaheuristics. The study first identifies which approach approximates better the RoI, and then, it analyzes the effect of parallelism in the performance. The results showed that NOACO found more best compromises in almost all the instances than NOSGAII. Hence, it can be concluded that NOACO approximates better the RoI. Also, the results showed a better average speedup with coarse-grained parallelism in NOACO than with data-flow parallelism, suggesting the conclusion that ants independently working are faster than ants working collaboratively. Finally, the main contributions are (1) the analysis of the performance of the two approaches for MPSP, (2) the five parallel designs for NOACO, and (3) the parallel NOACO that speedups up to 2x the sequential version when solving MPSP.																	1562-2479	2199-3211															10.1007/s40815-019-00794-9		MAR 2020											
J								Motion planning for redundant robotic manipulators using a novel multi-group particle swarm optimization	EVOLUTIONARY INTELLIGENCE										Multi-group PSO; The mechanism of pre-selection and interaction; Motion planning for redundant robotic manipulators	SPOTTED HYENA OPTIMIZER; IIR DIGITAL-FILTERS; SPACE MANIPULATOR; DESIGN; MODEL	Metaheuristic optimization algorithms are widely used in motion planning of redundant robotic manipulators. Existing methods may converge to a local minimum. In this paper, a new multi-group particle swarm optimization algorithm (PSOEL) is proposed to solve the motion planning of manipulators. PSOEL consists of one elite group and several child groups. The population is initialized with a pre-selection mechanism in which the members of the elite group are initialized with the best-performing particles of the child groups. In the process of iteration, the elite group and the child groups evolve separately. When the elite group falls into a local optimum or is inferior to child groups for a certain time, an interaction mechanism is triggered. In the interaction mechanism, some of the best particles selected from the child groups will replace the bad particles of the elite group. With these mechanism of pre-selection and interaction, PSOEL can jump out of the local optimum and reach the global optimum or global suboptimum. Simulation results show that the proposed algorithm PSOEL is superior to the compared algorithms and converges toward the optimum.																	1864-5909	1864-5917				DEC	2020	13	4					677	686		10.1007/s12065-020-00382-z		MAR 2020											
J								Feedback-based fuzzy resource management in IoT using fog computing	EVOLUTIONARY INTELLIGENCE										Resource management; Feedback based mechanism; Fog computing; Latency; Fuzzy; Virtual machines; Cloud with IoT		The goal of Internet of Things (IoT) is to make "things" (wearable devices, smart cameras, sensors and smart home appliances) connect to internet. Large storage is required to store huge volume of data that is generated, data processing need to be carried out between IoT devices and the massive number of applications. This process can be made effectively with the help of cloud computing technology. Resources can be effectively utilized with the help of cloud, and IoT plays a significant role in managing the tasks that are to be offloaded to the cloud. The performance of the application is to be enhanced by providing Quality of Service (QoS) and the performance is evaluated in terms of QoS parameters like Power utilization, Makespan and Execution Time. The tasks are allocated based on priority. Fog computing paradigm is used in the proposed model to decrease the makespan of time. The projected mechanism is tested and compared with different present systems and is shown that proposed methodology produced effective results.																	1864-5909	1864-5917															10.1007/s12065-020-00377-w		MAR 2020											
J								Characterization of Graph-Based Hierarchical Watersheds: Theory and Algorithms	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Hierarchical watersheds on edge-weighted graphs; Recognition of hierarchical watersheds; Binary partition hierarchy	SEGMENTATION	Watershed is a well-established clustering and segmentation method. In this article, we aim to achieve a better theoretical understanding of the hierarchical version of the watershed operator. More precisely, we propose a characterization of hierarchical watersheds in the framework of edge-weighted graphs. The proposed characterization leads to an efficient algorithm to recognize hierarchical watersheds.																	0924-9907	1573-7683				JUN	2020	62	5			SI		627	658		10.1007/s10851-019-00936-6		MAR 2020											
J								Predicting ground vibration induced by rock blasting using a novel hybrid of neural network and itemset mining	NEURAL COMPUTING & APPLICATIONS										Blasting; Ground vibration; Neural network; Itemset mining	PEAK PARTICLE-VELOCITY; OPEN-PIT MINE; MODEL	Blasting operation is considered as one of the cheapest methods to break the rock into small pieces in surface and underground mines. Ground vibration is a side effect of blasting and can result in damage to, or failure of, nearby structures. Therefore, it is imperative to predict ground vibration in the blasting sites. The primary objective of this paper is to propose a new model to predict ground vibration based on itemset mining (IM) and neural networks (NN), called IM-NN. It is worth mentioning that no research has tested the efficiency of IM-NN to predict ground vibration yet. IM-NN is composed of three steps; firstly, frequent and confident patterns (itemsets) were extracted by using IM. Secondly, for each test instance, the most appropriate instances were selected based on the extracted patterns. Thirdly, NN was only trained by the selected instances. To achieve the objective of this research, a dataset including 92 instances was collected from blasting events of two surface mines in Iran, Kerman province. To demonstrate the acceptability of IM-NN, the classical NN as well as several empirical equations were also developed in this study. The results indicated that IM-NN with the correlation squared (R-2) of 0.944 has better performance than NN withR(2)of 0.898 and may be a promising alternative to the NN for predicting ground vibration. Thus, the use of IM was a good idea to optimize and improve the NN performance.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14681	14699		10.1007/s00521-020-04822-w		MAR 2020											
J								Mobility aware autonomic approach for the migration of application modules in fog computing environment	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Fog computing; Genetic algorithm; Autonomic computing; MAPE loop; Container migration	SERVICE; EDGE; OPTIMIZATION	The fog computing paradigm has emanated as a widespread computing technology to support the execution of the internet of things applications. The paradigm introduces a distributed, hierarchical layer of nodes collaboratively working together as the Fog layer. User devices connected to Fog nodes are often non-stationary. The location-aware attribute of Fog computing, deems it necessary to provide uninterrupted services to the users, irrespective of their locations. Migration of user application modules among the Fog nodes is an efficient solution to tackle this issue. In this paper, an autonomic framework MAMF, is proposed to perform migrations of containers running user modules, while satisfying the Quality of Service requirements. The hybrid framework employing MAPE loop concepts and Genetic Algorithm, addresses the migration of containers in the Fog environment, while ensuring application delivery deadlines. The approach uses the pre-determined value of user location for the next time instant, to initiate the migration process. The framework was modelled and evaluated in iFogSim toolkit. The re-allocation problem was also mathematically modelled as an Integer Linear Programming problem. Experimental results indicate that the approach offers an improvement in terms of network usage, execution cost and request execution delay, over the existing approaches.																	1868-5137	1868-5145															10.1007/s12652-020-01854-x		MAR 2020											
J								Working together: a DBN approach for individual and group activity recognition	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Group activity recognition; Deep belief networks; Datasets	DEEP; SENSORS	Human activity recognition is gaining more and more the attention of researchers due to its applicability in many different fields such as health monitoring, smart environments, etc. Activity recognition solutions typically focus on the classification of single-user behavior. However, in a living or working environment, there are usually multiple inhabitants acting together, hence it makes sense to interpret the activities by considering the aggregated information from different subjects. In this paper, we address the problem of group activity recognition (GAR) in a hierarchical way by first examining individual person's actions, reconstructed by correlating data coming from body-worn and external positioning sensors. We then aggregate this information by considering each individual as an input of a hierarchical deep belief network (DBN). This aims to extract common temporal/spatial dynamics at the level of group activity. We evaluated the proposed approach in a laboratory environment, where the participants labeled their daily activities using an app on a mobile phone. Collected data contributed to the creation of two datasets respectively containing labeled single and group activities. The experimental results evaluated on these datasets and on a public one demonstrated the effectiveness of the proposed model with respect to a support vector machine (SVM) baseline.																	1868-5137	1868-5145															10.1007/s12652-020-01851-0		MAR 2020											
J								A deep neural network electrocardiogram analysis framework for left ventricular hypertrophy prediction	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Electrocardiogram; Left ventricular hypertrophy; Deep neural network; Machine learning	PERCEPTRON; DIAGNOSIS; CRITERIA; MODEL	Heart disease is ranked second in the top ten death causes in Taiwan in 2016, while the amount of heart disease fatalities is increasing by about 700 individuals each year. Therefore, it is urgent to improve heart disease diagnostic accuracy. This research combines profound neural networks and information from the ECG to create an ECG classifier for left ventricular hypertrophy. The classification system uses data preprocessing to impute missing values of ECG data, which can be collected from IoT equipment and monitor the body situation quickly, and transform these data for use by the deep neural network. Then, the data is input into a six-layers deep neural network designed for this study and is used to predict left ventricular hypertrophy. L2-regularization and dropout are used to avoid overfitting in each model iteration. The system has been named the electrocardiographic left ventricular hypertrophy classifier (ELVHC). The experimental findings demonstrate that the forecast model's precision is 73%, the sensitivity is 66%, and the specificity is 78%, which is considerably greater than two clinical techniques.																	1868-5137	1868-5145															10.1007/s12652-020-01826-1		MAR 2020											
J								Barriers to implementation of blockchain into supply chain management using an integrated multi-criteria decision-making method: a numerical example	SOFT COMPUTING										Blockchain; Supply chain management; Integrated MCDM; Fuzzy AHP; Fuzzy TOPSIS	FUZZY AHP; TECHNOLOGY; CHALLENGES; FRAMEWORK; SYSTEMS; BITCOIN	Information and product visibilities have been crucial in today's supply chain processes, as economic, environmental and social sustainability concepts, which have been frequently focused on in recent years, prioritize the transparency of business processes. Blockchain technology, with continuously expanding application areas, has been revealed to be applicable in supply chain processes. On the other hand, integration of blockchain technology into supply chain processes will not be as smooth as estimated, since some challenges and constraints have already been identified. Therefore, companies aiming to integrate blockchain into supply chain processes should qualify and investigate each of these challenges. In the literature, to the best of the authors' knowledge, there is no study addressing the issues arising during the integration process. Moreover, most studies related to blockchain technology have merely examined the positive aspects. This study, on the contrary, discusses the technologic, financial, organizational and environmental challenges that are confronted on a sectoral basis during the integration process. The fuzzy AHP and fuzzy TOPSIS methods, which are used in uncertainties and are capable of simultaneous multi-criteria evaluation, were employed. Furthermore, it is intended to produce a study that is of benefit to industrial actors by analyzing the findings related to the challenges merging during technological transformation. The outputs of this study are as follows: (i) High investment costs, data security and utility are important; (ii) integration is harder for the health and logistic sectors; (iii) the supply chains, which are less complicated, will be able to coordinate faster than the blockchain technology does. Consequently, the obtained results are evaluated, and strategic outputs are shared for the decision-makers aiming to integrate blockchain technology into the supply chain processes.																	1432-7643	1433-7479				OCT	2020	24	19					14771	14789		10.1007/s00500-020-04831-w		MAR 2020											
J								A novel quasi-reflected Harris hawks optimization algorithm for global optimization problems	SOFT COMPUTING										Harris hawks optimization; Quasi-reflection-based learning; Opposition-based learning; Benchmark functions; Swarm-based intelligent algorithms	SEARCH ALGORITHM; MODEL	Harris hawks optimization (HHO) is a recently developed meta-heuristic optimization algorithm based on hunting behavior of Harris hawks. Similar to other meta-heuristic algorithms, HHO tends to be trapped in low diversity, local optima and unbalanced exploitation ability. In order to improve the performance of HHO, a novel quasi-reflected Harris hawks algorithm (QRHHO) is proposed, which combines HHO algorithm and quasi-reflection-based learning mechanism (QRBL) together. The improvement includes two parts: the QRBL mechanism is introduced firstly to increase the population diversity in the initial stage, and then, QRBL is added in each population position update to improve the convergence rate. The proposed method will also be helpful to control the balance between exploration and exploitation. The performance of QRHHO has been tested on twenty-three benchmark functions of various types and dimensions. Through comparison with the basic HHO, HHO combined with opposition-based learning mechanism and HHO combined with quasi-opposition-based learning mechanism, the results demonstrate that QRHHO can effectively improve the convergence speed and solution accuracy of the basic HHO and two variants of HHO. At the same time, QRHHO is also better than other swarm-based intelligent algorithms.																	1432-7643	1433-7479				OCT	2020	24	19					14825	14843		10.1007/s00500-020-04834-7		MAR 2020											
J								Completion of multiview missing data based on multi-manifold regularised non-negative matrix factorisation	ARTIFICIAL INTELLIGENCE REVIEW										Multi-source data analysis; Multiview clustering; Missing data completion; Multi-manifold regularised; Non-negative matrix factorisation		In multi-source data analysis, the absence of data values or attributes is inevitably brought about by various influencing factors including environment, which results in the loss of knowledge to be conveyed by data. To solve the problem of missing data in multi-source data analysis, completion method for multiview missing data based on multi-manifold regularized non-negative matrix factorization was proposed in this paper. This method was based on the assumption of consistency of the multiview data and an algorithm of multi-manifold regularized non-negative matrix factorization is adopted to obtain homogeneous manifold and global clustering. On this basis, a multiview synergistic discrimination model is built of the non-missing view that referred to the Gaussian mixture model to pre-mark the clustering that the incremental missing data belonged to. Using the consistency of each view in the low-dimensional space, a prediction model of missing data at the specified view is established using the multiple linear regression technique to achieve accurate data completion under conditions of missing multi-attributes. Through the establishment of data filling model with three handling methods for missing values, namely CMMD-MNMF, FIMUS and Hot deck, the completion performance, clustering performance and classification performance of data sets including UCI, Flower17 and Flower102 are analyzed by simulation experiments. As shown in the results, the method of multi-view data missing completion is verified to be effective.																	0269-2821	1573-7462				OCT	2020	53	7					5411	5428		10.1007/s10462-020-09824-7		MAR 2020											
J								Decision making with probabilistic hesitant fuzzy information based on multiplicative consistency	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										decision making; distance measure; probabilistic hesitant fuzzy set; probabilistic linguistic term set	PREFERENCE RELATIONS; ADDITIVE-CONSISTENCY	The probabilistic hesitant fuzzy preference relations (PHFPRs) provide the decision makers with an efficient means to express the preference information on pairwise comparisons over alternatives. In this paper, we propose an automatic consistency improving model for PHFPRs. First, we propose a novel normalization algorithm to normalize probabilistic hesitant fuzzy elements (PHFEs) by using probability splitting idea and develop novel operational laws. Then, we define the consistency index to compute the degree of deviation between the PHFPRs and their multiplicative consistent PHFPRs. We also develop a novel consistency threshold estimation method for obtaining the threshold of consistency index and then put forward an automatic consistency improving algorithm for repairing inconsistent PHFPRs. Moreover, two probabilistic hesitant fuzzy aggregation operators are put forward to aggregate preference values in acceptably multiplicative consistent PHFPRs for obtaining the ranking orders of alternatives. Finally, an illustrative example is given to show the implementation process of our proposed automatic consistency improving model and also we compare our proposed model with the existing studies.																	0884-8173	1098-111X				AUG	2020	35	8					1233	1261		10.1002/int.22240		MAR 2020											
J								Lossless data transmission for Internet of things application over wireless multimedia sensor networks using enhanced and optimal path scheduling approach to maximizing the quality of service	COMPUTATIONAL INTELLIGENCE										congestions; efficient data transmission; enhanced and optimal path scheduling approach (EOPSA) multimedia sensor network; multimedia data; optimal path scheduling		The congestion of packet forwarding between a source and destination is challenging on downlink transmission in the entire file (ex. Audio and Video). Whenever file is been uploaded to the server, a user requests for file where server transmits it without knowledge of user's bandwidth, which is a major, cause of packet loss or time duration in the receiver end. To accumulate the better solution, Enhanced and Optimal Path Scheduling Approach (EOPSA) designs to find optimal path scheduling for multimedia data transmission in multimedia sensor network over cloud server using IoT devices. EOPSA studied the multisource video-on-demand streaming in multimedia sensor networks. The method introduced a heuristic distributed protocol to find optimal route for multimedia data transmissions. Efficient way to identify the bandwidth before the transmission ensures link establishment between sender and receiver. Here, the capture of bandwidth helps to check user's system capability to forward requested media data. Based on experiment evaluation, EOPSA improves 0.20 packet delivery ratio, 130 throughput, 0.20 second average delay and 14 communication overhead for 15, 25, 50, 75, and 100 nodes compared than conventional methods.																	0824-7935	1467-8640															10.1111/coin.12305		MAR 2020											
J								Agent Based Modelling and Simulation to estimate movement time of pilgrims from one place to another at Allahabad Jn. Railway Station during Kumbh Mela-2019	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Allahabad junction railway station; Kumbh Mela festival; Crowd management; Pilgrims boarding procedure; Agent Based Modelling and Simulation; AnyLogic	SOCIAL FORCE MODEL; FLOW; STAIRWAY	Kumbh Mela festival of India is one of the largest mass gathering event of huge religious importance all over the world. Large gatherings in these kind of religious events require rigorous monitoring and attention. Successful organization of such events requires synchronization among officials of different public departments such as police, health, security, communication, railways etc. The railway department plays a significant role in handling huge surge of passengers and their transportation during such events. Every 12-years Kumbh Mela is organized in the city of Prayagraj (formerly Allahabad) in northern India. The Allahabad Jn. railway station experiences huge inflow and outflow of pilgrims during Kumbh Mela. The railway authorities deploy predefined crowd movement strategies and boarding procedures for smooth transportation of pilgrims. However, these strategies are outlined based on previous experiences and ground knowledge of stakeholders. The strategies followed by railway authorities are needed to be evaluated and tested for realistic assessment and possible refinement before actual deployment. Our model is able to capture and simulate the real time behaviour of entities such as pilgrims and trains by programming them as synthetic agents. This model is helpful in analyzing the time taken by a group of pilgrims to move from a designated place to their target platform and board the train. The consumed time is calculated by simulating different movement and boarding procedures including the actual plans followed by the railway authorities. In this way it is possible to assess the efficiency of their movement plans and reasons about possible refinement.																	1387-2532	1573-7454				MAR 9	2020	34	1							30	10.1007/s10458-020-09454-x													
J								Deep learning-based classification model for botnet attack detection	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Security; Botnet; Feed-forward; Artificial neural network; Backpropagation; Deep learning		Botnets are vectors through which hackers can seize control of multiple systems and conduct malicious activities. Researchers have proposed multiple solutions to detect and identify botnets in real time. However, these proposed solutions have difficulties in keeping pace with the rapid evolution of botnets. This paper proposes a model for detecting botnets using deep learning to identify zero-day botnet attacks in real time. The proposed model is trained and evaluated on a CTU-13 dataset with multiple neural network designs and hidden layers. Results demonstrate that the deep-learning artificial neural network model can accurately and efficiently identify botnets.																	1868-5137	1868-5145															10.1007/s12652-020-01848-9		MAR 2020											
J								Neutrosophic fuzzy set and its application in decision making	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Neutrosophic fuzzy set; Single valued neutrosophic fuzzy set; Distance measure; Similarity measure	MULTIMOORA; OPERATORS; MODEL	Fuzzy set (FS) is used to tackle the uncertainty using the membership grade, whereas neutrosophic set (NS) is used to tackle uncertainty using the truth, indeterminacy and falsity membership grades which are considered as independent. In this paper, we introduce the notion of neutrosophic fuzzy set (NFS) by combining FS with NS, which gives rise to some new concepts. Since NFS finds some difficulties to deal with some real life problems due to the nonstandard interval of neutrosophic components, we introduce single valued NFS (SVNFS) which is considered as an instance of NFS. Some set theoretic operations of SVNFS are proposed and their properties are derived. We also propose the distance measures between two SVNFSs. Then a decision making approach is presented using similarity measures based on distance measures. Finally, we emonstrate the proposed approach using a numerical example.																	1868-5137	1868-5145															10.1007/s12652-020-01808-3		MAR 2020											
J								Multilevel Tetrolet transform based breast cancer classifier and diagnosis system for healthcare applications	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Median filtering; Multilevel Tetrolet transformation based features; UTVO	COMPUTER-AIDED DETECTION; FEATURE-EXTRACTION; MAMMOGRAMS; MASSES	Breast cancer is a deadly disease and nowadays affects middle-aged women. Doctors use various methods to find out the presence of micro calcifications. In general, images of digital mammogram are used for cancer diagnosis. But the precision that determines the presence of cancer requires multiple restrictions. An improved medical decision support system for the classification of mammogram images using Unsupervised Test Vector Optimization (UTVO) based classifier is developed for health care and is presented in this work. The median filtering in the preprocessing stage removes the speckle, salt and pepper noises for mammogram image enhancement and the multilevel Tetrolet transformation based Matrix vectors are obtained for feature extraction. Multilevel Tetrolet transformation based features (MTF) are used for the selection of texture features. The UTVO classification approach proposed for identifying breast cancer exploits the potential and analyzes its performance after classification of benign and malignant cases. Finally, the UTVO classification system's memory and computational time has been used to weigh the optimal weight and used to analyze the performance without compromise. The proposed system is expected to provide the best support for doctors to achieve maximum accuracy and improvement in health care.																	1868-5137	1868-5145															10.1007/s12652-020-01755-z		MAR 2020											
J								Examination of electrodermal and cardio-vascular reactivity in virtual reality through a combined stress induction protocol	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Virtual reality; Social stress; Anxiety disorders	EXPOSURE THERAPY; ANXIETY; TECHNOLOGY; PREDICTOR; RESPONSES; MODEL; PTSD; FEAR	In this study, task-related stress induction through Stroop task and social stress induction protocol based on elements of Trier Social Stress Test are examined. The aim of the paper is to find the optimal combination of social and task-related stress to be used to consistently and reliably induce a stressful reaction. In total 16 healthy subjects participated in this study that seeks to find and compare the different stressors and their relation to physiological reactivity. Our findings show that electrodermal activity measurements are suitable when using a combination of stressors while heart rate and Root Mean Square of the Successive Differences highlight a greater reactivity to task-stress.																	1868-5137	1868-5145															10.1007/s12652-020-01858-7		MAR 2020											
J								Aiming AI at a moving target: health (or disease)	AI & SOCIETY										Artificial intelligence; Data; Meaning; Deep medicine; Anticipation	EMPATHY	Justified by spectacular achievements facilitated through applied deep learning methodology (based on neural networks), the "Everything is possible" view dominates this new hour in the "boom and bust" curve of AI performance. The optimistic view collides head on with the "It is not possible"-ascertainments often originating in a skewed understanding of both AI and medicine. The meaning of the conflicting views can be assessed only by addressing the nature of medicine. Specifically: Which part of medicine, if any, can and should be entrusted to AI-now or at some moment in the future? AI or not, medicine should incorporate the anticipation perspective in providing care.																	0951-5666	1435-5655															10.1007/s00146-020-00943-x		MAR 2020											
J								Solving two-dimensional cutting stock problem via a DNA computing algorithm	NATURAL COMPUTING										Two-dimensional cutting stock problem; Garment and leather industries; DNA computing; Sticker model	STICKER-BASED MODEL	Two-dimensional cutting stock problem (TDCSP) is a well-known combinatorial optimization problem in which a given set of two-dimensional small pieces with different shapes should be cut from a given main board so that the demand of each small piece is satisfied and the total waste is minimized. Since TDCSP is an NP-complete problem, it is unsolvable in polynomial time on electronic computers. However, using the structure of DNA molecules, DNA computing algorithms are capable to solve NP-complete problems in polynomial time. In this paper, a DNA computing algorithm based on the sticker model is presented to find the optimal solution to TDCSP. It is proved that the time complexity of this algorithm on DNA computers is polynomial considering the number of small pieces and the length and width of the main board.																	1567-7818	1572-9796															10.1007/s11047-020-09786-3		MAR 2020											
J								Assessment of influence productivity in cognitive models	ARTIFICIAL INTELLIGENCE REVIEW										Influence productivity; Directed weighted signed graphs; Cognitive model; Optimization methods; Complex systems	MAP-BASED SCENARIOS; MANAGEMENT; SUCCESS	This article proposes a new influence productivity assessment methodology that is a cognitive intelligence system for the scenario planning of control impacts (generation and choice) for systems that are represented by directed weighted signed graphs based on the algorithm of effective controls. The algorithm implements a control model that expresses the direction of development (growth) of the system. The algorithm is based on the spectral properties of the adjacency matrix of a graph representing the model of a socioeconomic system and does not impose any constraints on the directions of the edges or the sign and weight range on the edges. Scenarios are assessed based on their compliance with tactical and strategic goals according to the codirectionality degree of the response vector with respect to the base vector of the model. The base vector is the effective control vector without constraints on the controls under the conditions of adequate model operation. The new methodology has three distinctive features: (1) the scenario approach is implemented with respect to a set of controls, (2) this approach is applicable for models with heterogeneous factors and does not require preliminary aggregation of the primary model elements of the system; and (3) this approach has a clear formalization metric for the selecting and generating of a set of control impacts. The process does not require the decision maker to have special mathematical training.																	0269-2821	1573-7462				OCT	2020	53	7					5383	5409		10.1007/s10462-020-09823-8		MAR 2020											
J								Detection and classification of mechanical faults of three phase induction motor via pixels analysis of thermal image and adaptive neuro-fuzzy inference system	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Induction motor diagnosis; Eccentricity fault; Thermography; Intelligent fault diagnosis; Adaptive neuro-fuzzy inference system (ANFIS) classifier; Thermal pixels counting (TPC) algorithm	INTER-TURN FAULT; AIRGAP ECCENTRICITY; DIAGNOSIS; MODEL	The fault diagnosis in motor is substantial as it results in breakdown of production line and the faults may damage motor and results in economic losses. Bearing failure, rotor eccentricity, shaft misalignment and load related faults are the most frequent failures under mechanical fault category. This paper addresses three such faults that may increase the stator temperature namely air gap eccentricity, shaft misalignment and cooling system failure. The thermography technique has been used widely for fault detection in induction motor. In three phase induction motor the thermal images are analyzed for healthy condition and the above mentioned faults conditions. This paper presents thermal pixels counting algorithm to calculate the diagnosis indicators and adaptive neuro-fuzzy inference system classifier is used to classify the faults based on the diagnosis indicator data base. Laboratory based experimental investigation are carried out to verify the accuracy of the proposed method. This method provides accurate diagnosis indicator that will be used as a bench mark value for preparing the maintenance schedule under non-destructive mode.																	1868-5137	1868-5145															10.1007/s12652-020-01857-8		MAR 2020											
J								Thumbs up, thumbs down: non-verbal human-robot interaction through real-time EMG classification via inductive and supervised transductive transfer learning	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Gesture classification; Human-robot interaction; Electromyography; Machine learning; Transfer learning; Inductive transfer learning; Supervised transductive transfer Learning; Myo armband; Pepper robot	RECOGNITION; FORESTS	In this study, we present a transfer learning method for gesture classification via an inductive and supervised transductive approach with an electromyographic dataset gathered via the Myo armband. A ternary gesture classification problem is presented by states of'thumbs up','thumbs down', and'relax'in order to communicate in the affirmative or negative in a non-verbal fashion to a machine. Of the nine statistical learning paradigms benchmarked over 10-fold cross validation (with three methods of feature selection), an ensemble of Random Forest and Support Vector Machine through voting achieves the best score of 91.74% with a rule-based feature selection method. When new subjects are considered, this machine learning approach fails to generalise new data, and thus the processes of Inductive and Supervised Transductive Transfer Learning are introduced with a short calibration exercise (15 s). Failure of generalisation shows that 5 s of data per-class is the strongest for classification (versus one through seven seconds) with only an accuracy of 55%, but when a short 5 s per class calibration task is introduced via the suggested transfer method, a Random Forest can then classify unseen data from the calibrated subject at an accuracy of around 97%, outperforming the 83% accuracy boasted by the proprietary Myo system. Finally, a preliminary application is presented through social interaction with a humanoid Pepper robot, where the use of our approach and a most-common-class metaclassifier achieves 100% accuracy for all trials of a '20 Questions' game.																	1868-5137	1868-5145															10.1007/s12652-020-01852-z		MAR 2020											
J								Crosstalk minimization in network on chip (NoC) links with dual binary weighted code CODEC	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cross talk; Dual binary weighted code; Links; Network-on-chip; Encoder; Decoder	AVOIDANCE CODEC	A number of bus encoding techniques are renowned in low power dissipation of network-on-chip. The objective of the proposed algebraic framework dual binary weighted code (DBWC) technique is to prevent the triplet opposite direction transition wholly in the network on chip links and reduces the troublesome cross talk delay. The additional aim is to utilize only six links for the entire five bit input range instead of seven links which have been used by other existing methods. The results of implementation show that the proposed method DBWC encoder reduces 9.492% of cell leakage power than the IMP FIBO encoder and 60.03% less than S2AP encoder. Likewise, the decoder cuts 18.43% of cell leakage power than the IMP FIBO decoder and 5.06% fewer than S2AP decoder.																	1868-5137	1868-5145															10.1007/s12652-020-01842-1		MAR 2020											
J								AI for dynamic packet size optimization of batteryless IoT nodes: a case study for wireless body area sensor networks	NEURAL COMPUTING & APPLICATIONS										Dynamic packet size optimization; Wireless body area network; Batteryless; AI in WBASN	ENERGY EFFICIENCY	Packet size optimization, with the purpose of minimizing the wireless packet transmission energy consumption, is crucial for the energy efficiency of the Internet of Things nodes. Meanwhile, energy scavenging from ambient energy sources has gained a significant attraction to avoid battery issues as the number of nodes increasingly grows. Packet size optimization algorithms have so far been proposed for battery-powered networks that have limited total energy with continuous power availability to prolong their lifetime. On the other hand, batteryless networks based on energy harvesting offer unlimited total energy with the interruption in availability. This is due to changing ambient conditions or the required time for harvesting and storing in small capacitors. Packet size optimization of batteryless networks has not been addressed so far. In this paper, an AI-based packet size optimization algorithm is proposed for batteryless networks that consider the amount of harvested energy at each node. Therefore, packet size is optimized dynamically for each round of data transmission. The proposed method is then evaluated via numerical simulations for a heterogenous wireless body area sensor network as a case study, considering 1-hop, cooperative, and 2-hop communication networks. Cooperative topology yields optimum energy efficiency for highly dynamic sensors, such as ECG, while 2-hop has shown to be optimum for the same type of sensors in battery-powered networks. Also, for sensors with slower dynamics such as body temperature, 1-hop turns out to be optimum in networks solely dependent on energy scavenging while cooperative topology is optimum for battery-powered networks. The algorithm applies to any heterogeneous fully batteryless networks to dynamically optimize packet size at each transmission instance.																	0941-0643	1433-3058				OCT	2020	32	20			SI		16167	16178		10.1007/s00521-020-04813-x		MAR 2020											
J								Stability analysis of dynamic nonlinear interval type-2 TSK fuzzy control systems based on describing function	SOFT COMPUTING										Interval type-2 Takagi-Sugeno-Kang fuzzy control systems; Stability; Limit cycle; Gain margin; Phase margin; Describing function	OPTIMIZATION; STABILIZATION; UNCERTAINTY; PREDICTION; SETS	This paper focuses on the limit cycles prediction problem to discuss the stability analysis of dynamic nonlinear interval type-2 Takagi-Sugeno-Kang fuzzy control systems (NIT2 TSK FCSs) with adjustable parameters. First, in order to alleviate computational burden, a simple architecture of NIT2 TSK FCS using two embedded nonlinear type-1 TSK fuzzy control systems (NT1 TSK FCSs) is proposed. Then, describing function (DF) of NIT2 TSK FCS is obtained based on the DFs of embedded NT1 TSK FCSs. Subsequently, integrating the stability equation and parameter plane approaches provides a solution to identify the limit cycle and the asymptotically stable regions. Moreover, particle swarm optimization technique is applied to minimize the limit cycle region. Furthermore, for robust design, a gain-phase margin tester is utilized to specify the minimum gain margin (GM(min)) and phase margin (PMmin) when limit cycles can arise. Finally, two simulation examples are considered to validate the advantages of the presented method.																	1432-7643	1433-7479				OCT	2020	24	19					14623	14636		10.1007/s00500-020-04811-0		MAR 2020											
J								Task Location for High Performance Human-Robot Collaboration	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Manipulability index; Closed kinematic chain; Genetic algorithm; Human-robot cooperation; Ergonomics; Admittance controller	STROKE-DIRECTION PREFERENCES; ARM MOVEMENTS; MANIPULATABILITY; COORDINATION; TIME	In this paper, an approach for the evaluation of human-robot collaboration towards high performance is introduced and implemented. The human arm and the manipulator are modelled as a closed kinematic chain and the proposed task performance criterion is used based on the manipulability index of this chain. The selected task is a straight motion in which the robot end-effector is guided by the human operator via an admittance controller. The best location of the selected task is determined by the maximization of the minimal manipulability along the path. Evaluation criteria for the performance are adopted considering the ergonomics literature. In the experimental set-up with a KUKA LWR manipulator, multiple subjects repeat the specified motion to evaluate the introduced approach experimentally.																	0921-0296	1573-0409				OCT	2020	100	1					183	202		10.1007/s10846-020-01181-5		MAR 2020											
J								Multimodal feature fusion for CNN-based gait recognition: an empirical comparison	NEURAL COMPUTING & APPLICATIONS										Gait signature; Convolutional neural networks; Multimodal fusion	MOTION; MULTITASK	People identification in video based on the way they walk (i.e., gait) is a relevant task in computer vision using a noninvasive approach. Standard and current approaches typically derive gait signatures from sequences of binary energy maps of subjects extracted from images, but this process introduces a large amount of non-stationary noise, thus conditioning their efficacy. In contrast, in this paper we focus on the raw pixels, or simple functions derived from them, letting advanced learning techniques to extract relevant features. Therefore, we present a comparative study of different convolutional neural network (CNN) architectures by using three different modalities (i.e., gray pixels, optical flow channels and depth maps) on two widely adopted and challenging datasets: TUM-GAID and CASIA-B. In addition, we perform a comparative study between different early and late fusion methods used to combine the information obtained from each kind of modalities. Our experimental results suggest that (1) the raw pixel values represent a competitive input modality, compared to the traditional state-of-the-art silhouette-based features (e.g., GEI), since equivalent or better results are obtained; (2) the fusion of the raw pixel information with information from optical flow and depth maps allows to obtain state-of-the-art results on the gait recognition task with an image resolution several times smaller than the previously reported results; and (3) the selection and the design of the CNN architecture are critical points that can make a difference between state-of-the-art results or poor ones.																	0941-0643	1433-3058				SEP	2020	32	17					14173	14193		10.1007/s00521-020-04811-z		MAR 2020											
J								Supervised information granulation strategy for attribute reduction	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Attribute reduction; Granular computing; Information granulation; Neighborhood relation; Rough set; Supervised learning	FEATURE-SELECTION; ROUGH SETS; DECISION; MODEL; FUSION; ACQUISITION	In rough set based Granular Computing, neighborhood relation has been widely accepted as one of the most popular approaches for realizing information granulation. Such approach is to group samples in terms of their similarities without the consideration of their labels. Therefore, it can be referred to as an unsupervised information granulation strategy. Nevertheless, it is obvious that such unsupervised mechanism may generate imprecise neighborhoods by comparing the actual labels of samples. It follows that it is not good enough for classification-oriented attribute reduction to select qualified attributes. To fill such a gap, a novel supervised information granulation strategy is proposed. Different from the unsupervised information granulation, samples are grouped by using not only the similarities over conditional attributes but also the labels. For such a purpose, our mechanism mainly contains two aspects: (1) intra-class radius, which aims to add samples with the same label into neighborhood; (2) extra-class radius, which aims to delete samples with different labels from the neighborhood. The experimental results over 12 UCI data sets demonstrate that, compared with previous researches, the reducts derived by our supervised information granulation may contribute to superior classification performances. This study suggests new trends and applications of considering information granulation from the viewpoint of supervised learning.																	1868-8071	1868-808X				SEP	2020	11	9					2149	2163		10.1007/s13042-020-01107-5		MAR 2020											
J								What is a Simulation Model?	MINDS AND MACHINES										Computer simulations; Simulation models; Computer-based explanation; Recasting; Representation; Novelty of computer simulations	SEMANTIC INTERPRETATION; COMPUTER-SIMULATIONS; IMPLEMENTATION; ABSTRACTION; EXPLANATION; PHILOSOPHY	Many philosophical accounts of scientific models fail to distinguish between a simulation model and other forms of models. This failure is unfortunate because there are important differences pertaining to their methodology and epistemology that favor their philosophical understanding. The core claim presented here is that simulation models are rich and complex units of analysis in their own right, that they depart from known forms of scientific models in significant ways, and that a proper understanding of the type of model simulations are fundamental for their philosophical assessment. I argue that simulation models can be distinguished from other forms of models by the many algorithmic structures, representation relations, and new semantic connections involved in their architecture. In this article, I reconstruct a general architecture for a simulation model, one that faithfully captures the complexities involved in most scientific research with computer simulations. Furthermore, I submit that a new methodology capable of conforming such architecture into a fully functional, computationally tractable computer simulation must be in place. I discuss this methodology-what I call recasting-and argue for its philosophical novelty. If these efforts are heading towards the right interpretation of simulation models, then one can show that computer simulations shed new light on the philosophy of science. To illustrate the potential of my interpretation of simulation models, I briefly discuss simulation-based explanations as a novel approach to questions about scientific explanation.																	0924-6495	1572-8641															10.1007/s11023-020-09520-z		MAR 2020											
J								Prediction of fundraising outcomes for crowdfunding projects based on deep learning: a multimodel comparative study	SOFT COMPUTING										Crowdfunding; Deep learning; Machine learning; Fundraising prediction; Multilayer perceptron	SOCIAL INNOVATION; SUSTAINABILITY; ALGORITHMS; INTERNET; MACHINE; SUCCESS; FOREST	As a new financing model, crowdfunding has been developed rapidly in recent years and has attracted the attention of investors and small- and medium-sized enterprises and entrepreneurs. However, many projects fail to be funded; thus, crowdfunding project fundraising outcomes forecasting and multimodel comparisons are meaningful ways to identify project quality and reduce market risk. It is important to reduce participation risk through automated methods, which is of great significance to the sustainable development of Internet finance. First, based on the data from the Kickstarter, preprocessing and exploratory analysis are conducted. Then, we introduce a deep learning algorithm (multilayer perceptron) and apply it to the prediction of crowdfunding financing performance. We compare deep learning with other commonly used machine learning algorithms, including decision tree, random forest, logistic regression, support vector machine, and K-nearest neighbors algorithm. We tune each machine learning algorithm to get the best parameters. The experimental results show that the deep learning model can obtain the best prediction results, with an accuracy of 92.3% when predicting the fundraising outcomes of crowdfunding financing, followed by the decision tree. Deep learning shows significant advantages in many evaluation criteria, which demonstrates the potential for crowdfunding project financing predictions. This study combines machine learning with Internet finance, providing inspiration for future research and resulting in many practical implications.																	1432-7643	1433-7479				JUN	2020	24	11			SI		8323	8341		10.1007/s00500-020-04822-x		MAR 2020											
J								A hybrid Persian sentiment analysis framework: Integrating dependency grammar based rules and deep neural networks	NEUROCOMPUTING										Persian sentiment analysis; Low-Resource natural language processing; Dependency-based rules; Deep learning	MACHINE	Social media hold valuable, vast and unstructured information on public opinion that can be utilized to improve products and services. The automatic analysis of such data, however, requires a deep understanding of natural language. Current sentiment analysis approaches are mainly based on word co-occurrence frequencies, which are inadequate in most practical cases. In this work, we propose a novel hybrid framework for concept-level sentiment analysis in Persian language, that integrates linguistic rules and deep learning to optimize polarity detection. When a pattern is triggered, the framework allows sentiments to flow from words to concepts based on symbolic dependency relations. When no pattern is triggered, the framework switches to its subsymbolic counterpart and leverages deep neural networks (DNN) to perform the classification. The proposed framework outperforms state-of-the-art approaches (including support vector machine, and logistic regression) and DNN classifiers (long short-term memory, and Convolutional Neural Networks) with a margin of 10-15% and 3-4% respectively, using benchmark Persian product and hotel reviews corpora. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 7	2020	380						1	10		10.1016/j.neucom.2019.10.009													
J								Delay tolerant containment control for second-order multi-agent systems based on communication topology design	NEUROCOMPUTING										Delay tolerant; Containment control; Multi-agent system; Time-varying delay; Topology design	SWITCHING TOPOLOGY; CONSENSUS	In this paper, a delay tolerant containment control problem for second-order multi-agent systems (MASs) is studied. Considering the disturbance caused by the time-varying delays between agents, we utilize the delay tolerant character of MASs with multiple leaders to redesign the topology of concerned MASs and solve the containment control problem. Corresponding sufficient and necessary conditions on the communication digraph of the MAS are developed to ensure the followers converging to interior points of a convex hull formed by the leaders. Furthermore, considering the inevitable delays between the communications of agents, we present the topology design algorithm in light of the proposed conditions to solve the delay tolerant containment control problem for the second-order MAS. In this case, we focus on how to redesign the communication topology structure instead of constructing a proper distributed cooperative controller. Finally, some examples are given to illustrate the effectiveness of the obtained results. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 7	2020	380						11	19		10.1016/j.neucom.2019.10.001													
J								Dealing with categorical and integer-valued variables in Bayesian Optimization with Gaussian processes	NEUROCOMPUTING										Parameter tuning; Bayesian optimization; Gaussian processes; Integer-valued variables; Categorical variables		Some optimization problems are characterized by an objective that is very expensive, that lacks an analytical expression, and whose evaluations can be contaminated by noise. Bayesian Optimization (BO) methods can be used to solve these problems efficiently. BO relies on a probabilistic model of the objective, which is typically a Gaussian process (GP). This model is used to compute an acquisition function that estimates the expected utility (for solving the optimization problem) of evaluating the objective at each potential new point. A problem with GPs is, however, that they assume real-valued input variables and cannot easily deal with categorical or integer-valued values. Common methods to account for these variables, before evaluating the objective, include assuming they are real and then using a one-hot encoding, for categorical variables, or rounding to the closest integer, for integer-valued variables. We show that this leads to suboptimal results and introduce a novel approach to tackle categorical or integer-valued input variables within the context of BO with GPs. Several synthetic and real-world experiments support our hypotheses and show that our approach outperforms the results of standard BO using GPs on problems with categorical or integer-valued input variables. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 7	2020	380						20	35		10.1016/j.neucom.2019.11.004													
J								Implementation of circuit for reconfigurable memristive chaotic neural network and its application in associative memory	NEUROCOMPUTING										Memristor; Associative memory; Reconfigurable; Chaotic neural network	MODEL	Chaotic neural networks is widely used in associative memory because of its abundant chaotic behavior. The bridge synaptic circuit of the memristor has been mostly used in artificial neural networks, because of its synapse-like and non-volatile properties, but the weight addition circuit has a complicated structure, the high power consumption and the high complexity of the network, so the associative memory neural network circuit is still less implemented. In this paper, the memory characteristics of the threshold memristor is used to build the synaptic circuit, on the one hand, when the continuous voltage is applied to the memristor to alter its memristance, it can realize continuous synaptic weights from - 1 to 1. Synaptic weight circuit has simple structure and low energy consumption, due to the configurability of the threshold memristor, and different weights can be obtained in the same circuits to achieve the function of associative memory. On the other hand, we can realize self-associative memory, hetero-associative memory, the separation of superimposed patterns, many-to-many associative memory and application in the three-view drawing, through simulation experiments. Because of the nanoscale characteristics of memristor, the hardware implementation of large-scale chaotic neural network will has simplified structure and be integrated easily. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				MAR 7	2020	380						36	42		10.1016/j.neucom.2019.10.100													
J								Containment control with multiple leaders for nonlinear multi-agent systems with unstabilizable linearizations	NEUROCOMPUTING										Containment control; Nonlinear systems; Directed topology	OUTPUT-FEEDBACK STABILIZATION; COOPERATIVE CONTROL; TRACKING; DESIGN	In this paper, we study the distributed containment control problem with multiple leaders for nonlinear multi-agent systems (MASs) with unstabilizable linearizations under the directed topology. A novel distributed control design approach is developed to simultaneously deal with the unstabilizable linearizations and nonlinear interactions among agents. The designed controller can guarantee that the followers' outputs will eventually converge to a limit point in the convex hull spanned by the leaders' outputs while all the states of the closed-loop system remain bounded. Finally, a mechanical system example is presented to illustrate the effectiveness of the proposed control scheme. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 7	2020	380						43	50		10.1016/j.neucom.2019.10.088													
J								Bayesian approach and time series dimensionality reduction to LSTM-based model-building for fault diagnosis of a reciprocating compressor	NEUROCOMPUTING										Deep learning; LSTM; Bayesian optimization; Time-series dimensionality reduction; Reciprocating compressor	ROTATING MACHINERY; VALVES; CLASSIFICATION; EXTRACTION	Reciprocating compression machinery is the primary source of compressed air in the industry. Undiagnosed faults in the machinery's components produce a high rate of unplanned stoppage of production processes that can even result in catastrophic consequences. Fault diagnosis in reciprocating compressors requires complex and time-consuming feature-extraction processes because typical fault diagnosers cannot deal directly with raw signals. In this paper, we streamline the deep learning and optimization algorithms for effective fault diagnosis on these machines. The proposed approach iteratively trains a group of long short-term memory (LSTM) models from a time-series representation of the vibration signals collected from a compressor. The hyperparameter search is guided by a Bayesian approach bounding the search space in each iteration. Our approach is applied to diagnose failures in intake/discharge valves on double-stage machinery. The fault-recognition accuracy of the best model reaches 93% after statistical selection between a group of candidate models. Additionally, a comparison with classical approaches, state-of-the-art deep learning-based fault-diagnosis approaches, and the LSTM-based model shows a remarkable improvement in performance by using the proposed approach. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				MAR 7	2020	380						51	66		10.1016/j.neucom.2019.11.006													
J								Training multi-layer spiking neural networks using NormAD based spatio-temporal error backpropagation	NEUROCOMPUTING										Supervised learning; Spiking neuron; Normalized approximate descent; Leaky integrate-and-fire; Multi-layer SNN; Spatio-temporal error backpropagation; NormAD; XOR problem	LEARNING ALGORITHM; GRADIENT DESCENT; CLASSIFICATION; NEURONS; RESUME	Spiking neural networks (SNNs) have garnered a great amount of interest for supervised and unsupervised learning applications. This paper deals with the problem of training multi-layer feedforward SNNs. The non-linear integrate-and-fire dynamics employed by spiking neurons make it difficult to train SNNs to generate desired spike trains in response to a given input. To tackle this, first the problem of training a multi-layer SNN is formulated as an optimization problem such that its objective function is based on the deviation in membrane potential rather than the spike arrival instants. Then, an optimization method named Normalized Approximate Descent (NormAD), hand-crafted for such non-convex optimization problems, is employed to derive the iterative synaptic weight update rule. Next, it is reformulated to efficiently train multi-layer SNNs, and is shown to be effectively performing spatio-temporal error backpropagation. The learning rule is validated by training 2-layer SNNs to solve a spike based formulation of the XOR problem as well as training 3-layer SNNs for generic spike based training problems. Thus, the new algorithm is a key step towards building deep spiking neural networks capable of efficient event-triggered learning. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 7	2020	380						67	77		10.1016/j.neucom.2019.10.104													
J								Quaternion broad learning system: A novel multi-dimensional filter for estimation and elimination tremor in teleoperation	NEUROCOMPUTING										Quaternion broad learning system(QBLS); Teleoperation; Physiological tremor	PHYSIOLOGICAL TREMOR; BAND IDENTIFICATION; ADAPTIVE ESTIMATION; KALMAN FILTER; COMPENSATION; EEG	In this paper, a novel quaternion broad learning system is proposed in this paper for tremor estimation and elimination in teleoperation. In the new proposed QBLS, the architecture can be divided into three layers, including quaternion feature layer, enhancement layer and the output layer. In quaternion feature layer, a quaternion-value auto-encoder (QAE) based on the quaternion algebra is proposed and employed to extract the unsupervised features in quaternion domain. Moreover, the enhancement nodes are mapped to improve the system's regression ability in enhancement layer. In the output layer, the nodes of feature layer and enhancement layer are concatenated to map the output of QBLS. The weight parameters of output layer can be calculated by the minimum norm least squares solutions. In addition, the semi-physical simulation experiment is completed and the new proposed QBLS has been compared with some existing methods. Finally, the effectiveness and efficiency of QBLS are demonstrated by experimental results. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 7	2020	380						78	86		10.1016/j.neucom.2019.10.059													
J								Adaptive neural fault-tolerant control for a class of strict-feedback nonlinear systems with actuator and sensor faults	NEUROCOMPUTING										Adaptive neural control; Fault-tolerant control; Actuator faults; Sensor faults	TRACKING CONTROL	y This paper investigates the adaptive neural fault-tolerant control problem for a class of strict-feedback nonlinear systems with simultaneous actuator and sensor faults. The faults considered in this paper are bias (lock-in-place), drift, loss of accuracy, and loss of effectiveness faults. Only one parameter law is updated at each step to reduce the computational burden. By utilizing the adaptive neural network backstepping control strategy, the closed-loop nonlinear system is guaranteed to be semi-globally uniform ultimate bounded, and all the signals are bounded. Finally, a simulation example is given to show the effectiveness of the proposed control strategy. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 7	2020	380						87	94		10.1016/j.neucom.2019.09.053													
J								Frame-GAN: Increasing the frame rate of gait videos with generative adversarial networks	NEUROCOMPUTING										Gait recognition; Generative adversarial networks; Metric learning; Deep learning	RECOGNITION; IMAGE	Most existing methods of identifying person except gait recognition require the cooperation of the subjects. Aiming at detecting the pattern of human walking movement, gait recognition takes advantage of the time-serial data and can identify a person distantly. The time-serial data, which is usually presented in video form, always has a limitation in frame rate, which intrinsically affects the performance of the recognition models. In order to increase the frame rate of gait videos, we propose a new kind of generative adversarial networks (GAN) named Frame-GAN to reduce the gap between adjacent frames. Inspired by the recent advances in metric learning, we also propose a new effective loss function named Margin Ratio Loss (MRL) to boost the recognition model. We evaluate the proposed method on the challenging CASIA-B and OU-ISIR gait databases. Extensive experimental results show that the proposed Frame-GAN and MRL are effective. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 7	2020	380						95	104		10.1016/j.neucom.2019.11.015													
J								Multi-view clustering by joint manifold learning and tensor nuclear norm	NEUROCOMPUTING										Multi-view clustering; Local manifold learning; Spectral clustering		In real-world applications, the variation between multi-view data points, which should belong to the same cluster, is larger than the variation between data points belonging to different clusters. This results in instability of most existing clustering algorithms, because they mainly employ the original data as input to learn the latent similarity matrix. To address these problems, we propose a novel latent similarity learning for multi-view clustering(LSLMC) by integrating manifold learning and tensor Singular Value Decomposition (t-SVD) into a uniform framework. LSLMC utilizes the similarity matrices calculated from each view to recover a latent representation matrix by local manifold learning coupled with spectral clustering. Thus, the adaptively recovered similarity matrix, which is shared by all views, can well characterize both the clustering structure and local manifold structure underlying multi-view data. In addition, by simultaneously using low-rank tensor constraint on the error matrix, the view-specific information and the noise can be well explored. Therefore, the recovered similarity matrix become more robust and accurate, leading to better clustering performance. Extensive experiments on five widely used multi-view datasets have demonstrated the superiority of the proposed method. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 7	2020	380						105	114		10.1016/j.neucom.2019.11.014													
J								Discriminative deep metric learning for asymmetric discrete hashing	NEUROCOMPUTING										Discrete binary code; Metric learning; Supervised hashing; Asymmetric hashing	BINARY-CODES; IMAGE; RETRIEVAL; SEARCH	Thanks to its storage and computation efficiency, hashing as a kind of nearest neighbor search method can facilitate massive data processing in recent vision and learning studies. Particularly, deep supervised hashing methods have significantly improved the retrieval performance compared with non-deep supervised hashing methods. However, most existing deep supervised hashing methods approximate the similarity between two images with the hamming distance between the outputs of the same hash function, i.e., the symmetric strategy. Consequently, it is typically time-consuming to train these symmetric hashing methods, and these methods can hardly take full advantage of the supervised information in the large-scale database. In this paper, we propose a novel discriminative deep metric learning approach for asymmetric discrete hashing (ADMH) approach for supervised hashing learning. ADMH integrates an asymmetric strategy with a deep metric learning method to learn the hash function for the query images and the discrete hash codes for database images directly. More specifically, we train a deep neural network to extract the features of the query images. Subsequently, we introduce a metric learning scheme for learning the discrete discriminative hash codes of the database images directly. Finally, the feature learning procedure for generating the hash codes of the query images and the discrete coding procedure for generating the hash codes of the database images are integrated into an end-to-end learning framework. Extensive experiments on various benchmark datasets show that the proposed asymmetric deep hashing method outperforms the existing hashing methods. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 7	2020	380						115	124		10.1016/j.neucom.2019.11.009													
J								Semantic-aware short path adversarial training for cross-domain semantic segmentation	NEUROCOMPUTING										Convolutional neural network; Generative adversarial network; Semantic segmentation; Unsupervised domain adaptation		Recently, many methods have been proposed to deal with the problem of cross-domain semantic segmentation. Most of them choose to conduct domain adversarial training either on the high-level convolutional neural network (CNN) features or on the output segmentation maps. Typically, a relatively small weight is given to the adversarial training loss to avoid the problem of mode collapse. However, one potential weakness of these methods is that low-level CNN layers may receive little gradients for domain adaptation, especially when the network is deep. To address this problem, we propose to conduct an auxiliary adversarial training on the fused multi-level CNN features. Gradients for domain adaptation can thus flow into low-level CNN layers more easily along a shorter path. Experiments are conducted on the dataset of Cityscapes with using the source datasets of GTA5 and SYNTHIA, respectively. Quantitative and qualitative results certify the efficacy of the proposed method. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 7	2020	380						125	132		10.1016/j.neucom.2019.11.008													
J								Quantized synchronization of memristive neural networks with time-varying delays via super-twisting algorithm	NEUROCOMPUTING										Memristive neural network; Super-twisting algorithm; Quantized control scheme; Synchronization; Time-varying delay	SLIDING MODE CONTROL; EXPONENTIAL SYNCHRONIZATION; STABILITY ANALYSIS; PASSIVITY; STABILIZATION; SYSTEMS; PASSIFICATION	In this paper, we investigate quantized synchronization control problem of memristive neural networks (MNNs) with time-varying delays via super-twisting algorithm. A feedback controller is introduced with quantized method. To enormously reduce the computational complexity of the controller under super-twisting algorithm, two quantized control schemes are proposed with uniform quantizer and logarithmic quantization. We obtain some sufficient conditions of specific control plans to guarantee that the driving MNNs can synchronize with the response MNNs. A neoteric Lyapunov functional is designed to analyze the synchronization problem. Finally, in this paper ending, some illustrative examples are given in support of our results. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 7	2020	380						133	140		10.1016/j.neucom.2019.11.003													
J								Network representation learning with ensemble methods	NEUROCOMPUTING										Network representation learning; Ensemble learning; Network embedding; Social network data mining		This paper investigates network representation learning which involves network structures and labels. Most methods proposed so far try to utilize different kinds of network related data available in just one perfect model to learn the set of perfect embeddings, and then evaluate its performance comparing with other methods for downstream applications, such as node classification or link prediction. In this paper, we introduce ensemble learning to the study of network representation learning. In conventional scenario, ensemble methods train multiple individual learners and combine all their results about the same input to produce an improved one. Inspired by this, we try to train multiple individual models using network related data, and aggregate all the learned embeddings as the final network representations, to expect a performance boost for downstream applications. In order to learn good and diverse individual models, bootstrap sampling is used and different individual model structures are designed. Experimental results show that the idea of aggregating network embeddings really works, and can outperform existing excellent methods under specific experimental setups. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 7	2020	380						141	149		10.1016/j.neucom.2019.10.098													
J								Multi-scale fully convolutional network for gland segmentation using three-class classification	NEUROCOMPUTING										Histological image; Segmentation; Multi-scale; Fully convolutional network; Dilated convolution	IMAGES; COLON	Automated precise segmentation of glands from the histological images plays an important role in glandular morphology analysis, which is a crucial criterion for cancer grading and planning of treatment. However, it is non-trivial due to the diverse shapes of the glands under different histological grades and the presence of tightly connected glands. In this paper, a novel multi-scale fully convolutional network with three class classification (TCC-MSFCN) is proposed to achieve gland segmentation. The multi-scale structure can extract different receptive field features corresponding to multi-size objects. However, the max-pooling in the convolution neural network will cause the loss of global information. To compensate for this loss, a special branch called high-resolution branch in our framework is designed. Besides, for effectively separating the close glands, a three-class classification with additional consideration of edge pixels is applied instead of the conventional binary classification. Finally, the proposed method is evaluated on Warwick-QU dataset and CRAG dataset with three reliable evaluation metrics, which are applied to our method and other popular methods. Experimental results show that the proposed method achieves the-state-of-the-art performance. Discussion and conclusion are presented afterwards. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 7	2020	380						150	161		10.1016/j.neucom.2019.10.097													
J								Cerebrovascular segmentation from TOF-MRA using model- and data-driven method via sparse labels	NEUROCOMPUTING										Cerebrovascular segmentation; Deep-learning; Dilated convolution; Sparse label; Model-driven method; Data-driven method	VESSEL ENHANCEMENT; LEVEL-SETS; 3D; ALGORITHM	Cerebrovascular segmentation from time-of-flight magnetic resonance angiography (TOF-MRA) data is of great importance in blood supply structure analysis, diagnosis, and treatment of cerebrovascular pathologies. However, complete and accurate segmentation is still a challenge due to the complex image context and vascular morphology. The existing model-driven methods are often difficult to obtain prominent accuracy and robustness. Deep-learning based technology has achieved unimaginable success, but always faces the problem of insufficient labeled data. In this paper, a novel strategy is proposed to automate cerebrovascular segmentation, which integrates model- and data-driven methods. Firstly, the TOF-MRA data are sparsely labeled by three radiologists. Secondly, a semi-supervised mixture probability model is proposed to fit the cerebrovascular intensity distribution precisely, which starts from the sparse annotations and generates massive labeled points. Thirdly, mislabeled points are corrected by a Clean-Mechanism model, to acquire a well-labeled point-set of good quality. Finally, we construct and train a dilated dense convolution network (DD-CNN) by the resultant labeled point-set. The proposed method is validated on 109 clinical TOR-MRA data from a public dataset. Compared with the other state-of-the-art segmentation methods, our method segments cerebrovascular structure with better completeness and sensibility, especially for slender vascularity. The experimental results show that our method reaches an average dice score of 93.20%, which also indicates that the DD-CNN is very competent for cerebrovascular segmentation from TOF-MRA volume. (C) 2019 The Authors. Published by Elsevier B.V.																	0925-2312	1872-8286				MAR 7	2020	380						162	179		10.1016/j.neucom.2019.10.092													
J								Feature agglomeration networks for single stage face detection	NEUROCOMPUTING										Feature agglomeration; Context-aware; Hierarchical loss; Single-stage detectors		Recent years have witnessed promising results of exploring deep convolutional neural network for face detection. Despite making remarkable progress, face detection in the wild remains challenging especially when detecting faces at vastly different scales and characteristics. In this paper, we propose a novel simple yet effective framework of "Feature Agglomeration Networks"(FANet) to build a new single-stage face detector, which not only achieves state-of-the-art performance but also runs efficiently. As inspired by Feature Pyramid Networks (FPN) (Lin et al., 2017), the key idea of our framework is to exploit inherent multi-scale features of a single convolutional neural network by aggregating higher-level semantic feature maps of different scales as contextual cues to augment lower-level feature maps via a hierarchical agglomeration manner at marginal extra computation cost. We further propose a Hierarchical Loss to effectively train the FANet model. We evaluate the proposed FANet detector on several public face detection benchmarks, including PASCAL face, FDDB, and WIDER FACE datasets and achieved state-of-the-art results(2). Our detector can run in real-time for VGA-resolution images on GPU. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 7	2020	380						180	189		10.1016/j.neucom.2019.10.087													
J								Optimizing zinc electrowinning processes with current switching via Deep Deterministic Policy Gradient learning	NEUROCOMPUTING										Current switching; Deep Deterministic Policy Gradient; Zinc electrowinning process	POWER-DISPATCHING SYSTEM; MATHEMATICAL-MODEL; NEURAL-NETWORKS; OPTIMIZATION; PRICES; GAME; GO	This paper proposes a model-free Deep Deterministic Policy Gradient (DDPG) learning controller for zinc electrowinning processes (ZEP) to save energy consumption during the current switching periods. To overcome the problems such as inaccurate modeling and various time delays, the proposed DDPG controller utilizes various control periods and parameters for different working conditions. Strategies such as action boundary setting, reward function definition, state normalization are applied to ensure its learning performance. Simulations and experiments show that the DDPG learning controller can significantly decrease energy consumption during the ZEP current switching periods. The optimal control policy will be learnt for different working conditions with only one group hyperparameters. Furthermore, the smoother control actions of the DDPG controller will improve the stability and reduce more energy consumption by comparing with traditional proportional-integral (PI) controller, model predictive control (MPC) and artificial experiences. The artificial intelligence-based optimal control framework brings both energy saving and intelligence to zinc manufacturing plants. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 7	2020	380						190	200		10.1016/j.neucom.2019.11.022													
J								Annealed gradient descent for deep learning	NEUROCOMPUTING										Gradient descent; Deep learning; DNNs; CNNs		In this paper, we propose a novel annealed gradient descent (AGD) algorithm for deep learning. AGD optimizes a sequence of gradually improving smoother mosaic functions that approximate the original non-convex objective function according to an annealing schedule during optimization process. We present a theoretical analysis on AGD's convergence properties and learning speed, as well as use some visualization methods to show its advantages. The proposed AGD algorithm is applied to learn both deep neural networks (DNNs) and Convolutional Neural Networks (CNNs) for variety of tasks includes image recognition and speech recognition. Experimental results on several widely-used databases, such as Switchboard, CIFAR-10 and Pascal VOC 2012, show that AGD yields better classification accuracy than SGD, and obviously accelerates the training speed of DNNs and CNNs. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 7	2020	380						201	211		10.1016/j.neucom.2019.11.021													
J								A two-stage attention aware method for train bearing shed oil inspection based on convolutional neural networks	NEUROCOMPUTING										Attention mechanism; Image segmentation; Object detection; Railway inspection; Transfer learning	FAULT-DETECTION; DIAGNOSIS	As an important component of trains, rolling bearing is always faced with the defection of shed oil, which inevitably threatens the train safety. Therefore, it is of great significance to conduct defection inspection on bearing shed oil. Due to the complex structure of rolling bearings, traditional signal analysis approaches cannot detect the defections of bearing shed oil with high-efficiency and low cost. In recent years, deep learning has achieved remarkable growth and been successfully applied to various computer-vision tasks. Motivated by this fact, we propose a two-stage attention aware method to recognize defections of bearing shed oil. The proposed method is based on convolutional neural networks, can automatically learn bearing defect features, and does not need manual feature design and extraction like traditional methods. The two-stage method cascades a bearing localization stage and a defection segmentation stage, to recognize the defect areas in a coarse-to-fine manner. The localization stage extracts the foremost bearing region and removes the useless part of images, so as to focus the attention of segmentation stage only on the target region. In segmentation stage, we propose a novel attention aware network APP-UNet16, to segment defect areas from extracted bearing region. APP-UNet16 stacks attention gates to enable the attention-aware features change adaptively, and thus can learn to focus on target defect areas automatically. We also utilize transfer learning in constructing the encoder of APP-UNet16, and introduce spatial pyramid pooling to connect the encoder and decoder, to improve traditional UNet. A series of comparative experiments are conducted, to compare our two-stage method with one-stage method which directly perform segmentation on original train images. The results indicate that the proposed two-stage inspection method achieves higher robustness and accuracy in recognizing defect areas with small oil spot. And the experimental results on proposed APP-UNet16 also demonstrate that a better segmentation performance is achieved, compared to traditional UNet and related state-of-art approaches. We will release the source code as well as the trained models to facilitate more research work. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 7	2020	380						212	224		10.1016/j.neucom.2019.11.002													
J								A GPSO-optimized convolutional neural networks for EEG-based emotion recognition	NEUROCOMPUTING										Convolutional neural networks (CNNs); Hyper-parameter optimization; Electroencephalography (EEG) analysis; Particle swarm optimization (PSO)	MUSIC; CLASSIFICATION; AUDIO	An urgent problem in the field of deep learning is the optimization of model construction, which frequently hinders its performance and often needs to be designed by experts. Optimizing the hyperparameters remains a substantial obstacle in designing deep learning models, such as CNNs, in practice. In this paper, we propose an automatical optimization framework using binary coding system and GPSO with gradient penalties to select the structure. Such swarm intelligence optimization approaches have been used but not extensively exploited, and the existing work focuses on models with a fixed depth of networks. We design an experiment to arouse three types of emotion states for each subject, and simultaneously collect EEG signals corresponding to each emotion category. The GPSO-based method efficiently explores the solution space, allowing CNNs to obtain competitive classification performance over the dataset. Results indicate that our method based on the GPSO-optimized CNN model enables us to achieve a prominent classification accuracy, and the proposed method provides an effective automatic optimization framework for CNNs of the emotion recognition task with an uncertain depth of networks. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 7	2020	380						225	235		10.1016/j.neucom.2019.10.096													
J								Exploring privileged information from simple actions for complex action recognition	NEUROCOMPUTING										Privileged information; Probability matrix; Simple action; Complex action recognition	MODEL	Complex action recognition is an important yet challenging problem in computer vision. Sufficient labeled training data are required for learning a robust model. However, labeling complex actions is often time-consuming and expensive. Considering that each complex action is composed of a sequence of simple actions, we propose a new learning framework for complex action recognition by using a sequence of existing simple actions. A matrix is first designed as the probability matrix by manual annotation, which encodes the occurrence of simple actions in complex actions. As the probability matrix depends on label information, it is only available for the training data and is regarded as privileged information. A new framework is proposed, which is called latent task learning with privileged information (LTL-PI). An efficient algorithm is also presented for solving the proposed LTL-PI formulation, which obtains the optimal sparse weight parameters. To validate the proposed LTL-PI algorithm, extensive experiments are carried out on two challenging complex action datasets: the Olympic Sports dataset and the UCF50 dataset. Experimental results show that LTL-PI is a promising development that improves the performance of complex action recognition, and the designed privileged information can offer promising enhancement. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 7	2020	380						236	245		10.1016/j.neucom.2019.11.020													
J								Enriching Non-negative Matrix Factorization with Contextual Embeddings for Recommender Systems	NEUROCOMPUTING										Collaborative Filtering; Word2vec; Convolutional Neural Network; Non-negative Matrix Factorization; Natural Language Processing	SPARSITY	Recommender Systems (RS) are used to give customized recommendations about specific items to users in a variety of applications including social web sites applications, media recommendation and commerce sites etc. Collaborative Filtering (CF) along with the Content Based Filtering (CBF) are two widely used methods which are being efficiently applied in RS implementation. CF suffers with sparseness problem where user-to-item ratings are amply sparse. On the other hand, CBF performance rely on methods of feature extraction for efficient use of items' description. The sparseness of user-to-item ratings and features extraction impede the performance of RS. Quality of rating prediction and item recommendation further degrades due to the negative values present in users/items latent factors. This paper proposes a novel RS that is built upon the semantics based items' content embedding model, enriched with contextual features extracted through Convolutional Neural Network (CNN). Non-negative Matrix Factorization (NMF), supplied with improvised embedding is used as CF technique. Embedding model captures the item details, thus resolving the sparsity, whereas, NMF caters for information loss due to negative values in latent factors. The proposed RS with contextually enriched NMF (Contx-NMF) simultaneously overcomes both the issues of sparseness and loss due to negative values, thus enhancing the rating prediction accuracy. The proposed model is evaluated on three public datasets (MovieLens 1M, MovieLens 10M)and Amazon Instant Video (AIV). The results demonstrate significant improvement in performance of Contx-NMF over state of the art RS models for sparse user-to-item ratings. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 7	2020	380						246	258		10.1016/j.neucom.2019.09.080													
J								A visual long-short-term memory based integrated CNN model for fabric defect image classification	NEUROCOMPUTING										Fabric defects; Image classification; Visual system; Visual perception; Visual short-term memory; Visual long-term memory	CONVOLUTIONAL NEURAL-NETWORK; SYSTEM	Fabric defect classification is traditionally achieved by human visual examination, which is inefficient and labor-intensive. Therefore, using intelligent and automated methods to solve this problem has become a hot research topic. With the increasing diversity of fabric defects, it is urgent to design effective methods to classify defects with a higher accuracy, which can contribute to ensuring the fabric products' quality. Considering that the fabric defect is not obvious against the texture background and many kinds of them are too confusing to distinguish, a visual long-short-term memory (VLSTM) based integrated CNN model is proposed in this paper. Inspired by the human visual perception and visual memory mechanism, three categories of features are extracted, which are the visual perception (VP) information extracted by stacked convolutional auto-encoders (SCAE), the visual short-term memory (VSTM) information characterized by a shallow convolutional neural network (CNN), and the visual long-term memory (VLTM) information characterized by non-local neural networks. Experimental results on three fabric defect datasets have shown that the proposed model provides competitive results to the current state-of-the-art methods on fabric defect classification. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 7	2020	380						259	270		10.1016/j.neucom.2019.10.067													
J								A cross-domain hierarchical recurrent model for personalized session-based recommendations	NEUROCOMPUTING										Cross-domain recommendations; Session-based recommendations; Recurrent neural networks; Personalization		Recently, much attention has been paid to personalized session-based recommendations, where detailed user information is available due to users' automatic or active login. Nevertheless, most methods focus on a single-domain scenario, assuming that users are only active in a single domain. Consequently, they always suffer from lack of data due to ignoring the fact that users' behaviors are scattered across domains. Therefore, we propose a novel model, called Cross-Domain Hierarchical Recurrent Model (CDHRM), to incorporate cross-domain sequential information by exploring correlations among users' cross-domain behaviors. Specifically, we devise a cross-domain user-level recurrent neural network (RNN) to systematically depict users' global interests by capturing the cross-domain inter-session dynamics. To separately capture intra-session dynamics of different domains, two domain-specific session-level RNNs, which can preserve behavioral differences, are constructed. Meanwhile, for achieving synchrony of interactions among domains, the user-level RNN exchanges information with different session-level RNNs in a chronological order. Moreover, fusion layers with different integration strategies are introduced to further capture behavioral differences. Finally, cross-domain user-level and session-level information are jointly exploited to predict users' future behaviors. Empirical results show CDHRM outperforms the state-of-the-art methods on three cross-domain datasets and can work well even with non-overlapping and sparse item information across domains. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 7	2020	380						271	284		10.1016/j.neucom.2019.11.013													
J								A discretely adaptive connection logic network	NEUROCOMPUTING										Neural network; Logic gate; Adaptive network; Discrete weights; Threshold functions; Multi-valued logic	NEURAL-NETWORKS; DELAYS	This paper proposes a new model for a theoretical neural network that can be used as a guide for the design of future (quantum or optical) computational technologies. The model utilizes a uniformly connected nodal structure, where the connections are discretely adaptive and the nodes act as simple gatekeepers. The model replicates all known logics used in current electronics, such as AND, OR, XNOR, XOR, NOR, XNOR, NAND and NOT. Additionally, by using recurrent negating connections the model easily creates XOR gates, and adds novel sided and favoured gates. This model also facilitates the creation of ternary to n-ary gates, and it simplifies the creation of a number of majority functions (especially for an odd number of inputs). Also, as a multi-layered neural network, the model allows learning back propagation through the use of its negating connections. Finally, because of its adaptive connections, parts of the network can be used as internal memory. Overall, the model provides backward compatibility to existing CMOS circuitry, while opening up a number of new logics and architectures for neural computing. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				MAR 7	2020	380						285	305		10.1016/j.neucom.2019.10.099													
J								Intelligent online catastrophe assessment and preventive control via a stacked denoising autoencoder	NEUROCOMPUTING										Catastrophe assessment; Preventive control; Power system; Transient stability assessment; Stacked denoising autoencoders; Cost sensitivity	DYNAMIC SECURITY ASSESSMENT; FUZZY DECISION TREES; FAULT-DIAGNOSIS; NEURAL-NETWORK; POWER-SYSTEMS; TRANSIENT; PREDICTION; SENSITIVITY; ALGORITHM	y In all areas of engineering, catastrophe assessment is an essential prerequisite for remedial action schemes. Modelers constantly push for more accurate models, and often meet goals by using increasingly complex, data mining-based blackbox models. However, system operators tend to favor interpretable models for after-the-fact preventive control (PC). While switching from blackbox to interpretable solutions, a tradeoffoccurs between accuracy and interpretability. To avoid this tradeoff, we develop an intelligent framework for online catastrophe assessment and PC via a blackbox stacked denoising autoencoder (SDAE) equipped with accuracy and the ability to derive a PC scheme. Specifically, we implement a transient stability cost-sensitive assessment (TSCA) and PC case in the context of a power grid. First, using only controllable variables, we build the TSCA model by adding a sigmoid unit on top of the SDAE. Considering power systems' conservatism, we explore a novel TSCA model's training criterion to determine the operation conditions' degrees of stability and divide them into three classes: stable, unstable, and boundary. Second, given an operation condition identified as unstable or boundary by TSCA model and its desired degree of stability, the PC model (the reverse of a TSCA model's mapping) consists of the top sigmoid's backward mapping and the stack of denoising decoders from trained SDAE. The former is formalized as an optimization problem to push back the desired degree of stability to a desired SDAE's highest-level abstraction. The latter decodes back the desired SDAE's highest-level abstraction to a desired operation condition (essentially, a PC scheme nearest to the controlled operation condition in the coordinates along the underlying causes that generate the observed data). This approach actually resembles operators' tendency to adjust and stabilize unstable conditions (in terms of underlying causes) with the fewest control actions. A simulation study on the IEEE New England 39-bus system shows that, as a blackbox technology, our framework not only provides superior online situational awareness, but also finds a viable PC scheme, thereby justifying its practicability in engineering. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 7	2020	380						306	320		10.1016/j.neucom.2019.10.090													
J								Bi-perspective Fisher discrimination for single depth map upsampling: A self-learning classification-based approach	NEUROCOMPUTING										Single depth map upsampling; Multi-stage classification-based self-learning; Fisher discrimination; Trilateral decomposition; Weighted low-rank representation; Bi-perspective modeling	IMAGE SUPERRESOLUTION; RESOLUTION	Mostly and differently, the recovery of high-resolution (HR) depth map has been demonstrated under the guidance of its corresponding color image. In this paper, without color guidance, we propose a single depth map upsampling algorithm. This algorithm adopts a new Bi-perspective discriminative self-learning approach which turns the HR depth recovery process into a multi-stage classification-based problem. It employs Fisher discriminant criterion over different splitted subspaces and sub-subspaces through a new trilateral decomposition process. This new trilateral decomposition approach utilizes joint weighted low-rank and sparse priors which ensure global and local consistency, respectively. In addition, with the proposed Bi-perspective and multi-stage discriminant classification, the HR construction process is converted from single-map into a 3D collaborative multi-map reconstruction process. Moreover, for more accurate discriminative classification-based behavior, the shared common bases or features between subspaces, that don't contribute basically in the classification, are addressed by a specific shared property learning to keep suitable overlapping consistency between different subspaces. Accordingly, the proposed depth upsampling algorithm shows superior accuracy among most of the state-of-the-art algorithms. The performance is tested by a set of depth maps from different depth sensing systems and with different degradation styles. The proposed algorithm achieves the first rank with a robust performance against TOF degradations. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAR 7	2020	380						321	340		10.1016/j.neucom.2019.08.074													
J								SOAR Improved Artificial Neural Network for Multistep Decision-making Tasks	COGNITIVE COMPUTATION										Cognitive planning; SOAR; Deep neural network; Decision-making; Robot grasping task	TREE; MANIPULATION; ARCHITECTURE	Recently, artificial neural networks (ANNs) have been applied to various robot-related research areas due to their powerful spatial feature abstraction and temporal information prediction abilities. Decision-making has also played a fundamental role in the research area of robotics. How to improve ANNs with the characteristics of decision-making is a challenging research issue. ANNs are connectionist models, which means they are naturally weak in long-term planning, logical reasoning, and multistep decision-making. Considering that a small refinement of the inner network structures of ANNs will usually lead to exponentially growing data costs, an additional planning module seems necessary for the further improvement of ANNs, especially for small data learning. In this paper, we propose a state operator and result (SOAR) improved ANN (SANN) model, which takes advantage of both the long-term cognitive planning ability of SOAR and the powerful feature detection ability of ANNs. It mimics the cognitive mechanism of the human brain to improve the traditional ANN with an additional logical planning module. In addition, a data fusion module is constructed to combine the probability vector obtained by SOAR planning and the original data feature array. A data fusion module is constructed to convert the information from the logical sequences in SOAR to the probabilistic vector in ANNs. The proposed architecture is validated in two types of robot multistep decision-making experiments for a grasping task: a multiblock simulated experiment and a multicup experiment in a real scenario. The experimental results show the efficiency and high accuracy of our proposed architecture. The integration of SOAR and ANN is a good compromise between logical planning with small data and probabilistic classification with big data. It also has strong potential for more complicated tasks that require robust classification, long-term planning, and fast learning. Some potential applications include recognition of grasping order in multiobject environment and cooperative grasping of multiagents.																	1866-9956	1866-9964															10.1007/s12559-020-09716-6		MAR 2020											
J								Machine learning based metaheuristic hybrids for S-box optimization	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Optimization; Machine learning; Leaders and Followers; Hybridization; S-box		Recent research has consistently shown that the concurrence between exploration and exploitation can significantly limit the effectiveness of exploration on heuristic search. This has led to the design of hybrid algorithms that separate both task and alleviate this limitation. Many of these hybrids are based on the Leaders and Followers metaheuristic, which is specifically designed to avoid this concurrence and achieve an unbiased exploration. In this paper we adapt Leaders and Followers to a combinatorial domain in order to optimize the non-linearity and transparency order of S-boxes. Hybrid algorithms are then presented using Hill-climbing to perform exploitation. Using machine learning techniques these hybrids are further improved by automatically identifying the optimum transition point between exploration and exploitation. The solutions found are among the best S-boxes reported in literature.																	1868-5137	1868-5145															10.1007/s12652-020-01829-y		MAR 2020											
J								Quality of service (Qos) and priority aware models for adaptive efficient image retrieval in WSN using TBL routing with RLBP features	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Image mining; Taxonomy; TBL routing; RLBP; WSN; QoS		The problem of image retrieval in wireless sensor networks has been well studied. Towards image retrieval in WSN, various methods have been discussed earlier, but suffer to achieve higher performance in image mining. To improve the performance, an efficient QoS adaptive image mining technique has been presented in this paper. The method focused on the efficiency in image mining as well as achieving QoS in WSN. The image has been processed to remove the noise by applying Gabor filters. From the noise removed image, the local binary pattern has been generated at each region of the image to produce regional local binary pattern (RLBP). The RLBP feature extracted has been used to measure the similarity between various images. The method maintains in the taxonomy of various image classes, and each class has different features. The input query has been measured for its similarity towards various classes from taxonomy. According to the similarity a single class has been identified. Based on the class identified, a subset of nodes from WSN has been identified where the relevant Images are available. To reach the data nodes the method identifies the list of routes and estimates traffic bandwidth latency (TBL) support. Based on the value of TBL support a specific route has been selected to perform image retrieval. The RLBP feature generated has been transferred to the data nodes, where the method estimates RLBPS (RLBP similarity). According to the value of RLBP similarity, subsets of images have been selected and transmitted the source node. The method improves the performance of image mining in WSN with less complexity.																	1868-5137	1868-5145															10.1007/s12652-020-01793-7		MAR 2020											
J								Performance improvement of elliptic curve cryptography system using low power, high speed 16 x 16 Vedic multiplier based on reversible logic	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Reversible logic gate; Vedic multiplier; Urdhva Tiryagbhyam; Low power; DSP		Multipliers act as processors and take on the notable work of many computing frameworks. The speed of the processor is profoundly affected by the speed of their multipliers. In order to improve the system speed, faster and more efficient multipliers should be used. A Vedic multiplier is one of the best solution that can be used to perform multiplications at a faster rate by eliminating the steps that are not needed in usual multiplication process. Power consumption is another critical issue in embedded systems that cannot be ignored. Reversible logic has become notable in the recent years because of its potential to reduce power utilization, which is a major concern in digital design. In this work, a high-speed 16 x 16 Vedic multiplier was designed using Urdhva Tiryagbhyam (UT) sutra, which is derived from Vedic mathematics. This is a simple structure as well as an unbeatable combination for creating any complex multiplication operations for services where speed is of prime importance. This work also proposes a new method based on Elliptic Curve Cryptography (ECC) system for encryption and decryption using Vedic multiplication. By using Vedic Multiplication in ECC the processing time is perfectly reduced. The proposed Elliptic curve cryptography method is much faster than other elliptic curve cryptographic algorithms. Compared to other cryptographic techniques, the key size required to provide equivalent security is small in ECC.																	1868-5137	1868-5145															10.1007/s12652-020-01795-5		MAR 2020											
J								A novel feature selection method based on comparison of correlations for human activity recognition problems	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Feature selection; Activity recognition; Wearable sensors; Machine learning	FUSION	In human activity recognition studies it is important to identify an optimal set with the minimum number of features that will potentially improve the recognition rate. In the current paper we introduce a promising feature selection method that exploits the differences on the correlation structure of the features, between the different classes of the target variable. Using the recordings of triaxial accelerometers and gyroscopes, we extracted several features and created subsets according to the activities performed. For each subset, we calculated the pairwise correlation coefficients of the features and compared the feature correlations of different subsets. By identifying the significantly different correlations we ranked the variables participating in those correlations based on their frequency of appearance and thus created a subset of features that will optimize the performance of a classification algorithm. The method allows the researcher to select the desired number of features to be included in the classification. Two publicly available datasets were used to evaluate the performance of the proposed methodology in binary and multiclass classification problems. The evaluation revealed quite promising results of the methodology that was compared to the performance of the whole feature set and of a feature selection method that has been extensively used in activity recognition studies.																	1868-5137	1868-5145															10.1007/s12652-020-01836-z		MAR 2020											
J								Formalization of Euler-Lagrange Equation Set Based on Variational Calculus in HOL Light	JOURNAL OF AUTOMATED REASONING										Euler-Lagrange equation set; Functional variations; Dynamics analysis; Theorem proving	THEOREM	As the theoretical foundation of Lagrangian mechanics, Euler-Lagrange equation sets are widely applied in building mathematical models of physical systems, especially in solving dynamics problems. However, their preconditions are often not fully satisfied in practice. Therefore, it is necessary to verify their applications. The purpose of the present work is to conduct such verification by establishing a formal theorem library of Lagrangian mechanics in HOL Light. For this purpose, some basic concepts such as functional variation and the necessary conditions for functional extreme are formalized. Then, the fundamental lemma of variational calculus is formally verified and some new constuctors and destructors are proposed. Finally, the Euler-Lagrange equation set is formalized. To validate the formalization, the formalization results are applied to verify the least resistance problem of gas flow. The present work not only lays a necessary and solid foundation for application involving Lagrangian mechanics but also extends the HOL Light theorem library.																	0168-7433	1573-0670															10.1007/s10817-020-09549-w		MAR 2020											
J								Robust control based on adaptive neural network for Rotary inverted pendulum with oscillation compensation	NEURAL COMPUTING & APPLICATIONS										Adaptive neural network; Oscillation compensation; Rotary inverted pendulum; Furuta	TRAJECTORY TRACKING; STABILIZATION	A new stable adaptive neural network (ANN) control scheme for the Furuta pendulum, as a two-degree-of-freedom underactuated nonlinear system, is proposed in this paper. This approach aims to address the control problem of the Furuta pendulum in the steady state and also in the presence of external disturbances. The adaptive classical control laws such as e-modification present some limitations in particular when oscillations are presented in the input. To avoid this problem, two ANNs are implemented using filtered tracking error in the control loop. The first one is a single hidden layer network, used to approximate the equivalent control online, and the second is the feed-forward network, used to minimize the oscillations. The goal of the control is to bring the pendulum close to the upright position in the presence of the various uncertainties and being able to compensate oscillations and external disturbances. The main purpose of the second ANN is to minimize the chattering phenomenon and response time by finding the optimal control input signal, which also leads to the reduction of energy consumption. The learning algorithms of the two ANNs are obtained using the direct Lyapunov stability method. The simulation results are given to highlight the performances of the proposed control scheme.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14667	14679		10.1007/s00521-020-04821-x		MAR 2020											
J								Multi-warehouse partial backlogging inventory system with inflation for non-instantaneous deteriorating multi-item under imprecise environment	SOFT COMPUTING										Multi-warehouse system; Multi-item; Non-instantaneous deterioration; Inflation; Partial backlogging; Fuzzy rough variables	STOCK-DEPENDENT DEMAND; ORDER QUANTITY MODEL; PERMISSIBLE DELAY; LINEAR TREND; TRANSPORTATION PROBLEM; REPLENISHMENT POLICY; IMPERFECT QUALITY; EXPECTED VALUE; SELLING-PRICE; TRADE CREDIT	In this study, we explored a multi-item inventory model for non-instantaneous deteriorating items under inflation in fuzzy rough environment with multiple warehouse facilities, where one is an owned warehouse and others are rented warehouses with limited storage capacity. Due to a number of uncertainties in the environment, the various expenditures and coefficients are considered as a fuzzy rough type. The objective and constraints in fuzzy rough are made deterministic using Tr-Pos chance constrained technique. The demand of items is considered as stock dependent, and deterioration of items is assumed to be constant over time. The model allows shortages in owned warehouse subject to partial backlogging. The purpose of this study is to find the retailer's optimal replenishment policies to maximize the total profit. To illustrate the proposed model and also test the validity of the same, a numerical example is solved using the Mathematica-8.0 software. Sensitivity analysis is also performed to study the impact of important parameters on system decision variables, and its implications are discussed.																	1432-7643	1433-7479				OCT	2020	24	19					14471	14490		10.1007/s00500-020-04800-3		MAR 2020											
J								Huffman quantization approach for optimized EEG signal compression with transformation technique	SOFT COMPUTING										Discrete cosine transform; EEG signal; Data compression; Huffman quantization	ALGORITHMS; SECURE	The significance of the electroencephalography (EEG) signal is used to read the brain activity in the form of electrical patterns. EEG signals help to diagnose anomalies in the brain at the time of head injuries, epilepsy, seizures, brain tumor, dizziness and sleep deprivation. So such types of crucial signals should be transported in a secure method to avoid any data loss or to prevent noise interruptions which can lead to the misdetection of diseases. As the EEG signals are in higher-dimensional size, it should be compressed for effective transportation. In this research, a lossless compression method named as Huffman-based discrete cosine transform is implemented to transmit the EEG data efficiently. The discrete cosine transform and inverse discrete cosine transform are proposed here to increase the privacy of the data and reduce the complexity of the data. This paper mainly focuses on to get a high accuracy ratio in reconstructing the original data after compression and transportation without any losses in minimum computational time. The preprocessing and sampling are made at the initial stages to remove the noises and transmit the original data. The Huffman quantization method based on discrete cosine transform achieves high-performance metrics in terms of peak signal-to-noise ratio, quality score and compression ratio when compared with existing methods in various transformations of data.																	1432-7643	1433-7479				OCT	2020	24	19					14545	14559		10.1007/s00500-020-04804-z		MAR 2020											
J								SETJoin: a novel top-ksimilarity join algorithm	SOFT COMPUTING										Set similarity join; Query processing; Candidate filtering	SIMILARITY JOINS; FRAMEWORK	As an important operation in data cleaning, near duplicate Web pages detection and data mining, similarity joins have received much attention recently. Existing similarity joins fall into two broad categories-thesimilarity-threshold-based similarity joinandtop-ksimilarity join(TopkJoin). Compared with the traditional one,TopkJoinis more suitable for cases where the similarity threshold is unknown before hand. In this paper, we focus on the performance optimization problem ofTopkJoin. Particularly, we observed that the state-of-the-artTopkJoinalgorithm has three serious performance issues, i.e., the inappropriate application of hash table, inefficient use of suffix filtering and unnecessary evaluation of excessive unqualified candidates. To resolve these problems, we proposed a novel algorithm, SETJoin, by combining the existing event-driven framework with three simple yet efficient optimization techniques, viz., (1) reducing the cost in hashing by rearranging the orders of the candidate filtering and hash table lookup operations; (2) maximizing the pruning capability of suffix filtering by judiciously choosing the (near) optimal recursion depth; and (3) terminating join operations earlier by setting a much tighter stop condition for iteration. The experimental results show that SETJoin achieves up to 1.26x-3.49xspeedup over the state-of-the-art algorithm on several real datasets.																	1432-7643	1433-7479				OCT	2020	24	19					14577	14592		10.1007/s00500-020-04807-w		MAR 2020											
J								Balloon-like coupling between head and posterior in a caterpillar	ADAPTIVE BEHAVIOR										Caterpillar; locomotion; gut		Investigating the viscoelastic mechanical coupling of a gut between the head and posterior is key to understanding the complicated movements of caterpillars. Caterpillar bodies are like a sac filled with fluid. In this article, we propose a locomotion model in which the head and posterior are connected with a spring and damper system instead of a rod; we refer to this as the "balloon model." The numerical experiments reveal the following benefits. Benefit 1: The model can explain a phenomenon that the posterior moves little behind the head. Our model showed such movement causes efficient movement. Benefit 2: The model can explain inching as crawling. The rod model cannot explain inching because the model cannot change the body length. Our balloon model can change the body length drastically and can locomote faster. Benefit 3: The model does not use the stiff organ. Benefit 4: The model can explain the phenomenon called "environmental skeleton," which causes the body to contract during the locomotion. The balloon model can change body length and create power between the head and posterior. This model not only simulates the behavior closer to the actual caterpillar but also can explain the caterpillar with a soft element.																	1059-7123	1741-2633														1059712320908360	10.1177/1059712320908360		MAR 2020											
J								SUM-optimal histograms for approximate query processing	KNOWLEDGE AND INFORMATION SYSTEMS										Approximate query processing; Histogram; Big data	ALGORITHMS	In this paper, we study the problem of the SUM query approximation with histograms. We define a new kind of histogram called the SUM-optimal histogram which can provide better estimation result for the SUM queries than the traditional equi-depth and V-optimal histograms. We propose three methods for the histogram construction. The first one is a dynamic programming method, and the other two are approximate methods. We use a greedy strategy to insert separators into a histogram and use the stochastic gradient descent method to improve the accuracy of separators. The experimental results indicate that our method can provide better estimations for the SUM queries than the equi-depth and V-optimal histograms.																	0219-1377	0219-3116				AUG	2020	62	8					3155	3180		10.1007/s10115-020-01450-7		MAR 2020											
J								An adaptive kernel sparse representation-based classification	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Sparse representation; Trace norm; Sparsity; Correlation; Kernel function	ROBUST FACE RECOGNITION; SIGNAL RECOVERY; METHODOLOGY; ALGORITHMS	In recent years, scholars have attached increasing attention to sparse representation. Based on compressed sensing and machine learning, sparse representation-based classification (SRC) has been extensively in classification. However, SRC is not suitable for samples with non-linear structures which arise in many practical applications. Meanwhile, sparsity is overemphasized by SRC, but the correlation information which is of great importance in classification is overlooked. To address these shortcomings, this study puts forward an adaptive kernel sparse representation-based classification (AKSRC). First, the samples were mapped to a high-dimensional feature space from the original feature space. Second, after selecting a suitable kernel function, a sample is represented as the linear combination of training samples of same class. Further more, the trace norm is adopted in AKSRC which is different from general approaches. It's adaptive to the structure of dictionary which means that a better linear representation which has the most discriminative samples can be obtained. Therefore, AKSRC has more powerful classification ability. Finally, the advancement and effectiveness of the proposed AKSRC are verified by carrying out experiments on benchmark data sets.																	1868-8071	1868-808X				OCT	2020	11	10					2209	2219		10.1007/s13042-020-01110-w		MAR 2020											
J								Gain ratio weighted inverted specific-class distance measure for nominal attributes	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Distance metric learning; Specific-class; Attribute weighting; Gain ratio	RULE MINING CLASSIFIER; NAIVE BAYES; STATISTICAL COMPARISONS; DIFFERENTIAL EVOLUTION; FILTER	Enhancing distance measures is key to improving the performances of many machine learning algorithms, such as instance-based learning algorithms. Although the inverted specific-class distance measure (ISCDM) is among the top performing distance measures addressing nominal attributes with the presence of missing values and non-class attribute noise in the training set, this still requires the attribute independence assumption. It is obvious that the attribute independence assumption required by the ISCDM is rarely true in reality, which harms its performance in applications with complex attribute dependencies. Thus, in this study we propose an improved ISCDM by utilizing attribute weighting to circumvent the attribute independence assumption. In our improved ISCDM, we simply define the weight of each attribute as its gain ratio. Thus, we denote our improved ISCDM as the gain ratio weighted ISCDM (GRWISCDM for short). We tested the GRWISCDM experimentally on 29 University of California at Irvine datasets, and found that it significantly outperforms the original ISCDM and some other state-of-the-art competitors in terms of the negative conditional log likelihood and root relative squared error.																	1868-8071	1868-808X				OCT	2020	11	10					2237	2246		10.1007/s13042-020-01112-8		MAR 2020											
J								Multi-objective unit commitment optimization with ultra-low emissions under stochastic and fuzzy uncertainties	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Multi-objective optimization; Unit commitment; Ultra-low emissions; Particle swarm optimization; Stochastic and fuzzy uncertainties	PORTFOLIO-SELECTION; POWER; DECISION; CONSTRAINTS; RELIABILITY; ALGORITHMS; RULES; TIME; CVAR	Low cost, high reliability and low pollution are prime targets when performing current unit commitment optimization. As an extension of previous works, this study establishes a multi-objective unit commitment model which takes into account all of the above targets. The main content includes: First, the pricing support for thermal units with ultra-low emissions is involved when analyzing the operation cost of generation systems, which accords with the current policy of power markets. Second, a conditional Value-at-Risk-based measurement is formed to estimate system reliability considering the stochastic and fuzzy uncertainties existed in future load, renewable generation and equipment failures, which is sensitive to tail risks and provides easy-to-adjust conservativeness against worst-case scenarios. Third, to deal with the proposed model, a practical approach is applied to develop a multi-objective particle swarm optimization algorithm, which improves the Pareto fronts obtained by existing methods. The effectiveness of this research is exemplified by two case studies, which demonstrate that the model finds appropriate pricing support for the reformed units, and the proposed reliability measurement is able to realize a number of trade-offs between cost effective and solution robustness, thus providing decision support for system operators. Finally, the comparisons on performance metrics such as spacing and hyper-volume also justify the superiority of the algorithm.																	1868-8071	1868-808X															10.1007/s13042-020-01103-9		MAR 2020											
J								Statistical convergence in measure for double sequences of fuzzy-valued functions	SOFT COMPUTING										Double sequence; Measure space; Egorov's theorem; Outer and inner statistical convergence; Fuzzy-valued function	SUMMABLE DOUBLE SEQUENCES; APPROXIMATION; ORDER	The aim of this paper is to define two sorts of convergence in measure, that is, outer and inner statistical convergence, for double sequences of fuzzy-valued measurable functions and demonstrate that both kinds of convergence are equivalent in a finite measurable set. We also define the notion of statistical convergence in measure for double sequences of fuzzy-valued measurable functions and establish several interesting results. In addition, we prove the statistical version of Egorov's theorem for double sequences of fuzzy-valued functions defined on a finite measure space.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6613	6622		10.1007/s00500-020-04805-y		MAR 2020											
J								Improving decision-making efficiency of image game based on deep Q-learning	SOFT COMPUTING										Deep reinforcement learning; Image game; Deep Q-learning; Experience rework; Mapping function	ARTIFICIAL NEURAL-NETWORK; CLASSIFICATION	To promote effective decision-making in video games and win high scores in a short time, the deep learning algorithms are integrated into game image processing for reinforcement learning. By changing the mapping function from priority to probability, a deep Q-learning priority experience replay algorithm is deduced, which is then compared with the single mapping function. Various researches have proved that the improved algorithm can reproduce the mapping function with higher probability of playback learning of the unit. The advantage of the agent is that it can master the most complete game strategy and ultimately obtain higher scores with the help of the strategy. Therefore, the proposed algorithm is to help the agent formulate a more useful strategy when playing video games. On the one hand, the agent can get better game records. On the other hand, the energy consumed in the game is greatly reduced.																	1432-7643	1433-7479				JUN	2020	24	11			SI		8313	8322		10.1007/s00500-020-04820-z		MAR 2020											
J								Choquet integration by Simpson's rule with application in Hellinger distance	SOFT COMPUTING										Non-additive measures; Choquet integral; Composite Simpson's rule		In non-additive measure theory, there are a few studies on the numerical Choquet integral in continuous case on real line. Recently, based on the statistics software R, Torra and Narukawa (Inf Fusion 31:137-145, 2016) considered the problem of computing a numerical Choquet integral and some algorithms for Hellinger distance between two monotone measures. In this paper, the composite Simpson rule for numerical Choquet integration is proposed. Then some algorithms in Mathematica software are given. As well as, CPU time of the Examples in terms of seconds is reported which shows, in computational view, the presented method has a high speed.																	1432-7643	1433-7479				OCT	2020	24	19					14463	14470		10.1007/s00500-020-04798-8		MAR 2020											
J								Efficient solving of strategic bidding issues under no Karush-Kuhn-Tucker optimality constraints	SOFT COMPUTING										Strategic bidding; Karush-Kuhn-Tucker; Group search optimizer; Optimization algorithms	OFFERING STRATEGIES; ELECTRICITY MARKETS; POWER; PRODUCER; POOL; MAXIMIZATION; COMPETITION; EQUILIBRIA; ALGORITHM; MODELS	A typical bidding procedure is to maximize the profit of the strategic producers, in a manner that the offer curve yields solutions to the ISO marketing clearing problem. The literature portrays the procedure in the form of a searching problem with dual levels, wherein the minimization of the market clearing function along with profit maximization is yielded using the offer curve. But, the elevated levels of complexity in computing and solving the bi-level searching problem have forced the researchers to transform it as a one-level maximization problem through employing Karush-Kuhn-Tucker (KKT) optimality conditions. Yet, serious problems emerge with the profit maximization algorithms incorporating KKT optimality conditions (for instance, the classical optimization algorithms). The complexity grows in size, if the transmission constraints get vital in the deregulated environment. Our research contribution devises the profit maximization problem to be a minimization function, lacking consideration on KKT optimality conditions for the ISO market clearing functions, the operating limits and the transmission constraints. A modified form of group search optimizer (GSO), one of the currently popular optimization algorithms, provides solutions to the developed model. The conventional GSO, an optimization algorithm that operates using population, has its motivation from the animals' searching activities. The modified GSO changes the maximum pursuit angle of the animal searching activity relative to the number of iterations. The testing of the proposed method employs both the IEEE 30 bus system as well as the IEEE 14 bus system. Performance comparison among the proposed method, the strategic bidding approaches that rely on the particle swarm optimization, the genetic algorithm and the GSO reveal excessive profit maximization and superiority for the proposed method.																	1432-7643	1433-7479				OCT	2020	24	19					14611	14622		10.1007/s00500-020-04809-8		MAR 2020											
J								Geometric distortion and mixed pixel elimination via TDYWT image enhancement for precise spatial measurement to avoid land survey error modeling	SOFT COMPUTING										Remote sensing; Land cover classification; Geometric distortion; Transverse dyadic wavelet transform	URBAN AREA DETECTION; LIDAR DATA; CLASSIFICATION; ALGORITHM; SEGMENTATION; VEGETATION; SYSTEM	In remote sensing, land cover classification of vegetation and water area from satellite image play a vital role for rural and urban planning and development. Existing algorithms of land cover classification require more sample image datasets for training. For existing algorithms, land cover classification of vegetation and water area is a challenging task because of mixed pixel and geometric distortion over boundary and curvature region. Mixed pixel affects the precise classification and measurement of land cover. Geometric distortion arises due to frame of isotropic and angular selectivity during image acquisition and affects the contour of land cover. In this paper, the proposed transverse dyadic wavelet transform (TDyWT) enhances and classifies vegetation and water area in land cover from LANDSAT image without training datasets. The proposed TDyWT uses Haar wavelet for decomposition and Burt 5 x 7 wavelet for reconstruction. The TDyWT enhances the contour, curvature, and boundary of vegetation and water area in LANDSAT image due to reversible and lifting properties of wavelet. TDyWT removes geometric distortion and spatial scale error of mixed pixel. In traditional land surveying spatial scale error reduction eliminates through total station and error modeling techniques. From the results, the proposed TDyWT algorithm classifies the area of subclass of vegetation and water with the 95% of accuracy with respect to ground truth survey methods.																	1432-7643	1433-7479				OCT	2020	24	19					14687	14705		10.1007/s00500-020-04814-x		MAR 2020											
J								A new feature extraction approach based on one dimensional gray level co-occurrence matrices for bearing fault classification	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Feature extraction; 1d-LBP; 1d-GLCM; fault classification; bearing fault diagnosis	LOCAL BINARY PATTERNS; EPILEPTIC EEG; ALGORITHM; IDENTIFICATION; TRANSFORM; DIAGNOSIS; DEFECTS; SCHEME	Recently, precise and deterministic feature extraction is one of the current research topics for bearing fault diagnosis. For this aim, an experimental bearing test setup was created in this study. In this setup, vibration signals were obtained from the bearings on which artificial faults were generated in specific sizes. A new feature extraction method based on co-occurrence matrices for bearing vibration signals was proposed instead of the conventional feature extraction methods, as in the literature. The One (1) Dimensional-Local Binary Patterns (1D-LBP) method was first applied to bearing vibration signals, and a new signal whose values ranged between 0-255 was obtained. Then, co-occurrence matrices were obtained from these signals. The correlation, energy, homogeneity, and contrast features were extracted from these matrices. Different machine learning methods were employed with these features to carry out the classification process. Three different data sets were used to test the proposed approach. As a result of analysing the signals with the proposed model, the success rate is 87.50% for dataset1 (different speed), 96.5% for dataset2 (fault size (mm)) and 99.30% for dataset3 (fault type - inner ring, outer ring, ball) was found, respectively.																	0952-813X	1362-3079															10.1080/0952813X.2020.1735530		MAR 2020											
J								The Collaboration of Human-Robot in Mixed-Model Four-Sided Assembly Line Balancing Problem	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Four-sided assembly line balancing; Human-robot collaboration; AMOPSO algorithm; Mixed-model assembly line	MATHEMATICAL-MODEL; GENETIC ALGORITHM; WORKERS	In the new era, robots play a significant role in assembly lines to assemble different products. In this paper, a combination of humans and robots is used in the mixed-model assembly lines (MMAL) to get better performance from the assembly line. The four-sided assembly line (4-AL) is also considered; that is, in addition to the usual work done on the left and right side, the assembly is also performed on the above and beneath side in some lines. The above-sided tasks are done only by the robot, and it is decided on the other three sides to be done by a robot or a human. The problem's model has two objectives, minimize the number of mated-station and the cost of utilizing different agents. A small-scale numerical example is solved by the GAMS which indicates the feasibility of the model. An Augmented Multi-Objective particle swarm optimization (AMOPSO) is used to solve the model in large dimensional. AMOPSO has utilized two new methods, local learning strategy and adaptive uniform mutation for the development of the MOPSO algorithm. The Multi-Objective particle swarm optimization (MOPSO) and AMOPSO solutions are compared with each other, and the results show that AMOPSO improves on the responses and has no significant effect on the complexity of solving the problem.																	0921-0296	1573-0409				OCT	2020	100	1					71	81		10.1007/s10846-020-01177-1		MAR 2020											
J								MimicGAN: Robust Projection onto Image Manifolds with Corruption Mimicking	INTERNATIONAL JOURNAL OF COMPUTER VISION										Generative adversarial networks; Robustness; Adversarial defense; Anomaly detection; Domain adaptation		In the past few years, Generative Adversarial Networks (GANs) have dramatically advanced our ability to represent and parameterize high-dimensional, non-linear image manifolds. As a result, they have been widely adopted across a variety of applications, ranging from challenging inverse problems like image completion, to problems such as anomaly detection and adversarial defense. A recurring theme in many of these applications is the notion of projecting an image observation onto the manifold that is inferred by the generator. In this context, Projected Gradient Descent (PGD) has been the most popular approach, which essentially optimizes for a latent vector that minimizes the discrepancy between a generated image and the given observation. However, PGD is a brittle optimization technique that fails to identify the right projection (or latent vector) when the observation is corrupted, or perturbed even by a small amount. Such corruptions are common in the real world, for example images in the wild come with unknown crops, rotations, missing pixels, or other kinds of non-linear distributional shifts which break current encoding methods, rendering downstream applications unusable. To address this, we propose corruption mimicking-a new robust projection technique, that utilizes a surrogate network to approximate the unknown corruption directly at test time, without the need for additional supervision or data augmentation. The proposed method is significantly more robust than PGD and other competing methods under a wide variety of corruptions, thereby enabling a more effective use of GANs in real-world applications. More importantly, we show that our approach produces state-of-the-art performance in several GAN-based applications-anomaly detection, domain adaptation, and adversarial defense, that benefit from an accurate projection.																	0920-5691	1573-1405				NOV	2020	128	10-11			SI		2459	2477		10.1007/s11263-020-01310-5		MAR 2020											
J								High-dimensional microarray dataset classification using an improved adam optimizer (iAdam)	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Optimization; Adam optimization; Learning rate; Microarray; High-dimensional dataset	STABLE FEATURE-SELECTION; SUPPORT VECTOR MACHINE; GRADIENT DESCENT; MOMENTUM	Classifying data samples into their respective categories is a challenging task, especially when the dataset has more features and only a few samples. A robust model is essential for the accurate classification of data samples. The logistic sigmoid model is one of the simplest model for binary classification. Among the various optimization techniques of the sigmoid function, Adam optimization technique iteratively updates network weights based on training data. Traditional Adam optimizer fails to converge model within certain epochs when the initial values for parameters are situated at the gentle region of the error surface. The continuous movement of the convergence curve in the direction of history can overshoot the goal and oscillate back and forth incessantly before converging to the global minima. The traditional Adam optimizer with a higher learning rate collapses after several epochs for the high-dimensional dataset. The proposed Improved Adam (iAdam) technique is a combination of the look-ahead mechanism and adaptive learning rate for each parameter. It improves the momentum of traditional Adam by evaluating the gradient after applying the current velocity. iAdam also acts as the correction factor to the momentum of Adam. Further, it works efficiently for the high-dimensional dataset and converges considerably to the smallest error within the specified epochs even at higher learning rates. The proposed technique is compared with several traditional methods which demonstrates that iAdam is suitable for the classification of high-dimensional data and it also prevents the model from overfitting by effectively handling bias-variance trade-offs.																	1868-5137	1868-5145															10.1007/s12652-020-01832-3		MAR 2020											
J								Multi-hop optimized routing algorithm and load balanced fuzzy clustering in wireless sensor networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Wireless sensor networks; Fuzzy; Routing; Clustering	ENERGY-EFFICIENT; PROTOCOL	Now-a-days, wireless sensor node acts as a basement for a higher level industrial internet of things application. Any kind of real time IoT applications sense the data using already in-built sensors. In real time physical environment, sensors are having minimum amount of power for their operations such as sensing, communicating and processing the data. Many researches are carried out to improve the energy efficiency of sensor nodes and improves their network lifetime. More concentration is needed on clustering and routing part of the communication to go for energy saving. We have proposed an algorithm based on fuzzy and unequal clustering (FBUCA) and a three tier multi-hop optimized routing technique (TM-ORT) to improve the efficiency of the network. Energy consumption and data reliability are stabilized by QOS based multi-hop routing protocols. Whenever clustering plays a main role, load balancing among the clusters has to be important to concentrate. Load balancing has been achieved by choosing best parameters for fuzzy based clustering to select an efficient cluster head for data aggregation and routing. We have done experimental simulations and concluded the results in terms of performance metrics such as standard deviation (SD), First node Die (FND), total energy consumption (TEC) and average energy consumption (AEC).																	1868-5137	1868-5145															10.1007/s12652-020-01827-0		MAR 2020											
