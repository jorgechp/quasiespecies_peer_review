PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	RP	EM	RI	OI	FU	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	D2	EA	PG	WC	SC	GA	UT	PM	OA	HC	HP	DA
J								STAR: A Structure and Texture Aware Retinex Model	IEEE TRANSACTIONS ON IMAGE PROCESSING										Retinex decomposition; low-light image enhancement; color correction	COLOR; ENHANCEMENT; ILLUMINATION; IMAGES	Retinex theory is developed mainly to decompose an image into the illumination and reflectance components by analyzing local image derivatives. In this theory, larger derivatives are attributed to the changes in reflectance, while smaller derivatives are emerged in the smooth illumination. In this paper, we utilize exponentiated local derivatives (with an exponent $\gamma $ ) of an observed image to generate its structure map and texture map. The structure map is produced by been amplified with $\gamma >1$ , while the texture map is generated by been shrank with $\gamma < 1$ . To this end, we design exponential filters for the local derivatives, and present their capability on extracting accurate structure and texture maps, influenced by the choices of exponents $\gamma $ . The extracted structure and texture maps are employed to regularize the illumination and reflectance components in Retinex decomposition. A novel Structure and Texture Aware Retinex (STAR) model is further proposed for illumination and reflectance decomposition of a single image. We solve the STAR model by an alternating optimization algorithm. Each sub-problem is transformed into a vectorized least squares regression, with closed-form solutions. Comprehensive experiments on commonly tested datasets demonstrate that, the proposed STAR model produce better quantitative and qualitative performance than previous competing methods, on illumination and reflectance decomposition, low-light image enhancement, and color correction. The code is publicly available at https://github.com/csjunxu/STAR.																	1057-7149	1941-0042					2020	29						5022	5037		10.1109/TIP.2020.2974060													
J								Image Denoising via Sequential Ensemble Learning	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image denoising; Noise reduction; Machine learning; Transforms; Noise measurement; Iterative methods; Manifolds; Image denoising; ensemble learning; image recovery; ensemble denoiser	SPARSE; REGULARIZATION; RESTORATION; ALGORITHMS; FIELDS	Image denoising is about removing measurement noise from input image for better signal-to-noise ratio. In recent years, there has been great progress on the development of data-driven approaches for image denoising, which introduce various techniques and paradigms from machine learning in the design of image denoisers. This paper aims at investigating the application of ensemble learning in image denoising, which combines a set of simple base denoisers to form a more effective image denoiser. Based on different types of image priors, two types of base denoisers in the form of transform-shrinkage are proposed for constructing the ensemble. Then, with an effective re-sampling scheme, several ensemble-learning-based image denoisers are constructed using different sequential combinations of multiple proposed base denoisers. The experiments showed that sequential ensemble learning can effectively boost the performance of image denoising.																	1057-7149	1941-0042					2020	29						5038	5049		10.1109/TIP.2020.2978645													
J								Robust Facial Landmark Detection via Heatmap-Offset Regression	IEEE TRANSACTIONS ON IMAGE PROCESSING										Heating systems; Feature extraction; Face; Shape; Estimation; Task analysis; Adaptation models; Facial landmark detection; structural hourglass network; global constraint network; heatmap regression; offset estimation	FACE; NETWORK	Facial landmark detection aims at localizing multiple keypoints for a given facial image, which usually suffers from variations caused by arbitrary pose, diverse facial expression and partial occlusion. In this paper, we develop a two-stage regression network for facial landmark detection on unconstrained conditions. Our model consists of a Structural Hourglass Network (SHN) for detecting the initial locations of all facial landmarks based on heatmap generation, and a Global Constraint Network (GCN) for further refining the detected locations based on offset estimation. Specifically, SHN introduces an improved Inception-ResNet unit as basic building block, which can effectively improve the receptive field and learn contextual feature representations. In the meanwhile, a novel loss function with adaptive weight is proposed to make the whole model focus on the hard landmarks precisely. GCN attempts to explore the spatial contextual relationship between facial landmarks and refine the initial locations of facial landmarks by optimizing the global constraint. Moreover, we develop a pre-processing network to generate features with different scales, which will be transmitted to SHN and GCN for effective feature representations. Different from existing models, the proposed method realizes the heatmap-offset framework, which combines the outputs of heatmaps generated by SHN and coordinates estimated by GCN, to obtain an accurate prediction. The extensive experimental results on several challenging datasets, including 300W, COFW, AFLW, and 300-VW confirm that our method achieve competitive performance compared with the state-of-the-art algorithms.																	1057-7149	1941-0042					2020	29						5050	5064		10.1109/TIP.2020.2976765													
J								Occlusion-Aware Region-Based 3D Pose Tracking of Objects With Temporally Consistent Polar-Based Local Partitioning	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image edge detection; Image color analysis; Three-dimensional displays; Histograms; Solid modeling; Optimization; Robustness; Pose estimation; 3D object pose tracking; region-based method; occlusion detection	EDGE; SEGMENTATION	Region-based methods have become the state-of-art solution for monocular 6-DOF object pose tracking in recent years. However, two main challenges still remain: the robustness to heterogeneous configurations (both foreground and background), and the robustness to partial occlusions. In this paper, we propose a novel region-based monocular 3D object pose tracking method to tackle these problems. Firstly, we design a new strategy to define local regions, which is simple yet efficient in constructing discriminative local color histograms. Contrary to previous methods which define multiple circular regions around the object contour, we propose to define multiple overlapped, fan-shaped regions according to polar coordinates. This local region partitioning strategy produces much less number of local regions that need to be maintained and updated, while still being temporally consistent. Secondly, we propose to detect occluded pixels using edge distance and color cues. The proposed occlusion detection strategy is seamlessly integrated into the region-based pose optimization pipeline via a pixel-wise weight function, which significantly alleviates the interferences caused by partial occlusions. We demonstrate the effectiveness of the proposed two new strategies with a careful ablation study. Furthermore, we compare the performance of our method with the most recent state-of-art region-based methods in a recently released large dataset, in which the proposed method achieves competitive results with a higher average tracking success rate. Evaluations on two real-world datasets also show that our method is capable of handling realistic tracking scenarios.																	1057-7149	1941-0042					2020	29						5065	5078		10.1109/TIP.2020.2973512													
J								Context-Integrated and Feature-Refined Network for Lightweight Object Parsing	IEEE TRANSACTIONS ON IMAGE PROCESSING										Semantics; Image segmentation; Computer architecture; Convolution; Convolutional codes; Computational complexity; Computational modeling; Object parsing; semantic segmentation; model efficiency; feature refinement; multi-scale context information	SEMANTIC SEGMENTATION	Semantic segmentation for lightweight object parsing is a very challenging task, because both accuracy and efficiency (e.g., execution speed, memory footprint or computational complexity) should all be taken into account. However, most previous works pay too much attention to one-sided perspective, either accuracy or speed, and ignore others, which poses a great limitation to actual demands of intelligent devices. To tackle this dilemma, we propose a novel lightweight architecture named Context-Integrated and Feature-Refined Network (CIFReNet). The core components of CIFReNet are the Long-skip Refinement Module (LRM) and the Multi-scale Context Integration Module (MCIM). The LRM is designed to ease the propagation of spatial information between low-level and high-level stages. Furthermore, channel attention mechanism is introduced into the process of long-skip learning to boost the quality of low-level feature refinement. Meanwhile, the MCIM consists of three cascaded Dense Semantic Pyramid (DSP) blocks with image-level features, which is presented to encode multiple context information and enlarge the field of view. Specifically, the proposed DSP block exploits a dense feature sampling strategy to enhance the information representations without significantly increasing the computation cost. Comprehensive experiments are conducted on three benchmark datasets for object parsing including Cityscapes, CamVid, and Helen. As indicated, the proposed method reaches a better trade-off between accuracy and efficiency compared with the other state-of-the-art methods.																	1057-7149	1941-0042					2020	29						5079	5093		10.1109/TIP.2020.2978583													
J								A Benchmark for Sparse Coding: When Group Sparsity Meets Rank Minimization	IEEE TRANSACTIONS ON IMAGE PROCESSING										Sparse coding; GSC; rank minimization; adaptive dictionary; weighted l(p)-norm minimization; image restoration; compressive sensing; nuclear norm.	IMAGE-RESTORATION; MATRIX COMPLETION; K-SVD; REPRESENTATION; ALGORITHM; NORM; COMPRESSION; DICTIONARY; TRANSFORM; RECOVERY	Sparse coding has achieved a great success in various image processing tasks. However, a benchmark to measure the sparsity of image patch/group is missing since sparse coding is essentially an NP-hard problem. This work attempts to fill the gap from the perspective of rank minimization. We firstly design an adaptive dictionary to bridge the gap between group-based sparse coding (GSC) and rank minimization. Then, we show that under the designed dictionary, GSC and the rank minimization problems are equivalent, and therefore the sparse coefficients of each patch group can be measured by estimating the singular values of each patch group. We thus earn a benchmark to measure the sparsity of each patch group because the singular values of the original image patch groups can be easily computed by the singular value decomposition (SVD). This benchmark can be used to evaluate performance of any kind of norm minimization methods in sparse coding through analyzing their corresponding rank minimization counterparts. Towards this end, we exploit four well-known rank minimization methods to study the sparsity of each patch group and the weighted Schatten $p$ -norm minimization (WSNM) is found to be the closest one to the real singular values of each patch group. Inspired by the aforementioned equivalence regime of rank minimization and GSC, WSNM can be translated into a non-convex weighted -norm minimization problem in GSC. By using the earned benchmark in sparse coding, the weighted -norm minimization is expected to obtain better performance than the three other norm minimization methods, i.e., -norm, -norm and weighted -norm. To verify the feasibility of the proposed benchmark, we compare the weighted -norm minimization against the three aforementioned norm minimization methods in sparse coding. Experimental results on image restoration applications, namely image inpainting and image compressive sensing recovery, demonstrate that the proposed scheme is feasible and outperforms many state-of-the-art methods.																	1057-7149	1941-0042					2020	29						5094	5109		10.1109/TIP.2020.2972109													
J								An Adaptive and Robust Edge Detection Method Based on Edge Proportion Statistics	IEEE TRANSACTIONS ON IMAGE PROCESSING										Adaptive threshold; edge detection; edge proportion statistics; edge segment detection; one-pixel wide; real-time	CONTOUR; EXTRACTION; ENERGY	Edge detection is one of the most fundamental operations in the field of image analysis and computer vision as a critical preprocessing step for high-level tasks. It is difficult to give a generic threshold that works well on all images as the image contents are totally different. This paper presents an adaptive, robust and effective edge detector for real-time applications. According to the 2D entropy, the images can be clarified into three groups, each attached with a reference percentage value based on the edge proportion statistics. Compared with the attached points along the gradient direction, anchor points were extracted with high probability to be edge pixels. Taking the segment direction into account, these points were then jointed into different edge segments, each of which was a clean, contiguous, 1-pixel wide chain of pixels. Experimental results indicate that the proposed edge detector outperforms the traditional edge following methods in terms of detection accuracy. Besides, the detection results can be used as the input information for post-processing applications in real-time.																	1057-7149	1941-0042					2020	29						5206	5215		10.1109/TIP.2020.2980170													
J								A Weighted Fidelity and Regularization-Based Method for Mixed or Unknown Noise Removal From Images on Graphs	IEEE TRANSACTIONS ON IMAGE PROCESSING										Tight wavelet frame; variational model; mixed or unknown noise; image on graph; image denoising	IMPULSE NOISE; LAPLACIAN REGULARIZATION; RESTORATION; SPARSE; TRANSFORM; POISSON; SURFACES; SYSTEMS	Image denoising technologies in a Euclidean domain have achieved good results and are becoming mature. However, in recent years, many real-world applications encountered in computer vision and geometric modeling involve image data defined in irregular domains modeled by huge graphs, which results in the problem on how to solve image denoising problems defined on graphs. In this paper, we propose a novel model for removing mixed or unknown noise in images on graphs. The objective is to minimize the sum of a weighted fidelity term and a sparse regularization term that additionally utilizes wavelet frame transform on graphs to retain feature details of images defined on graphs. Specifically, the weighted fidelity term with -norm is designed based on a analysis of the distribution of mixed noise. The augmented Lagrangian and accelerated proximal gradient methods are employed to achieve the optimal solution to the problem. Finally, some supporting numerical results and comparative analyses with other denoising algorithms are provided. It is noted that we investigate image denoising with unknown noise or a wide range of mixed noise, especially the mixture of Poisson, Gaussian, and impulse noise. Experimental results reported for synthetic and real images on graphs demonstrate that the proposed method is effective and efficient, and exhibits better performance for the removal of mixed or unknown noise in images on graphs than other denoising algorithms in the literature. The method can effectively remove mixed or unknown noise and retain feature details of images on graphs. It delivers a new avenue for denoising images in irregular domains.																	1057-7149	1941-0042					2020	29						5229	5243		10.1109/TIP.2020.2969076													
J								Tsallis Entropy Segmentation and Shape Feature-based Classification of Defects in the Simulated Magnetic Flux Leakage Images of Steam Generator Tubes	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Steam generator tube; magnetic flux leakage; shape features; image segmentation; neural network	SYSTEM	Early detection of water or steam leaks into sodium in the steam generator units of nuclear reactors is an important requirement from safety and economic considerations. Automated defect detection and classification algorithm for categorizing the defects in the steam generator tube (SGT) of nuclear power plants using magnetic flux leakage (MFL) technique has been developed. MFL detection is one of the most prevalent methods of pipeline inspection. Comsol 4.3a, a multiphysics modeling software has been used to obtain the simulated MFL defect images. Different thresholding methods are applied to segment the defect images. Performance metrics have been computed to identify the better segmentation technique. Shape-based feature sets such as area, perimeter, equivalent diameter, roundness, bounding box, circularity ratio and eccentricity for defect have been extracted as features for defect detection and classification. A feed forward neural network has been constructed and trained using a back-propagation algorithm. The shape features extracted from Tsallis entropy-based segmented MFL images have been used as inputs for training and recognizing shapes. The proposed method with Tsallis entropy segmentation and shape-based feature set has yielded the promising results with detection accuracy of 100% and average classification accuracy of 96.11%.																	0218-0014	1793-6381				JAN	2020	34	1							2054002	10.1142/S0218001420540026													
J								From Local Understanding to Global Regression in Monocular Visual Odometry	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Visual odometry; deep learning; convolutional neural network (CNN); simultaneous localization and mapping (SLAM); classification; regression		The most significant part of any autonomous intelligent robot is the localization module that gives the robot knowledge about its position and orientation. This knowledge assists the robot to move to the location of its desired goal and complete its task. Visual Odometry (VO) measures the displacement of the robots' camera in consecutive frames which results in the estimation of the robot position and orientation. Deep Learning, nowadays, helps to learn rich and informative features for the problem of VO to estimate frame-by-frame camera movement. Recent Deep Learning-based VO methods train an end-by-end network to solve VO as a regression problem directly without visualizing and sensing the label of training data in the training procedure. In this paper, a new approach to train Convolutional Neural Networks (CNNs) for the regression problems, such as VO, is proposed. The proposed method first changes the problem to a classification problem to learn different subspaces with similar observations. After solving the classification problem, the problem converts to the original regression problem to solve using the knowledge achieved by solving the classification problem. This approach helps CNN to solve regression problem globally in a local domain learned in the classification step, and improves the performance of the regression module for approximately 10%.																	0218-0014	1793-6381				JAN	2020	34	1							2055002	10.1142/S0218001420550022													
J								One Core Task of Interpretability in Machine Learning - Expansion of Structural Equation Modeling	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										SEM; ESEM; structure models; measurement models; observation models	INDEPENDENT COMPONENT ANALYSIS; LASSO	Structural equation modeling (SEM) is a system of two kinds of equations: a linear latent structural model (SM) and a linear measurement model (MM). The latent structure model is a causal model from the latent parent node to the latent child node. Meanwhile, MM's link is from latent variable parent node to observed variable child node. However, researchers should determine the initial causal order between variables based on experience when applying SEM. The main reason is that SEM does not fully construct causal models between observed variables (OVs) from big data. When the artificial causal order is contrary to the fact, the causal inference from SEM is doubtful, and the implicit causal information between the OVs cannot be extracted and utilized. This study first objectively identifies the causal order of variables using the DirectLiNGAM method widely accepted in recent years. Then traditional SEM is converted to expanded SEM (ESEM) consisting of SM, MM and observation model (OM). Finally, through model testing and debugging, ESEM with good fit with data is obtained.																	0218-0014	1793-6381				JAN	2020	34	1							2051001	10.1142/S0218001420510015													
J								A Fast Intra Mode Selection Algorithm Based on CU Size for Virtual Reality 360 degrees Video	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										360 degrees video; HEVC; fast intra coding; mode selection	DECISION	Virtual Reality (VR) 360 degrees video is spherical, and has ultra-high definition. It is time-consuming to code it using High Efficient Video Coding (HEVC) directly. In this paper, a fast intra mode selection algorithm based on Coding Unit (CU) size is proposed for VR 360 degrees video in Cubemap projection (CMP) format. A redefined mode selection strategy for two-stage Rough mode decision (RMD) is designed based on different depth CUs to reduce coding time during intra prediction process. The experimental results show that the proposed algorithm brings 27.70% saving in average coding time at the cost of only 0.8% Bjontegaard delta rate increase in All-Intra mode.																	0218-0014	1793-6381				JAN	2020	34	1							2055001	10.1142/S0218001420550010													
J								Active Learning Using Fuzzy-Rough Nearest Neighbor Classifier for Cancer Prediction from Microarray Gene Expression Data	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Active learning; cancer prediction; microarray gene expression data; fuzzy set; rough set	TUMOR CLASSIFICATION; CLUSTER-ANALYSIS; ALGORITHM	Cancer prediction from gene expression data is a very challenging area of research in the field of computational biology and bioinformatics. Conventional classifiers are often unable to achieve desired accuracy due to the lack of 'sufficient' training patterns in terms of clinically labeled samples. Active learning technique, in this respect, can be useful as it automatically finds only few most informative (or confusing) samples to get their class labels from the experts and those are added to the training set, which can improve the accuracy of the prediction consequently. A novel active learning technique using fuzzy-rough nearest neighbor classifier (ALFRNN) is proposed in this paper for cancer classification from microarray gene expression data. The proposed ALFRNN method is capable of dealing with the uncertainty, overlapping and indiscernibility often present in cancer subtypes (classes) of the gene expression data. The performance of the proposed method is tested using different real-life microarray gene expression cancer datasets and its performance is compared with five other state-of-the-art techniques (out of which three are active learning-based and two are traditional classification methods) in terms of percentage accuracy, precision, recall, F-1-measures and kappa. Superiority of the proposed method over the other counterpart algorithms is established from experimental results for cancer prediction and results of the paired t-test confirm statistical significance of the results in favor of the proposed method for almost all the datasets.																	0218-0014	1793-6381				JAN	2020	34	1							2057001	10.1142/S0218001420570013													
J								A SLAM-Based Mobile Augmented Reality Tracking Registration Algorithm	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Augmented reality; SLAM; AGAST-FREAK; tracking registration	MONOCULAR SLAM; KALMAN FILTER	This paper proposes a simultaneous localization and mapping (SLAM)-based markerless mobile-end tracking registration algorithm to address the problem of virtual image drift caused by fast camera motion in mobile-augmented reality (AR). The proposed algorithm combines the AGAST-FREAK-SLAM algorithm with inertial measurement unit (IMU) data to construct a scene map and localize the camera's pose. The extracted feature points are matched with feature points in a map library for real-time camera localization and precise registration of virtual objects. Experimental results show that the proposed method can track feature points in real time, accurately construct scene maps, and locate cameras; moreover, it improves upon the tracking and registration robustness of earlier algorithms.																	0218-0014	1793-6381				JAN	2020	34	1							2054005	10.1142/S0218001420540051													
J								Automatic Lip Reading Using Convolution Neural Network and Bidirectional Long Short-term Memory	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Automatic lip reading; deep learning; convolution neural network; bidirectional long short-term memory	MODELS	Traditional automatic lip-reading systems generally consist of two stages: feature extraction and recognition, while the handcrafted features are empirical and cannot learn the relevance of lip movement sequence sufficiently. Recently, deep learning approaches have attracted increasing attention, especially the significant improvements of convolution neural network (CNN) applied to image classification and long short-term memory (LSTM) used in speech recognition, video processing and text analysis. In this paper, we propose a hybrid neural network architecture, which integrates CNN and bidirectional LSTM (BiLSTM) for lip reading. First, we extract key frames from each isolated video clip and use five key points to locate mouth region. Then, features are extracted from raw mouth images using an eight-layer CNN. The extracted features have the characteristics of stronger robustness and fault-tolerant capability. Finally, we use BiLSTM to capture the correlation of sequential information among frame features in two directions and the softmax function to predict final recognition result. The proposed method is capable of extracting local features through convolution operations and finding hidden correlation in temporal information from lip image sequences. The evaluation results of lip-reading recognition experiments demonstrate that our proposed method outperforms conventional approaches such as active contour model (ACM) and hidden Markov model (HMM).																	0218-0014	1793-6381				JAN	2020	34	1							2054003	10.1142/S0218001420540038													
J								Parameter Matching Analysis of Hydraulic Hybrid Bergepanzers Based on RBF-Adaptive Artificial Immune Algorithm	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										RBF&AAIA; Bergepanzers; hydraulic hybrid system		Due to the multiple types of support tasks and high-energy efficiency equipment, energy saving research in a Bergepanzer has attracted much attention in recent years. A hydraulic hybrid system is an effective way for these problems of Bergepanzers. The initial problem is to optimize the parameter optimization-matching for a new hydraulic hybrid Bergepanzers (NHHB), which is a relatively complex and variable time-varying nonlinear system. In this paper, an RBF-adaptive artificial immune algorithm (RBF&AAIA) is presented to solve the parameter optimization-matching problem, and the performance of algorithm is analyzed. Finally, the RBF&AAIA is successfully used for the NHHB to optimize the parameters of key components for the minimum energy consumption of system, and the optimal parameter matching group is obtained.																	0218-0014	1793-6381				JAN	2020	34	1							2059002	10.1142/S0218001420590028													
J								New Algorithm of Response Curve for Fitting HDR Image	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										High dynamic range; low dynamic range; camera response curve; mapping images; texture features; exposure time	COMPRESSION	Based on the process of generating HDR images from LDR image sequences with different light exposures in the same scene, a new fitting method of camera response curves is proposed to solve the problem that the boundary of the fitting algorithm of camera response curves will be blurred and it is difficult to determine and verify the accuracy of the fitting curves. The optimal response curve is fitted by increasing LDR images step by step through considering the pixel value and texture characteristics. In order to validate the fitting effect of curves, we compare the photographed images and the real images in different time intervals on the basis of HDR images and response curves. We use RGB and gray image experiments to compare the current mainstream algorithms and the accuracy of our proposed algorithm can reach 96%, which has robustness.																	0218-0014	1793-6381				JAN	2020	34	1							2054001	10.1142/S0218001420540014													
J								Nonlinear Dynamics Tools for Offline Signature Verification Using One-class Gaussian Process	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Offline signature verification; wavelet sub-band; multitresolution box-counting method; probabilistic finite state automata; writer-independent; one-class Gaussian process	FRACTAL DIMENSION; DISTANCE	One of the major problems in biometrics and in document forensics is the offline mode of signature verification. This study aims to present a novel approach of verifying an individual's signature through offline images of handwriting. The approach proposed here relies on a global method which considers signature images as waveforms. First, image decompositions are in terms of a series of wavelet sub-bands at some specific levels. Wavelet sub-bands are then extended so as to obtain waveforms. Each waveform is quantized by two Nonlinear Dynamics Tools in order to generate feature vectors. Multi-Resolution Box-Counting (MRBC) fractal dimension algorithm as well as probabilistic finite state automata (PFSA) are applied separately to signature waveforms. In the training and verification phase, we propose the one-class Gaussian process (GP) priors based on writer-independent approach. As one of the main parameters, optimal decision threshold is selected from False Accept Rate (FAR) and False Reject Rate (FRR) curves. The presented system was tested on two Persian databases (PHBC and UTSig) as well as on two Latin databases (MCYT-75 and CEDAR). In fact, the results produced by this method were generally better in terms of the four signature databases than the state-of-the-art results.																	0218-0014	1793-6381				JAN	2020	34	1							2053001	10.1142/S0218001420530018													
J								Machine Translation Evaluation: Unveiling the Role of Dense Sentence Vector Embedding for Morphologically Rich Language	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Evaluation of Hindi MT; dense sentence embeddings; word vectors; linguistic knowledge; METEOR		Machine Translation (MT) evaluation metrics like BiLingual Evaluation Understudy (BLEU) and Metric for Evaluation of Translation with Explicit Ordering (METEOR) are known to have poor performance for word-order and morphologically rich languages. Application of linguistic knowledge to evaluate MTs for morphologically rich language like Hindi as a target language, is shown to be more effective and accurate [S. Tripathi and V. Kansal, Using linguistic knowledge for machine translation evaluation with Hindi as a target language, Comput. Sist. 21(4) (2017) 717-724]. Leveraging the recent progress made in the domain of word vector and sentence vector embedding [T. Mikolov and J. Dean, Distributed representations of words and phrases and their compositionality, Adv. Neural Inf. Process. Syst. 2 (2013) 3111-3119], authors have trained a large corpus of pre-processed Hindi text (similar to 112 million tokens) for obtaining the word vectors and sentence vector embedding for Hindi. The training has been performed on high end system configuration utilizing Google Cloud platform resources. This sentence vector embedding is further used to corroborate the findings through linguistic knowledge in evaluation metric. For morphologically rich language as target, evaluation metric of MT systems is considered as an optimal solution. In this paper, authors have demonstrated that MT evaluation using sentence embedding-based approach closely mirrors linguistic evaluation technique. The relevant codes used to generate the vector embedding for Hindi have been uploaded on code sharing platform Github.(a)																	0218-0014	1793-6381				JAN	2020	34	1							2059001	10.1142/S0218001420590016													
J								Color Reproduction Accuracy Promotion of 3D-Printed Surfaces Based on Microscopic Image Analysis	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Full-color printing; color optimization; structural characterization; image analysis; impregnating process; scanning electron microscope		Full-color three-dimensional (3D) printing technology is a powerful process to manufacture intelligent customized colorful objects with improved surface qualities; however, poor surface color optimization methods are the main impeding factors for its commercialization. As such, the paper explored the correlation between microstructure and color reproduction, then an assessment and prediction method of color optimization based on microscopic image analysis was proposed. The experimental models were divided into 24-color plates and 4-color cubes printed by ProJet 860 3D printer, then impregnated according to preset parameters, at last measured by a spectrophotometer and observed using both a digital microscope and a scanning electron microscope. The results revealed that the samples manifested higher saturation and smaller chromatic aberration (Delta E) after postprocessing. Moreover, the brightness of the same color surface increased with the increasing soaked surface roughness. Further, reduction in surface roughness, impregnation into surface pores, and enhancement of coating transparency effectively improved the accuracy of color reproduction, which could be verified by the measured values. Finally, the chromatic aberration caused by positioning errors on different faces of the samples was optimized, and the value of Delta E for a black cube was reduced from 8.12 to 0.82, which is undetectable to human eyes.																	0218-0014	1793-6381				JAN	2020	34	1							2054004	10.1142/S021800142054004X													
J								An Intelligent Security Defensive Model of SCADA Based on Multi-Agent in Oil and Gas Fields	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										SCADA; multi-agent; defensive model; decision tree; JADE		Supervisory Control and Data Acquisition (SCADA) system in the modern industrial automation control network is facing an increasing number of serious security threats. In order to meet the security defense requirements of oil and gas SCADA system, an intelligent security defense model based on multi-agent was designed by analyzing the security risks in oil and gas SCADA system and combining the advantages of multi-agent technology in distributed intrusion detection system. First, the whole structure of this model was divided into three layers: monitoring layer, decision layer and control layer. Then, the defense model was verified by C4.5 decision tree algorithm, and obtained a good result. Finally, the security defense prototype system of large-scale oil and gas SCADA system based on this model was realized. Results demonstrate that the application of multi-agent technology in the security defense of oil and gas SCADA system can achieve more comprehensive defense, more accurate detection, which can handle large-scale distributed attacks and improve the robustness and stability of security defenses. This study makes full use of the multi-agent architecture and has the advantage of accurate detection, high detection efficiency and timely response.																	0218-0014	1793-6381				JAN	2020	34	1							2059003	10.1142/S021800142059003X													
J								Study on Traffic Sign Recognition by Optimized Lenet-5 Algorithm	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Traffic sign recognition; Gabor filter; Lenet-5; SVM	DEEP NEURAL-NETWORK; SYSTEMS; VIDEO	Traffic sign recognition (TSR) is a key technology of intelligent vehicles, which is based on visual perception for road information. In view of the fact that the traditional computer vision identification technology cannot meet the requirements of real-time accuracy, the TSR algorithm has been proposed on the basis of improved Lenet-5 algorithm. Firstly, we performed picture noise elimination and image enhancement on selected traffic sign images. Secondly, we used Gabor filter kernel in the convolution layer for convolution operation. In the convolution process, we added normalization layer Batch Normality (BN) after each convolution layer and reduced the data dimension. In the down-sampling layer, we replaced Sigmoid with the Relu activator. Finally, we selected the expanded GTSRB traffic sign database for the comparison experiment on the Caff platform. The experimental results showed that the proposed improved Lenet-5 network test set had the recognition accuracy of 96%, which was better than the method that combined Gabor with Support Vector Machine (SVM) in terms of recognition accuracy and real-time performance.																	0218-0014	1793-6381				JAN	2020	34	1							2055003	10.1142/S0218001420550034													
J								Fuzzy 2D Linear Discriminant Analysis Based on Sub-image and Random Sampling for Face Recognition	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Face recognition; sub-image method; fuzzy set theory; 2D-LDA; random sampling	ANALYSIS FRAMEWORK; PCA; SUBSPACES	Face recognition, as a research hot topic, still faces many challenges. This paper proposes a new face recognition method by fusing the advantages of fuzzy set theory, sub-image method and random sampling technique. In this method, we partition an original image into some subimages to improve the robustness to different facial variations, and extract local features from each sub-image by using fuzzy 2D-Linear Discriminant analyzis (LDA) which makes use of the class information hidden in neighbor samples. In order to increase the diversity of component classifiers and retain as much as the structural information of the row vectors, we further randomly sample row vectors from each sub-image before performing fuzzy 2D-LDA. Experimental results on Yale A, ORL, AR and Extended Yale B face databases show its superiority to other related state-of-the-art methods on the different variations such as illumination, occlusion and facial expression. Furthermore, we analyze the diversity of our proposed method by virtue of Kappa diversity-error analyzis and frequency histogram and results show that the proposed method can construct more diverse component classifiers than other methods.																	0218-0014	1793-6381				JAN	2020	34	1							2056001	10.1142/S0218001420560017													
J								Real-time visualization to improve quality in computer mediated communication	WEB INTELLIGENCE										Design science research; captology; information visualization; online social networking	ARGUMENTATION	Within conversational media, how others perceive contributions affects his or her interactions with those contributions. This research explores a novel addition to conversational software, one that provides real-time assessment of quality across user contributions. An analysis of 2,157 online conversations examined attributes of quality, including lexical complexity and prompt-specific vocabulary. These factors helped to inform the redesign of an existing asynchronous online discussion board (AOD). More specifically, a real-time quality analyzer was constructed, which provides users with a visual breakdown of their post in relation to the overall group discussion thread. An experiment across two populations was performed and results found that the system increased overall levels of quality in conversations, while also increasing quality interactions across the system. The results were supplemented with survey data and a social network analysis (SNA), which discovered higher levels of system satisfaction and group cohesion.																	2405-6456	2405-6464					2020	18	1					1	13		10.3233/WEB-200425													
J								2SRM: Learning social signals for predicting relevant search results	WEB INTELLIGENCE										Social signals; social properties; social information retrieval; criteria evaluation; learning approaches	SELECTION	Search systems based on both professional meta-data (e.g., title, description, etc.) and social signals (e.g., like, comment, rating, etc.) from social networks is the trending topic in information retrieval (IR) field. This paper presents 2SRM (Social Signals Relevance Model), an approach of IR which takes into account social signals (users' actions) as an additional information to enhance a search. We hypothesize that these signals can play a role to estimate a priori social importance (relevance) of the resource (document). In this paper, we first study the impact of each such signal on retrieval performance. Next, some social properties such as popularity, reputation and freshness are quantified using several signals. The 2SRM combines the social relevance, estimated from these social signals and properties, with the conventional textual relevance. Finally, we investigate the effect of the social signals on the retrieval effectiveness using state-of-the-art learning approaches. In order to identify the most effective signals, we adopt feature selection algorithms and the correlation between the signals. We evaluated the effectiveness of our approach on both IMDb (Internet Movie Databese) and SBS (Social Book Search) datasets containing movies and books resources and their social characteristics collected from several social networks. Our experimental results are statistically significant, and reveal that incorporating social signals in retrieval model is a promising approach for improving the retrieval performance.																	2405-6456	2405-6464					2020	18	1					15	33		10.3233/WEB-200426													
J								Cross-platform personality exploration system for online social networks: Facebook vs. Twitter	WEB INTELLIGENCE										Big Five model; personality prediction; brand personality; machine learning; social media analysis		Social networking sites (SNS) are a rich source of latent information about individual characteristics. Crawling and analyzing this content provides a new approach for enterprises to personalize services and put forward product recommendations. In the past few years, commercial brands made a gradual appearance on social media platforms for advertisement, customers support and public relation purposes and by now it became a necessity throughout all branches. This online identity can be represented as a brand personality that reflects how a brand is perceived by its customers. We exploited recent research in text analysis and personality detection to build an automatic brand personality prediction model on top of the (Five-Factor Model) and (Linguistic Inquiry and Word Count) features extracted from publicly available benchmarks. Predictive evaluation on brands' accounts reveals that Facebook platform provides a slight advantage over Twitter platform in offering more self-disclosure for users' to express their emotions especially their demographic and psychological traits. Results also confirm the wider perspective that the same social media account carry a quite similar and comparable personality scores over different social media platforms. For evaluating our prediction results on actual brands' accounts, we crawled the Facebook API and Twitter API respectively for 100k posts from the most valuable brands' pages in the USA and we visualize exemplars of comparison results and present suggestions for future directions.																	2405-6456	2405-6464					2020	18	1					35	51		10.3233/WEB-200427													
J								Deep text classification of Instagram data using word embeddings and weak supervision	WEB INTELLIGENCE										Instagram; weak supervision; word embeddings; deep learning		With the advent of social media, our online feeds increasingly consist of short, informal, and unstructured text. Instagram is one of the largest social media platforms, containing both text and images. However, most of the prior research on text processing in social media is focused on analyzing Twitter data, and little attention has been paid to text mining of Instagram data. Moreover, many text mining methods rely on training data annotated manually by humans, which in practice is both difficult and expensive to obtain. In this paper, we present methods for weakly supervised text classification of Instagram text. We analyze a corpora of Instagram posts from the fashion domain and train a deep clothing classifier with weak supervision to classify Instagram posts based on the associated text. With our experiments, we demonstrate that in absence of annotated training data, using weak supervision to train models is a viable approach. With weak supervision we were able to label a large dataset in hours, something that would have taken months to do with human annotators. Using the dataset labeled with weak supervision in combination with generative modeling, an F-1 score of 0.61 is achieved on the task of classifying the image contents of Instagram posts based solely on the associated text, which is on level with human performance.																	2405-6456	2405-6464					2020	18	1					53	67		10.3233/WEB-200428													
J								Interactive minutes generation system based on hierarchical discussion structure	WEB INTELLIGENCE										Knowledge representation; information structuralization; automatic summarization; information retrieval system		This research leads to the development of a system that generates minutes while the system and user interactively structure and summarize the discussion. The system can look back on a meeting from a specific viewpoint or purpose by sending an inquiry to the minutes, receiving the result, and performing a new operation. To improve the reusability of the minutes, it is necessary to understand the semantic relationships among utterances at a deep level. We therefore propose a novel tree structure called the discussion time-span tree for representing a hierarchical discussion based on the relative importance of utterances derived from the meeting. The discussion time-span tree is a binary tree, each leaf of which corresponds to an utterance. Adjacent relevant utterances are hierarchically grouped, and the most important utterance becomes a head. In this way, the tree represents the implicit meaning and intention contained in meeting records, and can be used for summarization. Our system aims to grasp the main point of the meeting by interactively manipulating this tree structure and giving different viewpoints to each system user. An evaluation of the important utterance extraction using the proposed system showed that the score of ROUGE-2 was higher than that of the existing document summarizing technique. In a subsequent user evaluation conducted with ten participants, we found that the system can be used to grasp conference contents efficiently. The results of both evaluations demonstrate the usefulness of the system.																	2405-6456	2405-6464					2020	18	1					85	95		10.3233/WEB-200430													
J								IoT-enabled smart appliances under industry 4.0: A case study	ADVANCED ENGINEERING INFORMATICS										Industry 4.0; Internet of Things; Big data analytics; Smart products	FRAMEWORK; DESIGN; INTERNET; SYSTEMS; NETWORK; THINGS	Manufacturers expect the extra value of Industry 4.0 as the world is experiencing digital transformation. Studies have proved the potential of the Internet of Things (IoT) for reducing cost, improving efficiency, quality, and achieving data-oriented predictive maintenance services. Collecting a wide range of real-time data from products and the environment requires smart sensors, reliable communications, and seamless integration. loT, as a critical Industry 4.0 enabler emerges smart home appliances for higher customer satisfaction, energy efficiency, personalisation, and advanced Big data analytics. However, established factories with limited resources are facing challenges to change the longstanding production lines and meet customer's requirements. This study aims to fulfil the gaps by transforming conventional home appliances to IoT-enabled smart systems with the ability to integrate into a smart home system. An industry-led case study demonstrates how to turn conventional appliances to smart products and systems (SPS) by utilising the state-of-the-art Industry 4.0 technologies.																	1474-0346	1873-5320				JAN	2020	43								101043	10.1016/j.aei.2020.101043													
J								Improvement of transportation cost estimation for prefabricated construction using geo-fence-based large-scale GPS data feature extraction and support vector regression	ADVANCED ENGINEERING INFORMATICS										Transportation cost; SVR; Global positioning system (GPS) data; Fleet activity recognition; Panelized construction	HONG-KONG; EQUIPMENT; RECOGNITION; OPERATIONS; SIMULATION; SYSTEMS; WORKERS	In panelized construction, transportation is an essential process linking a manufacturing facility to a project's jobsite using hauling equipment (e.g., trucks and trailers). Accordingly, the cost associated with transportation operations is considerable compared to a traditional stick build. Nevertheless, transportation cost estimation has often relied on a fixed-cost approach, regarding the cost as part of the overhead cost, rather than conducting detailed estimation of actual transportation operations. This is because operation-level data might be challenging to collect and analyze in practice. In this regard, the prevalent use of GPS devices for construction equipment may provide an automated means of monitoring the operations of transportation equipment, and large and detailed spatial and temporal data can be generated from multiple pieces of equipment in multiple construction projects on a daily basis or even in real time. This study thus proposes a spatial and temporal data filtering and abstracting approach to transportation cost estimation using fleet GPS data which extracts equipment activities from the GPS data and accordingly predicts the transportation demands required for an individual project. From large-scale GPS data, key operation information, such as the number of trailers and durations required (i.e., transportation demands), is extracted using a geo-fence and a rule-based equipment operation analysis algorithm. Then, the extracted transportation demand information, along with related project specifications, is used to train support vector regression (SVR) models for the purpose of predicting the transportation demand in new projects, which is in turn utilized to estimate the transportation cost using the relevant transportation unit cost of the equipment. To evaluate the performance, GPS datasets collected from 221 panelized residential projects over a period of 8 months are used to train the prediction model and are compared with actual transportation costs estimated in practice. The results show that the SVR model has an accuracy of 86% and 88% in predicting the number of trailers and the duration, respectively. For the cost estimation performance, the results reveal that the average cost difference of 57% between the fixed cost and the actual transportation cost was reduced to 14% by implementing the GPS-data-based method in various project locations and for projects of various sizes. The GPSdata-based estimation approach thus is found to provide a more accurate transportation cost estimation result for various panelized construction projects, and the method improves the understanding of large-scale spatial and temporal equipment data while increasing the utilization of the GPS data already available.																	1474-0346	1873-5320				JAN	2020	43								101012	10.1016/j.aei.2019.101012													
J								Application of industrial pipelines data generator in the experimental analysis: Pipe spooling optimization problem definition, formulation, and testing	ADVANCED ENGINEERING INFORMATICS										Branch-and-bound; Data generator; Bin packing; Optimization; Pipe spooling	BIN-PACKING; HEURISTICS	Experimental analysis of algorithm performance can generally be obtained by running the algorithm of interest on a large number of diverse datasets from which statistical information regarding scalability and efficacy are obtained. In addition, these datasets can also be used to gain insight into the impact of a local modification on the global performance of a procedure. However, the main challenge in this area is related to the availability of real-world instance projects from which useable data can be collected. In fact, not only real-life data collection, documentation and management is expensive but more importantly they are generally confidential. As a result, building data simulators capable of generating instance datasets exhibiting features similar to those collected from real-life projects can help alleviate the challenge of availability and confidentiality of data for research. Building on previous work (Al-Alawi et al., 2018), this contribution illustrates the application of the industrial pipelines data generator in the experimental analysis of a pipe spooling optimization problem. The industrial project-based problem in the form of pipe spooling process was defined and projected as a three-dimensional bin-packing class of optimization problem. A branch-and-bound heuristic was proposed to solve the optimization problem and tested on 1000 instance problems generated using the industrial pipeline data generator. Two scenarios were tested the run time performance was reported and recorded as benchmark results for future use.																	1474-0346	1873-5320				JAN	2020	43								101007	10.1016/j.aei.2019.101007													
J								Guidelines for applied machine learning in construction industry-A case of profit margins estimation	ADVANCED ENGINEERING INFORMATICS										Applied machine learning; Profit margin forecasting; Construction simulation tool; Interpretable machine learning; Predictive modelling		The progress in the field of Machine Learning (ML) has enabled the automation of tasks that were considered impossible to program until recently. These advancements today have incited firms to seek intelligent solutions as part of their enterprise software stack. Even governments across the globe are motivating firms through policies to tape into ML arena as it promises opportunities for growth, productivity and efficiency. In reflex, many firms embark on ML without knowing what it entails. The outcomes so far are not as expected because the ML, as hyped by tech firms, is not the silver bullet. However, whatever ML offers, firms urge to capitalise it for their competitive advantage. Applying ML to real-life construction industry problems goes beyond just prototyping predictive models. It entails intensive activities which, in addition to training robust ML models, provides a comprehensive framework for answering questions asked by construction folks when intelligent solutions are getting deployed at their premises to substitute or facilitate their decision-making tasks. Existing ML guidelines used in the IT industry are vastly restricted to training ML models. This paper presents guidelines for Applied Machine Learning (AML) in the construction industry from training to operationalising models, which are drawn from our experience of working with construction folks to deliver Construction Simulation Tool (CST). The unique aspect of these guidelines lies not only in providing a novel framework for training models but also answering critical questions related to model confidence, trust, interpretability, bias, feature importance and model extrapolation capabilities. Generally, ML models are presumed black boxes; hence argued that nobody knows what a model learns and how it generates predictions. Even very few ML folks barely know approaches to answer questions asked by the end users. Without explaining the competence of ML, the broader adoption of intelligent solutions in the construction industry cannot be attained. This paper proposed a detailed process for AML to develop intelligent solutions in the construction industry. Most discussions in the study are elaborated in the context of profit margin estimation for new projects.																	1474-0346	1873-5320				JAN	2020	43								101013	10.1016/j.aei.2019.101013													
J								Benchmark value determination of energy efficiency indexes for coal-fired power units based on data mining methods	ADVANCED ENGINEERING INFORMATICS										Coal-fired power unit; Energy saving; Benchmark value; Fuzzy C-means; Support vector regression	OPTIMIZATION; STATE	The operational optimisation of coal-fired power units is important for saving energy and reducing losses in the electric power industry. One of the key issues is how to determine the benchmark values of the energy efficiency indexes of the units. Therefore, a new framework for determining these benchmark values is proposed, based on data mining methods. First, the energy efficiency key performance indicators (KPIs) associated with the net coal consumption rate (NCCR) were selected based on the domain knowledge. Second, the decision-making samples with minimal NCCR were acquired with the fuzzy C-means (FCM) clustering algorithm, and the corresponding clustering centres were employed as the benchmark values. Finally, based on the support vector regression (SVR) algorithm, the target values of the NCCR were obtained with the KPIs as input, and the energy saving potential was evaluated by comparing the target values with the historical values of the NCCR. An actual on-duty 1000 MW unit was taken as study unit, and the results show that the energy saving potential is remarkable when the operators adjust the KPIs based on the calculated benchmark values.																	1474-0346	1873-5320				JAN	2020	43								101029	10.1016/j.aei.2019.101029													
J								Design a personalised product service system utilising a multi-agent system	ADVANCED ENGINEERING INFORMATICS										Product service system; Multi-agent system; Personalisation; Pervasive computing	DECENTRALIZED ENERGY MANAGEMENT; METHODOLOGY; DEVELOP	To help achieve sustainability, reduce the risk of uncertainty, and fulfil a variety of customer needs, the concept of a Product Service System has evolved for use as a business model. While previous studies have addressed PSS development, there is yet no dynamic methodology enabling a PSS to adjust quickly to external changes and customer response. With that gap in mind, this paper presents a Multi-Agent-based Personalised Product Service System (MAPPSS) system. MAPPSS provides a reliable method for analysis which is applicable in various pervasive environments. With this proposed system, users are able to select expected service characteristics, after which service composition is conducted by selecting related products and services from a database. In a comparison of existing systems using different scenarios, we show that customer acceptance, energy utilization, and product utilization generate a two-digit improvement with the proposed system, while profit and costs may increase slightly. When using the proposed method, the competence of an enterprise can be enhanced as a result of these user-oriented features. The performance monitoring and iterating elements of the system allow an almost immediate improvement in the generated PSS.																	1474-0346	1873-5320				JAN	2020	43								101036	10.1016/j.aei.2020.101036													
J								Soldering defect detection in automatic optical inspection	ADVANCED ENGINEERING INFORMATICS										Automatic Optical Inspection (AOI); Localization and classification of solder joint defects; Semi-supervised learning; YOLO; Clustering; Active learning	JOINT INSPECTION; NEURAL-NETWORK; CLASSIFICATION	This paper proposes an integrated detection framework of solder joint defects in the context of Automatic Optical Inspection (AOI) of Printed Circuit Boards (PCBs). Both localization and classifications tasks were considered. For the localization part, in contrast to the existing methods that are highly specified for particular PCBs, we used a generic deep learning method which can be easily ported to different configurations of PCBs and soldering technologies and also gives real-time speed and high accuracy. For the classification part, an active learning method was proposed to reduce the labeling workload when a large labeled training database is not easily available because it requires domain-specified knowledge. The experiments show that the localization method is fast and accurate. In addition, high accuracy with only minimal user input was achieved in the classification framework on two different datasets. The results also demonstrated that our method outperforms three other active learning benchmarks.																	1474-0346	1873-5320				JAN	2020	43								101004	10.1016/j.aei.2019.101004													
J								BIM-based task-level planning for robotic brick assembly through image-based 3D modeling	ADVANCED ENGINEERING INFORMATICS										Robotic brick assembly; Task-level planning; BIM; Image-based 3D modeling	CONSTRUCTION; PLATFORM; PROGRESS	The application of robotics in the assembly of building bricks has become a popular topic, while the planning of construction robots is still lagged far behind the manufacturing industry. New robotic assembly task with manual teaching-planning method is always time consuming. A task-level planning method was proposed, and the implementation details were described to improve the planning efficiency of robotic brick assembly without affecting accuracy. In this work, a BIM (Building Information Model)-based robotic assembly model that contains all the required information for planning was proposed. Image-based 3D modeling was utilized to help the calibration of the robotic assembly scene and building task models. The placement point coordinates of each assembly brick were generated in the robot base coordinate system. Finally, three different building information model tasks of modular structures (e.g., wall, stair, and pyramid) were designed. The feasibility and effectiveness of the proposed method were verified by comparing the efficiency and accuracy of three models through manual teaching and task-level planning.																	1474-0346	1873-5320				JAN	2020	43								100993	10.1016/j.aei.2019.100993													
J								Memetic improved cuckoo search algorithm for automatic B-spline border approximation of cutaneous melanoma from macroscopic medical images	ADVANCED ENGINEERING INFORMATICS										Swarm intelligence; Cuckoo search algorithm; Medical image segmentation; Cutaneous melanoma; Border approximation; B-spline curves	NEURAL-NETWORK; FUNCTIONAL NETWORKS; POINT CLOUDS; RECONSTRUCTION; SEGMENTATION; OPTIMIZATION; CURVE; SURFACES	This work follows up a previous paper at conference Cyberworlds 2018 for automatic border approximation of cutaneous melanoma and other skin lesions from macroscopic medical images. Given a set of feature points on the boundary of the skin lesion obtained by a dermatologist, we introduce a new method for automatic least-squares B-spline curve fitting of such feature points. The method is based on the original cuckoo search algorithm used in the conference paper but with three major modifications: (1) we use an enhanced version of the algorithm in which the parameters change dynamically with the generations; (2) this improved method is coupled with the Luus-Jaakola local search heuristics for better performance; (3) the original Bezier curves are now replaced by the more powerful and more general B-spline curves, providing extra flexibility and lower polynomial degree. The new method (called memetic improved cuckoo search algorithm) has been applied to a benchmark comprised of ten medical images of skin lesions. The computer results show that it performs very well and yields a border curve enclosing the lesion and fitting the feature points with good accuracy. Furthermore, a comparison with ten alternative methods in the literature (six standard mathematical methods for B-spline fitting, two state-of-the art methods in medical imaging, the method in our conference paper and the non-memetic version of our new method) shows that it outperforms all these methods in terms of numerical accuracy for the instances in our reference benchmark.																	1474-0346	1873-5320				JAN	2020	43								101005	10.1016/j.aei.2019.101005													
J								Two-stage stochastic programming model for generating container yard template under uncertainty and traffic congestion	ADVANCED ENGINEERING INFORMATICS										Yard template; Two-stage stochastic programming; GA-based framework; Uncertainty; Traffic congestion	OPERATIONS-RESEARCH; STORAGE STRATEGY; OPTIMIZATION; MANAGEMENT; TERMINALS; TIME	Yard template is a space assignment at the tactical level, which is kept unchanged within a long period of time and significantly impacts the handling efficiency of a container terminal. This paper addresses a yard template planning problem considering uncertainty and traffic congestion. A two-stage stochastic programming model is formulated for minimizing the risk of containers with no available slots in the designated yard area and minimizing total transportation distances. The first-stage model is formulated for assigning vessels in each block without considering the physical location properties of blocks, and the second-stage model is formulated for designating physical locations to all blocks. Subsequently, a solving framework based on genetic algorithm is proposed for solving the first-stage model, and the CPLEX (a commercial solver) is used for solving the second-stage model. Finally, numerical experiments and scenario analysis are conducted to validate the effectiveness of the proposed model and the efficiency of the proposed solution approach.																	1474-0346	1873-5320				JAN	2020	43								101032	10.1016/j.aei.2020.101032													
J								Smart control of the assembly process with a fuzzy control system in the context of Industry 4.0	ADVANCED ENGINEERING INFORMATICS										Assembly line balancing; Re-balancing; Industry 4.0; Fuzzy control system	BALANCING PROBLEM; LINES; ALGORITHM; INTERNET; THINGS; MODEL	Assembly line balancing is important for the efficiency of the assembly process, however, a wide range of disruptions can break the current workload balance. Some researchers explored the task assignment plan for the assembly line balancing problem with the assumption that the assembly process is smooth with no disruption. Other researchers considered the impacts of disruptions, but they only explored the task re-assignment solutions for the assembly line re-balancing problem with the assumption that the re-balancing decision has been made already. There is limited literature exploring on-line adjustment solutions (layout adjustment and production rate adjustment) for an assembly line in a dynamic environment. This is because real-time monitoring of an assembly process was impossible in the past, and it is difficult to incorporate uncertainty factors into the balancing process because of the randomness and non-linearity of these factors. However, Industry 4.0 breaks the information barriers between different parts of an assembly line, since smart, connected products, which are enabled by advanced information and communication technology, can intelligently interact and communicate with each other and collect, process and produce information. Smart control of an assembly line becomes possible with the large amounts of real-time production data in the era of Industry 4.0, but there is little literature considering this new context. In this study, a fuzzy control system is developed to analyze the real-time information of an assembly line, with two types of fuzzy controllers in the fuzzy system. Type 1 fuzzy controller is used to determine whether the assembly line should be re-balanced to satisfy the demand, and type 2 fuzzy controller is used to adjust the production rate of each workstation in time to eliminate blockage and starvation, and increase the utilization of machines. Compared with three assembly lines without the proposed fuzzy control system, the assembly line with the fuzzy control system performs better, in terms of blockage ratio, starvation ratio and buffer level. Additionally, with the improvement of information transparency, the performance of an assembly line will be better. The research findings shed light on the smart control of the assembly process, and provide insights into the impacts of Industry 4.0 on assembly line balancing.																	1474-0346	1873-5320				JAN	2020	43								101031	10.1016/j.aei.2019.101031													
J								Saliency detection analysis of collective physiological responses of pedestrians to evaluate neighborhood built environments	ADVANCED ENGINEERING INFORMATICS										Built environment assessment; Physiological response; Wearable sensing; Saliency detection; crowdsensing; Smart city	BIG DATA; GAIT; CONSTRUCTION; PATTERNS; DISORDER; WALKING; SYSTEM; MODEL	Crowdsourcing pedestrians' physiological responses (e.g., electrodermal activity (EDA), gait patterns, and blood volume pulse) offers a unique opportunity for assessing and maintaining built environments in a neighborhood. However, raw physiological signals acquired from naturalistic ambulatory settings cannot effectively capture prominent local patterns in the data stream, since diverse technical challenges (e.g., electrode contact noise and motion artifacts) and confounding factors (e.g., heightened physiology due to the movement) make it difficult to detect significant fine-grain signal fluctuations. Motivated by this issue, this paper proposes a method to identify physical disorders that cause pedestrians physical discomfort and/or emotional distress, by using saliency detection analysis on physiological responses. A bottom-up segmentation approach was used as an unsupervised way to divide each physiological signal into homogeneous segments. A physiological saliency cue (PSC) is proposed to calculate the distinctiveness of physiological responses over each segment in contrast to the remaining segments, and the collective PSC of a physical point of interest is computed across participants. The results, obtained from physiological signals collected from wearable devices, indicate that the suggested saliency detection analysis is effectual in capturing prominent local patterns. Our statistical analysis further indicates that the proposed PSC features can be indicative of physical disorders. The outcome of this research will provide a foundation towards using physiological signals to evaluate built environments, and towards promoting neighborhood walkability, increasing feelings of safety in the urban space, and augmenting residents' well-being.																	1474-0346	1873-5320				JAN	2020	43								101035	10.1016/j.aei.2020.101035													
J								Dynamic BIM component recommendation method based on probabilistic matrix factorization and grey model	ADVANCED ENGINEERING INFORMATICS										Building information modeling; Grey model; Prefabricated building; Probabilistic matrix factorization; Recommendation system	KNOWLEDGE MANAGEMENT; CONSTRUCTION; DESIGN	With rapid advances in building information modeling (BIM), a huge amount of BIM components has been built to increase design efficiency. Meanwhile, finding the appropriate BIM component in the huge library has become a challenge. Besides the methods of case-based reasoning (CBR) or multi-attribute decision model (MADM), the probabilistic matrix factorization (PMF) method of a recommendation system can be an efficient alternative. However, the user behavior patterns (i.e., the rating matrices) are changing with time to influence the recommendation precision. Therefore, this study aims to enhance the dynamic recommendation ability for BIM components by proposing a hybrid probabilistic matrix factorization method (PMF-GMn). The latent user preference matrix and the latent BIM component feature matrix can be generated by the PMF method from the rating matrix. Then, the predicted latent matrices can be obtained by the optimized grey model. Finally, the predicted latent matrices are further combined into the predicted rating matrix to recommend the appropriate BIM components. An illustrative example of the prefabricated building design is used to demonstrate the feasibility. This experiment is implemented by inviting twenty users to use the proposed SharePBIM platform for five months. The statistical results indicated that PMF-GMn can provide better performance than PMF in both two criteria of RMSE and Recall@k.																	1474-0346	1873-5320				JAN	2020	43								101024	10.1016/j.aei.2019.101024													
J								Stakeholder-oriented systematic design methodology for prognostic and health management system: Stakeholder expectation definition	ADVANCED ENGINEERING INFORMATICS										Prognostic and health management; Design methodology; Stakeholder-oriented; System engineering	OF-THE-ART; REQUIREMENTS; MAINTENANCE; MODEL; TAXONOMY; MBSE	Prognostic and health management (PHM) describes a set of capabilities that enable to detect anomalies, diagnose faults and predict remaining useful lifetime (RUL), leading to the effective and efficient maintenance and operation of assets such as aircraft. Prior research has considered the methodological factors of PHM system design, but typically, only one or a few aspects are addressed. For example, several studies address system engineering (SE) principles for application towards PHM design methodology, and a concept of requirements from a theoretical standpoint, while other papers present requirement specification and flow-down approaches for PHM systems. However, the state of the art lacks a systematic methodology that formulates all aspects of designing and comprehensively engineering a PHM system. Meanwhile, the process and specific implementation of capturing stakeholders' expectations and requirements are usually lacking details. To overcome these drawbacks, this paper proposes a stakeholder-oriented design methodology for developing a PHM system from a systems engineering perspective, contributing to a consistent and reusable representation of the design. Further, it emphasizes the process and deployment of stakeholder expectations definition in detail, involving the steps of identifying stakeholders, capture their expectations/requirements, and stakeholder and requirement analysis. Two case studies illustrate the applicability of the proposed methodology. The proposed stakeholder-oriented design methodology enables the integration of the bespoke main tasks to design a PHM system, in which sufficient stakeholder involvement and consideration of their interests can lead to more precise and better design information. Moreover, the methodology comprehensively covers the aspects of traceability, consistency, and reusability to capture and define stakeholders and their expectations for a successful design.																	1474-0346	1873-5320				JAN	2020	43								101041	10.1016/j.aei.2020.101041													
J								Smart work packaging-enabled constraint-free path re-planning for tower crane in prefabricated products assembly process	ADVANCED ENGINEERING INFORMATICS										Crane path planning; Smart work packaging; Prefabrication housing production; Constraints management; Building information modeling	CONSTRUCTION; VISUALIZATION; SIMULATION; MANAGEMENT; MODEL	Lack of constraint-free crane path planning is one of the critical concerns in the dynamic on-site assembly process of prefabrication housing production (PHP). For decades, researchers and practitioners have endeavored to improve both the efficiency and safety of crane path planning from either static environment or re-planning the path when colliding with constraints or periodically updating the path in the dynamic environment. However, there is a lack of approach related to the in-depth exploration of the nature of dynamic constraints so as to assist the crane operators in making adaptive path re-planning decisions by categorizing and prioritizing constraints. To address this issue, this study develops the smart work packaging (SWP)-enabled constraints optimization service. This service embraces the core characteristics of SWP, including adaptivity, sociability, and autonomy to achieve autonomous initial path planning, networked constraints classification, and adaptive decisions on path re-planning. This service is simulated and verified in the BIM environment, and it is found that SWP-enabled constraints optimization service can generate the constraint-free path when it is necessary.																	1474-0346	1873-5320				JAN	2020	43								101008	10.1016/j.aei.2019.101008													
J								Do people follow the crowd in building emergency evacuation? A cross-cultural immersive virtual reality-based study	ADVANCED ENGINEERING INFORMATICS										Fire evacuation; Building emergency; Crowd flow; Herding; Immersive virtual reality; Culture	EXIT CHOICE; INDIVIDUAL-DIFFERENCES; HERDING BEHAVIOR; FIRE EVACUATION; ENVIRONMENTS; ESCAPE; ABILITY; STRESS; RISK	This study aimed to examine the influence of crowd flow on human evacuation behavior during building fire emergencies, when evacuees perceive high uncertainty in the environment and experience mental stress. Evacuation experiments were conducted in an immersive virtual metro station, in which each participant was presented with one of three different patterns of crowd flow and asked to complete an evacuation task. The patterns of crowd flow were represented by non-player characters that split differently at each wayfinding decision point in the metro station. The experiments were conducted in Beijing, Los Angeles and London. The results showed that uneven splits of crowd flow motivated participants under mental stress to follow the majority of the crowd. This influence of crowd flow was generally consistent over the course of evacuation, and such consistency could be reinforced by stronger directional information conveyed by the crowd flow as well as positive feedback from the outcomes of previous wayfinding decisions. The results also indicated that the influence of crowd flow was significant in all three cultures represented by the three cities, however, the impact of culture on how participants would respond to the directional information conveyed by the crowd flow was insignificant.																	1474-0346	1873-5320				JAN	2020	43								101040	10.1016/j.aei.2020.101040													
J								How sustainable is smart PSS? An integrated evaluation approach based on rough BWM and TODIM	ADVANCED ENGINEERING INFORMATICS										Smart product-service systems; Sustainability assessment; Prospect theory; Behavioral decision making; Rough set theory	PRODUCT-SERVICE SYSTEMS; CONNECTED PRODUCTS; SET APPROACH; LIFE-CYCLE; DESIGN; DECISION; CHALLENGES; FRAMEWORK; BUSINESS; MODEL	Smart Product-Service Systems (PSS) integrates smart products and e-services into a total solution with Information and Communication Technology (ICT). It is necessary to assess the Smart PSS from the perspective of sustainability in the early design phase to reduce potential failure to meet the environmental and social requirements in delivery stage. However, the existing PSS evaluation frameworks consider less about characteristics of digitalization and smartness. Moreover, the previous criteria weighting methods for PSS evaluation becomes time consuming and frustrating when comparing too many criteria to determine the weights for criteria. In addition, the existing PSS evaluation methods often omit decision makers' bounded rationality and vagueness of judgements. To solve these problems, a novel method is proposed by integrating the merit of Best Worst Method (BWM) in reducing the decision makers' burden of pairwise comparisons of criteria importance, the strengths of TODIM (an acronym in Portuguese of Interactive and Multi-Criteria Decision Making) in dealing with decision makers' bounded rationality and the flexibility of Rough Set Theory (RST) in handling vagueness without prior information. Finally, a case study of sustainability evaluation for smart air-conditioner PSS is used to validate the effectiveness and efficiency of the proposed method.																	1474-0346	1873-5320				JAN	2020	43								101042	10.1016/j.aei.2020.101042													
J								An integrated decision-making method for product design scheme evaluation based on cloud model and EEG data	ADVANCED ENGINEERING INFORMATICS										Design scheme evaluation; Multi-criteria decision making; Cloud model; EEG	EMOTION RECOGNITION; SELECTION; CRITERIA	Selecting the optimal design scheme is a vital task in the product design area. It not only improves the performance of the product, but also leads to the greatest satisfaction of customers. However, existing methods express qualitative evaluation information roughly, and none of them has taken the implicit psychological states of customers into consideration. Therefore, an integrated decision-making method for product design scheme evaluation is proposed. This method applies the cloud model to facilitate the evaluation process of experts and uses the EEG data to reveal the psychological states of customers. Benefit from the probability theory and fuzzy set theory, the cloud model deals with the fuzziness and randomness simultaneously. It can decrease the cognitive discrepancy of experts and allow the information distortion to be neutralized to a great extent. Since the experts are not the final users of products, the evaluation results from experts cannot truly reflect the psychological states of customers when they use the product. An experiment is designed to collect the EEG data which can reveal the implicit psychological states of customers. The recorded data are segmented based on the operation process and tagged with the self-reported psychological states. Subsequently, the wavelet packet decomposition is applied and the sample entropy of each EEG frequency band is extracted as the feature. Taking advantage of the random forest classifier, the psychological states of customers can be classified with the average accuracy of 90.76%. This study can lead to a practical system for automatic assessment of psychological states in future applications. The evaluation process of elevator design schemes is conducted as a case study to illustrate the feasibility of the proposed method.																	1474-0346	1873-5320				JAN	2020	43								101028	10.1016/j.aei.2019.101028													
J								Integrated parametric multi-level information and numerical modelling of mechanised tunnelling projects	ADVANCED ENGINEERING INFORMATICS										Building information modelling; Industry foundation classes, mechanised tunnelling; Multi-level modelling; Numerical simulation; Visualisation	FINITE-ELEMENT SIMULATION; DESIGN; FRAMEWORK; MANAGEMENT; SUPPORT; SOIL	This paper presents a concept for parametric modelling of mechanized tunnelling within a state of the art design environment, as the basis for design assessments for different levels of details (LoDs). To this end, a parametric representation of each system component (soil with excavation, tunnel lining with grouting, Tunnel Boring Machine (TBM) and buildings) is developed in an information model for three LoDs (high, medium and low) and used for the automated generation of numerical models of the tunnel construction process and soil-structure interaction. The platform enables a flexible, user-friendly generation of the tunnel structure for arbitrary alignments based on predefined structural templates for each component, supporting the design process and at the same time providing an insight into the stability and safety of the design. This model, with selected optimal LoDs for each component, dependent on the objective of the analysis, is used for efficient design and process optimisation in mechanized tunnelling. Efficiency and accuracy are further demonstrated through an error-free exchange of information between Building Information Modelling (BIM) and the numerical simulation and with significantly reduced computational effort. The interoperability of the proposed multi-level framework is enabled through the use of an efficient multi-level representation context of the Industry Foundation Classes (IFC). The results reveal that this approach is a major step towards sensible modelling and numerical analysis of complex tunnelling project information at the early design stages.																	1474-0346	1873-5320				JAN	2020	43								101011	10.1016/j.aei.2019.101011													
J								Convolutional neural networks for object detection in aerial imagery for disaster response and recovery	ADVANCED ENGINEERING INFORMATICS										Disaster management; Convolutional neural network (CNN); Aerial reconnaissance; Deep learning; Unmanned aerial vehicle (UAV)		Accurate and timely access to data describing disaster impact and extent of damage is key to successful disaster management (a process that includes prevention, mitigation, preparedness, response, and recovery). Airborne data acquisition using helicopter and unmanned aerial vehicle (UAV) helps obtain a bird's-eye view of disaster-affected areas. However, a major challenge to this approach is robustly processing a large amount of data to identify and map objects of interest on the ground in real-time. The current process is resource-intensive (must be carried out manually) and requires offline computing (through post-processing of aerial videos). This research introduces and evaluates a series of convolutional neural network (CNN) models for ground object detection from aerial views of disaster's aftermath. These models are capable of recognizing critical ground assets including building roofs (both damaged and undamaged), vehicles, vegetation, debris, and flooded areas. The CNN models are trained on an in-house aerial video dataset (named Volan2018) that is created using web mining techniques. Volan2018 contains eight annotated aerial videos (65,580 frames) collected by drone or helicopter from eight different locations in various hurricanes that struck the United States in 2017-2018. Eight CNN models based on You-Only-Look-Once (YOLO) algorithm are trained by transfer learning, i.e., pre-trained on the COCO/VOC dataset and re-trained on Volan2018 dataset, and achieve 80.69% mAP for high altitude (helicopter footage) and 74.48% for low altitude (drone footage), respectively. This paper also presents a thorough investigation of the effect of camera altitude, data balance, and pre-trained weights on model performance, and finds that models trained and tested on videos taken from similar altitude outperform those trained and tested on videos taken from different altitudes. Moreover, the CNN model pre-trained on the VOC dataset and re-trained on balanced drone video yields the best result in significantly shorter training time.																	1474-0346	1873-5320				JAN	2020	43								101009	10.1016/j.aei.2019.101009													
J								Data mining for recognition of spatial distribution patterns of building heights using airborne lidar data	ADVANCED ENGINEERING INFORMATICS										GIS; Digital Building Model; Spatial statistics; Spatial patterns; Spatial data mining; Kernel density; Spatial big data	DENSITY RESIDENTIAL DEVELOPMENTS; HEAT-ISLAND INTENSITY; POINT CLOUDS; URBAN; GIS; CLASSIFICATION; EXTRACTION; HIGHRISE; SYSTEM; MODELS	There is an increasing demand for spatial big data visualisation in Geographic Information Systems (GIS) in building construction and urban development. Exploring building height patterns is required to obtain and visualize essential information about spatio-temporal vertical urban developments due to the trends towards increasing building heights in different urban fabrics. While metrics to characterize horizontal patterns of urban fabric using spectral information exist, theoritical-based metrics identifying the patterns of vertical urban developments using height values are still scarce. In addition, there is a lack of reliable methods to analyze height information for modeling the distribution of building heights and to automatically detect three-dimensional urban patterns. In this paper, we propose to apply the spatial statistics of Local Moran's I (LMI), G(i)* and Kernel Density Estimation (KDE) on building heights to explore vertical urban patterns through detecting the concentration of relatively higher buildings. The proposed methods were applied on two different airborne lidar point cloud data sets. The results show overall good performance of LMI and Gi* methods compared to KDE. It is also found that there is a higher level of agreement between clusters of relatively higher buildings derived by the autocorrelation statistics of LMI and Gi*, compared with the patterns derived from the Kernel density. For the lower accuracies obtained from the KDE, the authors suggest to use either LMI or Gi* for this kind of study. The spatial closeness of clusters of higher buildings to major roads, defined by the mean distances of the clusters to major roads, were investigated and based on the Analysis of Variance (ANOVA) and Tukey's tests, the mean distances were found to be shorter than for all other buildings. Lastly, an analysis of clusters of the relatively higher buildings showed varying land uses for the two case studies.																	1474-0346	1873-5320				JAN	2020	43								101033	10.1016/j.aei.2020.101033													
J								Information requirements for multi-level-of-development BIM using sensitivity analysis for energy performance	ADVANCED ENGINEERING INFORMATICS										multi-LOD modelling; Variance-based sensitivity analysis; Uncertainty; Architectural design; Building energy model	UNCERTAINTY ANALYSIS; DESIGN; MODEL; SIMULATION; REGRESSION	The concept of multi-Level-of-Development (multi-LOD) modelling represents a flexible approach of information management and compilation in building information modelling (BIM) on a set of consistent levels. From an energy perspective during early architectural design, the refinement of design parameters by addition of information allows a more precise prediction of building performance. The need for energy-efficient buildings requires a designer to focus on the parameters in order of their ability to reduce uncertainty in energy performance to prioritise energy relevant decisions. However, there is no method for assigning and prioritising information for a particular level of multi-LOD. In this study, we performed a sensitivity analysis of energy models to estimate the uncertainty caused by the design parameters in energy prediction. This study allows to rank the design parameters in order of their influence on the energy prediction and determine the information required at each level of multi-LOD approach. We have studied the parametric energy model of different building shapes representing architectural design variation at the early design stage. A variance-based sensitivity analysis method is used to calculate the uncertainty contribution of each design parameter. The three levels in the uncertainty contribution by the group of parameters are identified which form the basis of information required at each level of multi-LOD BIM approach. The first level includes geometrical parameters, the second level includes technical specification and operational design parameters, and the third level includes window construction and system efficiency parameters. These findings will be specifically useful in the development of a multi-LOD approach to prioritise performance relevant decisions at early design phases.																	1474-0346	1873-5320				JAN	2020	43								101026	10.1016/j.aei.2019.101026													
J								Intelligent compilation of patent summaries using machine learning and natural language processing techniques	ADVANCED ENGINEERING INFORMATICS										Artificial intelligence; Machine learning; Natural language processing; Deep learning; Patent analysis		Patents are a type of intellectual property with ownership and monopolistic rights that are publicly accessible published documents, often with illustrations, registered by governments and international organizations. The registration allows people familiar with the domain to understand how to re-create the new and useful invention but restricts the manufacturing unless the owner licenses or enters into a legal agreement to sell ownership of the patent. Patents reward the costly research and development efforts of inventors while spreading new knowledge and accelerating innovation. This research uses artificial intelligence natural language processing, deep learning techniques and machine learning algorithms to extract the essential knowledge of patent documents within a given domain as a means to evaluate their worth and technical advantage. Manual patent abstraction is a time consuming, labor intensive, and subjective process which becomes cost and outcome ineffective as the size of the patent knowledge domain increases. This research develops an intelligent patent summarization methodology using artificial intelligence machine learning approaches to allow patent domains of extremely large sizes to be effectively and objectively summarized, especially for cases where the cost and time requirements of manual summarization is infeasible. The system learns to automatically summarize patent documents with natural language texts for any given technical domain. The machine learning solution identifies technical key terminologies (words, phrases, and sentences) in the context of the semantic relationships among training patents and corresponding summaries as the core of the summarization system. To ensure the high performance of the proposed methodology, ROUGE metrics are used to evaluate precision, recall, accuracy, and consistency of knowledge generated by the summarization system. The Smart machinery technologies domain, under the sub-domains of control intelligence, sensor intelligence and intelligent decision-making provide the case studies for the patent summarization system training. The cases use 1708 training pairs of patents and summaries while testing uses 30 randomly selected patents. The case implementation and verification have shown the summary reports achieve 90% and 84% average precision and recall ratios respectively.																	1474-0346	1873-5320				JAN	2020	43								101027	10.1016/j.aei.2019.101027													
J								A smart surface inspection system using faster R-CNN in cloud-edge computing environment	ADVANCED ENGINEERING INFORMATICS										Automated surface inspection; Smart product-service system; Convolutional neural networks; Cloud-edge computing	PRODUCT-SERVICE SYSTEMS; DESIGN; MODEL	Automated surface inspection has become a hot topic with the rapid development of machine vision technologies. Traditional machine vision methods need experts to carefully craft image features for defect detection. This limits their applications to wider areas. The emerging convolutional neural networks (CNN) can automatically extract features and yield good results in many cases. However, the CNN-based image classification methods are more suitable for flat surface texture inspection. It is difficult to accurately locate small defects in geometrically complex products. Furthermore, the computational power required in CNN algorithms is usually high and it is not efficient to be implemented on embedded hardware. To solve these problems, a smart surface inspection system is proposed using faster R-CNN algorithm in the cloud-edge computing environment. The faster R-CNN as a CNN-based object detection method can efficiently identify defects in complex product images and the cloud-edge computing framework can provide fast computation speed and evolving algorithm models. A real industrial case study is presented to illustrate the effectiveness of the proposed method. The results show that the proposed method can provide high detection accuracy within a short time.																	1474-0346	1873-5320				JAN	2020	43								101037	10.1016/j.aei.2020.101037													
J								Photo-realistic visualization of seismic dynamic responses of urban building clusters based on oblique aerial photography	ADVANCED ENGINEERING INFORMATICS										Visualization; Photo-realistic; Oblique aerial photography; Seismic dynamic response; City-scale nonlinear time-history analysis	EARTHQUAKE; DAMAGE; SIMULATION; PHOTOGRAMMETRY; MODEL	Highly realistic visualizations of seismic dynamic responses of building clusters are critical for earthquake safety education. To this end, a photo-realistic visualization method of the seismic dynamic responses of urban building clusters is proposed based on oblique aerial photography. Specifically, a sparsification algorithm of aerial photograph footprints and the model optimization solutions are designed to reduce the size of a city model reconstructed by oblique aerial photography. A building segmentation algorithm based on Boolean operations and building footprints is designed to separate buildings from a reconstructed three-dimensional city model. A visualization algorithm for the seismic dynamic responses of building clusters is designed based on the Callback mechanism, by which the shaking process of building clusters can be realistically displayed according to the results of a city-scale nonlinear time-history analysis. New Beichuan City in China is adopted as a case study to visualize seismic dynamic response. The visualization produced by the proposed method is more realistic than that of the finite element method and can support decision making on earthquake safety actions. The outcome of this study provides well-founded and photo-realistic scenes of the seismic dynamic response of building clusters and has promising application prospects for earthquake safety education.																	1474-0346	1873-5320				JAN	2020	43								101025	10.1016/j.aei.2019.101025													
J								An improved iterative stochastic multi-objective acceptability analysis method for robust alternative selection in new product development	ADVANCED ENGINEERING INFORMATICS										Alternative selection in NPD; Multi-criteria decision-making; Stochastic multi-objective acceptability analysis; Ranking sensitivity analysis	DECISION-MAKING; MODEL; RANKING; SMAA	Alternative selection in new product development (NPD) is a multi-criteria decision-making (MCDM) problem. It usually starts with incomplete, imprecise or even partially missing information. Currently, most existing methods in dealing with this problem cannot work well if required information is incomplete or missing. It is acknowledged that stochastic multi-objective acceptability analysis (SMAA) can be applied to address MCDM problem with incomplete preference information and uncertain criteria measurements. In SMAA, alternatives are evaluated based on SMAA measurements (acceptability index, central weight vector and confidence factor). The discriminability of SMAA for the optimum alternative heavily depends on differences of SMAA measurements among different alternatives. Usually, a large number of alternatives and high level of uncertainty are involved in alternative selection in NPD. In this situation, the differences among SMAA measurements are not obvious, and therefore SMAA cannot deal with such problem very well. To this end, this paper proposes an improved SMAA method called Iterative-SMAA (I-SMAA) for alternative selection in NPD. In the I-SMAA, an iterative multi-step decision-making process is suggested to improve differences of SMAA measurements among different alternatives, and thus assist decision makers (DMs) to positively discern from the most preferred alternative. To enhance the decision-making efficiency, sensitive criteria are acquired in each iteration by ranking sensitivity analysis. DMs are guided to provide partial preference information and give more accurate criteria measurements for sensitive criteria rather than all criteria. Eventually, to verify the proposed method, a numerical example of the existing literature is solved with the method, and the results are compared. And then, a practical example of a preparation equipment for coal samples is further employed to verify the practicability of the proposed I-SMAA.																	1474-0346	1873-5320				JAN	2020	43								101038	10.1016/j.aei.2020.101038													
J								An operation synchronization model for distribution center in E-commerce logistics service	ADVANCED ENGINEERING INFORMATICS										E-commerce logistics service; Distribution center; Operation synchronization; Synchronization measurement; Punctuality	DOCK ASSIGNMENT PROBLEM; SCHEDULING PROBLEM; OUTBOUND TRUCKS; TIME CONSTRAINT; CROSSDOCKING; MAKESPAN	This paper is among the first that proposes a synchronization measurement model for the distribution centre operation synchronization (DCOS) problem, which aims to ensure the E-commerce order's punctuality and synchronization at the same time. The main motivation of DCOS is that the intensified competition in E-commerce market makes efficient E-commerce logistics service extremely important, which means saving logistics cost and ensuring customer service at the same time. The synchronized operation may be a possible solution to ensure efficient order transhipment in the distribution center and to save cost. We thus introduce a measurement approach that is able to address the distribution center operation synchronization (DCOS) problem such as the trade-off relationship between synchronization and punctuality. In order to get persuasive conclusions, we adopt data from a real practice case and apply CPLEX to get the optimal solution. Our computational results show that considering the asynchronous cost in the total cost objective function will greatly improve the operation synchronization in the distribution center, by saving the storage space, the equipment, and the labour resources. And if the storage cost is in a reasonable range, the synchronized operation can be realized while the punctuality is also optimized. It is found in our case that the most efficient way to improve distribution center operation is expanding inbound operation capacity.																	1474-0346	1873-5320				JAN	2020	43								101014	10.1016/j.aei.2019.101014													
J								Onset detection of ultrasonic signals for the testing of concrete foundation piles by coupled continuous wavelet transform and machine learning algorithms	ADVANCED ENGINEERING INFORMATICS										Concrete pile foundation; Ultrasonic signals; Onset detection; Continuous wavelet transform; Machine learning; Error analysis	ACOUSTIC-EMISSION; NUMERICAL-SIMULATION; TIME; CLASSIFICATION; PREDICTION; CONSTRUCTION; LOCATION; SULFATE; PICKER; DEFECT	The construction of ultra-high-rise and long-span structures requires higher requirements for the integrity detection of piles. The acoustic signal detection has been verified an efficient and accurate nondestructive testing method. In fact, the integrity of piles is closely related to the onset time of signals. The accuracy of onset time directly affects the integrity evaluation of a pile. To achieve high-precision onset detection, continuous wavelet transform (CWT) preprocessing and machine learning algorithms were integrated into the software of high-sampling rate testing equipment. The distortion of waveforms, which could interfere with the accuracy of detection, was eliminated by CWT preprocessing. To make full use of the collected waveform data, three types of machine learning algorithms were used for classifying whether the data points are ambient or ultrasonic signals. The models involve a commonly used classifier (ELM), an individual classification tree model (DTC), an ensemble tree model (RFC) and a deep learning model (DBN). The classification accuracy of the ambient and ultrasonic signals of these models was compared by 5-fold validation. Results indicate that RFC performance is better than DBN and DTC after training. It is more suitable for the classification of points in waveforms. Then, a detection method of onset time based on classification results was therefore proposed to minimize the interference of classification errors on detection. In addition to the three data mining methods, the autocorrelation function method was selected as the control method to compare the proposed data mining based methods with the traditional one. The accuracy and error analysis of 300 waveforms proved the feasibility and stability of the proposed method. The RFC-based detection method is recommended because of the highest accuracy, lowest errors, and the most favorable error distribution among four onset detection methods. Successful applications demonstrate that it could provide a new way for ensuring the accurate testing of pile foundation integrity.																	1474-0346	1873-5320				JAN	2020	43								101034	10.1016/j.aei.2020.101034													
J								Smart concept design based on recessive inheritance in complex electromechanical system	ADVANCED ENGINEERING INFORMATICS										Complex electromechanical system; Smart concept design; The recessive inheritance; Singular phenomena		Due to the substantial increase in users demand, the scale of complex electromechanical systems is rapidly enlarging when systems are rapidly merging, and the complexity of the system has been greatly increased. Even if the original system is running well, singular phenomena often occurs in the process of complex electromechanical systems integration, which leads to the failure of the functional requirements of the new generation of products. In the face of the singular phenomena of complex electromechanical systems, traditional solutions focus more on solving the problems existing in the current complex electromechanical systems, and do not deeply study the root causes of the singular phenomena in the design process of complex electromechanical systems. The singular phenomena of system has high concealment and is not easy to be detected at the design stage, and similar singular phenomena will appear repeatedly in multiple generations of the same product. These characteristics are very similar to the characteristics of recessive inheritance of biological systems. Therefore, the paper will compare complex electromechanical systems with biological systems, and propose a recessive inheritance mechanism of complex electromechanical systems that can be used in smart concept design by introducing the recessive inheritance mechanism of biological system into the complex electromechanical system design process. First, use function trimming to build a functional gene for new complex electromechanical systems. Second, combine the Length and Time dimension chart (L-T chart) and Computer Aided Innovation (CAI) to find the recessive parameters that may exist in the design of the new generation system and analyze the possible coupling phenomena between the recessive parameters. Then, using the Invention Problem Solving Theory (TRIZ) tools to solve the coupling relationship between recessive parameters to reduce the possibility of singular phenomena in the new generation system, forming a new generation of complex electromechanical system smart concept design theory framework. Finally, for the design example of the new generation of energy-saving surface platform, the proposed method is used to determine the design scheme, establish a 3D model and verify the feasibility and scientificity of the theory by Ansys analysis.																	1474-0346	1873-5320				JAN	2020	43								101010	10.1016/j.aei.2019.101010													
J								Healthcare service configuration based on project scheduling	ADVANCED ENGINEERING INFORMATICS										Healthcare service configuration; Project scheduling; Bi-level genetic algorithm	GENETIC ALGORITHM; PRODUCT CONFIGURATION; META-HEURISTICS; PARTICLE SWARM; CLASSIFICATION; OPTIMIZATION; DESIGN; METAHEURISTICS; INTEGRATION; RESOURCES	Configuration has been recognized as an effective methodology to provide high product variety that caters to individual customer's needs in the manufacturing industry. For healthcare service configuration, this decision-making process can be taken as service package creation and treated analogously as a project, which is defined as a collection of tasks. This research develops a decision support model to integrate individual patients into the healthcare service configuration process. The healthcare service configuration is formulated as a Resource Constrained Project Scheduling Problem (RCPSP), and a bi-level optimization algorithm based on Genetic Algorithm (GA) is developed for problem solving. The methodology and algorithm are implemented with a case study based on the data obtained from a general hospital in Singapore, which has demonstrated the applicability of healthcare service configuration.																	1474-0346	1873-5320				JAN	2020	43								101039	10.1016/j.aei.2020.101039													
J								IoT edge computing-enabled collaborative tracking system for manufacturing resources in industrial park	ADVANCED ENGINEERING INFORMATICS										Collaborative tracking; Edge computing; Data processing; IoT; Manufacturing resources; Industrial park	CLOUD; INTERNET; SERVICE; THINGS	In manufacturing industry, the movement of manufacturing resources in production logistics often affects the overall efficiency. This research is motivated by a world-leading air-conditioner manufacturer. In order to provide the right manufacturing resources for subsequent production steps, excessive time and human effort has been consumed in locating the manufacturing resources in a huge industrial park. The development of Internet of Things (IoT) has made a profound impact on establish smart manufacturing workshop and tracking applications, however a growing trend of data quantity that generated from massive, heterogeneous and bottomed manufacturing resources objects pose challenge to centralized decision. In this study, the concept of edge-computing deeply integrated in collaborative tracking purpose in virtue of IoT technology. An IoT edge computing enabled collaborative tracking architecture is developed to offload the computation pressure and realize distributed decision making. A supervised learning of genetic tracking method is innovatively presented to ensure tracking accuracy and effectiveness. Finally, the research output is developed and implemented in a real-life industrial park for verification. The results show that the proposed tracking method not only performs constant improving accuracy up to 96.14% after learning compared to other tracking method, but also ensure quick responsiveness and scalability.																	1474-0346	1873-5320				JAN	2020	43								101044	10.1016/j.aei.2020.101044													
J								Deep learning-based extraction of construction procedural constraints from construction regulations	ADVANCED ENGINEERING INFORMATICS										Construction procedural constraints; Regulation information extraction; Deep learning; Long short-term memory; Construction procedure relation classification	AUTOMATED INFORMATION EXTRACTION; NEURAL-NETWORKS; MANAGEMENT; INSPECTION; SYSTEM; MODEL; FALLS	Construction procedural constraints are critical in facilitating effective construction procedure checking in practice and for various inspection systems. Nowadays, the manual extraction of construction procedural constraints is costly and time-consuming. The automatic extraction of construction procedural constraint knowledge (e.g., knowledge entities and interlinks/relationships between them) from regulatory documents is a key challenge. Traditionally, natural language processing is implemented using either rule-based or machine learning approaches. Limited efforts on rule-based extraction of construction regulations often rely on pre-defined vocabularies and involve heavy feature engineering. Based on characteristics of the knowledge expression of construction procedural constraints in Chinese regulations, this paper explores a hybrid deep neural network, combining the bidirectional long short-term memory (Bi-LSTM) and the conditional random field (CRF), for the automatic extraction of the qualitative construction procedural constraints. Based on the proposed deep neural network, the recognition and extraction of named entities and relations between them are realized. Unlike existing information extraction research efforts using rule-based methods, the proposed hybrid deep learning approach can be applied without complex handcrafted features engineering. Besides, the long distance dependency relationships between different entities in regulations are considered. The model implementation results demonstrate the good performance of the end-to-end deep neural network in the extraction of construction procedural constraints. This study can be considered as one of the early explorations of knowledge extraction from construction regulations.																	1474-0346	1873-5320				JAN	2020	43								101003	10.1016/j.aei.2019.101003													
J								Ensemble data mining modeling in corrosion of concrete sewer: A comparative study of network-based (MLPNN & RBFNN) and tree-based (RF, CHAID, & CART) models	ADVANCED ENGINEERING INFORMATICS										Concrete corrosion; Machine learning; Soft computing; Sewer systems; Artificial intelligence	ARTIFICIAL NEURAL-NETWORKS; CLASSIFICATION; DETERIORATION; PREDICTION; PIPES	This research aims to evaluate ensemble learning (bagging, boosting, and modified bagging) potential in predicting microbially induced concrete corrosion in sewer systems from the data mining (DM) perspective. Particular focus is laid on ensemble techniques for network-based DM methods, including multi-layer perceptron neural network (MLPNN) and radial basis function neural network (RBFNN) as well as tree-based DM methods, such as chi-square automatic interaction detector (CHAID), classification and regression tree (CART), and random forests (RF). Hence, an interdisciplinary approach is presented by combining findings from material sciences and hydrochemistry as well as data mining analyses to predict concrete corrosion. The effective factors on concrete corrosion such as time, gas temperature, gas-phase H2S concentration, relative humidity, pH, and exposure phase are considered as the models' inputs. All 433 datasets are randomly selected to construct an individual model and twenty component models of boosting, bagging, and modified bagging based on training, validating, and testing for each DM base learners. Considering some model performance indices, (e.g., Root mean square error, RMSE; mean absolute percentage error, MAPE; correlation coefficient, r) the best ensemble predictive models are selected. The results obtained indicate that the prediction ability of the random forests DM model is superior to the other ensemble learners, followed by the ensemble Bag-CHAID method. On average, the ensemble tree-based models acted better than the ensemble network-based models; nevertheless, it was also found that taking the advantages of ensemble learning would enhance the general performance of individual DM models by more than 10%.																	1474-0346	1873-5320				JAN	2020	43								101030	10.1016/j.aei.2019.101030													
J								Dynamic monitoring of software use with recurrent neural networks	DATA & KNOWLEDGE ENGINEERING										Recurrent neural networks; LSTM; Action embeddings; Action representation; Next action prediction; Crash monitoring; Medical imaging software		User interaction with a software may be formalized as a sequence of actions. In this paper we propose two methods - based on different representations of input actions - to address two distinct industrial issues: next action prediction and software crash risk detection. Both methods take advantage of the recurrent structure of Long Short Term Memory neural networks to capture dependencies among our sequential data as well as their capacity to potentially handle different types of input representations for the same data. Given the history of user actions in the interface, our first method aims at predicting the next action. The proposed recurrent neural network outperforms state-of-the-art proactive user interface algorithms with standard one-hot vectors as inputs. Besides, we propose to feed the LSTM with actions embeddings. This continuous representation performs better than one-hot encoded vector LSTM and its lower dimension reduces at the same time the computational cost. Using the same data set, the second method aims at crash risk detection. To address this task, we propose to use feature vectors composed of actions with above average crash probabilities as inputs of the LSTM - with the idea to take advantage of its ability to learn relevant past information to detect crash patterns. The method outperforms state-of-the-art sequence classification methods. Our approaches are demonstrated on medical imaging software logs from ten different hospitals worldwide, though they might be applied to various user interfaces in a wide range of applications.																	0169-023X	1872-6933				JAN	2020	125								101781	10.1016/j.datak.2019.101781													
J								An overlapping community detection algorithm based on rough clustering of links	DATA & KNOWLEDGE ENGINEERING										Complex networks; Community structure; Overlapping communities; Rough sets; Clustering	COMPLEX NETWORKS	The growth of networks is prevalent in almost every field due to the digital transformation of consumers, business and society at large. The unfolding of community structure in such real-world complex networks is crucial since it aids in gaining strategic insights leading to informed decisions. Moreover, the co-occurrence of disjoint, overlapping and nested community patterns in such networks demands methodologically rigorous community detection algorithms so as to foster cumulative tradition in data and knowledge engineering. In this paper, we introduce an algorithm for overlapping community detection based on granular information of links and concepts of rough set theory. First, neighborhood links around each pair of nodes are utilized to form initial link subsets. Subsequently, constrained linkage upper approximation of the link subsets is computed iteratively until convergence. The upper approximation subsets obtained during each iteration are constrained and merged using the notion of mutual link reciprocity. The experimental results on ten real-world networks and comparative evaluation with state-of-the-art community detection algorithms demonstrate the effectiveness of the proposed algorithm.																	0169-023X	1872-6933				JAN	2020	125								101777	10.1016/j.datak.2019.101777													
J								Utilizing adjacency of colleagues and type correlations for enhanced link prediction	DATA & KNOWLEDGE ENGINEERING										Heterogeneous information networks; Link prediction; Graph mining	NETWORKS	Discoveries of new relationships in the network of objects have been required in various applications such as social networks, DBLP bibliographic networks and biological networks. Specifically, link prediction in heterogeneous information networks (HINs) that consist of multiple types of nodes and links has received much attention recently because many information networks of the real world are HINs. We observe various factors that affect the existence of a link in HINs. Firstly, certain structural characteristics of nodes whose types are the same as that of a source (or target) node give important information for link prediction. Secondly, in the HINs, there can be meaningful correlation between links of a particular link type and paths of a particular path type (also called a meta-path). In other words, paths of different path types affect the existence of links differently. Finally, we use the number of paths between source and target nodes to measure proximity of two nodes. Based on these observations, we newly propose several features and a prediction model. We show through various experiments that our proposed method works effectively and performs better than the other existing methods.																	0169-023X	1872-6933				JAN	2020	125								101785	10.1016/j.datak.2019.101785													
J								Using similarity measures in prediction of changes in financial market stream data-Experimental approach	DATA & KNOWLEDGE ENGINEERING										Similarity measures; Stream data; Data prediction; Financial data	FUZZY TIME-SERIES; PATTERN-RECOGNITION; CLASSIFICATION	In this study, we experimentally investigated the possibilities of using selected similarity measures for predicting future price directions in the market. The basic premise for this approach was the common assumption relating to the technical analysis, namely that "history repeats itself," and the "instrument price reflects all factors that have an impact on its value." This approach has been studied extensively in many publications. We purport that the subjective interpretation of the chart by the decision-maker should be taken into account. As every decision in the market in the case of manual trading or decision support systems is eventually made by a human, it is necessary to emphasize that the same situation in the market may be interpreted in a different manner by two different decision-makers. Our goal is to use the proposed similarity measure to identify past situations that occurred in the market, and invest accordingly. Under these assumptions, we tested the usefulness of selected measures proposed in the literature, as well as the measure proposed by us, on 21 financial instrument datasets divided into three groups (stock companies, currency pairs, and stock indexes). Moreover, we statistically verified the prediction efficiency for different financial instruments, including stocks, currency pairs, and stock indexes. The statistical verification demonstrated that the proposed approach exhibited higher predictive strength than the classical measures proposed in the literature.																	0169-023X	1872-6933				JAN	2020	125								101782	10.1016/j.datak.2019.101782													
J								A knowledge representation of the beginning of the innovation process: The Front End of Innovation Integrative Ontology (FEI2O)	DATA & KNOWLEDGE ENGINEERING										Front end of innovation; Ontology; Entrepreneurship; Concept development; Design science	SCIENCE RESEARCH; DESIGN SCIENCE; PERSPECTIVE; PERFORMANCE; TECHNOLOGY; MANAGEMENT; SUPPORT	The initial phase of the innovation process is widely accepted as an important driver of positive results for new products and for the success of businesses. The Front End of Innovation (FEI) is a multidisciplinary area that includes a variety of activities, such as ideation, opportunity identification and analysis, feasibility analysis, global trends analysis, concept definition, customer and competitor analysis, and even business model development. Due to the number and variety of FEI responsibilities, this phase entails a considerable level of complexity and decision making. This fact is reflected in the literature, where one finds a variety of FEI approaches and proposals, seldom overlapping and offering no clear consensual guidance. This work aimed at overcoming this gap by proposing an Ontology for the Front End of Innovation as a comprehensive knowledge representation of the FEI, the so-called Front End of Innovation Integrative Ontology (FEI2O). The ontology balanced the differences and addressed the shortcomings of the main FEI Reference Models and included contributions from the field. This research builds on a combination of qualitative and quantitative methodologies. It combines the qualitative methods of interviewing and focus group discussion to collect the views of domain experts, used to refine the artefact and later to evaluate the final ontology. Quantitative analysis of data was carried out using the Attribute Agreement approach. The FEI2O explicitly provides a description of a domain regarding concepts, properties and relations of concepts. The main benefit of the FEI2O is to provide a comprehensive formal reference model and a common vocabulary.																	0169-023X	1872-6933				JAN	2020	125								101760	10.1016/j.datak.2019.101760													
J								Compressive approaches for cross-language multi-document summarization	DATA & KNOWLEDGE ENGINEERING										Cross-language text summarization; Sentence compression; Multi-sentence compression; Optimization	SENTENCE	The popularization of social networks and digital documents has quickly increased the multilingual information available on the Internet. However, this huge amount of data cannot be analyzed manually. This paper deals with Cross-Language Text Summarization (CLTS) that produces a summary in a different language from the source documents. We describe three compressive CLTS approaches that analyze the text in the source and target languages to compute the relevance of sentences. Our systems compress sentences at two levels: clusters of similar sentences are compressed using a multi-sentence compression (MSC) method and single sentences are compressed using a Neural Network model. The version of our approach using multi-sentence compression generated more informative French-to-English cross-lingual summaries than extractive state-of-the-art systems. Moreover, these cross-lingual summaries have a grammatical quality similar to extractive approaches.																	0169-023X	1872-6933				JAN	2020	125								101763	10.1016/j.datak.2019.101763													
J								PRIMULE: Privacy risk mitigation for user profiles	DATA & KNOWLEDGE ENGINEERING										Mobile phone data; Call detail record; Privacy; Anonymization		The availability of mobile phone data has encouraged the development of different data-driven tools, supporting social science studies and providing new data sources to the standard official statistics. However, this particular kind of data are subject to privacy concerns because they can enable the inference of personal and private information. In this paper, we address the privacy issues related to the sharing of user profiles, derived from mobile phone data, by proposing PRIMULE, a privacy risk mitigation strategy. Such a method relies on PRUDEnce (Pratesi et al., 2018), a privacy risk assessment framework that provides a methodology for systematically identifying risky-users in a set of data. An extensive experimentation on real-world data shows the effectiveness of PRIMULE strategy in terms of both quality of mobile user profiles and utility of these profiles for analytical services such as the Sociometer (Furletti 'et al., 2013), a data mining tool for city users classification.																	0169-023X	1872-6933				JAN	2020	125								101786	10.1016/j.datak.2019.101786													
J								Learning soft domain constraints in a factor graph model for template-based information extraction	DATA & KNOWLEDGE ENGINEERING										Template-based information extraction; Slot-filling; Probabilistic graphical models; Learning domain constraints; Database population		The ability to accurately extract key information from textual documents is necessary in several downstream applications e.g., automatic knowledge base population from text, semantic information retrieval, question answering, or text summarization. However, information extraction (IE) systems are far from being errorless and in some cases commit errors that seem obvious to a human expert as they violate common sense or domain knowledge. Towards improving the performance of IE systems, we focus on the question of how domain knowledge can be incorporated into IE models to reduce the number of spurious extractions. Starting from the assumption that such domain knowledge cannot be incorporated explicitly and manually by domain experts due to the amount of effort and technical complexities involved, we propose a machine learning approach in which domain constraints are acquired as a byproduct of learning a model that learns to extract key information in a supervised setting. We frame the task as a template-based information extraction problem in which several dependent slots need to be automatically filled and propose a factor graph based approach to model the joint distribution of slot assignments given a text. Beyond using standard textual features in factors that score the compatibility of slot fillers in relation to the text, we use additional features that are text-independent and capture soft domain constraints. During the training process, these constraints receive a weight as part of the parameter learning process indicating how strongly a constraint should be enforced. These domain constraints are thus 'soft' in the sense that they can be violated, but the system learns to penalize solutions that violate them. The soft constraints we introduce come in two flavors: on the one hand we incorporate information about the mean of numerical attributes and use features that indicate how far a certain value is from the mean. We call these features single slot soft constraints. On the other hand, we model the pairwise compatibility between slot filler assignments independent of the textual context, thus modeling the (domain) compatibility of the slot assignments, We call the latter ones pairwise slot soft constraints. As main result of our work, we show that learning pairwise slot soft constraints improves the performance of our extraction model compared to single slot soft constraints by up to 6 points in F-1, leading to an F-1 score of 0.91 for individual template types. Further, the human readable output format of our model enables the extraction and interpretation of the learned soft constraints. Based on this, we show in an evaluation by domain experts that more than 68% of the learned soft constraints are regarded as plausible.																	0169-023X	1872-6933				JAN	2020	125								101764	10.1016/j.datak.2019.101764													
J								Building social networking services systems using the relational shared-nothing parallel DBMS	DATA & KNOWLEDGE ENGINEERING												We propose methods to enable the relational model to meet scalability and functionality needs of a large-scale social networking services (SNS) system. NewSQL has emerged recently indicating that shared-nothing parallel relational DBMSs can be used to guarantee the ACID properties of transactions while keeping the high scalability of NoSQL. Leading commercial SNS systems, however, rely on a graph - not relational - data model with key-value storage and, for certain operations, suffer overhead of unnecessarily accessing multiple system nodes. Exploiting higher semantics with the relational data model could be the remedy. The solution we offer aims to perform a transaction as a set of independent local transactions whenever possible based on the conceptual semantics of the SNS database schema. First, it hierarchically clusters entities that are sitting on a path of frequently navigated one-to-many relationships, thereby avoiding inter-node joins. Second, when a multi-node delete transaction is performed over many-to-many relationships, it defers deletion of related references until they are accessed later, thereby amortizing the cost of multi-node updates. These solutions have been implemented in Odysseus/SNS - an SNS system using a shared nothing parallel DBMS. Performance evaluation using synthetic workload that reflects the real SNS workload demonstrates significant improvement in processing time. We also note that our work is the first to present the entity-relationship schema and its relational representation of the SNS database.																	0169-023X	1872-6933				JAN	2020	125								101756	10.1016/j.datak.2019.101756													
J								A modified bat algorithm with torus walk for solving global optimisation problems	INTERNATIONAL JOURNAL OF BIO-INSPIRED COMPUTATION										bat algorithm; torus walk; chaotic inertia weight; exploitation; exploration		Bat algorithm (BA) has been widely used to solve the diverse kinds of optimisation problems. In accordance with the optimisation problems, balance between the two major components: exploitation and exploration, plays a significant role in meta-heuristic algorithms. Several researchers have worked on the performance for the improvement of these algorithms. BA faces one of the major issues in high dimensions. In our work, we proposed a new variant of BA by introducing the torus walk (TW-BA) to solve this issue. To improve the local search capability instead of using the standard uniform walk, torus walk is incorporated in this paper. The simulation results performed on 19 standard benchmark functions depicts the efficiency and effectiveness of TW-BA compared with the traditional BA, directional bat algorithm, particle swarm optimisation, cuckoo search, harmony search algorithm, differential evolution and genetic algorithm. The promising experimental result suggests the superiority of proposed technique.																	1758-0366	1758-0374					2020	15	1					1	13															
J								NSGA-III algorithm with maximum ranking strategy for many-objective optimisation	INTERNATIONAL JOURNAL OF BIO-INSPIRED COMPUTATION										convergence; maximum ranking strategy; diversity; many-objective evolution algorithm	EVOLUTIONARY ALGORITHM; SELECTION; DIVERSITY	In recent years, a non-dominated sorting genetic algorithm III (NSGA-III) based on decomposition strategy had been extensively studied. However, there are still problems of lower Pareto selection pressure and insufficient diversity maintenance mechanism. To address these problems, NSGAIII algorithm with maximum ranking strategy (NSGAIII-MR) is proposed. In this algorithm, the convergence and diversity distance are balanced by adaptive parameter settings to achieve better performance. The maximum ranking strategy exploits the perpendicular distance from the solution to the weight vector to increase Pareto selection pressure. Moreover, the diversity of population is maintained with the reference point strategy to guide the solutions closer to the real Pareto front. Comparing with NSGAIII, the NSGAIII-MR algorithm enhances selection pressure and has good convergence and diversity performance. Also, the performance of algorithm is verified by comparing with other state-of-the-art evolutionary algorithms on the benchmark problems and the NSGAIII-MR is competitive.																	1758-0366	1758-0374					2020	15	1					14	23															
J								Improved density peaks clustering based on firefly algorithm	INTERNATIONAL JOURNAL OF BIO-INSPIRED COMPUTATION										density peaks clustering; DPC; firefly algorithm; cut-off distance; weighting factor; preference coefficient	FAST SEARCH; FIND	The cut-off distance of the density peaks clustering (DPC) algorithm need to be set manually; the two local densities of the algorithm have a large difference in the clustering effect on the same dataset. To address the issue, the paper proposes an improved DPC based on firefly algorithm. It combines the cut-off kernel and the Gaussian kernel defined by the DPC algorithm, and balances the effects of the two kernels by the weighting factor. Meanwhile, a cluster-like centre evaluation criterion based on local density and relative distance of preference coefficient is constructed. In order to determine the parameters of the cut-off distance, weighting factor and preference coefficient, the three parameters are optimised by the firefly algorithm with the Rand index as the objective function. The experiment results show that the performance of the proposed method on synthetic datasets and real datasets is better than DPC and its variants.																	1758-0366	1758-0374					2020	15	1					24	42															
J								Performance analysis of intrinsic embedded evolvable hardware using memetic and genetic algorithms	INTERNATIONAL JOURNAL OF BIO-INSPIRED COMPUTATION										evolvable hardware; EHW; embedded evolvable hardware; evolutionary algorithm; genetic algorithm; memetic algorithm; MicroBlaze processor; virtual reconfigurable architecture; VRA; evolution speed; evaluation time; combinational circuits; intrinsic evolution; bio-inspired algorithm	DESIGN	This paper discusses the performance analysis of memetic and genetic algorithms (GA and MA) as the optimising strategy for the design of embedded evolvable hardware. The optimisation algorithm with the fitness evaluation searches for the best configuration to evolve the hardware model. Here, an experimental setup is carried to intrinsically evolve combinational circuits to test the performance of MA and GA. The complete evaluation and evolution is built on a single Virtex 6 (XC6VLX240T-1FFG1156) ML605 Evaluation Kit FPGA. A virtual reconfigurable architecture (VRA) with the hardware fitness circuit is modelled as a second reconfigurable layer over the field programmable gate array (FPGA) to configure the target combinational logic. A FPGA soft core processor evaluates the search algorithm and the best solutions are utilised for the hardware evolution. The experimentation results showed that convergence and evolution time of MA was faster compared to GA when the search space was large. Thus, proving MA is a better option for large search space evaluations for evolvable hardware architectures.																	1758-0366	1758-0374					2020	15	1					43	51															
J								Performance-aware deployment of streaming applications in distributed stream computing systems	INTERNATIONAL JOURNAL OF BIO-INSPIRED COMPUTATION										performance awareness; application deployment; stream computing; artificial bee colony algorithm; distributed system	BEE COLONY ALGORITHM; BAT ALGORITHM; TOPOLOGY	Performance-aware deployment of streaming applications is one of the key challenging problems in distributed stream computing systems. We proposed a performance-aware deployment framework (Pa-Stream) for distributed stream computing systems. By addressing the important aspects of the framework, this paper makes the following contributions: 1) investigated the performance-aware deployment of a streaming application over distributed and heterogeneous computing nodes, and provided a general application deployment model; 2) demonstrated a streaming applications deployment scheme by proposing an artificial bee colony strategy that deploys application's vertices onto the best set of computing nodes; an incremental online redeployment strategy was used to redeploy the running application; 3) developed and integrated Pa-Stream into Apache Storm platform; 4) evaluated the fulfilment of low latency and high throughput objectives in a distributed stream computing environment. Experimental results demonstrate that the proposed Pa-Stream provided effective performance improvements on latency, throughput and resource utilisation.																	1758-0366	1758-0374					2020	15	1					52	62															
J								Dynamic economic emission dispatch problem with renewable integration focusing on deficit scenario in India	INTERNATIONAL JOURNAL OF BIO-INSPIRED COMPUTATION										renewable energy; dynamic economic emission dispatch; wind power; solar power; anti-predatory particle swarm optimisation	ALGORITHM; ENERGY	Present scenario power grid has increased penetration of renewable energy sources (RESs). RESs are clean sources of energy and power production from them with minimal emission is a national target. Already, increased carbon footprint has put nations into jeopardy. Nowadays to study the benefits due to RESs in power system is of greater importance. Stochastic nature of RESs made it difficult to manage power dispatch scenario. Dynamic power demand added even more difficulty in obtaining real time economic schedule of generation dispatch. A test system of ten generator and emission dispatch with wind turbine (WT) and photovoltaic (PV) having dynamic load for 24 hours is economised. Stochastic method of particle swarm optimisation (PSO) is compared with anti-predatory particle swarm optimisation (APSO). It is identified that APSO method gives a better economy with reduced emission for the given problem.																	1758-0366	1758-0374					2020	15	1					63	73															
J								Action graphs for proactive robot assistance in smart environments	JOURNAL OF AMBIENT INTELLIGENCE AND SMART ENVIRONMENTS										Action prediction; proactive assistance; intention recognition; symbolic AI; smart environment	ACTIVITY RECOGNITION; PLAN RECOGNITION	Smart environments can already observe the actions of a human through pervasive sensors. Based on these observations, our work aims to predict the actions a human is likely to perform next. Predictions can enable a robot to proactively assist humans by autonomously executing an action on their behalf. In this paper, Action Graphs are introduced to model the order constraints between actions. Action Graphs are derived from a problem defined in Planning Domain Definition Language (PDDL). When an action is observed, the node values are updated and next actions predicted. Subsequently, a robot executes one of the predicted actions if it does not impact the flow of the human by obstructing or delaying them. Our Action Graph approach is applied to a kitchen domain.																	1876-1364	1876-1372					2020	12	2					79	99		10.3233/AIS-200556													
J								Modelling, simulation, and optimization of diabetes type II prediction using deep extreme learning machine	JOURNAL OF AMBIENT INTELLIGENCE AND SMART ENVIRONMENTS										DELM; ANN; feedforward; backpropagation algorithm; diabetes prediction	NEURAL-NETWORK	Diabetes is among the most common medical issues which people are facing nowadays. It may cause physical incapacity or even death in some cases. It has two core types, namely type I and type II. Both types are chronic and influence the functions of the human body that regulate blood sugar. In the human body, glucose is the main element that boosts cells. However, insulin is a key that enters the cells to control blood sugar. People with diabetes type I do not have the ability to produce insulin. Whereas people with diabetes type II lack the ability to react to insulin and frequently do not make enough insulin. For adequate analysis of such a fatal disease, techniques with a minimum error rate must be utilized. In this regard, different models of artificial neural network (ANN) have been investigated in the literature to diagnose/predict the condition with a minimum error rate, however, there is a need for improvement. To further advance the accuracy, a deep extreme learning machine (DELM) based prediction model is proposed and investigated in this research. By using the DELM approach, a high level of reliability with a minimum error rate is achieved. The approach shows significant improvement in results compared to previous investigations. It is observed that during the investigation the proposed approach has the highest accuracy rate of 92.8% with 70% of training (9500 samples) and 30% of test and validation (4500 examples). Simulation results validate the prediction effectiveness of the proposed scheme.																	1876-1364	1876-1372					2020	12	2					125	138		10.3233/AIS-200554													
J								Health and wellness monitoring using ambient sensor networks	JOURNAL OF AMBIENT INTELLIGENCE AND SMART ENVIRONMENTS										Health and wellness; age-in-place; rule-based algorithm; smart homes; wireless sensor networks	ACTIVITY RECOGNITION; OLDER-ADULTS; HOME; BEHAVIOR; INTELLIGENCE; PEOPLE; SYSTEM	Smart homes equipped with ambient wireless sensor networks provide new opportunities to help older adults age-in-place, improve their quality of life and help better manage their health and wellness. In this paper, we present a methodology that estimates occupants' status as active, sedentary, in-bed, out-of-home and unobservable, their location in the house, and their daily activities related to overall health and wellness. The methodology is used to visualize and examine the daily patterns and activities of older adults living in their own homes and participating in a smart home research project. The proposed location and status estimation algorithm is highly accurate as validated by a mobile app that prompts participants with questions about the estimated time of their daily activities. A case study involving a significant health-related life event is presented where the participant's account of changes in her patterns and activities through bi-weekly interviews are shown to confirm inferences based on the results of the proposed methodology.																	1876-1364	1876-1372					2020	12	2					139	151		10.3233/AIS-200553													
J								Facilitating intergenerational storytelling for older adults in the nursing home: A case study	JOURNAL OF AMBIENT INTELLIGENCE AND SMART ENVIRONMENTS										Elderly; storytelling; tangible interface; social interaction	NARRATIVE ANALYSIS; LIFE; COMMUNICATION; DESIGN	In this paper, we present our study regarding facilitating storytelling of older adults living in the nursing home with their children. The paper was driven by the following research questions: (1) What life stories would the older adults like to share? And (2) In which ways, could design enable the older adults to tell their stories? We designed a tangible device named Slots-Story, and conducted a preliminary evaluation to refine it. In the field study, eight pairs of participants (each pair consisted of an elderly adult and his/her child) were recruited to use the prototype for around ten days. Semi-structured interviews were conducted before and after the implementation. In total 344 stories were collected. Thematic, structural, and interaction analyses were conducted with the stories. In the discussion, we conclude the paper with design considerations for promoting older adults' storytelling with their children.																	1876-1364	1876-1372					2020	12	2					153	177		10.3233/AIS-200552													
J								Generalized Nonbacktracking Bounds on the Influence in Independent Cascade Models	JOURNAL OF MACHINE LEARNING RESEARCH										Influence Estimation; Nonbacktracking Walk; Message Passing; Social Networks; Independent Cascade Model		This paper develops deterministic upper and lower bounds on the influence measure in a network, more precisely, the expected number of nodes that a seed set can influence in the independent cascade model. In particular, our bounds exploit r-nonbacktracking walks and Fortuin-Kasteleyn-Ginibre (FKG) type inequalities, and are computed by message passing algorithms. Further, we provide parameterized versions of the bounds that control the trade-off between efficiency and accuracy. Finally, the tightness of the bounds is illustrated on various network models.																	1532-4435						2020	21																						
J								Learning with Fenchel-Young Losses	JOURNAL OF MACHINE LEARNING RESEARCH										loss functions; output regularization; convex duality; structured prediction	MULTICLASS CLASSIFICATION; INFORMATION; ALGORITHM; DIVERGENCE; OPTIMIZATION; MINIMIZATION; CONVEXITY; BOUNDS; RISK	Over the past decades, numerous loss functions have been been proposed for a variety of supervised learning tasks, including regression, classification, ranking, and more generally structured prediction. Understanding the core principles and theoretical properties underpinning these losses is key to choose the right loss for the right problem, as well as to create new losses which combine their strengths. In this paper, we introduce Fenchel-Young losses, a generic way to construct a convex loss function for a regularized prediction function. We provide an in-depth study of their properties in a very broad setting, covering all the aforementioned supervised learning tasks, and revealing new connections between sparsity, generalized entropies, and separation margins. We show that Fenchel-Young losses unify many well-known loss functions and allow to create useful new ones easily. Finally, we derive efficient predictive and training algorithms, making Fenchel-Young losses appealing both in theory and practice.																	1532-4435						2020	21																						
J								Branch and Bound for Piecewise Linear Neural Network Verification	JOURNAL OF MACHINE LEARNING RESEARCH										Formal Verification; Branch and Bound; ReLU Branching		The success of Deep Learning and its potential use in many safety-critical applications has motivated research on formal verification of Neural Network (NN) models. In this context, verification involves proving or disproving that an NN model satisfies certain input-output properties. Despite the reputation of learned NN models as black boxes, and the theoretical hardness of proving useful properties about them, researchers have been successful in verifying some classes of models by exploiting their piecewise linear structure and taking insights from formal methods such as Satisifiability Modulo Theory. However, these methods are still far from scaling to realistic neural networks. To facilitate progress on this crucial area, we exploit the Mixed Integer Linear Programming (MIP) formulation of verification to propose a family of algorithms based on Branch-and-Bound (BaB). We show that our family contains previous verification methods as special cases. With the help of the BaB framework, we make three key contributions. Firstly, we identify new methods that combine the strengths of multiple existing approaches, accomplishing significant performance improvements over previous state of the art. Secondly, we introduce an effective branching strategy on ReLU non-linearities. This branching strategy allows us to efficiently and successfully deal with high input dimensional problems with convolutional network architecture, on which previous methods fail frequently. Finally, we propose comprehensive test data sets and benchmarks which includes a collection of previously released testcases. We use the data sets to conduct a thorough experimental comparison of existing and new algorithms and to provide an inclusive analysis of the factors impacting the hardness of verification problems.																	1532-4435						2020	21																						
J								Switching Regression Models and Causal Inference in the Presence of Discrete Latent Variables	JOURNAL OF MACHINE LEARNING RESEARCH										causal discovery; invariance; switching regression models; hidden Markov models; latent variables	MAXIMUM-LIKELIHOOD-ESTIMATION; MIXTURES	Given a response Y and a vector X = (X-1, ...,X-d) of d predictors, we investigate the problem of inferring direct causes of Y among the vector X. Models for Y that use all of its causal covariates as predictors enjoy the property of being invariant across different environments or interventional settings. Given data from such environments, this property has been exploited for causal discovery. Here, we extend this inference principle to situations in which some (discrete-valued) direct causes of Y are unobserved. Such cases naturally give rise to switching regression models. We provide sufficient conditions for the existence, consistency and asymptotic normality of the MLE in linear switching regression models with Gaussian noise, and construct a test for the equality of such models. These results allow us to prove that the proposed causal discovery method obtains asymptotic false discovery control under mild conditions. We provide an algorithm, make available code, and test our method on simulated data. It is robust against model violations and outperforms state-of-the-art approaches. We further apply our method to a real data set, where we show that it does not only output causal predictors, but also a process-based clustering of data points, which could be of additional interest to practitioners.																	1532-4435						2020	21																						
J								A Convex Parametrization of a New Class of Universal Kernel Functions	JOURNAL OF MACHINE LEARNING RESEARCH										kernel functions; multiple kernel learning; semi-definite programming; supervised learning; universal kernels		The accuracy and complexity of kernel learning algorithms is determined by the set of kernels over which it is able to optimize. An ideal set of kernels should: admit a linear parameterization (tractability); be dense in the set of all kernels (accuracy); and every member should be universal so that the hypothesis space is infinite-dimensional (scalability). Currently, there is no class of kernel that meets all three criteria - e.g. Gaussians are not tractable or accurate; polynomials are not scalable. We propose a new class that meet all three criteria - the Tessellated Kernel (TK) class. Specifically, the TK class: admits a linear parameterization using positive matrices; is dense in all kernels; and every element in the class is universal. This implies that the use of TK kernels for learning the kernel can obviate the need for selecting candidate kernels in algorithms such as SimpleMKL and parameters such as the bandwidth. Numerical testing on soft margin Support Vector Machine (SVM) problems show that algorithms using TK kernels outperform other kernel learning algorithms and neural networks. Furthermore, our results show that when the ratio of the number of training data to features is high, the improvement of TK over MKL increases significantly.																	1532-4435						2020	21																						
J								Provably robust estimation of modulo 1 samples of a smooth function with applications to phase unwrapping	JOURNAL OF MACHINE LEARNING RESEARCH										quadratically constrained quadratic programming (QCQP); trust-region subproblem; angular embedding; phase unwrapping; semidefinite programming; angular synchronization	TRUST-REGION SUBPROBLEM; CMOS IMAGE SENSOR; ALGORITHM; SYNCHRONIZATION; RADAR; OPTIMIZATION; EQUATION; MAPS; MRI	Consider an unknown smooth function f : [0, 1](d) -> R, and assume we are given n noisy mod 1 samples of f, i.e., y(i) = (f (x(i))+eta(i)) mod 1, for x(i) is an element of [0, 1] (d), where eta(i) denotes the noise. Given the samples (x(i), y(i))(i=)(n)(1) , our goal is to recover smooth, robust estimates of the clean samples f (x(i)) mod 1. We formulate a natural approach for solving this problem, which works with angular embeddings of the noisy mod 1 samples over the unit circle, inspired by the angular synchronization framework. This amounts to solving a smoothness regularized least-squares problem - a quadratically constrained quadratic program (QCQP) - where the variables are constrained to lie on the unit circle. Our proposed approach is based on solving its relaxation, which is a trust-region sub-problem and hence solvable efficiently. We provide theoretical guarantees demonstrating its robustness to noise for adversarial, as well as random Gaussian and Bernoulli noise models. To the best of our knowledge, these are the first such theoretical results for this problem. We demonstrate the robustness and efficiency of our proposed approach via extensive numerical simulations on synthetic data, along with a simple least-squares based solution for the unwrapping stage, that recovers the original samples of f (up to a global shift). It is shown to perform well at high levels of noise, when taking as input the denoised modulo 1 samples. Finally, we also consider two other approaches for denoising the modulo 1 samples that leverage tools from Riemannian optimization on manifolds, including a Burer-Monteiro approach for a semi-definite programming relaxation of our formulation. For the two-dimensional version of the problem, which has applications in synthetic aperture radar interferometry (InSAR), we are able to solve instances of real-world data with a million sample points in under 10 seconds, on a personal laptop.																	1532-4435						2020	21																						
J								Latent Simplex Position Model: High Dimensional Multi-view Clustering with Uncertainty Quantification	JOURNAL OF MACHINE LEARNING RESEARCH										Co-regularized Clustering; Consensus; PAC-Bayes; Random Cluster Graph; Variable Selection	SELECTION	High dimensional data often contain multiple facets, and several clustering patterns can co-exist under different variable subspaces, also known as the views. While multi-view clustering algorithms were proposed, the uncertainty quantification remains difficult - a particular challenge is in the high complexity of estimating the cluster assignment probability under each view, and sharing information among views. In this article, we propose an approximate Bayes approach - treating the similarity matrices generated over the views as rough first-stage estimates for the co-assignment probabilities; in its Kullback-Leibler neighborhood, we obtain a refined low-rank matrix, formed by the pairwise product of simplex coordinates. Interestingly, each simplex coordinate directly encodes the cluster assignment uncertainty. For multi-view clustering, we let each view draw a parameterization from a few candidates, leading to dimension reduction. With high model flexibility, the estimation can be efficiently carried out as a continuous optimization problem, hence enjoys gradient-based computation. The theory establishes the connection of this model to a random partition distribution under multiple views. Compared to single-view clustering approaches, substantially more interpretable results are obtained when clustering brains from a human traumatic brain injury study, using high-dimensional gene expression data.																	1532-4435						2020	21																						
J								Noise Accumulation in High Dimensional Classification and Total Signal Index	JOURNAL OF MACHINE LEARNING RESEARCH										Noise Accumulation; Classification; High Dimensional; Random Forest; Asymptotic; Total Signal Index		Great attention has been paid to Big Data in recent years. Such data hold promise for scientific discoveries but also pose challenges to analyses. One potential challenge is noise accumulation. In this paper, we explore noise accumulation in high dimensional two-group classification. First, we revisit a previous assessment of noise accumulation with principal component analyses, which yields a different threshold for discriminative ability than originally identified. Then we extend our scope to its impact on classifiers developed with three common machine learning approaches- random forest, support vector machine, and boosted classification trees. We simulate four scenarios with differing amounts of signal strength to evaluate each method. After determining noise accumulation may affect the performance of these classifiers, we assess factors that impact it. We conduct simulations by varying sample size, signal strength, signal strength proportional to the number predictors, and signal magnitude with random forest classifiers. These simulations suggest that noise accumulation affects the discriminative ability of high-dimensional classifiers developed using common machine learning methods, which can be modified by sample size, signal strength, and signal magnitude. We developed the measure total signal index (TSI) to track the trends of total signal and noise accumulation.																	1532-4435						2020	21																						
J								pyts: A Python Package for Time Series Classification	JOURNAL OF MACHINE LEARNING RESEARCH										time series; classification; machine learning; python		pyts is an open-source Python package for time series classification. This versatile toolbox provides implementations of many algorithms published in the literature, preprocessing functionalities, and data set loading utilities. pyts relies on the standard scientific Python packages numpy, scipy, scikit-learn, job1ib, and numba, and is distributed under the BSD-3-Clause license. Documentation contains installation instructions, a detailed user guide, a full API description, and concrete self-contained examples. Source code and documentation can be downloaded from https://github.com/johannf aouzi/pyts.																	1532-4435						2020	21																						
J								Causal Discovery Toolbox: Uncovering causal relationships in Python	JOURNAL OF MACHINE LEARNING RESEARCH										Causal Discovery; Graph recovery; open source; constraint-based methods; score-based methods; pairwise causality; Markov blanket		This paper presents a new open source Python framework for causal discovery from observational data and domain background knowledge, aimed at causal graph and causal mechanism modeling. The CDT package implements an end-to-end approach, recovering the direct dependencies (the skeleton of the causal graph) and the causal relationships between variables. It includes algorithms from the 'BNLEARN' (Scutari, 2018) and 'PCALG' (Kalisch et al., 2018) packages, together with algorithms for pairwise causal discovery such as ANM (Hover et al., 2009). CDT is available under the MIT License at https://github.com/FenTechSolutions/CausalDiscoveryToolbox.																	1532-4435						2020	21																						
J								Ancestral Gumbel-Top-k Sampling for Sampling Without Replacement	JOURNAL OF MACHINE LEARNING RESEARCH										sampling without replacement; ancestral sampling; bayesian networks; stochastic beam search; gumbel-max trick		We develop ancestral Gumbel-Top-k sampling: a generic and efficient method for sampling without replacement from discrete-valued Bayesian networks, which includes multivariate discrete distributions, Markov chains and sequence models. The method uses an extension of the Gumbel-Max trick to sample without replacement by finding the top k of perturbed log-probabilities among all possible configurations of a Bayesian network. Despite the exponentially large domain, the algorithm has a complexity linear in the number of variables and sample size k. Our algorithm allows to set the number of parallel processors m, to trade off the number of iterations versus the total cost (iterations times m) of running the algorithm. For m = 1 the algorithm has minimum total cost, whereas for m = k the number of iterations is minimized, and the resulting algorithm is known as Stochastic Beam Search. 1 We provide extensions of the algorithm and discuss a number of related algorithms. We analyze the properties of Gumbel-Top-k sampling and compare against alternatives on randomly generated Bayesian networks with different levels of connectivity. In the context of (deep) sequence models, we show its use as a method to generate diverse but high-quality translations and statistical estimates of translation quality and entropy.																	1532-4435						2020	21																						
J								On the Complexity Analysis of the Primal Solutions for the Accelerated Randomized Dual Coordinate Ascent	JOURNAL OF MACHINE LEARNING RESEARCH										accelerated randomized dual coordinate ascent; restart at any period; iteration complexity; primal solutions; dual first-order methods	DESCENT METHODS; ITERATION COMPLEXITY; 1ST-ORDER METHODS; ERROR-BOUNDS; CONVERGENCE	Dual first-order methods are essential techniques for large-scale constrained convex optimization. However, when recovering the primal solutions, we need T(epsilon(-2)) iterations to achieve an epsilon-optimal primal solution when we apply an algorithm to the non-strongly convex dual problem with T(epsilon(-1)) iterations to achieve an 6-optimal dual solution, where T(x) can be x or root x. In this paper, we prove that the iteration complexity of the primal solutions and dual solutions have the same O (1/root epsilon) order of magnitude for the accelerated randomized dual coordinate ascent. When the dual function further satisfies the quadratic functional growth condition, by restarting the algorithm at any period, we establish the linear iteration complexity for both the primal solutions and dual solutions even if the condition number is unknown. When applied to the regularized empirical risk minimization problem, we prove the iteration complexity of O (n log n + root n/epsilon) in both primal space and dual space, where n is the number of samples. Our result takes out the (log 1/epsilon)factor compared with the methods based on smoothing/regularization or Catalyst reduction. As far as we know, this is the first time that the optimal O (root n/epsilon) iteration complexity in the primal space is established for the dual coordinate ascent based stochastic algorithms. We also establish the accelerated linear complexity for some problems with nonsmooth loss, e.g., the least absolute deviation and SVM.																	1532-4435						2020	21																						
J								Tensor Train Decomposition on TensorFlow (T3F)	JOURNAL OF MACHINE LEARNING RESEARCH										tensor decomposition; tensor train; software; gpu; tensorflow		Tensor Train decomposition is used across many branches of machine learning. We present T3F a library for Tensor Train decomposition based on TensorFlow. T3F supports GPU execution, batch processing, automatic differentiation, and versatile functionality for the Riemannian optimization framework, which takes into account the underlying manifold structure to construct efficient optimization methods. The library makes it easier to implement machine learning papers that rely on the Tensor Train decomposition. T3F includes documentation, examples and 94% test coverage.																	1532-4435						2020	21																						
J								Graph-Dependent Implicit Regularisation for Distributed Stochastic Subgradient Descent	JOURNAL OF MACHINE LEARNING RESEARCH										Distributed machine learning; implicit regularisation; generalisation bounds; algorithmic stability; multi-agent optimisation	STABILITY; APPROXIMATION; OPTIMIZATION; ALGORITHMS	We propose graph-dependent implicit regularisation strategies for synchronised distributed stochastic subgradient descent (Distributed SGD) for convex problems in multi-agent learning. Under the standard assumptions of convexity, Lipschitz continuity, and smoothness, we establish statistical learning rates that retain, up to logarithmic terms, single-machine serial statistical guarantees through implicit regularisation (step size tuning and early stopping) with appropriate dependence on the graph topology. Our approach avoids the need for explicit regularisation in decentralised learning problems, such as adding constraints to the empirical risk minimisation rule. Particularly for distributed methods, the use of implicit regularisation allows the algorithm to remain simple, without projections or dual methods. To prove our results, we establish graph-independent generalisation bounds for Distributed SGD that match the single-machine serial SGD setting (using algorithmic stability), and we establish graph-dependent optimisation bounds that are of independent interest. We present numerical experiments to show that the qualitative nature of the upper bounds we derive can be representative of real behaviours.																	1532-4435						2020	21																						
J								Learning Linear Non-Gaussian Causal Models in the Presence of Latent Variables	JOURNAL OF MACHINE LEARNING RESEARCH										Causal Discovery; Structural Equation Models; Non-Gaussianity; Latent Variables; Independent Component Analysis		We consider the problem of learning causal models from observational data generated by linear non-Gaussian acyclic causal models with latent variables. Without considering the effect of latent variables, the inferred causal relationships among the observed variables are often wrong. Under faithfulness assumption, we propose a method to check whether there exists a causal path between any two observed variables. From this information, we can obtain the causal order among the observed variables. The next question is whether the causal effects can be uniquely identified as well. We show that causal effects among observed variables cannot be identified uniquely under mere assumptions of faithfulness and non-Gaussianity of exogenous noises. However, we are able to propose an efficient method that identifies the set of all possible causal effects that are compatible with the observational data. We present additional structural conditions on the causal graph under which causal effects among observed variables can be determined uniquely. Furthermore, we provide necessary and sufficient graphical conditions for unique identification of the number of variables in the system. Experiments on synthetic data and real-world data show the effectiveness of our proposed algorithm for learning causal models.																	1532-4435						2020	21																						
J								Dynamical Systems as Temporal Feature Spaces	JOURNAL OF MACHINE LEARNING RESEARCH										Recurrent Neural Network; Echo State Network; Dynamical Systems; Time Series; Kernel Machines	ECHO STATE NETWORKS; RECURRENT NEURAL-NETWORKS; MEMORY; COMPUTATION; DESIGN	Parametrised state space models in the form of recurrent networks are often used in machine learning to learn from data streams exhibiting temporal dependencies. To break the black box nature of such models it is important to understand the dynamical features of the input-driving time series that are formed in the state space. We propose a framework for rigorous analysis of such state representations in vanishing memory state space models such as echo state networks (ESN). In particular, we consider the state space a temporal feature space and the readout mapping from the state space a kernel machine operating in that feature space. We show that: (1) The usual ESN strategy of randomly generating input-to-state, as well as state coupling leads to shallow memory time series representations, corresponding to cross-correlation operator with fast exponentially decaying coefficients; (2) Imposing symmetry on dynamic coupling yields a constrained dynamic kernel matching the input time series with straightforward exponentially decaying motifs or exponentially decaying motifs of the highest frequency; (3) Simple ring (cycle) high-dimensional reservoir topology specified only through two free parameters can implement deep memory dynamic kernels with a rich variety of matching motifs. We quantify richness of feature representations imposed by dynamic kernels and demonstrate that for dynamic kernel associated with cycle reservoir topology, the kernel richness undergoes a phase transition close to the edge of stability.																	1532-4435						2020	21																						
J								On the consistency of graph-based Bayesian semi-supervised learning and the scalability of sampling algorithms	JOURNAL OF MACHINE LEARNING RESEARCH										semi-supervised learning; graph-based learning; Markov chain Monte Carlo; spectral gap	CHAIN MONTE-CARLO; MCMC METHODS; CONVERGENCE; REGULARIZATION; REDUCTION; LAPLACIAN; LIMIT	This paper considers a Bayesian approach to graph-based semi-supervised learning. We show that if the graph parameters are suitably scaled, the graph-posteriors converge to a continuum limit as the size of the unlabeled data set grows. This consistency result has profound algorithmic implications: we prove that when consistency holds, carefully designed Markov chain Monte Carlo algorithms have a uniform spectral gap, independent of the number of unlabeled inputs. Numerical experiments illustrate and complement the theory.																	1532-4435						2020	21																						
J								A New Class of Time Dependent Latent Factor Models with Applications	JOURNAL OF MACHINE LEARNING RESEARCH										Bayesian Nonparametrics; Latent Factor Models; Time Dependence	INFERENCE	In many applications, observed data are influenced by some combination of latent causes. For example, suppose sensors are placed inside a building to record responses such as temperature, humidity, power consumption and noise levels. These random, observed responses are typically affected by many unobserved, latent factors (or features) within the building such as the number of individuals, the turning on and off of electrical devices, power surges, etc. These latent factors are usually present for a contiguous period of time before disappearing; further, multiple factors could be present at a time. This paper develops new probabilistic methodology and inference methods for random object generation influenced by latent features exhibiting temporal persistence. Every datum is associated with subsets of a potentially infinite number of hidden, persistent features that account for temporal dynamics in an observation. The ensuing class of dynamic models constructed by adapting the Indian Buffet Process - a probability measure on the space of random, unbounded binary matrices - finds use in a variety of applications arising in operations, signal processing, biomedicine, marketing, image analysis, etc. Illustrations using synthetic and real data are provided.																	1532-4435						2020	21																						
J								Greedy Attack and Gumbel Attack: Generating Adversarial Examples for Discrete Data	JOURNAL OF MACHINE LEARNING RESEARCH										Adversarial Attack		We present a probabilistic framework for studying adversarial attacks on discrete data. Based on this framework, we derive a perturbation-based method, Greedy Attack, and a scalable learning-based method, Gumbel Attack, that illustrate various tradeoffs in the design of attacks. We demonstrate the effectiveness of these methods using both quantitative metrics and human evaluation on various state-of-the-art models for text classification, including a word-based CNN, a character-based CNN and an LSTM. As an example of our results, we show that the accuracy of character-based convolutional networks drops to the level of random selection by modifying only five characters through Greedy Attack.																	1532-4435						2020	21																						
J								The Maximum Separation Subspace in Sufficient Dimension Reduction with Categorical Response	JOURNAL OF MACHINE LEARNING RESEARCH										Categorical data analysis; Hellinger distance; semi-parametric; single index models; sliced inverse regression; sufficient dimension reduction	SLICED INVERSE REGRESSION; DISCRIMINANT-ANALYSIS	Sufficient dimension reduction (SDR) is a very useful concept for exploratory analysis and data visualization in regression, especially when the number of covariates is large. Many SDR methods have been proposed for regression with a continuous response, where the central subspace (CS) is the target of estimation. Various conditions, such as the linearity condition and the constant covariance condition, are imposed so that these methods can estimate at least a portion of the CS. In this paper we study SDR for regression and discriminant analysis with categorical response. Motivated by the exploratory analysis and data visualization aspects of SDR, we propose a new geometric framework to reformulate the SDR problem in terms of manifold optimization and introduce a new concept called Maximum Separation Subspace (MASES). The MASES naturally preserves the "sufficiency" in SDR without imposing additional conditions on the predictor distribution, and directly inspires a semi-parametric estimator. Numerical studies show MASES exhibits superior performance as compared with competing SDR methods in specific settings.																	1532-4435						2020	21																						
J								Optimal Bipartite Network Clustering	JOURNAL OF MACHINE LEARNING RESEARCH										Bipartite networks; stochastic block model; community detection; biclustering; network analysis; pseudo-likelihood; spectral clustering	COMMUNITY DETECTION; MAXIMUM-LIKELIHOOD; CONSISTENCY; APPROXIMATION; RECOVERY	We study bipartite community detection in networks, or more generally the network biclustering problem. We present a fast two-stage procedure based on spectral initialization followed by the application of a pseudo-likelihood classifier twice. Under mild regularity conditions, we establish the weak consistency of the procedure (i.e., the convergence of the misclassification rate to zero) under a general bipartite stochastic block model. We show that the procedure is optimal in the sense that it achieves the optimal convergence rate that is achievable by a biclustering oracle, adaptively over the whole class, up to constants. This is further formalized by deriving a minimax lower bound over a class of biclustering problems. The optimal rate we obtain sharpens some of the existing results and generalizes others to a wide regime of average degree growth, from sparse networks with average degrees growing arbitrarily slowly to fairly dense networks with average degrees of order root n. As a special case, we recover the known exact recovery threshold in the log n regime of sparsity. To obtain the consistency result, as part of the provable version of the algorithm, we introduce a sub-block partitioning scheme that is also computationally attractive, allowing for distributed implementation of the algorithm without sacrificing optimality. The provable algorithm is derived from a general class of pseudo-likelihood biclustering algorithms that employ simple EM type updates. We show the effectiveness of this general class by numerical simulations.																	1532-4435						2020	21																						
J								A lazy learning-based language identification from speech using MFCC-2 features	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Lazy learning; Speech recognition; Language identification; Mel frequency cepstral coefficient-based features	RECOGNITION; MACHINE	Developing an automatic speech recognition system for multilingual countries like India is a challenging task due to the fact that the people are inured to using multiple languages while talking. This makes language identification from speech an important and essential task prior to recognition of the same. In this paper a system is proposed towards language identification from multilingual speech signals. A new second level Mel frequency cepstral coefficient-based feature named MFCC-2 that handles the large and uneven dimensionality of MFCC has been used to characterize languages in the thick of English, Bangla and Hindi. The system has been tested with recordings of as many as 12,000 utterances of numerals and 41,884 clips extracted from YouTube videos considering background music, data from multiple environments, avoidance of noise suppression and use of keywords from different languages in a single phrase. The highest and average accuracies (for Top-3 classifiers from a pool of nine classifiers) of 98.09% and 95.54%, respectively were achieved for YouTube data.																	1868-8071	1868-808X				JAN	2020	11	1					1	14		10.1007/s13042-019-00928-3													
J								Symmetric uncertainty class-feature association map for feature selection in microarray dataset	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Feature selection; Microarray data; Graph-theoretic approach; Class-Feature Association Map	ANT COLONY OPTIMIZATION; HYBRID FEATURE-SELECTION; GENETIC ALGORITHM; CANCER; FILTER; INTEGRATION; NETWORKS	For a huge number of features versus a small size of samples, feature selection methods are useful preprocessing approaches that could eliminate the irrelevant and redundant features from the final feature subset. One of the recent research areas in feature selection is DNA microarray that the number of dimensions increase fast and requires further research in the field of feature selection. Modeling the feature search space as a graph leads to improving the visualizing of features and using graph theoretic concepts in the feature selection process. In this paper, a filer-based feature selection algorithm using graph technique is proposed for reducing the dimension of dataset named as Symmetric Uncertainty Class-Feature Association Map feature selection (SU-CFAM). In the first step, it uses the Symmetric Uncertainty concept for visualizing the feature search space as a graph. After clustering the graph into several clusters using a community detection algorithm, SU-CFAM constructs an adjacency matrix for each cluster and the final subset is selected by using the concept of maximal independent set. The performance of SU-CFAM has been compared with five well-known feature selection approaches using three classifiers including SVM, DT, NB. Experiments on fifteen public DNA microarray datasets show that SU-CFAM can achieve a better classification performance compared with other methods.																	1868-8071	1868-808X				JAN	2020	11	1					15	32		10.1007/s13042-019-00932-7													
J								Extreme vector machine for fast training on large data	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Support vector machine; Convex hull; Loss functions; Fast training	CONVEX-HULL; ALGORITHMS	Quite often, different types of loss functions are adopted in SVM or its variants to meet practical requirements. How to scale up the corresponding SVMs for large datasets are becoming more and more important in practice. In this paper, extreme vector machine (EVM) is proposed to realize fast training of SVMs with different yet typical loss functions on large datasets. EVM begins with a fast approximation of the convex hull, expressed by extreme vectors, of the training data in the feature space, and then completes the corresponding SVM optimization over the extreme vector set. When hinge loss function is adopted, EVM is the same as the approximate extreme points support vector machine (AESVM) for classification. When square hinge loss function, least squares loss function and Huber loss function are adopted, EVM corresponds to three versions, namely, L2-EVM, LS-EVM and Hub-EVM, respectively, for classification or regression. In contrast to the most related machine AESVM, with the retainment of its theoretical advantage, EVM is distinctive in its applicability to a wide variety of loss functions to meet practical requirements. Compared with the other state-of-the-art fast training algorithms CVM and FastKDE of SVMs, EVM indeed relaxes the limitation of least squares loss functions, and experimentally exhibits its superiority in training time, robustness capability and number of support vectors.																	1868-8071	1868-808X				JAN	2020	11	1					33	53		10.1007/s13042-019-00936-3													
J								Automatic optic disc detection using low-rank representation based semi-supervised extreme learning machine	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Retinal fundus images; Optic disc; Low-rank representation; Semi-supervised extreme learning machine	RETINAL IMAGES; FUNDUS IMAGES; COMMUNITY SEARCH; MODEL; LOCALIZATION; CLASSIFICATION; EXTRACTION; ALGORITHM; SELECTION; VESSELS	Optic disc detection plays an important role in developing automatic screening systems for diabetic retinopathy. Several supervised learning-based approaches have been proposed for optic disc detection. However, these approaches demand that the input training examples are completely labelled. Essentially, in medical image analysis, it is difficult to prepare several training samples which were given reliable class labels due to the fact that manually labelling data is very expensive. Moreover, retinal images such as complex vessels structures in the optic disc constituting nonlinear relationships in high-dimensional observation space, which cannot work well by traditional linear classifiers. In this study, a novel approach named low-rank representation based semi-supervised extreme learning machine (LRR-SSELM) is proposed for automated optic disc detection. Our model has the following advantages. First, it detects the optic disc from the viewpoint of semi-supervised learning and overcomes the problem there are small portion of labelled samples. Second, a nonlinear classifier is introduced into our model to fully explore the nonlinear data. Third, the local and global structures of original data can be greatly persevered by low-rank representation (LRR). The performance of the proposed method is validated on three publicly available databases, DIARETDB0, DIARETDB1 and Messidor. The experimental results indicate the advantages and effectiveness of the proposed approach.																	1868-8071	1868-808X				JAN	2020	11	1					55	69		10.1007/s13042-019-00939-0													
J								Image set face recognition based on extended low rank recovery and collaborative representation	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Image set; Low rank representation; Sparse representation; Face recognition; Image denoising	CLASSIFICATION	In the real-world face recognition problems, the collected query set images often suffer serious disturbances. To address the problem, we propose an image set face recognition method based on extended low rank recovery and collaborative representation. By exploiting a Frobenius norm term, an extended low rank representation model is firstly developed to remove all possible disturbances from the query set and reconstruct the rank-one query set. To improve the computational efficiency, a compact and discriminative dictionary is learned from the large gallery set, and the closed form solutions for both the dictionary atom and the coding coefficient are straightway derived. The final classification is performed by using any frame in the reconstructed query set instead of using the whole set, which can further improve the running efficiency. Extensive experiments are conducted on the benchmark Honda/USCD and Youtube Celebrities database to verify that the proposed method outperforms significantly the state-of-the-art methods in terms of robustness and efficiency.																	1868-8071	1868-808X				JAN	2020	11	1					71	80		10.1007/s13042-019-00941-6													
J								Local attribute reductions of formal contexts	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Formal concept analysis; Concept lattice; Decision rule; Attribute reduction	CONCEPT LATTICES; KNOWLEDGE REDUCTION; COMPLEXITY	Attribute reductions is an important topic in formal concept analysis. The existing attribute reduction approaches are dominated by global reductions and there is very limited investigation on local reductions. This paper is devoted to the study of decision rule specific reduction for formal decision context and concept specific reduction for formal context. The notion of decision rule specific reduction is proposed and the related reduction methods are presented. The relationships between existing reduction approaches and decision rule specific reduction approaches are analyzed. Accordingly, we make an analysis of attributes based on three-way classification by using reductions. Furthermore, the notion of concept specific reduction for formal context is proposed and the concept specific reduction methods are examined. The relationship between concept specific reduction and decision rule specific reduction is surveyed.																	1868-8071	1868-808X				JAN	2020	11	1					81	93		10.1007/s13042-019-00956-z													
J								Wavelet transform-based weighted nu-twin support vector regression	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Machine learning; Support vector regression; Twin support vector regression; Wavelet transform	MACHINE; ALGORITHM	In this paper, an efficient wavelet transform-based weighted nu-twin support vector regression (WTWTSVR) is proposed, inspired by twin support vector regression (TSVR) and nu-twin support vector machine-based regression. TSVR and its improved models work faster than support vector regression because they solve a pair of smaller-sized quadratic programming problems. However, they give the same emphasis to the training samples, i.e., all of the training samples share the same weights, and prior information is not used, which leads to the degradation of performance. Motivated by this, samples in different positions in the proposed WTWTSVR model are given different penalty weights determined by the wavelet transform. The weights are applied to both the quadratic empirical risk term and the first-degree empirical risk term to reduce the influence of outliers. The final regressor can avoid the overfitting problem to a certain extent and yield great generalization ability. Numerical experiments on artificial datasets and benchmark datasets demonstrate the feasibility and validity of our proposed algorithm.																	1868-8071	1868-808X				JAN	2020	11	1					95	110		10.1007/s13042-019-00957-y													
J								Incomplete label distribution learning based on supervised neighborhood information	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Label distribution learning; Incomplete annotation; Partial least squares; Supervised neighborhood information	PARTIAL LEAST-SQUARES; RANDOM FOREST; REGRESSION	Label distribution learning (LDL) assumes labels are associated with each instance to some degree and tries to model the relationship between labels and instances. LDL has achieved great success in many applications, but most existing LDL methods are designed for data with complete annotation information. However, in reality, supervised information often be incomplete due to the huge costs of data collection. In this paper, we propose a novel incomplete label distribution learning method based on supervised neighborhood information (IncomLDL-SNI). The proposed method uses partial least squares to project the original data into a supervised feature space where instances with similar labels are likely to be projected together. Then, IncomLDL-SNI utilizes the Euclidean distance to find the nearest neighbors for target samples in the supervised feature space and recovers the missing annotations from the neighborhood label Information. Extensive experiments on various data sets validate the effectiveness of our proposal.																	1868-8071	1868-808X				JAN	2020	11	1					111	121		10.1007/s13042-019-00958-x													
J								An uncertainty based incremental learning for identifying the severity of bug report	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Bug reports; Uncertainty; Active learning; Sample augmentation	NEURAL-NETWORKS; REGRESSION; SELECTION; MULTIPLE	To ensure the reliability of software system, software developers have to keep track of the severity of bug reports, and fix critical bugs as soon as possible. Recently, automatic methods to identify the severity of bug reports have emerged as a promising tool to lessen the work burden of software developers. However, most of such methods are supervised and data-driven models which fail to provide favorable performance in the presence of insufficient labeled sample or limited training data. In order to tackle with these issues, we propose an incremental learning for bug reports recognition. According to this framework of incremental learning, one active learning method is developed for tagging unlabeled bug reports, meanwhile, a sample augmentation method is utilized for sufficient training data. Both of these methods are based on uncertainty which is correlated to the informativeness and the classification risk of samples. Moreover, different types of connectionist models are employed to identify bug reports, and comprehensive experiments on real bug report datasets demonstrate that the generalization abilities of these models can be improved by this proposed incremental learning.																	1868-8071	1868-808X				JAN	2020	11	1					123	136		10.1007/s13042-019-00961-2													
J								Fine-art painting classification via two-channel dual path networks	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Image classification; Fine-art painting classification; Brush stroke; Gray-level co-occurrence; Dual path networks	NEURAL-NETWORKS	Fine-art painting expresses the state of mind and social culture of mankind. Automatic fine-art painting classification is an important task to assist the analysis of fine-art paintings. In this paper, we propose a novel two-channel dual path networks for the task of style, artist and genre classification on fine-art painting image. It includes the RGB and the brush stroke information channels. Besides the RGB information channel is used to represent the color information in fine-art painting images, the brush stroke information channel is used to extract brush stroke information from fine-art painting images. And the four-directional gray-level co-occurrence matrix is used in deep learning to detect the brush stroke information, which has never been considered in the task of fine-art painting classification. Experiments on two datasets demonstrate that the four-directional gray-level co-occurrence matrix is effective in feature representation of fine-art painting images. And the proposed model achieves best classification accuracy and good generalization performance when compared with other methods.																	1868-8071	1868-808X				JAN	2020	11	1					137	152		10.1007/s13042-019-00963-0													
J								Double quantitative fuzzy rough set-based improved AHP method and application to supplier selection decision making	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Supplier selection decision making; Double quantitative information; Fuzzy rough set; Analytic hierarchy process (AHP)	MULTISOURCE; PRECISION; MODELS	Selecting excellent supplier is the foundation of establishing efficient supply chain. This paper presents a novel hybrid model for supplier selection decision making problem by combining double quantitative fuzzy rough set and analytic hierarchy process (AHP), namely Fuzzy-AHP. we first transform the supplier decision-making into a rough-approximation problem in the double quantization decision approximation space with fuzzy decision objects, and then construct a new supplier selection decision model and method based on double-quantitative fuzzy rough sets. Then, the double quantitative fuzzy rough set is utilized to calculate the upper and lower approximations of the fuzzy decision object in the quantitative approximation space. Furthermore, the lower and upper approximations are applied to establish the pairwise comparison matrix and analytic hierarchy process is employed to rank these suppliers comprehensively. Finally, an experiment study with six ERP bidder in an Indian mining behemoth is carried out in this paper. The results reveal that the proposed technique can select effective suppliers, but also realize a comprehensive ranking. This research has enriched the methodology of supplier evaluation and selection, as well as owns theoretical value in exploring the coordinated development of supply chain to some extent.																	1868-8071	1868-808X				JAN	2020	11	1					153	167		10.1007/s13042-019-00964-z													
J								Knowledge reasoning approach with linguistic-valued intuitionistic fuzzy credibility	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Linguistic-valued intuitionistic fuzzy lattice; Linguistic-valued intuitionistic fuzzy credibility; Linguistic knowledge representation; Linguistic knowledge reasoning; Uncertain assessment		Linguistic term evaluations are always collected from two opposite sides at the same time in an assessment system. To process the linguistic knowledge, we propose an approximate reasoning approach with linguistic-valued intuitionistic fuzzy credibility based on linguistic-valued intuitionistic fuzzy lattice implication algebra and apply it to the assessment system. Firstly, we give a knowledge representation model with linguistic-valued intuitionistic fuzzy credibility. Based on the representation model, the forms and patterns of linguistic intuitionistic fuzzy modus ponens (LI-FMP) and linguistic intuitionistic fuzzy modus tollens (LI-FMT) are defined. Then there are three main phases of the knowledge reasoning with linguistic-valued intuitionistic fuzzy credibility. For a single rule, the similarity-based algorithms for LI-FMP and LI-FMT are given to get the sub-conclusion and the properties of similarity-based algorithms are discussed. For the multi-rule, we propose a rule aggregation operator to get the final conclusion by combining all the sub-conclusions. Some incomparable results are further processed if it is necessary. An intuitionistic linguistic-real valuation function is defined implying a linguistic intuitionistic fuzzy distance which is proved to be a positive valuation function. The ranking method of the incomparable results utilizes the linguistic intuitionistic distance. Lastly, the example about individual credit risk assessment shows how the proposed approach work and the contrast example illustrates that the proposed approach is rational and applied.																	1868-8071	1868-808X				JAN	2020	11	1					169	184		10.1007/s13042-019-00965-y													
J								Surface electromyography feature extraction via convolutional neural network	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Surface EMG; CNN; Traditional classifiers; Feature combination; Hand motion	PATTERN-RECOGNITION	Although a large number of surface electromyography (sEMG) features have been proposed to improve hand gesture recognition accuracy, it is still hard to achieve acceptable performance in inter-session and inter-subject tests. To promote the application of sEMG-based human machine interaction, a convolutional neural network based feature extraction approach (CNNFeat) is proposed to improve hand gesture recognition accuracy. A sEMG database is recorded from eight subjects while performing ten hand gestures. Three classic classifiers, including linear discriminant analysis (LDA), support vector machine (SVM) and K nearest neighbor (KNN), are employed to compare the CNNFeat with 25 traditional features. This work concentrates on the analysis of CNNFeat through accuracy, safety index and repeatability index. The experimental results show that CNNFeat outperforms all the tested traditional features in inter-subject test and is listed as the best three features in inter-session test. Besides, it is also found that combining CNNFeat with traditional features can further improve the accuracy by 4.35%, 3.62% and 4.7% for SVM, LDA and KNN, respectively. Additionally, this work also demonstrates that CNNFeat can be potentially enhanced with more data for model training.																	1868-8071	1868-808X				JAN	2020	11	1					185	196		10.1007/s13042-019-00966-x													
J								A robust multilayer extreme learning machine using kernel risk-sensitive loss criterion	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Extreme learning machine (ELM); Multilayer perceptron; Kernel risk-sensitive loss (KRSL); Deep learning	MEAN-SQUARE ERROR; CLASSIFICATION; CORRENTROPY; OPTIMIZATION; RECOGNITION; NETWORKS; SCHEME	More recently, extreme learning machine (ELM) has emerged as a novel computing paradigm that enables the neural network (NN) based learning to be achieved with fast training speed and good generalization performance. However, the single hidden layer NN using ELM may be not effective in addressing some large-scale problems with more computational efforts. To avoid such limitation, we utilize the multilayer ELM architecture in this article to reduce the computational complexity, without the physical memory limitation. Meanwhile, it is known to us all that there are a lot of noises in the practical applications, and the traditional ELM may not perform well in this instance. Considering the existence of noises or outliers in training dataset, we develop a more practical approach by incorporating the kernel risk-sensitive loss (KRSL) criterion into ELM, on the basis of the efficient performance surface of KRSL with high accuracy while still maintaining the robustness to outliers. A robust multilayer ELM, i.e., the stacked ELM using the minimum KRSL criterion (SELM-MKRSL), is accordingly proposed in this article to enhance the outlier robustness on large-scale and complicated dataset. The simulation results on some synthetic datasets indicate that the proposed approach SELM-MKRSL can achieve higher classification accuracy and is more robust to the noises compared with other state-of-the-art algorithms related to multilayer ELM.																	1868-8071	1868-808X				JAN	2020	11	1					197	216		10.1007/s13042-019-00967-w													
J								On selective learning in stochastic stepwise ensembles	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Variable selection ensemble; Ensemble pruning; Variable selection; Selection accuracy; Aggregation order	PARALLEL GENETIC ALGORITHM; VARIABLE SELECTION; STABILITY; SHRINKAGE; RELEVANCE; RANKING	Ensemble learning has attracted much attention of researchers studying variable selection due to its great power in improving selection accuracy and stabilizing selection results. In this paper, we present a novel ensemble pruning technique called Pruned-ST2E to obtain more effective variable selection ensembles. The order to aggregate the individuals generated by the ST2E algorithm (Xin and Zhu in J Comput Graph Stat 21(2):275-294, 2012) is rearranged. To estimate the importance of each candidate variable, only some members ranked ahead are remained. Experiments with simulated and real-world data show that the performance of Pruned-ST2E is comparable or superior to several other benchmark methods. Through analyzing the accuracy-diversity pattern in both ST2E and Pruned-ST2E, it is revealed that the inserted pruning step excludes less accurate members. The reserved members also become more concentrated on the true importance vector. Moreover, Pruned-ST2E is easy to implement. Therefore, Pruned-ST2E can be considered as an alternative for tackling variable selection tasks in practice.																	1868-8071	1868-808X				JAN	2020	11	1					217	230		10.1007/s13042-019-00968-9													
J								Almost Automorphic Solutions to Cellular Neural Networks With Neutral Type Delays and Leakage Delays on Time Scales	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Cellular neural networks; Almost automorphic solution; Exponential stability; Leakage delay; Neutral type delay	ALMOST-PERIODIC SOLUTIONS; ANTIPERIODIC SOLUTIONS; GLOBAL STABILITY; VARYING DELAYS; EXPONENTIAL STABILITY; DYNAMIC EQUATIONS; EXISTENCE; UNIQUENESS; DISCRETE	In this paper, cellular neural networks (CNNs) with neutral type delays and time-varying leakage delays are investigated. By applying the existence of the exponential dichotomy of linear dynamic equations on time scales, a fixed point theorem and the theory of calculus on time scales, a set of sufficient conditions which ensure the existence and exponential stability of almost automorphic solutions of the model are obtained. An example with its numerical simulations is given to support the theoretical findings. (C) 2020 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2020	13	1					1	11		10.2991/ijcis.d.200107.001													
J								Evolutionary Feature Optimization for Plant Leaf Disease Detection by Deep Neural Networks	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Apple leaf disease detection; PDDS; DNN; GOA; SURF; Accuracy	IDENTIFICATION; SEGMENTATION; SUPERPIXEL	Apple leaf disease is the foremost factor that restricts apple yield and quality. Usually, much time is taken for disease detection with the existing diagnostic techniques; therefore, farmers frequently miss the best time for preventing and treating diseases. The detection of apple leaf diseases is a significant research problem, and its main aim is to discover an efficient technique for disease leaf image diagnosis. This article has made an effort to propose a method that can detect the disease of apple plant leaf using deep neural network (DNN). Plant diseases detection system (PDDS) architecture is designed. Speeded up robust feature (SURF) is used for feature extraction and Grasshopper Optimization Algorithm (GOA) for feature optimization, which helps to achieve better detection and classification accuracy. Classification parameters, such as Precision, Recall, 1 7 -measure, Error, and Accuracy is computed, and a comparative analysis has been performed to depict the effectiveness of the proposed work. (C) 2020 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2020	13	1					12	23		10.2991/ijcis.d.200108.001													
J								Personalized Tag Recommendation Based on Convolution Feature and Weighted Random Walk	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Flickr; User group; Bipartite graph; Weighted random walk; Personalized tag recommendation	FACTORIZATION	Automatic image semantic annotation is of great importance for image retrieval, therefore, this paper aims to recommend tags for social images according to user preferences. With the rapid development of the image-sharing community, such as Flickr, the image resources of the social network with rich metadata information demonstrate explosive growth. How to provide semantic tagging words (also known as tag recommendation) to social images through image metadata information analysis and mining is still a question, which brings new challenges and opportunities to the semantic understanding of images. Making full use of metadata for semantic analysis of images can help to bridge the semantic gap. Thus, we propose a novel personalized tag recommendation algorithm based on the convolution feature and weighted random walk. Particularly, for a given target image, we select its visual neighbors and determine the weight of each neighbor by mining the influence of user group metadata in Flickr on image correlation, and combining group information and visual features extracted by Convolutional Neural Network (CNN). Afterwards, the weighted random walk algorithm is implemented on the neighbor-tag bipartite graph. Experimental results show that tags recommended by our proposed method can accurately describe the semantic information of images and satisfy the personalized requirements of users. (C) 2020 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2020	13	1					24	35		10.2991/ijcis.d.200114.001													
J								Light Weight Proactive Padding Based Crypto Security System in Distributed Cloud Environment	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Cloud security; Cryptanalysis; Key policy; Privacy; Prime padding	CHALLENGES	The organization maintains various information in cloud which is a loosely coupled environment. However, the nature of cloud encourages the threats in different level. Among them the data security has been a keen issue being identified and challenges the service provider. To improve the data security performance, different algorithms have been discussed, but suffer to achieve higher performance in data security. To design more secured data security algorithm, a light weight proactive padding based crypto security system (LPP-CS) is presented in this paper. The method generates keys to support the crypto systems based on the prime values. The keys are generated from the set of prime numbers which have been used to pad the cipher text generated. The end user will be given with the key which is generated and distributed at the assignment. The encryption is performed in block level and for each block of data different keys has been used which challenges the adversary highly. The selection of prime factors and keys are suitable for any specific time window and has been iterated frequently. The proposed LPP-CS algorithm improves the performance of cloud data security with less time complexity. (C) 2020 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2020	13	1					36	43		10.2991/ijcis.d.200110.001													
J								New Ant Colony Optimization Algorithm of the Traveling Salesman Problem	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Computational intelligence optimization; New ant colony optimization algorithm; Meeting strategy; Performance; Traveling salesman problem	ACCEPTANCE CRITERION; GENETIC ALGORITHM; SEARCH ALGORITHM; CUCKOO SEARCH; SYSTEM	As one suitable optimization method implementing computational intelligence, ant colony optimization (ACO) can be used to solve the traveling salesman problem (TSP). However, traditional ACO has many shortcomings, including slow convergence and low efficiency. By enlarging the ants' search space and diversifying the potential solutions, a new ACO algorithm is proposed. In this new algorithm, to diversify the solution space, a strategy of combining pairs of searching ants is used. Additionally, to reduce the influence of having a limited number of meeting ants, a threshold constant is introduced. Based on applying the algorithm to 20 typical TSPs, the performance of the new algorithm is verified to be good. Moreover, by comparison with 16 state-of-the-art algorithms, the results show that the proposed new algorithm is a highly suitable method to solve the TSP, and its performance is better than those of most algorithms. Finally, by solving eight TSPs, the good performance of the new algorithm has been ana lyzed more comprehensively by comparison with that of the typical traditional ACO. The results show that the new algorithm can attain a better solution with higher accuracy and less effort. (C) 2020 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2020	13	1					44	55		10.2991/ijcis.d.200117.001													
J								Coreference Resolution Using Neural MCDM and Fuzzy Weighting Technique	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Coreference resolution; Fuzzy weighting; Text mining; Mention extraction; Kohonen neural network		Coreference resolution has been an active field of research in the past several decades and plays a vital role in many areas such as information extraction, document summarization, machine translation, and question answering systems. This paper presents a new coreference resolution approach by incorporating RoBERTa embedding with a neural multi-criteria decision making (MCDM) method. The proposed model does not use any syntactic and dependency parser. Mentions were extracted from the text with an unhand engineered mention detector and features were extracted from a deep neural network. Next, the problem is modeled in the form of effective parameters of the performance such as error rate reduction and enhances the F1 by Kohonen MCDM neural network. The weights assigned to the features represent their importance and suggests the best reference for a mention where such weights are computed using a fuzzy weighting method. Comparing to state-of-the-art coreference resolution models, the simulation results show significant improvements for the proposed approach on different datasets in terms of precision and recall and achieving marginal improvements on the following datasets: English CoNLL-2012 shared task (+3.1 F1), Yahoo's news site (+6.6 F1), and English Gigaword (+7.04). (C) 2020 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2020	13	1					56	65		10.2991/ijcis.d.200121.001													
J								RunPool: A Dynamic Pooling Layer for Convolution Neural Network	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Dynamic pooling; Deep learning; Malicious classification; Social network		Deep learning (DL) has achieved a significant performance in computer vision problems, mainly in automatic feature extraction and representation. However, it is not easy to determine the best pooling method in a different case study. For instance, experts can implement the best types of pooling in image processing cases, which might not be optimal for various tasks. Thus, it is required to keep in line with the philosophy of DL. In dynamic neural network architecture, it is not practically possible to find a proper pooling technique for the layers. It is the primary reason why various pooling cannot be applied in the dynamic and multidimensional dataset. To deal with the limitations, it needs to construct an optimal pooling method as a better option than max pooling and average pooling. Therefore, we introduce a dynamic pooling layer called RunPool to train the convolutional neural network (CNN) architecture. RunPool pooling is proposed to regularize the neural network that replaces the deterministic pooling functions. In the final section, we test the proposed pooling layer to address classification problems with online social network (OSN) dataset. (C) 2020 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2020	13	1					66	76		10.2991/ijcis.d.200120.002													
J								Multi-Class Skin Lesions Classification System Using Probability Map Based Region Growing and DCNN	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Black frame removal; Gaussian filtering; Region growing; Optimal thresholding; Geometric features; SVM classification	CANCER DETECTION; SEGMENTATION; IMAGES; TEXTURE	Background: Melanoma is a type of threatening pigmented skin lesion, and as of now is among the most hazardous existing diseases. Suitable automated diagnosis of skin lesions and Melanoma classification can extraordinarily enhance early identification of Melanomas. Methods: However, classification models based on deterministic skin lesion can influence multi-dimensional nonlinear problem which leads to inaccurate and inefficient classification. This paper presents a Deep Convolutional Neural Network (DCNN) classification approach for segmented skin lesions in dermoscopy images. As an initial step, the skin lesion is preprocessed by an automatic preprocessing algorithm together with a fusion hair detection and removal strategy. Also a new probability map based region growing and optimal thresholding algorithm is integrated in our system which yields tremendous acc uracy. Results: For obtaining more prominent results a set of features containing ABCD features as well as geometric features are calculated in the feature extraction step to describe the malignancy of the lesion. Conclusions: The experimental result shows that the system is efficient and works well on dermoscopy images, achieving considerable accuracy. (C) 2020 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2020	13	1					77	84		10.2991/ijcis.d.200117.002													
J								A Hybrid Method for Traffic Flow Forecasting Using Multimodal Deep Learning	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Traffic flow forecasting; Multimodal deep learning; Gated recurrent units; Attention mechanism; Convolutional neural networks	PREDICTION; ALGORITHM; NETWORKS	Traffic flow forecasting has been regarded as a key problem of intelligent transport systems. In this work, we propose a hybrid multimodal deep learning method for short-term traffic flow forecasting, which can jointly and adaptively learn the spatial-temporal correlation features and long temporal interdependence of multi-modality traffic data by an attention auxiliary multimodal deep learning architecture. According to the highly nonlinear characteristics of multi-modality traffic data, the base module of our method consists of one-dimensional convolutional neural networks (ID CNN) and gated recurrent units (GRU) with the attention mechanism. The former is to capture the local trend features and the latter is to capture the long temporal dependencies. Then, we design a hybrid multimodal deep learning framework for fusing share representation features of different modality traffic data by multiple CNN-GRU-Attention modules. The experimental results indicate that the proposed multimodal deep learning model is capable of dealing with complex nonlinear urban traffic flow forecasting with satisfying accuracy and effectiveness. (C) 2020 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2020	13	1					85	97		10.2991/ijcis.d.200120.001													
J								Enhanced Particle Swarm Optimization Based on Reference Direction and Inverse Model for Optimization Problems	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Particle swarm optimization; Reference direction; Non dominated sorting; Inverse model; Neural networks optimization	DIFFERENTIAL EVOLUTION; GLOBAL OPTIMIZATION; ALGORITHM; PSO	While particle swarm optimization (PSO) shows good performance for many optimization problems, the weakness in premature convergence and easy trapping into local optimum, due to the ignorance of the diversity information, has been gradually recognized. To improve the optimization performance of PSO, an enhanced PSO based on reference direction and inverse model is proposed, RDIM-PSO for short reference. In RDIM-PSO, the reference particles which are used as reference directions are selected by non-dominated sorting method according to the fitness and diversity contribution of the population. Dynamic neighborhood strategy is introduced to divide the population into several sub-swarms based on the reference directions. For each sub-swarm, the particles focus on exploitation under the guidance of local best particle with a good guarantee of population diversity. Moreover, Gaussian process-based inverse model is introduced to generate equilibrium particles by sampling the objective space to further achieve a good balance between exploration and exploitation. Experimental results on CEC2014 test problems show that RDIM-PSO has overall better performance compared with other well-known optimization algorithms. Finally, the proposed RDIM-PSO is also applied to artificial neural networks and the promising results on the chaotic time series prediction show the effectiveness of RDIM-PSO. (C) 2020 Published by Atlantis Press SARL.																	1875-6891	1875-6883					2020	13	1					98	129		10.2991/ijcis.d.200127.001													
J								Optimisation of Group Consistency for Incomplete Uncertain Preference Relation	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Incomplete preference relation; Group decision making; Additive consistency; Uncertainty theory	GROUP DECISION-MAKING; CONSENSUS MODEL; ADDITIVE CONSISTENCY; PROGRAMMING APPROACH; PRIORITY WEIGHTS; VECTOR; VALUES	An incomplete uncertain preference relation (UPR) is typical in group decision making (GDM) for decision makers (DMs) to express preference over alternatives because of the information interaction barrier between people and decision making environment. Completing missing values can guarantee individual consistency and consensus level effectively. The operation of traditional interval preference relations (IPRs) is based only on the end point transformation, which may cause interval discretisation and information distortion easily. To overcome these limitations, pairwise comparison of alternatives in an IPR is treated as an uncertain distribution function of the subjective preference of the DM which avoids discretisation operation and handles interval numbers collectively. A belief degree is used to maintain the original information as much as possible. It guarantees the extent how people believe the estimated value is close to the incomplete original value. An uncertain chance constrained programming model is proposed herein to estimate incomplete values based on a belief degree when the preference relation obeys a linear uncertain distribution. A distance measure is defined to compute the consistency index and consensus degree. Subsequently, an iterative algorithm is presented for GDM with linear UPRs, which adjusts inconsistent preference relations and uses an operator to aggregate all individual preference relations. Furthermore, it is proven that the operation of UPRs is an extension of that of traditional IPRs under a certain belief degree. (C) 2020 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2020	13	1					130	141		10.2991/ijcis.d.200121.002													
J								Examining the Impact of Artificial Intelligence (AI)-Assisted Social Media Marketing on the Performance of Small and Medium Enterprises: Toward Effective Business Management in the Saudi Arabian Context	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Artificial intelligence; Machine learning; Deep learning	FACEBOOK; COMMUNICATION; POPULARITY; STRATEGIES; EQUITY	Purpose: To examine the impact of artificial intelligence-assisted social media marketing (AISMM)on the performance of startup businesses of small and medium enterprises (SMEs) in Saudi Arabia. Design/methodology/approach: A survey technique was employed whereby primary and secondary data was collected, analyzed, and interpreted. Participants involved business operators or employees of start-up businesses and SMEs in the Saudi Arabian context. Data were analyzed by using partial least square-structure equation modeling (PLS-SEM). Findings: AISMM, which exhibits an increasing trend among start-up businesses and SMEs in Saudi Arabia, accounts for an overall increase in the number of customers and customer bases and an additional tertiary effect of increased profitability. AISMM increases the effective business management and SMEs performance (SMEP). Moreover, effective business management increases the SMEP Originality/value: This study is quite unique as it is investigated that AISMM practices has significant role to enhance SMEP in which effective business management playing a mediating role. Implications: The practitioners can get help from this study to increase the performance by decreasing various problems of marketing by using AISMM. (C) 2020 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2020	13	1					142	152		10.2991/ijcis.d.200127.002													
J								Urban Real Estate Market Early Warning Based on Support Vector Machine: A Case Study of Beijing	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Real estate market; Early warning system; Support vector machine; Real estate prices	NEURAL-NETWORKS; OPERATORS; TUTORIAL; SYSTEM	Based on a multi-class support vector machine, an urban real estate early warning model is constructed for the Beijing real estate market. The initial indicator system is established based on the historical development of Beijing's real estate market and the selection of real estate early warning indicators. Early warning index data for Beijing from 2000 to 2018 are selected, and the leading index is selected by a time difference correlation analysis as the warning index to be used for further implementation. The model is found to have a good early warning judgment performance, and demonstrates generalization ability. The model analyzes the real estate market from the aspects of land supply, credit scale, housing supply structure, restriction of speculation, and strengthening of transparency of real estate information. It predicts that the real estate market in Beijing will run smoothly in 2019. Based on the model's findings, the paper proposes policy recommendations to promote the healthy operation of China's real estate market. (C) 2020 The Authors. Published by Atlantis Press SARI..																	1875-6891	1875-6883					2020	13	1					153	166		10.2991/ijcis.d.200129.001													
J								Interval Subsethood Measures with Respect to Uncertainty for the Interval-Valued Fuzzy Setting	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Aggregation function; Interval-valued fuzzy set; Subsethood measure	RESTRICTED EQUIVALENCE FUNCTIONS; RECIPROCAL RELATIONS; CARDINALITY; INCLUSION; SETS; TRANSITIVITY; ENTROPY; ORDERS; FAMILY	In this paper, the problem of measuring the degree of subsethood in the interval-valued fuzzy setting is addressed. Taking into account the widths of the intervals, two types of interval subsethood measures are proposed. Additionally, their relation and main properties are studied. These developments are made both with respect to the regular partial order of intervals and with respect to admissible orders. Finally, some construction methods of the introduced interval subsethood measures with the use interval-valued aggregation functions are examined. (C) 2020 The Authors. Published by Atlantis Press SARI.																	1875-6891	1875-6883					2020	13	1					167	177		10.2991/ijcis.d.202204.001													
J								DAMA: A Dynamic Classification of Multimodal Ambiguities	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Hidden Markov models; Human-machine interaction; Multimodal interaction; Natural language processing	RECOGNITION; NETWORKS; LANGUAGE	Ambiguities represent uncertainty but also a fundamental item of discussion for who is interested in the interpretation of languages and it is actually functional for communicative purposes both in human human communication and in human machine interaction. This paper faces the need to address ambiguity issues in human-machine interaction. It deals with the identification of the meaningful features of multimodal ambiguities and proposes a dynamic classification method that characterizes them by learning, and progressively adapting with the evolution of the interaction language, by refining the existing classes, or by identifying new ones. A new class of ambiguities can be added by identifying and validating the meaningful features that characterize and distinguish it compared to the existing ones. The experimental results demonstrate improvement in the classification rate over considering new ambiguity classes. (C) 2020 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2020	13	1					178	192		10.2991/ijcis.d.200208.001													
J								A Single Machine Scheduling with Periodic Maintenance and Uncertain Processing Time	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Single machine; Periodic maintenance; Uncertainty; Chance-constrained; Genetic algorithm	LQ OPTIMAL-CONTROL; GENETIC ALGORITHMS; PARALLEL MACHINES; LPT ALGORITHM; JOBS; MAKESPAN; MODEL; SERVER	A single machine scheduling problem with periodic maintenance is studied in this paper. Due to many uncertainties in reality, the processing time is recognized as an uncertain variable. The aim is to minimize the makespan at a confidence level. An uncertain chance-constrained programming model is developed to delve into the impact of uncertainties on decision variables, and an algorithm for calculating the objective function is proposed. According to the theoretical analysis, a novel method named longest shortest processing time (LSPT) rule is proposed. Subsequently, an improved genetic algorithm (GA-M) combined with LSPT rule is proposed. Numerical experiments are used to verify the feasibility of this model and algorithm. (C) 2020 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2020	13	1					193	200		10.2991/ijcis.d.200214.003													
J								On the Use of Conjunctors With a Neutral Element in the Modus Ponens Inequality	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Modus ponens; Fuzzy implication function; Conjunctor; Neutral element; Semi-copula; T-norm; Uninorm	FUZZY; EQUIVALENCE; NEGATION	The inference rule of Modus Ponens has been extensively investigated in the framework of approximate reasoning, especially for the case of t-norms. Recently, more general kinds of conjunctors have also been considered, like semi-copulas, copulas, and conjunctive uninorms. A common feature of all these kinds of conjunctors is the fact that they have a neutral element e is an element of ]0, 1]. This paper is devoted to the study of Modus Ponens for conjunctors with a neutral element with no additional conditions. Many properties are proved to be necessary for a fuzzy implication function I to satisfy the Modus Ponens with respect to a conjunctor with neutral element e is an element of ]0,1]. Although the most usual families of fuzzy implication functions do not satisfy all these properties, other possibilities for I are presented showing many new examples and generalizing some already known results on this topic. Moreover, all fuzzy implication functions satisfying the Modus Ponens with respect to the least (and with respect to the greatest) conjunctor with neutral element e is an element of ]0, 1[ are characterized. The particular case of e = 1, that provides semi-copulas, is studied separately, retrieving many known results that can be easily derived from the current study. (C) 2020 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2020	13	1					201	211		10.2991/ijcis.d.200205.002													
J								Sparse Least Squares Support Vector Machine With Adaptive Kernel Parameters	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Least squares support vector machine; Sparse representation; Dictionary learning; Kernel parameter optimization	OPTIMIZED PROJECTIONS; DICTIONARY	In this paper, we propose an efficient Least Squares Support Vector Machine (LS-SVM) training algorithm, which incorporates sparse representation and dictionary learning. First, we formalize the LS-SVM training as a sparse representation process. Second, kernel parameters are adjusted by optimizing their average coherence. As such, the proposed algorithm addresses the training problem via generating the sparse solution and optimizing kernel parameters simultaneously. Experimental results demonstrate that the proposed algorithm is capable of achieving competitive performance compared to state-of-the-art approaches. (C) 2020 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2020	13	1					212	222		10.2991/ijcis.d.200205.001													
J								An Approach for Evolving Transformation Sequences Using Hybrid Genetic Algorithms	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Program analysis; Program transformation; Genetic algorithms; Particle swarm optimization	SYSTEM	The digital transformation revolution has been crawling toward almost all aspects of our lives. One form of the digital transformation revolution appears in the transformation of our routine everyday tasks into computer executable programs in the form of web, desktop and mobile applications. The vast field of software engineering that has witnessed a significant progress in the past years is responsible for this form of digital transformation. Software development as well as other branches of software engineering has been affected by this progress. Developing applications that run on top of mobile devices requires the software developer to consider the limited resources of these devices, which on one side give them their mobile advantages, however, on the other side, if an application is developed without the consideration of these limited resources then the mobile application will neither work properly nor allow the device to run smoothly. In this paper, we introduce a hybrid approach for program optimization. It succeeded in optimizing the search process for the optimal program transformation sequence that targets a specific optimization goal. In this research we targeted the program size, to reach the lowest possible decline rate of the number of Lines of Code (LoC) of a targeted program. 'The experimental results from applying the hybrid approach on synthetic program transformation problems show a significant improve in the optimized output on which the hybrid approach achieved an LoC decline rate of 50.51% over the application of basic genetic algorithm only where 17.34% LoC decline rate was reached. (C) 2020 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2020	13	1					223	233		10.2991/ijcis.d.200214.001													
J								Using Market Sentiment Analysis and Genetic Algorithm-Based Least Squares Support Vector Regression to Predict Gold Prices	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Gold price prediction; Text mining; Opinion score; Genetic algorithms; Least square support vector regression	ARTIFICIAL NEURAL-NETWORKS; LSSVR LEARNING-PARADIGM; FORECASTING-MODEL; MACHINES; CONSUMPTION; SPACE	Gold price prediction has long been a crucial and challenging research topic for gold investors. In conventional models, most scholars have used the historical gold price or economic indicators to forecast gold prices. The gold prices depend mainly on confidence in the current market. to reduce the time delay of economic indicators in this study, the daily online global gold news undergoes a text mining approach. An opinion score is generated by ascertaining the opinion polarity and words in the daily gold news. The opinion score represents the current market trends and used as an input predictor in the forecasting model. Subsequently, the least square support vector regression (LSSVR) that is optimized by the genetic algorithm (GA) is employed to train and predict the future gold price. The mean absolute percentage error (MAPE) is adopted to evaluate the model performance. This study is the first to use the opinion score through text mining as an input predictor to GA-LSSVR in forecasting gold prices. The experiment results demonstrate that the input predictor, opinion score, can improve the predicting ability of GA-LSSVR model in terms of MAPE. (C) 2020 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2020	13	1					234	246		10.2991/ijcis.d.200214.002													
J								Feature Selection Based on a Novel Improved Tree Growth Algorithm	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Feature selection; Tree growth algorithm; Evolutionary population dynamics; Metaheuristic	OPTIMIZATION ALGORITHM; EXTREMAL OPTIMIZATION	Feature selection plays a significant role in the field of data mining and machine learning to reduce the data dimension, speed up the model building process and improve algorithm performance. Tree growth algorithm (TGA) is a recent proposed population-based metaheuristic, which shows great power of search ability in solving optimization of continuous problems. However, TGA cannot be directly applied to feature selection problems. Also, we find that its efficiency still leave room for improvement. To tackle this problem, in this study, a novel improved TGA (iTGA) is proposed, which can resolve the feature selection problem efficiently. The main contribution includes, (1) a binary TGA is proposed to tackle the feature selection problems, (2) a linearly increasing parameter tuning mechanism is proposed to tune the parameter in TGA, (3) the evolutionary population dynamics (EPD) strategy is applied to improve the exploration and exploitation capabilities of TGA, (4) the efficiency of iTGA is evaluated on fifteen UCI benchmark datasets, the comprehensive results indicate that iTGA can resolve feature selection problems efficiently. Furthermore, the results of comparative experiments also verify the superiority of iTGA compared with other state-of-the-art methods. (C) 2020 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2020	13	1					247	258		10.2991/ijcis.d.200219.001													
J								Optimal Core Operation in Supply Chain Finance Ecosystem by Integrating the Fuzzy Algorithm and Hierarchical Framework	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Supply chain finance (SCF); Fuzzy theory; Analytic hierarchy process (AHP); Smartphone industry supply chain; Core operation (CO)	DECISION-MAKING APPROACH; PROCESS FALLACY; CAPTURES VALUE; AHP; PERFORMANCE; MODEL; SELECTION; MANAGEMENT; RESOURCE; EVALUATE	Supply chain finance (SCF), which has the key concept of the delivery of credit, is a new type of financial service that can enhance the financial efficiency of a supply chain. Using the transaction records from the core operations (CO) of the members, financers can provide a higher level of cash flow to the ecosystem. Moreover, financial sectors can upgrade their operations through SCF activities. However, while SCF services can help financial sectors improve their operations, there are many risks implied in SCF activities from CO to relative members. Therefore, this paper presents a novel model that applies a combination of triangular fuzzy numbers and the analytic hierarchy process (AHP) to the decision-making process to evaluate the decision behaviors regarding the preference the CO in the smartphone industry supply chain for financers in the SCF service. Academically, the FAIIP-based decision-making framework can provide the decision makers and administrators of financial institutions with valuable guidance for measuring the optimal CO of the smartphone industry in the SCF ecosystem. Commercially, the proposed model could provide administrators with a useful tool to assess the optimal CO of the smariphone industry within the SCF ecosystem for financers. (C) 2020 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2020	13	1					259	274		10.2991/ijcis.d.200226.001													
J								Global finite-time consensus for fractional-order multi-agent systems with discontinuous inherent dynamics subject to nonlinear growth	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fractional multi-agent systems; global finite-time consensus; discontinuous inherent dynamics; fractional Lyapunov functional approach	TRACKING CONTROL; NEURAL-NETWORKS; CONTAINMENT CONTROL; SYNCHRONIZATION; ALGORITHMS	This paper is concerned with the global leader-following consensus issue for fractional-order nonlinear multi-agent systems, where the inherent dynamics is modeled to be discontinuous, and subject to nonlinear growth. A new nonlinear control protocol, which includes discontinuous factors, is designed to realize the global leader-following consensus goal. Under fractional Filippov differential inclusion framework, by applying fractional Lyapunov functional approach and Clarke's non-smooth analysis technique, the sufficient conditions with respect to the global consensus in finite time is achieved. In addition, the upper bound of the setting time is explicitly evaluated for the global leader-following consensus in finite time. Finally, two examples are performed to verify the feasibility of the proposed control protocol and the validity of the theoretical results.																	1064-1246	1875-8967					2020	38	3			SI		2401	2413		10.3233/JIFS-179529													
J								Swarm intelligence and ant colony optimization in accounting model choices	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Ant colony optimization; swarm intelligence; accounting; big data; market constraints; accounting models; choice	BIG DATA; INFORMATION; MANAGEMENT	Current accounting methods for small and medium-sized enterprises (SMEs) have long running times and low user satisfaction. Therefore, a method for the selection of accounting models for SMEs based on accounting market big data (AMBD) is proposed in this paper. Firstly, some indicators such as the current ratio, quick ratio, asset-liability ratio, accounts receivable turnover rate, and other indicators taken from the solvency, operating capacity, profitability, and growth capacity of a company are selected to set up an AMBD constraint system. Then, the principal component analysis method is used to achieve the classification of the constraints of the AMBD. Finally, by combining particle swarm optimization with ant colony optimization, the optimal accounting model is obtained through iteration. Experimental results show that the proposed method has high efficiency and user satisfaction, and achieves a high coefficient of rationality. Furthermore, the method incorporates the constraints found in the AMBD, and meets the selection requirements of the SME accounting model.																	1064-1246	1875-8967					2020	38	3			SI		2415	2423		10.3233/JIFS-179530													
J								Stability of delayed CHIKV dynamics model with cell-to-cell transmission	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Chikungunya infection; global stability; Lyapunov function; time delay; latency; cell-to-cell transmission	HIV-INFECTION MODELS; DISTRIBUTED INTRACELLULAR DELAYS; DIFFERENTIAL DRUG EFFICACY; VIRUS DYNAMICS; GLOBAL PROPERTIES; CHIKUNGUNYA-DISEASE; POPULATION-DYNAMICS; PROGRESSION	This paper investigates the global dynamics of Chikungunya virus (CHIKV) infection model with antibody immune response. We incorporate two ways of transmissions CHIKV-to-cell and cell-to-cell. We incorporate three types of discrete time delays into the model. The first time delay is the time between CHIKV entry an uninfected monocyte to become latent infected. The second time delay is the time between CHIKV entry on uninfected monocyte and the production of immature CHIKV. The third time delay represents the CHIKV's maturation time. Nonnegativity and boundedness properties of the solutions are proven. Further, global stability of the equilibria is established by constructing Lyapunov functions and by applying applying LaSalle's invariance principle.																	1064-1246	1875-8967					2020	38	3			SI		2425	2433		10.3233/JIFS-179531													
J								Entanglement and atomic Fisher information of a two qubits and optical field in squeezed thermal state	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Atomic Fisher information; entanglement; squeezed thermal field; concurrence; Mandel parameter	WEHRL ENTROPY; NONCLASSICAL PROPERTIES; ALGORITHM; DENSITY; PHASE; LASER	Quantum information technology depends on an invaluable and tenuous resource, named quantum correlation, that shows strong significant manifestation of the coherent overlap of states of a combined quantum systems. So the quantifier of this resource under factual conditions that is, whereas corrupted via generalized optical radiation field states is still limited, and common statements on entanglement dynamics. In this article, we describe quantitatively the nonlocal correlation between a two-qubit and squeezed thermal field. Especially, considering the effect thermal photons and number of photons transition between the two qubits and squeezed thermal field. Also, the Mandel parameter, entanglement and atomic Fisher information over the time evolution as a function of implicated parameters in the system are investigated. We have shown that the squeeze parameter and thermal photons have a potential effect of the dynamical properties of the atomic Fisher information, Mandel parameter and entanglement. Also, the AFI is very sensitive to the field parameters rather than the number of photons transition. Furthermore, the results interpreted in the case of thermal environment considering the effect of thermal photons and squeeze parameter on the evolution of the system under consideration.																	1064-1246	1875-8967					2020	38	3			SI		2435	2441		10.3233/JIFS-179532													
J								Attributes correlation coefficients and their application to attributes reduction	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Random decision information system; probability distribution of attributes set; information entropy of attributes set; correlation coefficient; attribute reduction	ALGORITHM	In this paper, information entropy about logarithmic form is used to measure uncertainty of knowledge, and the correlation connection between information entropy, conditional information entropy, joint information entropy and mutual information entropy are analyzed. Correlation coefficient rho(B, C) between two attribute subsets in random information systems and rho(C / B) for a conditional attributes set B relative to another condition attributes set C w.r.t. the decision attributes set D in random decision information systems are introduced. The properties of correlation coefficients are discussed, and some concepts about random information systems and random decision information systems, such as indispensable attributes, consistent attributes and core attributes, etc, are described by using correlation coefficients. Some new methods of attribute reduction are proposed by using correlation coefficients, the corresponding algorithms based on attribute correlation coefficient are given in random information systems and in random decision information systems, the listed examples and the experimental result show that the algorithms are effective.																	1064-1246	1875-8967					2020	38	3			SI		2443	2455		10.3233/JIFS-179533													
J								Two-dimensional mathematical model of the transport equations of some pollutants and their diffusion in a particular fluid	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Mathematical model; transport propagation equations; pollution; finite elements; finite differences time scheme	EXISTENCE	Motivated by our recent publication in ([9]), which was supported during the academic year 2016/1437 by the Deanship of Scientific Research in Project 1175, we will develop the previous mathematical model into a two-dimensional model of the transition equations of some pollutants and their rapid spread in a particular liquid, taking into account the given stability analysis of the numerical method which based on finite difference time scheme combined with Galerkin special approximation. This method has been given by our published work in ([12]).																	1064-1246	1875-8967					2020	38	3			SI		2457	2467		10.3233/JIFS-179534													
J								Application of PSO-RBF neural network in gesture recognition of continuous surface EMG signals	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Particle swarm optimization; RBF neural network; electromyogram signal; continuous gesture	MULTICHANNEL EMG; OPTIMIZATION; ALGORITHM; CLASSIFICATION; SEGMENTATION; SELECTION	In view of the fact that independent gesture recognition cannot fully meet the natural, convenient and effective needs of actual human-computer interaction, this paper analyzes the current research status of gesture recognition based on EMG signal, and considers the practical application value of EMG signal processing in prosthetic limb control, mobile device manipulation and sign language recognition. Therefore, in this paper, the particle swarm optimization (PSO) algorithm is used to optimize the center value and the width value of the radial basis function in the RBF neural network. And the author uses the EMG signal acquisition device and the electrode sleeve to collect the four-channel continuous EMG signals generated by eight consecutive gestures. Then, the author performs noise reduction and active segment detection based on the summation, and extracts the well-known 5 time domain features. Finally, the data obtained are normalized and divided into training set and test set to train and test the classifier. Simulation experiments show that the RBF neural network which optimizes the center value and width value of radial basis functions via particle swarm optimization algorithm achieves a high recognition rate in continuous gesture recognition.																	1064-1246	1875-8967					2020	38	3			SI		2469	2480		10.3233/JIFS-179535													
J								MATLAB-based framework for data analytics applied to Hajj dataset: Hajj health meter	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Big data; cloud; Edge-of-Things (EoT); health; Internet of Things (IoT); pilgrim; prediction	SYSTEM	The total number of pilgrims for the Hajj Season of 1438H reached 2,352, 122 - according to the General Authority for statistics Kingdom of Saudi Arabia. Pilgrims data analysis and prediction help concerned entities of the country in the future planning programs for the purpose of ensuring the necessary services - social, health, security, food and transportation services to name a few. Predictive analytics is the process of using data along with analysis, statistics, and machine learning techniques to create a predictive model for forecasting future events. Predictive analytics is often discussed in the context of big data as businesses apply algorithms to derive insights from large datasets using a framework like Hadoop, HDFS, and Spark. Building MATLAB-based framework for data analytics applied to Hajj dataset is the main aim of this research paper. The proposed framework is mainly relying on four main concepts; namely the cloud-based Internet of things (IoT), fog, Edge-of-Things (EoT), and predictive analytics. This proposed framework helps in reducing the amount of data sent, lowering network traffic, increasing bandwidth, and reducing power energy consumption. On top that, the framework including regression has the potential to predict how likely Hajj is susceptible to illness or even death.																	1064-1246	1875-8967					2020	38	3			SI		2481	2490		10.3233/JIFS-179536													
J								On the stability and Lyapunov direct method for fractional difference model of BAM neural networks	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Mittag-Leffler stability; fractional difference equations; Lyapunov function; Lyapunov direct method; BAM neural networks; equilibrium point	MITTAG-LEFFLER STABILITY; ROBUST STABILITY; RIEMANN	We provide sufficient conditions for the existence, uniqueness and global Mittag-Leffler stability for the solutions of fractional difference model of bidirectional associative memory (BAM) neural networks. Before proceeding to the main results, we introduce the newly established discrete fractional calculus and expose some of its features. The techniques of Lyapunov function and Lyapunov direct method are then employed to prove the main results. We give two examples to verify and illustrate the theory.																	1064-1246	1875-8967					2020	38	3			SI		2491	2501		10.3233/JIFS-179537													
J								Event-triggered synchronization in fixed time for complex dynamical networks with discontinuous nodes and disturbances	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Complex dynamical networks; synchronization in fixed time; event-triggered control; discontinuous nodes; linear matrix inequality	DELAYED NEURAL-NETWORKS; FINITE-TIME; MULTIAGENT SYSTEMS; GLOBAL CONVERGENCE; STABILITY	In this paper, the global robust synchronization in fixed time is discussed for discontinuous complex dynamical networks (CDNs) with uncertain disturbances based on event-triggered states-feedback control schemes. Several new hybrid controllers, which include event-triggered controller and discontinuous state-feedback controller, are designed to realize the global robust synchronization goal. By applying Lyapunov functional method, the common theorem with respect to the stability in fixed time for nonlinear systems, and inequality analysis technique, some sufficient synchronization criteria are addressed in terms of linear matrix inequalities (LMIs). In addition, the upper bound of the settling time, which is independent on initial conditions, can be determined to any desired values in advance on the basis of the configuration of parameters in the proposed control law. Finally, three examples are provided to illustrate the validity of the proposed design method and theoretical results.																	1064-1246	1875-8967					2020	38	3			SI		2503	2515		10.3233/JIFS-179538													
J								Distributed aggregate tracking of heterogeneous thermostatically controlled load clusters	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Aggregate tracking; distributed control; thermostatically controlled loads (TCLs)	DEMAND RESPONSE; COMPUTER-MODEL; CONSENSUS; POWER	This paper proposes a distributed aggregate tracking strategy for multiple heterogeneous TCL (thermostatically controlled load) clusters under a bus load agent. Specifically, these heterogeneous TCLs are composed of lots of homogeneous load clusters and each one can be modeled by a TISO (third-input single-output) aggregate model with the aggregate power being the output, and the changes of ambient/setpoint temperature and the ramping rate of the setpoint temperature being inputs. According to the aggregate model of the TCL cluster, a distributed leader-following communication protocol is proposed such that each cluster just needs to communicate with its neighbors and partial clusters are connected to the load agent, and then the aggregate power tracking problem can be solved in a distributed way. Finally, numerical results are conducted to demonstrate the effectiveness and performance of the proposed aggregate tracking algorithm.																	1064-1246	1875-8967					2020	38	3			SI		2529	2537		10.3233/JIFS-179540													
J								Gesture recognition based on multilevel multimodal feature fusion	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Gesture recognition; RGB-D image; multilevel and multimodal fusion; feature extraction	MODEL	With the development of human-computer interaction, gesture recognition has gradually become one of the research hotspots. The cost reduction and the richer information of RGB-D images make the research of gesture recognition based on RGB-D images more and more. However, the current gesture processing methods for RGB-D images still can not fully utilize the information contained Aiming at the above problems, this paper studies the feature extraction method of RGB-D image, and proposes a multimodal and multilevel feature extraction method. By extracting multimodal and multilevel image features for mapping and splicing, the utilization of RGB-D image information and the accuracy in recognition are improved effectively. Finally, the experiments verified the effectiveness and robustness of the proposed method based on the self-built gesture database. Compared and analyzed with several other RGB-D processing methods, the processing method of this paper is more advanced and effective, and can achieve better results in gesture recognition.																	1064-1246	1875-8967					2020	38	3			SI		2539	2550		10.3233/JIFS-179541													
J								The maximum norm analysis of a nonmatching grids method for a class of parabolic biharmonic equation with mixed boundary condition	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Maximum norm analysis; nonmatching grids method; Schwarz sequence; biharmonic differential equations	QUASI-VARIATIONAL INEQUALITIES; DOMAIN DECOMPOSITION; ASYMPTOTIC-BEHAVIOR; SCHWARZ METHOD	Motivated by the work of Boulaaras and Haiour in [12], we proved a maximum norm analysis of Schwarz alternating method for parabolic biharmonic equation with respect to the mixed boundary condition, where an optimal error analysis each subdomain between the discrete Schwarz sequence and the continuous solution of the presented problem is established.																	1064-1246	1875-8967					2020	38	3			SI		2551	2560		10.3233/JIFS-179542													
J								Two-stage data encryption using chaotic neural networks	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Encryption; neural networks; cryptography; security; privacy; trust		Securing a wireless sensor system is a hard task for researchers today. Strengthening the authentication system before connection establishment is the right way to enhance the architecture and also to provide secured communication from eavesdroppers. In this paper, we present a novel algorithm to enhance the security of data using a hybrid model which uses an adaptive encoding technique alongside Chaotic Hopfield Neural Network. The proposed computation upgrades the security of a key shared between any nodes. Our experimental results show that the security of transmitted data is better than traditional algorithms. Moreover, we also show that the computational time for the proposed algorithm is less than known traditional algorithms.																	1064-1246	1875-8967					2020	38	3			SI		2561	2568		10.3233/JIFS-179543													
J								Secure communication and synchronizations in light of the stability theory of the hyperchaotic complex nonlinear systems	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Hyperchaotic system; phase synchronization; anti-phase synchronization; active control; lyapunov stability analysis; complex	PHASE SYNCHRONIZATION; CHAOS SYNCHRONIZATION; ANTI-SYNCHRONIZATION; DYNAMICAL PROPERTIES; LAG SYNCHRONIZATION; OSCILLATORS; CHEN	This paper's main objective is to reflect on hyperchaotic complex nonlinear systems' phase synchronization (PS) and anti-phase synchronization (APS). In these complex systems, the number of state variables can be expanded by isolating the real and imaginative parts. In PS, while their amplitudes stay uncorrelated, the position between two coupled chaotic (or hyperchaotic) systems remains in step with each other. The absence of the sum of relevant variables can characterize APS. To study PS and APS with complex variables of hyperchaotic nonlinear systems, the active control technique on the basis of stability analysis is proposed. PS and APS investigations for high dimensional systems are demonstrated by studying a 6-dimensional Di system. Phase synchronization concerns were also used to construct a straightforward and simple secure communication application. Numerical influences outlined for clarifying the phase synchronization of the hyperchaotic Lu model and for examining the gravity of scientific articulations' control powers.																	1064-1246	1875-8967					2020	38	3			SI		2569	2583		10.3233/JIFS-179544													
J								A theoretical implementation for a proposed hyper-complex chaotic system	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Lu models; dynamic analysis; chaotic systems; symmetry; anti-symmetry; invariance and dissipation		In the present paper, a new proposed theoretical hyper-complex chaotic system is proposed, which is an extension for the complex Lu models. In the proposed model, the variables, x, and y are assumed to be functions for one real and three complex parameters, while, the z variable is assumed in terms of only real variable only. After long mathematical manipulations, a systems of nine equations are obtained which represents the hyper-complex chaotic system. The dynamics of the obtained system were analyzed theoretically herein, where symmetry, anti-symmetry, Invariance, and dissipation of the system are theoretically analyzed from mathematical point of view. The paper will followed by a generalized paper for the same proposed model to check its applicability on the secure communication systems.																	1064-1246	1875-8967					2020	38	3			SI		2585	2590		10.3233/JIFS-179545													
J								Statistical analysis of competing risks lifetime data from Nadarajaha and Haghighi distribution under type-II censoring	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Nadarajaha and Haghighi distribution; competing risks model; MCMC; classical and bayesian estimation	STEP-STRESS MODEL	Time to failure in the lifetime experiments, common that units or individuals are failure with more than one caused of failure defined by competing risks problem. In this paper, we consider the problem of competing risks model when the failure time of units or individuals have Nadarajaha and Haghighi lifetime distribution [1] based on type-II censoring schemes. The point and interval estimates of model parameters are discussed with maximum likelihood and B ayes methods. In Bayesian approach, the MCMC method is employed. The theoretical results are discussed through real life data analysis and simulation study. Finally, we discussed the numerical results in some brief comments.																	1064-1246	1875-8967					2020	38	3			SI		2591	2601		10.3233/JIFS-179546													
J								On the computational and numerical solutions of the transmission of nerve impulses of an excitable system (the neuron system)	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										The FitzHugh-Nagumo (FN) equation; the modified Khater (mK) method; B-spline scheme; stability property; computational and numerical solutions	TRAVELING-WAVE SOLUTIONS; EQUATIONS	This research paper studies the computational and numerical solutions of the transmission of nerve impulses of a nervous system (the neuron) by applying the modified Khater (mK) method and B-spline scheme to the FitzHugh-Nagumo (FN) equation where it is usually used as a model of the transmission of nerve impulses. This study focuses on finding the different types of soliton wave solutions, studying the stability property of them, and then use them to obtain the numerical solutions of the model. The obtained solutions are compared with each other to show the absolute value of error between them that will explain the accuracy of both types of solutions. Moreover, in the text of more explanation of the physical properties of the suggested model, some sketches are plotted. Also, the performance of both techniques is investigated to show its ability for applying to other nonlinear evolutions equation.																	1064-1246	1875-8967					2020	38	3			SI		2603	2610		10.3233/JIFS-179547													
J								Human models in human-in-the-loop control systems	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Human-in-the-loop; human behavior; cyber-physical systems	SUBSYSTEM IDENTIFICATION; HUMAN-BEHAVIOR; HUMAN RESPONSE; PREDICTION	Understanding the collaboration between physical systems and human is an essential task in man-machine systems. This area of research has been significantly explored in the recent years with the focus on the machine side. Much less attention has been directed to the other side in the man-machine systems, which is the human. The aim of this review is to discuses the major directions and challenges in man-machine systems from control theory perspective with a focus on human modeling and human attributes in the man-machine system. Four directions has been identified; these are: 1) Understanding the attributes and the limitation of the human operator; 2) Categorizing the human-in-the-loop applications and derive generic bounders for each category; 3) Building a realistic application-based model for the human behavior; and 4) Integrating the human models into a formal control synthesis methodology. We surveyed the contribution in each of the four challenges and pointed out the limitation in the proposed ideas. Particular attention is given to the model identification of the human behavior.																	1064-1246	1875-8967					2020	38	3			SI		2611	2622		10.3233/JIFS-179548													
J								Visualization of activated muscle area based on sEMG	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Visualization system; hand motion; upper arm muscle; active areas; sEMG	FORCE ESTIMATION; RECOGNITION; FATIGUE; SYSTEM	Based on HSV gamut space, a visualization system of muscle activity is proposed to study the mapping relationship between hand motion and active areas of upper arm muscle. There is a significant threshold change in the starting and ending points of the active segment in the original EMG signal, and the part that exceeds the threshold TH is the active segment date. Set the window width K and fixed increment Kt of time window to remove redundant data. The sEMG intensity information of each sampling electrode is obtained by calculating MAV in each window, and the simulation experiment is conducted in HSV gamut space. Through the human-computer interaction experiment of the visual system, it is proved that this system can visually display the relationship between different channels in the spatial domain, thus intuitively identify the activity intensity of different muscles in hand motion.																	1064-1246	1875-8967					2020	38	3			SI		2623	2634		10.3233/JIFS-179549													
J								An intelligent decision support system for effective handling of IT projects	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Decision support system; software project development; project estimation; project failure	SOFTWARE-DEVELOPMENT	Software projects are failing for several decades due to multiple reasons. In this regard a lot of research has been done to investigate the reasons behind the failure. However, most of this research was executed in developed countries while under-developed and developing countries got little attention. The main objective of this study is to assess the impact of critical factors on the success of software projects for under-developed countries (like Pakistan), because enterprise environmental factors along with staff working habits, their experience and expertise level also have an impact on the success of a project. To accomplish this, a survey was conducted through the Pakistan Software Export Board (PSEB), and logistic regression enquiry was executed to measure the relationship between various factors affecting software. The results reflect that improper planning along with wrong cost and time estimation are positively and significantly associated with software failure. Based on the finding of the survey, a model is proposed for intelligent decision support system (IDSs). The proposed model keeps track of the previous knowledge and behavior in a well-structured manner that might be helpful for project managers in the estimation and decision-making process of upcoming software projects. This research adds new knowledge from an under-developed country which will open new dimensions for the IT industry and project manager working under similar circumstances.																	1064-1246	1875-8967					2020	38	3			SI		2635	2647		10.3233/JIFS-179550													
J								Existence and blow-up of a new class of nonlinear damped wave equation	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Wave equation; existence and uniqueness; Faedo-Galerkin; blow-up		In this paper, we study a new class of nonlinear wave equation with variable exponents, where the existence of a unique weak solution is established under suitable assumptions on the variable exponents m and p by using the Faedo-Galerkin method. We also prove the finite time blow-up of solutions. Our results are natural extension of the work in [14].																	1064-1246	1875-8967					2020	38	3			SI		2649	2660		10.3233/JIFS-179551													
J								A new text-based w-distance metric to find the perfect match between words	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										k-NN algorithm; distance/similarity metric; text match; data mining; cosine similarity	CLASSIFICATION	The k-NN algorithm is an instance-based learning algorithm which is widely used in the data mining applications. The core engine of the k-NN algorithm is the distance/similarity function. The performance of the k-NN algorithm varies with the selection of distance function. The traditional distance/similarity functions in k-NN do not perfectly handle the mix-mode words such as when one string has multiple substrings/words. For example, a two-word string of "Employee Name", a one-word string of "Name" or more than one word such as, "Name of Employee". This ambiguity is faced by different distance/similarity functions causing difficulties in finding the perfect match of words. To improve the perfectmatch calculation functionality in the traditional k-NN algorithm, a new similarity distance metric is developed and named as word-distance (w-distance). The perfect match will help us to identify the exact required value. The proposed w-distance is a hybrid of distance and similarity in nature because it is to handle dissimilarity and similarity features of strings at the same time. The simulation results showed that w-distance has a better impact on the performance of the k-NN algorithm as compared to the Euclidean distance and the cosine similarity.																	1064-1246	1875-8967					2020	38	3			SI		2661	2672		10.3233/JIFS-179552													
J								Breast cancer detection in thermal images using extreme learning machine	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Thermogram; breast cancer; feature extraction; CAD system; homomorphic filtering; K-mean; signature boundary; Extreme Learning Machine (ELM)	DIAGNOSIS; THERMOGRAPHY; WAVELET; TUMOR	Breast cancer is one of the major causes of women death worldwide. WHO organization has reported that 1 in every 12 women could be subjected to a breast abnormality during her lifetime. To increase survival rates, it is found that early detection of breast tumor is very critical. Mammography-based breast cancer screening is the leading technology to achieve this aim. However, it still can not deal with the cases where the tumor size less than 2mm Thermography-based breast cancer detection methods can address this problem. In this paper, a breast cancer detection method is proposed. The proposed method is consists of four phases: (1) Image Pre-processing using homomorphic filtering, (2) Region of interest (ROI) Segmentation using K-mean clustering, (3) feature extraction using signature boundary, and (4) classification using Extreme Learning Machine (ELM). The proposed method is evaluated using the public dataset DMR-IR. Different activation functions in ELM are evaluated. The obtained results founded that "Tribas" is the best activation function under different experiments. It produced an accuracy result of 95.94% while talking 0.0469 second to detect the existence of malignant tumor, benign tumor or normal image. These promising results would be useful to develop thermography-based breast cancer detection system.																	1064-1246	1875-8967					2020	38	3			SI		2673	2681		10.3233/JIFS-179553													
J								Optimize star sensor calibration based on integrated modeling with hybrid WOA-LM algorithm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Star sensor; Whale optimization algorithm; levenberg-Marquardt algorithm; calibration; high accuracy	LABORATORY CALIBRATION; LOCATION	To ensure the high measurement accuracy, the star sensor must be accurately calibrated before use. Which calibration method is selected to complete the calibration of parameters and how to improve the precision of parameters calibration is the key to achieve high precision attitude measurement of star sensor. Aiming at the problem that the accuracy of the traditional calibration algorithm is greatly affected by the initial value, a parameter calibration algorithm based on hybrid WOA-LM algorithm is proposed. This algorithm can dynamically fuse levenberg-Marquardt (LM) algorithm with Whale Optimization Algorithm (WOA). By making full use of the global search ability of WOA algorithm and the local optimization ability of LM algorithm, the problems of traditional algorithm relying on the initial value, easy to fall into local convergence, low WOA algorithm's calibration efficiency and calibration results are solved. Compared with WOA method, Li method, and the two-step method, this method has better performance. Simulation and experimental results show that this method can optimize all parameters of the star sensor with high accuracy and efficiency.																	1064-1246	1875-8967					2020	38	3			SI		2683	2691		10.3233/JIFS-179554													
J								Estimation for the generalized Gompertz distribution of hybrid progressive censored samples	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Type-I hybrid Progressive; Bayesian and Non-Bayesian estimations; Markov chain Mont-Carlo; generalized Gompertz distribution	EXACT LIKELIHOOD INFERENCE; EXPONENTIAL POPULATIONS	In this paper, Bayesian and non Bayesian methods are adapted to determined interval and point estimations under Type-I hybrid Progressively censored for generalized Gompertz distribution. The special case from MCMC adjust Metropolis-Hastings within Gibbs sampling in the case Bayesian procedure is used to solve the double integrations. Numerical examples including generation data sets are shown to make a conclusion on methods estimations used here.																	1064-1246	1875-8967					2020	38	3			SI		2693	2702		10.3233/JIFS-179555													
J								An optimized on-ramp metering method for urban expressway based on reinforcement learning	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Reinforcement learning; on-ramp metering method; optimization; urban expressway		Aiming at the problem of mainline congestion and ramp queue spill in urban expressway, an optimized on-ramp control method based on reinforcement learning is put forward. Online reinforcement learning algorithm is used to optimize on-ramp control regulation by taking the metering rate as the action, the length of ramp queue, the throughput and the occupancy rate of the interweaving area as the state, and the volume of the road network as the reward function. By iterating the value function with actual behavior, the proposed method can avoid the establishment of an accurate traffic model and the reliance on prior knowledge. Meanwhile, the real-time update of the value function Q compensates for the defect of control hysteresis. Compared with the classical method in simulation scenarios of Nanjing Kazimen Expressway, the average delay of the proposed method is reduced by 16.83%, the total delay reduced by 15.83%, the average speed enhanced by 6.80%, and the total travel time decreased by 5.22%; the average queue length of the on-ramp decreased by 89%; the average occupancy rate of the weaving area is decreased by 2.42% at rush hours, and the average traffic volume increased by 109veh/h.																	1064-1246	1875-8967					2020	38	3			SI		2703	2715		10.3233/JIFS-179556													
J								New exact traveling wave solutions of space-time fractional nonlinear electrical transmission lines equation: arising in electrical engineering	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Auxiliary equation method; extended direct algebraic method; fractional complex transform; exact traveling wave solutions	OPTICAL SOLITONS; DIFFERENTIAL-EQUATIONS	In the published paper (Results in Physics, 9, (2018) 1497-1501), the authors introduced new traveling wave solutions for space-time nonlinear fractional differential equations (S-TNFDE) using, exp(-phi(xi))-expansion function and generalized Kudryashov methods. Here we present new solutions of the same equation considering another two modified methods, namely auxiliary equation method (AEM) and extended direct algebraic method (EDAM). To be able to solve the model of equations, fractional complex transform and the conformable fractional derivatives are used to convert fractional differential equation, into ordinary differential equations. The validity and reliability of the proposed methods are tested, and from the obtained results we generated new group of solutions of (S-TNFDE).																	1064-1246	1875-8967					2020	38	3			SI		2717	2723		10.3233/JIFS-179557													
J								Multi-object intergroup gesture recognition combined with fusion feature and KNN algorithm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Gesture recognition; EMG signal; activated muscle region; feature extraction	MYOELECTRIC CONTROL; PROSTHETIC HANDS; SEMG; OPTIMIZATION; EXTRACTION	SEMG signal is a bioelectrical signal produced by the contraction of human surface muscles. Human-computer interaction based on SEMG signal is of great significance in the field of rehabilitation robots. In this study, a feature extraction method of SEMG signal based on activated muscle regionis proposed, which is based on the study of activated muscle regionin human forearm and hand movement. At the same time, the main research object of this study is the multi-object intergroup SEMG signal which is closer to the practical application environment. The new feature extracted is fused with the sample entropy feature and the wavelength feature to obtain better signal features. After combining the fusion feature with KNN algorithm, the hand motion pattern recognition and classification between multi-object groups is carried out. The combination of the fusion feature and KNN classification algorithm can achieve 91.05% in the multi-object intergroup hand motion classification. This method has lower computational cost without expensive hardware support, and improves the robustness of hand motion recognition based on EMG signals.																	1064-1246	1875-8967					2020	38	3			SI		2725	2735		10.3233/JIFS-179558													
J								Statistical properties and nonlocal correlation between a two qubits and optical field in the even deformed binomial distribution	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Even deformed binomial distribution; mandel parameter; negativity; nonlocal correlation	COHERENT STATES; SQUEEZED STATES; 2-LEVEL ATOM; ENTANGLEMENT; GENERATION; ENTROPY; QUANTUM; PHASE	In this article we estimate the nonlocal correlation between a two qubits and OF (optical field) initially started in the even deformed binomial distribution (EDBS). Explicit forms of the EDBS as the initial states of the RF are presented. We investigate the dynamical and statistical properties of the linear entropy as a quantifier of the correlation and purity of the proposed system. Also, the correlation between the two qubits is quantified by the negativity. Some statistical estimations of the RF are investigated by of Mandel parameter and compared by the two types of correlation The relation between the qubit inversion, two qubits-RF nonlocal correlation, negativity and Mandel parameter is obtained. The effective of Stark shift on the dynamical operators for the RF distribution is examined. The results are shown the a high amount of field-qubit nonlocal is reached for a high value of deformation parameter. Also, the statistical properties of the field is Poissonian or sub-Poissonian photon distribution.																	1064-1246	1875-8967					2020	38	3			SI		2737	2744		10.3233/JIFS-179559													
J								Ample soliton waves for the crystal lattice formation of the conformable time-fractional (N+1) Sinh-Gordon equation by the modified Khater method and the Painleve property	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										(N+1) dimensional time-fractional Sinh-Gordon equation; Painleve property; modified auxiliary equation method; nonlinear wave; conformable fractional derivative		This research paper studies the soliton waves of the (N + 1) dimensional time-fractional conformable Sinh-Gordon equation by the implementation of the modified Khater method. This equation studies surfaces of constant negative curvature, such as the Gauss-Codazzi equation for surfaces of curvature. Using the Painleve property is employed to support the modified Khater method in formulating of abundant traveling wave solutions. The performance of the used technique emphasizes the power, effect, and ability for applying to many nonlinear evolution equations.																	1064-1246	1875-8967					2020	38	3			SI		2745	2752		10.3233/JIFS-179560													
J								Optimization of the positron emission tomography image resolution by adopting the emulsion cloud chamber technique	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Positron emission tomography; nuclear emulsion; PET; ECC; image resolution	PET	In the context of the development of our study (S. Mohammed et al.) entitled "Theoretical calculation of positronnuclear emulsion image resolution using Geant4 code" published on 2017, this research will evaluate the optimization of the image resolution of the Positron Emission Tomography (PET) when the so-called nuclear emulsion technology is introduced into tumor imaging. Upgraded and more reliable FLUKA Monte-Carlo simulation codes were performed in order to simulate a punctual source of 1 MeV positrons, emitting particles in all directions through a nuclear emulsion medium. Two new methods dedicated for the vertex reconstruction leading to an accurate recognition of the positron source are presented in this work. Moreover, this research focuses on the effects of the technical obstructions that would affect the PET imaging procedures using the nuclear emulsion, such as the scanning microscope efficiency and the actual accuracy of the slope and position measurement of the particle track on the emulsion. Taking such parameters into considerations gave a more precise and realistic estimate of the so-optimized image resolution for the PET medical imaging system.																	1064-1246	1875-8967					2020	38	3			SI		2753	2763		10.3233/JIFS-179561													
J								A methodology for sender-oriented anti-spamming	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Anti-spamming; collaborative filtering; e-mail; sender-oriented processing; resource wastage		A massive number of daily spam demands a method of anti-spam filtering that can efficiently detect unsolicited emails. Traditional spam filtering techniques block spam at receiver's side, unable to stop their propagation on the network, resulting in wastage of network bandwidth, processing power, and disk space. To address these issues, this paper proposes a methodology for sender oriented anti-spamming that identifies and filters spam at its origin, thus not only discourages spam but also reduces the resource wastage. To the best of our knowledge, addressing this wastage by spam has not been done before. Along with blocking spam at origin, this research proposes a diagonal hash algorithm to optimize image spam filtering, and email hash token generator to promote email authentication and discourage zombie accounts. Experiments suggest that the proposed technique has the potential to achieve better performance and accuracy compared to the current approaches.																	1064-1246	1875-8967					2020	38	3			SI		2765	2776		10.3233/JIFS-179562													
J								A novel and dependable image steganography model for strengthening the security of cloud storage	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Integer wavelet transform (IWT); embedded zero-tree wavelet (EZW); data hiding image steganography; stego-image quality	DATA HIDING SCHEME; ENCRYPTED IMAGES	Image steganography provides efficient techniques and methods for embedding secure data into an image. Researchers face many challenges in this field such as: ensuring the quality of the stego image is adequate, ensuring the hidden message is secure, increasing the hiding capacity, recovering the hidden message and the cover image losslessy, and overcoming the effects of lossy image compression on the hidden message. In this paper, we address the above challenges by proposing a new image steganography model to ensure the security of data in cloud storage. The fundamental processes within the base level of the proposed model are to preprocess both the cover image and the secret message. The cover image is transformed to the wavelet domain using integer-to-integer transform while the secret message is compressed using lossless entropy coding and then it is encrypted for additional security before embedding. The control level of the model drives the steganography process as follows. Firstly, it selects significant coefficients from the transformed cover image according to some threshold values. Then, it creates groups of 7-bits and 3-bits from non-lossy bits of selected significant coefficients and the encrypted bit stream of the secret message, respectively. Finally, the non-lossy bits in the selected significant coefficients are updated by injecting the secret bits. Through the process of steganography, the consistency between the payload of the secret message and the number of selected significant coefficients is checked. The model is validated and verified using extensive real experiments. Moreover, the performance of the proposed model is measured by comparison with other recent models.																	1064-1246	1875-8967					2020	38	3			SI		2777	2788		10.3233/JIFS-179563													
J								Stability of discrete-time latent pathogen dynamics model with delay and cellular infection	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Pathogen infection; cellular infection; global stability; nonstandard finite difference; Lyapunov function	VIRUS-TO-CELL; DIFFERENTIAL DRUG EFFICACY; GLOBAL PROPERTIES; POPULATION-DYNAMICS; THRESHOLD DYNAMICS; MATHEMATICAL-MODEL; IMMUNE-RESPONSES; HIV-1; PROGRESSION; SCHEME	This paper studies the global stability of pathogen dynamics model with pathogen-to-cell and cell-to-cell transmissions. We consider three types of time delays and two types of infected cells, latent and active. The model is given by a system of nonlinear delay differential equations which is discretized by utilizing nonstandard finite difference scheme. Positivity and boundedness properties of the solutions are proven. Global stability of the equilibria is established by constructing Lyapunov functions and applying LaSalle's invariance principle.																	1064-1246	1875-8967					2020	38	3			SI		2789	2799		10.3233/JIFS-179564													
J								Existential examination of the coupled fixed point in generalized b-metric spaces and an application	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Coupled fixed point; Hausdorff distance; stability of a set-valued functional equation	QUANTUM; STABILITY; THEOREM	In this work, we build up some coupled fixed point results in b-metric spaces and employ the same to study the stability for a coupled system of functional equations.																	1064-1246	1875-8967					2020	38	3			SI		2801	2807		10.3233/JIFS-179565													
J								A quantum classification algorithm for classification incomplete patterns based on entanglement measure	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Quantum neural networks; incomplete patterns; quantum computing models	INFORMATION ENTROPY; NEURAL-NETWORKS; 2-LEVEL ATOM; PHASE; UNCERTAINTY	In this paper, a novel quantum classification algorithm that is based on competitive learning is presented to classify an input pattern that results from the failures of some sensors. As long as an incomplete pattern is presented to our model, the proposed algorithm performs the competitions between the neurons by applying some unitary transformations then measures the degree of entanglement using concurrence measure to find the winner class based on the winner-take-all technique. The proposed algorithm finds the most likely winning class label in between two binary competitive classes for an incomplete pattern presented to the proposed model. Because larger scale quantum computers are still in the lab, we studied the proposed algorithm on a case study.																	1064-1246	1875-8967					2020	38	3			SI		2809	2816		10.3233/JIFS-179566													
J								Two-level atom and quantum system entanglement and squeezing with and without classical field and damping effects	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Entropy squeezing; negativity; classical field; damping; Stark shift	FISHER INFORMATION; ENTROPY	In this article, we study the entropy squeezing and the nonlocal correlation between a nonlinear quantum system initially in the BGS (Barut-Girardello state) and 2LA (two-level atom). The numerical solution is used for solving the differential equations to obtain the density matrix. The relation between the atomic inversion, nonlocal correlation measured by the negativity and entropy are discussed. The effective of classical field and damping on the dynamical properties of the entanglement and entropy squeezing is examined. The results are shown that a high amount of classical field due to the entanglement reached to minimum values. Finally the proposed quantum system being more resistant to the damping effect in the absence of classical field than the presence of classical field.																	1064-1246	1875-8967					2020	38	3			SI		2817	2822		10.3233/JIFS-179567													
J								Medical image encryption via lifting method	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Encryption; lifting transform; gray image; edge detection	ALGORITHM; CRYPTANALYSIS; COMBINATION	A new method to enhance image encryption is proposed. It is based on lifting transform and edge detection technique, the edge detection is selected as a way that it increases the capacity of hidden data, which smooth (non-edge) area hide small amount of information as compared to edge area. Initially, the lifting transform will apply on entering image. For approximate part, the simplest edge detection technique is applied, then, the image into N x N blocks is divided, the selection of key for edge will be different about non-edge block. To make the proposed method better enough, different techniques are used, one for the edge block and another for non-edge block. The proposed method is proved resists in most of security attacks. Further different testes analysis as entropy, correlation and histogram analysis is used to verify the performance of proposed method.																	1064-1246	1875-8967					2020	38	3			SI		2823	2832		10.3233/JIFS-179568													
J								On the stable computational, semi-analytical, and numerical solutions of the Langmuir waves in an ionized plasma	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										The nonlinear complex fractional generalized-Zakharov system; modified Khater method; Adomian decomposition method; Septic B-spline scheme; computational; semi-analytical; numerical solutions		This research paper investigates the stable computational, semi-analytical, and numerical solutions of the nonlinear complex fractional generalized-Zakharov system. This system describes the nonlinear interactions between the low-frequency, acoustic waves and high-frequency, electromagnetic waves. The modified Khater method is applied to find the analytical solutions then the stability property of these solutions is discussed by using the Hamiltonian system properties. Moreover, stable computational solutions are used as the initial condition in the semi-analytical and numerical schemes. The Adomian decomposition and septic B-spline schemes are used to find the semi-analytical and numerical. For more explanation of the obtained analytical solutions, some sketched are plotted in different types. Also, the comparison between the distinct types of obtained solutions is shown by calculating the absolute value of error. The performance of the used method explains the powerful, effective, and the ability for applying to different forms of nonlinear evolution equation.																	1064-1246	1875-8967					2020	38	3			SI		2833	2845		10.3233/JIFS-179569													
J								Triangular functions based method for the solution of system of linear Fredholm integral equations via an efficient finite iterative algorithm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fredhoim integral equation system; triangular functions; finite iterative algorithm; efficiency; accuracy	NUMERICAL-SOLUTION	Integral equation has been one of the essential tools for various areas of applied mathematics. The aim of this article is to propose a simple, accurate and efficient iterative method for obtaining solutions for Fredholm integral equation systems of one dimension of the second kind. The proposed numerical method is based on orthogonal triangular functions. The orthogonal Triangular Functions (TFs) based method is first applied to transform the Fredhoim system of integral equations to four coupled system of matrix algebraic equations. A finite iterative algorithm is then applied to solve this system to obtain the coefficients used to get the form of an approximate numerical solution of the unknown solution functions of the integral problems. Some examples are given to clarify the efficiency and accuracy of the method. The obtained numerical results are compared with other numerical methods and the exact solutions.																	1064-1246	1875-8967					2020	38	3			SI		2847	2858		10.3233/JIFS-179570													
J								New optical soliton solutions of space-time fractional nonlinear dynamics of microtubules via three integration schemes	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Generalized Kudryashov method; Bernoulli sub-equation function method; nonlinear dynamical equation of microtubules; improved exp(-Omega)-expansion equation	DIFFERENTIAL-EQUATIONS; ELECTRIC-FIELD; MODEL	In this study, we implement three efficient integration algorithms to retrieve the solutions of optical soliton spacetime fractional nonlinear equation for the dynamics of microtubules MTs, which considered as one of the most important part in cellular processes biology. In this work we used three integration methods, firstly, the method of exp(-Omega)-expansion equation function, secondly the Kudryashov equation method in the general case and the method of extended Bernoulli sub-equation function, with the help of the fractional complex transformation and conformable derivatives which including the solution of complex function, rational function, hyperbolic function and exponential function. Finally, our results give good solution and understanding of the properties of the non-linear waves in fractional medium.																	1064-1246	1875-8967					2020	38	3			SI		2859	2866		10.3233/JIFS-179571													
J								Genetic algorithm and numerical methods for solving linear and nonlinear system of equations: a comparative study	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Linear system equations; non-linear system equations; genetic algorithms; numerical optimization and Newton-Raphson		The problem of solving linear and nonlinear system of equations is a largely useful issue of significant importance such as that it is presented on real-time applications. Different numerical methods such as fixed-point, Newton Raphson, bisection and secant methods, and or others, namely; evolutionary and computational methods are used to solve nonlinear system under optimizing the space and time complexity. Genetic Algorithm (GA) proved to be an efficient soft computing approach to solve many linear/non-linear system of equations. In this article, a comparison between different GAs and numerical methods for solving a system of equations are introduced. From the results, a novel approach is introduced which is inspired by using a modified GA to get the optimistic solution for the system which has no numerical solution and verified with the highest performance measures in solving complicated problems.																	1064-1246	1875-8967					2020	38	3			SI		2867	2872		10.3233/JIFS-179572													
J								Lifetime competing risks data from Lomax distribution in the presence of accelerates life-testing model with Type-I censoring	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Lomax distribution; accelerates life tests; computing risks model; maximum likelihood estimation; Bayesian estimation	STEP-STRESS MODEL; EXPONENTIAL-DISTRIBUTION; INFERENCE; FAILURE	In a different area of life testing, designing experiments needs higher stress level than normal stress one. Also, the time to failure of experimental units is resulted by one of a fetal risk factors, only. In this paper, we adopted partially step-stress accelerates life-tests (ALTs) model on the Lomax lifetime unites under the two different risk factors with type-I censoring scheme. Under this assumption, we will estimate the Lomax parameters as well as accelerated factor by maximum likelihood method, point and asymptotic confidence intervals. Also, the Bayesian MCMC approach is used in obtaining point and credible interval estimations. Theoretical results are discussed and assessed through data analysis and Monte Carlo simulation study. comments.																	1064-1246	1875-8967					2020	38	3			SI		2873	2883		10.3233/JIFS-179573													
J								Soft set and its direct effect on a ring structure	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Soft set; rings; soft set theory; ring theory		The soft set and its direct on a ring structure is the main target of the proposed paper. The concept herein, can be considered as the connecting tool between the S-S-T, S-T and R-T. In the present paper, the basic properties for the proposed structure are developed and the relation between S-I-R and S-U-R are analyzed.																	1064-1246	1875-8967					2020	38	3			SI		2885	2888		10.3233/JIFS-179574													
J								Fuzzy c-means clustering method with the fuzzy distance definition applied on symmetric triangular fuzzy numbers	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Clustering; fuzzy c-means clustering method; triangular fuzzy number; fuzzy smaller	MODEL; SUM	The conventional fuzzy c-means (FCM) clustering method can be applied on data, where data features are crisp; however, when the features are fuzzy, the conventional FCM cannot be utilized. Recently, some researchers applied FCM on fuzzy numbers when the used metric has a crisp value. Since difference between two fuzzy numbers can be represented by a fuzzy value better than crisp one, in this paper, it is going to extend the FCM method for clustering symmetric triangular fuzzy numbers, where the used metric has a fuzzy value. It will be shown that the proposed fuzzy distance expresses the distance between two fuzzy numbers much better than crisp metrics. Then the proposed method has been applied on simulated and various real data, where it is compared with several new methods. The experimental results show better performance of the proposed method in compare to other ones.																	1064-1246	1875-8967					2020	38	3			SI		2891	2905		10.3233/JIFS-180971													
J								On aggregation operators for linguistic trapezoidal fuzzy intuitionistic fuzzy sets and their application to multiple attribute group decision making	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Linguistic variable; trapezoidal fuzzy linguistic fuzzy variable; linguistic intuitionistic fuzzy numbers; aggregation operators	NUMBERS; MODEL	Linguistic decision-making tools are very useful in determining the solution to real-life decision problems. In order to represent the complex fuzzy qualitative information more accurately, this paper introduces a novel concept called linguistic trapezoidal fuzzy intuitionistic fuzzy set (LTFIFS), where the degrees of membership (MS) and non-membership (NMS) are represented by trapezoidal fuzzy linguistic numbers. Further, work defines some basic operational laws, score and accuracy values, and a comparison method for LTFIFNs with a brief study of related properties. To aggregate different LTFIFNs, we develop several weighted arithmetic and weighted geometric aggregation operators such as the LTFIFWA operator, the LTFIFOWA operator, the LTFIFOWAWA operator, the LTFIFWG operator, the LTFIFOWG operator, and the LTFIFOWGWG operator. Paper also establishes several important properties and particular cases of these operators. Furthermore, by using proposed aggregation operators, we formulate a decision-making approach for solving group decision-making problems under the LTFIF environment with multiple attributes. Finally, the paper presents a real-life decision problem to validate the effectiveness and flexibility of the newly given approach.																	1064-1246	1875-8967					2020	38	3			SI		2907	2950		10.3233/JIFS-181197													
J								Lattice ordered soft group and its application in urban planning	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Soft set; soft group; lattice ordered soft set; lattice ordered soft group; decision making	SETS	Molodtsov introduced soft set theory. Soft set theory has been emerged as a mathematical tool to solve complicated problems with uncertainity. In this paper by combining lattices and soft group the new hybrid structure lattice ordered soft group and its algebraic operations are introduced. Finally an application of lattice ordered soft group on urban planning is analysed.																	1064-1246	1875-8967					2020	38	3			SI		2951	2959		10.3233/JIFS-182662													
J								A socialbots analysis-driven graph-based approach for identifying coordinated campaigns in twitter	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										social network analysis; coordinated campaign; socialbots infiltration; socialbots characterization; twitter spam	NETWORKS; DESIGN	Twitter is a popular microblogging platform, which facilitates users to express views and thoughts on day-today events using short texts limited to a maximum of 280 characters. However, it is generally targeted by socialbots for political astroturfing, advertising, spamming, and other illicit activities due to its open and real-time information sharing and dissemination nature. In this paper, we present a socialbots analysis-driven graph-based approach for identifying coordinated campaigns among Twitter users. To this end, we first present statistical insights derived from the analysis of logged data of 98 socialbots, which were injected in Twitter and associated with top-six Twitter using countries. In the analysis, we study and present the impact of socialbots' profile features, such as age and gender on infiltration. We also present a multi-attributed graph-based approach to model the profile attributes and interaction behavior of users as a similarity graph for identifying different groups of synchronized users involved in coordinated campaigns. The proposed approach is experimentally evaluated using four different evaluation parameters over a real dataset containing socialbots' trapped user profiles. The evaluation of identified campaigns in the form of clusters reveals the traces of spammers, botnets, and other malicious users.																	1064-1246	1875-8967					2020	38	3			SI		2961	2977		10.3233/JIFS-182895													
J								Visualization analysis of the journal of intelligent & fuzzy systems (2002-2018)	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Journal analysis; visualization analysis; CiteSpace; burst detection	AGGREGATION OPERATORS; EMERGING TRENDS; BIBLIOMETRIC ANALYSIS; DECISION-MAKING; INFORMATION; UNCERTAINTY; DISTANCE	To identify the continuous research status for a specific journal is the purpose of this study, employing a visualization software named CiteSpace, which is an effective scientometrics tool widely spread. 3,786 articles published in the Journal of Intelligent & Fuzzy Systems between 2002 and 2018 were retrieved from Web of Science. The co-citation network of references was linked and mapped to explore the important documents. Soon afterwards, the author co-citation network, the journal co-citation network, and the keywords co-citation network are equally important. The results of this study are instructive and meaningful to guide practice in scientific research.																	1064-1246	1875-8967					2020	38	3			SI		2979	2989		10.3233/JIFS-18326													
J								Incomplete multi-view spectral clustering	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Multi-View; spectral clustering; incomplete data		Multi-view clustering algorithms mostly apply to data without incomplete instances. However, in real-world applications, representations for the same instance are probably absent from several but not all views. This incompleteness disables traditional multi-view clustering methods from grouping incomplete multi-view data. Recently, multi-view clustering methods on incomplete data have been proposed, and the existing methods have two limitations. One is that most methods were developed for incomplete datasets only with two views. The other is that most methods were incapable of grouping data with complex distributions. In this paper, we propose a novel incomplete multi-view clustering algorithm named IMSVC, in which we adopt spectral analysis to supervise the common representation extracted from all the views. Firstly, IMVSC constructs a bipartite graph for each view. By introducing an instance-view indicator matrix to indicate whether a representation exists in a view or not, we calculate the edge weights of bipartite graph based on the point-to-point similarity. Secondly, IMVSC constructs the multi-view relationship by guiding the multiple views to share the same instance partitioning. Finally, we create a novel iterative method to optimize IMVSC. Experimental results show sound performance of the proposed algorithm on several incomplete datasets.																	1064-1246	1875-8967					2020	38	3			SI		2991	3001		10.3233/JIFS-190380													
J								New results on modified intuitionistic generalized fuzzy metric spaces by employing E.A property and common E.A property for coupled maps	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Coupled maps; complete subspace; property (E.A); fuzzy metric space (FM-space); common E.A property; modified intuitionistic generalized fuzzy metric space (MIGFM-space)	FIXED-POINT THEOREMS; COMPATIBLE MAPPINGS; CONTRACTION	In this paper, we introduce the notions of E.A and common E.A on modified intuitionistic generalized fuzzy metric space. We utilize our new notions to introduce and formulate some common fixed point theorems on modified intuitionistic generalized fuzzy metric space for coupled maps.																	1064-1246	1875-8967					2020	38	3			SI		3003	3010		10.3233/JIFS-190541													
J								Application of the ANP and fuzzy set to develop a construction quality index: A case study of Taiwan construction inspection	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Analytic network process; fuzzy set; construction quality index; construction inspection; grade	DECISION-MAKING; RISK-ASSESSMENT; SELECTION MODEL; AHP; CLASSIFICATION; PERFORMANCE; RANKING; NETWORK; SYSTEM; LOGIC	Construction inspection is a crucial mechanism for evaluating the construction quality of public construction in Taiwan. Inspection scores are results based on the experiences or subjective evaluations of auditors. Although general rating principles and procedures are specified, objective standards are not adopted to determine scores and cannot concretely demonstrate actual construction quality. This study integrated the analytic network process (ANP) and fuzzy set (FS) to develop a construction quality index (CQI) model as a concrete indicator and objective standard for evaluating construction quality. Based on past defect data inspected by auditors, dependent factors for defects were established, and the ANP was employed to calculate the weights of all factors. Subsequently, the frequencies of defects for all items in a construction inspection were multiplied by the factor weights and summed to obtain a set of CQI values. Furthermore, the use of FS reasoning affects important defects (10 items) in project quality and important values (IV). The CQI values obtained were as follows. Grade A was lower than 1.0, Grade B was between 1.0 and 3.0, Grade C was between 3.0 and 5.0, and Grade D was greater than 5.0 (IV). The CQI model is based on the frequency of defects and weights, improves the problem of intuitive on-site ratings provided by auditors, and establishes an objective and simple method for rating construction quality to effectively enhance rating standards for construction inspections.																	1064-1246	1875-8967					2020	38	3			SI		3011	3026		10.3233/JIFS-190608													
J								A new approach of interval-valued intuitionistic neutrosophic fuzzy weighted averaging operator based on decision making problem	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Linguistic; neutrosophic set; linguistic intuitionistic fuzzy number; linguistic interval-valued intuitionistic neutrosophic fuzzy numbers; interval-valued intuitionistic neutrosophic fuzzy weighted averaging operator; linguistic interval-valued intuitionistic neutrosophic fuzzy hybrid weighted averaging operator; MCDM; Numerical application		In this paper, we define the new concept of linguistic interval-valued intuitionistic neutrosophic fuzzy numbers and operational laws. We discuss the two aggregation operators, which are defined as linguistic interval-valued intuitionistic neutrosophic fuzzy weighted averaging operator and linguistic interval-valued intuitionistic neutrosophic fuzzy hybrid weighted averaging operator, for collection of data. We define the MAGDM problem under the LIVINF environment. Finally, we define a numerical example.																	1064-1246	1875-8967					2020	38	3			SI		3027	3039		10.3233/JIFS-190719													
J								ConvSRC: SmartPhone-based periocular recognition using deep convolutional neural network and sparsity augmented collaborative representation	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Periocular biometric; mobile biometrics; deep learning; convolutional neural network; sparse collaborative representation	CLASSIFICATION	Smartphone-based periocular recognition (SPR) has gained significant attention because of the limitations of face and iris biometric modalities. For this problem, most of the existing methods employ hand-crafted features. On the other hand, deep convolutional neural networks (CNN), which learn features automatically, have shown outstanding performance for many visual recognition tasks over hand-crafted features. In view of this paradigm shift, we propose an SPR method based on CNN model. A CNN model needs a huge volume of data, but for periocular recognition problem only limited amount of data is available. One solution for this issue is to use a CNN model pre-trained on the dataset from a related domain, but this raises the questions of how to extract discriminative features from a pre-trained CNN model and classify them. We introduce a simple, efficient and compact image representation method based on a pre-trained CNN model (VGG-Net). This method employs the wealth of information and sparsity existing in the activations of convolutional layers of a CNN model. For recognition, we use an efficient and robust Sparse Augmented Collaborative Representation based Classification (SA-CRC) technique. For a thorough evaluation of ConvSRC (the proposed system), experiments were carried out on the VISOB database, which was presented as the challenge dataset in ICIP2016. The results show the superiority of ConvSRC over the state-of-the-art methods; it obtains a GMR of more than 99% at FMR =10(-3) and outperforms the first winner of the ICIP2016 challenge by 10%.																	1064-1246	1875-8967					2020	38	3			SI		3041	3057		10.3233/JIFS-190834													
J								Numerical solutions for fuzzy Fredholm integral equations of the first kind using Landweber iterative method	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy numbers; Fuzzy Integral equation; Iterative method; Uniform modulus of continuity; Partial modulus of continuity	2ND KIND; EXISTENCE; UNIQUENESS; NUMBERS	In this paper, we proposed an iterative procedure based on Landweber iterative methods to solve fuzzy Fredholm integral equations of the first kind. In addition, this research be based on a strictly convex fuzzy number space and the Riemann integral of fuzzy-number-valued function which is taken value in the space. The error estimation of the proposed method in terms of uniform and partial modulus of continuity was given. The generalized difference of fuzzy numbers was controlled and estimated reasonably. And it was avoided to compute the generalized difference of fuzzy numbers during the iterations. Finally, two illustrative examples are included in order to demonstrate the accuracy and the convergence of the proposed method.																	1064-1246	1875-8967					2020	38	3			SI		3059	3074		10.3233/JIFS-190972													
J								Fuzzy reliability-oriented optimization for the road-rail intermodal transport system using tabu search algorithm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Road-rail intermodal transport; hub-and-spoke network; parallel-series system; fuzzy reliability; TS algorithm	PARALLEL-SERIES SYSTEM; HUB CENTER PROBLEM; QUOTIENT SPACE; LOCATION; SINGLE; DESIGN; MODEL	Failures on hub-and-spoke (H&S) network components (i.e., node and arc) of a road-rail intermodal transport (RRIT) system can give rise to severe degradation of the entire system reliability. This paper aims at developing a fuzzy reliability optimization approach for the RRIT system, which can be interpreted as a parallel-series system. To begin with, we provide a definition for the RRIT system reliability and relevant functions and subsequently, a fuzzy possibility measure-based optimization model. Different from previous studies, this model focuses on maximizing the network performance in terms of reliability by locating intermodal hubs for delivering flows among origin-destination (O-D) nodes. Next, we further discuss the complexity of the proposed model which has proven to be NP-hard. Hence, we use the tabu search (TS) algorithm with a two-dimensional array-based representation and two types of move operators to solve the proposed model and obtain optimal/near-optimal solutions for realistic instance sizes. Finally, we conduct a case study based on the well-known Turkish network to demonstrate the superiority of proposed model and the effectiveness of developed methods.																	1064-1246	1875-8967					2020	38	3			SI		3075	3091		10.3233/JIFS-191010													
J								On fixed point results for alpha(*)-psi- dominated fuzzy contractive mappings with graph	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fixed point; complete dislocated metric space; alpha(*)-dominated mapping; alpha(*)-psi-Ciric type rational contraction; graphic contraction; fuzzy mappings	MULTIVALUED MAPPINGS; CLOSED BALL; THEOREMS; PAIR	The purpose of this article is to establish fixed point results for a pair of alpha(*)-dominated fuzzy mappings fulfilling generalized locally new alpha(*)-psi-Ciric type rational contractive conditions on a closed ball in complete dislocated metric spaces. Example and application are given to demonstrate the novelty of our results. Our results extend several comparable results in the existing literature.																	1064-1246	1875-8967					2020	38	3			SI		3093	3103		10.3233/JIFS-191020													
J								k-order representative capacity	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Multicriteria decision making (MCDM); fuzzy measure; lower and upper k-order representative; k-nonadditive; capacity identification	NONADDITIVITY INDEX; FUZZY MEASURES; CRITERIA; CONTEXT; SUM	We propose a general notion of lower k-order representative capacity that is totally identified by the smallest k-order coefficients in one or another equivalent representation, and the k-additive, k-maxitive and k-tolerant capacities are just its special cases. We further construct its dual notion, the upper k-order representative capacity, which is accordingly determined by n - 1 to n - k-order representation coefficients and takes k-minitive and k-intolerant capacities as its concrete instances. Some particular and important cases of the upper and lower k-order representative capacities based on nonadditivity index as well as simultaneous interaction indices are introduced and studied. The identification scheme of lower/upper k-order capacity is also given and illustrated. This general framework allows one to tailor and simplify capacities while matching and representing the decision makers preferences in terms of interaction index or other desired representation of capacity.																	1064-1246	1875-8967					2020	38	3			SI		3105	3115		10.3233/JIFS-191049													
J								Improved feature size customized fast correlation-based filter for Naive Bayes text classification	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Feature selection; Naive Bayes; text classification; FCBF; IFSC-FCBF	FEATURE-SELECTION METHOD; ALGORITHM	Feature selection is an essential part in the data preprocessing. In the text classification, most of the previous feature selection algorithms rarely consider the redundancy between features. This paper focuses on eliminating redundancy. After modifying the formula of feature correlation of original fast correlation-based filter (FCBF) and updating the algorithm strategy, we propose a new approach named improved feature size customized fast correlation-based filter (IFSC-FCBF). In addition, we combine IFSC-FCBF with Naive Bayes (NB) classifier for text classification, and test it in four typical text corpus data sets. The results demonstrate that with the same feature size, IFSC-FCBF method has the advantages of higher accuracy and shorter running time than other methods.																	1064-1246	1875-8967					2020	38	3			SI		3117	3127		10.3233/JIFS-191066													
J								Reliability evaluation of gantry cranes based on fault tree analysis and Bayesian network	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Gantry crane; fault tree analysis; Bayesian network; reliability		We proposed a method to evaluate the reliability of gantry cranes and address current shortcomings in reliability evaluations. First, according to gantry crane characteristics, an evaluation system and fault tree model were established. Fault tree analysis (FTA) was then transformed into a Bayesian network topology model. Second, the prior probability of basic events was determined from expert scoring. Finally, the reliability of the whole crane was evaluated. The proposed method was applied to a company's gantry crane. The result indicated that failure probability could be calculated, and the root cause of failure could be precisely diagnosed. Fuzzy mathematical analysis could only evaluate risk levels of gantry cranes, and FTA provided only the minimum cut sets. The proposed method could precisely identify more risk factors than fuzzy mathematical analysis and FTA. This study confirmed that the proposed methodology was a more efficient reliability evaluation tool.																	1064-1246	1875-8967					2020	38	3			SI		3129	3139		10.3233/JIFS-191101													
J								The degree of (L, M)-fuzzy sigma-algebra and its related mappings	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										(L, M)-fuzzy sigma-algebra; (L, M)-fuzzy measurable mapping; (L, M)-fuzzy measurable-to-measurable mapping; isomorphism mapping; quotient mapping		The aim of this paper is mainly to study the (L, M)-fuzzy measurability in view of degree. Firstly, we generalize the notion of (L, M)-fuzzy sigma-algebra by defining the degree of an (L, M)-fuzzy sigma-algebra with respect to a mapping sigma : L-x -> M. Such kind of degrees is proved to satisfy some axioms of (L, M)-fuzzy sigma-algebra. Additionally, we define and study some special degrees such as the degree of (L, M)-fuzzy measurable mapping, (L, M)-fuzzy measurable-to-measurable mapping, isomorphic mapping and quotient mapping with respect to mappings between two (L, M)-fuzzy measurable spaces in details. Finally, we give characterizations of these degrees and investigate the relationships between them.																	1064-1246	1875-8967					2020	38	3			SI		3141	3150		10.3233/JIFS-191117													
J								ECG arrhythmia classification using modified visual geometry group network (mVGGNet)	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Deep learning; ECG; arrhythmia; VGGNet; convolutional neural network	BEAT CLASSIFICATION; NEURAL-NETWORK; TRANSFORM; SYSTEM; MODEL; PCA	In this paper, the authors propose an improved convolutional neural network for automatic arrhythmia classification using Electro-Cardio-Gram (ECG) signal. It is essential to periodically monitor the heart beat arrhythmia to reduce the risk of death due to cardiovascular disease (CVD). The Visual Geometry Group network (VGGNet) is being widely used in computer vision problems. However, the same network cannot be used for classification of ECG beats as ECG signal is different from image signal in terms of dimensionality and inherent features. Thus, the authors investigated the effect of decreasing depth and width of a convolutional neural network in context of cardiac arrhythmia classification. In this paper, six configurations differing in depth and width are evaluated using benchmark MIT-BIH database. A deep network having thirteen convolution layers but with a smaller number of filters (reduced width) showed outstanding performance for the given problem. Based on the findings, the work is further extended to propose an improved convolutional neural network named as modified VGGNet (mVGGNet) for the task of ECG arrhythmia classification into four classes which are normal (N), ventricular ectopic beat (VEB), supraventricular ectopic beat (SVEB) and fusion beat (F). The hyper-parameters of the proposed architecture were optimized using sequential model based global optimization (SMBO) algorithm. The proposed architecture is evaluated using subjected-oriented patient-independent evaluation protocol. The performance is evaluated using five-fold cross-validation. The proposed mVGGNet achieved 98.79% and 99.16% accuracy for ventricular ectopic beats (VEB) and supraventricular ectopic beats (SVEB) classification respectively. The proposed method resulted in higher specificity and precision as compared to other state-of-the-art algorithms. Thus, it can be effectively used for ECG arrhythmia classification.																	1064-1246	1875-8967					2020	38	3			SI		3151	3165		10.3233/JIFS-191135													
J								A privacy protection model for health care big data based on trust evaluation access control in cloud service environment	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Cloud computing; health care big data; information entropy; trust evaluation; fuzzy relation matrix; access control		In order to solve the trust problem generated by each node in the medical cloud system during the interaction process, a dynamic access control model based on trust evaluation is proposed in this paper. The model uses entropy weight method and fuzzy theory to fit the comprehensive interaction trust value of nodes , add the role-based two-way selection mechanism and third-party real-time monitoring mechanism for dynamic access control. Give the specific procedures and methods. Through simulation and comparison experiments with the classic Eigen-Trust model and RBAC, we can see that the model proposed in this paper has great advantages in dynamic controllability, time complexity and trust accuracy.																	1064-1246	1875-8967					2020	38	3			SI		3167	3178		10.3233/JIFS-191149													
J								Multi-criteria software quality model selection based on divergence measure and score function	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Intuitionistic fuzzy sets; divergence measure; MCDM; score function; software quality model	GROUP DECISION-MAKING; INTUITIONISTIC FUZZY INFORMATION; PREFERENCE RELATIONS; CONSENSUS MODEL; SETS	Due to advancement of modern computing technology, software and its products have played an important role in the success and failure of any business. Nowadays, numerous researchers have focused their work on the development of new software quality models and its scopes. Evaluating software quality models based on several attributes is an essential and complex multi-criteria decision making problem (MCDM). To tackle this issue, several studies have been presented for the evaluation of software quality models by utilizing decision making and the idea of fuzzy sets (FSs). Present research develops an intuitionistic fuzzy (IF) improved score function method to deal with MCDM problems under intuitionistic fuzzy sets (IFSs). Further, the efficiency and feasibility of the proposed method is illustrated to choose the best software quality model under IF environment. To authorize the results, comparative study with previous developed approaches is discussed.																	1064-1246	1875-8967					2020	38	3			SI		3179	3188		10.3233/JIFS-191153													
J								Fuzzy distributed two-stage hybrid flow shop scheduling problem with setup time: collaborative variable search	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										collaborative variable search; fuzzy scheduling; distributed scheduling; two-stage hybrid flow shop; sequence-dependent setup times	COMPETITIVE MEMETIC ALGORITHM; MINIMIZING MAKESPAN; GENETIC ALGORITHM; FLOWSHOPS; OPTIMIZATION	Distributed scheduling has attracted much attention in multiple factories. However, uncertainties are often neglected in the previous works and this neglecting may result in the low value of the obtained schedule. In this study, fuzzy distributed scheduling is addressed in two-stage hybrid flow shop with sequence-dependent setup times(SDST) which is a frequently studied constraint in single factory and seldom handled in multiple factories. A collaborative variable search (CVS) is proposed to optimize total agreement index and fuzzy makespan simultaneously. Seven neighborhood structures and two global search operators are used in two cooperated variable search parts to generate high quality solutions. Experiments are conducted and the computational results reveal that CVS has promising advantages on the considered fuzzy distributed scheduling problem.																	1064-1246	1875-8967					2020	38	3			SI		3189	3199		10.3233/JIFS-191175													
J								Extreme values, first hitting time and time integral of solution of uncertain spring vibration equation	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Uncertain differential equation; spring vibration equation; extreme value; first hitting time; time integral	UNIQUENESS THEOREM; NUMERICAL-METHOD; EXISTENCE; STABILITY	Uncertain spring vibration equations modell the vibration of a spring subjected to a time varying external force driven by the Liu process. In this paper, we investigate uncertainty distributions of the extreme values, first hitting time, and time integral of solutions to uncertain spring vibration equations. Moreover, a numerical algorithm is designed to obtain the extreme values, and numerical examples are presented to illustrate the effectiveness of the proposed algorithm.																	1064-1246	1875-8967					2020	38	3			SI		3201	3211		10.3233/JIFS-191179													
J								Clustering based approach for incomplete data streams processing	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Data streams; incomplete data clustering; fuzzy clustering; nearest neighbor rule	CLASSIFICATION; ENSEMBLE; ALGORITHM; DOMINANCE; FRAMEWORK; ROUGH	Recent applications such as sensor networks generate continuous and dynamic data streams. Data streams are often gathered from multiple data sources with some incompleteness. Clustering such data is constrained by incompleteness of data, data distribution, and continuous nature of data streams. Ignoring missing values in incomplete data clustering, especially in high missing rates decreases the clustering performance. Traditional clustering is applied on the whole data without dealing with data distribution. This paper presents an efficient framework called Fuzzy c-means clustering for Incomplete Data streams (FID) that works adaptively with incomplete data streams even with high missing rates. The proposed FID estimates missing values based on the corresponding nearest-neighbors' intervals. To overcome the previously mentioned data streams clustering problems, the continuous clustering mechanism is adopted and extended to accurately handle the incomplete data streams. Experimental results using two different data sets prove the efficiency of the proposed FID comparing to the alternative approaches.																	1064-1246	1875-8967					2020	38	3			SI		3213	3227		10.3233/JIFS-191184													
J								Comparison of particle swarm optimization variants with fuzzy dynamic parameter adaptation for modular granular neural networks for human recognition	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Modular neural networks; granular computing; particle swarm optimization; fuzzy adaptation; human recognition; ear recognition; iris recognition; face recognition; pattern recognition	ALGORITHM; LOGIC; DESIGN; PSO	In this paper dynamic parameter adjustment in particle swarm optimization (PSO) for modular neural network (MNN) design using granular computing and fuzzy logic (FL) is proposed. Nowadays, there are a plethora of optimization techniques, but their implementations require having knowledge about these techniques in order to establish their parameters, because the performance and final results of a particular technique depend on the optimal parameter values. For this reason, in this paper the fuzzy adjustment of parameters during the execution is proposed, and this proposal allows to adjust the parameters depending on current PSO behavior in each iteration. The proposed method performs modular neural network optimization applied to human recognition using benchmark ear, iris and face databases. Two fuzzy inference systems are proposed to perform this dynamic adjustment, comparisons against a PSO without this dynamic adjustment (simple PSO) are performed to verify if the proposed adjustment using a fuzzy system is better improving recognition rate and execution time. The PSO variants presented in this paper are aimed at performing MNNs optimization. This optimization consists on finding optimal parameters, such as: the number of modules (or sub granules), percentage of data for the training phase, learning algorithm, goal error, number of hidden layers and their number of neurons.																	1064-1246	1875-8967					2020	38	3			SI		3229	3252		10.3233/JIFS-191198													
J								Fuzzy complete lattices, Alexandrov L-topologies and fuzzy rough sets	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Complete residuated lattices; fuzzy join(meet)-complete lattices Alexandrov L-pre(co)topologies; fuzzy rough sets	APPROXIMATION OPERATORS; CATEGORIES; PREORDERS	We introduce the concepts of fuzzy join complete lattices and Alexandrov L-pretopologies in complete residuated lattices. We show that fuzzy join complete lattices, Alexandrov L-pretopologies, fuzzy meet complete lattices and Alexandrov L-precotopologies are equivalent. Moreover, we define L-preinterior operators (resp. L-preclosure operators) as a viewpoint of fuzzy joins (resp. fuzzy meet) and fuzzy rough sets. Furthermore their properties and examples are investigated.																	1064-1246	1875-8967					2020	38	3			SI		3253	3266		10.3233/JIFS-191344													
J								A priority index based method for identifying influential design factors in product service-oriented designs	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Service-oriented design; fuzzy analytic hierarchy process; PageRank; MATLAB GUI; influential design factors	QUALITY FUNCTION DEPLOYMENT; GROUP DECISION-MAKING; OF-THE-ART; FUZZY QFD; SYSTEM; IDENTIFICATION; FRAMEWORK; SUPPORT	Service-oriented design is used in product development to accommodate diverse customer requirements and provide a profit-making strategy. Existing designs encounter difficulties in assessing design alternatives systematically during conceptual design involving customer heterogeneity and cognition vagueness. To evaluate these alternatives, a new systematic service-oriented design is proposed. The fuzzy analytic hierarchy process is used to handle the subjectivity and uncertainty of expert judgments and customer desires. In addition, the structure of service-oriented design within a mapping information flow is illustrated and then associated with technical characteristics via the results of the House of Quality. A consideration of influential design factors is developed to identify optimal alternative on the basis of the PageRank algorithm. Based on the integrated methods, a priority index is proposed to evaluate these alternatives, which can flexibly handle customer heterogeneity under limited technical conditions. At the same time, a design calculation program of a front axle suspension system was developed based on MATLAB GUI, which shows the design extensibility and robustness of the proposed approach. Overall, the results of the priority index-based method clearly demonstrate the superiority and appropriateness of the technique in selecting the optimal alternative. It also standardizes the design process from the case study of the front axle suspension system, provides rapid reasonable selection of the design scheme, and thereby improving intelligent design capacity from the perspective of product and its services.																	1064-1246	1875-8967					2020	38	3			SI		3267	3284		10.3233/JIFS-191499													
J								Mobility robustness optimization and load balancing in self-organized cellular networks: Towards cognitive network management	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Next-generation mobile networks; reinforcement learning; handover optimization; load balancing; network automation		Self-Organization networking (SON) consists of function sets which are responsible for automatically reliable configuring, planning and optimizing next generation mobile networks. Effective self-organization functions improve the level of network key performance indicators by determining optimal network setting and continuously finding efficient solutions that will be very hard for experts to distinguish. Most current self-organization networking functions apply rule-based recommended systems to control network resources in which performance metrics are evaluated and the effective actions are performed in accordance with a set of command sequences which such algorithms are too complicated to design, because rules and command sequences should be derived for each target index during each possible scenario. This research has proposed cognitive wireless networks as a fully intelligent approach to self-organization networking. We generalize the concept of network automation considering fuzzy-based self-organization networking functions as Q-learning problems in which, a framework is described to find the fuzzy optimal solution of linear programming optimization problem. The achieved results prove that the proposed cognitive approach, provides a prominent cellular framework for developing self-organization solutions, particularly where the relevance of metrics to the control indices is not clearly known. Also, assessment of the scheme in multiple-speed scenarios revealed that Q-learning load balancing obtains more accurate results compared to rule-based adaptive load balancing methods. This is particularly correct in dynamic networks, with high-speed users.																	1064-1246	1875-8967					2020	38	3			SI		3285	3300		10.3233/JIFS-191558													
J								An implementation of Chinese postman problem with priorities	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Graph; Eulerian tour; road network; higher priority node		In this paper a simplification of the CPP is measured, a set of important nodes is given in a linear order and the work is to path all edges at least once in such a technique that the advanced priority nodes are stayed as soon as possible. All roads in the networks are covered by postman tour.postman tour covering all the roads in the network. The solutions found here are valid for the case, where the cost of additional edges traversed is much bigger that the cost of delays and delay for the first priority node is much bigger than the cost of delay for the second node, and so on. More detailed study of various cost functions may be an interesting topic for future research.																	1064-1246	1875-8967					2020	38	3			SI		3301	3305		10.3233/JIFS-190035													
J								Pythagorean fuzzy multi-criteria decision making method based on CODAS with new score function	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Pythagorean fuzzy number; CODAS; score function; MCDM	AGGREGATION OPERATORS; FUNDAMENTAL PROPERTIES; MEMBERSHIP GRADES; TOPSIS METHOD; SOFT SETS; INFORMATION; ALGORITHMS; EXTENSION; WDBA	Pythagorean fuzzy set (PFS), as a generalization of intuitionistic fuzzy set (IFS), is more suitable to capture the indeterminacy of the experts' decision making information. This paper is designed to build new algorithm for managing multi-criteria decision making (MCDM) issue under Pythagorean fuzzy environment. First, we initiate a novel score function based Pythagorean fuzzy number (PFN). Later, we explore an algorithm for solving MCDM problem based on CODAS (COmbinative Distance-based AS sessment). Ultimately, the availability of method is stated by some numerical examples. The dominating traits of the developed algorithm, compared to some existing Pythagorean fuzzy decision making algorithms, are (1) derive a ranking without the complex process; (2) achieve the optimal alternative without counterintuitive phenomena; (3) strong ability to differentiate the optimal alternative.																	1064-1246	1875-8967					2020	38	3			SI		3307	3318		10.3233/JIFS-190043													
J								Adaptive soft subspace clustering combining within-cluster and between-cluster information	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Soft subspace clustering; within-cluster compactness; between-cluster distance; not increase any control parameters	FEATURES	For the uncertain problem that between-cluster distance influences clustering in the soft subspace clustering (SSC) process, a novel clustering technique called adaptive soft subspace clustering (ASSC) is proposed by employing both within-cluster and between-cluster information. First, a new objective function is constructed by minimizing the within-cluster compactness and maximizing the between-cluster distance based on the framework of SSC algorithm. Based on this objective function, a new way of computing clusters' feature weights, centers and membership is then derived by using Lagrange multiplier method. The uniqueness of ASSC is that the objective function does not increase any control parameters, which can avoid the sensitivity of clustering results to the initial points of the control parameters. The properties of this algorithm are investigated and the performance is evaluated experimentally using UCI datasets. The contrastive experiment results demonstrate that the accuracy and the stability of the proposed algorithm outperform the four existing clustering algorithms, i.e., ESSC, EWKM, FWKM and CIM_QPSO_SSC.																	1064-1246	1875-8967					2020	38	3			SI		3319	3330		10.3233/JIFS-190146													
J								A modified nonmonotone QP-free method without penalty function or filter	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Inequality constrained optimization; QP-free method; nonmonotone; working set; global convergence	SUPERLINEARLY CONVERGENT ALGORITHM; LINEAR-EQUATIONS ALGORITHM; GLOBALLY CONVERGENT; SEQUENTIAL SYSTEMS	In this paper, a modified nonmonotone QP-free method without penalty function or filter is proposed for inequality constrained optimization. There is only two or three systems of linear equations with the same coefficients are solved at each iteration. We obtain a fundamental direction and the corresponding multiplier by the first equation, and then make full use of Lagrangian function information and multiplier to bend the search direction appropriately and obtain the search direction by the second linear equation. Moreover, the acceptable criterion of trial points is relaxed by the modified nonmonotone linear search technique. Under mild conditions, the global convergence of the algorithm is proved. Numerical results are given at the end of the paper.																	1064-1246	1875-8967					2020	38	3			SI		3331	3342		10.3233/JIFS-190475													
J								Generalized Shapley probability neutrosophic hesitant fuzzy Choquet aggregation operators and their applications	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Probabilistic neutrosophic hesitant fuzzy set; multi-criteria group decision-making (MCGDM); Choquet integral; shapley function; aggregation operator	DECISION-MAKING METHOD; SIMILARITY MEASURES; SETS; FILTERS	The probabilistic neutrosophic hesitant fuzzy numbers are considered to be effective tools for dealing with such decision problems when both subjective and objective uncertainties exist simultaneously. However, the existing methods for dealing with real-life decision-making in the context is based on the assumption that the relationships between all criteria are independent and irrelevant. It is worth noting that this assumption is not sufficient. In fact, there may be interrelationships between attributes. In order to consider the correlation between factors from a more global perspective, the generalized Shapley probabilistic neutrosophic hesitant fuzzy Choquet averaging (GS-PNHFCA) operator and the generalized Shapley probabilistic hesitant fuzzy Choquet geometric (GS-PNHFCG) operator are investigated. Next, in order to find the optimal weight vector about DMs and criteria, a model is constructed by the maximizing score deviation (MSD) method. In addition, based on the integrated operators and built models, an algorithm for solving the MCGDM problem of PNHFN is designed. The effectiveness and practicability of the algorithm is proved by comparison with existing results.																	1064-1246	1875-8967					2020	38	3			SI		3343	3357		10.3233/JIFS-190493													
J								Fuzzy AHP based identification model for efficient application development	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Agile process; fuzzy AHP; TOPSIS; web application; identification model	GROUP DECISION-MAKING; ANALYTIC HIERARCHY PROCESS; PREFERENCE RELATIONS; TOPSIS; WEIGHTS	Irrespective of the nature of the software, the choice of a software development process plays a crucial role. A good choice may help expand the business to heights while a bad choice may wreak havoc for the stakeholders. Therefore, selecting the right software development process is an indispensable element during project planning. Undoubtedly, web-based applications differ from conventional software and are becoming so popular in society due to their distinguishing features and widespread coverage. Web application software is generally found fit candidates to be developed according to agile software processes. This article attempts to provide a model for advising an agile software process during the planning phase of a web application development project. The proposed model takes into account the underlying web project characteristics and uses the Fuzzy Analytic Hierarchy Process (FAHP) along with TOPSIS techniques for suitable agile candidate selection. The model is also validated on a primary dataset obtained by developing twenty web application projects through four student teams. The results show that the proposed model is successful in suggesting a correct agile process with a probability of the order of 0.8.																	1064-1246	1875-8967					2020	38	3			SI		3359	3370		10.3233/JIFS-190508													
J								Optimal selection of design scheme in cloud environment: A novel hybrid approach of multi-criteria decision-making based on F-ANP and F-QFD	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy analytic network process; fuzzy quality function development; multi-criteria decision-making; optimal selection; cloud environment	MCDM APPROACH; FRAMEWORK; EVALUATE; SYSTEMS; MODEL; SET	In order to achieve optimal selection of design schemes in cloud environment, this paper proposed a novel hybrid Multi-Criteria Decision-Making (MCDM) model integrated Fuzzy Analytic Network Process (fuzzy-ANP) and Fuzzy Quality Function Development (fuzzy-QFD). There are three steps in the novel approach. In the first step, the evaluation target system of design scheme is identified considering four dimensions: economy, society, environment, and culture. The proper indicators are identified by the integration of multi-intelligent techniques. In the second step, decision-makers are asked to compare the decision indicators, and the weight for each indicator is determined using the intuitionistic fuzzy number and fuzzy-ANP. In the third step, the decision-making system of design scheme is developed to compare and rank alternatives. The decision-makers are invited over to compare different options and rank them with the aid of fuzzy-QFD in the cloud environment. A case study is provided to validate the proposed approach. Twenty-five sensitivity analysis experiments are conducted to figure out the influence of evaluation indicators on decision making process. The novel approach makes use of the strength of fuzzy set theory in handing vagueness and uncertainty, the fuzzy-ANP in non-independent hierarchy evaluation on the indicator system, the advantage of fuzzy-QFD in multiple-objective decision analysis. Based on the comparative study and assessment, the results show that the proposed approach is more efficient and provided users with multidimensional evaluation.																	1064-1246	1875-8967					2020	38	3			SI		3371	3388		10.3233/JIFS-190630													
J								Early fault warning of wind turbine based on BRNN and large sliding window	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										BRNN; box-plot; large sliding window; Letts'criterion; early fault warning	FUZZY; SYSTEM; MODEL	With the construction of large-scale wind turbines, how to reduce the operation and maintenance costs has become an urgent problem to be solved. In this paper, by extracting the actual operation data of the wind turbine in Supervisory Control and Data Acquisition (SCADA) system, the Bidirectional Recurrent Neural Networks (BRNN) is used to establish the wind turbine operation prediction model. By eliminating abnormal data points caused by accidental factors through box diagram, the fault risk threshold of wind turbine components is optimized Then, based on the residual between the actual value and the measured value of the large sliding window, the early fault warning is realized according to Wright criterion. Finally, the model proposed in this paper is applied to the actual wind turbine, which proves the reliability and accuracy of the method.																	1064-1246	1875-8967					2020	38	3			SI		3389	3401		10.3233/JIFS-190642													
J								The uncertain DEA models for specific scale efficiency identification	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Data envelopment analysis; uncertainty theory; specific scale efficiency identification; stability analysis	DATA ENVELOPMENT ANALYSIS	Identifying the specific scale status by evaluating scale efficiency of each DMU (decision making unit) is critical for policy makers to make a correct decision on inefficiency improvement. Although some traditional DEA (Data Envelopment Analysis) models can evaluate different scale efficiencies and identify the specific status of each DMU, they are unable to deal with the evaluation indicators with imprecise observation. Therefore, we propose two different uncertain DEA models with imprecise inputs and outputs to recognize two scale status of increasing returns to scale (IRS) and decreasing returns to scale (DRS), which can better help policy makers make a correct decision on inefficiency improvement without accurate data. Moreover, we analyse the stability of these uncertain DEA models to make effective suggestions for decision makers to further improve the inefficient DMU(o )or to adjust the inputs and outputs on the premise of remaining the new DMUo efficient.																	1064-1246	1875-8967					2020	38	3			SI		3403	3417		10.3233/JIFS-190662													
J								Application of nature-inspired algorithms (NIA) for optimization of video compression	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Motion estimation; nature inspired algorithms; genetic algorithm; particle swarm optimization; cuckoo search	DIAMOND SEARCH ALGORITHM; BLOCK-MATCHING ALGORITHM; PATTERN SEARCH; MOTION	Video compression is applied for reducing the requirement of hardware, bandwidth, hard drives and power consumption for storing and processing an excessive amount of data generated by videos. The computationally intensive and most time-consuming segment of video compression is known as motion estimation (ME). ME process can be regarded as an optimization problem where search is carried out in a predefined search area of the target frame to locate the identical macroblock (MB) corresponding to each MB in anchor frame by minimizing the objective function cum search criterion as minimum value of search criterion identifies the location of the best matching MB. Since the efficiency of ME decides the efficiency of Video compression, a rich number of fast block matching algorithms (BMAs) were reported to maintain the tradeoff between the computational complexity and visual experience of video during the ME process. Investigation reveals that most of the pattern-based BMAs are prone to the local optimum and stuck in sub-optimal results. Due to the emergence of various nature-inspired algorithms (NIA) like particle swarm optimization (PSO), genetic algorithm (GA), evolutionary algorithm, etc. and their application in optimizing all types of day to day problems has opened a new era in the field of ME. Our investigation focuses on the application of all types of NIA reported to date for optimizing the ME process in terms of speed, accuracy, and quality. This investigation will analyze all the NIAs and their methodologies through an extensive study of their accompanying publications and will enable us to do a detailed comparison to highlight the competitive advantage of soft computing techniques over existing pattern-based algorithms.																	1064-1246	1875-8967					2020	38	3			SI		3419	3443		10.3233/JIFS-190308													
J								Numerical solution of several kinds of differential equations using block neural network method with improved extreme learning machine algorithm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Differential equations; block neural network; IELM algorithm	FEEDFORWARD NETWORKS; APPROXIMATE	In this paper, block neural network (BNN) method is proposed to solve several kinds of differential equations. BNN is used to construct approximating functions and its derivatives, the improved extreme learning machine (IELM) algorithm are designed to train network weights. To evaluate the performance of the proposed method, numerical examples are performed by the presented method. Comparison of the numerical results with exact solutions validate the feasibility of the proposed method in accuracy. Results compared with other recent research works also validate the superiority of the proposed approach. Numerical results show that the proposed BNN with IELM algorithm perform well in accuracy and requires less hidden neurons.																	1064-1246	1875-8967					2020	38	3			SI		3445	3461		10.3233/JIFS-190406													
J								A hierarchical coarse-to-fine perception for small-target categorization of butterflies under complex backgrounds	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Convolutional neural network; coarse-to-fine perception; deep learning; small target categorization; butterfly images	IMAGE; CLASSIFICATION; NETWORKS; SCALE; MODEL	Small-target categorization of butterflies suffers from large-scale search space of candidate target locations, subtler discriminations, camouflaged appearances, and complex backgrounds. Precise localization and domain-specific discrimination extraction are crucial for this issue. In this work, a novel hierarchical coarse-to-fine convolutional neural network (C-t-FCNN) was proposed. It consists of CoarseNet and FineNet, which incorporate object-level and part-level representations into framework. Specifically, the coarse-grained features containing the orientation description are generated by CoarseNet, while the fine-grained discriminations with semantic distinctiveness are captured by FineNet. Next, the correspondences are established to mark the target regions, background regions, and mismatched regions depending on the quantification of scale-invariant feature transform (SIFT) descriptors. Then, the features are subsampled via spatial pyramid pooling (SPP) for size uniformity and integration. Finally, the irrelevant background and mismatched regions are eliminated by the support vector machine (SVM) with a radial basis function (RBF) kernel, leaving only the target-specific patches for finer-scale extraction. Hence the numeracy can be economized from identifying irrelevant areas and can be rescheduled in feature extraction and final decision, which can suppress time complexity simultaneously. A total of 119,016 augmented butterfly images spanning 47 categories are utilized for model training, while 13,734 images are evaluated for effectiveness verification. The C-t-FCNN delivers impressive performance, i.e., it achieves a validation accuracy of 92.08% and a testing accuracy of 91.6%, which outperforms state-of-the-arts.																	1064-1246	1875-8967					2020	38	3			SI		3463	3487		10.3233/JIFS-190747													
J								Optimization of spares varieties in the uncertain systems	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Spares varieties; uncertain systems; optimization; data envelopment analysis	DATA ENVELOPMENT ANALYSIS; DEA MODEL; EFFICIENCY; LOGISTICS	Stocking strategy for spare parts has a pivotal influence on the productivity and efficiency of industrial plants. Considering the situation of lacking historical statistics causing by uncertainty, this paper proposes a method to optimize spares varieties based on uncertainty theory and uncertain data envelopment analysis (DEA) model. Firstly, a recursive hierarchy structure is established to construct an evaluation system to meet the requirement of avoiding attributes' redundancy. Then, an uncertain spares optimization model (USOM), which is based on uncertain DEA model, is developed to optimize spares varieties. Furthermore, the uncertainty theory is utilized to convert the USOM into an equivalent deterministic model for simplification. Finally, a numerical example is given to illustrate the performance of this model. The results show that the stocking strategy obtained from the proposed decision model can satisfy the purpose of saving resources and prompting continuous operation.																	1064-1246	1875-8967					2020	38	3			SI		3489	3499		10.3233/JIFS-190838													
J								Token based crack detection	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Crack detection; crack token; machine learning; edge detection; crack recognition		Crack detection has drawn much attention in the last two decades, because of dramatic bloom in monitoring images and the urgent need of corresponding crack detection. However, recent methods have not taken advantage of structure information effectively, resulting in low accuracy when dealing with crack-like noises. In this paper, we propose a novel crack detection framework, which is able to identify cracks from noisy background. The main contributions of this paper are as follows: (1) giving a new edge-based crack detection framework to improve the detection performance; (2) proposing a novel mid-level feature, named Crack Token, which captures the local structure information of cracks; (3) introducing a new evaluation strategy for crack detection task, which provides a comprehensive system for approach evaluation and comparison in this area. In addition, we provide a novel definition of pavement crack and verify our framework and evaluation strategy in this real world application. Extensive experiments demonstrate the state-of-the-art results of the proposed framework.																	1064-1246	1875-8967					2020	38	3			SI		3501	3513		10.3233/JIFS-190868													
J								Blind image quality assessment based on statistics features and perceptual features	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Blind image quality assessment; natural scene statistics feature; perceptual feature; color statistics feature; support vector regression	NATURAL SCENE STATISTICS; ARTIFACTS	Blind image quality assessment (BIQA) aims to evaluate the quality of an image without information regarding its reference image. In this paper, we proposed a novel BIQA method, which combines thirty six natural scene statistics (NSS) features, two color statistics features and four perceptual features to construct an image quality assessment model. Support Vector Regression (SVR) is adopted to build the relationship between these features and image quality scores, yielding a measure of image quality. Experimental results in LIVE, TID2013 databases and their cross validations show that the proposed method records a higher correlations with human subjective judgments of visual quality and delivers highly competitive performance with state-of-the-art BIQA models.																	1064-1246	1875-8967					2020	38	3			SI		3515	3526		10.3233/JIFS-190998													
J								Spectral Variability Aware Blind Hyperspectral Image Unmixing Based on Convex Geometry	IEEE TRANSACTIONS ON IMAGE PROCESSING										Geometry; Hyperspectral imaging; Estimation; Robustness; Electronic mail; Analytical models; Hyperspectral imaging; remote sensing; spectral unmixing; endmember variability; convex geometry; nonnegative matrix factorization	ENDMEMBER VARIABILITY; INTRINSIC DIMENSION; COMPONENT ANALYSIS; FAST ALGORITHM; SIMPLEX; EXTRACTION; IDENTIFIABILITY; PROJECTION	Hyperspectral image unmixing has proven to be a useful technique to interpret hyperspectral data, and is a prolific research topic in the community. Most of the approaches used to perform linear unmixing are based on convex geometry concepts, because of the strong geometrical structure of the linear mixing model. However, many algorithms based on convex geometry are still used in spite of the underlying model not considering the intra-class variability of the materials. A natural question is to wonder to what extent these concepts and tools (Intrinsic Dimensionality estimation, endmember extraction algorithms, pixel purity) are still relevant when spectral variability comes into play. In this paper, we first analyze their robustness in a case where the linear mixing model holds in each pixel, but the endmembers vary in each pixel according to a prescribed variability model. In the light of this analysis, we propose an integrated unmixing chain which tries to adress the shortcomings of the classical tools used in the linear case, based on our previously proposed extended linear mixing model. We show the interest of the proposed approach on simulated and real datasets.																	1057-7149	1941-0042					2020	29						4568	4582		10.1109/TIP.2020.2974062													
J								3D Skeletal Gesture Recognition via Hidden States Exploration	IEEE TRANSACTIONS ON IMAGE PROCESSING										Hidden Markov models; Three-dimensional displays; Gesture recognition; Matrix decomposition; Electronic mail; Manifolds; Robustness; Gesture recognition; hidden Markov model; deep neural networks; matrix decomposition	ACTION SEGMENTATION; ACTIONLET ENSEMBLE; POSE	Temporal dynamics is an open issue for modeling human body gestures. A solution is resorting to the generative models, such as the hidden Markov model (HMM). Nevertheless, most of the work assumes fixed anchors for each hidden state, which make it hard to describe the explicit temporal structure of gestures. Based on the observation that a gesture is a time series with distinctly defined phases, we propose a new formulation to build temporal compositions of gestures by the low-rank matrix decomposition. The only assumption is that the gesture's "hold" phases with static poses are linearly correlated among each other. As such, a gesture sequence could be segmented into temporal states with semantically meaningful and discriminative concepts. Furthermore, different to traditional HMMs which tend to use specific distance metric for clustering and ignore the temporal contextual information when estimating the emission probability, we utilize the long short-term memory to learn probability distributions over states of HMM. The proposed method is validated on multiple challenging datasets. Experiments demonstrate that our approach can effectively work on a wide range of gestures, and achieve state-of-the-art performance.																	1057-7149	1941-0042					2020	29						4583	4597		10.1109/TIP.2020.2974061													
J								Compressive Radar Imaging of Stationary Indoor Targets With Low-Rank Plus Jointly Sparse and Total Variation Regularizations	IEEE TRANSACTIONS ON IMAGE PROCESSING										Clutter; Radar imaging; TV; Image reconstruction; Optimization; Antenna measurements; Through-the-wall radar imaging; wall clutter mitigation; compressed sensing; regularized optimization; low-rank matrix recovery; sparse signal reconstruction; proximal gradient technique	WALL CLUTTER MITIGATION; THRESHOLDING ALGORITHM; WAVELET SHRINKAGE; SIGNAL RECOVERY; RECONSTRUCTION; PROJECTION	This paper addresses the problem of wall clutter mitigation and image reconstruction for through-wall radar imaging (TWRI) of stationary targets by seeking a model that incorporates low-rank (LR), joint sparsity (JS), and total variation (TV) regularizers. The motivation of the proposed model is that LR regularizer captures the low-dimensional structure of wall clutter; JS guarantees a small fraction of target occupancy and the similarity of sparsity profile among channel images; TV regularizer promotes the spatial continuity of target regions and mitigates background noise. The task of wall clutter mitigation and target image reconstruction is formulated as an optimization problem comprising LR, JS, and TV regularization terms. To handle this problem efficiently, an iterative algorithm based on the forward-backward proximal gradient splitting technique is introduced, which captures wall clutter and yields target images simultaneously. Extensive experiments are conducted on real radar data under compressive sensing scenarios. The results show that the proposed model enhances target localization and clutter mitigation even when radar measurements are significantly reduced.																	1057-7149	1941-0042					2020	29						4598	4613		10.1109/TIP.2020.2973819													
J								Image Deblurring Utilizing Inertial Sensors and a Short-Long-Short Exposure Strategy	IEEE TRANSACTIONS ON IMAGE PROCESSING										Kernel; Sensors; Image sensors; Estimation; Cameras; Image restoration; Blur kernel estimation; deblur; exposure strategy; half-blind deconvolution; inertial sensors	MODEL	Image blur caused by camera movement is common in long-exposure photography. A recent approach to address image blur is to record camera motion via inertial sensors in imaging equipment such as smartphones and single-lens reflex (SLR) cameras. However, because of device performance limitations, directly estimating a blur kernel from sensor data is infeasible. Previous works that have attempted to correct blurry image content via sensor data have also been susceptible to theoretical defects. Here, we propose a novel method of deblurring images that uses inertial sensors and a short-long-short (SLS) exposure strategy. Assisted short-exposure images captured before and after the formal long-exposure image are employed to correct the sensor data. A half-blind deconvolution algorithm is proposed to refine the estimated kernel. An extra smoothing filter is integrated into the framework to address the coarse initial kernel. Hence, we propose a fast solution for optimization that uses the iteratively reweighted least squares (IRLS) method in the frequency domain. We evaluate these methods via several blind deconvolutions. Quantitative indicators and the visual performance of the image deblurring results show that our method performs better than previous methods in terms of image quality restoration and computational time cost. This method will increase the feasibility of applying deblurring to imaging devices.																	1057-7149	1941-0042					2020	29						4614	4626		10.1109/TIP.2020.2973499													
J								Text Co-Detection in Multi-View Scene	IEEE TRANSACTIONS ON IMAGE PROCESSING										Task analysis; Visualization; Distortion; Couplings; Feature extraction; Image color analysis; Kernel; Text co-detection; cycle consistency; epipolar geometrical guidance	GRAPH; LOCALIZATION; OPTIMIZATION; RECOGNITION; ALGORITHM; TRACKING	Multi-view scene analysis has been widely explored in computer vision, including numerous practical applications. The texts in multi-view scenes are often detected by following the existing text detection method in a single image, which however ignores the multi-view corresponding constraint. The multi-view correspondences may contain structure, location information and assist difficulties induced by factors like occlusion and perspective distortion, which are deficient in the single image scene. In this paper, we address the corresponding text detection task and propose a novel text co-detection method to identify the co-occurring texts among multi-view scene images with compositions of detection and correspondence under large environmental variations. In our text co-detection method, the visual and geometrical correspondences are designed to explore texts holding high pairwise representation similarity and guide the exploitation of texts with geometrical correspondences, simultaneously. To guarantee the pairwise consistency among multiple images, we additionally incorporate the cycle consistency constraint, which guarantees alignments of text correspondences in the image set. Finally, text correspondence is represented by a permutation matrix and solved via positive semidefinite and low-rank constraints. Moreover, we also collect a new text co-detection dataset consisting of multi-view image groups obtained from the same scene with different photographing conditions. The experiments show that our text co-detection obtains satisfactory performance and outperforms the related state-of-the-art text detection methods.																	1057-7149	1941-0042					2020	29						4627	4642		10.1109/TIP.2020.2973511													
J								Deep Collaborative Multi-View Hashing for Large-Scale Image Search	IEEE TRANSACTIONS ON IMAGE PROCESSING										Semantics; Optimization; Fuses; Feature extraction; Collaboration; Correlation; Visualization; Deep multi-view hashing; image search; efficient discrete optimization		Hashing could significantly accelerate large-scale image search by transforming the high-dimensional features into binary Hamming space, where efficient similarity search can be achieved with very fast Hamming distance computation and extremely low storage cost. As an important branch of hashing methods, multi-view hashing takes advantages of multiple features from different views for binary hash learning. However, existing multi-view hashing methods are either based on shallow models which fail to fully capture the intrinsic correlations of heterogeneous views, or unsupervised deep models which suffer from insufficient semantics and cannot effectively exploit the complementarity of view features. In this paper, we propose a novel Deep Collaborative Multi-view Hashing (DCMVH) method to deeply fuse multi-view features and learn multi-view hash codes collaboratively under a deep architecture. DCMVH is a new deep multi-view hash learning framework. It mainly consists of 1) multiple view-specific networks to extract hidden representations of different views, and 2) a fusion network to learn multi-view fused hash code. DCMVH associates different layers with instance-wise and pair-wise semantic labels respectively. In this way, the discriminative capability of representation layers can be progressively enhanced and meanwhile the complementarity of different view features can be exploited effectively. Finally, we develop a fast discrete hash optimization method based on augmented Lagrangian multiplier to efficiently solve the binary hash codes. Experiments on public multi-view image search datasets demonstrate our approach achieves substantial performance improvement over state-of-the-art methods.																	1057-7149	1941-0042					2020	29						4643	4655		10.1109/TIP.2020.2974065													
J								Soft-Edge Assisted Network for Single Image Super-Resolution	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image reconstruction; Image edge detection; Image resolution; Feature extraction; Task analysis; Convolutional neural networks; Computational modeling; Edge assistance; soft-edge; convolutional neural network; single image super-resolution; image restoration		The task of single image super-resolution (SISR) is a highly ill-posed inverse problem since reconstructing the high-frequency details from a low-resolution image is challenging. Most previous CNN-based super-resolution (SR) methods tend to directly learn the mapping from the low-resolution image to the high-resolution image through some complex convolutional neural networks. However, the method of blindly increasing the depth of the network is not the best choice because the performance improvement of such methods is marginal but the computational cost is huge. A more efficient method is to integrate the image prior knowledge into the model to assist the image reconstruction. Indeed, the soft-edge has been widely applied in many computer vision tasks as the role of an important image feature. In this paper, we propose a Soft-edge assisted Network (SeaNet) to reconstruct the high-quality SR image with the help of image soft-edge. The proposed SeaNet consists of three sub-nets: a rough image reconstruction network (RIRN), a soft-edge reconstruction network (Edge-Net), and an image refinement network (IRN). The complete reconstruction process consists of two stages. In Stage-I, the rough SR feature maps and the SR soft-edge are reconstructed by the RIRN and Edge-Net, respectively. In Stage-II, the outputs of the previous stages are fused and then fed to the IRN for high-quality SR image reconstruction. Extensive experiments show that our SeaNet converges rapidly and achieves excellent performance under the assistance of image soft-edge. The code is available at https://gitlab.com/junchenglee/seanet-pytorch.																	1057-7149	1941-0042					2020	29						4656	4668		10.1109/TIP.2020.2973769													
J								Research on the evaluation model of deep foundation pit supporting structures in urban traffic tunnels	INTERNATIONAL JOURNAL OF BIOMETRICS										urban traffic tunnel; supporting structure; load; evaluation model; mechanics	BEHAVIOR	In order to improve the building mechanics performance of the foundation pit supporting structure of urban traffic tunnel and guide the engineering design and construction, the evaluation model of the deep foundation pit supporting structure of urban traffic tunnel is proposed based on mechanical anchoring and linear mechanical loading. The mechanical anchoring method is used to load the support structure of deep foundation pit of urban traffic tunnel. The constitutive relation of dynamic node of deep foundation pit supporting structure of urban traffic tunnel under moving load is constructed, and the tangent elastic modulus of supporting structure of deep foundation pit of tunnel is calculated according to equivalent stress-strain relation. Thus, the mechanical evaluation and optimisation design of deep foundation pit supporting structure of urban traffic tunnel are realised. The test results show that the mechanical loading performance of the model is good for the design of the deep foundation pit supporting structure of urban traffic tunnel, and the load capacity of the supporting structure is improved, and the engineering design of the foundation pit supporting structure is optimised.																	1755-8301	1755-831X					2020	12	1			SI		3	12		10.1504/IJBM.2020.105621													
J								Research on the optimisation of complex models of large-scale building structures dependent on adaptive grey genetic algorithms	INTERNATIONAL JOURNAL OF BIOMETRICS										large-scale buildings; adaptive; grey genetic algorithm; structural optimisation		Genetic algorithm (GA) is a bionics algorithm based on the biological evolution theory that has received extensive attention in the field of computer science and optimisation in recent years. This paper analyses and integrates the relevant contents of genetic algorithm and its application in the optimal design of large-scale building structures and analyses and researches briefly several key factors when the genetic algorithm is applied to the optimal design of large-scale building structures, such as mathematical modelling, constraint condition treatment, generation of initial population and selection of control parameters of genetic algorithm. However, because the simple genetic algorithm is only good at global search, and the local search ability is not enough, it will take quite a long time to achieve the real optimal solution. For the shortcomings of simple genetic algorithm, an improved adaptive grey genetic algorithm is proposed in this paper. The example shows that the obtained adaptive genetic algorithm can improve the convergence and calculation speed when the genetic algorithms is applied to structural optimisation design.																	1755-8301	1755-831X					2020	12	1			SI		13	28		10.1504/IJBM.2020.105620													
J								Linearisation control of AC permanent magnet synchronous motor servo system based on sensor technology	INTERNATIONAL JOURNAL OF BIOMETRICS										permanent magnet synchronous motor; PMSM; direct feedback linearisation; linearisation control; MATLAB simulation; sensor		AC permanent magnet synchronous motor servo control system is a complex nonlinear, strong coupling and time-varying system. It has strong uncertainty and nonlinearity, and when the system is running, it also will be disturbed to varying degrees, so the conventional control strategy is difficult to meet the control requirements of high accuracy, high speed and high-performance servo system. This paper adopts a direct feedback linearisation control strategy based on sensor technology and uses, as the output of the system to realise the decoupling of the system. In addition, the grey prediction is added to overcome the shortcomings of direct feedback linearisation that is sensitive to parameters. Adjusting the uncertain factors block by grey prediction to adapt to the direct feedback linearisation rule and achieve the desired effect. MATLAB/Simulink is used to complete the simulation of servo control algorithm. The simulation results show that the direct feedback linearisation control is better than the conventional PID control, and the direct feedback linearisation control algorithm with grey prediction can improve the performance of the permanent magnet synchronous motor servo control system and can meet the basic requirements of the high-performance servo control system.																	1755-8301	1755-831X					2020	12	1			SI		29	46		10.1504/IJBM.2020.105619													
J								The design of embedded image teaching systems based on ARM technology	INTERNATIONAL JOURNAL OF BIOMETRICS										ARM technology; embedded image; teaching system		With the continuous development of multimedia and speech teaching laboratory in colleges and universities, and the application of digital processing technology, digital voice teaching equipment is attracting more and more attention from domestic universities and instrument manufacturers. Based on the core technology of ARM and DSP dual core technology, the theoretical analysis of digital language learning system is carried out, and the student terminal circuit and teaching software system are designed in detail, and the function and technical index of the whole system are tested. The test results show that the digital speech learning system constructed by the student terminal controlled by ARM and DSP binuclear is fully satisfied with the actual language teaching requirements.																	1755-8301	1755-831X					2020	12	1			SI		47	55		10.1504/IJBM.2020.105618													
J								Research on network intrusion detection security based on improved extreme learning algorithms and neural network algorithms	INTERNATIONAL JOURNAL OF BIOMETRICS										extreme learning; network intrusion; neural network algorithm; detection; nonlinear time series analysis		In order to improve the ability of network fuzzy intrusion detection, a network intrusion detection method based on improved extreme learning algorithm and neural network algorithm is proposed to improve the security of the network. ARMA and other linear detection methods are used to construct the network intrusion signal model, and the nonlinear time series and chaos analysis methods are used to extract the feature of network intrusion and big data information analysis. The limit learning method is used for active detection of network intrusion; the adaptive learning method is used for iterative analysis of network intrusion detection, and the correlation characteristic decomposition method is used to improve the convergence of network intrusion detection. The fuzzy neural network algorithm is used to classify the network intrusion features to improve the intrusion detection performance. The simulation results show that this method has high accuracy and strong anti-jamming ability; it has good application value in network security.																	1755-8301	1755-831X					2020	12	1			SI		56	66		10.1504/IJBM.2020.105623													
J								Research on educational informatisation platforms based on Web 2.0	INTERNATIONAL JOURNAL OF BIOMETRICS										internet; Web 20; technology; network education; education platform		Educational informatisation is an important part of national informatisation strategy. The realisation of educational informatisation plays a fundamental, global and lasting role in social development. The advent of the Web 2.0 era has injected new impetus into the development of network education. Web 2.0 has the characteristic of personalisation, decentralisation, openness, interactivity, and sociality. It corresponds to the educational idea advocated by modern educational theory. Through the method of literature analysis, this paper collates and think about the current educational application research under the Web 2.0, and finally through specific case analysis and comparison to verify the views and draw conclusions and provide reference for the future research of Web 2.0 and educational informatisation platform.																	1755-8301	1755-831X					2020	12	1			SI		67	80		10.1504/IJBM.2020.105622													
J								Research on manipulator motion control based on neural network algorithms	INTERNATIONAL JOURNAL OF BIOMETRICS										neural network; manipulator; fuzzy PID; neural network		The manipulator is the new artificial intelligence device, and its motion control is the basis for ensuring the stability of the manipulator's attitude. The traditional manipulator motion control adopts the static neuron control method, which will lead to small disturbance in the attitude control of the manipulator, and cause the stable motion performance of the manipulator. A motion control algorithm for manipulator is proposed based on variable structure fuzzy PID neural network. The coordinate system structure description and manipulator dynamics analysis of the controlled system are carried out. The motion control algorithm of the manipulator is improved by using variable structure PID neural network control and adaptive disturbance suppression method. Combined with the strict feedback control method, the motion error of the manipulator is compensated, and the steady-state error is corrected by the adaptive inertial compensation method to realise the motion control optimisation of the manipulator. The simulation results show that the motion control algorithm of the manipulator has better positioning performance and better control stability, reduces the steady-state error and improves the control stability.																	1755-8301	1755-831X					2020	12	1			SI		81	90		10.1504/IJBM.2020.105624													
J								A study on the fatigue of bus drivers based on biological models	INTERNATIONAL JOURNAL OF BIOMETRICS										biological mathematical model; public transport driver; fatigue test; vision; nerve		In order to improve the fatigue detection ability of public transport drivers, the biometric modelling method is used to test and evaluate the fatigue of drivers. A fatigue detection model for public transport drivers is proposed based on biological mathematical model analysis and the prevention and evaluation according to the fatigue test model is constructed. According to the visual, neural and blood supply characteristics of public transport drivers, the mathematical model of quantitative recursive statistical analysis of public transport drivers' biological fatigue is established by using descriptive statistical analysis method. The problem of public transport driver testing is transformed into an optimal solution problem for a continuous time-delay non-smooth system. Under the condition of non-smooth autonomous continuous boundedness, a biologic mathematical model of fatigue detection is constructed. The delay-dependent sufficient conditions of public transport driver fatigue testing are obtained to prevent and monitor driving fatigue accurately. The test results show that the model is accurate for public traffic drivers' fatigue testing. According to the biometric test results, it can reliably reflect whether the driver is tired or not, so that the danger alarm can be carried out and the driving safety can be ensured.																	1755-8301	1755-831X					2020	12	1			SI		91	99		10.1504/IJBM.2020.105626													
J								Research on logistics distribution path analysis based on artificial intelligence algorithms	INTERNATIONAL JOURNAL OF BIOMETRICS										artificial intelligence algorithm; logistics distribution; ant colony search; shortest path optimisation		Logistics distribution path optimisation model design is the key to ensure the smooth flow of logistics distribution path network, the logistics distribution path optimisation is designed to improve the efficiency of logistics distribution, a logistics distribution path optimisation model is proposed based on artificial intelligence algorithm. A logistics distribution path search model based on rough set theory is established. Ant colony search method is used to design the artificial intelligence algorithm of logistics distribution path optimisation. Adaptive weighting method is used to extract and schedule the information of logistics distribution path, and the shortest path optimisation method is used to optimise the route planning of logistics distribution, which can reduce the path overhead and time cost of logistics distribution. The efficiency of logistics distribution is improved. The simulation results show that this method is used to construct the logistics distribution path model, which reduces the time cost and the road cost of the logistics distribution, and improves the throughput of the logistics distribution significantly.																	1755-8301	1755-831X					2020	12	1			SI		100	108		10.1504/IJBM.2020.105625													
J								Research on the optimal design of municipal roads based on genetic algorithms	INTERNATIONAL JOURNAL OF BIOMETRICS										genetic algorithm; municipal road; image; feature segmentation; networking		In order to improve the rationality of the design of municipal roads and realise the optimal planning and design of municipal roads, an empirical road optimisation design method based on genetic algorithm is proposed. In the sample of remote sensing image of municipal road, by solving the zero uniform ergodic characteristic and logical difference variable scale characteristic of objective function, the analysis and design of complex urban road pattern is realised, according to the principle of pixel correlation, genetic algorithm is used to optimise the design of municipal roads, and the image merging planning analysis model of municipal roads is constructed. The feature classification of municipal roads is carried out by genetic method, and the cross edge contour feature segmentation method of municipal roads is adopted. Optimise planning and network design of municipal roads. The simulation results show that this method can improve the rational layout of municipal road planning and improve the spatial planning capacity and traffic capacity of municipal roads.																	1755-8301	1755-831X					2020	12	1			SI		109	119		10.1504/IJBM.2020.105630													
J								Algorithm research of spoken English assessment based on fuzzy measure and speech recognition technology	INTERNATIONAL JOURNAL OF BIOMETRICS										fuzzy measure; speech recognition; spoken English; evaluation algorithm		At present, many speech recognition algorithms are difficult to effectively evaluate the fuzziness of the evaluation algorithm. Based on this, this dissertation uses the speech recognition technology based on fuzzy measure to evaluate the spoken English. In the study the fuzzy measure, based on the traditional algorithm, is used to evaluate the spoken English and different characteristic parameters are extracted to construct the corresponding evaluation model. Simultaneously, the pronunciation is evaluated through automatic learning rules. The English speaking assessment model based on fuzzy measure and speech recognition technology is constructed and validated. The research shows that compared with the traditional algorithms, the spoken language evaluation algorithm based on fuzzy measure and speech recognition technology has the incomparable superiority, and can provide a reference for the follow-up related research.																	1755-8301	1755-831X					2020	12	1			SI		120	129		10.1504/IJBM.2020.105631													
J								Proactive and reactive context reasoning architecture for smart web services	INTERNATIONAL JOURNAL OF DATA MINING MODELLING AND MANAGEMENT										smart web service; the web of things; context reasoning; proactive; reactive; multi-entity Bayesian networks; MEBNs; PR-OWL	SITUATION-AWARENESS; LANGUAGE	The web of things (WoT) uses web technologies to connect embedded objects to each other and to deliver services to stakeholders. The context of these interactions (situation) is a key source of information which can be sometimes uncertain. In this paper, we focus on the development of intelligent web services. The main requirements for intelligent service are to deal with context diversity, semantic context representation and the capacity to reason with uncertain information. From this perspective, we propose a framework for intelligent services to deal with various contexts, to reactively respond to real-time situations and proactively predict future situations. For the semantic representation of context, we use PR-OWL, a probabilistic ontology based on multi-entity Bayesian networks. PR-OWL is flexible enough to represent complex and uncertain contexts. We validate our framework with an intelligent plant watering use case to show its reasoning capabilities.																	1759-1163	1759-1171					2020	12	1					1	27		10.1504/IJDMMM.2020.105609													
J								Grey relational classification algorithm for software fault proneness with SOM clustering	INTERNATIONAL JOURNAL OF DATA MINING MODELLING AND MANAGEMENT										self-organising map; SOM; grey relational analysis; GRA; unsupervised classification; fault-proneness; object-oriented; OO	GENETIC ALGORITHM; PREDICTION; QUALITY; METRICS	The estimation by the human judgment to deal with the inherent uncertainty of software gives a vague and imprecise solution. To cope with this challenge, we propose a new hybrid analogy model based on the integration of grey relational analysis (GRA) classification with self-organising map (SOM) clustering. In this paper, a new classification approach is proposed to distribute the data to similar groups. The attributes are selected based on GRC values. In the proposed, the similarity measure between reference project and cluster head is computed to determine the cluster to which target project belongs. The fault-proneness of reference project is estimated based on the regression equation of the selected cluster. The proposed algorithm gives resilience to users to select features for both continuous and categorical attributes. In this study, two scenarios based on the integration of proposed classification with regression have been proposed. Experimental results show significant results indicating that proposed methodology can be used for the prediction of faults and produce conceivable results when compared with the results of multilayer-perceptron, logistic regression, bagging, naive Bayes and sequential minimal optimisation (SMO).																	1759-1163	1759-1171					2020	12	1					28	64		10.1504/IJDMMM.2020.105599													
J								Intrusion detection using classification techniques: a comparative study	INTERNATIONAL JOURNAL OF DATA MINING MODELLING AND MANAGEMENT										data mining; classification; network security; intrusion detection; KDD99	ALGORITHM	Today's highly connected world suffers from the increase and variety of cyber-attacks. To mitigate those threats, researchers have been continuously exploring different methods for intrusion detection through the last years. In this paper, we study the use of data mining techniques for intrusion detection. The research intends to compare the performances of classification techniques for intrusion detection. To reach the goal, we involve 74 classification techniques in this comparative study. The study shows that no technique outperforms the others in all situations. However, some classification methods lead to promising results and give clues for further combinations.																	1759-1163	1759-1171					2020	12	1					65	86		10.1504/IJDMMM.2020.105596													
J								An insight into application of big data analytics in healthcare	INTERNATIONAL JOURNAL OF DATA MINING MODELLING AND MANAGEMENT										big data; Hadoop; machine learning algorithms; healthcare; map-reduce; chronic diseases; accuracy rate; prevention; analytics	PERFORMANCE; DISEASE	The main aim of this paper is to comprehend, gain insight of the current trends in application of big data in healthcare, and to identify the various potential healthcare horizons. A brief analysis was done on 'big data analytics in healthcare' focusing on collection of data, the tools employed, the aspects of health that were addressed, the type of machine learning algorithms and which statistics commissioned to compare the performance of these algorithms. The focus was mainly on prediction of the diseases, emergency department visits or a disease outbreak, using 'HADOOP' and 'WEKA' tool, by obtaining data from University of California machine learning repository, hospitals and government agencies. Support vector machine, artificial neural networks, naive Bayes and decision tree were commonly used algorithms whose efficacy was compared statistically using 'accuracy'. In my perspective, apart from prediction of disease other domains of health are to be addressed.																	1759-1163	1759-1171					2020	12	1					87	117		10.1504/IJDMMM.2020.105598													
J								Overlapping community detection with a novel hybrid metaheuristic optimisation algorithm	INTERNATIONAL JOURNAL OF DATA MINING MODELLING AND MANAGEMENT										overlapping community; modified density; Tabu search; TS; Bat algorithm; BA; link clustering; social network	BAT ALGORITHM	Social networks are ubiquitous in our daily life. Due to the rapid development of information and electronic technology, social networks are becoming more and more complex in terms of sizes and contents. It is of paramount significance to analyse the structures of social networks in order to unveil the myth beneath complex social networks. Network community detection is recognised as a fundamental tool towards social networks analytics. As a consequence, numerical community detection methods are proposed in the literature. For a real-world social network, an individual may possess multiple memberships, while the existing community detection methods are mainly designed for non-overlapping situations. With regard to this, this paper proposes a hybrid metaheuristic method to detect overlapping communities in social networks. In the proposed method, the overlapping community detection problem is formulated as an optimisation problem and a novel bat optimisation algorithm is designed to solve the established optimisation model. To enhance the searchability of the proposed algorithm, a local search operator based on tabu search is introduced. To validate the effectiveness of the proposed algorithm, experiments on benchmark and real-world social networks are carried out. The experiments indicate that the proposed algorithm is promising for overlapping community detection.																	1759-1163	1759-1171					2020	12	1					118	139		10.1504/IJDMMM.2020.105601													
J								Tomographic Reconstruction of the Beltrami Fields	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Beltrami fields; Inverse problems; Ray transform; Computerized tomography; Vector spherical harmonics	VECTOR TOMOGRAPHY; VELOCITY	We introduce and study a tomography method and have developed a numerical algorithm for the reconstruction of the linear Beltrami fields del xB=kappa B in a bounded domain of R3 by using a known ray transform. This method is based on the expansion of a vector field over the special basis of vector functions. The results of computer simulation are given.																	0924-9907	1573-7683				JAN	2020	62	1					1	9		10.1007/s10851-019-00900-4													
J								Accelerated Variational PDEs for Efficient Solution of Regularized Inversion Problems	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Nesterov acceleration; Accelerated gradient descent; PDE acceleration; Nonlinear wave equations; Image denoising; Image deblurring; Image restoration; Total variation; Beltrami regularization	HEAVY BALL; OPTIMIZATION	We further develop a new framework, called PDE acceleration, by applying it to calculus of variation problems defined for general functions on Rn, obtaining efficient numerical algorithms to solve the resulting class of optimization problems based on simple discretizations of their corresponding accelerated PDEs. While the resulting family of PDEs and numerical schemes are quite general, we give special attention to their application for regularized inversion problems, with particular illustrative examples on some popular image processing applications. The method is a generalization of momentum, or accelerated, gradient descent to the PDE setting. For elliptic problems, the descent equations are a nonlinear damped wave equation, instead of a diffusion equation, and the acceleration is realized as an improvement in the CFL condition from Delta t similar to Delta x2 (for diffusion) to Delta t similar to Delta x (for wave equations). We work out several explicit as well as a semi-implicit numerical scheme, together with their necessary stability constraints, and include recursive update formulations which allow minimal-effort adaptation of existing gradient descent PDE codes into the accelerated PDE framework. We explore these schemes more carefully for a broad class of regularized inversion applications, with special attention to quadratic, Beltrami, and total variation regularization, where the accelerated PDE takes the form of a nonlinear wave equation. Experimental examples demonstrate the application of these schemes for image denoising, deblurring, and inpainting, including comparisons against primal-dual, split Bregman, and ADMM algorithms.																	0924-9907	1573-7683				JAN	2020	62	1					10	36		10.1007/s10851-019-00910-2													
J								Fractal Dimension Estimation for Color Texture Images	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Fractal dimension; Color image; Box-counting method; Texture; Fractal Brownian surface model	BOX-COUNTING METHOD; CLASSIFICATION; ALGORITHMS	As an important feature of image texture, fractal dimension is widely used to index, segment or classify texture images. Previously, many methods have been proposed to estimate fractal dimension. However, most of them are for binary and gray-scale images. Only a few of them are for color images. In this paper, by extending the differential box-counting approach to five-dimensional Euclidean hyper-space, we present a new robust and simple algorithm to estimate fractal dimension for color texture images. By the differential box-counting method, a real empty box not covering any pixel even with ideal resolution is also counted as a box having pixels, resulting in errors of computations. This problem is eliminated by our method based on fractal Brownian surface model. Our experiments demonstrate that our algorithm is able to capture the complexity of color natures, and outperforms other methods in terms of color texture complexity ranking and discrimination, robustness and computational complexity.																	0924-9907	1573-7683				JAN	2020	62	1					37	53		10.1007/s10851-019-00912-0													
J								DVL-SLAM: sparse depth enhanced direct visual-LiDAR SLAM	AUTONOMOUS ROBOTS										Direct visual SLAM; Camera-LiDAR; Sparse depth	ROBUST; VERSATILE	This paper presents a framework for direct visual-LiDAR SLAM that combines the sparse depth measurement of light detection and ranging (LiDAR) with a monocular camera. The exploitation of the depth measurement between two sensor modalities has been reported in the literature but mostly by a keyframe-based approach or by using a dense depth map. When the sparsity becomes severe, the existing methods reveal limitation. The key finding of this paper is that the direct method is more robust under sparse depth with narrow field of view. The direct exploitation of sparse depth is achieved by implementing a joint optimization of each measurement under multiple keyframes. To ensure real-time performance, the keyframes of the sliding window are kept constant through rigorous marginalization. Through cross-validation, loop-closure achieves the robustness even in large-scale mapping. We intensively evaluated the proposed method using our own portable camera-LiDAR sensor system as well as the KITTI dataset. For the evaluation, the performance according to the LiDAR of sparsity was simulated by sampling the laser beam from 64 to 16 and 8. The experiment proves that the presented approach is significantly outperformed in terms of accuracy and robustness under sparse depth measurements.																	0929-5593	1573-7527				JAN	2020	44	2					115	130		10.1007/s10514-019-09881-0													
J								Attention-based active visual search for mobile robots	AUTONOMOUS ROBOTS										Active visual search; Visual attention; Probabilistic lost target search; Top-down modulation; Search and rescue	TOP-DOWN; OBJECT; COMPLEXITY; MODEL; TARGET; ENVIRONMENTS; FRAMEWORK; VISION; PATH	We present an active visual search model for finding objects in unknown environments. The proposed algorithm guides the robot towards the sought object using the relevant stimuli provided by the visual sensors. Existing search strategies are either purely reactive or use simplified sensor models that do not exploit all the visual information available. In this paper, we propose a new model that actively extracts visual information via visual attention techniques and, in conjunction with a non-myopic decision-making algorithm, leads the robot to search more relevant areas of the environment. The attention module couples both top-down and bottom-up attention models enabling the robot to search regions with higher importance first. The proposed algorithm is evaluated on a mobile robot platform in a 3D simulated environment. The results indicate that the use of visual attention significantly improves search, but the degree of improvement depends on the nature of the task and the complexity of the environment. In our experiments, we found that performance enhancements of up to 42% in structured and 38% in highly unstructured cluttered environments can be achieved using visual attention mechanisms.																	0929-5593	1573-7527				JAN	2020	44	2					131	146		10.1007/s10514-019-09882-z													
J								Online learning for 3D LiDAR-based human detection: experimental analysis of point cloud clustering and classification methods	AUTONOMOUS ROBOTS										Online learning; Human detection; Point cloud segmentation; 3D LiDAR-based tracking; Dataset	TRACKING; PEOPLE	This paper presents a system for online learning of human classifiers by mobile service robots using 3D LiDAR sensors, and its experimental evaluation in a large indoor public space. The learning framework requires a minimal set of labelled samples (e.g. one or several samples) to initialise a classifier. The classifier is then retrained iteratively during operation of the robot. New training samples are generated automatically using multi-target tracking and a pair of "experts" to estimate false negatives and false positives. Both classification and tracking utilise an efficient real-time clustering algorithm for segmentation of 3D point cloud data. We also introduce a new feature to improve human classification in sparse, long-range point clouds. We provide an extensive evaluation of our the framework using a 3D LiDAR dataset of people moving in a large indoor public space, which is made available to the research community. The experiments demonstrate the influence of the system components and improved classification of humans compared to the state-of-the-art.																	0929-5593	1573-7527				JAN	2020	44	2					147	164		10.1007/s10514-019-09883-y													
J								Simultaneous planning of sampling and optimization: study on lazy evaluation and configuration free space approximation for optimal motion planning algorithm	AUTONOMOUS ROBOTS										Motion planning; Lazy collision checking; Trajectory optimization; Configuration-free space approximation		A recent trend in optimal motion planning has broadened the research area toward the hybridization of sampling, optimization, and grid-based approaches. A synergy from such integrations can be expected to bring the overall performance improvement, but seamless integration and generalization is still an open problem. In this paper, we suggest a hybrid motion planning algorithm utilizing both sampling and optimization techniques, while simultaneously approximating a configuration-free space. Unlike conventional optimization-based approaches, the proposed algorithm does not depend on a priori information or resolution-complete factors, e.g., a distance field. Ours instead learns spatial information on the fly by exploiting empirical collisions found during the execution, and decentralizes the information over the constructed graph for an efficient reference. With the help of the learned information, we associate the constructed search graph with the approximate configuration-free space so that our optimization-based local planner exploits the local area to identify the connectivity of free space without depending on the precomputed workspace information. To show the novelty of the proposed algorithm, we apply the proposed idea to asymptotic optimal planners with lazy collision checking; lazy PRM*\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$<^>*$$\end{document} and Batch Informed Tree*\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$<^>*$$\end{document}, and evaluate them against other state-of-the-arts in both synthetic and practical benchmarks with varying degrees of freedom. We also discuss the performance analysis, properties of different algorithm frameworks of lazy collision checking and our approximation method.																	0929-5593	1573-7527				JAN	2020	44	2					165	181		10.1007/s10514-019-09884-x													
J								Asynchronous microphone arrays calibration and sound source tracking	AUTONOMOUS ROBOTS										Asynchronous microphone array; Calibration; Sound source localisation	LOCALIZATION; TDOA	In this paper, we proposed an optimisation method to solve the problem of sound source localisation and calibration of an asynchronous microphone array. This method is based on the graph-based formulation of the simultaneous localisation and mapping problem. In this formulation, a moving sound source is considered to be observed from a static microphone array. Traditional approaches for sound source localisation rely on the well-known geometrical information of the array and synchronous readings of the audio signals. Recent work relaxed these two requirements by estimating the temporal offset between pair of microphones based on the assumption that the clock timing of each microphone is exactly the same. This assumption requires the sound cards to be identically manufactured, which in practice is not possible to achieve. Hereby an approach is proposed to jointly estimate the array geometrical information, time offset and clock difference/drift rate of each microphone together with the location of a moving sound source. In addition, an observability analysis of the system is performed to investigate the most suitable configuration for sound source localisation. Simulation and experimental results are presented, which prove the effectiveness of the proposed methodology.																	0929-5593	1573-7527				JAN	2020	44	2					183	204		10.1007/s10514-019-09885-w													
J								Model-referenced pose estimation using monocular vision for autonomous intervention tasks	AUTONOMOUS ROBOTS										Underwater navigation; Underwater robot; Model-referenced pose estimation; Underwater intervention task	VISUAL TRACKING; MISSIONS; EDGE	This study addresses vision-based underwater navigation techniques to automate underwater intervention tasks with robotic vehicles. A systematic procedure of model-referenced pose estimation is introduced to obtain the relative pose information between the underwater vehicle and the underwater structures whose geometry and shape are known. The vision-based pose estimation combined with inertial navigation enables underwater robots to navigate precisely around underwater structures for challenging underwater intervention tasks such as subsea construction, maintenance, and inspection. To demonstrate the feasibility of the proposed approach, a set of experiments were carried out in a test tank using an autonomous underwater vehicle.																	0929-5593	1573-7527				JAN	2020	44	2					205	216		10.1007/s10514-019-09886-9													
J								A velocity obstacles approach for autonomous landing and teleoperated robots	AUTONOMOUS ROBOTS										Optimal trajectory; Trajectory planning; Velocity obstacles; Autonomous mobile robots; Teleoperated robots	BILATERAL TELEOPERATION; AVOIDANCE; MOTION	Velocity obstacles (VO) are one of the most successful methods to compute collision-free trajectory for multi-agent systems. VO provide for each autonomous robot the set of velocities that avoids collisions with other robots (sharing or not the same motion policy) and with moving or static obstacles in the environment. In this paper we will focus on a particular efficient implementation of the VO paradigm available in the literature, called optimal reciprocal collision avoidance. After highlighting and solving a couple of deadlock situations that the current implementation cannot manage, we extend this approach to two challenging applications: (1) the landing of a UAV onto a UGV in crowded environments, and (2) the generation of force feedback for teleoperated vehicles. The theoretical outcomes are validated in simulated scenarios using V-REP as a virtual robot development tool.																	0929-5593	1573-7527				JAN	2020	44	2					217	232		10.1007/s10514-019-09887-8													
J								Interactive visual data exploration with subjective feedback: an information-theoretic approach	DATA MINING AND KNOWLEDGE DISCOVERY										Exploratory data analysis; Dimensionality reduction; Information theory; Subjective interestingness; Maximum entropy distribution	NONLINEAR DIMENSIONALITY REDUCTION; FIT	Visual exploration of high-dimensional real-valued datasets is a fundamental task in exploratory data analysis (EDA). Existing projection methods for data visualization use predefined criteria to choose the representation of data. There is a lack of methods that (i) use information on what the user has learned from the data and (ii) show patterns that she does not know yet. We construct a theoretical model where identified patterns can be input as knowledge to the system. The knowledge syntax here is intuitive, such as "this set of points forms a cluster", and requires no knowledge of maths. This background knowledge is used to find a maximum entropy distribution of the data, after which the user is provided with data projections for which the data and the maximum entropy distribution differ the most, hence showing the user aspects of data that are maximally informative given the background knowledge. We study the computational performance of our model and present use cases on synthetic and real data. We find that the model allows the user to learn information efficiently from various data sources and works sufficiently fast in practice. In addition, we provide an open source EDA demonstrator system implementing our model with tailored interactive visualizations. We conclude that the information theoretic approach to EDA where patterns observed by a user are formalized as constraints provides a principled, intuitive, and efficient basis for constructing an EDA system.																	1384-5810	1573-756X				JAN	2020	34	1					21	49		10.1007/s10618-019-00655-x													
J								Feedback2Code: A Deep Learning Approach to Identifying User-Feedback-Related Source Code Files	INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING										User feedback; deep learning; feedback-code linkage		Users frequently raise feedback when using software products. Feedback from users regarding their experiences and expectations and software defects they found adds values to software maintenance and evolution - software managers collect user feedback and then dispatch feedback issues that developers (and/or maintainers) need to track and process. Feedback tracking is often supported by open source platforms and collaborative software systems. Meanwhile, there still exists a gap between feedback issues and source code: since user feedback is usually informal and arbitrary, engineers have to spend much effort on comprehending issues and identifying which source code files need to be improved or fixed. This paper introduces a deep learning approach, Feedback2Code, which facilitates identification of user-feedback-related source code files. The core idea is to (1) explore latent semantics of user feedback and source code using several deep learning techniques such as Multi-Layer Perceptron (MLP), Convolutional Neutral Network (CNN) and skip-gram and (2) establish a multicorrelation model to explore linkages between feedback issues and source code files. Given a feedback issue, the linkages then allow engineers to identify source code files that are highly relevant to the issue. We have implemented Feedback2Code and evaluated it against ChangeAdvisor (a state-ofthe-art approach) on 24 open source projects. The evaluation results clearly show the strength of Feedback2Code: for 103793 feedback issues, Feedback2Code successfully established 101190 feedback-code linkages and achieved a precision that is 3.11 x higher than that of ChangeAdvisor. Feedback2Code also achieved an MRR and an MAP that are 2.83x and 2.79x higher than those of ChangeAdvisor, respectively. Furthermore, we also found that a Feedback2Code-trained model can be easily transferred, allowing feedback-code linkages to be established in new projects with a little history data.																	0218-1940	1793-6403				JAN	2020	30	1					1	22		10.1142/S0218194020500011													
J								Applicability of Machine Learning Methods on Mobile App Effort Estimation: Validation and Performance Evaluation	INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING										Effort estimation; fuzzy inference application; machine learning; MAR; mobile apps; SAMOA dataset	NEURAL-NETWORKS; SOFTWARE; ISSUES; MODELS	Software cost estimation is one of the most crucial tasks in a software development life cycle. Some well-proven methods and techniques have been developed for effort estimation in case of classical software. Mobile applications (apps) are different from conventional software by their nature, size and operational environment; therefore, the established estimation models for traditional desktop or web applications may not be suitable for mobile app development. The objective of this paper is to propose a framework for mobile app project estimation. The research methodology adopted in this work is based on selecting different features of mobile apps from the SAMOA dataset. These features are later used as input vectors to the selected machine learning (ML) techniques. The results of this research experiment are measured in mean absolute residual (MAR). The experimental outcomes are then followed by the proposition of a framework to recommend an ML algorithm as the best match for superior effort estimation of a project in question. This framework uses the Mamdani-type fuzzy inference method to address the ambiguities in the decision-making process. The outcome of this work will particularly help mobile app estimators, development professionals, and industry at large to determine the required efforts in the projects accurately.																	0218-1940	1793-6403				JAN	2020	30	1					23	41		10.1142/S0218194020500023													
J								Story Point-Based Effort Estimation Model with Machine Learning Techniques	INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING										Analogy-based effort estimation; Scrum; machine learning; story point; Gradient Boosting algorithm; Support Vector Regression; Random Forest Regression; Multi-Layer Perceptron	SOFTWARE; SCRUM	Until now, numerous effort estimation models for software projects have been developed, most of them producing accurate results but not providing the flexibility to decision makers during the software development process. The main objective of this study is to objectively and accurately estimate the effort when using the Scrum methodology. A dynamic effort estimation model is developed by using regression-based machine learning algorithms. Story point as a unit of measure is used for estimating the effort involved in an issue. Projects are divided into phases and the phases are respectively divided into iterations and issues. Effort estimation is performed for each issue, then the total effort is calculated with aggregate functions respectively for iteration, phase and project. This architecture of our model provides flexibility to decision makers in any case of deviation from the project plan. An empirical evaluation demonstrates that the error rate of our story point-based estimation model is better than others.																	0218-1940	1793-6403				JAN	2020	30	1					43	66		10.1142/S0218194020500035													
J								Software Analysis Method for Assessing Software Sustainability	INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING										Sustainability assessment; machine learning; software-based approach	PREDICTION	Software sustainability evaluation has become an essential component of software engineering (SE) owing to sustainability considerations that must be incorporated into software development. Several studies have been performed to address the issues associated with sustainability concerns in the SE process. However, current practices extensively rely on participant experiences to evaluate sustainability achievement. Moreover, there exist limited quantifiable methods for supporting software sustainability evaluation. Our primary objective is to present a methodology that can assist software engineers in evaluating a software system based on well-defined sustainability metrics and measurements. We propose a novel approach that combines machine learning (ML) and software analysis methods. To simplify the application of the proposed approach, we present a semi-automated tool that supports engineers in assessing the sustainability achievement of a software system. The results of our study demonstrate that the proposed approach determines sustainability criteria and defines sustainability achievement in terms of a traceable matrix. Our theoretical evaluation and empirical study demonstrate that the proposed support tool can help engineers identify sustainability limitations in a particular feature of a software system. Our semi-automated tool can identify features that must be revised to enhance sustainability achievement.																	0218-1940	1793-6403				JAN	2020	30	1					67	95		10.1142/S0218194020500047													
J								Thread Scheduling Sequence Generation Based on All Synchronization Pair Coverage Criteria	INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING										Multi-thread program testing; synchronization pair; all synchronization pairs coverage criteria; thread scheduling sequence		Testing multi-thread programs becomes extremely difficult because thread interleavings are uncertain, which may cause a program getting different results in each execution. Thus, Thread Scheduling Sequence (TSS) is a crucial factor in multi-thread program testing. A good TSS can obtain better testing efficiency and save the testing cost especially with the increase of thread numbers. Focusing on the above problem, in this paper, we discuss a kind of approach that can efficiently generate TSS based on the concurrent coverage criteria. First, we give a definition of Synchronization Pair (SP) as well as all Synchronization Pairs Coverage (ASPC) criterion. Then, we introduce the Synchronization Pair Thread Graph (SPTG) to describe the relationships between SPs and threads. Moreover, this paper presents a TSS generation method based on the ASPC according to SPTG. Finally, TSSs automatic generation experiments are conducted on six multi-thread programs in Java Library with the help of Java Path Finder (JPF) tool. The experimental results illustrate that our method not only generates TSSs to cover all SPs but also requires less state number, transition number as well as TSS number when satisfying ASPC, compared with other three widely used TSS generation methods. As a result, it is clear that the efficiency of TSS generation is obviously improved.																	0218-1940	1793-6403				JAN	2020	30	1					97	118		10.1142/S0218194020500059													
J								A Case Representation and Similarity Measurement Model with Experience-Grounded Semantics	INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING										Case representation; knowledge representation; Case-Based Reasoning (CBR); Experience-Grounded Semantics (EGS); Non-Axiomatic Reasoning System (NARS); Artificial General Intelligence (AGI)	REASONING APPROACH	Case-based reasoning heavily depends on the structure and content of the cases, and semantics is essential to effectively represent cases. In the field of structured case representation, most of the works regarding case representation and measurement of semantic similarity between cases are based on model-theoretic semantics and their extensions. The purpose of this study is to explore the potential of experienced-grounded semantics in case representation and semantic similarity measurement. The main contents in this study are as follows: (i) a case representation model based on experience-grounded semantic is proposed, (ii) a novel semantic similarity measurement method with multi-strategy reasoning is introduced, and (iii) a case-based reasoning software for urban firefighting field based on the proposed model is designed and implemented. Theoretically, compared with traditional structured case representation methods, the proposed model not only represents case in a fully formalized way, but also provides a novel metric for computing the strength of the semantic relationship between cases. The proposed model has been applied in an intelligent decision-support software for urban firefighting.																	0218-1940	1793-6403				JAN	2020	30	1					119	146		10.1142/S0218194020500060													
J								Pseudo-equality Algebras and Residuated Posets	JOURNAL OF MULTIPLE-VALUED LOGIC AND SOFT COMPUTING										Pseudo-equality algebra; residuated poset; quantum B-algebra; BCK-algebra; implicative semigroup	BCK ALGEBRAS; FILTERS	In this paper, we study the relationship between pseudo-equality algebras and quantum B-algebras and the relationship between pseudo-equality algebras and residuated posets. Also, we present some properties of residuated posets and get a one-to-one correspondence between the congruences and normal filters in a residuated poset. We introduce the notion of implicative filters of a residuated poset and characterize these filters. Moreover, we investigate implicative homomorphisms of residuated posets and obtain the homomorphism theorem.																	1542-3980	1542-3999					2020	34	1-2					1	23															
J								Chaotic and Co-variance Based Artificial Bee Colony Algorithm	JOURNAL OF MULTIPLE-VALUED LOGIC AND SOFT COMPUTING										Swarm intelligence; artificial bee colony (ABC); chaotic map; co-variance; opposition-based learning	EVOLUTIONARY ALGORITHMS; GENETIC ALGORITHM; OPTIMIZATION	Artificial Bee Colony algorithm (ABC) is a nature inspired heuristics optimization methodology which is competitive to other population-based stochastic algorithms. Recent studies have shown that ABC is good in exploration but poor in exploitation. In this paper, we have applied co-variance matrix adoption and chaotic map in order to improve the convergence toward the solution of ABC algorithm. Proposed chaotic and covariance-based ABC (C2ABC) gives a better result than ABC and it's variants most of the time when applied to Black-Box Optimization Benchmarking (BBOB). Our literature proves that proposed C2ABC is better than most of the variants of ABC for continuous global optimization problem.																	1542-3980	1542-3999					2020	34	1-2					25	42															
J								New Robust Portfolio Selection Models Based on the Principal Components Analysis: An Application on the Turkish Holding Stocks	JOURNAL OF MULTIPLE-VALUED LOGIC AND SOFT COMPUTING										Portfolio selection; imprecise probability; robust optimization; worst-case analysis; possibility theory; fuzzy logic; triangular fuzzy numbers; principal components analysis		Robust optimization is a significant tool to deal with the uncertainty of parameters. However, the robust versions of the mean - variance (MV) model have serious shortcomings. Thus, we propose new robust versions of the MV model and its possibilistic counterpart, based on the Principal Component Analysis. We also derive their analytical solutions when the risk-free asset and short positioning are allowed. In addition, we suggest an eigenvalue approach to manage their conservativeness. After laying down the theoretical points, we illustrate them by using a real data set of six holding stocks trading on the Borsa Istanbul (BIST). We also compare the profitability and performance results of the existing models and the proposed robust models.																	1542-3980	1542-3999					2020	34	1-2					43	58															
J								Performance Evaluation of An Apparel Retailer's Stores by Using Stochastic Imprecise DEA	JOURNAL OF MULTIPLE-VALUED LOGIC AND SOFT COMPUTING										DEA; IDEA; stochastic imprecise DEA; performance evaluation		In today's competitive environment evaluating firms' performance is an important issue not only for investors and creditors but also for the stores of the firms to increase their efficiencies and take place in the sector. Determining the competitiveness of the firms and evaluating the financial performance of them is also crucial for the sector's development and this depends on their stores' performances. The aim of this study is developing a data envelopment analysis model to evaluate the performance of the stores of a well-known apparel retailer in Turkey. In conventional data envelopment analysis models, a performance measure whether as an input or output usually has to be known. However, in some cases, the type of a performance measure is not clear and we may not decide on whether the criteria is input or output which is called flexible measure. In addition to this, we believe that future planning is more important than past evaluation, so that we present a stochastic imprecise DEA model (SIDEA) that incorporates future planning with flexible measures. To document the practicality of this proposed model SIDEA, it is applied on a well-known apparel retailer in Turkey and get insights on the performance of the stores of this retailer.																	1542-3980	1542-3999					2020	34	1-2					59	75															
J								Description of the Triangle-free Prime Graphs Having at Most Two Non Critical Vertices	JOURNAL OF MULTIPLE-VALUED LOGIC AND SOFT COMPUTING										Graphs; prime; module; critical vertex; partially critical	TOURNAMENTS	In a graph G = (V, E), a module is a vertex subset M such that every vertex outside M is adjacent to all or none of Al. For example, empty set, {x} (x is an element of V) and V are modules of G, called trivial modules. A graph, all the modules of which are trivial, is prime; otherwise, it is decomposable. A vertex x of a prime graph G is critical if G - x is decomposable. We generalize this definition. A prime graph G = (V, E) is X-critical, where X is a subset of V, if for each x is an element of X, x is a critical vertex. Moreover, G is (-k)-critical, where 0 <= k <= vertical bar V vertical bar, if G is (V\X)-critical for some subset X of V such that vertical bar X vertical bar = k. In 1993, J.H. Schmerl and W.T. Trotter characterized the V-critical graphs, called critical graphs with vertex set V. Recently, H. Belkhechine, I. Boudabbous and M.B. Elayech characterized the (-1)-critical graphs. In this paper, we characterize the (-k)-critical graphs within the family of triangle-free graphs where k <= 2.																	1542-3980	1542-3999					2020	34	1-2					77	103															
J								Online Testing of Ternary Reversible Multiple-Controlled Unary Gate Circuits	JOURNAL OF MULTIPLE-VALUED LOGIC AND SOFT COMPUTING										Line fault; missing gate fault; missing control fault; online testing; ternary reversible circuit; ternary extended multiple-controlled unary gates; ternary modified Feynman gate	KRONECKER DECISION DIAGRAMS; FAULT MODELS; LOGIC	Recently a synthesis method for ternary reversible circuit using Max-Min algebra has been introduced, which synthesizes the circuit as a cascade of multiple-controlled unary gates. It has been shown that this method outperforms the previous TGFSOP-based method in terms of both quantum cost and number of ancilla inputs. We propose two techniques for online testing of ternary reversible circuits synthesized using multiple-controlled unary gates, one for testing a single line fault and another for testing a single fault of any fault model. For this purpose, we propose two new ternary reversible gates, namely a modified Feynman gate and extended multiple-controlled unary gates. We show realizations of these gates using elementary unary and M-S gates. In both the online testable circuits, one pre-fix and another post-fix circuit are added to the original circuit. For online testing of a single line fault, the multiple-controlled unary gates of the original circuit are replaced by their corresponding extended multiple-controlled unary gates. For online testing of a single fault of any fault model, for each multiple-controlled unary gate of the original circuit, we add an additional multiple-controlled unary gate with different target operation. We experimented with 24 benchmark logic functions of up to five variables. For online testing of a single line fault, on average the proposed technique requires 29.77% quantum cost overhead. For online testing of a single fault of any fault model, on average the proposed technique requires 123.94% quantum cost overhead.																	1542-3980	1542-3999					2020	34	1-2					105	127															
J								Irreducible and Prime Deductive Systems of Some Generalizations of BCK Algebras	JOURNAL OF MULTIPLE-VALUED LOGIC AND SOFT COMPUTING										RM; BH; BCI; BCK algebra; (irreducible, prime, maximal) deductive system; homomorphism; subdirect product	HILBERT-ALGEBRAS	Some generalizations of BCK algebras (such as, from example, RM, BH, pre-BBBCC and pre-BCK algebras) are considered. It is proved that the set of all deductive systems of a pre-BBBCC algebra is an algebraic distributive lattice. Irreducible deductive systems and prime deductive systems of RM algebras are introduced and studied. In particular, the homomorphic properties of these deductive systems are provided. Finally, subdirect products of BH algebras by using prime deductive systems are investigated.																	1542-3980	1542-3999					2020	34	1-2					129	148															
J								A FCM-Based Systematic Approach for Building and Analyzing Problem Solving Networks in Open Innovation	JOURNAL OF MULTIPLE-VALUED LOGIC AND SOFT COMPUTING										Fuzzy cognitive mapping; network analysis; open innovation; problem solving	KNOWLEDGE; PERSPECTIVE	The purpose of this paper is to propose a fuzzy cognitive mapping (FCM)-based systematic approach for building and analyzing problem solving networks in open innovation and illustrate the use of this approach with an application in a company operating in prepress preparation sector. First, the proposed approach is explained step by step. Then, for illustrating the use of this approach, case study method was used and data was collected in a company using a visual form developed for this study and analysis of data was performed using social network analysis (SNA) and FCM methods. The results of the study show that by using the proposed approach managers can better understand, sustain, and develop knowledge management practices in open innovation networks in their companies by considering dynamic contextual factors on their relationships. Visual representation of the resulting network helps managers to better understand their internal and external relationships and identify the most valuable network participants. To the best of our knowledge, this is the first study using SNA and FCM methods together in order to capture the effect of dynamic contextual factors on the relationships in innovation field.																	1542-3980	1542-3999					2020	34	1-2					149	176															
J								Prioritization of Competitive Suppliers Using an Interval-Valued Pythagorean Fuzzy QFD & COPRAS Methodology	JOURNAL OF MULTIPLE-VALUED LOGIC AND SOFT COMPUTING										Supplier selection; COPRAS; QFD; interval-valued Pythagorean fuzzy sets; WASPAS	QUALITY FUNCTION DEPLOYMENT; MCDM APPROACH; HESITANT FUZZY; INTEGRATING QFD; PRODUCT DESIGN; SELECTION; TOPSIS; MODEL; FRAMEWORK; OPERATOR	Prioritization of competitive suppliers can be considerably challenging in the absence of predetermined and systematical assessment procedures. This study develops an integrated methodology of quality function deployment (QFD) and complex proportional assessment (COPRAS) methods under fuzzy environment to prioritize competitive suppliers. In the proposed model, QFD is utilized to determine the evaluation criteria weights by considering the customers' views and COPRAS is used to generate the ranking orders of the competitive firms. The proposed approach allows to capture the uncertainty and vagueness of decision makers' judgements with the aid of interval-valued Pythagorean fuzzy (IVPF) sets. An aircraft manufacturer is used as an example for illustrating the proposed model and the obtained results are compared with IVPF WASPAS method.																	1542-3980	1542-3999					2020	34	1-2					177	199															
J								Image Captioning With End-to-End Attribute Detection and Subsequent Attributes Prediction	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image captioning; semantic attention; end-to-end training; multimodal attribute detector; subsequent attribute predictor		Semantic attention has been shown to be effective in improving the performance of image captioning. The core of semantic attention based methods is to drive the model to attend to semantically important words, or attributes. In previous works, the attribute detector and the captioning network are usually independent, leading to the insufficient usage of the semantic information. Also, all the detected attributes, no matter whether they are appropriate for the linguistic context at the current step, are attended to through the whole caption generation process. This may sometimes disrupt the captioning model to attend to incorrect visual concepts. To solve these problems, we introduce two end-to- end trainable modules to closely couple attribute detection with image captioning as well as prompt the effective uses of attributes by predicting appropriate attributes at each time step. The multimodal attribute detector (MAD) module improves the attribute detection accuracy by using not only the image features but also the word embedding of attributes already existing in most captioning models. MAD models the similarity between the semantics of attributes and the image object features to facilitate accurate detection. The subsequent attribute predictor (SAP) module dynamically predicts a concise attribute subset at each time step to mitigate the diversity of image attributes. Compared to previous attribute based methods, our approach enhances the explainability in how the attributes affect the generated words and achieves a state-of-the-art single model performance of 128.8 CIDEr-D on the MSCOCO dataset. Extensive experiments on the MSCOCO dataset show that our proposal actually improves the performances in both image captioning and attribute detection simultaneously. The codes are available at: https://github.com/RubickH/Image-Captioning-with-MAD-and-SAP.																	1057-7149	1941-0042					2020	29						4013	4026		10.1109/TIP.2020.2969330													
J								Balanced training/test set sampling for proper evaluation of classification models	INTELLIGENT DATA ANALYSIS										Classification; training and test sets; accuracy; random sampling; balanced sampling	ARTIFICIAL NEURAL-NETWORKS; QSAR; SELECTION; DIVERSITY; DESIGN	In machine learning, classification involves identifying the categories or classes to which a new observation belongs based on a training set. The performance of a classification model is generally measured by the classification accuracy of a test set. The first step in developing a classification model is to divide an acquired dataset into training and test sets through random sampling. In general, random sampling does not guarantee that test accuracy reflects the performance of a developed classification model. If random sampling produces biased training/test sets, the classification model may result in bias. In this study, we show the problems of random sampling and propose balanced sampling as an alternative. We also propose a measure for evaluating sampling methods. We perform empirical experiments using benchmark datasets to verify that our sampling algorithm produces proper training and test sets. The results confirm that our method produces better training and test sets than random and several non-random sampling methods can.																	1088-467X	1571-4128					2020	24	1					5	18		10.3233/IDA-194477													
J								DNN models based on dimensionality reduction for stock trading	INTELLIGENT DATA ANALYSIS										Deep neural networks; dimensionality reduction; statistical test; trading performance	SUPPORT VECTOR MACHINE; FEATURE-SELECTION; COMPONENT ANALYSIS; NEURAL-NETWORKS; PRICE INDEX; DIRECTION; PERFORMANCE; VOLATILITY; ALGORITHM; RETURNS	In order to avoid missing representative features, we should select a lot of features as far as possible when using machine learning algorithms in stock trading. Meanwhile, these high dimensional features can lead to redundancy of information and reduce the efficiency, and accuracy of learning algorithms. It is worth noting that dimensionality reduction operation (DRO) is one of the main means to deal with stock high-dimensional data. However, there are few studies on whether DRO can significantly improve the trading performance of deep neural network (DNN) algorithms. Therefore, this paper selects large-scale stock datasets in the American market and in the Chinese market as the research objects. For each stock, we firstly apply four most widely used DRO, namely principal component analysis (PCA), least absolute shrinkage and selection operator (LASSO), classification and regression trees (CART), and autoencoder (AE) to deal with original features respectively, and then use the new features as inputs of the most six popular DNN algorithms such as Multilayer Perceptron (MLP), Deep Belief Network (DBN), Stacked Auto-Encoders (SAE), Recurrent Neural Network (RNN), Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU) to generate trading signals. Finally, we apply the trading signals to conduct a lot of daily trading back-testing and non-parameter statistical testing. The experiments show that LASSO can significantly improve the performance of RNN, LSTM, and GRU. In addition, any DRO mentioned in this paper do not significantly improve trading performance and the speed of generating trading signals of the other DNN algorithms.																	1088-467X	1571-4128					2020	24	1					19	45		10.3233/IDA-184403													
J								Detailed investigation of deep features with sparse representation and dimensionality reduction in CBIR: A comparative study	INTELLIGENT DATA ANALYSIS										Low-level features; deep features; similarity measures; sparse representation; content-based image retrieval	IMAGE RETRIEVAL; CLASSIFICATION; RECOGNITION; SPACE; HOG	Research on content-based image retrieval (CBIR) has been under development for decades, and numerous methods have been competing to extract the most discriminative features for improved representation of the image content. Recently, deep learning methods have gained attention in computer vision, including CBIR. In this paper, we present a comparative investigation of different features, including low-level and high-level features, for CBIR. We compare the performance of CBIR systems using different deep features with state-of-the-art low-level features such as SIFT, SURF, HOG, LBP, and LTP, using different dictionaries and coefficient learning techniques. Furthermore, we conduct comparisons with a set of primitive and popular features that have been used in this field, including colour histograms and Gabor features. We also investigate the discriminative power of deep features using certain similarity measures under different validation approaches. Furthermore, we investigate the effects of the dimensionality reduction of deep features on the performance of CBIR systems using principal component analysis, discrete wavelet transform, and discrete cosine transform. Unprecedentedly, the experimental results demonstrate high (95% and 93%) mean average precisions when using the VGG-16 FC7 deep features of Corel-1000 and Coil-20 datasets with 10-D and 20-D K-SVD, respectively.																	1088-467X	1571-4128					2020	24	1					47	68		10.3233/IDA-184411													
J								Label distribution learning with climate probability for ensemble forecasting	INTELLIGENT DATA ANALYSIS										Ensemble forecasting; label distribution learning; post-processing; domain knowledge	SEASONAL CLIMATE; WEATHER	In meteorology, ensemble forecasting aims to post-process an ensemble of multiple members' forecasts and make better weather predictions. While multiple individual forecasts are generated to represent the uncertain weather system, the performance of ensemble forecasting is unsatisfactory. In this paper we conduct data analysis based on the expertise of human forecasters and introduce a machine learning method for ensemble forecasting. The proposed method, Label Distribution Learning with Climate Probability (LDLCP), can improve the accuracy of both deterministic forecasting and probabilistic forecasting. The LDLCP method utilizes the relevant variables of previous forecasts to construct the feature matrix and applies label distribution learning (LDL) to adjust the probability distribution of ensemble forecast. Our proposal is novel in its specialized target function and appropriate conditional probability function for the ensemble forecasting task, which can optimize the forecasts to be consistent with local climate. Experimental testing is performed on both artificial data and the data set for ensemble forecasting of precipitation in East China from August to November, 2017. Experimental results show that, compared with a baseline method and two state-of-the-art machine learning methods, LDLCP shows significantly better performance on measures of RMSE and average continuous ranked probability score.																	1088-467X	1571-4128					2020	24	1					69	82		10.3233/IDA-184446													
J								An end-to-end distance measuring for mixed data based on deep relevance learning	INTELLIGENT DATA ANALYSIS										Distance measuring; mixed data; deep relevance learning	ALGORITHM	Distance Measuring between two mixed data objects is the basis of many learning algorithms. The complex relevance between heterogeneous - various types/scales - attributes has a significant influence on the measured results. In this paper, we propose an End-to-End Distance Measuring method for mixed data based on deep relevance learning, called (EDM)-D-2. Existing methods confuse the attributes space by mapping the discrete attribute values to new continuous values, or discretize continuous attributes values without considering the relevance. In contrast, (EDM)-D-2 directly manipulates on the original data with data conversion and relevance learning simultaneously to avoid information loss and attribute space confusion. (EDM)-D-2 firstly estimates internal relevance (i.e., relevance within the attribute) influenced distance by considering the categorical attribute value frequency and mapping numerical attribute values into multiple bins. Then it takes a wrapper approach to iteratively optimize relevance influenced distance and bin boundaries using a Frobenius-norm deviation as its objective function. Co-occurrence Mover's Distance is proposed to explicitly explore relevance between attributes in each iteration. Finally, the distance for numerical attribute values is refined based on the original values and the fallen bin centers. Experimental results on a number of real-world datasets demonstrate that (EDM)-D-2 outperforms the state-of-the-art methods.																	1088-467X	1571-4128					2020	24	1					83	99		10.3233/IDA-184399													
J								A latent-label denoising method for relation extraction with self-directed confidence learning	INTELLIGENT DATA ANALYSIS										Distant supervision; relation extraction; latent label; confidence learning; discriminative loss		Distant supervision for relation extraction aims to automatically obtain a large number of relational facts as training data, but it often leads to noisy label problem. In this paper, we propose a self-directed confidence learning based latent-label denoising method for distantly supervised relation extraction. Concretely, a self-directed algorithm that combines the semantic information of model prediction and distant supervision is designed to predict the confidence score of latent labels. Since this mechanism utilizes the obtained latent labels of easy examples to produce the latent labels of hard examples step by step, it is a robust and reliable learning process. Besides, it facilitates dynamic exploration of the confidence space to achieve better denoising performance. Moreover, to cope with the common imbalance problem in large corpus where the negative instances account for a much larger percentage, we introduce a discriminative loss function to solve the misclassification between non-relational and relational instances. Empirically, in order to verify the generality of the proposed denoising method, we use different neural models - CNN, PCNN and BiLSTM for representation learning. Experimental results show that our method can correct the noisy labels with high accuracy and outperform the state-of-the-art relation extraction systems.																	1088-467X	1571-4128					2020	24	1					101	117		10.3233/IDA-184414													
J								Community detection in dynamic networks using constraint non-negative matrix factorization	INTELLIGENT DATA ANALYSIS										Community detection; dynamic networks; evolutionary clustering; geometric structure; non-negative matrix factorization		Community structure, a foundational concept in understanding networks, is one of the most important properties of dynamic networks. A large number of dynamic community detection methods proposed are based on the temporal smoothness framework that the abrupt change of clustering within a short period is undesirable. However, how to improve the community detection performance by combining network topology information in a short period is a challenging problem. Additionally, previous efforts on utilizing such properties are insufficient. In this paper, we introduce the geometric structure of a network to represent the temporal smoothness in a short time and propose a novel Dynamic Graph Regularized Symmetric NMF method (DGR-SNMF) to detect the community in dynamic networks. This method combines geometric structure information sufficiently in current detecting process by Symmetric Non-negative Matrix Factorization (SNMF). We also prove the convergence of the iterative update rules by constructing auxiliary functions. Extensive experiments on multiple synthetic networks and two real-world datasets demonstrate that the proposed DGR-SNMF method outperforms the state-of-the-art algorithms on detecting dynamic community.																	1088-467X	1571-4128					2020	24	1					119	139		10.3233/IDA-184432													
J								A study on rare fraud predictions with big Medicare claims fraud data	INTELLIGENT DATA ANALYSIS										Big data; Medicare fraud detection; class imbalance; data sampling; rare classes	OUTLIER DETECTION; CLASSIFICATION	Access to copious amounts of information has reached unprecedented levels, and can generate very large data sources. These big data sources often contain a plethora of useful information but, in some cases, finding what is actually useful can be quite problematic. For binary classification problems, such as fraud detection, a major concern therein is one of class imbalance. This is when a dataset has more of one label versus another, such as a large number of non-fraud observations with comparatively few observations of fraud (which we consider the class of interest). Class rarity further delineates class imbalance with significantly smaller numbers in the class of interest. In this study, we assess the impacts of class rarity in big data, and apply data sampling to mitigate some of the performance degradation caused by rarity. Real-world Medicare claims datasets with known excluded providers are used as fraud labels for a fraud detection scenario, incorporating three machine learning models. We discuss the necessary data processing and engineering steps in order to understand, integrate, and use the Medicare data. From these already imbalanced datasets, we generate three additional datasets representing varying levels of class rarity. We show that, as expected, rarity significantly decreases model performance, but data sampling, specifically random undersampling, can help significantly with rare class detection in identifying Medicare claims fraud cases.																	1088-467X	1571-4128					2020	24	1					141	161		10.3233/IDA-184415													
J								EEG-based tonic cold pain assessment using extreme learning machine	INTELLIGENT DATA ANALYSIS										Common spatial pattern (CSP); electroencephalogram (EEG); extreme learning machine (ELM); tonic cold pain	ELECTROENCEPHALOGRAM; POWER; CLASSIFICATION; QUESTIONNAIRE; STIMULATION; PERCEPTION; ACTIVATION; MECHANISMS; RESPONSES; INDEXES	The purpose of this study is to present a novel method which can objectively identify the subjective perception of tonic pain. To achieve this goal, scalp EEG data are recorded from 16 subjects under the cold stimuli condition. The proposed method is capable of classifying four classes of tonic pain states, which include No pain, Minor Pain, Moderate Pain, and Severe Pain. Due to multi-class problem of our research an extended Common Spatial Pattern (ECSP) method is first proposed for accurately extracting features of tonic pain from captured EEG data. Then, a single-hidden-layer feedforward network is used as a classifier for pain identification. With the aid of extreme learning machine (ELM) algorithm, the classifier is trained here. The advantages of ELM-based classifier can obtain an optimal and generalized solution for multi-class tonic cold pain. Experimental results demonstrate that the proposed method discriminates the tonic pain successfully. Additionally, to show the superiority for the ELM-based classifier, compared results with the well-known support vector machine (SVM) method show the ELM-based classifier outperform than the SVM-based classifier. These findings may pay the way for providing a direct and objective measure of the subjective perception of tonic pain.																	1088-467X	1571-4128					2020	24	1					163	182		10.3233/IDA-184388													
J								Storyline extraction from news articles with dynamic dependency	INTELLIGENT DATA ANALYSIS										Storyline extraction; dynamic dependency; topic model; event extraction		Storyline generation aims to produce a concise summary of related events unfolding over time from a collection of news articles. It can be cast into an evolutionary clustering problem by separating news articles into different epochs. Existing unsupervised approaches to storyline generation are typically based on probabilistic graphical models. They assume that the storyline distribution at the current epoch depends on the weighted combination of storyline distributions in the latest previous M epochs. The evolutionary parameters of such long-term dependency are typically set by a fixed exponential decay function to capture the intuition that events in more recent epochs have stronger influence to the storyline generation in the current epoch. However, we argue that the amount of relevant historical contextual information should vary for different storylines. Therefore, in this paper, we propose a new Dynamic Dependency Storyline Extraction Model ((DSEM)-S-2) in which the dependencies among events in different epochs but belonging to the same storyline are dynamically updated to track the time-varying distributions of storylines over time. The proposed model has been evaluated on three news corpora and the experimental results show that it outperforms the state-of-the-art approaches and is able to capture the dependency on historical contextual information dynamically.																	1088-467X	1571-4128					2020	24	1					183	197		10.3233/IDA-184448													
J								A new optimization layer for real-time bidding advertising campaigns	INTELLIGENT DATA ANALYSIS										Demand Side Platform (DSP); online advertising; gradient descent; optimization; Real Time Bidding (RTB)	AUCTION	While it is relatively easy to start an online advertising campaign, obtaining a high Key Performance Indicator (KPI) can be challenging. A large body of work on this subject has already been performed and platforms known as DSPs are available on the market that deal with such an optimization. From the advertiser's point of view, each DSP is a different black box, with its pros and cons, that needs to be configured. In order to take advantage of the pros of every DSP, advertisers are well-advised to use a combination of them when setting up their campaigns. In this paper, we propose an algorithm for advertisers to add an optimization layer on top of DSPs. The algorithm we introduce, called SKOTT, maximizes the chosen KPI by optimally configuring the DSPs and putting them in competition with each other. SKOTT is a highly specialized iterative algorithm loosely based on gradient descent that is made up of three independent sub-routines, each dealing with a different problem: partitioning the budget, setting the desired average bid, and preventing under-delivery. In particular, one of the novelties of our approch lies in our taking the perspective of the advertisers rather than the DSPs. Synthetic market data is used to evaluate the efficiency of SKOTT against other state-of-the-art approaches adapted from similar problems. The results illustrate the benefits of our proposals, which greatly outperforms the other methods.																	1088-467X	1571-4128					2020	24	1					199	224		10.3233/IDA-194527													
J								An approach to design reconfigurable manufacturing tools to manage product variability: the mass customisation of eyewear	JOURNAL OF INTELLIGENT MANUFACTURING										Mass customisation; Eyewear; Thermoforming mould; KBE system	FAMILY DESIGN; SYSTEMS; ART	In Mass Customisation (MC), products are intrinsically variable, because they aim at satisfying end-users' requests. Modular design and flexible manufacturing technologies are useful strategies to guarantee a wide product variability. However, in the eyewear field, the current strategies are not easily implementable, due to some eyewear peculiarities (e.g., the large variability of the frame geometry and material, and the necessity to use specific manufacturing phases). For example, acetate spectacle frames are bent through a thermoforming process. This particular phase requires dedicated moulds, whose geometry strictly depends on the frame model to be bent; consequently, changes of the frame geometry continuously require new moulds, which have to be designed, manufactured, used, and finally stored. The purpose of this paper is to propose a new strategy to transform a dedicated tool (i.e., a thermoforming mould) into a reconfigurable one, to optimise the tool design, manufacturing and use. First, how the frame features influence the mould geometry has been investigated, creating a map of relations. On the basis of this map, the conventional monolithic-metallic mould was divided into "standard" (re-usable) and "special" (ad-hoc) modules, where the "special" ones are in charge of managing the variability of the product geometry. The mapped relations were formalised as mathematical equations and then, implemented into a Knowledge Based Engineering (KBE) system, to automatically design the "special" modules and guarantee the mould assemblability. This paper provides an original example of how a reconfigurable thermoforming mould can be conceived and how a KBE system can be used to this aim.																	0956-5515	1572-8145				JAN	2020	31	1					87	102		10.1007/s10845-018-1436-5													
J								Kappa Updated Ensemble for drifting data stream mining	MACHINE LEARNING										Machine learning; Data streams; Concept drift; Classification; Ensemble learning	DYNAMIC WEIGHTED MAJORITY; ABSTAINING CLASSIFIERS; FEATURE-SELECTION; CLASSIFICATION; ADAPTATION	Learning from data streams in the presence of concept drift is among the biggest challenges of contemporary machine learning. Algorithms designed for such scenarios must take into an account the potentially unbounded size of data, its constantly changing nature, and the requirement for real-time processing. Ensemble approaches for data stream mining have gained significant popularity, due to their high predictive capabilities and effective mechanisms for alleviating concept drift. In this paper, we propose a new ensemble method named Kappa Updated Ensemble (KUE). It is a combination of online and block-based ensemble approaches that uses Kappa statistic for dynamic weighting and selection of base classifiers. In order to achieve a higher diversity among base learners, each of them is trained using a different subset of features and updated with new instances with given probability following a Poisson distribution. Furthermore, we update the ensemble with new classifiers only when they contribute positively to the improvement of the quality of the ensemble. Finally, each base classifier in KUE is capable of abstaining itself for taking a part in voting, thus increasing the overall robustness of KUE. An extensive experimental study shows that KUE is capable of outperforming state-of-the-art ensembles on standard and imbalanced drifting data streams while having a low computational complexity. Moreover, we analyze the use of Kappa versus accuracy to drive the criterion to select and update the classifiers, the contribution of the abstaining mechanism, the contribution of the diversification of classifiers, and the contribution of the hybrid architecture to update the classifiers in an online manner.																	0885-6125	1573-0565				JAN	2020	109	1					175	218		10.1007/s10994-019-05840-z													
J								Research on group animation design technology based on artificial fish swarm algorithm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Artificial fish swarm algorithm; group animation design; intelligent algorithm; 3D scene		Group animation is a technology that simulates group behavior by computer. It matures with the continuous development of computer graphics. The things that need to be described are expressed in the form of anime, which not only clearly reflects the process of the event, but also shows a more realistic picture, and can also increase the cognitive range of group behavior to a certain extent. Based on the analysis of the advantages and disadvantages of artificial fish swarm algorithm, this paper improves the research on artificial fish swarm algorithm, and it is easy to fall into local optimum, and it is improved by using convergence and divergence behavior in social learning mechanism., to some extent, improve the search performance of the algorithm. Based on the collision detection and avoidance, the improved artificial fish swarm algorithm is used to plan the group behavior path, and the obtained path data is imported into Maya. The tools provided by Maya will be used to establish the characters, 3D scenes and planning. The paths are merged together to get the corresponding group animation effects.																	1064-1246	1875-8967					2020	38	2			SI		1137	1145		10.3233/JIFS-179475													
J								Evaluation of vertical cooperative algebra connectivity in agricultural production-marketing chain based on control parameters	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Production-marketing; community discovery algorithm; evaluation; numerical nonlinear equations; optimization algorithm		If the rural economy wants to be in line with the development of the times, it must improve the production-marketing model of agricultural products, grasp the market dynamics, and use the advantages of the Internet to conduct marketing and promotion. Moreover, the networked social background also provides a better platform for the expansion and development of agricultural production-marketing channels, which also creates better opportunities. This paper proposes an evaluation method of vertical cooperative algebra connectivity in agricultural production-marketing chain based on control parameters, making full use of the advantages of both community discovery algorithm and Ward hierarchical clustering algorithm, which is applied to solve numerical nonlinear equations. It can be seen from the simulation example that the algorithm not only has good stability, but also has high computational speed and precision. It is a feasible and effective optimization algorithm. The modification method can be applied not only in the agricultural production-marketing network, but also it provides a new way for the algorithm to be applied in other aspects.																	1064-1246	1875-8967					2020	38	2			SI		1147	1157		10.3233/JIFS-179476													
J								Analysis of social network user behaviour and its influence	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Social network; user behaviour; user Influence		As a communication tool, social networks have become an important part of people's daily lives. The proliferation of data in the big data era has spurred waves of research. Twitter is one of the most popular sites among all social network platforms. Twitter API allows researchers to easily study user behaviour and influence. In this paper, we use data obtained through the Twitter API to study the influence maximization problem based on the relationship graph of the social network and information dissemination model. In the current independent cascade propagation model, the influence weights between nodes are based largely on the fixed-value assumption and the randomized probability of acceptance; however, this does not conform to real life. The influence weight between users is closely related to the strength of relationships, content of communication, and so on. Focusing on user relation-ships, we introduce an improved weighted cascade mode combined with a heuristic algorithm to find an approximate solution to the problem of influence maximization. Example analysis indicates that the improved weighted cascade model can obtain a more significant and influential node set compared to conventional methods.																	1064-1246	1875-8967					2020	38	2			SI		1159	1171		10.3233/JIFS-179477													
J								Artificial emotion modeling based on container (CUP) algorithm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Artificial emotion; container algorithm; STMS model		Human emotion is an extremely complex social phenomenon that dominates many social phenomena. The paper proposes a method of emotional modeling in the context of complex systems based on the container model, which is more intuitive and can be solved than the conventional emotional modeling methods. Meanwhile the container algorithm also has the advantages of high parallelism, so it has practical significance for the simulation and analysis of emotions.																	1064-1246	1875-8967					2020	38	2			SI		1173	1179		10.3233/JIFS-179478													
J								Big data metrics: Time sensitivity analysis of multimedia news	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Big data analysis; temporal metric; temporal sensitivity; multimedia news	INFORMATION	With the daily release of the huge amount of new information, the web information such as video, image, audio and text information is growing dramatically. Most of this vast information changes over time. It may become ineffective, obsolete and worthless, and even affect user's understanding of web information, degrade their experience. Therefore, analyzing this temporal-sensitive information in multimedia is a vital issue. To this end, this paper analyses the temporal sensitivity of multimedia web information to find what kind of information is temporal-sensitive, how this information is responsive to time, and how to evaluate the temporal sensitivity of information. We start with four types of features, that is time, content, user behavior and related multimedia news, then set up a triple model to depict news. By establishing the energy transfer relationship between news and related and similar news, time, and user behavior, we measure the energy, and use the change ratio of energy as the temporal sensitivity of news. The data set is a multimedia news corpus, including video, image and text news. In the experiment, we take the users' comments as the validation set. The result basically matches the validation set, and it shows our metric is reasonable.																	1064-1246	1875-8967					2020	38	2			SI		1181	1188		10.3233/JIFS-179479													
J								Web user preferences and behavior clustering based on BP neural network	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										BP neural network; cluster analysis; web user preference analysis; SOFM		The paper uses BP neural network method to analyze the behavior preference of web users to achieve user behavior clustering, help advertisers to find online advertising design and optimize web design. At the same time, the paper analyzes the user preference cluster analysis, which helps users to find the desired webpage and content more conveniently, shorten the retrieval time and improve the retrieval efficiency. Firstly, the paper analyzes the log files of the web server, and then conducts session classification, finds the frequent data from the session vector, and normalizes the generated pattern vector. The BP SOFM model is used to cluster the user behavior preferences to generate users. Clustering. The experimental results show that BP neural network analysis method can effectively analyze user preferences and cluster user behavior.																	1064-1246	1875-8967					2020	38	2			SI		1189	1196		10.3233/JIFS-179480													
J								Navigational risk assessment of Three Gorges ship lock: Field data analysis using intelligent expert system	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Ship lock; intelligent expert system; data mining; field data; navigational risk	DYNAMICS	The Three Gorges ship lock, as one of the world leading navigational architectures, plays a significant role in the inland water transport of China. The rapid development of marine transportation increases great pressure on the maritime monitoring and administration in the ship locks. Hence, it is essential to explore the navigational risk factors. This study proposes a new dynamics model to investigate the evolution mechanism of navigational risk in Three Gorges ship lock. Intelligent expert system was used to analyze the risk factors. Experimental validation results demonstrate that the weather may impact the ship navigational safety with a high risk probability. Meanwhile, the navigational risk in flood season is relatively higher than that in dry season due to the increase of flow velocity and rain fall. The present study may provide useful insights to the evolution of inland navigational risk as well as maritime administration in the Three Gorges ship lock.																	1064-1246	1875-8967					2020	38	2			SI		1197	1202		10.3233/JIFS-179481													
J								Research on data retrieval and analysis system based on Baidu reptile technology in big data era	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Big data; Baidu crawler technology; data retrieval; dynamic page capture; automatic navigation browsing		With the rapid development of the Internet, the current Web has become the main platform for people to publish and retrieve information. How to quickly and accurately find the information required by users in a large amount of network information resources has become an urgent need of the people. Web crawlers are research fields that appear to meet this demand. Based on this, the paper designs and implements a distributed web crawler system based on the existing research work, and its goal is to provide high quality data support for the network public opinion system. The web crawler system designed and implemented in this paper solves the problems of low efficiency, poor scalability and low automation of single-machine crawlers, which improves the speed of webpage collection and data extraction precision and expands the scale of webpage collection. At the end of the article, the system related interface screenshots and test results are displayed. It can be seen from the test results that the crawler system can effectively collect dynamic web pages, and the result of automatic extraction of web pages has high precision, and also realizes the entire crawling system.																	1064-1246	1875-8967					2020	38	2			SI		1203	1213		10.3233/JIFS-179482													
J								Research on campus network cloud storage open platform based on cloud computing and big data technology	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Cloud computing; big data technology; campus network; cloud storage platform; Hadoop platform; HDFS plus HBase distributed storage		Based on the analysis of campus cloud storage requirements, the paper plans a cloud storage structure model to achieve the goal of breaking the bottleneck of traditional campus network resource utilization. The model established in the thesis fully considers the problems of cluster data security and big data analysis and utilization, and proposes an optimization scheme for cloud storage open platform system design. Plan the HDFS + HBase distributed storage deployment in the Hadoop cluster, elaborate the process of behavior analysis, and find a way to optimize and improve the cloud storage in colleges and universities. HBase technology implements operations on HDFS distributed storage systems in Hadoop clusters, effectively improving the efficiency of resource access and resource analysis. The cloud platform storage platform designed by the thesis solves the problem of integrating the superior resources of colleges and universities.																	1064-1246	1875-8967					2020	38	2			SI		1215	1223		10.3233/JIFS-179483													
J								Research on Weibo user behavior system for subjective perception and big data mining technology	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Subjective perception; big data mining technology; microblog user behavior analysis; MURank algorithm; QoE hierarchical indicator system		Firstly, the paper analyzes the information propagation mode of Sina Weibo, and proposes a new MURank algorithm based on PageRank algorithm, which fully considers the relationship between users, the transmission relationship between users and microblogs, and the forwarding relationship between microblogs. At the same time build the user-microblog mode diagram. The design of the thesis is also more in line with the information dissemination mode of the actual Weibo platform. Experiments prove that MURank can effectively solve the impact of "zombie powder" and "level sinking" problems, and more realistically reflect the level of influence of users. Secondly, the paper combines BP neural network algorithm to improve the QoE hierarchical index system, and proposes a five-level index model. The mobile video service is taken as an example to illustrate the establishment process of the indicator system and the QoE quantitative evaluation method. By verifying on the open source distributed platform Hadoop platform, it is found that the proposed method is a solution to deal with massive data analysis.																	1064-1246	1875-8967					2020	38	2			SI		1225	1234		10.3233/JIFS-179484													
J								Exploration of marine ship anomaly real-time monitoring system based on deep learning	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Deep learning; abnormal monitoring; DDBN-OCSVM algorithm		Maritime transportation is the main traffic mode of globalization, accounting for about 90% of the proportion of global trade. Maritime safety is an important problem of marine transportation. Therefore, it is very important to set up the ship abnormal real-time monitoring system. In order to realize the real-time monitoring of abnormal data in the course of ship driving, a hybrid single classification framework based on depth learning is designed. DDBN-OCSVM framework uses the deep network to solve the problem that complex high-dimensional data are difficult to reduce and learn. The single classification algorithm is used to avoid the impact of data imbalance on the results of ship real-time monitoring anomaly detection. Finally, the experimental analysis and discussion of the data are carried out. The experimental results show that DDBN-OCSVM can effectively reduce the detection error under the accelerating effect of GPU and cuDNN. The DDBN-OCSVM algorithm proves that the unsupervised feature learning and hierarchical representation are effective and feasible. It is also proved that it is feasible to apply this deep learning mode to real-time monitoring of ship anomalies.																	1064-1246	1875-8967					2020	38	2			SI		1235	1240		10.3233/JIFS-179485													
J								Design and exploration of virtual marine ship engine room system based on Unity3D platform	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Ship engine room; virtual reality technology; ship system		In order to enable the students to experience the actual engine room working environment, have a full understanding of the Marine ship equipment, and have the same operating experience as the working environment in the real ship cabin to improve the effect of education and training, the rudder cabin of a Dalian ocean 10000-ton super large oil tanker "Yuanshan lake" is used as the virtual design object. First, the mathematical model of the rudder is set up on the basis of introducing the related concepts of the ship system. Then, the layout of the actual cabin is analyzed, and a complete three-dimensional model of the rudder cabin is established with 3ds Max. Finally, the three-dimensional model of the cabin is introduced into the virtual reality engine Unity3D, which realizes the interactive operation of the cabin equipment. An extended user interface is designed to complete the generation and release of the final virtual scene program. The research and design of this subject has certain reference significance in applying virtual reality technology to the various different compartments of the whole ship and broadening its application to different ship types.																	1064-1246	1875-8967					2020	38	2			SI		1241	1247		10.3233/JIFS-179486													
J								Construction of marine ship automatic identification system data mining platform based on big data	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										AIS data; cloud computing; distributed technology; big data	AIS	In order to process massive Marine Ship Automatic Identification System (AIS) data in real time and extract the required information from it used for subsequent data mining and rule extraction, based on big data background, cloud computing and distributed technology are applied, and combined with data mining methods, the massive AIS data mining platform is designed and realized, and the ship trajectory is analyzed with big data. The research results show that the cloud computing distributed technology AIS data mining platform studied is feasible, which has practical value and significance.																	1064-1246	1875-8967					2020	38	2			SI		1249	1255		10.3233/JIFS-179487													
J								Heat exchanger simulation and recovery device design of waste heat boiler of gas turbine generator set on ocean platform	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Gas turbine; waste heat recovery; waste heat boiler; combined cycle		In order to study the heat exchanger simulation and recovery device design of waste heat boiler of gas turbine generator set on Ocean platform, the theory of thermal cycle of gas turbine and the principle of steam turbine are introduced. The waste heat of exhaust gas of a certain type of gas turbine generator set on Ocean platform is taken as the object, to design the waste heat recovery device of this type of gas turbine generator set. According to the characteristics of the selected waste heat boiler, the mathematical model is reasonably simplified, and the steady-state simulation model of the waste heat boiler is established by using the computer simulation software, thus forming the whole model of the waste heat boiler. It is attempted to recover the waste heat from the exhaust gas of gas turbine generator set in Ocean platform by using gas-steam combined cycle power generation, which has certain reference significance for the design, application and improvement of a common gas turbine waste heat recovery device on Ocean platform. The established steady-state model simulation model and thermal efficiency calculation model have good accuracy, and have a certain auxiliary role in design calculation and performance research.																	1064-1246	1875-8967					2020	38	2			SI		1257	1263		10.3233/JIFS-179488													
J								Application of improved genetic algorithm in barge loading of offshore platform	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Genetic algorithms; offshore platform; barge; stowage	SYSTEM	In order to complete the loading operation in a short time, the genetic algorithm was improved, and the improved mechanisms such as the total crossover of the population and the distributed dynamic penalty function method were proposed. The distribution of ballast water in barge stowage was optimized The calculation of the ship's floating state, stability, the check of the program strength, and the improvement of the genetic algorithm were discussed. By simplifying the model, all ballast water tanks were involved in the stowage, so as to better meet the limitation of tide level, tank capacity and strength. A more adaptable load optimization solution was obtained. The results show that the improved genetic algorithm has shorter time and higher efficiency than the basic loading scheme. The improved genetic algorithm was used to optimize the barge loading plan, which met the engineering requirements and shorten the working time of the barge. Therefore, the research on the improved genetic algorithm can effectively improve the quality and reliability of the project, which is beneficial to improve the scientific, safety and reliability of the barge loading operation. This program has important implications for marine engineering.																	1064-1246	1875-8967					2020	38	2			SI		1265	1271		10.3233/JIFS-179489													
J								Application of 3D laser scanning technology in structural design of key parts of marine port machinery	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Three-dimensional laser; scanning technology; marine port; mechanical parts		In order to improve the accuracy of the mechanical key component model constructed by 3D laser scanning and reduce the economic loss to the marine port, the factors affecting the accuracy of 3D laser scanning were studied. By studying the error of the instrument, errors related to the target object, errors caused by external environmental conditions, errors caused by human operations, and errors caused by different measurement methods were discussed. In view of the errors caused by different measurement methods and different measurement methods for different parts, the scanning experiment was carried out and the obtained data was analyzed. A method of improving the scanning accuracy was proposed, which made the three-dimensional scanning more accurate. This method was applied to the design of key part structures for marine port machinery. The results show that the research in this paper can help designers accurately and completely obtain 3D point cloud data of mechanical key parts, improve scanning accuracy and reconstruct 3D model. Therefore, according to the application prospect of 3D scanning technology and the status quo in China, it is of great practical significance to study the accuracy of 3D laser scanning technology.																	1064-1246	1875-8967					2020	38	2			SI		1273	1279		10.3233/JIFS-179490													
J								Exploration of port intelligent AGV path tracking based on vision	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Visual system; BP algorithm; AGV system	OBJECT TRACKING; SIMULATION; SYSTEM	In order to prove the practical application of intelligent AGV (Automated Guided Vehicle) in port, the special requirements of port AGV are analyzed, BP (Back Propagation) network algorithm is improved, AGV road sign recognition system and AGV visual system are established, and finally the recognition accuracy and response speed of visual system are tested through experiments. The results show that the characteristics of AGV can meet the complex port conditions, and prove that the improved algorithm has good practical application value, as well as good recognition rate and accuracy of the visual system height. This will be of positive significance to the future research on the application of port AGV system in China.																	1064-1246	1875-8967					2020	38	2			SI		1281	1285		10.3233/JIFS-179491													
J								Multi-lane traffic flow monitoring and detection system based on video detection	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Ligent traffic; traffic flow detection; ViBe; background subtraction; ZigBee IoT technology		The paper adopts the improved ViBe algorithm, and initializes the background sample according to the original algorithm. Using the idea of image binarization, the current frame image is subjected to the foreground detection of the traditional ViBe, and then the second judgment is made according to the number of times the target is judged, and the original is changed. The fixed background factor in the algorithm background update strategy increases the background update probability during shadow elimination. Experiments show that the method can eliminate shadows in time and improve the accuracy of motion vehicle detection in different scenarios. At the same time, the paper designs a multi-lane traffic flow monitoring system based on improved ViBe algorithm combined with IoT ZigBee technology. The flow monitoring accuracy of the algorithm is verified.																	1064-1246	1875-8967					2020	38	2			SI		1287	1298		10.3233/JIFS-179492													
J								Research on mathematical model and dynamic positioning control algorithm of six degrees of freedom maneuvering in marine ships	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Ship; six degrees of freedom operation; dynamic positioning control; fourth-order Runge-Kutta algorithm		In this paper, based on the idea of separation modelling, through the analysis of ship motion and force, a mathematical model of ship motion under the action of external wind, waves and current is established in 6 free direction modelling. According to the force analysis of the ship, the calculation model of the 6-degree-of-freedom nude hull hydrodynamics and torque, the 6-DOF fluid dynamics and torque, the 6-DOF external force interference force and the moment, and the thrust and thrust of the pod propeller are established. Computing model. Finally, the computer simulation is carried out, and the fourth-order Runge-Kutta algorithm of numerical analysis method is used to calculate the 6-degree-of-freedom motion state of the ship. Through the experimental analysis and comparison with the related 6-degree-of-freedom literature, the 6-degree-of-freedom mathematical model designed in this paper is basically fulfil requirements. At the same time, according to the characteristics of ship dynamic positioning, the hybrid intelligent control strategy is used to design an adaptive dynamic positioning controller based on hybrid intelligent control. By testing and analysing the designed dynamic positioning controller, the results show that it responds in time and rises. The time is basically within the controllable range, has a certain ability to resist external environment, has good robustness, and the overshoot rate is relatively low, and the control quality is also good.																	1064-1246	1875-8967					2020	38	2			SI		1299	1309		10.3233/JIFS-179493													
J								Research on node localization based on 3D wireless sensor network	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Geological disaster warning; 3D-DVHop algorithm; localization problem; wireless sensor network		In emergency, it is very important to make a real-time and precise positioning of emergency address. As a large amount of emergencies take place in mountainous region, river valley and high building, original node location based on plane does not apply any more, thus, in order to improve precision of early warning and location of disaster, we have proposed a 3D-DVHop localization algorithm, and compare it with the typical 3-D location algorithm APIS. Through the simulation experiment, the localization error of the improved 3D-DVHop algorithm is far less than that of APIS algorithm, which improves the localization accuracy greatly under unexpected circumstances and laying a solid foundation for the following decision-making and rescuing work.																	1064-1246	1875-8967					2020	38	2			SI		1311	1318		10.3233/JIFS-179494													
J								Research on key technology analysis and system design of enterprise patent management system	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Patent management analysis system; workflow; integrated knowledge environment; key technology analysis		Patent documents, the paper constructs a patent model tree and establishes a similar patent determination method based on vector space model and patent document features. The paper then proposes to use the patent application number as the association identifier, and associate the ontology and the ontology instance with the existing patent information database, which improves the speed of establishing the ontology and reduces the maintenance and update of the ontology. At the same time, based on the integration of existing workflow management and knowledge flow management, this paper proposes an integration framework with patent knowledge as the core knowledge and integration link, and describes the nature of knowledge retrieval and the method of importance differentiation. Finally, the paper designs a patent retrieval system, establishes the structural model and functional model of the system and the technical framework of the implementation system. On this basis, the design of the update module of the PMAS system is discussed. Finally, the performance optimization of the PMAS system is designed in detail.																	1064-1246	1875-8967					2020	38	2			SI		1319	1328		10.3233/JIFS-179495													
J								Development and application of massive unstructured big data retrieval technology based on cloud computing platform	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Cloud computing; big data; unstructured stability retrieval; data retrieval engine	HEALTH-CARE; FRAMEWORK	In view of the large data volume and complex structure of big data in the operation of major systems, the traditional cloud computing distributed multi-layer architecture data retrieval method has been difficult to meet the needs of big data retrieval. Therefore, related research must be centered around the structure of complex big data is unfolded, especially unstructured big data, to shape a new data cluster infrastructure. The cluster environment needs to implement stable retrieval of unstructured data through a cloud-centric method. The system introduces an unstructured big data retrieval framework as an unstructured database, as a data retrieval engine to improve the cloud computing big data non-structure retrieval service, and obtain the key code of unstructured big data under the retrieval technology. Finally, the system is applied to the exsperimental system, and an experimental unstructured big data integration system is built to realize the unified retrieval of large-scale different business functions and business data, so as to satisfy the users to quickly retrieve from a large number of heterogeneous business systems and massive data. Claim. Through search engine technology, the use of text mining, natural language processing, information retrieval and other fields of technology to further improve the precision and recall rate of full-text search. The application of this technology can meet the needs of unified retrieval of large-scale different business data; and at the same time meet the rapid response requirements of large-scale data retrieval requests. The experimental results show that the designed system has high precision and low retrieval time in the process of retrieving unstructured big data under cloud computing, which can realize the stable retrieval of unstructured cloud data.																	1064-1246	1875-8967					2020	38	2			SI		1329	1337		10.3233/JIFS-179496													
J								A MCCDMA-based cognitive radio system for the internet of things	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Cognitive radio; IoT; MCCDMA; overlay; spectrum hole; underlay	NETWORKS	In order to solve the problem of effective allocation of spectrum resources and provide an effective spectrum access method for the Internet of Things, a cognitive wireless system based on MCCDMA structure is proposed, and the performance of the system is studied. Two strategies named Underlay and Overlay are proposed and corresponding BER performance are analyzed when the main users are occupying the spectrum. The trend of probability of outage are researched when the number of secondary users are mutative. Theoretical analysis and simulation results show that the cognitive wireless system can use spectrum resources more flexibly and effectively, which is very suitable for application in IoT systems.																	1064-1246	1875-8967					2020	38	2			SI		1339	1347		10.3233/JIFS-179497													
J								Dynamic analysis and adaptive obstacle avoidance algorithm of wave glider based on fuzzy control	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Wave glider; dynamic analysis; fuzzy control; modelling; obstacle avoidance		This paper takes the wave glider as the research object. The wave glider is studied comprehensively from the aspects of motion mechanism, motion law and hydrodynamic performance. Firstly, based on the turbulence model of RANS equation, dynamic mesh method, the hydrodynamic performance of two-dimensional tandem hydrofoils is analyzed, which has been developed twice. The paper puts forward the a obstacle avoidance algorithm of wave glider based on fuzzy control. Combining the principle of line-of-sight navigation with the idea of fuzzy control, the path planning of wave glider is carried out. The simulation and experimental results show that the algorithm can provide real-time and stable motion path of the wave glider, effectively avoid obstacles and safely reach the target point.																	1064-1246	1875-8967					2020	38	2			SI		1349	1358		10.3233/JIFS-179498													
J								Application of improved multidimensional spatial data mining algorithm in agricultural informationization	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Data mining; WebGIS; agricultural informatization; middleware; multidimensional space	GEOSPATIAL DATA; INTERNET	At this stage, with the continuous development of data acquisition technology, the mining of high-dimensional spatial data has become a hot issue. And so on. The spatial data mining component studied in this paper is part of the spatiotemporal data middleware. The main research result is to propose a density-based and grid-based extended adjacent spatial clustering method and apply this method to the space for agricultural informatition. In data mining, we also used the middleware technology to complete the agricultural geographic information system based on MapXtreme, and applied the spatial data mining for agricultural informatization to develop the middleware of agricultural variety division, which solved the agricultural informatization. The practical application is of our country.																	1064-1246	1875-8967					2020	38	2			SI		1359	1369		10.3233/JIFS-179499													
J								Research on university scientific research patent management information system based on BS mode	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										University scientific research patent management; BS mode; ASP technology; SQL server database; information system design		The development of this system is based on ASP technology, and through the application of B/S structure and SQL Server database, it provides necessary scientific research data support for the sharing of scientific research information in colleges and universities, and basically realizes the management of personnel files, personnel management and professional title. Management integration of scientific research projects and management of information systems. Develop a research goal of web-based network research management information system. Through this system, it can manage and standardize more scientific research projects. The system is a complete set of college plans, declarations, contracts, Research management information systems for results, patents, funds, personnel, equipment, etc. The system is based on the B/S mode, perfecting the output system of the report, and providing corresponding data support for the smooth development of scientific research projects, providing a solid material foundation and software hardware for the research and management of scientific research projects. Support, providing an electronic data platform for scientific research and academic exchanges, successfully achieved design tasks and goals.																	1064-1246	1875-8967					2020	38	2			SI		1371	1379		10.3233/JIFS-179500													
J								Application of ant colony clustering algorithm in coal mine gas accident analysis under the background of big data research	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Coal and gas outburst; prediction; ant colony clustering algorithm; K-means algorithm		In order to better solve the problem of gas outburst prediction, based on the in-depth study of ant colony algorithm, the ant colony clustering algorithm is improved, and the population classification and ant sensory perception characteristics are applied to make the ant colony the most likely to find. The optimal solution effectively avoids the possibility of local optimization, improves the global optimization performance and convergence speed of the algorithm, and reduces the influence of human subjective factors. Based on the prominent basic speech and actual working conditions, the paper selects five indexes of gas velocity, initial gas velocity, gas content, gas pressure and coal firmness coefficient as clustering attributes, and uses ant colony clustering algorithm to judge outstanding the state of occurrence. The paper uses MATLAB programming language to write a coal and gas outburst prediction program based on improved ant colony clustering algorithm, and predicts a coal mine. The final result is the same as the actual observation.																	1064-1246	1875-8967					2020	38	2			SI		1381	1390		10.3233/JIFS-179501													
J								Formal verification of a task scheduler for embedded operating systems	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Task scheduler; program verification; embedded operating system; formal methods		A task scheduler plays an important role in Embedded Operating Systems (EOS). In order to ensure the scheduling properties of an EOS, it is important to verify the implementation of its task scheduler. However, the implementation of a task scheduler requires the manipulation of machine registers directly and uses very complex C pointer structures to manage the task related data. Thus, we need a verification system that can formally describe both C pointer programs and embedded code pointers (e.g., update the stack register) naturally. Based on the Verified Software Toolchain and the CompCert ARM assembly syntax and semantics, we utilize the Coq proof assistant to develop formal reasoning support for the verification of a low-level task scheduler. Our verification system has the capability to verify both C and ARM assembly language; it supports the separation of logical abstractions and specific implementations. More importantly, all the verification can be done in a unified modular verification framework, which minimizes semantic gaps at specification interfaces.																	1064-1246	1875-8967					2020	38	2			SI		1391	1399		10.3233/JIFS-179502													
J								RSSI Estimation for wireless sensor network through-the-earth communication at frequency 433 MHz	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Wireless sensor networks; experiments; node high; received signal strength; horizontal distance between nodes	INTERNET	To explore wireless sensor network signal transmission characteristics in through-the-earth communication, this paper focused on wireless network signal transmission attenuation by using 433MHz carrier frequency. Sensor nodes electromagnetic wave transmission characteristics were studied under the condition the different receiving node height, horizontal distance between the transmitting node and the receiving node through the wheat field experiments and computer simulation, the relationship between the received signal strength was established, transmission characteristics of field soil information collection sensor nodes in the soil under four wheat stages were put forwarded. The experiment demonstrated that 8 model goodness of fit R-2 of effection of receiving node high and inter-nodes horizontal distance on RSSI(Received Signal Strength Indicator). Besides, three-dimensional surface of RSSI was built in four wheat growing periods, which the fitting model and goodness of fit of RSSI are obtained, and the model verification was conducted through SPASS software. Validation results showed that the model could better predict the received signal strength at different condition through-the-earth communication. The study can provide the technical support for sensor network node deployment and the establishment of the system in soil information acquisition.																	1064-1246	1875-8967					2020	38	2			SI		1401	1410		10.3233/JIFS-179503													
J								Optimization design of automatic filing system of financial management information under the background of information technology development	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Information technology; financial management information; automatic; filing system; optimization	COMMUNICATION-SYSTEM	The collection stability of traditional systems is low, it is not easy to delete interference data, the access speed is slow, and the error cannot be effectively detected. For this reason, this paper designs a new automatic filing optimization system of financial management information under the background of information technology development. From four aspects of data dynamic collection, data de-duplication, data access control and RAID error detection scheme improvement, the traditional system is optimized The financial management information is collected through a data collection method that ensures high efficiency and high stability. In the process of accessing, the semantic ontology is formed, and the access group in the time domain is classified by the support vector machine method to improve the access speed. The repetitive data model is established, and the deduplication processing is realized by the fractional Fourier transform technique combined with the post-processing result of fourth-order cumulant. The limited set of HDD error detection is reset to help with long-term filing of RAID system data. The result is high filing speed and good filing effect, and the conclusion that the design system is highly practical is obtained.																	1064-1246	1875-8967					2020	38	2			SI		1411	1422		10.3233/JIFS-179504													
J								Quantitative trading system based on machine learning in Chinese financial market	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Machine learning; quantitative trading; hidden markov; neural network		Quantitative Trading based on Machine Learning can increase the stock exchanging competitive and further enhance stability in the Chinese financial market, while the Risk to income ratio in the A share sector haven't been studied well enough so far in the Quantitative Trading. The paper study the risk and opportunity in the Chinese share market over the period 2005-2013 under Hidden Markov Model (HMM) system estimator. And then, the quantitative stock selection strategy based on neural network is studied based on multiple factors of the total market value of the constituent stocks in the SSE 50 Index, the OBV energy wave, the price-earnings ratio, the Bollinger Bands, the KDJ stochastic index, and the RSI indicators. Back testing obtained the conclusion that the Machine Learning strategy is equally valid for Chinese finical market. By analysing the risk of strategic returns, we can also conclude that the Chinese share market is effective in Quantitative Trading.																	1064-1246	1875-8967					2020	38	2			SI		1423	1433		10.3233/JIFS-179505													
J								Research on key technologies of wine quality and safety system using ANN	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Wine identification; fusion spectrum; compressive sampling; artificial neural network	FOOD QUALITY	At present, the identification of wine is artificial, and it has certain subjectivity. An alternative way that predicting brand based on fusion spectrum combined with Compressive Sampling as a dimensionality reduction technique is proposed in order to achieve the fast, scientific quality evaluation for wine. Successful results have been obtained in the wine identification use Artificial Neural Networks (ANNs) which has the faster speed and precision than typical algorithm. In the end, a multi-spectral information fusion analysis method for wine identification is proposed, which is an efficient solution to the problem of counterfeiting wine.																	1064-1246	1875-8967					2020	38	2			SI		1435	1441		10.3233/JIFS-179506													
J								Inertial sensor-based human activity recognition via ensemble extreme learning machines optimized by quantum-behaved particle swarm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Activity recognition; extreme learning machine (ELM); quantum-behaved particle swarm optimization (QPSO); multi-classifier fusion	CLASSIFICATION; FRAMEWORK; SYSTEM	Sensor-based human activity recognition has a wide range of applications including caring the elderly, helping chronic patients, fitness, etc. As a new kind of single-layer feedforward network, extreme learning machine (ELM) has faster training speed and stronger generalization performance, which provides an effective technique for activity recognition. However, due to the random determination of input weights and hidden deviations, the ELM may converge to a local minimum in some cases. Therefore, in order to overcome the shortcomings of ELM and design a reliable and accurate recognition system, this paper proposes a multi-classifier recognition framework which utilizes extreme learning machines optimized by quantum-behaved particle swarm optimization (QPSO) as the base classifiers. The quantum-behaved particle swarm optimization was used to select the optimal parameters of base ELMs which are trained on different attribute characteristics. The proposed approach is assessed with two inertial sensor data sets. Comparative experiments with other optimization methods indicated that QPSO-ELM has better accuracy performance for inertial sensor-based human activity recognition. The experiment showed that the proposed ensemble QPSO-ELM recognition method achieves an accuracy of 96.4% for recognizing six activities.																	1064-1246	1875-8967					2020	38	2			SI		1443	1453		10.3233/JIFS-179507													
J								Design and implementation for a fast reaction testing and training system to fight sports based on real data acqusition	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Data acquisition; fast response; system model; design and implement		In order to master the training state and the characteristics of fighting athletes, it need to obtain a lot of real-time data. By analyzing the characteristics of the test and training process of fighting sports, a defensive and offensive model of sport response test and training is constructed, which is easy to monitor and collect the human fighting sports data, aiming at the special needs for the fast response and data accquisition of sport training Based on this model, the basic requirements of fast response training and testing system are summed up, and a four-layer architecture design scheme is put forward by using software engineering technology, which is realized by computer hardware and software technology combined with real-time data acquisition and interface design. Finally, the practical verification is carried out by the application of the system, and the visual expression is given by the graph, and the design and implement can meet the requirement of the task.																	1064-1246	1875-8967					2020	38	2			SI		1455	1461		10.3233/JIFS-179508													
J								Analysis of textile defects based on PCA-NLM	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Textiles; defect detection; texture enhancement; class separability	FEATURES	Textile defects are not only the key to product quality control, but also play an important role in the textile industry. In this paper, by analyzing and studying the traditional detection methods of textile defects, the defects in the detection are confirmed. In view of how to reduce the deviation caused by environmental factors in the information collection process, as well as the improvement of the class separability between the defect part and the normal area, the introduction of PCA-NLM is proposed. PCA-NLM adopts the principle of combination of texture image enhancement module and automatic defect detection. By enhancing the texture to achieve the original characteristics of the product itself, PCA-NLM is more excellent in de-noising effect and image acquisition accuracy compared with the same type of texture enhancement method. To improve the separation accuracy and defect recognition and detection ability of defect parts, effectively improve the image collect Ion of the product's own texture characteristics, at the same time, enhance the effectiveness of defect parts and reduce defect detection defect rate. It has been proved that the ability of seven kinds of defects in textile production is significantly improved compared with the traditional methods.																	1064-1246	1875-8967					2020	38	2			SI		1463	1470		10.3233/JIFS-179509													
J								Periodic behavior of a class of nonlinear dynamic systems based on the Runge-Kutta algorithm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Runge-Kutta algorithm; nonlinear dynamic system; periodic solution; Liapunov function method	ASYMPTOTIC-BEHAVIOR; SMOOTH SOLUTIONS	In this paper, the periodic state of a class of nonlinear hydrodynamic models is studied. This study is applicable to many fields, such as engineering, cybernetics and so on. We use the Runge Kutta algorithm. First, we give a four order nonlinear hydrodynamic model based on the Runge Kutta algorithm. Then, we study the periodic dynamic characteristics of the nonlinear model and establish some new sufficient conditions. The results of our study promote and improve the existing results. And the numerical results are also verified by the numerical experiments. Therefore, the study of this paper has a great help to master the periodic motion of the fluid.																	1064-1246	1875-8967					2020	38	2			SI		1471	1476		10.3233/JIFS-179510													
J								Failure recognition method of rolling bearings based on the characteristic parameters of compressed data and FCM clustering	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Failure recognition; compressive sensing; FCM clustering; characteristic parameters	INTERNET	Compressive sensing retains the substantive characteristics of the original signal based on several sampling values and can realize distortionless reconstruction of signals with high probability. Hence, compressive sensing solves the contradictions among massive sampling data, signal processing speed and hardware equipment in the framework of traditional Shannon's sampling theorem. In this study, as a means of compressive sensing, a new failure recognition method of rolling bearing based on the characteristic parameters of compressed data and fuzzy-C mean (FCM) clustering is proposed. In this method, kurtosis, variance and waveform factor are used as the characteristic parameters. Using the proposed method, the sensitive characteristics were extracted by using the compressed information directly, and the eigenvectors of the sample signals were classified and identified through FCM clustering. In the experiment, the proposed method was compared with different compressed matrix methods and the traditional signal acquisition methods under the same axis diameter and different axis diameters. The results demonstrated the improved performance of the proposed method compared to the other methods.																	1064-1246	1875-8967					2020	38	2			SI		1477	1485		10.3233/JIFS-179511													
J								Prediction method of cyanobacterial blooms spatial-temporal sequence based on deep belief network and fuzzy expert system	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Cyanobacteria bloom prediction; deep belief networks; fuzzy expert system; spatiotemporal sequence	WATER	The process of cyanobacteria bloom in rivers and lakes is a highly non-stationary and non-linear process. The existing cyanobacterial bloom prediction method mainly uses time series model and single intelligent model, but time series model and single intelligent model cannot effectively explain the cyanobacterial bloom generation process, and the prediction accuracy is not high. In view of the above deficiencies, this paper proposes to use the cyanobacteria bloom spatiotemporal sequence data for modeling. Considering the characteristics of large-scale nonlinear trend term and small-scale residual term in the cyanobacteria bloom spatial-temporal sequence, the deep belief networks is used to model and explain the large-scale nonlinear trend term of the cyanobacteria bloom spatiotemporal sequence. Then use the time autocorrelation model and the multivariate spatiotemporal autocorrelation model to model and interpret the small-scale residual term; finally, after superimposing the large-scale nonlinear trend term and the small-scale residual term, the adaptive neuro-fuzzy system model is used to predict the chlorophyll a value of the water. Therefore, a fuzzy spatial and temporal sequence prediction method based on fuzzy expert system is proposed. The model verification results show that compared with the existing time series model and single intelligent model, the method can more fully explain the non-stationary and nonlinear dynamic changes of the cyanobacterial bloom spatial-temporal sequence. It provides a new method for accurately predicting cyanobacteria bloom in rivers and lakes.																	1064-1246	1875-8967					2020	38	2			SI		1487	1498		10.3233/JIFS-179512													
J								Research on the two-player two-way principal-agent model and optimization algorithm with lower and upper bounds	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Lower and upper bounds; two-way principal-agent model; virtual principal; pivoting algorithm; sequential quadratic programming	FORECASTING ACCURACY; RESELLERS; INTERNET; SYSTEM; IMPACT	An optimization algorithm is designed for the solution of the two-way principal-agent model with lower and upper bounds. This work analyzed the two-way principal-agent relationship between the two players who both have dual status: the principal status and the agent status, then proposed the expected utility function for the virtual principal, and a two-way principal-agent model with lower and upper bounds which embodies two players and two-side constraints was established too, then the upper and lower bounds of the parameters are determined by the fixed point theorem, finally Pivoting algorithm and sequence of quadratic programming method were used to solve those models. The example indicated that it is necessary to balance the investment and return values of the alliance members to maximize the utility of the alliance and achieve real incentive for them by determining the appropriate reserved utility value.																	1064-1246	1875-8967					2020	38	2			SI		1499	1508		10.3233/JIFS-179513													
J								Predictive analysis of impact hazard level of coal rock mass based on fuzzy inference network	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Coal rock mass impact; fuzzy; neural network		In view of identification of strong mine quake and rock burst, the mine geology, structural mechanics, mining production data and acoustic emission monitoring process data are infused to build two fuzzy process neural network models based on fuzzy set theory. The model integrates fuzzy logic inference mechanism with process neural network process signal analysis and learning capacity. It presents domain knowledge based on fuzzy set and membership function, and adaptively establishes computational logic and fuzzy decision rules based on process signal distribution features, which can effectively infuse multi-source information and prior knowledge, demonstrating good ability to comprehensively analyze various quantitative and qualitative mixed information and identify microquake features, as well as small sample modeling capacity. It has good adaptability for predictive analysis of strong mine quake and rock burst with uncertainty.																	1064-1246	1875-8967					2020	38	2			SI		1509	1518		10.3233/JIFS-179514													
J								A forward collision warning system based on self-learning algorithm of driver characteristics	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Car-following braking; forward collision warning system; self-learning; signal detection theory	SIGNAL-DETECTION-THEORY; BENEFITS; VEHICLE; MODEL; IMAGE	The frequent false alarms in Forward Collision Warning systems not only disturb the normal operation of drivers, but also reduce the user acceptance of the warning systems. However, drivers with disparate driving characteristics possess different safety cognition of car-following braking behavior; systems with stationary warning thresholds inevitably lead to higher false positive and false negative rates for aggressive and conservative drivers, respectively. In this study, we proposed an adaptive algorithm that learns the characteristics of individual drivers during car-following braking processes, and determined the optimal threshold online to adapt to different drivers. Signal detection theory was employed and the results of the accuracy, false negative rate, and false positive rate were used to capture drivers' characteristics of car-following braking behavior. The optimal warning thresholds were continuously selected online during the learning stage based on changes in the drivers' characteristics. The developed algorithm by conducting actual vehicle tests with two participants were evaluated. The offline statistical analysis results of the participants' car-following braking characteristics were compared with the online results of the warning threshold adjustments from the adaptive algorithm. The comparison results indicated that the adaptive algorithm could effectively capture the drivers' car-following braking characteristics and determine an appropriate warning threshold.																	1064-1246	1875-8967					2020	38	2			SI		1519	1530		10.3233/JIFS-179515													
J								Risk recognition and risk classification diagnosis of bank outlets based on information entropy and BP neural network	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Information Entropy; BP neural network; risk classification		In view of the current demand for risk identification and classification prevention of bank outlets caused by the difficulty in identifying operational efficiency and wind control capability, a risk data measurement and warning classification model based on information entropy and BP neural network is proposed. The model establishes two-level risk data measurement elements from three dimensions. Based on the data set itself, the information entropy is used to determine the weights of the two-level risk elements, and then calculates the risk quantities recorded under the first-level risk measurement elements in the data set. The BP neural network is used to output the risk data classification results without presupposing the weights of the measurement. The proposed model obtains smaller reductions and higher classification accuracies with relatively low computational cost. Experiments show that the model can measure and classify risk data with very low mis-judgment rate and small mis-judgment bias.																	1064-1246	1875-8967					2020	38	2			SI		1531	1538		10.3233/JIFS-179516													
J								Distributed management of permission for access control model	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Network security; access control; distributed model		Access control is an important mechanism to protect sensitive information and relational system resources. The traditional access control model (TACM), such as DAC, MAC, RBAC, etc., is no longer suitable for open network due to the lack of dynamic permission management. The increasing network nodes make the information storage and resource access becoming distributed. The traditional access control model has the characteristics of low adaptive ability and single deployment and application mode due to the centralized management mode. Therefore, this access control environment inevitably puts access control pressure on access control authorization. In order to overcome the shortcomings of traditional access control model, a new access control model named DMPAC (Distributed management of permission for access control model) is proposed in the paper. The authorization mechanism of the model has a distributed and dynamic management access permission, and all nodes covered by the model have the opportunity to participate in the execution of access and control. The model DMPAC provides the benefits of traditional access control models in terms of secure access and dynamic management. We also describe the framework and execution process of the model and the application of DMPAC in access control. At last, we will present some experimental results to show that while maintaining the effectiveness of distributed access control through the management of access permissions, DMPAC can achieve the performance of traditional access control models.																	1064-1246	1875-8967					2020	38	2			SI		1539	1548		10.3233/JIFS-179517													
J								Research on machine learning method and its application technology in intrusion information security detection	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Information security; machine learning method; intrusion detection technology; Elman neural network; robust SVM	INTERNET	With the increasingly serious network security situation, intrusion detection technology has become an important means to ensure network security. Therefore, it has become a consensus to introduce the theory and method of machine learning into intrusion detection, and has made good progress in this research field in recent years. In this paper, a machine learning intrusion detection system is proposed. The system uses the intrusion detection of Elman neural network and the intrusion detection of robust SVM neighbour classification to solve the above problems. Elman neural network intrusion detection uses clustering algorithm to cluster the text of the network packet, which overcomes the defect of missing the text information of the network packet. At the same time, the ability to detect abnormal behaviour between network packet sequences is improved. At the same time, robust SVM neighbour classification intrusion detection can achieve the feature space weighting of the optimal classification face host system log, eliminate the negative impact of noise data, reduce the false alarm rate of intrusion detection, and improve the detection accuracy. Under the requirement of false alarm rate of 0, the intrusion detection based on robust SVM neighbour classification can achieve 87.3% detection rate; when the false alarm rate is 2.8%, the detection rate is 100%.																	1064-1246	1875-8967					2020	38	2			SI		1549	1558		10.3233/JIFS-179518													
J								Name disambiguation using meta clusters and clustering ensemble	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Name disambiguation; meta clusters; clustering ensemble; graph partition		The name disambiguation task is designed to solve the name ambiguity problem of documents of multiple persons who have the same name with one another. The task aims to partition all the publications belonging to multiple person with the same name and realize that each decomposed partition is composed of publications of a unique person. Many works on name disambiguation task have a common feature that clustering method is usually used in the last step. The paper presents a complementary study to these works from another point of view. Based on the idea that documents with strong association relationships are likely to belong to the same author, this paper proposes a method of discovering meta clusters by graph partition with a heuristic rule to improve these clustering-based works. Specially, different from these works, this work uses clustering ensemble method instead of clustering method in the last step. Experimental results on areal-life dataset show that the improved method has satisfactory performance compared with the clustering-based baseline method.																	1064-1246	1875-8967					2020	38	2			SI		1559	1568		10.3233/JIFS-179519													
J								Dimensionality reduction of image feature based on geometric parameter adaptive LLE algorithm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Image feature; dimensionality reduction; LLE; parameter adaptive	EIGENFACES; MANIFOLD	Locally linear embedding (LLE) is a classical nonlinear dimensionality reduction algorithm, and it has been widely used in image feature selection. LLE reduces the dimensions of a data set only by exploring the geometric structure, which is calculated by Euclidean distance and makes the embedding result be sensitive to noise. Moreover, the choice of the number of nearest neighbors is fixed for all data points and only given by human experience. In order to overcome these problems, a geometric parameter adaptive LLE (PALLE) algorith(m) is proposed in this paper. This algorithm jointly uses Geodesic distance and Cosine similarity to replace Euclidean distance, and then the number of neighbors is adaptable selected by weak-sigma rule. Extensive experimental results over various real-life data sets have demonstrated the superiority of the proposed algorithm in terms of image feature dimensionality reduction compared with classical LLE and other well-known algorithms.																	1064-1246	1875-8967					2020	38	2			SI		1569	1577		10.3233/JIFS-179520													
J								Fuzzy comprehensive evaluation of purchase intention of retailer private brands based on improved AHP method	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy comprehensive evaluation; intention of retailer private brands; improved AHP method		This paper uses the improved AHP comprehensive evaluation method to evaluate the purchase intention of Chinese retailer private brands. This method can reduce the subjectivity of the weight selection and improve the accuracy of weights. The paper also uses the degree of membership to grade the evaluation levels and conducts the backward evaluation to the different levels of indicators one by one, thus obtaining the relative development level of each indicator. The evaluation results show that consumers have higher acceptance of retailer private brands and they have strong purchase intentions.																	1064-1246	1875-8967					2020	38	2			SI		1579	1584		10.3233/JIFS-179521													
J								Research on safety management system optimization of B2C e-commerce intelligent logistics information system based on data cube	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Data Cube; B2C E-Commerce; Intelligent Logistics	USAGE	With the rapid popularization and rapid development of the Internet in the world, e-commerce has gradually become the mainstream trade mode. E-commerce has its own unique trading mode and brand-new global business opportunities. More and more people conduct online transactions through the Internet. According to the characteristics of data in B2C e-commerce system, data mining management module is designed in B2C e-commerce management system. Data mining technology is used to preprocess data, data mining and mining results. It is implemented using J2EE's B/S architecture. With the continuous growth of B2C e-commerce scale, logistics bottlenecks have become increasingly prominent, and e-commerce distribution model based on cloud logistics integrates IT information technology with traditional logistics information systems. Integrate logistics service demand and logistics distribution capabilities, and provide corresponding cloud logistics information and management platform system. It will help solve the problem of logistics and distribution of B2C e-commerce in China and promote the healthy and rapid development of e-commerce economy.																	1064-1246	1875-8967					2020	38	2			SI		1585	1592		10.3233/JIFS-179522													
J								Evaluation of automatic algorithm for solving differential equations of plane problems based on BP neural network algorithm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										BP neural network algorithm; Plane problem; Difference equation	RESPONSE-SURFACE; OPTIMIZATION; SYSTEM; YIELD	Plane problem is a typical combinatorial optimization problem Aiming at the difference method of plane problem, BP neural network is proposed, the algorithm of solving difference equation is established, and the corresponding program is compiled. By calculating the calculation example, the continuity condition under the condition of modulus abruption is further discussed. The correctness and practicability of the difference equation algorithm are verified. A dynamic model of the parallel difference equation is constructed according to the characteristics of the parallel structure of BP neural network. The study shows that the two groups of differential equations are used to identify and verify the model, and the energy function that satisfies both the linear embedding condition and the correct wiring is given. Furthermore, BP neural network is used to realize the search and routing of the maximum plane subgraph of the planable line and the non-planar plan. The results show that the verification used is effective. Difference equation calculations have the ability to help BP networks get rid of local minima and get better results.																	1064-1246	1875-8967					2020	38	2			SI		1593	1602		10.3233/JIFS-179523													
J								Driver's intention recognition algorithm based on recessive Markoff model	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Hidden Markov model; driving behavior; driving intention; intention recognition		In order to effectively recognize the driving intention triggering on the vehicle safety assistant system, we studied the relationship between the driving intention and driving behavior. Based on hidden Markov model (HMM) hierarchical and sequential characteristics, we established a double layer HMM model, the behavior layer and the intention layer, respectively, used for recognizing driving behavior and driving intention. After using the experimental data processed, we carried out off-line training for each identification model of the double layer HMM model. Moreover, through comparing the offline validation results of different model parameters, we selected the appropriate parameters for each model, which can achieve the best recognition results. The research results show that the intention layer can identify long and complex driving intentions in real time. It is concluded that this research has successfully built the online recognition model of driver's intention.																	1064-1246	1875-8967					2020	38	2			SI		1603	1614		10.3233/JIFS-179524													
J								( )Some fuzzy anti lambda-ideal convergent double sequence spaces	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Ideal; filter; I-convergence; fuzzy antinormed space	STATISTICAL CONVERGENCE	The concept of fuzzy sets was introduced by Zadeh as a means of representing data that was not precise but rather fuzzy. Recently, Kocinac [12] studied some topological properties of fuzzy antinormed linear spaces. This has motivated us to introduce and study the fuzzy antinormed double sequence spaces with respect to ideal by using a compact linear operator and prove some theorems, in particular convergence and completeness theorems on these new double sequence spaces.																	1064-1246	1875-8967					2020	38	2			SI		1617	1622		10.3233/JIFS-182575													
J								A decision-making approach based on multi Q-dual hesitant fuzzy soft rough model	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Q-dual hesitant fuzzy soft set; M(k)QDHFS rough set; UAS failure detection approach; decision-making method	SET	In this paper, we present a novel hybrid model called multi Q-dual hesitant fuzzy soft (M(k)QDHFS) rough set model, by the combination of rough sets and M(k)QDHFS sets. We investigate some fundamental properties of M(k)QDHFS rough set model. Further, we develop M(k)QDHFS rough approximation operators with the help of M(k)QDHFS relations. We discuss some useful properties of lower and upper M(k)QDHFS rough approximation operators. Furthermore, we present a novel framework for handling vagueness in decision-making problems based on M(k)QDHFS rough sets. Finally, we give an application to analyze the failure detection in unmanned aircraft systems (UAS) to show the validity of the proposed decision-making method.																	1064-1246	1875-8967					2020	38	2			SI		1623	1635		10.3233/JIFS-182624													
J								Uncertainty measure of Z-soft covering rough models based on a knowledge granulation	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Z-soft rough covering sets; uncertainty; knowledge granulation; axiomation; decision making	DECISION-MAKING; SET MODELS; ENTROPY; (I	Z-soft rough covering models introduced by zhan et al are important generalizations of classical rough set theory to deal with more complex problems of real world. So far, the existing studies mainly focus on constructing various forms of approximation operators and their related properties by means of neighborhoods. In this paper, we introduce different kinds of uncertainty measures related to Z-soft rough covering sets and discuss their limitations. An axiomatic definition of knowledge granulation for soft covering approximations space is introduced. Some main theoretical results are obtained and investigated with the help of examples. Finally, a fully developed example describing the application of the proposed theory in multicriteria decision making is constructed.																	1064-1246	1875-8967					2020	38	2			SI		1637	1647		10.3233/JIFS-182708													
J								CODAS method using Z-fuzzy numbers	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										CODAS; Z-fuzzy; restriction function; reliability; pairwise comparison; supplier selection; SAW	MULTICRITERIA DECISION-MAKING; RENEWABLE ENERGY; SELECTION	CODAS is a new multicriteria decision making method based on the maximum distances to negative ideal solutions, which are obtained from Euclidean and Taxicab distances. This paper develops a Z-fuzzy CODAS method based on restriction and reliability functions under uncertainty. We obtain the criteria weights from the Z-fuzzy pairwise comparison matrix. An illustrative application to supplier selection problem is also given. A comparative analysis is presented with ordinary fuzzy simple additive weighting (SAW) method.																	1064-1246	1875-8967					2020	38	2			SI		1649	1662		10.3233/JIFS-182733													
J								Scaling density-based community detection to large-scale social networks via MapReduce framework	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Community detection; overlapping community; connected components; mapreduce; large-scale networks		Community detection from networks is one of the long standing and challenging tasks in the field of complex network research. Detection of communities poses numerous challenges in terms of their overlapping and hierarchical nature, dynamics of networks and underlying communities, scalability of detection algorithms on large scale networks to mention a few. Traditional community detection methods are not readily scalable to large networks mainly due to the computation of global network metrics. This paper presents a novel scalable overlapping community detection approach for large scale networks by presenting a MapReduce framework based implementation of a density-based local community detection method. The method is divided in two stages where the first stage uses a MapReduce approach to identify a mutual-core connected subgraph of the underlying network. The second stage uses an existing connected component detection method, implemented via MapReduce, to identify connected components in the mutual-core connected subgraph generated in the first stage. A community is then taken as the union of the core-nodes in a connected component and the respective density-based neighborhood of each core-node in the connected component. The resulting approach is among the first scalable overlapping community detection methods proposed in literature.																	1064-1246	1875-8967					2020	38	2			SI		1663	1674		10.3233/JIFS-182765													
J								Detection and prevention of spoofing attacks in mobile adhoc networks using hybrid optimization algorithm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Mobile ad hoc network (MANETs); optimal intrusion detection system (OIDS); hybrid optimization algorithm (HOA); improved whale optimization (IWO); intrusion detection system (IDS); spoofing attack; trust degree	AD-HOC NETWORKS; INTRUSION-DETECTION; PROTOCOL; SCHEME; SECURE; SYSTEM	The nature of mobile nodes are affects the performance of MANET. The main requirement of secured MANET is use of secure protocols which ensure the network integrity, confidentiality, availability and authenticity. Recently, the secure protocols have been used to resists such attacks in MANET. Spoofing attack is one of them, which affects network by an attacker program successfully acts on another node behalf by impersonating data. In this paper, we investigate the spoofing attack problems in MANET and propose an optimal intrusion detection system (OIDS) using hybrid optimization algorithm (HOA) named as OIDS-HOA. Here, we propose an improved whale optimization (IWO) algorithm for efficient cluster formation and a cluster chain weight metrics approach (CCWW) for trust degree computation of each mobile node. Moreover, the intrusion detection system (IDS) is done by the condition check algorithm and the intrusion response action (IRA) is performed by average route trust with three different responses are: No Punishment (NP), Route Change (RC) and Route Isolate (RI). The simulation results show that the effectiveness of proposed OIDS-HOA scheme in terms of packet delivery fraction, end-end-delay, energy consumption, overhead and packet drop ratio.																	1064-1246	1875-8967					2020	38	2			SI		1691	1704		10.3233/JIFS-182881													
J								Generalized interval-valued trapezoidal fuzzy best-worst multiple criteria decision-making method with applications	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Generalized interval-valued trapezoidal fuzzy best worst method; Generalized interval-valued trapezoidal fuzzy numbers; Generalized interval-valued trapezoidal fuzzy reference comparison; Consistency ratio; Multiple criteria decision-making	POWER; SELECTION; SUPPLIER; RANKING; NUMBERS	Best-worst method (BWM) extended to uncertain situations, generalized interval-valued trapezoidal fuzzy best-worst method (GITrFBWM) is proposed by using generalized interval-valued trapezoidal fuzzy multiplicative preference relation for multiple criteria group decision-making problems. The reference comparison of the best criterion and the worst criterion are described by the linguistic terms, which are expressed in generalized interval-valued trapezoidal fuzzy numbers, of the decision-makers. Weights of criteria are calculated by using graded mean integration representation method. Using the concept of BWM, nonlinearly constrained optimization problems are formed to obtain generalized interval-valued trapezoidal fuzzy weights of different criteria and alternatives. To check the reliability of the GITrFBWM, consistency ratio is proposed. The advantage and suitability of the proposed GITrFBWM are determined by three case studies. The results indicate that the GITrFBWM, due to higher comparison consistency as compared to BWM and fuzzy-best worst method, obtain plausible preference ranking for alternatives.																	1064-1246	1875-8967					2020	38	2			SI		1705	1719		10.3233/JIFS-182932													
J								On the existence of Nash equilibria in fuzzy generalized discontinuous games with infinite players	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy generalized discontinuous game; Nash equilibrium; maximal element; transfer continuity	PAYOFFS; PURE	This paper introduces new concepts of fuzzy generalized quasi-weak transfer continuity and fuzzy generalized pseudo quasi-weak transfer continuity for fuzzy generalized discontinuous games with infinite players. Furthermore, by using a collectively maximal element theorem, we obtain some new existence results of Nash equilibria in fuzzy generalized discontinuous games with infinite players. Finally, as applications, some existence results of Nash equilibria in generalized discontinuous games are given in crisp senses. The results presented in this paper improve and generalize some known results in the literature.																	1064-1246	1875-8967					2020	38	2			SI		1721	1736		10.3233/JIFS-182954													
J								The induced generalized interval-valued intuitionistic fuzzy Einstein hybrid geometric aggregation operator and their application to group decision-making	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Group decision-making; I-GIVIFEOWG aggregation operator; I-GIVIFEHG aggregation operator; some einstein operations	INFORMATION	For the multi-attribute group decision-making problems where attribute values are the interval-valued intuitionistic fuzzy numbers, the group decision-making method based on induced generalized Einstein geometric aggregation operators is developed. Firstly, induced generalized interval-valued intuitionistic fuzzy Einstein ordered weighted geometric (I-GIVIFEOWG) aggregation operator and induced generalized interval-valued intuitionistic fuzzy Einstein hybrid weighted geometric (I-GIVIFEHWG) aggregation operator, were proposed. Some general properties such as, idempotency, commutativity, monotonicity and boundedness, were discussed and some special cases were analyzed. Furthermore, the method for multi-attribute group decision-making problems was developed, and the operational processes were illustrated in detail. The main advantage of using the proposed methods and operators is that these operators and methods give a more complete view of the problem to the decision makers. The proposed methods provide more general, more accurate and precise results. Therefore these methods play a vital role in real world problems. Finally the proposed operators have been applied to decision making problems to show the validity, practicality and effectiveness of the new approach.																	1064-1246	1875-8967					2020	38	2			SI		1737	1752		10.3233/JIFS-182955													
J								The supreme-core on multicriteria fuzzy games	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Multicriteria fuzzy TU games; the core; supreme-utilities; inequality constraint; domination	STABLE SETS; AXIOMATIZATIONS; CONSISTENCY; POWER	By considering the supreme-utilities among fuzzy action vectors, we adopt several sensible inequality constraints to propose a different generalization of the core on multicriteria fuzzy transferable-utility (TU) games. We also adopt the duality results of linear programming theory to analyze the non-emptiness for this extended core under inequality constraints. Further, we define an extended reduction to characterize this extended core. Based on the notion of domination among fuzzy payoff vectors, some coincidences are investigated.																	1064-1246	1875-8967					2020	38	2			SI		1753	1760		10.3233/JIFS-18811													
J								An efficient approach to state space management in model checking of complex software systems using machine learning techniques	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS											GRAPH TRANSFORMATION; HEURISTIC SOLUTION; N-GRAMS; VERIFICATION; SYMMETRY	Since complexity of computer systems is growing increasingly, assuring flawless operation of these systems has become more difficult. Therefore, it is important that these systems whether software or hardware are executed as expected. Consequently, verifying system before implementation at model level is necessary. Model checking is a formal technique for validating the system automatically which decides whether the finite state system satisfies temporal property by scanning the whole state space or not. One of the most important problems in model checking is state space explosion of models which results in memory shortage in generation of all states. Therefore, this paper presents a method which employs machine learning techniques without exploring the whole state space to predict temporal properties of trajectories in systems based on graph transmission system. the proposed method is implemented in Groove; results indicate desirable accuracy and speed of this method compared to other methods.																	1064-1246	1875-8967					2020	38	2			SI		1761	1773		10.3233/JIFS-190023													
J								A prospect theory-based QUALIFLEX for uncertain linguistic Z-number multi-criteria decision-making with unknown weight information	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Z-numbers; linguistic Z-numbers; multi-criteria decision-making; QUALIFLEX; prospect theory	SPHERICAL FUZZY-SETS; AGGREGATION OPERATORS; MODEL; REPRESENTATION; STRATEGIES	Uncertain linguistic Z-numbers (ULZNs), which inherit the prominent characteristics of linguistic term sets and Znumbers, can flexibly describe qualitative information as well as its reliability. To cautiously solve a qualitative multi-criteria decision-making (MCDM) problem with larger number of criteria than alternatives, this paper develops an ULZN-based QUALIFLEX (QUALItative FLEXible multiple criteria method) by considering the decision-maker (DM)'s psychological behavior character. First, the likelihood and diversity degree of ULZNs are determined and a comparison method is proposed. Second, a decision model combining the QUALIFLEX and prospect theory is developed to address MCDM problems with ULZNs, considering the incomplete compensation of criteria. An extended maximizing deviation method is developed to objectively obtain the weights of the criteria. Subsequently, an illustrative example concerning risk evaluation of high-tech project investment with larger number of criteria than alternatives is provided to demonstrate the application of the proposed approach. Finally, sensitivity analysis and comparative analyses are conducted to validate the proposed approach. The result shows that the proposed approach can effectively address MCDM problems with ULZNs, considering the DM's psychological behavior.																	1064-1246	1875-8967					2020	38	2			SI		1775	1787		10.3233/JIFS-190065													
J								Fuzzy soft set theory with applications in hyper BCK-algebras	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										fuzzy soft hyper BCK-ideal; fuzzy soft weak hyper BCK-ideal; fuzzy soft s-weak hyper BCK-ideal; fuzzy soft strong hyper BCK-ideal	SIMILARITY; OPERATIONS; MATRICES	In the paper [P.K. Maji, R. Biswas, A.R. Roy, Fuzzy soft sets, J. Fuzzy Math. 9 (3) (2001) 589-602], Maji et al. introduced the concept of fuzzy soft sets as a generalization of the standard soft sets, and presented an application of fuzzy soft sets in a decision making problem. The aim of this paper is to apply fuzzy soft set for dealing with several kinds of theories in hyper BCK-algebras. The notions of fuzzy soft hyper BCK-ideal, fuzzy soft weak hyper BCK-ideal, fuzzy soft s-weak hyper BCK-ideal and fuzzy soft strong hyper BCK-ideal are introduced, and related properties and relations are investigated.																	1064-1246	1875-8967					2020	38	2			SI		1789	1797		10.3233/JIFS-190103													
J								Observer based adaptive interval type-2 fuzzy sliding mode control for unknown nonlinear systems	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Adaptive control; interval type-2 fuzzy logic system; observer; sliding mode control; unknown nonlinear systems	LOGIC SYSTEMS	In this paper, a novel observer based direct adaptive interval type-2 (IT2) fuzzy sliding mode control (SMC) method is proposed for a certain class of high order unknown nonlinear dynamical systems with unmeasured states. Firstly, a high-gain observer is designed to estimate the tracking errors of the system states. Then on the basis of the observer, an IT2 fuzzy logic system (FLS) is established to approximate the equivalent control law, in which the estimate of the sliding surface function is applied as the input. The sliding mode control law is developed to guarantee the robustness of the system, in which an adaptive switching control gain is designed to handle the chattering problem. The free parameters of the controller are adjusted online by the adaptive laws. Finally, the stability of the overall closed-loop system is proved in the Lyapunov sense based on Meyer-Kalman-Yakubovich (MKY) lemma. In the proposed control system, the constraints on the knowledge of mathematical model, disturbances, and state vector can be avoid completely, and the trajectory tracking control can be achieved only using the information of system output. Simulation results demonstrate the effectiveness and the high control performance of the proposed controller.																	1064-1246	1875-8967					2020	38	2			SI		1799	1810		10.3233/JIFS-190132													
J								Intelligence - based decision support system for diagnosing the incidence of hypertensive type	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Medical decision support system; medical diagnosis; pre-processing; feature selection; machine learning; multiclass classifier; hypertension	KINEMATIC FEATURE-EXTRACTION; FEATURE-SELECTION; CLASSIFICATION; ALGORITHMS; SOLVE	Hypertension is a major non-communicable disease, a silent killer that serves as a root cause for many entangled maladies. Early analysis and detection will play vital roles in reducing the prevalence of hypertension and its associated risk factors. As medicine moves forward, there is a need for sophisticated decision support systems to make real-time predictions. Since most medical applications need to deal with multi-class problems, high diagnostic prediction accuracy is extremely important. The quality of data also significantly affects the learning model's performance. These issues induce the need for proper exploration and investigation of the multi-class medical dataset. This research intends to present an intelligent learning model that can explore medical data and offer decision support for domain experts and individuals. As clinical data tend to be, grimy appropriate pre-processing techniques are essential to ensure high data quality. This paper deals with the poor-quality data using computational statistical techniques. The prominent features are obtained by employing diverse feature selection techniques and provide a competitive report. We evolved a supervised learning model that can handle multi-class issues in diagnosing medical data categories. This model will learn from the data samples by using a multi-class support vector machine technique to generate precise predictions. We evaluated our learning model by using a real-time hypertension dataset obtained from primary health centres. The proposed approach improves predictive accuracy, precision and recall for handling the multi-class dataset above that of existing techniques. The outcome positively reveals that the proposed intelligent model is effective in undertaking medical decision-making task.																	1064-1246	1875-8967					2020	38	2			SI		1811	1825		10.3233/JIFS-190143													
J								Isolating botnet attacks using Bootstrap Aggregating Surflex-PSIM Classifier in IoT	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Random Poisson forest; Linear Random Euler complex-valued filter; Psim-protein similarity	FRAMEWORK; DDOS	In an Internet of Things (IoT) environment, any object, which is equipped with sensor node and other electronic devices can involve in the communication over wireless network. Hence, this environment is highly vulnerable to botnet attack. Nevertheless, the challenge prevailed in detection of botnet attack due to its unique structurally repetitive nature, performing dissimilar activities that are non-linear, and an invisible nature by deleting the history. Even though existing mechanisms have taken action against the botnet attack proactively, it failed to capture the frequent abnormal activities of botnet attackers due to frequent monitoring. Moreover, when the number of devices in the IoT environment has increased, existing mechanisms has missed more number of botnets due to functional complexity. Therefore, to overwhelm the issues in detecting the botnet attack, our work has proposed a Bootstrap Aggregating Surflex-PSIM Classifier. It gathers data from several sensor nodes, which are then preprocessed using Linear Random Euler complex-valued Filter (LRECF). Accordingly, the linearized data is subjected to the training phase comprising of Random Poison Forest (RPF) to predict accurately the botnet creating Distributed Denial of Service (DDoS) and Spam attacks within less time. After being trained, similar botnets are clustered using surflex-PSIM that isolates the botnet attacked clusters based on automatic trained characteristics pocket value. Thus, with the aid our proposed classifier, botnet is detected and isolated with high accuracy at reduced time, thereby ensures system reliability with enhanced system performance.																	1064-1246	1875-8967					2020	38	2			SI		1827	1840		10.3233/JIFS-190183													
J								Iterative self-correction for secured images using turbo codes and soft input decryption	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Iterative process; learning iterations; landmark authentication; image authentication; message authentication codes; region of interests; turbo codes; trellis diagram	AUTHENTICATION; WATERMARKING	An algorithm for improved error correction of the data protected using standard security mechanisms, like Message Authentication Codes, is introduced in this paper. Images are taken as example data, and the correction of images using the additionally available authentication data is investigated. Parts of the protected image, the so called "landmarks", are considered as the Region of Interest and protected by their respective authentication tags. An iterative process is used as a basis for the learning ability of the algorithm: in every iteration, parts of the decoder's Trellis path are learned by the algorithm, thereby improving the channel decoding results. In this way, the knowledge gained in the previous iterations of the algorithm is used in the current iteration for further improvement of decoding results. Additionally, iterative processes used for error corrections are supported by authentication tags, which are used as a measure of correction success. Simulation results for images are presented to show the effectiveness of the proposed algorithm. A security analysis of the proposed algorithm is also given in the paper.																	1064-1246	1875-8967					2020	38	2			SI		1841	1854		10.3233/JIFS-190234													
J								A special fuzzy star-shaped numbers space with endograph metric	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy star-shaped numbers; fuzzy numbers; the endograph metric; the Hilbert cube; the pseudoboundary of the Hilbert cube	ALONG-RAYS PROPERTY; COMPACT-SETS; CONVEXITY; MAPS	In this paper, for a non-degenerate convex set Y in R-n containing 0, two special function spaces S-0(Y) and E-0(Y) which consist of all fuzzy star-shaped numbers and of all fuzzy numbers in R-n with respect to 0 and their supports being included in Y with the endograph metric D are investigated. Some conclusions and methods in topology are used to discuss the topological structure of (S-0(Y), D) and the pair ((S-0(Y), D), (E-0(Y), D)). The main results are as follows: 1. The space (S-0(Y), D) is homeomorphic to the Hilbert cube Q = [-1, 1](N) if and only if S-0(Y) is compact if and only if Y is compact. 2. There exists a homeomorphism h : (S-0(Y), D) -> Q such that h(E-0(Y)) = {1} x [-1, 1](N\{1}) if Y is compact but not a segment. 3. The space (S-0(Y), D) homeomorphic to the pseudoboundary of the Hilbert cube if and only if S-0(Y) is non-compact and sigma-compact if and only if Y is non-compact and locally compact.																	1064-1246	1875-8967					2020	38	2			SI		1855	1864		10.3233/JIFS-190272													
J								A model for evaluating the production system of an intelligent mine based on unascertained measurement theory	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Unascertained measurement; intelligent production system; analytic hierarchy process; evaluation model; credible degree recognition	COAL-MINE; DECISION; TECHNOLOGY; INTERNET	Nowadays, the pattern of global industrial competition is undergoing significant adjustment. As the basic energy industry in the development of national economy, the traditional coal industry is gradually becoming intellectualized to seek for a safe and efficient production mode. According to review of the current researches about the intelligent mine, most of them focused on the intellectualization of particular businesses, while the overall development of intelligent mines was rarely mentioned, and there were very few researches on the evaluation criteria of intelligent mines. Therefore, to evaluate the production system of an intelligent mine both reasonably and effectively, an evaluation index system was established by considering "eight systems" involved in the production setup of the intelligent mine. In addition, the weights for all the evaluation indexes were determined using an analytic hierarchy process (AHP). An evaluation model was built using unascertained measurement theory, and the unascertained measurements of each index were calculated. Finally, credible degree recognition criteria were used to determine the intelligence levels. The intelligent production system was evaluated using the 11504W working face in the Zhaizhen Coal Mine in Taian City, Shandong Province, China as an example of a practical application. The judgment results show that the coal mine is relatively intelligent, thus the feasibility of the evaluation model was verified, which provides a scientific basis for evaluating intelligent mines.																	1064-1246	1875-8967					2020	38	2			SI		1865	1875		10.3233/JIFS-190329													
J								Robust control design of nonlinear roll-to-roll dynamic system in printed electronics technology	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										R2R manufacturing; printed electronics; integrator-backstepping control; strict-feedback system	TENSION CONTROL; WEB TENSION	A systematic approach is presented for nonlinear control design of roll-to-roll (R2R) dynamic system for printed electronics manufacturing technology. Control design is firstly proposed for a general multi-output multi-input (MIMO) strict-feedback class of nonlinear dynamic systems. The approach starts with formulation of integrator-backstepping control (IBSC) law for the system by introducing a modified tracking error, which is structured by adding an integrator of normal tracking error in state coordinate transformation. Considering each subsystem of control design process via a chosen Control Lyapunov function (CLF) and parameter assignment results in an IBSC law. Then, an IBSC law-based control strategy is provided for the design model. Web velocity and tension control of nonlinear R2R web dynamics is applied by using the the proposed method. Dynamic analysis is made to transform a general nonlinear R2R web dynamics into a standard MIMO strict-feedback form for control analysis and design. With the achieved model, an IBSC-based control algorithm is provided for the study of numerical simulation and experiment. Control software is developed using the Labview FPGA on PXI platform for implementing studies of numerical simulation and experiment to validate the reliability and feasibility of the proposed method.																	1064-1246	1875-8967					2020	38	2			SI		1877	1888		10.3233/JIFS-190368													
J								Generalized convexity of n-dimensional fuzzy number-valued functions and its application in fuzzy optimization	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Invex fuzzy number-valued functions; n-dimensional fuzzy number; fuzzy optimization; semicontinuity; Gordan's Theorem	QUOTIENT SPACE; MAPPINGS	In this paper, we mainly introduced the invexity and generalized invexity of n-dimensional fuzzy number-valued functions based on the new ordering which defined by Gong and Hai in [9]. Simultaneously, we discussed the relationship between semicontinuous and preinvex fuzzy number-valued functions, and some properties among invexity and generalized invexity of n-dimensional fuzzy number-valued functions. Finally, we studied the necessary and sufficient conditions for weakly efficient point of fuzzy optimization.																	1064-1246	1875-8967					2020	38	2			SI		1889	1900		10.3233/JIFS-190370													
J								The ordered weighted average inflation	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Inflation; induced aggregation operators; economics; Mexico	AGGREGATION OPERATORS; ECONOMY; KNOWLEDGE; GROWTH; MODELS	This paper introduces the ordered weighted average inflation (OWAI). The OWAI operator aggregates the information of a set of inflations and provides a range of scenarios from the minimum and the maximum inflation. The advantage of this approach is that it can provide a flexible inflation formula that can be adapted to the specific characteristics of the enterprise, region, state or country. Therefore, the novelty of this operator is that experts can forecast the information and provide optimistic or pessimistic results of the expected average inflation according to the knowledge, aptitude or expectations for the whole country or an event that represents a specific sector, market or industry. The paper develops several extensions by using the induced, heavy and prioritized aggregation operators. The work studies the applicability of the operator to the analysis of Mexican inflation by developing some aggregation systems that consider the average inflation of Mexico.																	1064-1246	1875-8967					2020	38	2			SI		1901	1913		10.3233/JIFS-190442													
J								Developing a computationally effective Interval Type-2 TSK Fuzzy Logic Controller	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Interval type-2 fuzzy logic controller; TSK fuzzy logic controller; uncertainty bounds output processing	UNCERTAINTY BOUNDS; SYSTEMS; DESIGN; SETS; IDENTIFICATION	Type-2 fuzzy logic controllers are capable of handling different types of uncertainties that naturally exist in most practical situations. However, the high computation cost of type-2 fuzzy logic controllers is a bottleneck for practically applying them to real-world applications. This paper introduces a novel approach for designing a computationally effective type-2 fuzzy logic controller. For this purpose, on the antecedent side, interval type-2 fuzzy sets are employed to capture the signal readings, which significantly reduce the computation costs while preserving the major advantages of general type-2 fuzzy logic systems. On the consequent side, however, the Takagi-Sugeno-Kang (TSK) technique is integrated with the proposed controller to render the control outputs in a parallel way. To further reduce the computation cost, the theory of uncertainty bounds is employed for the output processing of the proposed controller. To develop this control structure, a decomposition technique is integrated to break down the original type-2 fuzzy processes into type-1 and take advantage of type-1 fuzzy techniques, followed by an aggregation mechanism to calculate the collective output. The approach is applied to the control of an inverted pendulum and cart model. The simulation results of the developed interval type-2 fuzzy logic controller is compared with a type-1 TSK fuzzy logic controller and a classical proportional derivative (PD) controller. From the results, we have found a 16.6% and 23.3% improvement in Root Mean Square (RMS) error compared to type-1 TSK fuzzy logic controller and classical PD controller, respectively.																	1064-1246	1875-8967					2020	38	2			SI		1915	1928		10.3233/JIFS-190446													
J								Bidirectional IndRNN malicious webpages detection algorithm based on convolutional neural network and attention mechanism	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Malicious webpages; convolutional neural network; attention mechanism; bidirectional independently recurrent neural network	CLASSIFICATION	A convolutional neural network combined with attention mechanism and a parallel joint algorithm model (CATTB) of bidirectional independent recurrent neural network are proposed. The algorithm extracts the relocation feature and the "texture fingerprint" feature for expressing the similarity of the URL (Uniform Resource Locator) binary file content of the malicious web page, and uses the word vector tool word2vec to train the URL word vector feature and extract the URL static vocabulary feature. CNN (Convolutional Neural Network) is used to extract deep local features. Secondly, Attention mechanism adjusts weight and BiIndRNN (Bidirectional Independently Recurrent Neural Network) to extract global features. Finally, softmax is used for classification. This paper extracts more comprehensive features from different angles and using different methods. The experimental results show that the test results are higher than other researchers, and compared with other algorithms, the proposed CATTB algorithm improves the accuracy of malicious web page detection.																	1064-1246	1875-8967					2020	38	2			SI		1929	1941		10.3233/JIFS-190455													
J								Detection and classification of hard exudates in retinal images	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Diabetic retinopathy (DR); microvasculature; hard exudates; morphological reconstruction; DIARETDB1	AUTOMATED DETECTION; DIABETIC-RETINOPATHY; SEGMENTATION	Diabetic retinopathy (DR) is a chronic disease of the retinal microvasculature which leads to loss of central visual acuity. Early detection of hard exudates in retinal images using computer aided tool helps the ophthalmologist to diagnose the blindness problem. This article presents a novel method to detect and classify the hard exudates in retinal images. For detection, the optic disc (OD) of the retinal image is masked and then the bright patches that contribute to hard exudates are segmented based on thresholding and morphological reconstruction techniques. Here OD is identified using brightness and variance features of the OD followed by Circular Hough Transformation. For classification, features such as color, size, and texture are extracted from each segmented candidate regions based on these features and the regions are classified by using multilayered perceptron neural network (MLP). The proposed method is experimented on the DIARETDB1 retinal dataset and also compared with the existing methods.																	1064-1246	1875-8967					2020	38	2			SI		1943	1949		10.3233/JIFS-190492													
J								An enhanced symbiotic organisms search algorithm with perturbed global crossover operator for global optimization	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Symbiotic organisms search; perturbed global crossover operator; exploration; exploitation	LEARNING-BASED OPTIMIZATION; EVOLUTIONARY; NETWORK	Symbiotic organisms search (SOS) algorithm is a nature-inspired meta-heuristic algorithm, which has been successfully applied to solve a wide range of benchmarks and real-world optimization problems. In this paper, an extended version of SOS, namely symbiotic organisms search with perturbed global crossover operator (PGCSOS), is introduced to enhance the performance of the basic SOS. In parasitism phase, an organism can benefit from other organisms that are better than it, and a perturbed crossover scheme is incorporated into the parasitism phase, which aims at maintaining the trade-off between exploration and exploitation effectively, and preventing the current best solution from getting trapped into local optima. The performance of PGCSOS is assessed by solving global optimization functions with different characteristics and real-world problems. Compared to the SOS, modified SOS and other promising heuristic methods, numerical results reveal that the PGCSOS has better optimization performance.																	1064-1246	1875-8967					2020	38	2			SI		1951	1965		10.3233/JIFS-190546													
J								Design and implementation of intelligent traffic and big data mining system based on internet of things	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Internet of things; intelligent transportation; big data; system; Hadoop; prediction		Intelligent transportation is a new type of transportation system, and it can improve the transport service system through utilizing lots of modern electronic information technologies, such as Internet of Things, cloud computing, artificial intelligence, automatic control, mobile internet.Through the comprehensive analysis and transmission of information, the system helps people properly arrange their travel routes, thus ensuring that people's traffic travel rate can be effectively improved. This paper uses the fusion of data mining technology and distributed parallel Hadoop technology to realize the architecture of the intelligent traffic system, and uses the mining algorithm to realize the analysis of running state,and the Internet of things are used to obtain the data from the perceptive layer. The simulation results show that the proposed system has significant performance on prediction.																	1064-1246	1875-8967					2020	38	2			SI		1967	1975		10.3233/JIFS-190558													
J								A novel approach to censuses process by using Pythagorean m-polar fuzzy Dombi's aggregation operators	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Pythagorean m-polar fuzzy set (PMPFS); Pythagorean m-polar fuzzy Dombi weighted arithmetic average (PMPFDWAA) operator; Pythagorean m-polar fuzzy Dombi weighted geometric average (PMPFDWGA) operator; multicriteria decision-making; censuses process	GROUP DECISION-MAKING; SOFT SETS	This manuscript provides an advanced mathematical model for the censuses process to reduce the drawbacks of the existing methods. From the living ideas of m-polar fuzzy set (MPFS) and Pythagorean fuzzy set (PFS), we establish a novel concept of Pythagorean m-polar fuzzy set (PMPFS). We introduce some fundamental operations on Pythagorean m-polar fuzzy sets and explain these concepts with the help of illustrations. With this novel perspective, we build up modified forms of Dombi's aggregation operators named as Pythagorean m-polar fuzzy Dombi weighted arithmetic average (PMPFDWAA) operator and Pythagorean m-polar fuzzy Dombi weighted geometric average (PMPFDWGA) operator. We discuss certain properties of the proposed operators based on Pythagorean m-polar fuzzy numbers (PMPFNs). Mathematical modeling on real world problems often implicate multi-factors, multi-attributes and multi-polar information. We discuss a case study for the censuses process to elaborate the proposed algorithm for multi-criteria decision-making (MCDM). We also discuss how the drawbacks of existing methods can be handled by applying this novel perspective. Lastly, we present a comparative analysis, validity of proposed algorithm, influence of operational parameter, convergence and sensitivity analysis to indicate the flexibility and advantages of the proposed method.																	1064-1246	1875-8967					2020	38	2			SI		1977	1995		10.3233/JIFS-190613													
J								Double dominating energy of m-polar fuzzy graphs	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Spectrum; dominating energy; double dominating energy; out-dominating energy; double out-dominating energy; m-polar fuzzy preference relation	DECISION-MAKING METHOD; SETS	In this paper, we introduce the concepts of dominating and double dominating energy of m -polar fuzzy graphs. Using the notion of eigenvalues of m-polar fuzzy relations, we study certain formulae, lower and upper bounds of dominating and double-dominating energy. We also propose the idea of out-dominating and double out-dominating energy of m-polar fuzzy digraphs. We present a decision model based on m-polar fuzzy preference relations to solve multi-criteria decision-making problems.																	1064-1246	1875-8967					2020	38	2			SI		1997	2008		10.3233/JIFS-190621													
J								Correlative decision preference information consistency check and comprehensive dominance representation method	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Decision preference information; capacity; inconsistency check and adjustment; multiple goals linear programming; comprehensive dominance relationship	ROBUST ORDINAL REGRESSION; FUZZY MEASURES; NONADDITIVITY INDEX; AXIOMATIC APPROACH; CAPACITIES; CRITERIA	The Choquet capacity and integral is an efficient tool to flexibly show the decision maker's correlative preference information on multiple decision criteria as well as decision alternatives. In this paper, we enrich the capacity and Choquet integral based decision making scheme from two aspects. The first aspect is to introduce the multiple goals linear programming to aid the decision maker to find out and adjust the inconsistency among the correlative preference information, which needs little prior knowledge of field but can identify the inconsistent constraints efficiently and give the adjust suggestions flexibly. The second aspect is to propose a random simulation based comprehensive decision aid algorithm to show the dominance relationships between all decision alternatives and then unify them into the most creditable ranking order of alternatives, which is quite different to the thought of unique representative ranking order accomplished by the optimization model of some existing methods. An illustrative example is given to show that the enriched decision making scheme is robust to deal with the inconsistent correlative preference information and also practicable to obtain the comprehensive dominance relationship of multiple decision alternatives.																	1064-1246	1875-8967					2020	38	2			SI		2009	2019		10.3233/JIFS-190652													
J								A purpose-oriented shuffled complex evolution optimization algorithm for energy management of multi-microgrid systems considering outage duration uncertainty	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Energy management strategy; multi-microgrid (MMG); optimization; purpose-oriented shuffled complex evolution (POSCE); reliability	NETWORKED MICROGRIDS; KINETIC-PARAMETERS; RELIABILITY; GENERATION; STATE	Integration of several single microgrids (MGs) into the conventional grids which is tends to form multi-microgrid (MMG) systems can enhance the complexity of the operation. On the other hand, the fault occurrence in the uncertain place of system has a notable influence on the energy scheduling results of each operation interval. Thus, in order for MG systems to emerge on a large scale it is necessary to design an efficient and reliable energy management strategy. From this point of view, this paper proposed a novel problem formulation along with a modified optimization algorithm to perform optimized energy management of MMG systems. In the proposed problem formulation, the probability of the failure has been taken into consideration and decentralized multi-agents determine their optimum energy management for both faulted and normal operation modes using variable weighted multi objective function (VWMOF). In the proposed optimization algorithm which is called purpose-oriented shuffled complex evolution (POSCE), it has been tried to modify the shuffled complex evolution (SCE) algorithm to make it become an appropriate optimization method for multi-objective problems with higher coverage rate and higher success rate of search. For this purpose, all data processing and simulation results were made in MATLAB software environment and the effectiveness of the proposed method has been demonstrated on a modified reliability based case study with considering different optimization methods.																	1064-1246	1875-8967					2020	38	2			SI		2021	2038		10.3233/JIFS-190666													
J								Establishment and application of a metro station safety evaluation system based on extension theory	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Extension theory; evaluation system; risk; safety; metro station	RISK-ASSESSMENT; FUZZY AHP; MANAGEMENT	The safety of metro stations is threatened by various types of accidents caused by both man-made and natural causes. Knowing the accurate safety status of a metro station will help managers taking countermeasures timely, thus the safety of passengers and staff members in the station can be ensured. In order to evaluate the safety status of metro stations effectively, based on the Extension Theory, a metro station safety evaluation system was established. Fire, stampedes, violence and terrorist attacks, equipment failures, floods, and earthquake were taken into consideration in the established evaluation system. The evaluation index system includes 1 first-level index, 5 second-level indexes, 15 third-level indexes, and 55 fourth-level indexes. Then the method to determine the weights of indexes was introduced, also how to determine the score and safety grade of every index, and how to calculate the safety score for the third to first level indexes based on the Extension Theory were introduced in detail. After that, a real metro station was selected to verify the operability of the established safety evaluation system, the results show the established safety evaluation system can be used for the safety evaluation of metro stations, and can help the station safety management personnel knowing the safety status better.																	1064-1246	1875-8967					2020	38	2			SI		2039	2054		10.3233/JIFS-190709													
J								Prediction of epileptic seizures using fNIRS and machine learning	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Epileptic seizure; seizure prediction; functional near infrared spectroscopy (fNIRS); electroencephalogram (EEG); multi-layer perceptron (MLP); support vector machine (SVM)	NEAR-INFRARED SPECTROSCOPY; EEG; FMRI; NIRS; ALGORITHM; FEATURES; MODEL	Research to predict epileptic seizures has been mainly focused on the analysis of electroencephalography (EEG) signals; however, recent research efforts have encouraged the use of a relatively new optical signal modality, called functional Near-Infrared Spectroscopy (fNIRS). In fNIRS, near-infrared light is injected into the scalp and the intensity of the reflected light is registered in optodes. Light absorption in hemoglobin depends on the level of blood oxygenation, which is related to brain activity. In this technique, two parameters are measured at each optode, the relative level of oxygenated hemoglobin (HbO) and the relative level of deoxygenated hemoglobin (HbR). In this work we investigated the feasibility of predicting epileptic seizures, using either fNIRS, EEG, or a combination of both signals. In one set of experiments, different implementations for epileptic seizure prediction are tested by using (1) different combinations of electrical and optical signals (EEG, HbO, HbR, EEG+HbO, EEG+HbR, HbO+HbR, EEG+HbO+HbR) and (2) two different classifiers, (Support Vector Machine - SVM and Multi-Layer Perceptron - MLP). In the second set of experiments, seizures are predicted within a five-minute window that is moved up to 15 minutes before the start of the epileptic seizure. By computing the Positive Predictive Value (PPV) and the accuracy, it is demonstrated that fNIRS -based epileptic prediction outperforms EEG-based epileptic prediction. By using optical signals and the SVM classifier, a PPV greater than 99% and an accuracy of 100% were obtained. PPV values of 100% are also obtained when seizures are predicted up to 15 minutes in advance. Furthermore, Kernel Discriminant Analysis (KDA) is used to demonstrate that the highest separability among the classes, corresponding to different epileptic signal phases (pre-ictal, ictal, and inter-ictal), is achieved when fNIRS recordings are used as features for prediction. Finally, fNIRS-based epileptic seizure prediction is tested with Random Chance classifiers. In this study, we showed that fNIRS signals are an effective tool to predict epileptic seizures, even without the use of EEG signals, which are the current standard for seizure prediction.																	1064-1246	1875-8967					2020	38	2			SI		2055	2068		10.3233/JIFS-190738													
J								Patient waiting time management through fuzzy based failure mode and effect analysis	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Patient waiting time; out-patient departments; failure mode and effects analysis; fuzzy best-worst method; fuzzy multi-objective optimization by ratio analysis	DECISION-MAKING; RISK; FMEA; AHP	The amount of time patients spends on services to be delivered in clinics, still is a major problem of some health centers. To solve this problem, various methods proposed by researchers. Failure Mode and Effects Analysis (FMEA) is one of the most used approaches to identify influential failure modes in prolongation of waiting time. In the FMEA method, numeric scores assign to failure modes, using the Risk Priority Number (RPN), but RPN criticized for its shortcoming and leads to unreal results. In this paper, to cover the conventional FMEA shortcoming, firstly, eleven risk factors result in prolongation of waiting time introduced by experts. Secondly, integration of the triangular fuzzy number (TFN) with the Best Worth Method (fuzzy-BWM) was utilized to determine the weights of effective criteria. In the following, failure modes ranked through fuzzy Multi-Objective Optimization by Ratio Analysis (fuzzy-MOORA). Finally, the ranks of eleven failure modes compared in three different methods (Conventional FMEA, conventional MOORA, and fuzzy-MOORA). The potential usage of this method is covering the shortcoming of previous methods and contribute certainty in identifying significant failure modes of the patient waiting time reduction in Out-Patient Departments (OPD). According to the analysis, three main failures for managing waiting time are: the patients never follow up for a later date by the center which can result in chaos in OPD, because of phone or in-person referrals. Secondly, unable to manage canceling/postponing an appointment in emergency cases, Thirdly, office visit not done in the estimated time, which results in a disordering in the center.																	1064-1246	1875-8967					2020	38	2			SI		2069	2080		10.3233/JIFS-190777													
J								Double Wijsman asymptotically statistical equivalence of order alpha	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Asymptotic equivalence; statistical convergence; Cesaro summability; order alpha; Wijsman convergence; Hausdorff convergence; double set sequences	DOUBLE SEQUENCES; CONVERGENCE; SETS	In this paper, we study the concepts of Wijsman asymptotically statistical equivalence of order alpha, Wijsman asymptotically strongly p-Cesaro equivalence of order a and Hausdorff asymptotically statistical equivalence of order a for double set sequences. Also, we research some properties of these concepts and give the inclusion relationships between them.																	1064-1246	1875-8967					2020	38	2			SI		2081	2087		10.3233/JIFS-190796													
J								A new multi-criteria group decision model based on Single-valued triangular Neutrosophic sets and EDAS method	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Single-valued triangular neutrosophic sets; power average operator; power geometric operator; EDAS	POWER AGGREGATION OPERATORS; FUZZY	This paper proposes a new method, which combines the Power operator and the evaluation based on distance from average solution (EDAS) method to deal with the fuzzy multi-criteria group decision problem in the Single-valued triangular Neutrosophic sets (SVTNS) environment. Firstly, we define the normalized Euclidean distance between two single-valued triangular Neutrosophic numbers, and propose the Power weighted averaging (PWA) operator, Power weighted geometric (PWG) operator, generalized Power weighted averaging (GPWA) operator and generalized Power weighted geometric (GPWG) operator of SVTNS. Then, those operators and EDAS method are combined to construct a new multi-criteria group decision-making model. Finally, the model is applied to the problem of the investment projects selection, which proves its applicability and feasibility.																	1064-1246	1875-8967					2020	38	2			SI		2089	2102		10.3233/JIFS-190811													
J								An adjustable weighted soft discernibility matrix based on generalized picture fuzzy soft set and its applications in decision making	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Generalized picture fuzzy soft set; adjustable approach; soft discernibility matrix; multi-criteria decision-making	AGGREGATION OPERATORS; ALGORITHMS; SELECTION; VIEW	The basic idea underneath the generalized picture fuzzy soft set is very constructive in decision-making, since it considers, how to exploit an extra picture fuzzy input from the director to make up for any distortion in the information provided by the evaluation experts, which is defined by Khan et al. In this paper, we introduce a method to solve decision-making problems using adjustable weighted soft discernibility matrix in a generalized picture fuzzy soft set. We define the threshold functions like mid threshold, top-bottom-bottom threshold, bottom-bottom-bottom threshold, top-top-top threshold, med threshold functions and their level soft sets for generalized picture fuzzy soft sets. After, we propose two algorithms based on threshold functions, weighted soft discernibility matrix, and generalized picture fuzzy soft set. To show the supremacy of the given method we illustrate a descriptive example using weighted soft discernibility matrix in the generalized picture fuzzy soft set. Results indicate that the proposed method is more effective and generalized overall existing methods of the fuzzy soft set.																	1064-1246	1875-8967					2020	38	2			SI		2103	2118		10.3233/JIFS-190812													
J								Middleware interoperability performance using interoperable reinforcement learning technique for enterprise business applications	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Interoperability; multithreading; remote procedure call; CORBA; object request broker		Business level service can be implemented by middleware in enterprise systems to improve interoperability and also provides the transparent implementation of client and server process. To improve the performance of Business to Business (B2B) and Business to Consumer (B2C) in terms of enterprise-wide Service Oriented Architecture (SOA), we need to have middleware interoperability especially with representative engineering namely Common Object Request Broker Architecture (CORBA) proposed by Object Management Group, Object Request Broker (ORB) software named ORBeline is available as CORBA complaint. Distinctive models for client-server communications have already been developed and executed in particular Handle Driven ORB (H-ORB), Forwarding ORB (F-ORB), Process Planner ORB (P-ORB), and the Adaptive ORB (A-ORB). This paper focuses how to improve the performance of the interoperability in Process Planner ORB (P-ORB) with respect to client-server interaction in N-tier architecture along with multithreading environment.																	1064-1246	1875-8967					2020	38	2			SI		2119	2127		10.3233/JIFS-190841													
J								Implicative neutrosophic LI-ideals of lattice implication algebras	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										(implicative) LI-ideal; (implicative) neutrosophic LI-ideal		Lattice implication algebra is a new logical algebraic system which is established by combining lattice and implication algebra. As a generalization of intuitionistic fuzzy set, neutrosophic set is introduced which deals with indeterminate membership in addition to degree of membership and degree of non-membership. In this article, the notion of neutrosophic set theory is applied to lattice implication algebras. The concept of implicative neutrosophic LI-ideals of a lattice implication algebra is introduced, and several properties are investigated. Relationship between a neutrosophic LI-ideal and an implicative neutrosophic LI-ideal is discussed, and conditions for a neutrosophic LI-ideal to be an implicative neutrosophic LI-ideal are provided. Characterizations of an implicative neutrosophic LI-ideal are considered, and the extension property of an implicative neutrosophic LI-ideal is studied.																	1064-1246	1875-8967					2020	38	2			SI		2141	2149		10.3233/JIFS-190866													
J								Combining multi-features with a neural joint model for Android malware detection	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Dense convolutional network; Independently Recurrent Neural Network; hierarchical attention; neural joint model; android malware detection	ATTACKS	Combining natural language processing technology, image analysis technology and malware detection technology, a novel Android malware detection method, named BIHAD (an improved IndRNN and attention-treated DenseNet-based pipeline model), is proposed in this paper. First, in order to describe the behavior of Android malware, multiple features are used to construct a more stable discriminant method. Second, the embedding technology is introduced to map all behavior information into a vector space, which implements the extraction of the joint embedded information of semantics and images. Third, an improved Independently Recurrent Neural Network (IndRNN) is used to extract valuable texture information from the original values of the gray image, and effectively utilized the long distance information contained in the gray image. Finally, Hierarchical Attention Dense Convolutional Network (HADenseNet) is used to ensure the maximization of information flow between layers in the network, improving the utilization of semantic distribution and spatial context information. Especially, Hierarchical Attention can enhance the representational ability for key features. The comparison of the BIHAD model with several existing malware detection methods indicated a significant improvement in F-score achieved by the BIHAD.																	1064-1246	1875-8967					2020	38	2			SI		2151	2163		10.3233/JIFS-190888													
J								Three-way group decisions based on multigranulation hesitant fuzzy decision-theoretic rough set over two universes	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Three-way decisions; rough set over two universes; hesitant fuzzy rough set; multigranulation; multi-criteria group decision-making; green supplier selection	MODEL; FRAMEWORK; CONSENSUS	Three-way decisions, deduced from decision-theoretic rough set (DTRS), provide useful approaches for uncertain decision-making problems from a new perspective. Considering situations where decision-makers hesitate among several evaluation values, the hesitant fuzzy set, an innovative extension of fuzzy set, is introduced into multigranulation DTRS under multi-criteria group decision-making (MCGDM) environment. More specifically, we incorporate DTRS with hesitant fuzzy information into MCGDM in this paper, and explore the related decision-making mechanism. Firstly, the variable precision multigranulation hesitant fuzzy DTRS over two universes is defined by utilizing hesitant relation and conditional probability of a hesitant fuzzy event, and the associated decision rules and properties are derived and carefully investigated. Accordingly, as two special types, optimistic and pessimistic multigranulation hesitant fuzzy DTRS over two universes are constructed similarly at the same time. Then, different from loss functions with fixed values in most of the existing DTRS models, the loss functions in this paper are not given directly but calculated from evaluation values expressed by hesitant fuzzy elements. With the aid of the distance measure of hesitant fuzzy elements, the calculation of loss functions is presented. Furthermore, the three-way group decision-making model based on multigranulation hesitant fuzzy DTRS over two universes is established to address MCGDM problems, and key steps of this model are designed. Finally, we elaborate the application of the model by an example of green supplier selection problem.																	1064-1246	1875-8967					2020	38	2			SI		2165	2179		10.3233/JIFS-190970													
J								Exploration of an innovative geometric parameter based on performance enhancement for foot print recognition	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Features; foot images; filters; matching		Foot print, a distinct parameter, is commonly associated with biometrics. Though several methods have been developed in the field of automated biometric based identification, foot print recognition has different characteristics and plays a pivotal role for identification. In this modern world, it is invariably required to have enhanced security and footprint is one such biometric system which is useful for providing authentication and identification. Footprint is also gaining popularity as a technology to identify new born babies at hospitals. The continuous growth in human population necessitates the absolute need for unique identification technique. Recently, there are several incidences of security breach in hospitals where improper tagging of babies lead to child theft and child mismatch. The main objective of this work is to identify a new foot parameter by using two newly proposed algorithms, which helps in better recognition of the foot image. The foot feature extraction methodologies and the procedure for image recognition have been briefly described. The proposed work also highlights the future work to be carried out in this aspect.																	1064-1246	1875-8967					2020	38	2			SI		2181	2196		10.3233/JIFS-190982													
J								Two algorithms for group decision making based on the consistency of intuitionistic multiplicative preference relation	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Decision making; intuitionistic multiplicative preference relation; consistency	WEIGHTS; MODELS	In order to express the uncertainty associated with imprecision or vagueness, a decision maker may give his/her judgments in terms of intuitionistic multiplicative preference relations. In the process of decision making with intuitionistic multiplicative preference relations, how to obtain the intuitionistic multiplicative priority weight vector is an important issue. In this paper, based on the consistency of intuitionistic multiplicative preference relation, a new transformation formula is introduced to construct a consistent intuitionistic multiplicative preference relation from a given normalized intuitionistic multiplicative weight vector. Then, we propose two techniques for group decision making with intuitionistic multiplicative preference relations. In the first case where the decision maker acts as separate individual, the intuitionistic multiplicative priority weight vector of each decision maker can be derived from the individual intuitionistic multiplicative preference relation, after that an overall priority weight vector can be obtained by synthesizing these individual priority weight vectors together. When the decision makers are taken as a group, the overall intuitionistic multiplicative priority weight vector can be generated directly by building a goal programming model without using the aggregation operator. Furthermore, two examples are presented to illustrate the validity and applicability of the proposed methods.																	1064-1246	1875-8967					2020	38	2			SI		2197	2210		10.3233/JIFS-190996													
J								Method of MAGDM based on pythagorean trapezoidal uncertain linguistic hesitant fuzzy aggregation operator with Einstein operations	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Trapezoidal fuzzy numbers; pythagorean trapezoidal fuzzy numbers; einstein operation; aggregation operation; multiple attribute group decision-making	DECISION-MAKING; NUMBERS	In this paper, we define some Einstein operations on Pythagorean trapezoidal uncertain linguistic hesitant fuzzy (PTULHF) set and developed Pythagorean trapezoidal uncertain linguistic hesitant fuzzy Einstein weighted geometric (PTULHFEWG) operator, Pythagorean trapezoidal uncertain linguistic hesitant fuzzy Einstein ordered weighted geometric (PTULHFEOWG) operator and Pythagorean trapezoidal uncertain linguistic hesitant fuzzy Einstein hybrid weighted geometric (PTULHFEHG) operator. Furthermore, we origin ate the relationship between the current aggregation operators and suggested operators and establish many properties of these operators. We apply the proposed aggregation operators to deal with multi-attribute group decision making (MAGDM) problems. Finally, an illustrative example is given to illustrate the decision-making steps and to demonstrate its practicality and effectiveness. We also compare the result of proposed method with existing methods.																	1064-1246	1875-8967					2020	38	2			SI		2211	2230		10.3233/JIFS-191003													
J								Single-machine scheduling problem with fuzzy time delays and mixed precedence constraints	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Single-machine scheduling problem; mixed precedence constraints; fuzzy time delays; makespan; complexity	2-MACHINE FLOWSHOP PROBLEM; COMPLETION-TIME; RELEASE DATES; DUE-DATE; SUBJECT; SHOP; JOBS	In this paper, we study the single-machine scheduling problem and obtain some new results on the special time-delay structure and mixed precedence constraints. We first demonstrate the complexity of the ordinary problem under different circumstances and obtain two cases, namely a polynomial solvable case and an NP-complete case. Then, we present a fuzzy extension of the ordinary problem, based on which a representation of the non-dominated solutions is given for the fuzzy scheduling problem. Finally, we demonstrate the complexity of fuzzy extension problems under different time delays by proposing corresponding algorithms.																	1064-1246	1875-8967					2020	38	2			SI		2231	2244		10.3233/JIFS-191007													
J								Ulam-Hyers stability of uncertain functional differential equation in fuzzy setting with Caputo-Hadamard fractional derivative concept	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy-type Caputo-Hadamard fractional derivative; Ulam-Hyers-Mittag-Leffler stability theory; delay fractional fuzzy problems	INTERVAL-VALUED FUNCTIONS; EXISTENCE; CALCULUS; ORDER; SYSTEMS	In this work, by using the Caputo-Hadamard fractional derivative concept, we propose a new class of fuzzy functional differential equation. In this sense, we establish the existence of the solution, the Ulam-Hyers stability and the Ulam-Hyers-Mittag-Leffler stability for the given problem by means of successive approximation method.																	1064-1246	1875-8967					2020	38	2			SI		2245	2259		10.3233/JIFS-191025													
J								Cosine-similarity based approach for weights determination under hesitant fuzzy environment and its extension to priority derivation	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Hesitant fuzzy set; hesitant multiplicative preference relation; cosine similarity; weights determination; multi-criteria decision making	LINGUISTIC TERM SETS; DECISION-MAKING; CORRELATION-COEFFICIENTS; INFORMATION; DISTANCE	The theory of hesitant fuzzy set is powerful in capturing fuzziness when a decision maker hesitates among some possible values in assessment. In this paper, a novel method is developed for weight determination based on the hesitant fuzzy cosine similarity. This method fully considers the impact of each criterion as well as their mutual relationship on decision analysis. Additionally, we extend this cosine similarity-based method for deriving priorities from a hesitant multiplicative preference relation. This method has a clearer modeling mechanism that the priority vector derived from a hesitant multiplicative preference relation should be as highly cosine-similarity correlated with its column vectors as possible. Then an optimization model can be sequentially established. After that, the proposed methods are demonstrated with two numerical examples and compared with other similar approaches to show the validity and superiority.																	1064-1246	1875-8967					2020	38	2			SI		2261	2271		10.3233/JIFS-191034													
J								Short-term load forecasting with discrete state Hidden Markov Models	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Short term load forecast; discrete state Hidden Markov Model; Gaussian mixture model; multivariate normal distribution; clustering; filtering; Euclidean distance	NEURAL-NETWORKS; TUTORIAL	The paper presents a novel approach for hourly short term forecast of load active power using discrete state Hidden Markov Models. The load data used belongs to the New York Independent System Operator (NYISO) and has been recorded in the years 2014-2017. In the first step, features the best explaining load power changes from the set of weather data, market data (price for load, losses or congestion) and calendar data (day type, day of week, season) are defined. Due to strong seasonality in the data, also a filtering step is included. Finally, the forecast itself is executed with 24 discrete state Hidden Markov Models with a high number of states. Besides the direct comparison with the forecast results obtained by NYISO, the approach is evaluated using an additional benchmark method.																	1064-1246	1875-8967					2020	38	2			SI		2273	2284		10.3233/JIFS-191036													
J								Extended topsis method based on Pythagorean cubic fuzzy multi-criteria decision making with incomplete weight information	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Pythagorean cubic fuzzy set; maximize deviation model; unknown weight information; multi criteria decision-making	AGGREGATION OPERATORS; MEMBERSHIP GRADES; VIKOR; EXTENSION; SETS	The purpose of this article is to propose a new technique to Pythagorean cubic fuzzy (PCF) multiple criteria decision making (MCDM) using the Topsis method with incomplete weight information. Firstly, the maximum deviation model is proposed to find the criteria for determining the optimal weights. Based on the developed method, a MCDM approach is proposed using PCF information. Furthermore, a numerical example is presented to show the feasibility and effectiveness of the proposed information. Finally, a systematic evaluation analysis is given to compare the present work with the existing work.																	1064-1246	1875-8967					2020	38	2			SI		2285	2296		10.3233/JIFS-191089													
J								Identifying key elements in a car-sharing system for constructing a comprehensive car-sharing model	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Car-sharing system; key elements; user requirements; willingness to participate; shared cars	MOBILITY; INTERNET; USAGE	Car-sharing aims to reduce energy consumption by making the best use of carrying capacity of a car. Nevertheless, like any new things, it does not go on quite smoothly. Passengers may not be willing to participate in a car-sharing system (CSS), with consideration of safety, convenience, privacy, and so on. To solve these problems, key elements in CSS are identified, and a comprehensive car-sharing model is established. Finally, the design process of CSS is proposed with four steps: identification of user requirements, service development, optimization & feedback, and verification.																	1064-1246	1875-8967					2020	38	2			SI		2297	2309		10.3233/JIFS-191146													
J								Fuzzy soft matrices on fuzzy soft multiset and its applications in optimization problems	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy soft set; fuzzy soft matrices; fuzzy soft multiset; fuzzy soft max-min method	DECISION-MAKING; SET-THEORY	There has been an increase in the use of mathematical techniques involving extensions of fuzzy sets, of soft sets, and other structures of uncertain knowledge for dealing with multi-criteria decision making under vague or approximate information. Hybrid models have also proliferated because their applicability has been consistently tested by applied scholars. In this paper, we introduce the concept of fuzzy soft matrix on a fuzzy soft multiset and apply it to a decision making problem. An algorithm to choose the ideal alternative is given along with an example, for obtaining the optimal solution. A case study with a comparative analysis shows its feasibility and implementability in decision making systems. An experimental study with adjustable fuzzy inputs, done for a group decision making process, validates the proposal.																	1064-1246	1875-8967					2020	38	2			SI		2311	2322		10.3233/JIFS-191177													
J								Guided attention mechanism: Training network more efficiently	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Guided attention mechanism; natural language processing; prior knowledge	FUSION	With the wide application of attention mechanism in multitudinous field of natural language processing (NLP), to date various deep neural networks based on this mechanism have been introduced and developed. However, a major problem with this kind of application is that a long time will be consumed due to the current networks still need to rely on their own ability to form attention values from scratch during the training In this paper, we propose an auxiliary method called the Guided Attention Mechanism (GAM), which utilizes the prior knowledge to guide the network to form attention values in NLP field, thereby shortening the network training time and making the attention values more accurate. This work designed two sets of prior knowledge generation processes based on the regularization method and the deep learning method respectively. And the prior knowledge is used to guide the attention values of the original network in terms of values and angles. The experimental results show that compared with the original network, the classification accuracy of the network using GAM is improved by about 2%, and the training time is reduced by 5 similar to 9%.																	1064-1246	1875-8967					2020	38	2			SI		2323	2335		10.3233/JIFS-191257													
J								Fuzzy multi-polygroups	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										polygroup; multiset; fuzzy multiset; multi-polygroup; fuzzy multi-polygroup	TOPOLOGY	A multiset is a set containing repeated elements. The objective of this paper is to combine the innovative concept of multisets and polygroups. In particular, we use multisets and fuzzy multisets to introduce the concepts of multi-polygroups and fuzzy multi-polygroups respectively and we discuss their properties. Moreover, we construct fuzzy multi-polygroups from multi-polygroups.																	1064-1246	1875-8967					2020	38	2			SI		2337	2345		10.3233/JIFS-191262													
J								Building prediction model for a machine tool with genetic algorithm optimization on a general regression neural network	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										CNC controller; machine tool; general regression neural network		With the emergence of Industry 4.0, the development of smart machinery has become a goal of mainstream research. The computer numerical control (CNC) machine controller focuses on achieving excellent-quality finished products in a decreased amount of time, a stable surface roughness, and superior geometric accuracy. Therefore, a machining model based on the parameters of the CNC controller could be highly beneficial in industry. In this study, we analyzed the processing parameters of the CNC controller of Delta Electronics. A genetic algorithm (GA)-optimized general regression neural network (GRNN) prediction model based on Taguchi experimental data learning was constructed for a three-axis CNC machine. A fitness function with weighting value on developed GA-GRNN model was devised and navigated to deploy on different machining process needs. Each GA/GA-GRNN model finds a solution of five controller parameters inputs. Experiment results show the improvement of reducing machining time, jerk and corner error was achieved. The machining performance of each set of optimized parameters indicated that the parameter optimization system can assist users to obtain the CNC parameter combination that satisfies the processing requirements. This multi-objective GA/GA-GRNN model gives the intelligent CNC controller characteristics for recent smart manufacturing.																	1064-1246	1875-8967					2020	38	2			SI		2347	2357		10.3233/JIFS-191264													
J								An improved quantum clustering algorithm with weighted distance based on PSO and research on the prediction of electrical power demand	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Particle swarm optimization; weighted distance; quantum cluster; electric power demand; prediction	ENERGY-CONSUMPTION; GREY PREDICTION; MODEL; OPTIMIZATION; METHODOLOGY	The ability to accurately and reliably predict annual electricity demand is essential in modern society for effective planning, economic development, and to ensure the sustainability of the electricity supply. Considering the correlation between annual electricity demand and economic development, as well as annual electricity demand under low-carbon-economy targets, this paper proposes an improved quantum clustering algorithm (particle swarm optimization-weighted distance quantum clustering, PSO-WDQC) as a power demand forecasting model. This method can not only improve the accuracy of predictions but also accurately evaluate the economic development of a region. To demonstrate this ability, the paper applies the proposed method to low-dimensional Iris data as well as high-dimensional Wine data in order to verify the effectiveness of the method. Then, the method is combined with ridge regression to predict the demand for electricity under the low-carbon-economy target of China. The experimental results show that the method can accurately predict annual power demand with a relative error of 0.1674%. Moreover, the model accurately reflects that the Chinese economy has entered a new normal state since 2012, meaning that the economic growth rate has changed from high-speed to medium-high-speed.																	1064-1246	1875-8967					2020	38	2			SI		2359	2367		10.3233/JIFS-191325													
J								Linguistic connection number of set pair analysis based on TOPSIS method and numerical scale function	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Set pair analysis; numerical scale function; cosine distance measure; TOPSIS; linguistic intuitionistic fuzzy set	GROUP DECISION-MAKING; INTUITIONISTIC FUZZY-SETS; AGGREGATION OPERATORS; PREFERENCE; MECHANISM; CONSENSUS; MODEL; TRUST	Set pair analysis is a successful theory to deal with the certainty and uncertainty in the system based on "identity""discrepancy"-"contrary" degree of the connection number. It is very important to construct a reasonable connection number for handling the uncertainty between the feature of set pair. Considering the semantics of linguistic terms, we define a new approach to propose a linguistic connection number(LCN) corresponding to linguistic intuitionistic fuzzy number based on the set pair analysis theory and numerical scale function for semantics of linguistic terms. Then we define a cosine distance measure between the new LCNs and develop the TOPSIS method to the proposed cosine distance measure. Furthermore, we give a numerical example of coal mine safety evaluation to illustrate the proposed method, and the sensitivity analysis of the ranking results is made based on different numerical scale functions. The feasibility and effectiveness of the proposed method are also verified by comparison with the existing methods.																	1064-1246	1875-8967					2020	38	2			SI		2369	2382		10.3233/JIFS-191396													
J								A novel integrated diagnosis method for breast cancer detection	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Breast cancer; feature selection; machine learning predictive model; diagnosis system; classification	SUPPORT VECTOR MACHINE; FEATURE-SELECTION; OPTIMIZATION; HYBRID; GA	The effective detection of breast cancer is particularly essential for recovery and treatment in the initial phases. The existing methods are not successfully diagnosis breast cancer in the initial phases. Thus the initial recognition of breast cancer is expressively a great challenge for health professionals and scientists. To resolve the problem of initial stages recognition of breast cancer, we recommended a machine learning based diagnosis method which will excellently classify the malignant and benign persons. In the designing of our method machine learning model support vector machine has been applied to classify the malignant and benign persons. To increase the classification performances of the method, we used Minimal Redundancy Maximal Relevance and Chi-square algorithms to choose more appropriate features from the breast cancer dataset. The training/testing splitting technique is used for training and testing of the model. Additionally, the performance of the model has been evaluated by performance assessment metrics. The experimental results demonstrated that the classifier support vector machine obtained best classification performance on the selected subset of features as selected by Minimal Redundancy Maximal Relevance feature selection algorithm. The performances of support vector machine on selected features by Chi square feature selection algorithm are low as compared to Minimal Redundancy Maximal Relevance algorithm. From experimental results analysis, we determined that the integrated system based on Minimal Redundancy Maximal Relevance and support vector machine performances are high due to the selection of more suitable features and obtained 99.71% accuracy. According to McNemar's statistical test the proposed method is more significant then existing methods. Thus, we recommend that the proposed diagnosis method for effective detection of breast cancer.																	1064-1246	1875-8967					2020	38	2			SI		2383	2398		10.3233/JIFS-191461													
J								Handling Crimes of Omission by reconciling a criminal core ontology with UFO	APPLIED ONTOLOGY										AI & Law; Legal Ontology; Criminal Domain; Foundational Ontology; Crimes of Omission	LEGAL; PATTERN; DESIGN	Whereas ordinary legal documents are deployed as text documents for human consumption, AI & LAW focuses on tackling the challenges surrounding the development of legal ontologies to assign meaning to legal provisions. We have witnessed an explosion of legal domain (and application) ontologies loosely based on sustainable theories, harming reusability. Fortunately, foundational ontologies foster interoperability between these domain ontologies, providing context-independent concepts that are usable and reusable across multiple domains. In particular, legal ontologies require this "conciliation" with other foundational ontologies, given the inherent heterogeneity of the domain, such as the criminal domain. This paper describes a core legal ontology, built top-down from the UFO foundational ontology, engineered using the positivist legal theory of Hans Kelsen, fostering reusability across a plurality of the criminal field. The UFO proposal makes it possible to evaluate and leverage the conceptual quality of class hierarchies and concept taxonomies through stereotyped modeling primitives and constraints. The latter regulates how the primitives can be combined to support the design of ontologically well-founded conceptual models. This study was conducted to further highlight the depiction of Crimes of Omission, which have not yet been properly explored in the available literature. Although the conceptualization is tied directly to the Brazilian legal domain, it can be applied, with slight adaptations, to other legal systems around the world that follow the Roman-Germanic (or Civil Law) system and Kelsen's legal theory. Through these ontologies, it is possible to perform lawsuit simulation, such as the identification of a crime and imposition of penalties. At the end of this paper, we detail these possibilities for future work.																	1570-5838	1875-8533					2020	15	1					7	39		10.3233/AO-200223													
J								How versioning of terminology systems can be supported by ontological models - a case study on TNM tumor classification	APPLIED ONTOLOGY										Ontology; TNM classification; pancreas tumor; SWRL rules; TNM-O; mapping		The TNM classification (Tumor-Node-Metastasis) is the most important coding scheme used to stage tumors based on size and location. Its coding rules may change across different TNM versions, such that the same tumor is represented by different codes in different versions. We present an ontology-based modular architecture for the management of TNM, using the coding rules for pancreas tumors in the considerably different TNM versions 7 and 8 in order to demonstrate how mappings (in the sense of re-classification) between TNM versions can be supported. To enable re-classification of tumor instances between TNM versions, mapping ontologies were created. This work describes two version mapping approaches, one using SWRL rules and the other intermediate classes representing the mapping criteria between the TNM versions. We show that tumor instances with defined characteristics were correctly classified in different TNM versions. In addition, ontological inconsistencies in classification systems based on informal text labels and possible conversion problems due to different categorization criteria in different TNM versions are demonstrated.																	1570-5838	1875-8533					2020	15	1					41	60		10.3233/AO-190220													
J								Natural necessity: An introductory guide for ontologists	APPLIED ONTOLOGY										Natural necessity; causation; disposition; law of nature; counterfactual	COORDINATED EVOLUTION; CONDITIONAL ANALYSIS; CAUSATION; LAWS; DISPOSITIONS; CAUSALITY; REALISM; METHODOLOGY; KNOWLEDGE; ARGUMENT	Natural necessity is the kind of necessity that is supplied by 'nature': e.g., necessarily, a glass is broken when it is pressed with a certain force. Relevant topics of natural necessity include causation, dispositions, laws of nature, and counterfactuals. Those subjects are vital for a proper ontological characterization of various entities of scientific inquiry. They are also intimately connected to some scientifically important epistemic notions such as observation, hypothesis, explanation, and prediction. In the field of formal ontology, different notions of natural necessity have been investigated individually to a differing degree and the close interrelationship between them has been little explored. This paper provides an introductory panoramic study of natural necessity in formal ontology to help ontologists utilize it in their actual practice. As for ontology of natural necessity, fundamental commitments to type-level causation, type-level dispositions, and laws of nature are especially discussed. As for its epistemology, close consideration is given to the link between natural necessity and scientific explanation. The practicality of ontology of natural necessity is also illustrated with a case study of suitable primary choices of natural necessity for upper and domain ontologies and its potential application to some domain-specific tasks.																	1570-5838	1875-8533					2020	15	1					61	89		10.3233/AO-190219													
J								Ontology summit 2019 communique: Explanations	APPLIED ONTOLOGY										Ontologies; context; inference; commonsense reasoning; explainable AI; narrative; financial explanations; medical explanations		With the increasing amount of software devoted to industrial automation and process control, it is becoming more important than ever for systems to be able to explain their behavior. In some domains, such as financial services, explainability is mandated by law. In spite of this, explanation today is largely handled in an unsystematic manner, if it is handled at all. The decisions of modern artificially intelligent systems, such as those built on deep neural networks, are especially difficult to explain. The goal of the recent Ontology Summit 2019 was concerned with the role of ontologies for explaining the functioning of a system. More specifically, the Ontology Summit focused on critical explanation gaps and the role of ontologies for dealing with these gaps. The sessions examined current technologies and real needs driven by risks and requirements to meet legal or other standards. The sessions covered explainable artificial intelligence, commonsense reasoning and knowledge, the role of narrative, and explanations in the fields of finance and medicine. The goal of this Communique is to foster research and development of approaches to explanations and to drive towards explanation support which can be incorporated into both knowledge engineering processes and ontology design best practices.																	1570-5838	1875-8533					2020	15	1					91	107		10.3233/AO-200226													
J								Multi-label learning for concept-oriented labels of product image data	IMAGE AND VISION COMPUTING										Multi-label learning; Concept-oriented labels; Product image data; Feature correlation learning; Gram matrices	CONVOLUTIONAL NEURAL-NETWORKS; FEATURE-SELECTION	In the designing field, designers usually retrieve the images for reference according to product attributes when designing new proposals. To obtain the attributes of the product, the designers take lots of time and effort to collect product images and annotate them with multiple labels. However, the labels of product images represent the concept of subjective perception, which makes the multi-label learning more challenging to imitate the human aesthetic rather than discriminate the appearance. In this paper, a Feature Correlation Learning (FCL) network is proposed to solve this problem by exploiting the potential feature correlations of product images. Given a product image, the FCL network calculates the features of different levels and their correlations via gram matrices. The FCL is aggregated with the DenseNet to predict the labels of the input product image. The proposed method is compared with several outstanding multi-label learning methods, as well as DenseNet. Experimental results demonstrate that the proposed method outperforms the state-of-the-arts for multi-label learning problem of product image data. (C) 2019 Published by Elsevier B.V. reserved.																	0262-8856	1872-8138				JAN	2020	93								103821	10.1016/j.imavis.2019.10.007													
J								Multiple stream deep learning model for human action recognition	IMAGE AND VISION COMPUTING										Deep learning; Information fusion; Action recognition		Human action recognition is one of the most important and challenging topic in the fields of image processing. Unlike object recognition, action recognition requires motion feature modeling which contains not only spatial but also temporal information. In this paper, we use multiple models to characterize both global and local motion features. Global motion patterns are represented efficiently by the depth-based 3-channel motion history images (MHIs). Meanwhile, the local spatial and temporal patterns are extracted from the skeleton graph. The decisions of these two streams are fused. At the end, the domain knowledge, which is the object/action dependency is considered. The proposed framework is evaluated on two RGB-D datasets. The experimental results show the effectiveness of our proposed approach. The performance is comparable with the state-of-the-art. (C) 2019 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				JAN	2020	93								103818	10.1016/j.imavis.2019.10.004													
J								Improving head pose estimation using two-stage ensembles with top-k regression	IMAGE AND VISION COMPUTING										3D head pose estimation; Average top-k regression; Task-dependent weights; Two-stage ensembles		Conventional head pose estimation methods are regarded as a classification or regression paradigm, individually. The accuracy of classification-based approaches is limited to pose quantized interval and regression-based methods are fragile due to extremely large pose in non-ideal conditions. On the contrary to these methods, this paper introduces a novel head pose estimation method using two-stage ensembles with average top-k regression. The first stage is a binned classification subtask with the optimal pose partition. The second stage achieves average top-k regression based on the former prediction. Then we combine the two subtasks by considering the task-dependent weights instead of setting coefficients by grid search. We conduct several experiments to analyze the optimal pose partition for classification part and to validate the average top-k loss for regression part. Furthermore, we report the performance of proposed method on MW, AFLW2000 and BIWI datasets and results show rather competitive performance in head pose prediction. (C) 2019 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				JAN	2020	93								103827	10.1016/j.imavis.2019.11.005													
J								View-based weight network for 3D object recognition	IMAGE AND VISION COMPUTING										View-based weights layer; 3D object recognition; Extreme learning machine; Multi-channel integrated classifier	EXTREME LEARNING-MACHINE; NEURAL-NETWORKS	Projective methods generally achieve better results in 3D object recognition in recent years. This may be similar to that human visual 3D shapes rely on various 2D observations which are unconscious on retina. Each projection is treated fairly in existing methods. However, we note that different viewpoint images of the same object have different discriminative features, and only some of images are completely significant. We propose a novel View-based Weight Network (VWN) for 3D object recognition where the different view-based weights are assigned to different projections. The trainable view-level weights are incorporated as a pooling layer of the multi-view residual network. The pooling layer contains 7 sub-layers. Meanwhile, we find a simple unsupervised criterion to evaluate the prediction results before they output. To improve the recognition accuracy, a new multi-channel integrated classifier combining Extreme Learning Machine, KNN, SVM and Random Forest is proposed based on the criterion. The multi-channel classifier can make the accuracy of Top1 close to Top2. Experiments on Princeton ModelNet 3D datasets demonstrate our proposed method outperforms the state-of-the-art approaches significantly in recognition accuracy. (C) 2019 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				JAN	2020	93								103828	10.1016/j.imavis.2019.11.006													
J								Face presentation attack detection in mobile scenarios: A comprehensive evaluation	IMAGE AND VISION COMPUTING										Face presentation attack; Face recognition; Performance evaluation; Biometrics		The vulnerability of face recognition systems to different presentation attacks has aroused increasing concern in the biometric community. Face presentation detection (PAD) techniques, which aim to distinguish real face samples from spoof artifacts, are the efficient countermeasure. In recent years, various methods have been proposed to address 2D type face presentation attacks, including photo print attack and video replay attack. However, it is difficult to tell which methods perform better for these attacks, especially in practical mobile authentication scenarios, since there is no systematic evaluation or benchmark of the state-of-the-art methods on a common ground (i.e., using the same databases and protocols). Therefore, this paper presents a comprehensive evaluation of several representative face PAD methods (30 in total) on three public mobile spoofing datasets to quantitatively compare the detection performance. Furthermore, the generalization ability of existing methods is tested under cross-database testing scenarios to show the possible database bias. We also summarize meaningful observations and give some insights that will help promote both academic research and practical applications. (C) 2019 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				JAN	2020	93								103826	10.1016/j.imavis.2019.11.004													
J								Transfer learning in computer vision tasks: Remember where you come from	IMAGE AND VISION COMPUTING										Transfer learning; Parameter regularization; Computer vision		Fine-tuning pre-trained deep networks is a practical way of benefiting from the representation learned on a large database while having relatively few examples to train a model. This adjustment is nowadays routinely performed so as to benefit of the latest improvements of convolutional neural networks trained on large databases. Fine-tuning requires some form of regularization, which is typically implemented by weight decay that drives the network parameters towards zero. This choice conflicts with the motivation for fine-tuning, as starting from a pre-trained solution aims at taking advantage of the previously acquired knowledge. Hence, regularizers promoting an explicit inductive bias towards the pre-trained model have been recently proposed. This paper demonstrates the versatility of this type of regularizer across transfer learning scenarios. We replicated experiments on three state-of-the-art approaches in image classification, image segmentation, and video analysis to compare the relative merits of regularizers. These tests show systematic improvements compared to weight decay. Our experimental protocol put forward the versatility of a regularizer that is easy to implement and to operate that we eventually recommend as the new baseline for future approaches to transfer learning relying on fine-tuning. (C) 2019 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				JAN	2020	93								103853	10.1016/j.imavis.2019.103853													
J								ResFeats: Residual network based features for underwater image classification	IMAGE AND VISION COMPUTING										Deep learning; Residual networks; Deep features; Image classification; Underwater image classification		Oceanographers rely on advanced digital imaging systems to assess the health of marine ecosystems. The majority of the imagery collected by these systems do not get annotated due to lack of resources. Consequently, the expert labeled data is not enough to train dedicated deep networks. Meanwhile, in the deep learning community, much focus is on how to use pre-trained deep networks to classify out-of-domain images and transfer learning. In this paper, we leverage these advances to evaluate how well features extracted from deep neural networks transfer to underwater image classification. We propose new image features (called ResFeats) extracted from the different convolutional layers of a deep residual network pre-trained on ImageNet. We further combine the ResFeats extracted from different layers to obtain compact and powerful deep features. Moreover, we show that ResFeats consistently perform better than their CNN counterparts. Experimental results are provided to show the effectiveness of ResFeats with state-of-the-art classification accuracies on MLC, Benthoz15, EILAT and RSMAS datasets. (C) 2019 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				JAN	2020	93								103811	10.1016/j.imavis.2019.09.002													
J								Depth prediction from 2D images: A taxonomy and an evaluation study	IMAGE AND VISION COMPUTING										Depth prediction; Machine learning; Deep learning; Computer vision		Among the various cues that help us understand and interact with our surroundings, depth is of particular importance. It allows us to move in space and grab objects to complete different tasks. Therefore, depth prediction has been an active research field for decades and many algorithms have been proposed to retrieve depth. Some imitate human vision and compute depth through triangulation on correspondences found between pixels or handcrafted features in different views of the same scene. Others rely on simple assumptions and semantic knowledge of the structure of the scene to get the depth information. Recently, numerous algorithms based on deep learning have emerged from the computer vision community. They implement the same principles as the non-deep learning methods and leverage the ability of deep neural networks of automatically learning important features that help to solve the task. By doing so, they produce new state-of-the-art results and show encouraging prospects. In this article, we propose a taxonomy of deep learning methods for depth prediction from 2D images. We retained the training strategy as the sorting criterion. Indeed, some methods are trained in a supervised manner which means depth labels are needed during training while others are trained in an unsupervised manner. In that case, the models learn to perform a different task such as view synthesis and depth is only a by-product of this learning. In addition to this taxonomy, we also evaluate nine models on two similar datasets without retraining. Our analysis showed that (i) most models are sensitive to sharp discontinuities created by shadows or colour contrasts and (ii) the post processing applied to the results before computing the commonly used metrics can change the model ranking. Moreover, we showed that most metrics agree with each other and are thus redundant. (C) 2019 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				JAN	2020	93								103825	10.1016/j.imavis.2019.11.003													
J								Spec-Net and Spec-CGAN: Deep learning models for specularity removal from faces	IMAGE AND VISION COMPUTING										Specularity; Dichromatic reflection model; Deep learning; Convolutional neural networks	REFLECTION COMPONENTS; IMAGE	The process of splitting an image into specular and diffuse components is a fundamental problem in computer vision, because most computer vision algorithms, such as image segmentation and tracking, assume diffuse surfaces, so existence of specular reflection can mislead algorithms to make incorrect decisions. Existing decomposition methods tend to work well for images with low specularity and high chromaticity, but they fail in cases of high intensity specular light and on images with low chromaticity. In this paper, we address the problem of removing high intensity specularity from low chromaticity images (faces). We introduce a new dataset, Spec-Face, comprising face images corrupted with specular lighting and corresponding ground truth diffuse images. We also introduce two deep learning models for specularity removal, Spec-Net and Spec-CGAN. Spec-Net takes an intensity channel as input and produces an output image that is very close to ground truth, while Spec-CGAN takes an RGB image as input and produces a diffuse image very similar to the ground truth RGB image. On Spec-Face, with Spec-Net, we obtain a peak signal-to-noise ratio (PSNR) of 3.979, a local mean squared error (LMSE) of 0.000071, a structural similarity index (SSIM) of 0.899, and a Frechet Inception Distance (FID) of 20.932. With Spec-CGAN, we obtain a PSNR of 3.360, a LMSE of 0.000098, a SSIM of 0.707, and a FID of 31.699. With Spec-Net and Spec-CGAN, it is now feasible to perform specularity removal automatically prior to other critical complex vision processes for real world images, i.e., faces. This will potentially improve the performance of algorithms later in the processing stream, such as face recognition and skin cancer detection. (C) 2019 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				JAN	2020	93								103823	10.1016/j.imavis.2019.11.001													
J								Region-based Fitting of Overlapping Ellipses and its application to cells segmentation	IMAGE AND VISION COMPUTING										Cell segmentation; 2D shape modeling; Overlapping objects; Ellipse fitting; AIC	LEVEL SET METHOD; IMAGE SEGMENTATION; ROBUST	We present RFOVE, a region-based method for approximating an arbitrary 2D shape with an automatically determined number of possibly overlapping ellipses. RFOVE is completely unsupervised, operates without any assumption or prior knowledge on the object's shape and extends and improves the Decremental Ellipse Fitting Algorithm (DEFA) [1]. Both RFOVE and DEFA solve the multi-ellipse fitting problem by performing model selection that is guided by the minimization of the Akaike Information Criterion on a suitably defined shape complexity measure. However, in contrast to DEFA, RFOVE minimizes an objective function that allows for ellipses with higher degree of overlap and, thus, achieves better ellipse-based shape approximation. A comparative evaluation of RFOVE with DEFA on several standard datasets shows that RFOVE achieves better shape coverage with simpler models (less ellipses). As a practical exploitation of RFOVE, we present its application to the problem of detecting and segmenting potentially overlapping cells in fluorescence microscopy images. Quantitative results obtained in three public datasets (one synthetic and two with more than 4000 actual stained cells) show the superiority of RFOVE over the state of the art in overlapping cells segmentation. (C) 2019 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				JAN	2020	93								103810	10.1016/j.imavis.2019.09.001													
J								FALF ConvNets: Fatuous auxiliary loss based filter-pruning for efficient deep CNNs	IMAGE AND VISION COMPUTING										Filter pruning; Model compression; Convolutional neural network; Image recognition; Deep learning		Obtaining efficient Convolutional Neural Networks (CNNs) are imperative to enable their application for a wide variety of tasks (classification, detection, etc.). While several methods have been proposed to solve this problem, we propose a novel strategy for solving the same that is orthogonal to the strategies proposed so far. We hypothesize that if we add a fatuous auxiliary task, to a network which aims to solve a semantic task such as classification or detection, the filters devoted to solving this frivolous task would not be relevant for solving the main task of concern. These filters could be pruned and pruning these would not reduce the performance on the original task. We demonstrate that this strategy is not only successful, it in fact allows for improved performance for a variety of tasks such as object classification, detection and action recognition. An interesting observation is that the task needs to be fatuous so that any semantically meaningful filters would not be relevant for solving this task. We thoroughly evaluate our proposed approach on different architectures (LeNet, VGG-16, ResNet, Faster RCNN, SSD-512, C3D, and MobileNet V2) and datasets (MNIST, CIFAR, ImageNet, GTSDB, COCO, and UCF101) and demonstrate its generalizability through extensive experiments. Moreover, our compressed models can be used at run-time without requiring any special libraries or hardware. Our model compression method reduces the number of FLOPS by an impressive factor of 6.03X and GPU memory footprint by more than 17X for VGG-16, significantly outperforming other state-of-the-art filter pruning methods. We demonstrate the usability of our approach for 3D convolutions and various vision tasks such as object classification, object detection, and action recognition. (C) 2019 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				JAN	2020	93								103857	10.1016/j.imavis.2019.103857													
J								Salient object detection via double random walks with dual restarts	IMAGE AND VISION COMPUTING										Salient object detection; Double random walks; Propagation distance; Dual restarts	REGION DETECTION; ATTENTION; MODEL	In this paper, we propose a novel saliency model based on double random walks with dual restarts. Two agents (also known as walkers) respectively representing the foreground and background properties simultaneously walk on a graph to explore saliency distribution. First, we propose the propagation distance measure and use it to calculate the initial distributions of the two agents instead of using geodesic distance. Second, the two agents traverse the graph starting from their own initial distribution, and then interact with each other to correct their travel routes by the restart mechanism, which enforces the agents to return to some specific nodes with a certain probability after every movement. We define the dual restarts to take into account interaction between and weighting of two agents. Extensive evaluations demonstrate that the proposed algorithm performs favorably against other state-of-the-art methods on four benchmark datasets. (C) 2019 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				JAN	2020	93								103822	10.1016/j.imavis.2019.10.008													
J								Fine-Grained Image Retrieval via Piecewise Cross Entropy loss	IMAGE AND VISION COMPUTING										Fine-Grained Image Retrieval; CNN; Piecewise cross entropy loss	SIMILARITY	Fine-Grained Image Retrieval is an important problem in computer vision. It is more challenging than the task of content-based image retrieval because it has small diversity within the different classes but large diversity in the same class. Recently, the cross entropy loss can be utilized to make Convolutional Neural Network (CNN) generate distinguish feature for Fine-Grained Image Retrieval, and it can obtain further improvement with some extra operations, such as Normalize-Scale layer. In this paper, we propose a variant of the cross entropy loss, named Piecewise Cross Entropy loss function, for enhancing model generalization and promoting the retrieval performance. Besides, the Piecewise Cross Entropy loss is easy to implement. We evaluate the performance of the proposed scheme on two standard fine-grained retrieval benchmarks, and obtain significant improvements over the state-of-the-art, with 11.8% and 3.3% over the previous work on CARS196 and CUB-200-2011, respectively. (C) 2019 Published by Elsevier B.V. reserved.																	0262-8856	1872-8138				JAN	2020	93								103820	10.1016/j.imavis.2019.10.006													
J								An integrated ship segmentation method based on discriminator and extractor	IMAGE AND VISION COMPUTING										Ship segmentation; Sea fog; Classification; Interference Factor Discriminator; Ship extractor	RECONSTRUCTION	Ship segmentation is an important task in maritime surveillance systems. A great deal of research on image segmentation has been done in the past few years, but there appears to be some problems when directly utilizing them for ship segmentation under complex maritime background. The interference factors decreasing segmentation performance usually are from the peculiarity of complex maritime background, such as the existence of sea fog, large wakes and large waves. To deal with these interference factors, this paper presents an integrated ship segmentation method based on discriminator and extractor (ISDE). Different from traditional segmentation methods, our method consists of two components in light of the structure: Interference Factor Discriminator (IFD) and Ship Extractor (SE). SqueezeNet is employed for the implementation of IFD as the first step to make a judgment on what interference factors are contained in the input image. While DeepLabv3 + and improved DeepLabv3 + are employed for the implementation of SE as the second step to finally extract ships. We collect a ship segmentation dataset and conduct intensive experiments on it. The experimental results demonstrate that our method for ship segmentation outperforms state-of-the-art methods in terms of segmentation accuracy, especially for the images contain sea fog. Besides our method can run in real time as well. (C) 2019 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				JAN	2020	93								103824	10.1016/j.imavis.2019.11.002													
J								Out-of-region keypoint localization for 6D pose estimation	IMAGE AND VISION COMPUTING										6D pose estimation; Keypoint representation; Localization confidence; Real-time processing		This paper addresses the problem of instance level 6D pose estimation from a single RGB image. Our approach simultaneously detects objects and recovers poses by predicting the 2D image locations of the object's 3D bounding box vertices. Specifically, we focus on the challenge of locating virtual keypoints outside the object region proposals, and propose a boundary-based keypoint representation which incorporates classification and regression schemes to reduce output space. Moreover, our method predicts localization confidences and alleviates the influence of difficult keypoints by a voting process. We implement the proposed method based on 2D detection pipeline, meanwhile bridge the feature gap between detection and pose estimation. Our network has real-time processing capability, which runs 30 fps on a GTX 1080Ti GPU. For single object and multiple objects pose estimation on two benchmark datasets, our approach achieves competitive or superior performance compared with state-of-the-art RGB based pose estimation methods. (C) 2019 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				JAN	2020	93								103854	10.1016/j.imavis.2019.103854													
J								Large-scale relation extraction from web documents and knowledge graphs with human-in-the-loop	JOURNAL OF WEB SEMANTICS										Relation extraction; Web mining; Knowledge graph mining; Human-in-the-loop	SEMANTIC WEB	The Semantic Web movement has produced a wealth of curated collections of entities and facts, often referred as Knowledge Graphs. Creating and maintaining such Knowledge Graphs is far from being a solved problem: it is crucial to constantly extract new information from the vast amount of heterogeneous sources of data on the Web. In this work we address the task of Knowledge Graph population. Specifically, given any target relation between two entities, we propose an approach to extract positive instances of the relation from various Web sources. Our relation extraction approach introduces a human-in-the-loop component in the extraction pipeline, which delivers significant advantage with respect to other solely automatic approaches. We test our solution on the ISWC 2018 Semantic Web Challenge, with the objective to identify supply-chain relations among organizations in the Thomson Reuters Knowledge Graph. Our human-in-the-loop extraction pipeline achieves top performance among all competing systems. (C) 2020 Elsevier B.V. All rights reserved.																	1570-8268	1873-7749				JAN	2020	60								100546	10.1016/j.websem.2019.100546													
J								The Internet of Musical Things Ontology	JOURNAL OF WEB SEMANTICS										Internet of Musical Things; Smart musical instruments; Semantic audio	PERFORMANCE; DESIGN; AGENTS; TOOL	The Internet of Musical Things (IoMusT) is an emerging research area consisting of the extension of the Internet of Things paradigm to the music domain. Interoperability represents a central issue within this domain, where heterogeneous objects dedicated to the production and/or reception of musical content (Musical Things) are envisioned to communicate between each other. This paper proposes an ontology for the representation of the knowledge related to IoMusT ecosystems to facilitate interoperability between Musical Things. There was no previous comprehensive data model for the IoMusT domain, however the new ontology relates to existing ontologies, including the SOSA Ontology for the representation of sensors and actuators and the Music Ontology focusing on the production and consumption of music. This paper documents the design of the ontology and its evaluation with respect to specific requirements gathered from an extensive literature review, which was based on scenarios involving IoMusT stakeholders, such as performers and audience members. The IoMusT Ontology can be accessed at: https://w3id.org/iomust#. (C) 2020 Elsevier B.V. All rights reserved.																	1570-8268					JAN	2020	60								100548	10.1016/j.websem.2020.100548													
J								SOBA: Semi-automated Ontology Builder for Aspect-based sentiment analysis	JOURNAL OF WEB SEMANTICS										Domain ontology; Aspect-based sentiment analysis; Ontology learning; Reviews; Semi-automatization	FEATURES	This research explores the possibility of improving knowledge-driven aspect-based sentiment analysis (ABSA) in terms of efficiency and effectiveness. This is done by implementing a Semi-automated Ontology Builder for Aspect-based sentiment analysis (SOBA). Semi-automatization of the ontology building process could produce more extensive ontologies, whilst shortening the building time. Furthermore, SOBA aims to improve the effectiveness of its ontologies in ABSA by attaching to concepts the semantics provided by a semantic lexicon. To evaluate the performance of SOBA, ontologies are created using the ontology builder for the restaurant and laptop domains. The use of these ontologies is then compared with the use of manually constructed ontologies in a state-of-the-art knowledge-driven ABSA model, the Two-Stage Hybrid Model (TSHM). The results show that it is difficult for a machine to beat the quality of a human made ontology, as SOBA does not improve the effectiveness of TSHM, achieving similar results. Including the semantics provided by a semantic lexicon in general increases the performance of TSHM, albeit not significantly. However, SOBA decreases by 50% or more the human time needed to build ontologies, so that it is recommended to use SOBA for knowledge-driven ABSA frameworks, as it leads to greater efficiency. (C) 2019 Elsevier B.V. All rights reserved.																	1570-8268					JAN	2020	60								100544	10.1016/j.websem.2019.100544													
J								Using knowledge anchors to facilitate user exploration of data graphs	SEMANTIC WEB										Data graphs; knowledge utility; data exploration; meaningful learning; knowledge anchors; exploration paths	FORMAL CONCEPT ANALYSIS; ADVANCE ORGANIZERS; ONTOLOGIES; OBJECTS; SEARCH	This paper investigates how to facilitate users' exploration through data graphs. The prime focus is on knowledge utility, i.e. increasing a user's domain knowledge while exploring a data graph, which is crucial in the vast number of user-facing semantic web applications where the users are not experts in the domain. We introduce a highly unique exploration support mechanism underpinned by the subsumption theory for meaningful learning. A core algorithmic component for operationalising the subsumption theory for meaningful learning is the automatic identification of knowledge anchors in a data graph (KA(DG)). We present several metrics for identifying KA(DG) which are evaluated against familiar concepts in human cognitive structures. The second key component is a subsumption algorithm that utilises KA(DG) for generating exploration paths for knowledge expansion. The implementation of the algorithm is applied in the context of a Semantic data browser in a music domain. The resultant exploration paths are evaluated in a task-driven experimental user study compared to free data graph exploration. The findings show that exploration paths, based on subsumption and using knowledge anchors, lead to significantly higher increase in the users' conceptual knowledge and better usability than free exploration of data graphs. The work opens a new avenue in semantic data exploration which investigates the link between learning and knowledge exploration. We provide the first framework that adopts educational theories to inform data graph exploration for knowledge expansion which extends the value of exploration and enables broader applications of data graphs in systems where the end users are not experts in the specific domain.																	1570-0844	2210-4968					2020	11	2					205	234		10.3233/SW-190347													
J								XMLSchema2ShEx: Converting XML validation to RDF validation	SEMANTIC WEB										ShEx; XML Schema; Shape Expressions; formats mapping; data validation	SCHEMA	RDF validation is a field where the Semantic Web community is currently focusing attention. Besides, there is a recent trend to migrate data from different sources to semantic web formats. Therefore, in order to facilitate this transformation, we propose: a set of mappings that can be used to convert from XML Schema to Shape Expressions (ShEx), a prototype that implements a subset of the proposed mappings, an example application to obtain a ShEx schema from an XML Schema and a discussion on conversion implications of non-deterministic schemata. We demonstrate that an XML and its corresponding XML Schema are still valid when converted to their RDF and ShEx counterparts. This conversion, along with the development of other format mappings, could drive to an improvement of data interoperability due to the reduction of the technological gap.																	1570-0844	2210-4968					2020	11	2					235	253		10.3233/SW-180329													
J								Information extraction meets the Semantic Web: A survey	SEMANTIC WEB										Information Extraction; Entity Linking; Keyword Extraction; Topic Modeling; Relation Extraction; Semantic Web	LEARNING CONCEPT HIERARCHIES; NAMED ENTITY DISAMBIGUATION; KNOWLEDGE-BASE; TERM EXTRACTION; LARGE-SCALE; ONTOLOGY; TEXT; RECOGNITION; TABLES; LINKING	We provide a comprehensive survey of the research literature that applies Information Extraction techniques in a Semantic Web setting. Works in the intersection of these two areas can be seen from two overlapping perspectives: using Semantic Web resources (languages/ontologies/knowledge-bases/tools) to improve Information Extraction, and/or using Information Extraction to populate the Semantic Web. In more detail, we focus on the extraction and linking of three elements: entities, concepts and relations. Extraction involves identifying (textual) mentions referring to such elements in a given unstructured or semi-structured input source. Linking involves associating each such mention with an appropriate disambiguated identifier referring to the same element in a Semantic Web knowledge-base (or ontology), in some cases creating a new identifier where necessary. With respect to entities, works involving (Named) Entity Recognition, Entity Disambiguation, Entity Linking, etc. in the context of the Semantic Web are considered. With respect to concepts, works involving Terminology Extraction, Keyword Extraction, Topic Modeling, Topic Labeling, etc., in the context of the Semantic Web are considered. Finally, with respect to relations, works involving Relation Extraction in the context of the Semantic Web are considered. The focus of the majority of the survey is on works applied to unstructured sources (text in natural language); however, we also provide an overview of works that develop custom techniques adapted for semi-structured inputs, namely markup documents and web tables.																	1570-0844	2210-4968					2020	11	2					255	335		10.3233/SW-180333													
J								HDTcrypt: Compression and encryption of RDF datasets	SEMANTIC WEB										RDF; HDT; compression; encryption; linked data protection		The publication and interchange of RDF datasets online has experienced significant growth in recent years, promoted by different but complementary efforts, such as Linked Open Data, the Web of Things and RDF stream processing systems. However, the current Linked Data infrastructure does not cater for the storage and exchange of sensitive or private data. On the one hand, data publishers need means to limit access to confidential data (e.g. health, financial, personal, or other sensitive data). On the other hand, the infrastructure needs to compress RDF graphs in a manner that minimises the amount of data that is both stored and transferred over the wire. In this paper, we demonstrate how HDT - a compressed serialization format for RDF - can be extended to cater for supporting encryption. We propose a number of different graph partitioning strategies and discuss the benefits and tradeoffs of each approach.																	1570-0844	2210-4968					2020	11	2					337	359		10.3233/SW-180335													
J								A visual modeling approach for the Semantic Web Rule Language	SEMANTIC WEB										SWRL; OWL; modeling language; semantic processing; SeMFIS	SWRL RULES; CONCEPTUALIZATION	Although the Semantic Web Rule Language (SWRL) is not a W3C standard, it is widely used for semantic web-based projects as well as for innovative rule-based applications. Thereby, it is used to infer new knowledge from a given fact base. Today, SWRL rules are developed and managed by technical experts in text-based editors using software applications such as the Stanford Protege toolkit. In this paper we present a visual approach which allows users to analyse and modify SWRL rules. By building upon a visual modeling language, the approach includes validation mechanisms and layouting algorithms for visually representing new as well as existing rules. The approach further provides import and export interfaces to common SWRL exchange formats. In this way, its compatibility with widely-used reasoners and semantic web platforms is guaranteed. For ensuring its feasibility, the approach has been prototypically realized using the SeMFIS platform and evaluated using the sample rules as provided in the SWRL specification.																	1570-0844	2210-4968					2020	11	2					361	389		10.3233/SW-180340													
J								Predicting dementia with routine care EMR data	ARTIFICIAL INTELLIGENCE IN MEDICINE										Dementia; Prediction; Random forest; EMR; Machine learning	ALZHEIMERS-DISEASE; DIAGNOSIS; MODEL	Our aim is to develop a machine learning (ML) model that can predict dementia in a general patient population from multiple health care institutions one year and three years prior to the onset of the disease without any additional monitoring or screening. The purpose of the model is to automate the cost-effective, non-invasive, digital pre-screening of patients at risk for dementia. Towards this purpose, routine care data, which is widely available through Electronic Medical Record (EMR) systems is used as a data source. These data embody a rich knowledge and make related medical applications easy to deploy at scale in a cost-effective manner. Specifically, the model is trained by using structured and unstructured data from three EMR data sets: diagnosis, prescriptions, and medical notes. Each of these three data sets is used to construct an individual model along with a combined model which is derived by using all three data sets. Human-interpretable data processing and ML techniques are selected in order to facilitate adoption of the proposed model by health care providers from multiple institutions. The results show that the combined model is generalizable across multiple institutions and is able to predict dementia within one year of its onset with an accuracy of nearly 80% despite the fact that it was trained using routine care data. Moreover, the analysis of the models identified important predictors for dementia. Some of these predictors (e.g., age and hypertensive disorders) are already confirmed by the literature while others, especially the ones derived from the unstructured medical notes, require further clinical analysis.																	0933-3657	1873-2860				JAN	2020	102								101771	10.1016/j.artmed.2019.101771													
J								A methodology based on multiple criteria decision analysis for combining antibiotics in empirical therapy	ARTIFICIAL INTELLIGENCE IN MEDICINE										Multiple criteria decision analysis; Clinical decision support; Combination therapy; Multiobjective-optimization; Empiric antimicrobial therapy	COMBINATION ANTIBIOGRAM; ANTIMICROBIAL STEWARDSHIP; MCDA	Background: The current situation of critical progression in resistance to more effective antibiotics has forced the reuse of old highly toxic antibiotics and, for several reasons, the extension of the indications of combined antibiotic therapy as alternative options to broad spectrum empirical mono-therapy. A key aspect for selecting an appropriate and adequate antimicrobial therapy is that prescription must be based on local epidemiology and knowledge since many aspects, such as prevalence of microorganisms and effectiveness of antimicrobials, change from hospitals, or even areas and services within a single hospital. Therefore, the selection of combinations of antibiotics requires the application of a methodology that provides objectivity, completeness and reproducibility to the analysis of the detailed microbiological, epidemiological, pharmacological information on which to base a rational and reasoned choice. Methods: We proposed a methodology for decision making that uses a multiple criteria decision analysis (MCDA) to support the clinician in the selection of an efficient combined empiric therapy. The MCDA includes a multi objective constrained optimization model whose criteria are the maximum efficacy of therapy, maximum activity, the minimum activity overlapping, the minimum use of restricted antibiotics, the minimum toxicity of antibiotics and the activity against the most prevalent and virulent bacteria. The decision process can be defined in 4 steps: (1) selection of clinical situation of interest, (2) definition of local optimization criteria, (3) definition of constraints for reducing combinations, (4) manual sorting of solutions according to patient's clinical conditions, and (5) selection of a combination. Experiments and results: In order to show the application of the methodology to a clinical case, we carried out experiments with antibiotic susceptibility tests in blood samples taken during a five years period at a university hospital. The validation of the results consists of a manual review of the combinations and experiments carried out by an expert physician that has explained the most relevant solutions proposed according to current clinical knowledge and their use. Conclusion: We show that with the decision process proposed, the physician is able to select the best combined therapy according to different criteria such as maximum efficacy, activity and minimum toxicity. A method for the recommendation of combined antibiotic therapy developed on the basis of a multi-objective optimization model may assist the physicians in the search for alternatives to the use of broad-spectrum antibiotics or restricted antibiotics for empirical therapy. The decision proposed can be easily reproduced for any local epidemiology and any different clinical settings.																	0933-3657	1873-2860				JAN	2020	102								101751	10.1016/j.artmed.2019.101751													
J								Artificial intelligence and the future of psychiatry: Insights from a global physician survey	ARTIFICIAL INTELLIGENCE IN MEDICINE										Deep learning; Mental health; Autonomous agents; Empathy		Background: Futurists have predicted that new autonomous technologies, embedded with artificial intelligence (AI) and machine learning (ML), will lead to substantial job losses in many sectors disrupting many aspects of healthcare. Mental health appears ripe for such disruption given the global illness burden, stigma, and shortage of care providers. Objective: To characterize the global psychiatrist community's opinion regarding the potential of future autonomous technology (referred to here as AI/ML) to replace key tasks carried out in mental health practice. Design: Cross sectional, random stratified sample of psychiatrists registered with Sermo, a global networking platform open to verified and licensed physicians. Main outcome measures: We measured opinions about the likelihood that AI/ML tools would be able to fully replace - not just assist - the average psychiatrist in performing 10 key psychiatric tasks. Among those who considered replacement likely, we measured opinions about how many years from now such a capacity might emerge. We also measured psychiatrist's perceptions about whether benefits of AI/ML would outweigh the risks. Results: Survey respondents were 791 psychiatrists from 22 countries representing North America, South America, Europe and Asia-Pacific. Only 3.8 % of respondents felt it was likely that future technology would make their jobs obsolete and only 17 % felt that future AI/ML was likely to replace a human clinician for providing empathetic care. Documenting and updating medical records (75 %) and synthesizing information (54 %) were the two tasks where a majority predicted that AI/ML could fully replace human psychiatrists. Female- and US-based doctors were more uncertain that the benefits of AI would outweigh risks than male- and non-US doctors, respectively. Around one in 2 psychiatrists did however predict that their jobs would be substantially changed by AI/ML. Conclusions: Our findings provide compelling insights into how physicians think about AI/ML which in turn may help us better integrate technology and reskill doctors to enhance mental health care.																	0933-3657	1873-2860				JAN	2020	102								101753	10.1016/j.artmed.2019.101753													
J								Automatic detection of epileptic seizure based on approximate entropy, recurrence quantification analysis and convolutional neural networks	ARTIFICIAL INTELLIGENCE IN MEDICINE										EEG; Approximate entropy; Recurrence quantification analysis; Convolutional neural network	DYNAMICAL ANALYSIS; EEG SIGNALS; PLOTS; CLASSIFICATION; DEPRESSION	Epilepsy is the most common neurological disorder in humans. Electroencephalogram is a prevalent tool for diagnosing the epileptic seizure activity in clinical, which provides valuable information for understanding the physiological mechanisms behind epileptic disorders. Approximate entropy and recurrence quantification analysis are nonlinear analysis tools to quantify the complexity and recurrence behaviors of non-stationary signals, respectively. Convolutional neural networks are powerful class of models. In this paper, a new method for automatic epileptic electroencephalogram recordings based on the approximate entropy and recurrence quantification analysis combined with a convolutional neural network were proposed. The Bonn dataset was used to assess the proposed approach. The results indicated that the performance of the epileptic seizure detection by approximate entropy and recurrence quantification analysis is good (all of the sensitivities, specificities and accuracies are greater than 80%); especially the sensitivity, specificity and accuracy of the recurrence rate achieved 92.17%, 91.75% and 92.00%. When combines the approximate entropy and recurrence quantification analysis features with convolutional neural networks to automatically differentiate seizure electroencephalogram from normal recordings, the classification result can reach to 98.84%, 99.35% and 99.26%. Thus, this makes automatic detection of epileptic recordings become possible and it would be a valuable tool for the clinical diagnosis and treatment of epilepsy.																	0933-3657	1873-2860				JAN	2020	102								101711	10.1016/j.artmed.2019.101711													
J								Deep supervised learning with mixture of neural networks	ARTIFICIAL INTELLIGENCE IN MEDICINE										Deep neural network; Mixture model; Expectation maximization; Diabetes determination	MODELS; CLASSIFICATION	Deep Neural Network (DNN), as a deep architectures, has shown excellent performance in classification tasks. However, when the data has different distributions or contains some latent non-observed factors, it is difficult for DNN to train a single model to perform well on the classification tasks. In this paper, we propose mixture model based on DNNs (MoNNs), a supervised approach to perform classification tasks with a gating network and multiple local expert models. We use a neural network as a gating function and use DNNs as local expert models. The gating network split the heterogeneous data into several homogeneous components. DNNs are combined to perform classification tasks in each component. Moreover, we use EM (Expectation Maximization) as an optimization algorithm. Experiments proved that our MoNNs outperformed the other compared methods on determination of diabetes, determination of benign or malignant breast cancer, and handwriting recognition. Therefore, the MoNNs can solve the problem of data heterogeneity and have a good effect on classification tasks.																	0933-3657	1873-2860				JAN	2020	102								101764	10.1016/j.artmed.2019.101764													
J								An enhanced deep learning approach for brain cancer MRI images classification using residual networks	ARTIFICIAL INTELLIGENCE IN MEDICINE										Machine learning; Artificial neural network; Convolutional neural network; Deep residual network; Cancer classification	CONVOLUTIONAL NEURAL-NETWORK	Cancer is the second leading cause of death after cardiovascular diseases. Out of all types of cancer, brain cancer has the lowest survival rate. Brain tumors can have different types depending on their shape, texture, and location. Proper diagnosis of the tumor type enables the doctor to make the correct treatment choice and help save the patient's life. There is a high need in the Artificial Intelligence field for a Computer Assisted Diagnosis (CAD) system to assist doctors and radiologists with the diagnosis and classification of tumors. Over recent years, deep learning has shown an optimistic performance in computer vision systems. In this paper, we propose an enhanced approach for classifying brain tumor types using Residual Networks. We evaluate the proposed model on a benchmark dataset containing 3064 MRI images of 3 brain tumor types (Meningiomas, Gliomas, and Pituitary tumors). We have achieved the highest accuracy of 99% outperforming the other previous work on the same dataset.																	0933-3657	1873-2860				JAN	2020	102								101779	10.1016/j.artmed.2019.101779													
J								Optimized artificial neural network based performance analysis of wheelchair movement for ALS patients	ARTIFICIAL INTELLIGENCE IN MEDICINE										Electroencephalogram; Locked in State; Brain machine interface; Local binary patterns; Grey wolf optimization neural network; Common spatial pattern	BRAIN-COMPUTER INTERFACES; EEG; CLASSIFICATION	Individuals with neurodegenerative attacks loose the entire motor neuron movements. These conditions affect the individual actions like walking, speaking impairment and totally make the person in to locked in state (LIS). To overcome the miserable condition the person need rehabilitation devices through a Brain Computer Interfaces (BCI) to satisfy their needs. BMI using Electroencephalogram (EEG) receives the mental thoughts from brain and converts into control signals to activate the exterior communication appliances in the absence of biological channels. To design the BCI, we conduct our study with three normal male subjects, three normal female subjects and three ALS affected individuals from the age of 20-60 with three electrode systems for four tasks. One Dimensional Local Binary Patterns (LBP) technique was applied to reduce the digitally sampled features collected from nine subjects was treated with Grey wolf optimization Neural Network (GWONN) to classify the mentally composed words. Using these techniques, we compared the three types of subjects to identify the performances. The study proves that subjects from normal male categories performance was maximum compared with the other subjects. To assess the individual performance of the subject, we conducted the recognition accuracy test in offline mode. From the accuracy test also, we obtained the best performance from the normal male subjects compared with female and ALS subjects with an accuracy of 98.33 %, 95.00 % and 88.33 %. Finally our study concludes that patients with ALS attack need more training than that of the other subjects.																	0933-3657	1873-2860				JAN	2020	102								101754	10.1016/j.artmed.2019.101754													
J								Using multi-layer perceptron with Laplacian edge detector for bladder cancer diagnosis	ARTIFICIAL INTELLIGENCE IN MEDICINE										Artificial intelligence; Image pre-processing; Laplacian edge detector; Multi-layer perceptron; Urinary bladder cancer	ARTIFICIAL NEURAL-NETWORKS; SQUAMOUS-CELL CARCINOMA; PROSTATE-CANCER; CLASSIFICATION-SYSTEM; URINARY-BLADDER; DISCRIMINATION; GRADIENT; SUPPORT; BENIGN; IMAGES	In this paper, the urinary bladder cancer diagnostic method which is based on Multi-Layer Perceptron and Laplacian edge detector is presented. The aim of this paper is to investigate the implementation possibility of a simpler method (Multi-Layer Perceptron) alongside commonly used methods, such as Deep Learning Convolutional Neural Networks, for the urinary bladder cancer detection. The dataset used for this research consisted of 1997 images of bladder cancer and 986 images of non-cancer tissue. The results of the conducted research showed that using Multi-Layer Perceptron trained and tested with images pre-processed with Laplacian edge detector are achieving AUC value up to 0.99. When different image sizes are compared it can be seen that the best results are achieved if 50 x 50 and 100 x 100 images were used.																	0933-3657	1873-2860				JAN	2020	102								101746	10.1016/j.artmed.2019.101746													
J								Prediction of fetal weight at varying gestational age in the absence of ultrasound examination using ensemble learning	ARTIFICIAL INTELLIGENCE IN MEDICINE										Ensemble learning; Fetal weight estimation; Genetic algorithm; Intersection-over-union; Machine learning	INTERNATIONAL STANDARDS; SONOGRAPHIC ESTIMATION; INTRAUTERINE GROWTH; BIRTH-WEIGHT; RESTRICTION; RECOGNITION; HEAD	Obstetric ultrasound examination of physiological parameters has been mainly used to estimate the fetal weight during pregnancy and baby weight before labour to monitor fetal growth and reduce prenatal morbidity and mortality. However, the problem is that ultrasound estimation of fetal weight is subject to population's difference, strict operating requirements for sonographers, and poor access to ultrasound in low-resource areas. Inaccurate estimations may lead to negative perinatal outcomes. This study aims to predict fetal weight at varying gestational age in the absence of ultrasound examination within a certain accuracy. We consider that machine learning can provide an accurate estimation for obstetricians alongside traditional clinical practices, as well as an efficient and effective support tool for pregnant women for self-monitoring. We present a robust methodology using a data set comprising 4212 intrapartum recordings. The cubic spline function is used to fit the curves of several key characteristics that are extracted from ultrasound reports. A number of simple and powerful machine leaming algorithms are trained, and their performance is evaluated with real test data. We also propose a novel evaluation performance index called the intersection-over-union (loU) for our study. The results are encouraging using an ensemble model consisting of Random Forest, XGBoost, and LightGBM algorithms. The experimental results show the loU between predicted range of fetal weight at any gestational age that is given by the ensemble model and ultrasound respectively. The machine learning based approach applied in our study is able to predict, with a high accuracy, fetal weight at varying gestational age in the absence of ultrasound examination.																	0933-3657	1873-2860				JAN	2020	102								101748	10.1016/j.artmed.2019.101748													
J								Disease phenotype synonymous prediction through network representation learning from PubMed database	ARTIFICIAL INTELLIGENCE IN MEDICINE										Synonyms relation; Phenotype terminology; Network representation; Classification	COMPARATIVE TOXICOGENOMICS DATABASE; NORMALIZATION; CLASSIFIER	Synonym mapping between phenotype concepts from different terminologies is difficult because terminology databases have been developed largely independently. Existing maps of synonymous phenotype concepts from different terminology databases are highly incomplete, and manually mapping is time consuming and laborious. Therefore, building an automatic method for predictive mapping of synonymous phenotypes is of special importance. We propose a classifier-based phenotype mapping prediction model (CPM) to predict synonymous relationships between phenotype concepts from different terminology databases. The model takes network semantic representations of phenotypes as input and predicts synonymous relationships by training binary classifiers with a voting strategy. We compared the performance of the CPM with a similarity-based phenotype mapping prediction model (SPM), which predicts mapping based on the ranked cosine similarity of candidate mapping concepts. Based on a network representation N2V-TFIDF, with a majority voting strategy method MV, the CPM achieved accuracy of 0.943, which was 15.4% higher than that of the SPM using the cosine similarity method (0.789) and 23.8% higher than that of the SSDTM method (0.724) proposed in our previous work.																	0933-3657	1873-2860				JAN	2020	102								101745	10.1016/j.artmed.2019.101745													
J								A modular cluster based collaborative recommender system for cardiac patients	ARTIFICIAL INTELLIGENCE IN MEDICINE										Recommender system; Clustering; Collaborative filtering; Cardiovascular disease; Decision support		In the last few years, hospitals have been collecting a large amount of health related digital data for patients. This includes clinical test reports, treatment updates and disease diagnosis. The information extracted from this data is used for clinical decisions and treatment recommendations. Among health recommender systems, collaborative filtering technique has gained a significant success. However, traditional collaborative filtering algorithms are facing challenges such as data sparsity and scalability, which leads to a reduction in system accuracy and efficiency. In a clinical setting, the recommendations should be accurate and timely. In this paper, an improvised collaborative filtering technique is proposed, which is based on clustering and sub-clustering. The proposed methodology is applied on a supervised set of data for four different types of cardiovascular diseases including angina, non-cardiac chest pain, silent ischemia, and myocardial infarction. The patient data is partitioned with respect to their corresponding disease class, which is followed by k-mean clustering, applied separately on each disease partition. A query patient once directed to the correct disease partition requires to get similarity scores from a reduced sub-cluster, thereby improving the efficiency of the system. Each disease partition has a separate process for recommendation, which gives rise to modularization and helps in improving scalability of the system. The experimental results demonstrate that the proposed modular clustering based recommender system reduces the spatial search domain for a query patient and the time required for providing accurate recommendations. The proposed system improves upon the accuracy of recommendations as demonstrated by the precision and recall values. This is significant for health recommendation systems particularly for those related to cardiovascular diseases.																	0933-3657	1873-2860				JAN	2020	102								101761	10.1016/j.artmed.2019.101761													
J								Fully-automated deep learning-powered system for DCE-MRI analysis of brain tumors	ARTIFICIAL INTELLIGENCE IN MEDICINE										Deep neural network; Pharmacokinetic model; Tumor segmentation; DCE-MRI; Perfusion; Brain	ATLAS-BASED SEGMENTATION; ARTERIAL INPUT FUNCTION; LESION SEGMENTATION; IMAGE SEGMENTATION; ALGORITHM; CLASSIFICATION; EXTRACTION; FIELD	Dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) plays an important role in diagnosis and grading of brain tumors. Although manual DCE biomarker extraction algorithms boost the diagnostic yield of DCE-MRI by providing quantitative information on tumor prognosis and prediction, they are time-consuming and prone to human errors. In this paper, we propose a fully-automated, end-to-end system for DCE-MRI analysis of brain tumors. Our deep learning-powered technique does not require any user interaction, it yields reproducible results, and it is rigorously validated against benchmark and clinical data. Also, we introduce a cubic model of the vascular input function used for pharmacokinetic modeling which significantly decreases the fitting error when compared with the state of the art, alongside a real-time algorithm for determination of the vascular input region. An extensive experimental study, backed up with statistical tests, showed that our system delivers state-of-the-art results while requiring less than 3 min to process an entire input DCE-MRI study using a single GPU.																	0933-3657	1873-2860				JAN	2020	102								101769	10.1016/j.artmed.2019.101769													
J								Multi-objective evolutionary design of antibiotic treatments	ARTIFICIAL INTELLIGENCE IN MEDICINE										Antibiotic resistance; Antimicrobial resistance; AMR; Evolutionary computation; Stochastic model	OPTIMIZATION; ALGORITHM	Antibiotic resistance is one of the major challenges we face in modern times. Antibiotic use, especially their overuse, is the single most important driver of antibiotic resistance. Efforts have been made to reduce unnecessary drug prescriptions, but limited work is devoted to optimising dosage regimes when they are prescribed. The design of antibiotic treatments can be formulated as an optimisation problem where candidate solutions are encoded as vectors of dosages per day. The formulation naturally gives rise to competing objectives, as we want to maximise the treatment effectiveness while minimising the total drug use, the treatment duration and the concentration of antibiotic experienced by the patient. This article combines a recent mathematical model of bacterial growth including both susceptible and resistant bacteria, with a multi-objective evolutionary algorithm in order to automatically design successful antibiotic treatments. We consider alternative formulations combining relevant objectives and constraints. Our approach obtains shorter treatments, with improved success rates and smaller amounts of drug than the standard practice of administering daily fixed doses. These new treatments consistently involve a higher initial dose followed by lower tapered doses.																	0933-3657	1873-2860				JAN	2020	102								101759	10.1016/j.artmed.2019.101759													
J								Evidence of the benefits, advantages and potentialities of the structured radiological report: An integrative review	ARTIFICIAL INTELLIGENCE IN MEDICINE										Structured report; Radiology; Radiological exam; Methodologies of structured report; Evidence		The structured report is a new trend for the preparation and manipulation of radiological examination reports. The structuring of the radiological report data can bring many benefits and advantages over other existing methodologies. Research and studies about the structured radiological report are highly relevant in clinical and academic subjects, improving medical practice, reducing unobserved problems by radiologists, improving reporting practices and medical diagnoses. Exposing the benefits, advantages and potential of the structured radiological report is important in encouraging the acceptance and implementation of this method by radiology professionals who are still somewhat resistant. The present review highlights the factors that contribute to the consolidation of adopting the structured radiology report methodology, addressing a variety of studies focused on the structuring of the radiological report. This integrative review of the literature is proposed by searching publications and journals databases (CAPES - Coordination of Improvement of Higher-Level Personnel, SciELO - Scientific Electronic Library Online, and PubMed - Publisher Medline) to develop a complete and unified understanding of the subject, so that it becomes a major part of evidence-based initiatives.																	0933-3657	1873-2860				JAN	2020	102								101770	10.1016/j.artmed.2019.101770													
J								SemBioNLQA: A semantic biomedical question answering system for retrieving exact and ideal answers to natural language questions	ARTIFICIAL INTELLIGENCE IN MEDICINE										Biomedical question answering; Information retrieval; Passage retrieval; Natural language processing; Machine leaming; Biomedical informatics; BioASQ	CLINICAL QUESTIONS; INFORMATION; ONTOLOGY; TEXT; CLASSIFICATION; SIMILARITY; LEXRANK; MEDLINE	Background and objective: Question answering (QA), the identification of short accurate answers to users questions written in natural language expressions, is a longstanding issue widely studied over the last decades in the open-domain. However, it still remains a real challenge in the biomedical domain as the most of the existing systems support a limited amount of question and answer types as well as still require further efforts in order to improve their performance in terms of precision for the supported questions. Here, we present a semantic biomedical QA system named SemBioNLQA which has the ability to handle the kinds of yes/no, factoid, list, and summary natural language questions. Methods: This paper describes the system architecture and an evaluation of the developed end-to-end biomedical QA system named SemBioNLQA, which consists of question classification, document retrieval, passage retrieval and answer extraction modules. It takes natural language questions as input, and outputs both short precise answers and summaries as results. The SemBioNLQA system, dealing with four types of questions, is based on (1) handcrafted lexico-syntactic patterns and a machine learning algorithm for question classification, (2) PubMed search engine and UMLS similarity for document retrieval, (3) the BM25 model, stemmed words and UMLS concepts for passage retrieval, and (4) UMLS metathesaurus, BioPortal synonyms, sentiment analysis and term frequency metric for answer extraction. Results and conclusion: Compared with the current state-of-the-art biomedical QA systems, SemBioNLQA, a fully automated system, has the potential to deal with a large amount of question and answer types. SemBioNLQA retrieves quickly users' information needs by returning exact answers (e.g., "yes", "no", a biomedical entity name, etc.) and ideal answers (i.e., paragraph-sized summaries of relevant information) for yes/no, factoid and list questions, whereas it provides only the ideal answers for summary questions. Moreover, experimental evaluations performed on biomedical questions and answers provided by the BioASQ challenge especially in 2015, 2016 and 2017 (as part of our participation), show that SemBioNLQA achieves good performances compared with the most current state-of-the-art systems and allows a practical and competitive alternative to help information seekers find exact and ideal answers to their biomedical questions.																	0933-3657	1873-2860				JAN	2020	102								101767	10.1016/j.artmed.2019.101767													
J								Artificial plant optimization algorithm to detect heart rate & presence of heart disease using machine learning	ARTIFICIAL INTELLIGENCE IN MEDICINE										Modified artificial plant optimization algorithm; Machine learning; Savitzky-Golay filter; Extreme gradient boosting; Artificial neural network	SOLVE TOY MODEL	In today's world, cardiovascular diseases are prevalent becoming the leading cause of death; more than half of the cardiovascular diseases are due to Coronary Heart Disease (CHD) which generates the demand of predicting them timely so that people can take precautions or treatment before it becomes fatal. For serving this purpose a Modified Artificial Plant Optimization (MAPO) algorithm has been proposed which can be used as an optimal feature selector along with other machine learning algorithms to predict the heart rate using the fingertip video dataset which further predicts the presence or absence of Coronary Heart Disease in an individual at the moment. Initially, the video dataset has been pre-processed, noise is filtered and then MAPO is applied to predict the heart rate with a Pearson correlation and Standard Error Estimate of 0.9541 and 2.418 respectively. The predicted heart rate is used as a feature in other two datasets and MAPO is again applied to optimize the features of both datasets. Different machine learning algorithms are then applied to the optimized dataset to predict values for presence of current heart disease. The result shows that MAPO reduces the dimensionality to the most significant information with comparable accuracies for different machine learning models with maximum dimensionality reduction of 81.25%. MAPO has been compared with other optimizers and outperforms them with better accuracy.																	0933-3657	1873-2860				JAN	2020	102								101752	10.1016/j.artmed.2019.101752													
J								State recognition of decompressive laminectomy with multiple information in robot-assisted surgery	ARTIFICIAL INTELLIGENCE IN MEDICINE										Medical robot; Tele-surgery; State recognition; Semantic segmentation; Information fusion	DATA FUSION; FLUOROSCOPY; NAVIGATION; SAFETY; SCREW	The decompressive laminectomy is a common operation for treatment of lumbar spinal stenosis. The tools for grinding and drilling are used for fenestration and internal fixation, respectively. The state recognition is one of the main technologies in robot-assisted surgery, especially in tele-surgery, because surgeons have limited perception during remote-controlled robot-assisted surgery. The novelty of this paper is that a state recognition system is proposed for the robot-assisted tele-surgery. By combining the learning methods and traditional methods, the robot from the slave-end can think about the current operation state like a surgeon, and provide more information and decision suggestions to the master-end surgeon, which aids surgeons work safer in tele-surgery. For the fenestration, we propose an image-based state recognition method that consists a U-Net derived network, grayscale redistribution and dynamic receptive field assisting in controlling the grinding process to prevent the grinding-bit from crossing the inner edge of the lamina to damage the spinal nerves. For the internal fixation, we propose an audio and force-based state recognition method that consists signal features extraction methods, LSTM-based prediction and information fusion assisting in monitoring the drilling process to prevent the drilling-bit from crossing the outer edge of the vertebral pedicle to damage the spinal nerves. Several experiments are conducted to show the reliability of the proposed system in robot-assisted surgery.																	0933-3657	1873-2860				JAN	2020	102								101763	10.1016/j.artmed.2019.101763													
J								Signal identification system for developing rehabilitative device using deep learning algorithms	ARTIFICIAL INTELLIGENCE IN MEDICINE										Elecctrooculograpy; Human computer interface; Time delay neural network; Amyotrophic lateral sclerosis; Spinal card injury	ELECTROOCULOGRAM SIGNALS	Paralyzed patients were increasing day by day. Some of the neurodegenerative diseases like amyotrophic lateral sclerosis, Brainstem Leison, Stupor and Muscular dystrophy affect the muscle movements in the body. The affected persons were unable to migrate. To overcome from their problem they need some assistive technology with the help of bio signals. Electrooculogram (EOG) based Human Computer Interaction (HCI) is one of the technique used in recent days to overcome such problem. In this paper we clearly check the possibilities of creating nine states HCI by our proposed method. Signals were captured through five electrodes placed on the subjects face around the eyes. These signals were amplified with ADT26 bio amplifier, filtered with notch filter, and processed with reference power and band power techniques to extract features to detect the eye movements and mapped with Time Delay Neural Network to classify the eye movements to generate control signal to control external hardware devices. Our experimental study reports that maximum average classification of 91.09% for reference power feature and 91.55%-for band power feature respectively. The obtained result confirms that band power features with TDNN network models shows better performance than reference features for all subjects. From this outcome we conclude that band power features with TDNN network models was more suitable for classifying the eleven difference eye movements for individual subjects. To validate the result obtained from this method we categorize the subjects in age wise to check the accuracy of the system. Single trail analysis was conducted in offline to identify the recognizing accuracy of the proposed system. The result summarize that band power features with TDNN network models exceed the reference power with TDNN network model used in this study. Through the outcome we conclude that that band power features with TDNN network was more suitable for designing EOG based HCI in offline mode.																	0933-3657	1873-2860				JAN	2020	102								101755	10.1016/j.artmed.2019.101755													
J								DESIGN AND DEVELOPMENT OF HUMAN COMPUTER INTERFACE USING ELECTROOCULOGRAM WITH DEEP LEARNING	ARTIFICIAL INTELLIGENCE IN MEDICINE										Electrooculogram (EOG); Band Power (BP); Human Computer Interface (HCI); Amyotrophic lateral sclerosis (ALS); Pattern Recognition Neural Network (PRNN)	SYSTEM	Today's life assistive devices were playing significant role in our life to communicate with others. In that modality Human Computer Interface (HCI) based Electrooculogram (EOG) playing vital part. By using this method we can able to overcome the conventional methods in terms of performance and accuracy. To overcome such problem we analyze the EOG signal from twenty subjects to design nine states EOG based HCI using five electrodes system to measure the horizontal and vertical eye movements. Signals were preprocessed to remove the artifacts and extract the valuable information from collected data by using band power and Hilbert Huang Transform (HHT) and trained with Pattern Recognition Neural Network (PRNN) to classify the tasks. The classification results of 92.17% and 91.85% were shown for band power and HHT features using PRNN architecture. Recognition accuracy was analyzed in offline to identify the possibilities of designing HCI. We compare the two feature extraction techniques with PRNN to analyze the best method for classifying the tasks and recognizing single trail tasks to design the HCI. Our experimental result confirms that for classifying as well as recognizing accuracy of the collected signals using band power with PRNN shows better accuracy compared to other network used in this study. We compared the male subjects performance with female subjects to identify the performance. Finally we compared the male as well as female subjects in age group wise to identify the performance of the system. From that we concluded that male performance was appreciable compared with female subjects as well as age group between 26 to 32 performance and recognizing accuracy were high compared with other age groups used in this study.																	0933-3657	1873-2860				JAN	2020	102								101765	10.1016/j.artmed.2019.101765													
J								Implementation of artificial intelligence in medicine: Status analysis and development suggestions	ARTIFICIAL INTELLIGENCE IN MEDICINE										Medical artificial intelligence; Current implementation; Public demand; Future development	HEALTH-CARE; FUTURE; ATTITUDES; ROBOT	The general public's attitudes, demands, and expectations regarding medical AI could provide guidance for the future development of medical AI to satisfy the increasing needs of doctors and patients. The objective of this study is to investigate public perceptions, receptivity, and demands regarding the implementation of medical AI. An online questionnaire was designed to investigate the perceptions, receptivity, and demands of general public regarding medical AI between October 13 and October 30, 2018. The distributions of the current achievements, public perceptions, receptivity, and demands among individuals in different lines of work (i.e., healthcare vs non-healthcare) and different age groups were assessed by performing descriptive statistics. The factors associated with public receptivity of medical AI were assessed using a linear regression model. In total, 2,780 participants from 22 provinces were enrolled. Healthcare workers accounted for 54.3 % of all participants. There was no significant difference between the healthcare workers and non-healthcare workers in the high proportion (99 %) of participants expressing acceptance of AI (p = 0.8568), but remarkable distributional differences were observed in demands (p < 0.001 for both demands for AI assistance and the desire for AI improvements) and perceptions (p < 0.001 for safety, validity, trust, and expectations). High levels of receptivity (approximately 100 %), demands (approximately 80 %), and expectations (100 %) were expressed among different age groups. The receptivity of medical AI among the non-healthcare workers was associated with gender, educational qualifications, and demands and perceptions of AI. There was a very large gap between current availability of and public demands for intelligence services (p < 0.001). More than 90 % of healthcare workers expressed a willingness to devote time to learning about AI and participating in AI research. The public exhibits a high level of receptivity regarding the implementation of medical AI. To date, the achievements have been rewarding, and further advancements are required to satisfy public demands. There is a strong demand for intelligent assistance in many medical areas, including imaging and pathology departments, outpatient services, and surgery. More contributions are imperative to facilitate integrated and advantageous implementation in medical AI.																	0933-3657	1873-2860				JAN	2020	102								101780	10.1016/j.artmed.2019.101780													
J								Electroencephalogram based communication system for locked in state person using mentally spelled tasks with optimized network model	ARTIFICIAL INTELLIGENCE IN MEDICINE										Locked in state; Continuous Wavelet Transform; Spinal Cord Injury; Information Transfer Rate; Brain Computer Interface; Whale Optimization Algorithm; Electroencephalogram	MOTOR IMAGERY; CLASSIFICATION	Due to growth in population, Individual persons with disabilities are increasing daily. To overcome the disability especially in Locked in State (LIS) due to Spinal Cord Injury (SCI), we planned to design four states moving robot from four imagery tasks signals acquired from three electrode systems by placing the electrodes in three positions namely T1, T3 and FP1. At the time of the study we extract the features from Continuous Wavelet Transform (CWT) and trained with Optimized Neural Network model to analyze the features. The proposed network model showed the highest performances with an accuracy of 93.86 % then that of conventional network model. To confirm the performances we conduct offline test. The offline test also proved that new network model recognizing accuracy was higher than the conventional network model with recognizing accuracy of 97.50 %. To verify our result we conducted Information Transfer Rate (ITR), from this analysis we concluded that optimized network model outperforms the other network models like conventional ordinary Feed Forward Neural Network, Time Delay Neural Network and Elman Neural Networks with an accuracy of 21.67 bits per sec. By analyzing classification performances, recognizing accuracy and Information Transformation Rate (ITR), we concluded that CWT features with optimized neural network model performances were comparably greater than that of normal or conventional neural network model and also the study proved that performances of male subjects was appreciated compared to female subjects.																	0933-3657	1873-2860				JAN	2020	102								101766	10.1016/j.artmed.2019.101766													
J								An improved fuzzy set-based multifactor dimensionality reduction for detecting epistasis	ARTIFICIAL INTELLIGENCE IN MEDICINE										Fuzzy set; Epistasis; Multifactor dimensionality reduction; Classification	INFERENCE; GENES	Objective: Epistasis identification is critical for determining susceptibility to human genetic diseases. The rapid development of technology has enabled scalability to make multifactor dimensionality reduction (MDR) measurements an effective calculation tool that achieves superior detection. However, the classification of high-risk (H) or low-risk (L) groups in multidrug resistance operations calls for extensive research. Methods and material: In this study, an improved fuzzy sigmoid (FS) method using the membership degree in MDR (FSMDR) was proposed for solving the limitations of binary classification. The FS method combined with MDR measurements yielded an improved ability to distinguish similar frequencies of potential multifactor genotypes. Results: We compared our results with other MDR-based methods and FSMDR achieved superior detection rates on simulated data sets. The results indicated that the fuzzy classifications can provide insight into the uncertainty of H/L classification in MDR operation. Conclusion: FSMDR successfully detected significant epistasis of coronary artery disease in the Wellcome Trust Case Control Consortium data set.																	0933-3657	1873-2860				JAN	2020	102								101768	10.1016/j.artmed.2019.101768													
J								Skin cancer diagnosis based on optimized convolutional neural network	ARTIFICIAL INTELLIGENCE IN MEDICINE										Skin cancer diagnosis; Deep learning; Convolutional neural networks; Whale optimization algorithm; Levy flight	FORECAST ENGINE; MELANOMA DIAGNOSIS; FEATURE-SELECTION; ALGORITHM; SEGMENTATION; CLASSIFICATION	Early detection of skin cancer is very important and can prevent some skin cancers, such as focal cell carcinoma and melanoma. Although there are several reasons that have bad impacts on the detection precision. Recently, the utilization of image processing and machine vision in medical applications is increasing. In this paper, a new image processing based method has been proposed for the early detection of skin cancer. The method utilizes an optimal Convolutional neural network (CNN) for this purpose. In this paper, improved whale optimization algorithm is utilized for optimizing the CNN. For evaluation of the proposed method, it is compared with some different methods on two different datasets. Simulation results show that the proposed method has superiority toward the other compared methods.																	0933-3657	1873-2860				JAN	2020	102								101756	10.1016/j.artmed.2019.101756													
J								Matching Graph, a Method for Extracting Parallel Information from Comparable Corpora	ACM TRANSACTIONS ON ASIAN AND LOW-RESOURCE LANGUAGE INFORMATION PROCESSING										Information extraction; comparable corpora; parallel fragments; statistical machine translation; generative model; natural language processing; Persian; English; and Arabic languages	MACHINE TRANSLATION	Comparable corpora are valuable alternatives for the expensive parallel corpora. They comprise informative parallel fragments that are useful resources for different natural language processing tasks. In this work, a generative model is proposed for efficient extraction of parallel fragments from a pair of comparable documents. The core of the proposed model is a graph called the Matching Graph. The ability of the Matching Graph to be trained on a small initial seed makes it a proper model for language pairs suffering from the scarce resource problem. Experiments show that the Matching Graph performs significantly better than other recently published models. According to the experiments on English-Persian and Arabic-Persian language pairs, the extracted parallel fragments can be used instead of parallel data for training statistical machine translation systems. Results reveal that the extracted fragments in the best case are able to retrieve about 90% of the information of a statistical machine translation system that is trained on a parallel corpus. Moreover, it is shown that using the extracted fragments as additional information for training statistical machine translation systems leads to an improvement of about 2% for English-Persian and about 1% for Arabic-Persian translation on BLEU score.																	2375-4699	2375-4702				JAN	2020	19	1							11	10.1145/3329713													
J								NeuMorph: Neural Morphological Tagging for Low-Resource Languages-An Experimental Study for Indic Languages	ACM TRANSACTIONS ON ASIAN AND LOW-RESOURCE LANGUAGE INFORMATION PROCESSING										Tagging; Indic languages; multitask learning; convolutional neural network; recurrent neural network	ANALYZER	This article deals with morphological tagging for low-resource languages. For this purpose, five Indic languages are taken as reference. In addition, two severely resource-poor languages, Coptic and Kurmanji, are also considered. The task entails prediction of the morphological tag (case, degree, gender, etc.) of an in-context word. We hypothesize that to predict the tag of a word, considering its longer context such as the entire sentence is not always necessary. In this light, the usefulness of convolution operation is studied resulting in a convolutional neural network (CNN) based morphological tagger. Our proposed model (BLSTM-CNN) achieves insightful results in comparison to the present state-of-the-art. Following the recent trend, the task is carried out under three different settings: single language, across languages, and across keys. Whereas the previous models used only character-level features, we show that the addition of word vectors along with character-level embedding significantly improves the performance of all the models. Since obtaining high-quality word vectors for resource-poor languages remains a challenge, in that scenario, the proposed character-level BLSTM-CNN proves to be most effective.(1)																	2375-4699	2375-4702				JAN	2020	19	1							16	10.1145/3342354													
J								Transform, Combine, and Transfer: Delexicalized Transfer Parser for Low-resource Languages	ACM TRANSACTIONS ON ASIAN AND LOW-RESOURCE LANGUAGE INFORMATION PROCESSING										Transfer parsing; delexicalization; cross-lingual transfer parsing; syntax; dependency parsing		Transfer parsing has been used for developing dependency parsers for languages with no treebank by using transfer from treebanks of other languages (source languages). In delexicalized transfer, parsed words are replaced by their part-of-speech tags. Transfer parsing may not work well if a language does not follow uniform syntactic structure with respect to its different constituent patterns. Earlier work has used information derived from linguistic databases to transform a source language treebank to reduce the syntactic differences between the source and the target languages. We propose a transformation method where a source language pattern is transformed stochastically to one of the multiple possible patterns followed in the target language. The transformed source language treebank can be used to train a delexicalized parser in the target language. We show that this method significantly improves the average performance of single-source delexicalized transfer parsers. We also show that, in the multi-source settings, parsers trained using a concatenation of transformed source language treebanks work better when a subset of the source language treebanks is used rather than concatenating all of them or only one. However, the problem of selecting the subset of treebanks whose combination gives the best-performing parser from the set of all the available treebanks is hard. We propose a greedy selection heuristic based on the labelled attachment scores of the corresponding single-source parsers trained using the treebanks after transformation.																	2375-4699	2375-4702				JAN	2020	19	1							4	10.1145/3325886													
J								Towards Burmese (Myanmar) Morphological Analysis: Syllable-based Tokenization and Part-of-speech Tagging	ACM TRANSACTIONS ON ASIAN AND LOW-RESOURCE LANGUAGE INFORMATION PROCESSING										Burmese (Myanmar); annotated corpus; tokenization; POS-tagging; morphological analysis; CRF; LSTM-based RNN		This article presents a comprehensive study on two primary tasks in Burmese (Myanmar) morphological analysis: tokenization and part-of-speech (POS) tagging. Twenty thousand Burmese sentences of newswire are annotated with two-layer tokenization and POS-tagging information, as one component of the Asian Language Treebank Project. The annotated corpus has been released under a CC BY-NC-SA license, and it is the largest open-access database of annotated Burmese when this manuscript was prepared in 2017. Detailed descriptions of the preparation, refinement, and features of the annotated corpus are provided in the first half of the article. Facilitated by the annotated corpus, experiment-based investigations are presented in the second half of the article, wherein the standard sequence-labeling approach of conditional random fields and a long short-term memory (LSTM)-based recurrent neural network (RNN) are applied and discussed. We obtained several general conclusions, covering the effect of joint tokenization and POS-tagging and importance of ensemble from the viewpoint of stabilizing the performance of LSTM-based RNN. This study provides a solid basis for further studies on Burmese processing.																	2375-4699	2375-4702				JAN	2020	19	1							5	10.1145/3325885													
J								Explicitly Modeling Word Translations in Neural Machine Translation	ACM TRANSACTIONS ON ASIAN AND LOW-RESOURCE LANGUAGE INFORMATION PROCESSING										Neuralmachine translation; word translation; word sense disambiguation; cross-lingual encoder		In this article, we show that word translations can be explicitly incorporated into NMT effectively to avoid wrong translations. Specifically, we propose three cross-lingual encoders to explicitly incorporate word translations into NMT: (1) Factored encoder, which encodes a word and its translation in a vertical way; (2) Gated encoder, which uses a gated mechanism to selectively control the amount of word translations moving forward; and (3) Mixed encoder, which stitchingly learns a word and its translation annotations over sequences where words and their translations are alternatively mixed. Besides, we first use a simple word dictionary approach and then a word sense disambiguation (WSD) approach to effectively model the word context for better word translation. Experimentation on Chinese-to-English translation demonstrates that all proposed encoders are able to improve the translation accuracy for both traditional RNN-based NMT and recent self-attention-based NMT (hereafter referred to as Transformer). Specifically, Mixed encoder yields the most significant improvement of 2.0 in BLEU on the RNN-based NMT, while Gated encoder improves 1.2 in BLEU on Transformer. This indicates the usefulness of an WSD approach in modeling word context for better word translation. This also indicates the effectiveness of our proposed cross-lingual encoders in explicitly modeling word translations to avoid wrong translations in NMT. Finally, we discuss in depth how word translations benefit different NMT frameworks from several perspectives.																	2375-4699	2375-4702				JAN	2020	19	1							15	10.1145/3342353													
J								Adversarial Training for Unknown Word Problems in Neural Machine Translation	ACM TRANSACTIONS ON ASIAN AND LOW-RESOURCE LANGUAGE INFORMATION PROCESSING										Neural machine translation; UNK; generative adversarial network; value iteration		Nearly all of the work in neural machine translation (NMT) is limited to a quite restricted vocabulary, crudely treating all other words the same as an < unk > symbol. For the translation of language with abundant morphology, unknown (UNK) words also come from the misunderstanding of the translation model to the morphological changes. In this study, we explore two ways to alleviate the UNK problem in NMT: a new generative adversarial network (added value constraints and semantic enhancement) and a preprocessing technique that mixes morphological noise. The training process is like a win-win game in which the players are three adversarial sub models (generator, filter, and discriminator). In this game, the filter is to emphasize the discriminator's attention to the negative generations that contain noise and improve the training efficiency. Finally, the discriminator cannot easily discriminate the negative samples generated by the generator with filter and human translations. The experimental results show that the proposed method significantly improves over several strong baseline models across various language pairs and the newly emerged Mongolian-Chinese task is state-of-the-art.																	2375-4699	2375-4702				JAN	2020	19	1							17	10.1145/3342482													
J								Urdu Named Entity Recognition: Corpus Generation and Deep Learning Applications	ACM TRANSACTIONS ON ASIAN AND LOW-RESOURCE LANGUAGE INFORMATION PROCESSING										Resource poor languages; deep learning; Urdu NER corpus; Word2vec; fastText; word embeddings		Named Entity Recognition (NER) plays a pivotal role in various natural language processing tasks, such as machine translation and automatic question-answering systems. Recognizing the importance of NER, a plethora of NER techniques for Western and Asian languages have been developed. However, despite having over 490 million Urdu language speakers worldwide, NER resources for Urdu are either non-existent or inadequate. To fill this gap, this article makes four key contributions. First, we have developed the largest Urdu NER corpus, which contains 926,776 tokens and 99,718 carefully annotated NEs. The developed corpus has at least doubled the number of manually tagged NEs as compared to any of the existing Urdu NER corpora. Second, we have generated six new word embeddings using three different techniques, fastText, Word2vec, and Glove, on two corpora of Urdu text. These are the only publicly available embeddings for the Urdu language, besides the recently released Urdu word embeddings by Facebook. Third, we have pioneered in the application of deep learning techniques, NN and RNN, for Urdu named entity recognition. Finally, we have performed 10-folds of 32 different experiments using the combinations of a traditional supervised learning and deep learning techniques, seven types of word embeddings, and two different Urdu NER datasets. Based on the analysis of the results, several valuable insights are provided about the effectiveness of deep learning techniques, the impact of word embeddings, and variations of datasets.																	2375-4699	2375-4702				JAN	2020	19	1							8	10.1145/3329710													
J								Chinese Zero Pronoun Resolution: A Chain-to-chain Approach	ACM TRANSACTIONS ON ASIAN AND LOW-RESOURCE LANGUAGE INFORMATION PROCESSING										Chinese zero pronoun resolution; chain-to-chain approach; zero pronoun coreferential chains; chain-level features		Chinese zero pronoun (ZP) resolution plays a critical role in discourse analysis. Different from traditional mention-to-mention approaches, this article proposes a chain-to-chain approach to improve the performance of ZP resolution in three aspects. First, consecutive ZPs are clustered into coreferential chains, each working as one independent anaphor as a whole. In this way, those ZPs far away from their overt antecedents can be bridged via other consecutive ZPs in the same coreferential chains and thus better resolved. Second, common noun phrases (NPs) are automatically grouped into coreferential chains using traditional approaches, each working as one independent antecedent candidate as a whole. That is, those NPs occurring in the same coreferential chain are viewed as one antecedent candidate as a whole, and ZP resolution is made between ZP coreferential chains and common NP coreferential chains. In this way, the performance can be much improved due to the effective reduction of the search space by pruning singletons and negative instances. Third and finally, additional features from ZP and common NP coreferential chains are employed to better represent anaphors and their antecedent candidates, respectively. Comprehensive experiments on the OntoNotes V5.0 corpus show that our chain-to-chain approach significantly outperforms the state-of-the-art mention-to-mention approaches. To our knowledge, this is the first work to resolve zero pronouns in a chain-to-chain way.																	2375-4699	2375-4702				JAN	2020	19	1							2	10.1145/3321129													
J								Ancient-Modern Chinese Translation with a New Large Training Dataset	ACM TRANSACTIONS ON ASIAN AND LOW-RESOURCE LANGUAGE INFORMATION PROCESSING										Ancient-Modern Chinese parallel corpus; bilingual text alignment; neural machine translation		Chinese brings the wisdom and spirit culture of the Chinese nation. Automatic translation from ancient Chinese to modern Chinese helps to inherit and carry forward the quintessence of the ancients. However, the lack of large-scale parallel corpus limits the study of machine translation in ancient-modern Chinese. In this article, we propose an ancient-modern Chinese clause alignment approach based on the characteristics of these two languages. This method combines both lexical-based information and statistical-based information, which achieves 94.2 F1-score on our manual annotation Test set. We use this method to create a new large-scale ancient-modern Chinese parallel corpus that contains 1.24M bilingual pairs. To our best knowledge, this is the first large high-quality ancient-modern Chinese dataset. Furthermore, we analyzed and compared the performance of the SMT and various NMT models on this dataset and provided a strong baseline for this task.																	2375-4699	2375-4702				JAN	2020	19	1							6	10.1145/3325887													
J								mu-Forcing: Training Variational Recurrent Autoencoders for Text Generation	ACM TRANSACTIONS ON ASIAN AND LOW-RESOURCE LANGUAGE INFORMATION PROCESSING										Variational autoencoders; variational recurrent autoencoders; uninformative latent variables issues		It has been previously observed that training Variational Recurrent Autoencoders (VRAE) for text generation suffers from serious uninformative latent variables problems. The model would collapse into a plain language model that totally ignores the latent variables and can only generate repeating and dull samples. In this article, we explore the reason behind this issue and propose an effective regularizer-based approach to address it. The proposed method directly injects extra constraints on the posteriors of latent variables into the learning process of VRAE, which can flexibly and stably control the tradeoff between the Kullback-Leibler (KL) term and the reconstruction term, making the model learn dense and meaningful latent representations. The experimental results show that the proposed method outperforms several strong baselines and can make the model learn interpretable latent variables and generate diverse meaningful sentences. Furthermore, the proposed method can perform well without using other strategies, such as KL annealing.																	2375-4699	2375-4702				JAN	2020	19	1							12	10.1145/3341110													
J								Deep Contextualized Word Embeddings for Universal Dependency Parsing	ACM TRANSACTIONS ON ASIAN AND LOW-RESOURCE LANGUAGE INFORMATION PROCESSING										Natural language processing; deep contextualized word embeddings; universal dependency parsing; POS tagging; out-of-vocabulary words; visualization		Deep contextualized word embeddings (Embeddings from Language Model, short for ELMo), as an emerging and effective replacement for the static word embeddings, have achieved success on a bunch of syntactic and semantic NLP problems. However, little is known about what is responsible for the improvements. In this article, we focus on the effect of ELMo for a typical syntax problem-universal POS tagging and dependency parsing. We incorporate ELMo as additional word embeddings into the state-of-the-art POS tagger and dependency parser, and it leads to consistent performance improvements. Experimental results show the model using ELMo outperforms the state-of-the-art baseline by an average of 0.91 for POS tagging and 1.11 for dependency parsing. Further analysis reveals that the improvements mainly result from the ELMo's better abstraction ability on the out-of-vocabulary (OOV) words, and the character-level word representation in ELMo contributes a lot to the abstraction. Based on ELMo's advantage on OOV, experiments that simulate low-resource settings are conducted and the results show that deep contextualized word embeddings are effective for data-insufficient tasks where the OOV problem is severe.																	2375-4699	2375-4702				JAN	2020	19	1							9	10.1145/3326497													
J								Sentiment Analysis for a Resource Poor Language-Roman Urdu	ACM TRANSACTIONS ON ASIAN AND LOW-RESOURCE LANGUAGE INFORMATION PROCESSING										Resource poor language; Roman Urdu; Roman Urdu sentiment analysis	CLASSIFICATION	Sentiment analysis is an important sub-task of Natural Language Processing that aims to determine the polarity of a review. Most of the work done on sentiment analysis is for the resource-rich languages of the world, but very limited work has been done on resource-poor languages. In this work, we focus on developing a Sentiment Analysis System for Roman Urdu, which is a resource-poor language. To this end, a dataset of 11,000 reviews has been gathered from six different domains. Comprehensive annotation guidelines were defined and the dataset was annotated using the multi-annotator methodology. Using the annotated dataset, state-of-the-art algorithms were used to build a sentiment analysis system. To improve the results of these algorithms, four different studies were carried out based on: word-level features, character level features, and feature union. The best results showed that we could reduce the error rate by 12% from the baseline (80.07%). Also, to see if the improvements are statistically significant, we applied t-test and Confidence Interval on the obtained results and found that the best results of each study are statistically significant from the baseline.																	2375-4699	2375-4702				JAN	2020	19	1							10	10.1145/3329709													
J								Importance of Signal Processing Cues in Transcription Correction for Low-Resource Indian Languages	ACM TRANSACTIONS ON ASIAN AND LOW-RESOURCE LANGUAGE INFORMATION PROCESSING										Transcription mismatch errors; signal processing cue based on group delay; hidden Markov model-forced Viterbi alignment; automatic speech recognition; text-to-speech synthesis	CONTINUOUS SPEECH; SEGMENTATION	Accurate phonetic transcriptions are crucial for building robust acoustic models for speech recognition as well as speech synthesis applications. Phonetic transcriptions are not usually provided with speech corpora. A lexicon is used to generate phone-level transcriptions of speech corpora with sentence-level transcriptions. When lexical entries are not available, letter-to-sound (LTS) rules are used. Whether it is a lexicon or LTS, the rules for pronunciation are generic and may not match the spoken utterance. This can lead to transcription errors. The objective of this study is to address the issue of mismatch between the transcription and its acoustic realisation. In particular, the issue of vowel deletions is studied. Group-delay-based segmentation is used to determine insertion/deletion of vowels in the speech utterance. The transcriptions are corrected in the training data based on this. The corrected data are used in automatic speech recognition (ASR) and text to speech synthesis (TTS) systems. ASR and TTS systems built with the corrected transcriptions show improvements in the performance.																	2375-4699	2375-4702				JAN	2020	19	1							14	10.1145/3342352													
J								An Automatic and a Machine-assisted Method to Clean Bilingual Corpus	ACM TRANSACTIONS ON ASIAN AND LOW-RESOURCE LANGUAGE INFORMATION PROCESSING										Statistical Machine Translation; Bilingual Corpus Cleaning	TRANSLATION; WEB	Two different methods of corpus cleaning are presented in this article. One is a machine-assisted technique, which is good to clean small-sized parallel corpus, and the other is an automatic method, which is suitable for cleaning large-sized parallel corpus. A baseline SMT (MOSES) system is used to evaluate these methods. The machine-assisted technique used two features: word alignment and length of the source and target language sentence. These features are used to detect mistranslations in the corpus, which are then handled by a human translator. Experiments of this method are conducted on the English-to-Indian Language Machine Translation (EILMT) corpus (English-Hindi). The Bilingual Evaluation Understudy (BLEU) score is improved by 0.47% for the clean corpus. Automatic method of corpus cleaning uses a combination of two features. One feature is length of source and target language sentence and the second feature is Viterbi alignment score generated by Hidden Markov Model for each sentence pair. Two different threshold values are used for these two features. These values are decided by using a small-sized manually annotated parallel corpus of 206 sentence pairs. Experiments of this method are conducted on the HindEnCorp corpus, released in the workshop of the Association of Computational Linguistics (ACL 2014). The BLEU score is improved by 0.6% on clean corpus. A comparison of the two methods is also presented on EILMT corpus.																	2375-4699	2375-4702				JAN	2020	19	1							13	10.1145/3342351													
J								From Genesis to Creole Language: Transfer Learning for Singlish Universal Dependencies Parsing and POS Tagging	ACM TRANSACTIONS ON ASIAN AND LOW-RESOURCE LANGUAGE INFORMATION PROCESSING										Dependency parsing; universal dependencies; part-of-speech tagging; transfer learning; neural stacking; multi-task network; creole language; Singlish		Singlish can be interesting to the computational linguistics community both linguistically, as a major low-resource creole based on English, and computationally, for information extraction and sentiment analysis of regional social media. In our conference paper, Wang et al. (2017), we investigated part-of-speech (POS) tagging and dependency parsing for Singlish by constructing a treebank under the Universal Dependencies scheme and successfully used neural stacking models to integrate English syntactic knowledge for boosting Singlish POS tagging and dependency parsing, achieving the state-of-the-art accuracies of 89.50% and 84.47% for Singlish POS tagging and dependency, respectively. In this work, we substantially extend Wang et al. (2017) by enlarging the Singlish treebank to more than triple the size and with much more diversity in topics, as well as further exploring neural multi-task models for integrating English syntactic knowledge. Results show that the enlarged treebank has achieved significant relative error reduction of 45.8% and 15.5% on the base model, 27% and 10% on the neural multi-task model, and 21% and 15% on the neural stacking model for POS tagging and dependency parsing, respectively. Moreover, the state-of-the-art Singlish POS tagging and dependency parsing accuracies have been improved to 91.16% and 85.57%, respectively. We make our treebanks and models available for further research.																	2375-4699	2375-4702				JAN	2020	19	1							1	10.1145/3321128													
J								Chinese Syntax Parsing Based on Sliding Match of Semantic String	ACM TRANSACTIONS ON ASIAN AND LOW-RESOURCE LANGUAGE INFORMATION PROCESSING										Chinese parsing; semantic code template; sliding match of semantic string; SMOSS		Different from the current syntax parsing based on deep learning, we present a novel Chinese parsing method, which is based on Sliding Match of Semantic String (SMOSS). (1) Training stage: In a treebank, headwords of tree nodes are represented by semantic codes given in the Synonym Dictionary (Tongyici Cilin). N-gram semantic templates are extracted from every layer of a syntax tree by means of sliding window to establish one N-gram semantic template library. (2) Parsing stage: Words of a sentence, including headwords of chunks, are represented by the semantic codes from Tongyici Cilin. With the sliding window method, N-gram semantic code strings are extracted to match with the templates in the N-gram semantic template library; subsequently, the mapping information of the matched templates is employed to guide the chunking of semantic code strings. The Chinese syntax parsing is completed through continuous matching and chunking. On the same training scale, N-gram semantic template can create favorable conditions for flexible matching and improve the syntax parsing performance. With train and test sets from the Tsinghua Chinese Treebank (TCT), the results are F1-score 99.71% (closed test) and F1-score 70.43% (open test), respectively.																	2375-4699	2375-4702				JAN	2020	19	1							7	10.1145/3329707													
J								Chinese Zero Pronoun Resolution: A Collaborative Filtering-based Approach	ACM TRANSACTIONS ON ASIAN AND LOW-RESOURCE LANGUAGE INFORMATION PROCESSING										Zero pronoun resolution; deep neural network; collaborative filtering		Semantic information that has been proven to be necessary to the resolution of common noun phrases is typically ignored by most existing Chinese zero pronoun resolvers. This is because that zero pronouns convey no descriptive information, which makes it almost impossible to calculate semantic similarities between the zero pronoun and its candidate antecedents. Moreover, most of traditional approaches are based on the single-candidate model, which considers the candidate antecedents of a zero pronoun in isolation and thus overlooks their reciprocities. To address these problems, we first propose a neural-network-based zero pronoun resolver (NZR) that is capable of generating vector-space semantics of zero pronouns and candidate antecedents. On the basis of NZR, we develop the collaborative filtering-based framework for Chinese zero pronoun resolution task, exploring the reciprocities between the candidate antecedents of a zero pronoun to more rationally re-estimate their importance. Experimental results on the Chinese portion of the OntoNotes 5.0 corpus are encouraging: Our proposed model substantially surpasses the Chinese zero pronoun resolution baseline systems.																	2375-4699	2375-4702				JAN	2020	19	1							3	10.1145/3325884													
J								Neural-symbolic integration and the Semantic Web	SEMANTIC WEB										Neural-symbolic integration; deductive reasoning; artificial neural networks; deep learning		Symbolic Systems in Artificial Intelligence which are based on formal logic and deductive reasoning are fundamentally different from Artificial Intelligence systems based on artificial neural networks, such as deep learning approaches. The difference is not only in their inner workings and general approach, but also with respect to capabilities. Neural-symbolic Integration, as a field of study, aims to bridge between the two paradigms. In this paper, we will discuss neural-symbolic integration in its relation to the Semantic Web field, with a focus on promises and possible benefits for both, and report on some current research on the topic.																	1570-0844	2210-4968					2020	11	1					3	11		10.3233/SW-190368													
J								Leveraging Knowledge Graphs for Big Data Integration: the XI Pipeline	SEMANTIC WEB										Knowledge Graphs; Big Data Integration; crowdsourcing		This article gives an overview of recent efforts focusing on integrating heterogeneous data using Knowledge Graphs. I introduce a pipeline consisting of five steps to integrate semi-structured or unstructured content. I discuss some of the key applications of this pipeline through three use-cases, and present the lessons learnt along the way while designing and building data integration systems.																	1570-0844	2210-4968					2020	11	1					13	17		10.3233/SW-190371													
J								The Semantic Web identity crisis: In search of the trivialities that never were	SEMANTIC WEB										Vision; Web; semantics		For a domain with a strong focus on unambiguous identifiers and meaning, the Semantic Web research field itself has a surprisingly ill-defined sense of identity. Started at the end of the 1990s at the intersection of databases, logic, and Web, and influenced along the way by all major tech hypes such as Big Data and machine learning, our research community needs to look in the mirror to understand who we really are. The key question amid all possible directions is pinpointing the important challenges we are uniquely positioned to tackle. In this article, we highlight the community's unconscious bias toward addressing the Paretonian 80% of problems through research - handwavingly assuming that trivial engineering can solve the remaining 20%. In reality, that overlooked 20% could actually require 80% of the total effort and involve significantly more research than we are inclined to think, because our theoretical experimentation environments are vastly different from the open Web. As it turns out, these formerly neglected "trivialities" might very well harbor those research opportunities that only our community can seize, thereby giving us a clear hint of how we can orient ourselves to maximize our impact on the future. If we are hesitant to step up, more pragmatic minds will gladly reinvent technology for the real world, only covering a fraction of the opportunities we dream of.																	1570-0844	2210-4968					2020	11	1					19	27		10.3233/SW-190372													
J								Neural language models for the multilingual, transcultural, and multimodal Semantic Web	SEMANTIC WEB										Neural networks; multilingual representations; cross-linguistic modeling	TECHNOLOGIES	A vision of a truly multilingual Semantic Web has found strong support with the Linguistic Linked Open Data community. Standards, such as OntoLex-Lemon, highlight the importance of explicit linguistic modeling in relation to ontologies and knowledge graphs. Nevertheless, there is room for improvement in terms of automation, usability, and interoperability. Neural Language Models have achieved several breakthroughs and successes considerably beyond Natural Language Processing (NLP) tasks and recently also in terms of multimodal representations. Several paths naturally open up to port these successes to the Semantic Web, from automatically translating linguistic information associated with structured knowledge resources to multimodal question-answering with machine translation. Language is also an important vehicle for culture, an aspect that deserves considerably more attention. Building on existing approaches, this article envisions joint forces between Neural Language Models and Semantic Web technologies for multilingual, transcultural, and multimodal information access and presents open challenges and opportunities in this direction.																	1570-0844	2210-4968					2020	11	1					29	39		10.3233/SW-190373													
J								On the role of knowledge graphs in explainable AI	SEMANTIC WEB										Knowledge graph; explainable AI; machine learning; artificial intelligence	WEB	The current hype of Artificial Intelligence (AI) mostly refers to the success of machine learning and its sub-domain of deep learning. However, AI is also about other areas, such as Knowledge Representation and Reasoning, or Distributed AI, i.e., areas that need to be combined to reach the level of intelligence initially envisioned in the 1950s. Explainable AI (XAI) now refers to the core backup for industry to apply AI in products at scale, particularly for industries operating with critical systems. This paper reviews XAI not only from a Machine Learning perspective, but also from the other AI research areas, such as AI Planning or Constraint Satisfaction and Search. We expose the XAI challenges of AI fields, their existing approaches, limitations and opportunities for Knowledge Graphs and their underlying technologies.																	1570-0844	2210-4968					2020	11	1					41	51		10.3233/SW-190374													
J								Hybrid reasoning in knowledge graphs: Combing symbolic reasoning and statistical reasoning	SEMANTIC WEB										Knowledge graphs; hybrid reasoning; knowledge representation		Knowledge graphs (KGs) contain rich resources that represent human knowledge in the world. There are mainly two kinds of reasoning techniques in knowledge graphs, symbolic reasoning and statistical reasoning. However, both of them have their merits and limitations. Therefore, it is desirable to combine them to provide hybrid reasoning in a knowledge graph. In this paper, we present the first work on the survey of methods for hybrid reasoning in knowledge graphs. We categorize existing methods based on applications of reasoning techniques, and introduce the key ideas of them. Finally, we re-examine the remaining research problems to be solved and provide an outlook to future directions for hybrid reasoning in knowledge graphs.																	1570-0844	2210-4968					2020	11	1					53	62		10.3233/SW-190375													
J								A map without a legend	SEMANTIC WEB										Semantic web; linked data; big data; open data; knowledge; knowledge representation; ontology; machine learning; web of data; reproducible research; eScience; cultural evolution	SEMANTIC WEB	The current state of the semantic web is focused on data. This is a worthwhile progress in web content processing and interoperability. However, this does only marginally contribute to knowledge improvement and evolution. Understanding the world, and interpreting data, requires knowledge. Not knowledge cast in stone for ever, but knowledge that can seamlessly evolve; not knowledge from one single authority, but diverse knowledge sources which stimulate confrontation and robustness; not consistent knowledge at web scale, but local theories that can be combined. We discuss two different ways in which semantic web technologies can greatly contribute to the advancement of knowledge: semantic eScience and cultural knowledge evolution.																	1570-0844	2210-4968					2020	11	1					63	68		10.3233/SW-190376													
J								Creative AI: A new avenue for the Semantic Web?	SEMANTIC WEB										Computational creativity; artificial intelligence; Semantic Web; knowledge graph; ontology	COMPUTERS; FRAMEWORK	Computational Creativity (or artificial creativity) is a multidisciplinary field, researching how to construct computer programs that model, simulate, exhibit or enhance creative behaviour. This vision paper explores a potential of the Semantic Web and its technologies for creative AI. Possible uses of the Semantic Web and semantic technologies are discussed, regarding three types of creativity: i) exploratory creativity, ii) combinational creativity, and iii) transformational creativity and relevant research questions. For exploratory creativity, how can we explore the limits of what is possible, while remaining bound by a set of existing domain axioms, templates, and rules, expressed with semantic technologies? To achieve a combinational creativity, how can we combine or blend existing concepts, frames, ontology design patterns, and other constructs, and benefit from cross-fertilization? Ultimately, can we use ontologies and knowledge graphs, which describe an existing domain with its constraints and, applying a meta-rule for transformational creativity, start dropping constraints and adding new constraints to produce novel artifacts? Together with these new challenges, the paper also provides pointers to emerging and growing application domains of Semantic Web related to computational creativity: from recipe generation to scientific discovery and creative design.																	1570-0844	2210-4968					2020	11	1					69	78		10.3233/SW-190377													
J								Ontologies as nested facet systems for human-data interaction	SEMANTIC WEB										Web-interface; ontology; biomedical Big Data; nested facet system; user experience	HIERARCHICAL RELATIONS; CLINICAL-RESEARCH; NCI THESAURUS; SNOMED-CT; INFORMATICS; TERMINOLOGY	Irrespective of data size and complexity, query and exploration tools for accessing data resources remain a central linkage for human-data interaction. A fundamental barrier in making query interfaces easier to use, ultimately as easy as online shopping, is the lack of faceted, interactive capabilities. We propose to repurpose existing ontologies by transforming them into nested facet systems (NFS) to support human-data interaction. Two basic issues need to be addressed for this to happen: one is that the structure and quality of ontologies need to be examined and elevated for the purpose of NFS; the second is that mappings from data-source specific metadata to a corresponding NFS need to be developed to support this new generation of NFS-enabled web-interfaces. The purpose of this paper is to introduce the concept of NFS and outline opportunities involved in using ontologies as NFS for querying and exploring data, especially in the biomedical domain.																	1570-0844	2210-4968					2020	11	1					79	86		10.3233/SW-190378													
J								Are we better off with just one ontology on the Web?	SEMANTIC WEB										Ontology; Knowledge Representation	LINKED DATA; TOOL	Ontologies have been used on the Web to enable semantic interoperability between parties that publish information independently of each other. They have also played an important role in the emergence of Linked Data. However, many ontologies on theWeb do not see much use beyond their initial deployment and purpose in one dataset and therefore should rather be called what they are - (local) schemas, which per se do not provide any interoperable semantics. Only few ontologies are truly used as a shared conceptualization between different parties, mostly in controlled environments such as the BioPortal. In this paper, we discuss open challenges relating to true re-use of ontologies on the Web and raise the question: "are we better off with just one ontology on the Web?".																	1570-0844	2210-4968					2020	11	1					87	99		10.3233/SW-190379													
J								A more decentralized vision for Linked Data	SEMANTIC WEB										Linked Data; decentralization; Semantic Web	WEB; ONTOLOGIES	In this deliberately provocative position paper, we claim that more than ten years into Linked Data there are still (too?) many unresolved challenges towards arriving at a truly machine-readable and decentralized Web of data. We take a deeper look at key challenges in usage and adoption of Linked Data from the ever-present "LOD cloud" diagram.1 Herein, we try to highlight and exemplify both key technical and non-technical challenges to the success of LOD, and we outline potential solution strategies. We hope that this paper will serve as a discussion basis for a fresh start towards more actionable, truly decentralized Linked Data, and as a call to the community to join forces.																	1570-0844	2210-4968					2020	11	1					101	113		10.3233/SW-190380													
J								Semantics for Cyber-Physical Systems: A cross-domain perspective	SEMANTIC WEB										Cyber-Physical Systems; Industrie4.0; smart energy networks; smart buildings; Semantic Web technologies	SMART; TECHNOLOGIES; GENERATION; CHALLENGES	Modern life is increasingly made more comfortable, efficient, and sustainable by the smart systems that surround us: smart buildings monitor and adjust temperature levels to achieve occupant comfort while optimizing energy consumption; smart energy grids reconfigure dynamically to make the best use of ad-hoc energy produced by a host of distributed energy producers; smart factories can be reconfigured on the shop-floor to efficiently produce a diverse range of products. These complex systems can only be realized by tightly integrating components in the physical space (sensors, actuators) with advanced software algorithms in the cyber-space, thus creating so-called Cyber-Physical Systems (CPS). Semantic Web technologies (SWT) have seen a natural uptake in several areas based on CPS, given that CPS are data and knowledge intensive while providing advanced functionalities typical of semantics-based intelligent systems. Yet, so far, this uptake has primarily happened within the boundaries of application domains resulting in somewhat disconnected research communities. In this paper, we take a cross-domain perspective by synthesizing our experiences of using SWTs during the engineering and operation of CPSs in smart manufacturing, smart buildings, and smart grids. We discuss use cases that are amenable to the use of SWTs, benefits and challenges of using these technologies in the CPS lifecycle as well as emerging future trends. While non-exhaustive, our paper aims at opening up a dialog between these fields and at putting the foundation for a research area on semantics in CPS.																	1570-0844	2210-4968					2020	11	1					115	124		10.3233/SW-190381													
J								Ontology engineering: Current state, challenges, and future directions	SEMANTIC WEB										Ontologies; ontology engineering; methods; standards; tooling; patterns; challenges; future research	KNOWLEDGE-BASE; TOOL; REPOSITORY; EVOLUTION	In the last decade, ontologies have become widely adopted in a variety of fields ranging from biomedicine, to finance, engineering, law, and cultural heritage. The ontology engineering field has been strengthened by the adoption of several standards pertaining to ontologies, by the development or extension of ontology building tools, and by a wider recognition of the importance of standardized vocabularies and formalized semantics. Research into ontology engineering has also produced methods and tools that are used more and more in production settings. Despite all these advancements, ontology engineering is still a difficult process, and many challenges still remain to be solved. This paper gives an overview of how the ontology engineering field has evolved in the last decade and discusses some of the unsolved issues and opportunities for future research.																	1570-0844	2210-4968					2020	11	1					125	138		10.3233/SW-190382													
J								Closing the Loop between knowledge patterns in cognition and the Semantic Web	SEMANTIC WEB										Relations; roles; frames; knowledge patterns; cognitive semantics; semantic interoperability		We discuss currently open issues in the discovery and representation of knowledge patterns in computational processing of meaning, in order to improve interoperability and cognitive validity of web-based semantics. We present the current state of knowledge patterns (KP) in Knowledge Representation, the SemanticWeb and Cognitive Sciences, focusing on an intensional abstraction of heterogeneous predicates as a formal foundation for KP.																	1570-0844	2210-4968					2020	11	1					139	151		10.3233/SW-190383													
J								Towards a new generation of ontology based data access	SEMANTIC WEB										OBDA; data translation; query translation; mapping translation		Ontology Based Data Access (OBDA) refers to a range of techniques, algorithms and systems that can be used to deal with the heterogeneity of data that is common inside many organisations as well as in inter-organisational settings and more openly on the Web. In OBDA, ontologies are used to provide a global view over multiple local datasets; and mappings are commonly used to describe the relationships between such global and local schemas. Since its inception, this area has evolved in several directions. Initially, the focus was on the translation of original sources into a global schema, and its materialisation, including non-OBDA approaches such as the use of Extract Transform Load (ETL) workflows in data warehouses and, more recently, in data lakes. Then OBDA-based query translation techniques, relying on mappings, were proposed, with the aim of removing the need for materialisation, something especially useful for very dynamic data sources. We think that we are now witnessing the emergence of a new generation of OBDA approaches. It is driven by the fact that a new set of declarative mapping languages, most of which stem from the W3C Recommendation R2RML for Relational Databases (RDB), are being created. In this vision paper, we enumerate the reasons why new mapping languages are being introduced. We discuss why it may be relevant to work on translations among them, so as to benefit from the engines associated to each of them whenever one language and/or engine is more suitable than another. We discuss the emerging concept of "mapping translation", the basis for this new generation of OBDA, together with some of its desirable properties: information preservation and query result preservation. We show several scenarios where mapping translation can be or is being already applied, even though this term has not necessarily been used in existing literature.																	1570-0844	2210-4968					2020	11	1					153	160		10.3233/SW-190384													
J								Ontological challenges to cohabitation with self-taught robots	SEMANTIC WEB										Applied ontology; robotics; interaction; heterogeneous agents; multi-agent systems; artificial intelligence		When you meet a delivery robot in a narrow street it stops to let you pass. It was built to give you precedence. What happens if you run into a robot that was not trained by or for humans? The existence in our environment of robots which do not abide by human behavioral rules and social systems might sound odd, but is a case we may encounter in the future. In this paper, self-taught robots are artificial embodied agents that, thanks for instance to AI learning techniques, manage to survive in the environment without embracing behavioral or judgment rules given and used by humans. The paper argues that our ontological systems are not suitable to understand and cope with artificial agents. The arguments are speculative rather than empirical, and the goal is to drive attention to new ontological challenges.																	1570-0844	2210-4968					2020	11	1					161	167		10.3233/SW-190385													
J								The Semantic Web: Two decades on	SEMANTIC WEB										Semantic Web; ontologies; Linked Data; knowledge graphs	LINKED DATA; INFERENCE ENGINE; DBPEDIA; SCALE	More than two decades have passed since the establishment of the initial cornerstones of the Semantic Web. Since its inception, opinions have remained divided regarding the past, present and potential future impact of the Semantic Web. In this paper - and in light of the results of over two decades of development on both the Semantic Web and related technologies - we reflect on the current status of the Semantic Web, the impact it has had thus far, and future challenges. We first review some of the external criticism of this vision that has been put forward by various authors; we draw together the individual critiques, arguing both for and against each point based on the current state of adoption. We then present the results of a questionnaire that we have posed to the SemanticWeb mailing list in order to understand respondents' perspective(s) regarding the degree to which the original Semantic Web vision has been realised, the impact it can potentially have on the Web (and other settings), its success stories thus far, as well as the degree to which they agree with the aforementioned critiques of the SemanticWeb in terms of both its current state and future feasibility. We conclude by reflecting on future challenges and opportunities in the area.																	1570-0844	2210-4968					2020	11	1					169	185		10.3233/SW-190387													
J								Using the Semantic Web in digital humanities: Shift from data publishing to data-analysis and serendipitous knowledge discovery	SEMANTIC WEB										Digital Humanities; Linked Data; Semantic portals; Data analysis; knowledge discovery		This paper discusses a shift of focus in research on Cultural Heritage semantic portals, based on Linked Data, and envisions and proposes new directions of research. Three generations of portals are identified: Ten years ago the research focus in semantic portal development was on data harmonization, aggregation, search, and browsing ("first generation systems"). At the moment, the rise of Digital Humanities research has started to shift the focus to providing the user with integrated tools for solving research problems in interactive ways ("second generation systems"). This paper envisions and argues that the next step ahead to "third generation systems" is based on Artificial Intelligence: future portals not only provide tools for the human to solve problems but are used for finding research problems in the first place, for addressing them, and even for solving them automatically under the constraints set by the human researcher. Such systems should preferably be able to explain their reasoning, which is an important aspect in the source critical humanities research tradition. The second and third generation systems set new challenges for both computer scientists and humanities researchers.																	1570-0844	2210-4968					2020	11	1					187	193		10.3233/SW-190386													
J								Machine Learning for the Semantic Web: Lessons learnt and next research directions	SEMANTIC WEB										Machine Learning; symbol-based methods; numeric-based methods		Machine Learning methods have been introduced in the Semantic Web for solving problems such as link and type prediction, ontology enrichment and completion (both at terminological and assertional level). Whilst initially mainly focussing on symbol-based solutions, recently numeric-based approaches have received major attention, motivated by the need to scale on the very large Web of Data. In this paper, the most representative proposals, belonging to the aforementioned categories are surveyed, jointly with the analysis of their main peculiarities and drawbacks. Afterwards the main envisioned research directions for further developing Machine Learning solutions for the Semantic Web are presented.																	1570-0844	2210-4968					2020	11	1					195	203		10.3233/SW-200388													
J								Local Proximity for Enhanced Visibility in Haze	IEEE TRANSACTIONS ON IMAGE PROCESSING										Dehazing; contrast enhancement; patch similarity; underwater	SINGLE IMAGE; UNDERWATER IMAGE; FRAMEWORK; ALGORITHM; CONTRAST; WEATHER; VISION	Atmospheric medium often constrains the visibility of outdoor scenes due to scattering of light rays. This causes attenuation in the irradiance reaching the imaging device along with an additive component to render a hazy effect in the image. The visibility is further reduced for poorly illuminated scenes. The attenuation becomes wavelength dependent in underwater scenario, causing undesired color cast along with hazy effect. In order to suppress the effect of different atmospheric/underwater conditions such as haze and to enhance the contrast of such images, we reformulate local haziness in a generalized manner. The parameters are estimated by harnessing the similarity of patches within a local neighborhood. Unlike existing methods, our approach is developed based on the assumption that for outdoor scenes, the depth of patches changes gradually in a local neighborhood surrounding the patch. This change in depth can be approximated by patch similarity in that neighborhood. As the attenuation in irradiance of an image in presence of atmospheric medium relies on the depth of the scene, the coefficients related to the attenuation are estimated from the weights of patch similarity. The additive haze effect is deduced using non-local mean of the patch. Our experimental results demonstrate the effectiveness of our approach in reducing the haze component as well as in enhancing the image under different conditions of haze (daytime, nighttime, and underwater).																	1057-7149	1941-0042					2020	29						2478	2491		10.1109/TIP.2019.2957931													
J								Fast and Accurate Depth Estimation From Sparse Light Fields	IEEE TRANSACTIONS ON IMAGE PROCESSING										3D reconstruction; depth map; light-field video; multi-view stereo (MVS); superpixel segmentation	BELIEF PROPAGATION; STEREO; PATCHMATCH	We present a fast and accurate method for dense depth reconstruction, which is specifically tailored to process sparse, wide-baseline light field data captured with camera arrays. In our method, the source images are over-segmented into non-overlapping compact superpixels. We model superpixel as planar patches in the image space and use them as basic primitives for depth estimation. Such superpixel-based representation yields desired reduction in both memory and computation requirements while preserving image geometry with respect to the object contours. The initial depth maps, obtained by plane-sweeping independently for each view, are jointly refined via iterative belief-propagation-like optimization in superpixel domain. During the optimization, smoothness between the neighboring superpixels and geometric consistency between the views are enforced. To ensure rapid information propagation into textureless and occluded regions, together with the immediate superpixel neighbors, candidates from larger neighborhoods are sampled. Additionally, in order to make full use of the parallel graphics hardware a synchronous message update schedule is employed allowing to process all the superpixels of all the images at once. This way, the distribution of the scene geometry becomes distinctive already after the first iterations, facilitating stability and fast convergence of the refinement procedure. We demonstrate that a few refinement iterations result in globally consistent dense depth maps even in the presence of wide textureless regions and occlusions. The experiments show that while the depth reconstruction takes about a second per full high-definition view, the accuracy of the obtained depth maps is comparable with the state-of-the-art results, which otherwise require much longer processing time.																	1057-7149	1941-0042					2020	29						2492	2506		10.1109/TIP.2019.2959233													
J								Adaptive Regularization of Some Inverse Problems in Image Analysis	IEEE TRANSACTIONS ON IMAGE PROCESSING										Adaptive regularization; Huber-Huber model; convex optimization; ADMM; segmentation; optical flow; denoising	ILL-POSED PROBLEMS; PARAMETER SELECTION; SMOOTHING PARAMETER; L-CURVE; RESTORATION; SEGMENTATION; SPACE	We present an adaptive regularization scheme for optimizing composite energy functionals arising in image analysis problems. The scheme automatically trades off data fidelity and regularization depending on the current data fit during the iterative optimization, so that regularization is strongest initially, and wanes as data fidelity improves, with the weight of the regularizer being minimized at convergence. We also introduce a Huber loss function in both data fidelity and regularization terms, and present an efficient convex optimization algorithm based on the alternating direction method of multipliers (ADMM) using the equivalent relation between the Huber function and the proximal operator of the one-norm. We illustrate and validate our adaptive Huber-Huber model on synthetic and real images in segmentation, motion estimation, and denoising problems.																	1057-7149	1941-0042					2020	29						2507	2521		10.1109/TIP.2019.2960587													
J								Evaluating Local Geometric Feature Representations for 3D Rigid Data Matching	IEEE TRANSACTIONS ON IMAGE PROCESSING										Performance evaluation; 3D point cloud; local descriptors; feature representation; 3D matching	OBJECT RECOGNITION; REGISTRATION; ALGORITHM	Local geometric descriptors act as an essential component for 3D rigid data matching. A rotational invariant local geometric descriptor usually consists of two components: local reference frame (LRF) and feature representation. However, existing evaluation efforts have mainly been paid on the LRF or the overall descriptor and the quantitative comparison of feature representations remains unexplored. This paper fills the gap by comprehensively evaluating nine state-of-the-art local geometric feature representations. In particular, our evaluation assesses feature representations based on ground-truth LRFs such that the ranking of tested methods is more convincing as compared with existing studies. The experiments are deployed on six standard datasets with various application scenarios (shape retrieval, point cloud registration, and object recognition) and data modalities (LiDAR, Kinect, and Space Time) as well as perturbations including Gaussian noise, shot noise, data decimation, clutter, occlusion, and limited overlap. The evaluated terms cover the major concerns for a feature representation, e.g., distinctiveness, robustness, compactness, and efficiency. The outcomes present interesting findings that may shed new light on this community and provide complementary perspectives to existing evaluations on the topic of local geometric feature description. A summary of evaluated methods regarding their peculiarities is finally presented to guide real-world applications and new descriptor crafting.																	1057-7149	1941-0042					2020	29						2522	2535		10.1109/TIP.2019.2959236													
J								Quality Measurement of Images on Mobile Streaming Interfaces Deployed at Scale	IEEE TRANSACTIONS ON IMAGE PROCESSING										Streaming media; Databases; Distortion; Visualization; Image coding; Transform coding; Image quality assessment (IQA); mobile streaming	STRUCTURAL SIMILARITY	With the growing use of smart cellular devices for entertainment purposes, audio and video streaming services now offer an increasingly wide variety of popular mobile applications that offer portable and accessible ways to consume content. The user interfaces of these applications have become increasingly visual in nature, and are commonly loaded with dense multimedia content such as thumbnail images, animated GIFs, and short videos. To efficiently render these and to aid rapid download to the client display, it is necessary to compress, scale and color subsample them. These operations introduce distortions, reducing the appeal of the application. It is desirable to be able to automatically monitor and govern the visual qualities of these small images, which are usually small images. However, while there exists a variety of high-performing image quality assessment (IQA) algorithms, none have been designed for this particular use case. This kind of content often has unique characteristics, such as overlaid graphics, intentional brightness, gradients, text, and warping. We describe a study we conducted on the subjective and objective quality of images embedded in the displayed user interfaces of mobile streaming applications. We created a database of typical "billboard" and "thumbnail" images viewed on such services. Using the collected data, we studied the effects of compression, scaling and chroma-subsampling on perceived quality by conducting a subjective study. We also evaluated the performance of leading picture quality prediction models on the new database. We report some surprising results regarding algorithm performance, and find that there remains ample scope for future model development.																	1057-7149	1941-0042					2020	29						2536	2551		10.1109/TIP.2019.2939733													
J								Deep Retinal Image Segmentation With Regularization Under Geometric Priors	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image segmentation; Task analysis; Retinal vessels; Deep learning; Training; Feature extraction; Retinal images; deep learning; priors	BLOOD-VESSEL SEGMENTATION; FUNDUS IMAGES; MATCHED-FILTER; MODEL; DELINEATION; EXTRACTION; SELECTION	Vessel segmentation of retinal images is a key diagnostic capability in ophthalmology. This problem faces several challenges including low contrast, variable vessel size and thickness, and presence of interfering pathology such as micro-aneurysms and hemorrhages. Early approaches addressing this problem employed hand-crafted filters to capture vessel structures, accompanied by morphological post-processing. More recently, deep learning techniques have been employed with significantly enhanced segmentation accuracy. We propose a novel domain enriched deep network that consists of two components: 1) a representation network that learns geometric features specific to retinal images, and 2) a custom designed computationally efficient residual task network that utilizes the features obtained from the representation layer to perform pixel-level segmentation. The representation and task networks are jointly learned for any given training set. To obtain physically meaningful and practically effective representation filters, we propose two new constraints that are inspired by expected prior structure on these filters: 1) orientation constraint that promotes geometric diversity of curvilinear features, and 2) a data adaptive noise regularizer that penalizes false positives. Multi-scale extensions are developed to enable accurate detection of thin vessels. Experiments performed on three challenging benchmark databases under a variety of training scenarios show that the proposed prior guided deep network outperforms state of the art alternatives as measured by common evaluation metrics, while being more economical in network size and inference time.																	1057-7149	1941-0042					2020	29						2552	2567		10.1109/TIP.2019.2946078													
J								PML-LocNet: Improving Object Localization With Prior-Induced Multi-View Learning Network	IEEE TRANSACTIONS ON IMAGE PROCESSING										Training; Optimization; Reliability; Detectors; Noise measurement; Computer architecture; Object detection; Image representation; object detection; semi-supervised learning		This paper introduces a new model for Weakly Supervised Object Localization (WSOL) problems where only image-level supervision is provided. The key to solve such problems is to infer the object locations accurately. Previous methods usually model the missing object locations as latent variables, and alternate between updating their estimates and learning a detector accordingly. However, the performance of such alternative optimization is sensitive to the quality of the initial latent variables and the resulted localization model is prone to overfitting to improper localizations. To address these issues, we develop a Prior-induced Multi-view Learning Localization Network (PML-LocNet) which exploits both view diversity and sample diversity to improve object localization. In particular, the view diversity is imposed by a two-phase multi-view learning strategy, with which the complementarity among learned features from different views and the consensus among localized instances from each view are leveraged to benefit localization. The sample diversity is pursued by harnessing coarse-to-fine priors at both image and instance levels. With these priors, more emphasis would go to the reliable samples and the contributions of the unreliable ones would be decreased, such that the intrinsic characteristics of each sample can be exploited to make the model more robust during network learning. PML-LocNet can be easily combined with existing WSOL models to further improve the localization accuracy. Its effectiveness has been proved experimentally. Notably, it achieves 69.3 CorLoc and 50.4 mAP on PASCAL VOC 2007, surpassing the state-of-the-arts by a large margin.																	1057-7149	1941-0042					2020	29						2568	2582		10.1109/TIP.2019.2947155													
J								Accurate Transmission Estimation for Removing Haze and Noise From a Single Image	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image dehazing; denoising; transmission-aware regularization; semantic segmentation		Image noise usually causes depth-dependent visual artifacts in single image dehazing. Most existing dehazing methods exploit a two-step strategy in the restoration, which inevitably leads to inaccurate transmission maps and low-quality scene radiance for noisy and hazy inputs. To address these problems, we present a novel variational model for joint recovery of the transmission map and the scene radiance from a single image. In the model, we propose a transmission-aware non-local regularization to avoid noise amplification by adaptively suppressing noise and preserving fine details in the recovered image. Meanwhile, to improve the accuracy of transmission estimation, we introduce a semantic-guided regularization to smooth out the transmission map while keeping depth inconsistency at the boundaries of different objects. Furthermore, we design an alternating scheme to jointly optimize the transmission map and the scene radiance as well as the segmentation map. Extensive experiments on synthetic and real-world data demonstrate that the proposed algorithm performs favorably against state-of-the-art dehazing methods on noisy and hazy images.																	1057-7149	1941-0042					2020	29						2583	2597		10.1109/TIP.2019.2949392													
J								Super-Resolution Phase Retrieval From Designed Coded Diffraction Patterns	IEEE TRANSACTIONS ON IMAGE PROCESSING										Diffraction; Apertures; Optical diffraction; Optical imaging; Super-resolution; phase retrieval; smoothed function; coded aperture design; coded diffraction pattern	CRYSTALLOGRAPHY; RECOVERY	Super-resolution phase retrieval is an inverse problem that appears in diffractive optical imaging (DOI) and consists in estimating a high-resolution image from low-resolution phaseless measurements. DOI has three diffraction zones where the data can be acquired, known as near, middle, and far fields. Recent works have studied super-resolution phase retrieval under a setup that records coded diffraction patterns at the near and far fields. However, the attainable resolution of the image is mainly governed by the sensor characteristics, whose cost increases in proportion to the resolution. Also, these methodologies lack theoretical analysis. Hence, this work derives super-resolution models from low-resolution coded phaseless measurements at any diffraction zone that in contrast to prior contributions, the attainable resolution of the image is determined by the resolution of the coded aperture. For the proposed models, the existence of a unique solution (up to a global unimodular constant) is guaranteed with high probability, which can be increased by designing the coded aperture. Therefore, a strategy that designs the spatial distribution of the coded aperture is developed. Additionally, a super-resolution phase retrieval algorithm that minimizes a smoothed nonconvex least-squares objective function is proposed. The method first approximates the image by a spectral algorithm, which is then refined based upon a sequence of alternate steps. Simulation results show that the proposed algorithm overcomes state-of-the-art methods in reconstructing the high-resolution image. In addition, the reconstruction quality using designed coded apertures is higher than that of the non-designed ensembles.																	1057-7149	1941-0042					2020	29						2598	2609		10.1109/TIP.2019.2949436													
J								Distilling Channels for Efficient Deep Tracking	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image coding; Visualization; Correlation; Feature extraction; Adaptation models; Benchmark testing; Minimization; Visual tracking; deep tracking; channel distillation; CNNs	OBJECT TRACKING	Deep trackers have proven success in visual tracking. Typically, these trackers employ optimally pre-trained deep networks to represent all diverse objects with multi-channel features from some fixed layers. The deep networks employed are usually trained to extract rich knowledge from massive data used in object classification and so they are capable to represent generic objects very well. However, these networks are too complex to represent a specific moving object, leading to poor generalization as well as high computational and memory costs. This paper presents a novel and general framework termed channel distillation to facilitate deep trackers. To validate the effectiveness of channel distillation, we take discriminative correlation filter (DCF) and ECO for example. We demonstrate that an integrated formulation can turn feature compression, response map generation, and model update into a unified energy minimization problem to adaptively select informative feature channels that improve the efficacy of tracking moving objects on the fly. Channel distillation can accurately extract good channels, alleviating the influence of noisy channels and generally reducing the number of channels, as well as adaptively generalizing to different channels and networks. The resulting deep tracker is accurate, fast, and has low memory requirements. Extensive experimental evaluations on popular benchmarks clearly demonstrate the effectiveness and generalizability of our framework.																	1057-7149	1941-0042					2020	29						2610	2621		10.1109/TIP.2019.2950508													
J								Improved Techniques for Adversarial Discriminative Domain Adaptation	IEEE TRANSACTIONS ON IMAGE PROCESSING										Task analysis; Training; Gallium nitride; Sensors; Proposals; Cameras; Neuromorphics; Adversarial methods; domain adaptation; neuromorphic vision sensing		Adversarial discriminative domain adaptation (ADDA) is an efficient framework for unsupervised domain adaptation in image classification, where the source and target domains are assumed to have the same classes, but no labels are available for the target domain. While ADDA has already achieved better training efficiency and competitive accuracy on image classification in comparison to other adversarial based methods, we investigate whether we can improve its performance with a new framework and new loss formulations. Following the framework of semi-supervised GANs, we first extend the discriminator output over the source classes, in order to model the joint distribution over domain and task. We thus leverage on the distribution over the source encoder posteriors (which is fixed during adversarial training) and propose maximum mean discrepancy (MMD) and reconstruction-based loss functions for aligning the target encoder distribution to the source domain. We compare and provide a comprehensive analysis of how our framework and loss formulations extend over simple multi-class extensions of ADDA and other discriminative variants of semi-supervised GANs. In addition, we introduce various forms of regularization for stabilizing training, including treating the discriminator as a denoising autoencoder and regularizing the target encoder with source examples to reduce overfitting under a contraction mapping (i.e., when the target per-class distributions are contracting during alignment with the source). Finally, we validate our framework on standard datasets like MNIST, USPS, SVHN, MNIST-M and Office-31. We additionally examine how the proposed framework benefits recognition problems based on sensing modalities that lack training data. This is realized by introducing and evaluating on a neuromorphic vision sensing (NVS) sign language recognition dataset, where the source domain constitutes emulated neuromorphic spike events converted from conventional pixel-based video and the target domain is experimental (real) spike events from an NVS camera. Our results on all datasets show that our proposal is both simple and efficient, as it competes or outperforms the state-of-the-art in unsupervised domain adaptation, such as DIFA and MCDDA, whilst offering lower complexity than other recent adversarial methods.																	1057-7149	1941-0042					2020	29						2622	2637		10.1109/TIP.2019.2950768													
J								Group-Group Loss-Based Global-Regional Feature Learning for Vehicle Re-Identification	IEEE TRANSACTIONS ON IMAGE PROCESSING										Vehicle re-identification; CNN; global-regional feature learning; distance metric learning		Vehicle Re-Identification (Re-ID) is challenging because vehicles of the same model commonly show similar appearance. We tackle this challenge by proposing a Global-Regional Feature (GRF) that depicts extra local details to enhance discrimination power in addition to the global context. It is motivated by the observation that, vehicles of same color, maker, and model can be distinguished by their regional difference, e.g., the decorations on the windshields. To accelerate the GRF learning and promote its discrimination power, we propose a Group-Group Loss (GGL) to optimize the distance within and across vehicle image groups. Different from the siamese or triplet loss, GGL is directly computed on image groups rather than individual sample pairs or triplets. By avoiding traversing numerous sample combinations, GGL makes the model training easier and more efficient. Those two contributions highlight this work from previous methods on vehicle Re-ID task, which commonly learn global features with triplet loss or its variants. We evaluate our methods on two large-scale vehicle Re-ID datasets, i.e., VeRi and VehicleID. Experimental results show our methods achieve promising performance in comparison with recent works.																	1057-7149	1941-0042					2020	29						2638	2652		10.1109/TIP.2019.2950796													
J								Color Channel Compensation (3C): A Fundamental Pre-Processing Step for Image Enhancement	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image enhancement; dehazing; color-constancy	CONSTANCY	This article introduces a novel solution to improve image enhancement in terms of color appearance. Our approach, called Color Channel Compensation (3C), overcomes artifacts resulting from the severely non-uniform color spectrum distribution encountered in images captured under hazy night-time conditions, underwater, or under non-uniform artificial illumination. Our solution is founded on the observation that, under such adverse conditions, the information contained in at least one color channel is close to completely lost, making the traditional enhancing techniques subject to noise and color shifting. In those cases, our pre-processing method proposes to reconstruct the lost channel based on the opponent color channel. Our algorithm subtracts a local mean from each opponent color pixel. Thereby, it partly recovers the lost color from the two colors (red-green or blue-yellow) involved in the opponent color channel. The proposed approach, whilst simple, is shown to consistently improve the outcome of conventional restoration methods. To prove the utility of our 3C operator, we provide an extensive qualitative and quantitative evaluation for white balancing, image dehazing, and underwater enhancement applications.																	1057-7149	1941-0042					2020	29						2653	2665		10.1109/TIP.2019.2951304													
J								Fast Online 3D Reconstruction of Dynamic Scenes From Individual Single-Photon Detection Events	IEEE TRANSACTIONS ON IMAGE PROCESSING										3D reconstruction; single-photon Lidar; Bayesian filtering; online estimation	LIDAR; SENSOR	In this paper, we present an algorithm for online 3D reconstruction of dynamic scenes using individual times of arrival (ToA) of photons recorded by single-photon detector arrays. One of the main challenges in 3D imaging using single-photon Lidar is the integration time required to build ToA histograms and reconstruct reliably 3D profiles in the presence of non-negligible ambient illumination. This long integration time also prevents the analysis of rapid dynamic scenes using existing techniques. We propose a new method which does not rely on the construction of ToA histograms but allows, for the first time, individual detection events to be processed online, in a parallel manner in different pixels, while accounting for the intrinsic spatiotemporal structure of dynamic scenes. Adopting a Bayesian approach, a Bayesian model is constructed to capture the dynamics of the 3D profile and an approximate inference scheme based on assumed density filtering is proposed, yielding a fast and robust reconstruction algorithm able to process efficiently thousands to millions of frames, as usually recorded using single-photon detectors. The performance of the proposed method, able to process hundreds of frames per second, is assessed using a series of experiments conducted with static and dynamic 3D scenes and the results obtained pave the way to a new family of real-time 3D reconstruction solutions.																	1057-7149	1941-0042					2020	29						2666	2675		10.1109/TIP.2019.2952008													
J								Learning No-Reference Quality Assessment of Multiply and Singly Distorted Images With Big Data	IEEE TRANSACTIONS ON IMAGE PROCESSING										Distortion; Feature extraction; Image coding; Transform coding; Image quality; Prediction algorithms; Databases; No reference quality assessment; multiple distortions; distortion parameter estimation; contrast change	GRADIENT MAGNITUDE; SIMILARITY; MASKING	Previous research on no-reference (NR) quality assessment of multiply-distorted images focused mainly on three distortion types (white noise, Gaussian blur, and JPEG compression), while in practice images can be contaminated by many other common distortions due to the various stages of processing. Although MUSIQUE (MUltiply- and Singly-distorted Image QUality Estimator) [Zhang et al., TIP 2018] is a successful NR algorithm, this approach is still limited to the three distortion types. In this paper, we extend MUSIQUE to MUSIQUE-II to blindly assess the quality of images corrupted by five distortion types (white noise, Gaussian blur, JPEG compression, JPEG2000 compression, and contrast change) and their combinations. The proposed MUSIQUE-II algorithm builds upon the classification and parameter-estimation framework of its predecessor by using more advanced models and a more comprehensive set of distortion-sensitive features. Specifically, MUSIQUE-II relies on a three-layer classification model to identify 19 distortion types. To predict the five distortion parameter values, MUSIQUE-II extracts an additional 14 contrast features and employs a multi-layer probability-weighting rule. Finally, MUSIQUE-II employs a new most-apparent-distortion strategy to adaptively combine five quality scores based on outputs of three classification models. Experimental results tested on three multiply-distorted and six singly-distorted image quality databases show that MUSIQUE-II yields not only a substantial improvement in quality predictive performance as compared with its predecessor, but also highly competitive performance relative to other state-of-the-art FR/NR IQA algorithms.																	1057-7149	1941-0042					2020	29						2676	2691		10.1109/TIP.2019.2952010													
J								Unsupervised Single Image Dehazing Using Dark Channel Prior Loss	IEEE TRANSACTIONS ON IMAGE PROCESSING										Energy functions; deep neural networks; unsupervised learning; single image dehazing; dark channel prior	VISION	Single image dehazing is a critical stage in many modern-day autonomous vision applications. Early prior-based methods often involved a time-consuming minimization of a hand-crafted energy function. Recent learning-based approaches utilize the representational power of deep neural networks (DNNs) to learn the underlying transformation between hazy and clear images. Due to inherent limitations in collecting matching clear and hazy images, these methods resort to training on synthetic data, constructed from indoor images and corresponding depth information. This may result in a possible domain shift when treating outdoor scenes. We propose a completely unsupervised method of training via minimization of the well-known, Dark Channel Prior (DCP) energy function. Instead of feeding the network with synthetic data, we solely use real-world outdoor images and tune the network's parameters by directly minimizing the DCP. Although our "Deep DCP" technique can be regarded as a fast approximator of DCP, it actually improves its results significantly. This suggests an additional regularization obtained via the network and learning process. Experiments show that our method performs on par with large-scale supervised methods.																	1057-7149	1941-0042					2020	29						2692	2701		10.1109/TIP.2019.2952032													
J								High-Order Feature Learning for Multi-Atlas Based Label Fusion: Application to Brain Segmentation With MRI	IEEE TRANSACTIONS ON IMAGE PROCESSING										High-order features; multi-atlas; ROI segmentation	IMAGE REGISTRATION; HIPPOCAMPUS; MODEL; REPRESENTATION; PREDICTION; ALGORITHM; SELECTION; SYSTEM; TRUTH	Multi-atlas based segmentation methods have shown their effectiveness in brain regions-of-interesting (ROIs) segmentation, by propagating labels from multiple atlases to a target image based on the similarity between patches in the target image and multiple atlas images. Most of the existing multi-atlas based methods use image intensity features to calculate the similarity between a pair of image patches for label fusion. In particular, using only low-level image intensity features cannot adequately characterize the complex appearance patterns (e.g., the high-order relationship between voxels within a patch) of brain magnetic resonance (MR) images. To address this issue, this paper develops a high-order feature learning framework for multi-atlas based label fusion, where high-order features of image patches are extracted and fused for segmenting ROIs of structural brain MR images. Specifically, an unsupervised feature learning method (i.e., means-covariances restricted Boltzmann machine, mcRBM) is employed to learn high-order features (i.e., mean and covariance features) of patches in brain MR images. Then, a group-fused sparsity dictionary learning method is proposed to jointly calculate the voting weights for label fusion, based on the learned high-order and the original image intensity features. The proposed method is compared with several state-of-the-art label fusion methods on ADNI, NIREP and LONI-LPBA40 datasets. The Dice ratio achieved by our method is 88.30, 88.83, 79.54 and 81.02 on left and right hippocampus on the ADNI, NIREP and LONI-LPBA40 datasets, respectively, while the best Dice ratio yielded by the other methods are 86.51, 87.39, 78.48 and 79.65 on three datasets, respectively.																	1057-7149	1941-0042					2020	29						2702	2713		10.1109/TIP.2019.2952079													
J								PaDNet: Pan-Density Crowd Counting	IEEE TRANSACTIONS ON IMAGE PROCESSING										Crowd counting; density level analysis; pan-density evaluation; convolutional neural networks	PARTIALLY OCCLUDED HUMANS; BAYESIAN COMBINATION; MULTIPLE; IMAGE	Crowd counting is a highly challenging problem in computer vision and machine learning. Most previous methods have focused on consistent density crowds, i.e., either a sparse or a dense crowd, meaning they performed well in global estimation while neglecting local accuracy. To make crowd counting more useful in the real world, we propose a new perspective, named pan-density crowd counting, which aims to count people in varying density crowds. Specifically, we propose the Pan-Density Network (PaDNet) which is composed of the following critical components. First, the Density-Aware Network (DAN) contains multiple subnetworks pretrained on scenarios with different densities. This module is capable of capturing pan-density information. Second, the Feature Enhancement Layer (FEL) effectively captures the global and local contextual features and generates a weight for each density-specific feature. Third, the Feature Fusion Network (FFN) embeds spatial context and fuses these density-specific features. Further, the metrics Patch MAE (PMAE) and Patch RMSE (PRMSE) are proposed to better evaluate the performance on the global and local estimations. Extensive experiments on four crowd counting benchmark datasets, the ShanghaiTech, the UCFCC50, the UCSD, and the UCF-QNRF, indicate that PaDNet achieves state-of-the-art recognition performance and high robustness in pan-density crowd counting.																	1057-7149	1941-0042					2020	29						2714	2727		10.1109/TIP.2019.2952083													
J								MAVA: Multi-Level Adaptive Visual-Textual Alignment by Cross-Media Bi-Attention Mechanism	IEEE TRANSACTIONS ON IMAGE PROCESSING										Correlation; Visualization; Media; Semantics; Deep learning; Adaptation models; Games; Cross-media multi-pathway fine-grained network; visual-textual bi-attention mechanism; cross-media multi-level adaptive alignment		The rapidly developing information technology leads to a fast growth of visual and textual contents, and it comes with huge challenges to make correlation and perform cross-media retrieval between images and sentences. Existing methods mainly explore cross-media correlation from either global-level instances as the whole images and sentences, or local-level fine-grained patches as the discriminative image regions and key words, which ignore the complementary information from the relation between local-level fine-grained patches. Naturally, relation understanding is highly important for learning cross-media correlation. People focus on not only the alignment between discriminative image regions and key words, but also their relations lying in the visual and textual context. Therefore, in this paper, we propose Multi-level Adaptive Visual-textual Alignment (MAVA) approach with the following contributions. First, we propose cross-media multi-pathway fine-grained network to extract not only the local fine-grained patches as discriminative image regions and key words, but also visual relations between image regions as well as textual relations from the context of sentences, which contain complementary information to exploit fine-grained characteristics within different media types. Second, we propose visual-textual bi-attention mechanism to distinguish the fine-grained information with different saliency from both local and relation levels, which can provide more discriminative hints for correlation learning. Third, we propose cross-media multi-level adaptive alignment to explore global, local and relation alignments. An adaptive alignment strategy is further proposed to enhance the matched pairs of different media types, and discard those misalignments adaptively to learn more precise cross-media correlation. Extensive experiments are conducted to perform image-sentence matching on 2 widely-used cross-media datasets, namely Flickr-30K and MS-COCO, comparing with 10 state-of-the-art methods, which can fully verify the effectiveness of our proposed MAVA approach.																	1057-7149	1941-0042					2020	29						2728	2741		10.1109/TIP.2019.2952085													
