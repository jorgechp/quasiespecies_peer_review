PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	RP	EM	RI	OI	FU	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	D2	EA	PG	WC	SC	GA	UT	PM	OA	HC	HP	DA
J								On the recognition of Devanagari ancient handwritten characters using SIFT and Gabor features	SOFT COMPUTING										Ancient manuscripts; Devanagari historical documents; SIFT; Gabor; SVM		Recognition of Devanagari ancient handwritten character is an important task for resourceful contents' exploitation of the priceless information contained in them. There are numerous Devanagari ancient handwritten documents from fifteenth to the nineteenth century. This paper presents an optical character recognition system for the recognition of Devanagari ancient manuscripts. In this paper, improved recognition results for Devanagari ancient characters have been presented using the scale-invariant feature transform (SIFT) and Gabor filter feature extraction techniques. Support vector machine (SVM) classifier is used for the classification task in this work. For experimental results, a database consisting of 5484 samples of Devanagari characters was collected from various ancient manuscripts placed in libraries and museums. SIFT- and Gabor filter-based features are used to extract the properties of the handwritten Devanagari ancient characters for recognition. Principle component analysis is used to reduce the length of the feature vector for reducing training time of the model and to improve recognition accuracy. Recognition accuracy of 91.39% has been achieved using the proposed system based on tenfold cross-validation technique and poly-SVM classifier.																	1432-7643	1433-7479				NOV	2020	24	22					17279	17289		10.1007/s00500-020-05018-z		MAY 2020											
J								Fuzzy hyperconnected proximity spaces and fuzzy summability over CW complexes. Application of Smirnov fuzzy similarity in video analysis	SOFT COMPUTING										Cell complex; Closure finite weak topology; Fuzzy Hyperconnected proximity spaces; Fuzzy summability; Smirnov fuzzy similarity; Video analysis		This article introduces proximal cell complexes in a hyperconnected space. Hyperconnectedness encodes how collections of path-connected sub-complexes in a Alexandroff-Hopf-Whitehead CW space are near to or far from each other. Several main results are given, namely a hyperconnectedness form of CW (Closure Finite Weak topology) complex, the existence of continuous functions that are paths in hyperconnected relator spaces and hyperconnected chains with overlapping interiors that are path graphs in a relator space. The centroids of surface holes in an image are used as seed points for the triangulation. An application of these results to the definition of cycles using the centroids of triangles is given.																	1432-7643	1433-7479															10.1007/s00500-020-04945-1		MAY 2020											
J								Healthcare and anomaly detection: using machine learning to predict anomalies in heart rate data	AI & SOCIETY										Anomaly detection; Healthcare; Heart rate; Machine learning	INTRUSION DETECTION; NETWORKS; SYSTEMS	The application of machine learning algorithms to healthcare data can enhance patient care while also reducing healthcare worker cognitive load. These algorithms can be used to detect anomalous physiological readings, potentially leading to expedited emergency response or new knowledge about the development of a health condition. However, while there has been much research conducted in assessing the performance of anomaly detection algorithms on well-known public datasets, there is less conceptual comparison across unsupervised and supervised performance on physiological data. Moreover, while heart rate data are both ubiquitous and noninvasive, there has been little research specifically for anomaly detection of this type of data. Considering that heart rate data are indicative of both potential health complications and an individual's physical activity, this is a rich source of largely overlooked data. To this end, we employed and evaluated five machine learning algorithms, two of which are unsupervised and the remaining three supervised, in their ability to detect anomalies in heart rate data. These algorithms were then evaluated on real heart rate data. Findings supported the effectiveness of local outlier factor and random forests algorithms in the task of heart rate anomaly detection, as each model generalized well from their training on simulated heart rate data to real world heart rate data. Furthermore, results support that simulated data can help configure algorithms to a degree of performance when real labeled data are not available and that this type of learning might be especially helpful in initial deployment of a system without prior data.																	0951-5666	1435-5655															10.1007/s00146-020-00985-1		MAY 2020											
J								Socially responsive technologies: toward a co-developmental path	AI & SOCIETY										Artificial intelligence; Human-computer interaction; Human-robot interaction; Moral responsibility; Blame; Social responsiveness	ROBOTS; BEHAVIOR; CHILDREN	Robotic and artificially intelligent (AI) systems are becoming prevalent in our day-to-day lives. As human interaction is increasingly replaced by human-computer and human-robot interaction (HCI and HRI), we occasionally speak and act as though we are blaming or praising various technological devices. While such responses may arise naturally, they are still unusual. Indeed, for some authors, it is the programmers or users-and not the system itself-that we properly hold responsible in these cases. Furthermore, some argue that since directing blame or praise at technology itself is unfitting, designing systems in ways that encourage such practices can only exacerbate the problem. On the other hand, there may be good moral reasons to continue engaging in our natural practices, even in cases involving AI systems or robots. In particular, daily interactions with technology may stand to impact the development of our moral practices in human-to-human interactions. In this paper, we put forward an empirically grounded argument in favor of some technologies being designed for social responsiveness. Although our usual practices will likely undergo adjustments in response to innovative technologies, some systems which we encounter can be designed to accommodate our natural moral responses. In short, fostering HCI and HRI that sustains and promotes our natural moral practices calls for a co-developmental process with some AI and robotic technologies.																	0951-5666	1435-5655															10.1007/s00146-020-00982-4		MAY 2020											
J								Leveraging cluster backbones for improving MAP inference in statistical relational models	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Maximum-a-posteriori inference; Markov logic network; Survey propagation	BELIEF-PROPAGATION; LOCAL COMPUTATIONS; MARKOV; ALGORITHM; PHASE	A wide range of important problems in machine learning, expert system, social network analysis, bioinformatics and information theory can be formulated as a maximum a-posteriori (MAP) inference problem on statistical relational models. While off-the-shelf inference algorithms that are based on local search and message-passing may provide adequate solutions in some situations, they frequently give poor results when faced with models that possess high-density networks. Unfortunately, these situations always occur in models of real-world applications. As such, accurate and scalable maximum a-posteriori (MAP) inference on such models often remains a key challenge. In this paper, we first introduce a novel family of extended factor graphs that are parameterized by a smoothing parameter chi is an element of [0,1]. Applying belief propagation (BP) message-passing to this family formulates a new family of W eighted S urvey P ropagation algorithms (WSP-chi) applicable to relational domains. Unlike off-the-shelf inference algorithms, WSP-chi detects the "backbone" ground atoms in a solution cluster that involve potentially optimal MAP solutions: the cluster backbone atoms are not only portions of the optimal solutions, but they also can be exploited for scaling MAP inference by iteratively fixing them to reduce the complex parts until the network is simplified into one that can be solved accurately using any conventional MAP inference method. We also propose a lazy variant of this WSP-chi family of algorithms. Our experiments on several real-world problems show the efficiency of WSP-chi and its lazy variants over existing prominent MAP inference solvers such as MaxWalkSAT, RockIt, IPP, SP-Y and WCSP.																	1012-2443	1573-7470				AUG	2020	88	8					907	949		10.1007/s10472-020-09698-z		MAY 2020											
J								A novel approach for multi-cue feature fusion for robust object tracking	APPLIED INTELLIGENCE										Object tracking; Feature fusion; Classifier; Reliability	VISUAL TRACKING; DISCRIMINATIVE TRACKING; CONTEXT; SCALE	Object tracking is a significant problem of computer vision due to challenging environmental variations. Single cue appearance model is not sufficient to handle the variations. To this end, we propose a multi-cue tracking framework in which complementary cues namely, LBP and HOG were exploited to develop a robust appearance model. The proposed feature fusion captures the high-level relationship between the features and diminishes the low-level relationship. Transductive reliability is also integrated at each frame to make tracker adaptive with the changing environment. In addition, K-Means based classifier creates clear and concise boundary between positive and negative fragments which are further used to update the reference dictionary. This adaptation strategy prevents the erroneous updation of the proposed tracker during background clutters, occlusion, and fast motion. Qualitative and quantitative analysis on challenging video sequences from OTB-100 dataset, VOT dataset and UAV123 reveal that the proposed tracker performs favorably against 13 others state-of-the-art trackers.																	0924-669X	1573-7497				OCT	2020	50	10					3201	3218		10.1007/s10489-020-01649-9		MAY 2020											
J								Matrix profile goes MAD: variable-length motif and discord discovery in data series	DATA MINING AND KNOWLEDGE DISCOVERY										Data series; Time series; Motif discovery; Variable length; Data mining	UNUSUAL TIME-SERIES; SIMILARITY SEARCH; VALMOD	In the last 15 years, data series motif and discord discovery have emerged as two useful and well-used primitives for data series mining, with applications to many domains, including robotics, entomology, seismology, medicine, and climatology. Nevertheless, the state-of-the-art motif and discord discovery tools still require the user to provide the relative length. Yet, in several cases, the choice of length is critical and unforgiving. Unfortunately, the obvious brute-force solution, which tests all lengths within a given range, is computationally untenable. In this work, we introduce a new framework, which provides an exact and scalable motif and discord discovery algorithm that efficiently finds all motifs and discords in a given range of lengths. We evaluate our approach with five diverse real datasets, and demonstrate that it is up to 20 times faster than the state-of-the-art. Our results also show that removing the unrealistic assumption that the user knows the correct length, can often produce more intuitive and actionable results, which could have otherwise been missed.																	1384-5810	1573-756X				JUL	2020	34	4					1022	1071		10.1007/s10618-020-00685-w		MAY 2020											
J								TOPSIS-2NE's Proposal	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										TOPSIS; TOPSIS-2N; Multicriteria decision aid; TOPSIS-2NE		This document presents a proposal of structuring the Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS) twice normalized. Instead of using Euclidean distances, TOPSIS was reckoned by the ellipse length because it was necessary to observe a distance without the basic assumption about to know the way where the "pulse" might go it. The dataset was observed by other works without grouping, as it was possible to observe the distances towards the decision occurred and also the concentration of alternatives at same ranked place, forming a kind of "group" at this proposal.																	1562-2479	2199-3211				JUN	2020	22	4					1118	1122		10.1007/s40815-020-00871-4		MAY 2020											
J								Fast feature selection for interval-valued data through kernel density estimation entropy	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Kernel density estimation; Entropy; Feature selection; Kernel function; Interval-valued decision table	ATTRIBUTE SELECTION; ROUGH; UNCERTAINTY; SYSTEMS	Kernel density estimation, which is a non-parametric method about estimating probability density distribution of random variables, has been used in feature selection. However, existing feature selection methods based on kernel density estimation seldom consider interval-valued data. Actually, interval-valued data exist widely. In this paper, a feature selection method based on kernel density estimation for interval-valued data is proposed. Firstly, the kernel function in kernel density estimation is defined for interval-valued data. Secondly, the interval-valued kernel density estimation probability structure is constructed by the defined kernel function, including kernel density estimation conditional probability, kernel density estimation joint probability and kernel density estimation posterior probability. Thirdly, kernel density estimation entropies for interval-valued data are proposed by the constructed probability structure, including information entropy, conditional entropy and joint entropy of kernel density estimation. Fourthly, we propose a feature selection approach based on kernel density estimation entropy. Moreover, we improve the proposed feature selection algorithm and propose a fast feature selection algorithm based on kernel density estimation entropy. Finally, comparative experiments are conducted from three perspectives of computing time, intuitive identifiability and classification performance to show the feasibility and the effectiveness of the proposed method.																	1868-8071	1868-808X				DEC	2020	11	12					2607	2624		10.1007/s13042-020-01131-5		MAY 2020											
J								Multimodal visual image processing of mobile robot in unstructured environment based on semi-supervised multimodal deep network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Mobile robots; Image processing algorithms; Deep learning; Multimodal deep network; Feature fusion	FUSION; SYSTEM	With the continuous development of computer technology, machine vision and image processing algorithms, people's research on mobile robots with vision systems is becoming deeper and deeper. This paper studies the related problems of visual image processing of mobile robots in outdoor unstructured environments. In this work, we propose a new approach that integrates heterogeneous features through a well-designed Semi-supervised multimodal deep network (SMMDN). For each modality, there is a multi-layer sub-neural network with a separate structure corresponding to it, which is used to transform features in different modes into the same modal features. At the same time, through a network layer common to all modes above these sub-neural networks, a connection is established between these different modes, and finally a plurality of heterogeneous modes is converted into the same mode and a plurality of them are extracted from fusion characteristics of data modalities. The simulation results prove that SMMDN improves the perception and recognition ability of mobile robots for outdoor complex environments.																	1868-5137	1868-5145															10.1007/s12652-020-02037-4		MAY 2020											
J								Discovery of evolving companion from trajectory data streams	KNOWLEDGE AND INFORMATION SYSTEMS										Spatial-temporal data; Group pattern discovery; Trajectory data stream; Moving object clustering	ALGORITHM; PATTERNS	The widespread use of position-tracking devices leads to vast volumes of spatial-temporal data aggregated in the form of the trajectory data streams. Extracting useful knowledge from moving object trajectories can benefit many applications, such as traffic monitoring, military surveillance, and weather forecasting. Most of the knowledge gleaned from the trajectory data illustrates different kinds of group patterns, i.e., objects that travel together for some time. In the real world, the trajectory of the moving objects can change with time. Thus, existing approaches can miss a new pattern because they have a stringent requirement for moving object participators in a group movement pattern. To address this issue, we introduced a new type of moving object group pattern called an evolving companion. It allows some members of the group to leave and join anytime if some participators stay connected for all time intervals. In this pattern discovery, we model an incremental discovery solution to retrieve the evolving companion efficiently over the data stream. We evaluated the efficiency and effectiveness of our approach on two real vehicles and one synthetic dataset. Our method performed well compared with existing pattern discovery methods; for example, it was about 50% faster than Tang et al.'s buddy-based clustering method.																	0219-1377	0219-3116				SEP	2020	62	9					3509	3533		10.1007/s10115-020-01471-2		MAY 2020											
J								Finite-Time Synchronization of Hybrid-Coupled Delayed Dynamic Networks via Aperiodically Intermittent Control	NEURAL PROCESSING LETTERS										Delayed dynamic networks; Finite-time synchronization; Transmission delay; Self-feedback delay; Aperiodically intermittent control	CHAOS SYNCHRONIZATION; COMPLEX NETWORKS; NEURAL-NETWORKS; SYSTEMS	In this paper, we investigate finite-time synchronization for a class of delayed dynamic networks with hybrid coupling via aperiodically intermittent control. First, more general models of dynamic networks with transmission delay and self-feedback delay are given. Second, a lemma with a newly added parameter is proposed to ensure finite-time synchronization of dynamic networks via an aperiodically intermittent control scheme, in which the newly added parameter can make the convergence time shorter. Third, by constructing a novel piecewise Lyapunov function and applying linear matrix inequality technique, some sufficient conditions ensuring finite-time synchronization for delayed dynamic networks are obtained. Moreover, the convergence time is affected by some decision parameters besides the newly added parameter, one of which is a maximum uncontrolled ratio generated by the definition of aperiodic intermittent control itself. Finally, a numerical example is presented to verify its validity and rationality.																	1370-4621	1573-773X				AUG	2020	52	1			SI		291	311		10.1007/s11063-020-10245-4		MAY 2020											
J								Multimodal human eye blink recognition method using feature level fusion for exigency detection	SOFT COMPUTING										Multimodal; Unimodal; Feature level fusion; Eye blink; Gabor filter	PALMPRINT; FACE; ROBUST; IRIS	In this paper, a precise multimodal eye blink recognition method using feature level fusion (MmERMFLF) is proposed. A new feature: eye-eyebrow facet ratio (EEBFR) (formed by fusing eye facet ratio (EFR: ratio of diagonal length and width of eye) and eyebrow to nose facet ratio (EBNFR: distance between eyebrow landmarks and nose landmark)) for approximating the eye state is computed. Initially, an improved intellectual framework (Sagacious Information Recuperation Technique) that senses the emergency state using information retrieved from eye blinks, pulse rate as well as behavioral patterns(emotions) exhibited by an individual is presented. Further a novel multimodal method (MmERMFLF) for detection and counting of eye blinks is implemented. For training, one state-of-the-art database-ZJU is used. To additionally improve the performance, feature-level fusion schemes [simple concatenate and fusion codes (gaborization)] are enforced and equated. Receiver operating characteristics, error rate, sensitivity, specificity, and precision are used to demonstrate the performance of the proposed method qualitatively and quantitatively. Accuracy with proposed MmERMFLF is increased to 99.02% (using EEBGFR method with bagged ensemble classifier) in comparison to unimodal eye blink recognition system (97.60%). 99.80% genuine blinks are classified by MmERMFLF (when gaborization fusion is used) using simple tree classifier.																	1432-7643	1433-7479				NOV	2020	24	22					16829	16845		10.1007/s00500-020-04979-5		MAY 2020											
J								A two-stage similarity clustering-based large group decision-making method with incomplete probabilistic linguistic evaluation information	SOFT COMPUTING										Incomplete probabilistic linguistic evaluation information; Expert expertise; Two-stage similarity measurement; Large group decision making	TERM SETS; CONSENSUS MODEL; SELECTION	In recent years, probabilistic linguistic term set (PLT) is widely used in large group decision making (LGDM) for its integrity. However, the complexity of probabilistic linguistic LGDM and the large span of experts' profession cause two problems. On the one hand, it is difficult for all experts to give complete evaluation information in the form of PLTs. For this, we propose an expertise-based probabilistic linguistic evaluation information complement method. First, we identify authoritative experts under each attribute through professional hesitation and professional consistency. Then, we establish an optimization function to obtain the optimal missing value through the expectation score of authoritative experts and the linguistic term using habit of pending experts. On the other hand, the similarity between two experts cannot be fully represented by the sum of the distance of expert evaluation value. For this, we propose a two-stage similarity measurement method and introduce the distance weighting process, which not only measures the similarity between two expert evaluation values, but also measures the difference in degree of distance between two experts under different attributes. Finally, we apply this LGDM method to hot dry rock exploration site selection in southeast coast of China.																	1432-7643	1433-7479				NOV	2020	24	22					16869	16883		10.1007/s00500-020-04981-x		MAY 2020											
J								Deep OCR for Arabic script-based language like Pastho	EXPERT SYSTEMS										DenseNet; ligature recognition; MobileNet; ResNet; SqueezeNet	RECOGNITION	Developing cursive script recognition systems have always been a challenging task for researchers. This article proposes a ligature-based recognition system for the cursive Pashto script using four pre-trained CNN models using a fine-tuned approach. The SqueezeNet, ResNet, MobileNet and DenseNet models have been observed for the classification and the recognition of Pashto sub-word (ligature). Overall, the proposed system is divided into two domains (Source and Target). The source domain contains the pre-trained models used on the ImageNet Dataset. These models are later fine-tuned using the transfer learning approach to be used for the Pashto ligature recognition. The data augmentation techniques of negative and contour are used to increase the representation of ligature images and the dataset size. The CNN models have been evaluated on the benchmarks Pashto ligatures FAST-NU dataset. The proposed system achieved the highest recognition rate of up to 99.31% using the DenseNet architecture of Convolutional Neural Network for Pashto ligature.																	0266-4720	1468-0394				OCT	2020	37	5			SI				e12565	10.1111/exsy.12565		MAY 2020											
J								Deep heterogeneous network embedding based on Siamese Neural Networks	NEUROCOMPUTING										Network embedding; Heterogeneous network; Deep embedding; Siamese Neural Networks		Heterogeneous network embedding aims at mapping a heterogeneous network into a low-dimensional latent space. There exist diverse relations among different types of objects in heterogeneous networks. However, most existing heterogeneous network embedding methods focus on exploring network structures instead of relations among different objects, so some redundant and fuzzy relations are inevitably captured. To address the problem, we propose a Relation-Oriented Deep Embedding (RODE) framework for heterogeneous networks that explores different relations among nodes. The captured relations are modeled through node similarity and dissimilarity. Based on the similarity and dissimilarity, a multi-task Siamese Neural Network is formulated to perform network embedding and optimize embedding representations. Extensive experiments are conducted on four heterogeneous networks. Experimental results demonstrate our method outperforms state-of-the-art embedding algorithms on several network mining tasks, such as link prediction, node classification and node clustering. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 7	2020	388						1	11		10.1016/j.neucom.2020.01.012													
J								Integration of an actor-critic model and generative adversarial networks for a Chinese calligraphy robot	NEUROCOMPUTING										Robotic calligraphy; Motion planning; Deep reinforcement learning; Generative adversarial nets; Robot control	OPTIMIZATION	As a combination of robotic motion planning and Chinese calligraphy culture, robotic calligraphy plays a significant role in the inheritance and education of Chinese calligraphy culture. Most existing calligraphy robots focus on enabling the robots to learn writing through human participation, such as human-robot interactions and manually designed evaluation functions. However, because of the subjectivity of art aesthetics, these existing methods require a large amount of implementation work from human engineers. In addition, the written results cannot be accurately evaluated. To overcome these limitations, in this paper, we propose a robotic calligraphy model that combines a generative adversarial network (GAN) and deep reinforcement learning to enable a calligraphy robot to learn to write Chinese character strokes directly from images captured from Chinese calligraphic textbooks. In our proposed model, to automatically establish an aesthetic evaluation system for Chinese calligraphy, a GAN is first trained to understand and reconstruct stroke images. Then, the discriminator network is independently extracted from the trained GAN and embedded into a variant of the reinforcement learning method, the "actor-critic model", as a reward function. Thus, a calligraphy robot adopts the improved actor-critic model to learn to write multiple character strokes. The experimental results demonstrate that the proposed model successfully allows a calligraphy robot to write Chinese character strokes based on input stroke images. The performance of our model, compared with the state-of-the-art deep reinforcement learning method, shows the efficacy of the combination approach. In addition, the key technology in this work shows promise as a solution for robotic autonomous assembly. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 7	2020	388						12	23		10.1016/j.neucom.2020.01.043													
J								A power-type varying gain discrete-time recurrent neural network for solving time-varying linear system	NEUROCOMPUTING										Convergence; Varying gain; Discrete-time; Time-varying linear equation; BFGS quasi-Newton method	EQUATION	Many practical engineering problems can be described as an online time-varying linear system (TVLS), and thus solving TVLS is very important in control theory and control engineering. In this paper, a novel power-type varying gain discrete-time recurrent neural network (PVG-DTRNN) is proposed to solve the TVLS problem. Compared with the state-of-art method, i.e., the fixed-parameter discrete-time zeroing neural network (FP-DTZNN), the proposed PVG-DTRNN has better convergent rate and higher accuracy. To do so, a vector error function is firstly defined. Secondly, a power-type gain implicit dynamic model is derived and needs to be further discretized. Thirdly, by using Euler forward-difference rule, a discretized dynamic model is designed. In order to get the explicit dynamic model, the Broyden-Fletcher-Goldfarb-Shanno (BFGS) quasi-Newton method is utilized to estimate the inverse of the Hessian matrix. Comparisons of computer simulations verify the effectiveness and superiority of the proposed PVG-DTRNN models. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 7	2020	388						24	33		10.1016/j.neucom.2020.01.027													
J								Unimodal regularized neuron stick-breaking for ordinal classification	NEUROCOMPUTING										Ordinal regression; Deep neural network; Stick-breaking	AGE ESTIMATION; REGRESSION METHODS; CANCER	This paper targets for the ordinal regression/classification, which objective is to learn a rule to predict labels from a discrete but ordered set. For instance, the classification for medical diagnosis usually involves inherently ordered labels corresponding to the level of health risk. Previous multi-task classifiers on ordinal data often use several binary classification branches to compute a series of cumulative probabilities. However, these cumulative probabilities are not guaranteed to be monotonically decreasing. It also introduces a large number of hyper-parameters to be fine-tuned manually. This paper aims to eliminate or at least largely reduce the effects of those problems. We propose a simple yet efficient way to rephrase the output layer of the conventional deep neural network. Besides, in order to alleviate the effects of label noise in ordinal datasets, we propose a unimodal label regularization strategy. It also explicitly encourages the class predictions to distribute on nearby classes of ground truth. We show that our methods lead to the state-of-the-art accuracy on the medical diagnose task (e.g., Diabetic Retinopathy and Ultrasound Breast dataset) as well as the face age prediction (e.g., Adience face and MORPH Album II) with very little additional cost. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 7	2020	388						34	44		10.1016/j.neucom.2020.01.025													
J								Multi-manifold locality graph preserving analysis for hyperspectral image classification	NEUROCOMPUTING										Manifold learning; Hyperspectral image; Dimensionality reduction; Multi-manifold structure; Discriminant projection matrix	DIMENSIONALITY REDUCTION; FEATURE-EXTRACTION; SPARSE	Manifold learning has been successfully applied to hyperspectral image (HSI) classification by modeling different land covers as a smooth manifold embedded in a high-dimensional space. However, traditional manifold learning algorithms were proposed with the assumption of single manifold structure in HSI, while the samples in different subsets may belong to different sub-manifolds. In this paper, a novel dimensionality reduction (DR) method called multi-manifold locality graph preserving analysis (MLGPA) was proposed for feature learning of HSI data. According to the label information of HSI, MLGPA divides the samples data into different subsets, and each subset is treated as a sub-manifold. Then, it constructs a within-manifold graph and a between-manifold graph for each sub-manifold to characterize within-manifold compactness and between-manifold separability, and a discriminant projection matrix can be obtained by maximizing the between-manifold scatter and minimizing the within-manifold scatter simultaneously. Finally, low-dimensional embedding features of different sub-manifolds are fused to improve the classification performance. MLGPA can effectively reveal the multi-manifold structure and improve the classification performance of HSI. Experimental results on three real-world HSI data sets demonstrate that MLGPA is superior to some state-of-the-art methods in terms of classification accuracy. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 7	2020	388						45	59		10.1016/j.neucom.2019.12.112													
J								Deterministic generative adversarial imitation learning	NEUROCOMPUTING										Robot learning; Imitation learning; Reinforcement learning; GAN; DGAIL		This paper proposes a deterministic generative adversarial imitation learning method which allows the robot to implement the motion planning task rapidly by learning from the demonstration data without reward function. In our method, the deep deterministic policy gradient method is used as the generator for learning the action policy on the basis of discriminator, and the demonstration data is input into the generator to ensure its stability. Three experiments on the push and pick-and-place tasks are conducted in the gym robotic environment. Results show that the learning speed of our method is much faster than the stochastic generative adversarial imitation learning method, and it can effectively learn from the demonstration data in different states of the task with higher learning stability. The proposed method can complete the motion planning task without environmental reward quickly and improve the stability of the training process. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 7	2020	388						60	69		10.1016/j.neucom.2020.01.016													
J								Automatic evaluation of facial nerve paralysis by dual-path LSTM with deep differentiated network	NEUROCOMPUTING										Facial nerve paralysis; Facial asymmetry; Dual-path LSTM; Deep differentiated network; Deep learning methods	RECOGNITION	Facial nerve paralysis adversely affects the patients' mental and physical health, and several existing evaluation methods of facial paralysis are put forward based on the static facial asymmetry. However, these traditional methods still suffer from two drawbacks: (1) the facial movement information is always being ignored, which plays important roles for facial paralysis analysis; (2) the shallow machine learning models have their limitations on extracting useful facial features for the evaluation of facial paralysis. To solve these problems, we present dual-path LSTM with deep differentiated network (DP-LSTM-DDN) to evaluate the severity of facial paralysis automatically. The key idea behind DP-LSTM-DDN is that the diagnosis results are sensitive to the facial asymmetry and the patterns of facial muscular movements when the patients were doing the diagnostic facial actions. Therefore, we design a deep differentiated network to analyze the difference between two sides of patients' faces. Furthermore, since the involved facial regions are as important as the whole face for the diagnostic of facial paralysis analysis, we propose a dual-path LSTM network to extract both global and local facial movement features. Then these extracted high level representations are fused for the final evaluation of facial paralysis. The experimental results have verified the better performance of DP-LSTM-DDN compared with the state-of-the-art methods. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 7	2020	388						70	77		10.1016/j.neucom.2020.01.014													
J								Hybrid-loss supervision for deep neural network	NEUROCOMPUTING										Hybrid-loss supervision; Deep neural network		Multi-loss-joint-optimization has been proven to be valid in computer vision literature. However, the learned deep sub-features usually fit their disjoint constraints, which yield confrontation and spatial inconsistency among the sub-features with nonshared FC layers. In this paper, we propose a Hybrid-loss supervision (HLS) framework in order to obtain smoother and more spatially consistent features with shared FC layers. First, we analyze the shortcomings of the monitoring with single-loss in the existing framework theoretically. Then, we selected two notable loss functions (e.g., Center loss and Weighted loss) to instantiate the HLS framework by linear combination. By instantiating the framework with two standard loss functions, the network has learned more compact intra-class deep features and uniform inter-class deep features. The HLS framework can significantly boost the efficiency of existing convolution networks for both image classification task and object detection task without increasing network parameters and computational complexity. Extensive experimental results on different vision tasks demonstrate consistent improvement can be achieved across a variety of datasets (e.g., CIFAR-10/100, ImageNet-2012, PASCAL VOC and MS-COCO) and different convolutional neural network architectures. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 7	2020	388						78	89		10.1016/j.neucom.2020.01.047													
J								A robust approach to reading recognition of pointer meters based on improved mask-RCNN	NEUROCOMPUTING										Deep learning; Mask-RCNN; Pointer meters; PrRolPooling; Reading recognition	INSTRUMENTS; SYSTEM	In this paper, we address a challenging task in real-word applications, i.e., automatic reading recognition for pointer meters, called PRM1 This application is valuable in the fields of military, industry, and aerospace. However, the accuracy of recognizing the readings of pointer meters by machine vision is, oftentimes, affected by several factors, such as uneven illumination in each image, large range variation of illumination in different images, complex backgrounds, tilting of pointer meters, image blur, and scale change, resulting in the recognized readings with unacceptable accuracy. In this paper, a new robust approach to reading recognition of pointer meters is proposed. The proposed method consists of three main contributions: (1) constructing a novel deep learning algorithm in which the PrRolPooling is used in lieu of the RoiAlign in the existing Mask-RCNN, (2) classifying the type of pointer meters while fitting the pointer binary mask, and (3) calculating the readings of pointer meters by the proposed angle method. In addition, we also report and release a new dataset for the community. Experiments show that the new algorithm can significantly improve the accuracy of the recognized readings of pointer meters, meanwhile, the proposed approach is also robust to the natural environments and computationally efficient. (C) 2020 Published by Elsevier B.V.																	0925-2312	1872-8286				MAY 7	2020	388						90	101		10.1016/j.neucom.2020.01.032													
J								Dialogue systems with audio context	NEUROCOMPUTING										Dialogue systems; Audio features; Language generation; Multimodality		Research on building dialogue systems that converse with humans naturally has recently attracted a lot of attention. Most work on this area assumes text-based conversation, where the user message is modeled as a sequence of words in a vocabulary. Real-world human conversation, in contrast, involves other modalities, such as voice, facial expression and body language, which can influence the conversation significantly in certain scenarios. In this work, we explore the impact of incorporating the audio features of the user message into generative dialogue systems. Specifically, we first design an auxiliary response retrieval task for audio representation learning. Then, we use word-level modality fusion to incorporate the audio features as additional context to our main generative model. Experiments show that our audioaugmented model outperforms the audio-free counterpart on perplexity, response diversity and human evaluation. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 7	2020	388						102	109		10.1016/j.neucom.2019.12.126													
J								Empirical Mode Decomposition based Multi-objective Deep Belief Network for short-term power load forecasting	NEUROCOMPUTING										Empirical Mode Decomposition; Multi-objective optimization algorithm; Ensemble learning; Deep belief network; Power load forecasting	ENSEMBLE; NORMALIZATION; PROPOSAL	With the rapid development of power grid data, the data generated by the operation of the power system is increasingly complex, and the amount of data increases exponentially. In order to fully exploit and utilize the deep relationship between data to achieve accurate prediction of power load, this paper proposes an Empirical Mode Decomposition Based Multi-objective Deep Belief Network prediction method (EMD-MODBN). In the training process of DBN, a multi-objective optimization model is constructed aiming at accuracy and diversity, and MOEA/D is used to optimize the parameters of the model to enhance the generalization ability of the prediction model. Finally, the final load forecasting results are obtained by summing up the weighted outputs of each forecasting model with ensemble learning method. The experimental results show that compared with several current better load forecasting methods, this method has obvious advantages in prediction accuracy and generalization ability. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 7	2020	388						110	123		10.1016/j.neucom.2020.01.031													
J								A simple saliency detection approach via automatic top-down feature fusion	NEUROCOMPUTING										Salient object detection; Saliency detection; Multi-level feature fusion	OBJECT DETECTION; FRAMEWORK; MODEL	It is widely accepted that the top sides of convolutional neural networks (CNNs) convey high-level semantic features, and the bottom sides contain low-level details. Therefore, most of recent salient object detection methods aim at designing effective fusion strategies for side-output features. Although significant progress has been achieved in this direction, the network architectures become more and more complex, which will make the future improvement difficult and heavily engineered. Moreover, the manually designed fusion strategies would be sub-optimal due to the large search space of possible solutions. To address above problems, we propose an Automatic Top-Down Fusion (ATDF) method, in which the global information at the top sides are flowed into bottom sides to guide the learning of low layers. We design a novel valve module and add it at each side to control the coarse semantic information flowed into a specific bottom side. Through these valve modules, each bottom side at the top-down pathway is expected to receive necessary top information. We also design a generator to improve the prediction capability of fused deep features for saliency detection. We perform extensive experiments to demonstrate that ATDF is simple yet effective and thus opens a new path for saliency detection. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 7	2020	388						124	134		10.1016/j.neucom.2019.12.123													
J								Aspect-based sentiment classification with multi-attention network	NEUROCOMPUTING										Aspect-based sentiment classification; Sentiment analysis; Attention mechanism; Neural network	ASPECT EXTRACTION	Aspect-based sentiment classification aims to predict the sentiment polarity of an aspect term in a sentence instead of the sentiment polarity of the entire sentence. Neural networks have been used for this task, and most existing methods have adopted sequence models, which require more training time than other models. When an aspect term comprises several words, most methods involve a coarse-level attention mechanism to model the aspect, and this may result in information loss. In this paper, we propose a multi-attention network (MAN) to address the above problems. The proposed model uses intra- and inter-level attention mechanisms. In the former, the MAN employs a transformer encoder instead of a sequence model to reduce training time. The transformer encoder encodes the input sentence in parallel and preserves long-distance sentiment relations. In the latter, the MAN uses a global and a local attention module to capture differently grained interactive information between aspect and context. The global attention module focuses on the entire relation, whereas the local attention module considers interactions at word level; this was often neglected in previous studies. Experiments demonstrate that the proposed model achieves superior results when compared to the baseline models. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 7	2020	388						135	143		10.1016/j.neucom.2020.01.024													
J								Systematic evaluation of deep face recognition methods	NEUROCOMPUTING										Deep face recognition; Facial model evaluation; Model design recommendation		Face recognition is an important task in both academia and industry. With the development of deep convolutional neural networks, many deep face recognition methods have been proposed and have achieved remarkable results. However, these methods show great diversity among their datasets, network architectures, loss functions, and parameter learning strategies. For those who want to apply these technologies to establish a deep face recognition system, it is bewildering to evaluate which improvements are more suitable and effective. This study systematically summarizes and evaluates the state-of-the-art face recognition methods. However, unlike general reviews, on the basis of a survey, this study presents a comprehensive evaluation framework and measures the effects of multifarious settings in five components, including data augmentation, network architecture, loss function, network training, and model compression. Based on the experimental results, the influences of these five components on the deep face recognition are summarized. In terms of the datasets, a high sample-identity ratio is conducive to generalization, but it leads to increased difficulty for the training to converge. For the network architecture, deep ResNet has an advantage over other designs. Various normalization operations in the network are also necessary. For the loss function, whose performance is closely related to network design and training conditions. The angle-margin loss has a higher upper bound performance, but the traditional Euclidean-margin loss has a stable performance in limited training condition and shallower network. In terms of the training strategy, the step-declining learning rate and large batch size are recommended for recognition tasks. Furthermore, this study compares several popular model compression methods and shows that MobileNet has advantages over the others in terms of both compression ratio and robustness. Finally, a detailed list of recommended settings is provided. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 7	2020	388						144	156		10.1016/j.neucom.2020.01.023													
J								Conformal prediction based active learning by linear regression optimization	NEUROCOMPUTING										Conformal prediction; Active learning; Linear regression; Image classification	SUPPORT VECTOR MACHINES; FACE RECOGNITION; CLASSIFICATION; MODELS; ROBUST	Conformal prediction uses the degree of strangeness (nonconformity) of data instances to determine the confidence values of new predictions. We propose a conformal prediction based active learning algorithm, referred to as CPAL-LR, to improve the performance of pattern classification algorithms. CPAL-LR uses a novel query function that determines the relevance of unlabeled instances through the solution of a constrained linear regression model, incorporating uncertainty, diversity, and representativeness in the optimization problem. Furthermore, we present a nonconformity measure that produces reliable confidence values. CPAL-LR is implemented in conjunction with support vector machines, sparse coding algorithms, and convolutional networks. Experiments conducted on face and object recognition databases demonstrate that CPAL-LR improves the classification performance of a variety classifiers, outperforming previously proposed active learning techniques, while producing reliable confidence values. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 7	2020	388						157	169		10.1016/j.neucom.2020.01.018													
J								Emission stations location selection based on conditional measurement GAN data	NEUROCOMPUTING										Vehicle emission monitoring; Generative Adversarial Networks; Station location selection	NETWORKS	Urban vehicle emission monitoring can help make suggestions for the pollution emission control, and can protect public health. However, it is hard to get an overview of the vehicle emission in the city scale due to the sparse emission remote sensing stations in city, and selecting the appropriate stations locations which can reflect the emission variation in the given region mostly is another challenge. The existing methods solve the problem by spatial interpolation based on geographical statistics methods, without considering that urban vehicle emission varies by locations non-linearly and depends on many complex external factors. To tackle the spatial sparsity of vehicle remote sensing data, we design a data augmentation strategy based on Generative Adversarial Networks (GAN), which leverages prior model COPERT with conditional measurement data to filling the missing entries at other locations. With this strategy, we can generate realistic emission data while accelerating the training process. In addition, to address the emission stations location selection problem, we design a novel location selection strategy based on Spearman's rank correlation coefficients, which leverages the realistic data generated to discover the grids with maximum link correlation for the pre-deployed station. Finally, we present experiments with the remote emission sensing data in Hefei, and the results demonstrate that our proposed model can mimics the real vehicle emission distribution in the given region effectively. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 7	2020	388						170	180		10.1016/j.neucom.2020.01.013													
J								Robust detection of intermittent sensor faults in stochastic LTV systems	NEUROCOMPUTING										Fault detection; Intermittent Fault (IF); Stochastic uncertainties; Linear Time-Varying (LTV) systems	DIAGNOSIS; OBSERVER; MODEL; DIAGNOSABILITY; FAILURES	This paper addresses the detection problem of intermittent sensor faults for linear time-varying (LTV) systems with stochastic uncertainties. A robust filter is proposed which has advantages of zero mean and minimum state estimation error covariance. Then a corresponding residual generator is constructed and the quantitative influence of sensor faults on it is analyzed. Next, we design the evaluation function and detection threshold to achieve intermittent fault detection (IFD). Besides, the detectability of sensor faults is also provided. Finally, a simulation study is carried out to illustrate the effectiveness and applicability of our proposed method. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 7	2020	388						181	187		10.1016/j.neucom.2019.12.111													
J								Adversarial dictionary learning for a robust analysis and modelling of spontaneous neuronal activity	NEUROCOMPUTING										Dictionary learning; Supervised machine learning; Biological neural networks	K-SVD	The field of neuroscience is experiencing rapid growth in the complexity and quantity of the recorded neural activity allowing us unprecedented access to its dynamics in different brain areas. The objective of this work is to discover directly from the experimental data rich and comprehensible models for brain function that will be concurrently robust to noise. Considering this task from the perspective of dimensionality reduction, we develop an innovative, robust to noise dictionary learning framework based on adversarial training methods for the identification of patterns of synchronous firing activity as well as within a time lag. We employ real-world binary datasets describing the spontaneous neuronal activity of laboratory mice over time, and we aim to their efficient low-dimensional representation. The results on the classification accuracy for the discrimination between the clean and the adversarial-noisy activation patterns obtained by an SVM classifier highlight the efficacy of the proposed scheme compared to other methods, and the visualization of the dictionary's distribution demonstrates the multifarious information that we obtain from it. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 7	2020	388						188	201		10.1016/j.neucom.2020.01.041													
J								Segmentation based rotated bounding boxes prediction and image synthesizing for object detection of high resolution aerial images	NEUROCOMPUTING										Object detection; Arbitrary orientations; Aerial images; High resolution images		Object detection for aerial images is becoming an active topic in computer vision with many real-world applications. It is a very challenging task due to many factors such as highly complex background, arbitrary object orientations, high input resolution, etc. In this paper, we develop a new training and inference mechanism, which is shown to significantly improve the detection accuracy for high resolution aerial images. Instead of estimating the orientations of objects using direct regressions like in previous methods, we propose to predict the rotated bounding boxes by leveraging a segmentation task, which is easier to train and yields more accurate detection results. In addition, an image synthesizing based data augmentation strategy is presented to address the data imbalance issues in aerial object detection. Extensive experiments have been conducted to verify our contribution. The proposed method sets new state-of-the-art performance on the challenging DOTA dataset. The source codes will be available at http://ice.dlut.edu.cn/lu/publications.html. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 7	2020	388						202	211		10.1016/j.neucom.2020.01.039													
J								Deep convolution network based emotion analysis towards mental health care	NEUROCOMPUTING										Facial expression recognition; Deep convolution network; Mental health care; Emotion analysis	FACIAL EXPRESSION RECOGNITION; NEURAL-NETWORK; EXPERIENCE; FACE	Facial expressions play an important role during communications, allowing information regarding the emotional state of an individual to be conveyed and inferred. Research suggests that automatic facial expression recognition is a promising avenue of enquiry in mental healthcare, as facial expressions can also reflect an individual's mental state. In order to develop user-friendly, low-cost and effective facial expression analysis systems for mental health care, this paper presents a novel deep convolution network based emotion analysis framework to support mental state detection and diagnosis. The proposed system is able to process facial images and interpret the temporal evolution of emotions through a new solution in which deep features are extracted from the Fully Connected Layer 6 of the AlexNet, with a standard Linear Discriminant Analysis Classifier exploited to obtain the final classification outcome. It is tested against 5 benchmarking databases, including JAFFE, KDEF,CK+, and databases with the images obtained 'in the wild' such as FER2013 and AffectNet. Compared with the other state-of-the-art methods, we observe that our method has overall higher accuracy of facial expression recognition. Additionally, when compared to the state-of-the-art deep learning algorithms such as Vgg16, GoogleNet, ResNet and AlexNet, the proposed method demonstrated better efficiency and has less device requirements. The experiments presented in this paper demonstrate that the proposed method outperforms the other methods in terms of accuracy and efficiency which suggests it could act as a smart, low-cost, user-friendly cognitive aid to detect, monitor, and diagnose the mental health of a patient through automatic facial expression analysis. (C) 2020 Published by Elsevier B.V.																	0925-2312	1872-8286				MAY 7	2020	388						212	227		10.1016/j.neucom.2020.01.034													
J								Landmark map: An extension of the self-organizing map for a user-intended nonlinear projection	NEUROCOMPUTING										Landmark map; Self-organizing map; Semi-supervised learning; Artificial neural network	RECOGNITION; SOM; GTM	A self-organizing map (SOM) is an unsupervised artificial neural network that is widely used in, e.g., data mining and visualization. Supervised and semi-supervised learning methods have been proposed for the SOM. However, their teacher labels do not describe the relationship between the data and the location of nodes. This study proposes a landmark map (LAMA), which is an extension of SOMs that utilizes several landmarks, e.g., pairs of nodes and data points. LAMA is designed to obtain a user-intended nonlinear projection to achieve, e.g., the landmark-oriented data visualization. To reveal the learning properties of LAMA, the Zoo dataset from the UCI Machine Learning Repository, the McDonald's dataset from Kaggle, and an artificial formant dataset were analyzed. The analysis results of the Zoo dataset indicated that LAMA could provide a new data view such as the landmark-centered data visualization. McDonald's dataset analysis demonstrated menu recommendation examples based on a few designated items. Furthermore, the artificial formant data analysis revealed that LAMA successfully provided the intended nonlinear projection associating articular movement with vertical and horizontal movement of a computer cursor. Potential applications of LAMA include data mining, recommendation systems, and human-computer interaction. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 7	2020	388						228	245		10.1016/j.neucom.2019.12.125													
J								Neighbor similarity and soft-label adaptation for unsupervised cross-dataset person re-identification	NEUROCOMPUTING										Person re-identification; Unsupervised learning; Domain adaptation		Most of the existing person re-identification algorithms rely on supervised model learning from a large number of labeled training data per-camera-pair. However, the manual annotations often require expensive human labor, which limits the application of supervised methods for large-scale real-world deployments. To address this problem, we formulate a Neighbor Similarity and Soft-label Adaptation (NSSA) algorithm to transfer the supervised information from source domain to a new unlabeled target dataset. Specifically, we introduce a distance metric on the target domain, which incorporates inner-domain neighbor similarity and inter-domain soft-label adapted from source domain. The unlabeled samples which are close in this metric are considered to share the same pseudo-id and further selected to fine-tune the model. The training is performed iteratively to incorporate more credible sample pairs and incrementally improve the model. Extensive experimental results validate the superiority of our proposed NESSA algorithm, which significantly outperforms the state-of-the-art unsupervised and domain adaptation re-identification methods. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 7	2020	388						246	254		10.1016/j.neucom.2019.12.115													
J								PoISAR image classification via a novel semi-supervised recurrent complex-valued convolution neural network	NEUROCOMPUTING										PoISAR terrain classification; Network overfitting; RCV-CNN1; RCV-CNN2	SCATTERING MODEL; SAR; DECOMPOSITION	Due to that polarimetric synthetic aperture radar (PoISAR) data suffers from missing labeled samples and complex-valued data, this article presents a novel semi-supervised PolSAR terrain classification method named recurrent complex-valued convolution neural network (RCV-CNN) which combines semi-supervised learning and complex-valued convolution neural network (CV-CNN). The proposed method only needs a small number of labeled samples to achieve good classification results. First, a Wishart classifier is used to select some reliable PolSAR samples. Then, two new semi-supervised deep classification model RCV-CNN1 and RCV-CNN2 have been proposed to improve PolSAR image classification accuracy. Moreover, our proposed methods could solve the problem of network overfitting phenomenon to some extend when the number of training samples is too small. Finally, three real PolSAR dataset are applied to verify the effectiveness of our algorithms. Compared with the other five state-of-the-art methods, the proposed RCV-CNN1 and RCV-CNN2 classification models show good performance in accuracy and generalization. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 7	2020	388						255	268		10.1016/j.neucom.2020.01.020													
J								Multivariate time series forecasting via attention-based encoder-decoder framework	NEUROCOMPUTING										Multivariate time series; Temporal attention; Multi-step forecasting; Encoder-decoder; Deep learning; Long short-term memory networks	PREDICTION; NETWORK	Time series forecasting is an important technique to study the behavior of temporal data and forecast future values, which is widely applied in many fields, e.g. air quality forecasting, power load forecasting, medical monitoring, and intrusion detection. In this paper, we firstly propose a novel temporal attention encoder-decoder model to deal with the multivariate time series forecasting problem. It is an end-to-end deep learning structure that integrates the traditional encode context vector and temporal attention vector for jointly temporal representation learning, which is based on bi-directional long short-term memory networks (Bi-LSTM) layers with temporal attention mechanism as the encoder network to adaptively learning long-term dependency and hidden correlation features of multivariate temporal data. Extensive experimental results on five typical multivariate time series datasets showed that our model has the best forecasting performance compared with baseline methods. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 7	2020	388						269	279		10.1016/j.neucom.2019.12.118													
J								Detecting Alzheimer's disease Based on 4D fMRI: An exploration under deep learning framework	NEUROCOMPUTING										Functional magnetic resonance imaging; Alzheimer's disease; Convolutional neural network; Long short-term memory network	NETWORK	Applying machine learning methods to various modality medical images and clinical data for early diagnosis of Alzheimer's disease (AD) and its prodromal stage has many significant results. So far, the image data input to classifier mainly focus on 2D or 3D images. Although some functional imaging technologies, such as functional magnetic resonance imaging (fMRI), generate 4D data which contain both spatial and time-varying information of the brain, for the lack of suitable 4D image processing algorithm, these 4D data were always used by transforming them into functional connectivity or slicing them into 2D/3D pictures which causing apparent information loss. In this paper, we present a 4D deep learning model (C3d-LSTM) for AD discrimination, which is able to utilize the spatial and time-varying information simultaneously by dealing with 4D fMRI data directly. The proposed C3d-LSTM combines a series of 3D convolutional neural networks (CNNs) to extract spatial features from each volume of 3D static image in fMRI image sequence. Then the feature maps obtained were put into the long short-term memory (LSTM) network to capture the time-varying information contained in the data. Because of the design of structure, C3d-LSTM became an end-to-end data-driven model, which was more convenient for processing 4D fMRI data. The model proposed conducted on the AD Neuroimaging Initiative dataset for algorithm evaluation compared with controlled experiments. Results showed that using 4D fMRI data directly with the proposed method did make a far better result for AD detection than the methods using functional connectivity, 2D, or 3D fMRI data. It demonstrated our assumption that making the most of the natural spatial and temporal information preserved in 4D fMRI data is significant for AD detection and can increase the performance of the classifier. Meanwhile the result implied that the C3d-LSTM model proposed is a suitable model for processing 4D fMRI data and extracting the spatio-temporal property of fMRI data fully for diagnosis of AD. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 7	2020	388						280	287		10.1016/j.neucom.2020.01.053													
J								Convergence analysis of accelerated proximal extra-gradient method with applications	NEUROCOMPUTING										First-order proximal methods; Sparsity-inducing regularizers; Extragradient-based fixed-point iterations	MAXIMAL MONOTONE-OPERATORS; POINT ALGORITHM; DISCRIMINANT-ANALYSIS; SELECTION METHODS; WEAK-CONVERGENCE; EXTRAGRADIENT; CLASSIFICATION; MINIMIZATION; RECOGNITION; COMPLEXITY	Proximal algorithms are popular class of methods for handling sparsity structure in the datasets due to their low iteration costs and faster convergence. In this paper, we consider the framework of the sum of two convex functions, one of which is a smooth function with a Lipschitz gradient, while the other may be a non-smooth function. The usages of such non-smooth functions for identifying complex sparsity-structures in datasets in form of non-smooth regularizers has been an active research direction in the recent past. In this paper, we present the convergence analysis for the extragradient-based fixed-point method with an inertial component, based on which recently a new accelerated proximal extragradient algorithm is designed. In addition, extending the application areas of this algorithm, we applied it to solve (i) the logistic regression problem with complex l(1)-based penalties, namely, overlapping group lasso and fused lasso frameworks, and (ii) a recently proposed structurally-regularized learning problem for representation selection where the objective function consists of a reconstruction error and structured regularizers as combination of group sparsity regularizer, diversity regularizer, and locality-sensitivity regularizer. With the help of extensive experiments on several publicly available real-world datasets, the efficacy of the inertial-based extragradient methods has been demonstrated for solving the extended lasso and representation selection problems of machine learning. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 7	2020	388						288	300		10.1016/j.neucom.2020.01.049													
J								Seamless indoor pedestrian tracking by fusing INS and UWB measurements via LS-SVM assisted UFIR filter	NEUROCOMPUTING										Seamless localization; Pedestrian tracking; Unbiased FIR filter; LS-SVM	KALMAN; INTEGRATION; NAVIGATION; LOCALIZATION; DIAGNOSIS	A seamless indoor pedestrian tracking scheme using a least square-support vector machine (LS-SVM) assisted unbiased finite impulse response (UFIR) filter is designed to achieve seamless reliable human position monitoring in indoor environments in this work. This novel scheme is based on the loosely-coupled integrated localization model, which can fuse the inertial navigation system (INS)-derived and ultra-wide-band (UWB)-derived positions and compensate for the INS position error. Based on the loosely-coupled model, the hybrid scheme includes a training stage and a predict stage. In the training stage, the UWB position is available, and the scheme employs a UFIR filter to compensate for the INS position error and provide training the data robustly. Meanwhile, the LS-SVM is used for training the mapping between the INS position and its error utilizing the INS position and UFIR filter outputs. When the UFIR filter can not work due to a UWB outage, the hybrid scheme is in the prediction stage; the LS-SVM replaces the UFIR filter to compensate for the INS position error with the mapping built in the training stage. An experimental study shows that the proposed scheme is capable of seamless reliable indoor pedestrian tracking. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 7	2020	388						301	308		10.1016/j.neucom.2019.12.121													
J								Human skeleton mutual learning for person re-identification	NEUROCOMPUTING										Person re-identification; Joint segmentation; 2D pose skeleton estimation; Mutual learning		Person re-identification refers to matching people across non-overlapping camera views on different locations and at different times. In the case of changes in perspective, light, background, veil, and person's clothing, traditional method cannot achieve person recognition effectively and reliably. In this paper, we propose a novel biometric metric learning method named Human Skeleton Mutual Learning person re-identification (HSMLP-Reid). The purpose of HSML person re-identification method (HSMLP-Reid) largely aims to use the new pedestrian local segmentation method proposed in this paper combined with the global skeleton information to solve the influence of background and local posture change. Firstly, bottom-up method is used to estimate the pedestrian posture and skeleton and the joint points of the pedestrians will be marked in this process. A new local segmentation method proposed in this paper named joint segmentation is used to locally segment pedestrians and perform local block matching. Furthermore, we learn the global skeleton information by defined joint distances from the pedestrian 2D skeleton estimation by the bottom-up method and use global skeleton information to global skeleton matching. Finally, we use local match and global skeleton match for mutual learning. Local match based on pedestrian nodes and global skeleton match based on pedestrian skeleton are based on biometrics. We learn the classification loss and metric learning loss to train model. Metric loss includes global skeletal distance and local block metric distance. Extensive experimental results on the large-scale Market1501, CUHK03 and CUHK-SYSU data sets demonstrate that the proposed method achieves consistently superior performance and outperforms most of the state-of-the-art methods. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 7	2020	388						309	323		10.1016/j.neucom.2019.12.120													
J								Joint low rank embedded multiple features learning for audio-visual emotion recognition	NEUROCOMPUTING										Common subspace learning; Low-rank representation; Audio-visual emotion recognition; Multi-view learning	CANONICAL CORRELATION-ANALYSIS; MACHINE; REDUCTION; ALGORITHM; MATRIX	Audio-visual emotion recognition is a challenging problem in the research fields of human-computer interaction and pattern recognition. Seeking a common subspace among the heterogeneous multi-modal data is essential for audio-visual emotion recognition. In this paper, we study the subspace learning for audio-visual emotion recognition by combing the similarity of intra-modality and the correlation of inter-modality. First, we enforce a low-rank constraint on the self-representation of the features in the subspace to exploit the structural similarity of intra-modality. It is based on a key observation that each modality and the corresponding features usually lie in a low-dimensional manifold. Second, we propose a joint low-rank model on the representation of inter-modality to keep consistency across different modalities. Finally, the intra-modality similarity and inter-modality correlation are integrated within a unified framework, for which we develop an efficient computational algorithm to pursue the common subspace. Experimental results on three typical audio-visual emotion datasets demonstrate the superior performance of our method on audio-visual emotion recognition. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 7	2020	388						324	333		10.1016/j.neucom.2020.01.017													
J								Riemann-Theta Boltzmann machine	NEUROCOMPUTING										Boltzmann machines; Neural networks; Riemann-Theta function; Density estimation; Data classification	NEURAL-NETWORKS	A general Boltzmann machine with continuous visible and discrete integer valued hidden states is introduced. Under mild assumptions about the connection matrices, the probability density function of the visible units can be solved for analytically, yielding a novel parametric density function involving a ratio of Riemann-Theta functions. The conditional expectation of a hidden state for given visible states can also be calculated analytically, yielding a derivative of the logarithmic Riemann-Theta function. The conditional expectation can be used as activation function in a feedforward neural network, thereby increasing the modelling capacity of the network. Both the Boltzmann machine and the derived feedforward neural network can be successfully trained via standard gradient- and non-gradient-based optimization techniques. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				MAY 7	2020	388						334	345		10.1016/j.neucom.2020.01.011													
J								Detection of tomato organs based on convolutional neural network under the overlap and occlusion backgrounds	MACHINE VISION AND APPLICATIONS										Tomato detection; Multi-scale feature fusion; Repulsion loss; Soft-NMS; Migration learning; Convolutional neural network	RECOGNITION; IMAGES	Traditional detection methods are not sensitive to small-sized tomato organs (flowers and fruits), because the immature green tomatoes are highly similar to the background color. The overlap among fruits and the occlusion of stems and leaves on tomato organs can lead to false and missing detection, which decreases the accuracy and generalization ability of the model. Therefore, a tomato organ recognition method based on improved Feature Pyramid Network was proposed in this paper. To begin with, multi-scale feature fusion was used to fuse the detailed bottom features and high-level semantic features to detect small-sized tomato organs to improve recognition rate. And then repulsion loss was used to take place of the original smooth L-1 loss function. Besides, Soft-NMS (Soft non-maximum suppression) was adopted to replace non-maximum suppression to screen the bounding boxes of tomato organs to construct a recognition model of tomato key organ. Finally, the network was trained and verified on the collected image data set. The results showed that compared with the traditional Faster R-CNN model, the performance was greatly improved (mean average precision was improved from 90.7 to 99.5%). Subsequently, the training model can be compressed so that it can be embedded into the microcontroller to develop further precise pesticide targeting application system of tomato organs and the automatic picking device.																	0932-8092	1432-1769				MAY 6	2020	31	5							31	10.1007/s00138-020-01081-6													
J								Application of structural equation based on big data in Korean automobile industry	EVOLUTIONARY INTELLIGENCE										Automobile Industry; Big data; Structural equation model		Korea's automobile industry has not developed for a long time, but it has achieved a leading position in the world, and also occupied part of the Chinese market. What causes this result is worth studying. Based on the big data, this paper uses the result equation model to analyze the development of Korean automobile industry in China																	1864-5909	1864-5917															10.1007/s12065-020-00414-8		MAY 2020											
J								An ensembled data frequency prediction based framework for fast processing using hybrid cache optimization	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Big data; Prediction; Pre-fetching; Query; Cache; Frequency; Memory	PATTERNS; MACHINE	Technological advancements have led to an exponential growth in input-output intensive data that demands high-performance computing. To access the data, the frequency of firing the same query is quite high. Hence, predicting and prefetching these frequently-used queries can enhance the performance in terms of execution time and cache hit ratio. Therefore, a prediction-based framework has been proposed which initially, generates memory traces to identify the data usage patterns in terms of query frequency. The future query requests have been predicted and classified using an ensembled approach that yields 87.5% accuracy. It successfully reduces the error rate up to 11%. Furthermore, the predicted classified results have been tagged as hot and cold data on the basis of threshold frequency. The identified hot data has been prefetched into the cache that provides 96.5% cache hits with 9.7% decreased execution time. Hybrid cache replacement algorithm has been utilized to keep the cache updated with the hot data. The experimental results have been compared with the existing frameworks and benchmarks, which shows 6.8% improvement in accuracy with 9% increment in cache hits.																	1868-5137	1868-5145															10.1007/s12652-020-01973-5		MAY 2020											
J								Fault mitigation through multi converter UPQC with hysteresis controller in grid connected wind system	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Wind turbine; Grid; Multi converter UPQC; Hysteresis controller; Reactive active power; Total harmonic distortion; Power quality issues	SMALL/LARGE-SIGNAL STABILITY; REACTIVE POWER COMPENSATION; SLIDING MODE CONTROL; DISTRIBUTED GENERATION; PERFORMANCE ANALYSIS; QUALITY CONDITIONER; CONTROL STRATEGIES; CONTROL SCHEME; FUZZY-LOGIC; MANAGEMENT	The ability of parallel compensation for voltage and current in bus system is known as Multi Converter Unified Power Quality Conditioner (MC-UPQC). It improve the Power Quality (PQ) of the renewable energy sources. The combination of current and voltage quality is known as PQ. The PQ problems, named as imbalance, flicker, and harmonics, the electronically switched and nonlinear devices with increasing applications in distribution systems and industries have become serious concerns. In this paper, improved fuzzy-based MC-UPQC is proposed for improving the PQ in grid connected wind energy system (WES). Improved fuzzy, that is hybrid fuzzy incremental conductance controlled MC-UPQC is the novelty in this study. Fuzzy incremental conductance is introduced to search the maximum power point in wind energy conversion system. From the grid, the proposed controller joined with MC-UPQC improve the WES. At the same time, the system can recompense the current and voltage imperfections in adjacent feeders. A normal dc-link capacitor is shared by all converters in this configuration. Hence, the power transfer from one to adjacent feeders is possible. The proposed work is simulated in MATLAB/SIMULINK platform. Through the hysteresis controller, the performance analysis is carried out. Finally the results are compared with existing controller's namely proportional integral and a conventional fuzzy logic controller. Thus, the overall performance can display the effectiveness of the proposed approach. Compared to other two existing approaches, the proposed scheme achieves very low THD values i.e. 3.53% for load 1 and 3.25% for load 2.																	1868-5137	1868-5145															10.1007/s12652-020-01855-w		MAY 2020											
J								A kernel support vector machine based anomaly detection using spatio-temporal motion pattern models in extremely crowded scenes	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Object tracking; Video surveillance; Anomaly detection; KSVM; Texture feature; Extended Kalman filters; Learning vector quantization	MOVING OBJECT DETECTION; CLASSIFICATION; VIDEO	Millions of security cameras were placed in public spaces, generating large quantities of video data. There is a need to develop smart techniques to identify and classify objects tracking instantly. Most of them concentrate on spatial information, resulting in exposure to noise and background movement. In addition, monitoring individuals in overcrowded scenes is a difficult task, due to the variation of movement and appearance created by the large amount of people in the scene. In this paper, initially, utilizing threshold value, the video is split into frames. Then segmentation of moving objects using Extended Kalman Filters (EKF) to improve the accuracy of the classification. Instead, to distinguish between the foreground object and the background object, the texture features is removed. The artifacts are then labelled using improved Learning Vector Quantization (LVQ) for efficient identification of anomalies. Also an effective classification of Kernel Support Vector Machine (KSVM) predicated on anomaly detection has been suggested utilizing spatio-temporal movement pattern models in overcrowded scenes to solve these problems. Hence, KSVM is more advantageous in accuracy which is used to monitor the object. The result shows the performance of the proposed KSVM obtained high performance compared with SVM and Hidden Markov Model (HMM).																	1868-5137	1868-5145															10.1007/s12652-020-02000-3		MAY 2020											
J								Task scheduling system for UAV operations in agricultural plant protection environment	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Multi-objective optimization; Dragonfly Algorithm; Agricultural plant protection; UAV	FLIGHT; ALGORITHM	Unmanned aerial vehicle (UAV) can greatly reduce manpower in agricultural plant protection such as watering, sowing, pesticide spraying. It helps in creating an autonomous manufacturing system by executing tasks with less human intervention in time-efficient manner. Consequently, reasonable and efficient planning is one essential component to be focused on; yet to the best of our knowledge, there are few research on practical agricultural plant protection UAV scheduling. This work proposes method to solve the Agricultural Plant Protection UAV scheduling problem in practice such as pesticide spraying, flying and charging. At first, tasks are assigned to UAVs, then schedules are planned for UAVs. To find a near optimal schedule quickly, our method is incorporated with Dragonfly Algorithm. This proposed method is implemented and tested on datasets generated based on a real agricultural plant protection environment. Performance evaluation of our method is discussed in detail and the set of parameters that determining the best solution is reported.																	1868-5137	1868-5145															10.1007/s12652-020-01969-1		MAY 2020											
J								Prediction of mechanical and penetrability properties of cement-stabilized clay exposed to sulfate attack by use of soft computing methods	NEURAL COMPUTING & APPLICATIONS										Cement-stabilized soil; Strength; Penetrability; BPNN; ANFIS; Soft computing	STRENGTH DEVELOPMENT; BEHAVIOR; DENSITY	Similar to its effects on any type of cementitious composite, it is a well-known fact that sulfate attack has also a negative influence on engineering behavior of cement-stabilized soils. However, the level of degradation in engineering properties of the cement-stabilized soils still needs more scientific attention. In the light of this, a database including a total of 260 unconfined compression and chloride ion penetration tests on cement-stabilized kaolin specimens exposed to sulfate attack was constituted. The data include information about cement type (sulfate resistant-SR; normal portland (N) and pozzolanic-P), and its content (0, 5, 10 and 15%), sulfate type (sodium or magnesium sulfate) as well as its concentration (0.3, 0.5, 1%) and curing period (1, 7, 28 and 90 days). Using this database, linear and nonlinear regression analysis (RA), backpropagation neural networks and adaptive neuro-fuzzy inference techniques were employed to question whether these methods are capable of predicting unconfined compressive strength and chloride ion penetration of cement-stabilized clay exposed to sulfate attack. The results revealed that these methods have a great potential in modeling the strength and penetrability properties of cement-stabilized clays exposed to sulfate attack. While the performance of regression method is at an acceptable level, results show that adaptive neuro-fuzzy inference systems and backpropagation neural networks are superior in modeling.																	0941-0643	1433-3058				NOV	2020	32	21			SI		16707	16722		10.1007/s00521-020-04972-x		MAY 2020											
J								Analysis of hydraulic conductivity of fractured groundwater flow media using artificial neural network back propagation	NEURAL COMPUTING & APPLICATIONS										Artificial neural network backpropagation; Clustering of parameters; Fractured media; Lithology permeability index; Hydraulic conductivity; Rock quality designation	IRIAN-JAYA; ROCK; DESIGN; MINE	Groundwater flow in the Grasberg open-pit mine is governed by fractured flow media. Groundwater modeling in fractured media requires detailed hydraulic conductivity (K) value distribution to illustrate hydrogeological conditions of the Grasberg open-pit mine properly. The value of K is estimated by using the hydraulic conductivity (HC) system method based on rock quality designation (RQD), lithology permeability index (LPI), depth index (DI), and gouge content designation (GCD) data. Accordingly, all parameters must be available, but in this case, this information are only partially available. This paper proposes a method to solve this problem with artificial neural network (ANN) through a five approach scenario to find the K value with a model of incomplete empirical parameters. The artificial neural network back propagation (ANNBP) method is used to estimate the distributed K value based on the distributed observed RQD and LPI, as shown in the block model. This paper uses five optimum architectural schemes consisting of learning rate selection, momentum coefficient, number of nodes, number of hidden layers, activation method (sigmoid, tangent hyperbolic and Gaussian). Verification of the results of the K value is approached by the statistical approaches determination coefficient (R-2), correlation coefficient (R), standard error of estimate (SEE), root-mean-squared (RMS), and normal root mean square (NRMS). ANNBP modeling used data for training and testing based on packer test and slug test as many as 49 points. Through five scenarios, it was found that scenario with learning rates of 10(-5), momentum coefficient 0.1, number of nodes 10, number of hidden layer 5, with hyperbolic tangent activation methods is the most optimum result with R-2 = 0.822, the accuracy of predicting/validating results is expressed by using R-2 = 0.863, with R = 0.929, and the error percentage = 3.67%. The K value prediction results are used as an input for groundwater modeling. The modeling results show a significant correlation with high validity between the data extracted from the model and field observation. In a 3D model, the K value distribution is similar to the field RQD data distribution. Furthermore, the K value distribution results are clustered in order to understand the relationship between geological as well as geotechnical data. The highest K values are found in volcanic breccia rocks (DLMVB), volcanic sediments (DLMVS), volcanic andesite (DLMVA) and quaternary glacial till. The geological condition is confirmed by the average rock RQD value between 12.4 and 39.6%.																	0941-0643	1433-3058															10.1007/s00521-020-04970-z		MAY 2020											
J								Anatomical region identification in medical X-ray computed tomography (CT) scans: development and comparison of alternative data analysis and vision-based methods	NEURAL COMPUTING & APPLICATIONS										Image classification; Computed tomography; Pattern recognition; Region detection; Machine learning	NETWORKS	Many medical image processing applications rely on targeted regions of interest within a larger volumetric image. Whole-body scans represent an extreme case in which large volumes must be broken into smaller sub-volumes for regional analysis. In this work, we sought automatic solutions to divide medical X-ray computed tomography (CT) images into six main anatomical regions: head, neck, chest, abdomen, pelvis and legs. We implemented and compared three methods: (1) an analytical approach which does not require training and solely relies on utilizing critical points in image intensity profiles to derive cut-planes that divide the scan into the mentioned regions, (2) a classical convolutional neural network (CNN) approach, which classifies each transaxial 2D plane independently and then concatenates classification results, and (3) CNN followed by a context-based correction algorithm (CBCA) which improves the CNN classification using positional relationships between all CT slices. The analytical approach achieved acceptable accuracy for anatomical region segmentation without the need for explicit data labeling and was effective for batch labeling whole-body CTs, greatly reducing manual labeling efforts. CNNs achieved superior accuracy and allowed for rapid development and training, but required labeled data and were susceptible to produce discontinuous anatomical regions and therefore ambiguous anatomical boundaries. Post hoc correction of CNN results using CBCA overcame these limitations, achieving nearly perfect CT slice labeling and anatomical region segmentation.																	0941-0643	1433-3058															10.1007/s00521-020-04923-6		MAY 2020											
J								Brain storm optimization using a slight relaxation selection and multi-population based creating ideas ensemble	APPLIED INTELLIGENCE										Brain storm optimization; Multi-population; Creating ideas; Slight relaxation selection mechanism	DIFFERENTIAL EVOLUTION; ALGORITHM; COLONY	Brain storm optimization is a swarm intelligence algorithm inspired by the brainstorming process in human beings. Many researchers have paid much more attention to it, and many attempts have been made to improve it's performance. The search ability of brain storm optimization is maintained by the creating process of ideas, but it still suffers from sticking into stagnation during exploitation phase. This paper proposes a novel brain storm optimization variant, named RMBSO, in which a slight relaxation selection and multi-population based creating ideas ensemble are employed to improve the performance of brain storm optimization on global optimization problem with diverse landscapes. Firstly, the basic framework of original brain storm optimization is imbedded into multi-population based ensemble of heterogeneous but complementary creating ideas to make the algorithm jump out of stagnation with strong searching ability. Secondly, a new triangular mutation ruler and a simple partition of subpopulations are designed to better balance exploration and exploitation. Thirdly, a slight relaxation selection mechanism instead of greedy choice is first developed to keep the population's diversity. Finally, extensive experiments on the suit of CEC 2015 benchmark functions and statistical comparisons are executed. Experimental results indicate that the proposed algorithm is significantly better than, or at least comparable to the state-of-the-art brain storm optimization variants and several improved differential evolution algorithms.																	0924-669X	1573-7497				OCT	2020	50	10					3137	3161		10.1007/s10489-020-01690-8		MAY 2020											
J								An intrusion detection approach based on improved deep belief network	APPLIED INTELLIGENCE										Intrusion detection; Deep belief network (DBN); Probabilistic mass function (PMF); likelihood function; Kullback-Leibler (KL) divergence	INTERNET	In today's interconnected society, cyberattacks have become more frequent and sophisticated, and existing intrusion detection systems may not be adequate in the complex cyberthreat landscape. For instance, existing intrusion detection systems may have overfitting, low classification accuracy, and high false positive rate (FPR) when faced with significantly large volume and variety of network data. An intrusion detection approach based on improved deep belief network (DBN) is proposed in this paper to mitigate the above problems, where the dataset is processed by probabilistic mass function (PMF) encoding and Min-Max normalization method to simplify the data preprocessing. Furthermore, a combined sparsity penalty term based on Kullback-Leibler (KL) divergence and non-mean Gaussian distribution is introduced in the likelihood function of the unsupervised training phase of DBN, and sparse constraints retrieve the sparse distribution of the dataset, thus avoiding the problem of feature homogeneity and overfitting. Finally, simulation experiments are performed on the NSL-KDD and UNSW-NB15 public datasets. The proposed method achieves 96.17% and 86.49% accuracy, respectively. Experimental results show that compared with the state-of-the-art methods, the proposed method achieves significant improvement in classification accuracy and FPR.																	0924-669X	1573-7497				OCT	2020	50	10					3162	3178		10.1007/s10489-020-01694-4		MAY 2020											
J								Detecting salient regions in a bi-temporal hyperspectral scene by iterating clustering and classification	APPLIED INTELLIGENCE										change detection; clustering; classification; hyperspectral data	UNSUPERVISED CHANGE DETECTION; EXTREME LEARNING-MACHINE; LAND-COVER; ALGORITHM; IMAGES; FUZZY; KERNEL; GPU	Hyperspectral (HS) images captured from Earth by satellite and aircraft have become increasingly important in several environmental and ecological contexts (e.g. agriculture and urban areas). In the present study we propose an iterative learning methodology for the change detection of HS scenes taken at different times in the same areas. It cascades clustering and classification through iterative learning, in order to separate salient regions, where a change occurs in the scene from the unchanged background. The iterative learning is evaluated in both the clustering and the classification steps. The experiments performed with the proposed methodology provide encouraging results, also compared to several recent state-of-the-art competitors.																	0924-669X	1573-7497				OCT	2020	50	10					3179	3200		10.1007/s10489-020-01701-8		MAY 2020											
J								A force levels and gestures integrated multi-task strategy for neural decoding	COMPLEX & INTELLIGENT SYSTEMS										Neural decoding; Multi-task learning (MTL); Pseudo-task augmentation (PTA); Convolutional neural network (CNN)	SEMG; CLASSIFICATION; RECOGNITION; SIGNALS; HAND; ARM	This paper discusses the problem of decoding gestures represented by surface electromyography (sEMG) signals in the presence of variable force levels. It is an attempt that multi-task learning (MTL) is proposed to recognize gestures and force levels synchronously. First, methods of gesture recognition with different force levels are investigated. Then, MTL framework is presented to improve the gesture recognition performance and give information about force levels. Last but not least, to solve the problem that using the greedy principle in MTL, a modified pseudo-task augmentation (PTA) trajectory is introduced. Experiments conducted on two representative datasets demonstrate that compared with other methods, frequency domain information with convolutional neural network (CNN) is more suitable for gesture recognition with variable force levels. Besides, the feasibility of extracting features that are closely related to both gestures and force levels is verified via MTL. By influencing learning dynamics, the proposed PTA method can improve the results of all tasks, and make it applicable to the case where the main tasks and auxiliary tasks are clear.																	2199-4536	2198-6053				OCT	2020	6	3					469	478		10.1007/s40747-020-00140-9		MAY 2020											
J								Hadamard Matrix Guided Online Hashing	INTERNATIONAL JOURNAL OF COMPUTER VISION										Binary code; Online hashing; Hadamard matrix; Image retrieval		Online image hashing has attracted increasing research attention recently, which receives large-scale data in a streaming manner to update the hash functions on-the-fly. Its key challenge lies in the difficulty of balancing the learning timeliness and model accuracy. To this end, most works follow a supervised setting, i.e., using class labels to boost the hashing performance, which defects in two aspects: first, strong constraints, e.g., orthogonal or similarity preserving, are used, which however are typically relaxed and lead to large accuracy drops. Second, large amounts of training batches are required to learn the up-to-date hash functions, which largely increase the learning complexity. To handle the above challenges, a novel supervised online hashing scheme termed Hadamard Matrix Guided Online Hashing (HMOH) is proposed in this paper. Our key innovation lies in introducing Hadamard matrix, which is an orthogonal binary matrix built via Sylvester method. In particular, to release the need of strong constraints, we regard each column of Hadamard matrix as the target code for each class label, which by nature satisfies several desired properties of hashing codes. To accelerate the online training, LSH is first adopted to align the lengths of target code and to-be-learned binary code. We then treat the learning of hash functions as a set of binary classification problems to fit the assigned target code. Finally, extensive experiments on four widely-used benchmarks demonstrate the superior accuracy and efficiency of HMOH over various state-of-the-art methods. Codes can be available at https:// github. com/lmbxmu/ mycode..																	0920-5691	1573-1405				SEP	2020	128	8-9			SI		2279	2306		10.1007/s11263-020-01332-z		MAY 2020											
J								3DFaceGAN: Adversarial Nets for 3D Face Representation, Generation, and Translation	INTERNATIONAL JOURNAL OF COMPUTER VISION										3D; Face; GAN; Generation; Translation; Representation		Over the past few years, Generative Adversarial Networks (GANs) have garnered increased interest among researchers in Computer Vision, with applications including, but not limited to, image generation, translation, imputation, and super-resolution. Nevertheless, no GAN-based method has been proposed in the literature that can successfully represent, generate or translate 3D facial shapes (meshes). This can be primarily attributed to two facts, namely that (a) publicly available 3D face databases are scarce as well as limited in terms of sample size and variability (e.g., few subjects, little diversity in race and gender), and (b) mesh convolutions for deep networks present several challenges that are not entirely tackled in the literature, leading to operator approximations and model instability, often failing to preserve high-frequency components of the distribution. As a result, linear methods such as Principal Component Analysis (PCA) have been mainly utilized towards 3D shape analysis, despite being unable to capture non-linearities and high frequency details of the 3D face-such as eyelid and lip variations. In this work, we present 3DFaceGAN, the first GAN tailored towards modeling the distribution of 3D facial surfaces, while retaining the high frequency details of 3D face shapes. We conduct an extensive series of both qualitative and quantitative experiments, where the merits of 3DFaceGAN are clearly demonstrated against other, state-of-the-art methods in tasks such as 3D shape representation, generation, and translation.																	0920-5691	1573-1405				NOV	2020	128	10-11			SI		2534	2551		10.1007/s11263-020-01329-8		MAY 2020											
J								Partial label metric learning by collapsing classes	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Partial label learning; Metric learning; Collapsing classes; Weakly supervised data		Partial label learning (PLL) is a weakly supervised learning framework proposed recently, in which the ground-truth label of training sample is not precisely annotated but concealed in a set of candidate labels, which makes the accuracy of the existing PLL algorithms is usually lower than that of the traditional supervised learning algorithms. Since the accuracy of a learning algorithm is usually closely related to its distance metric, the metric learning technologies can be employed to improve the accuracy of the existing PLL algorithms. However, only a few PLL metric learning algorithms have been proposed up to the present. In view of this, a novel PLL metric learning algorithm is proposed by using the collapsing classes model in this paper. The basic idea is first to take each training sample and its neighbor with shared candidate labels as a similar pair, while each training sample and its neighbor without shared candidate labels as a dissimilar pair, then two probability distributions are defined based on the distance and label similarity of these pairs, respectively, finally the metric matrix is obtained via minimizing the Kullback-Leibler divergence of these two probability distributions. Experimental results on six UCI data sets and four real-world PLL data sets show that the proposed algorithm can obviously improve the accuracy of the existing PLL algorithms.																	1868-8071	1868-808X				NOV	2020	11	11					2453	2460		10.1007/s13042-020-01129-z		MAY 2020											
J								Real-time Mamdani-like fuzzy and fusion-based fuzzy controllers for balancing two-wheeled inverted pendulum	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Two-wheeled inverted pendulum; Fuzzy logic controllers; Real-time control; Mamdani-like fuzzy; Fusion-based fuzzy	DESIGN	Two wheeled inverted pendulum (TWIP) resembles many industrial as well as real life applications. TWIP is underactuated, nonlinear and unstable system. Therefore, it is widely used as benchmark for illustrating control concepts, theories, and algorithms that deal with these challenges. The main motivations of this paper are designing controllers that deal with lack of information that accompanying an inexpensive cheap TWIP and verified its stability, proper speed, robustness and smooth tracking. Consequently, intelligent technique instead of conventional ones that depends on mathematical models should be used. In this article hybrid controllers, consisting of fuzzy controllers and three mode controllers' (PID), are developed. Two major techniques are used, Mamdani-Like Fuzzy and Fusion-Based function, to design five controllers. Moreover, dynamic indices and steady state indices are implemented to choose the most proper controllers among various designed ones. A new criterion is defined to measure steady state performance of oscillation. All controllers were tested in real-time to control the angle of the TWIP for stability and disturbance rejection. The experimental results showed that both strategies of Mamdani-Like Fuzzy and Fusion-Based fuzzy can control and balance adequately the TWIP, but the PD-Like fuzzy yielded more better performance.																	1868-5137	1868-5145															10.1007/s12652-020-01991-3		MAY 2020											
J								Deep neural network combined with MapReduce for abnormal data mining and detection in cloud storage	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Hadoop distributed file system; Deep neural network; MapReduce; Cloud storage; Abnormal data mining and detection		Aiming at the abnormal data behavior such as huge amount of data and easy to be stolen or lost in the process of distributed cloud computing in cloud storage environment, an abnormal data mining and detection algorithm of MapReduce based on Hadoop distributed file system (HDFS) and deep neural network is proposed. Firstly, the algorithm analyzes the MAC timestamp characteristics generated by HDFS folder replication, establishes the detection and measurement methods of replication behavior, and ensures that all the patterns that lead to data anomalies, including theft, packet loss and malicious attack, can be detected. Secondly, the algorithm combines deep neural network to design a task partition strategy suitable for arbitrary MapReduce data, and records the input dataset of HDFS hierarchical relationship. Finally, combined with the parallel processing ability of MapReduce, the efficient analysis of massive timestamp data is realized by designing the dataset and algorithm execution scheme suitable for MapReduce task partition. The experimental results show that the algorithm can control the missed detection rate and the number of false detection folders through the segmentation detection strategy. Compared with the existing big data anomaly detection method, the algorithm has higher execution efficiency and good scalability.																	1868-5137	1868-5145															10.1007/s12652-020-01996-y		MAY 2020											
J								Rule extraction from neural network trained using deep belief network and back propagation	KNOWLEDGE AND INFORMATION SYSTEMS										Neural network; Classification; Rule extraction; Back propagation; Deep learning; Deep belief network; Restricted Boltzmann machine	ALGORITHM	Representing the knowledge learned by neural networks in the form of interpretable rules is a prudent technique to justify the decisions made by neural networks. Heretofore many algorithms exist to extract symbolic rules from neural networks, but among them, a few extract rules from deep neural networks trained using deep learning techniques. So, this paper proposes an algorithm to extract rules from a multi-hidden layer neural network, pre-trained using deep belief network and fine-tuned using back propagation. The algorithm analyzes each node of a layer and extracts knowledge from each layer separately. The process of knowledge extraction from the first hidden layer is different from the other layers. Consecutively, the algorithm combines all the knowledge extracted and refines them to construct a final ruleset consisting of symbolic rules. The algorithm further subdivides the subspace of a rule in the ruleset if it satisfies certain conditions. Results show that the algorithm extracted rules with higher accuracy compared to some existing rule extraction algorithms. Other than accuracy, the efficacy of the extracted rules is also validated with fidelity and various other performance measures.																	0219-1377	0219-3116				SEP	2020	62	9					3753	3781		10.1007/s10115-020-01473-0		MAY 2020											
J								Adaptive ELM neural computing framework with fuzzy PI controller for speed regulation in permanent magnet synchronous motors	SOFT COMPUTING										PI controller; Speed regulation; Artificial bee colony; Fuzzy controller; Adaptive extreme learning machine; Permanent magnet synchronous motor	PMSM; NETWORK; ALGORITHM; FILTER	In this work, a new adaptive extreme learning machine (ELM) neural network-based fuzzy controller is designed and simulated for implementing speed regulation in a permanent magnet synchronous motor. ELM is a neural model wherein the number of hidden neurons to be placed in the hidden layer is tuned during the process of neural network training itself. A new adaptive ELM model is developed for placing the number of hidden neurons in the hidden layer, and this new adaptive ELM is tuned with artificial bee colony (ABC) algorithm for optimizing its weight parameters and also the number of hidden neurons. Fuzzy proportional-integral (PI) controller is developed in this work in order to eliminate the steady-state error. The new adaptive ELM neural model optimized with ABC algorithm is applied to tune the input parameters of the fuzzy PI controller and also on optimizing the rules and fuzzy membership functions. The optimized adaptive ELM neural network-based fuzzy PI controller is utilized to investigate the speed regulation of permanent magnet synchronous motor (PMSM) in this work. The developed new PI controller with the PMSM is tested for its performance characteristics and to prove its validity is compared with the traditional controller and other heuristic controllers proposed in earlier literature works.																	1432-7643	1433-7479				JUL	2020	24	14					10963	10980		10.1007/s00500-020-04994-6		MAY 2020											
J								A novel Chaotic Flower Pollination-based intrusion detection framework	SOFT COMPUTING										Intrusion detection system; Flower Pollination Algorithm; Chaotic distribution; Feature selection; support vector machine	PARTICLE SWARM OPTIMIZATION; DETECTION SYSTEM; FEATURE-SELECTION; ALGORITHM; CLASSIFICATION; ATTACKS	With the rise of network on handheld devices, security of the network has become critical issue. Intrusion detection system is used to predict intrusive packets on network; two-step procedure has been used to predict the intrusion, i.e., feature selection and then classification. Firstly, unwanted and expandable features in data lead to network classification problem which affect the decision capability of the classifiers, so we need optimize feature selection technique. Feature selection technique used in this paper is based on the correlation information known as correlation-based feature selection (CFS). In this paper, CFS's search algorithm is implemented using Chaotic Flower Pollination Algorithm (CFPA) that logically selects the most favorable features for classification referred as CFPA-CFS. Further, hybridization of CFPA and support vector machine classifier is implemented and named as CFPSVM. Finally, novel IDS framework uses CFPA-CFS and CFPSVM in sequence to predict the intrusion. Further, performance of proposed framework is evaluated using two intrusion detection evaluation datasets, namely KDDCup99 and NSL-KDD. The results demonstrate that proposed CFPA-CFS contributes more critical features for CFPSVM to achieve better accuracy compared with the state-of-the-art methods.																	1432-7643	1433-7479				NOV	2020	24	21					16249	16267		10.1007/s00500-020-04937-1		MAY 2020											
J								Multi-objective evolutionary algorithm for solving energy-aware fuzzy job shop problems	SOFT COMPUTING										Job shop scheduling; Fuzzy durations; Multi-objective; Due dates; Energy efficiency; Genetic algorithm	TOTAL WEIGHTED TARDINESS; SCHEDULE GENERATION SCHEMES; GENETIC ALGORITHM; PERFORMANCE ASSESSMENT; LOCAL SEARCH; SETUP TIMES; OPTIMIZATION; CONSUMPTION; OPTIMIZERS	A growing concern about the environmental impact of manufacturing processes and in particular the associated energy consumption has recently driven some researchers within the scheduling community to consider energy costs in addition to more traditional performance-related measures, such as satisfaction of due-date commitments. Recent research is also devoted to narrowing the gap between real-world applications and academic problems by handling uncertainty in some input data. In this paper, we address the job shop scheduling problem, a well-known hard problem with many applications, using fuzzy sets to model uncertainty in processing times and with the target of finding solutions that perform well with respect to both due-date fulfilment and energy efficiency. The resulting multi-objective problem is solved using an evolutionary algorithm based on the NSGA-II procedure, where the decoding operator incorporates a new heuristic procedure in order to improve the solutions' energy consumption. This heuristic is based on a theoretical analysis of the changes in energy consumption when a solution is subject to slight changes, referred to as local right shifts. The experimental results support the theoretical study and show the potential of the proposal.																	1432-7643	1433-7479				NOV	2020	24	21					16291	16302		10.1007/s00500-020-04940-6		MAY 2020											
J								MapReduce-based big data framework using modified artificial neural network classifier for diabetic chronic disease prediction	SOFT COMPUTING										MapReduce; Diabetic chronic disease; Feature selection; Aggregation function; Disease prediction; Artificial neural networks and big data	RECORDS	Recently, healthcare data consist of an enormous amount of information, which is challenging to maintain by manual methods. Due to the development of big data in the communities of biomedical and health care, accurate study of the medical data helps the recognition of the disease in early stage, patient care and community services. It mainly focuses on predicting and exploring the conditions due to some significant effects on health which are on the increase in multiple cities. The existing system in the medical field cannot extract complete information from the chronic disease database. It is complicated for the healthcare practitioner to analyze and diagnose constant disease since it plays a challenging task. This paper presents a modified artificial neural network (ANN) classifier technique with a MapReduce framework for the prediction of disease. For preprocessing, min-max normalization is carried out to enhance the accuracy of system. This MapReduce is applied for providing a feasible framework in predictive programming algorithms for the map and reduce functions. This is a simple programming interface, which helps in efficiently solving predictive problems. The primary intention of the proposed system is to analyze accurate, fast and optimal results on chronic disease datasets. It increases the throughput and redundancy in cases of retrieving the vast data. Thus, integrating a modified ANN classifier with a reduced framework is useful in providing better outcomes. The experimental results over chronic diabetic dataset prove that the proposed artificial neural network with MapReduce structure is capable of predicting the precision, sensitivity and specificity level modified on comparing with other existing deep neural network approaches.																	1432-7643	1433-7479				NOV	2020	24	21					16335	16345		10.1007/s00500-020-04943-3		MAY 2020											
J								Semantic analysis-based relevant data retrieval model using feature selection, summarization and CNN	SOFT COMPUTING										Semantic analysis; Feature selection; Data summarization; Classification; Deep neural network; Semantic similarity	TEXT; CLASSIFICATION; PREDICTION; SYSTEM	Semantic analysis is playing a major role and task in text mining process caused by the presence of huge number of relevant and irrelevant data in Internet and other resources. Here, the semantic-based text summarization must be incorporated for the successful relevant data extraction by using data classification. The accurate classification process is done by using deep learning techniques recently. However, no existing model is achieved reasonable relevancy accuracy. For overcoming the drawbacks, we propose an effective semantic analysis-based relevant data retrieval model for retrieving the relevant data from local repository or web applications in Internet. This new model consists of (i) semantic similarity-based feature selection and (ii) enrichment technique, (iii) data summarization technique and iv) text relationship-based deep neural network classifier. Here, we propose a new semantic analysis-based feature selection algorithm to select the similarity indexed relevant data from local repositories or web applications. In addition, a new semantic-based data summarization technique is also introduced for summarizing the text that is available in the online resources. Finally, a new semantic similarity-based deep neural network-based classifier is also introduced for categorizing the data according to the semantic relation. The proposed model is proved the effectiveness of the data retrieval process by conducting various experiments based on the relevant data extraction from Internet resources, and it also tested with the recognized datasets.																	1432-7643	1433-7479				NOV	2020	24	22					16983	17000		10.1007/s00500-020-04990-w		MAY 2020											
J								Probabilistic neural network-based 2D travel-time tomography	NEURAL COMPUTING & APPLICATIONS										Neural networks; Mixture density networks; Uncertainty estimation; Seismic tomography	INVERSE PROBLEMS; WAVE; VELOCITY	Travel-time tomography for the velocity structure of a medium is a highly nonlinear and nonunique inverse problem. Monte Carlo methods are becoming increasingly common choices to provide probabilistic solutions to tomographic problems but those methods are computationally expensive. Neural networks can often be used to solve highly nonlinear problems at a much lower computational cost when multiple inversions are needed from similar data types. We present the first method to perform fully nonlinear, rapid and probabilistic Bayesian inversion of travel-time data for 2D velocity maps using a mixture density network. We compare multiple methods to estimate probability density functions that represent the tomographic solution, using different sets of prior information and different training methodologies. We demonstrate the importance of prior information in such high-dimensional inverse problems due to the curse of dimensionality: unrealistically informative prior probability distributions may result in better estimates of the mean velocity structure; however, the uncertainties represented in the posterior probability density functions then contain less information than is obtained when using a less informative prior. This is illustrated by the emergence of uncertainty loops in posterior standard deviation maps when inverting travel-time data using a less informative prior, which are not observed when using networks trained on prior information that includes (unrealistic) a priori smoothness constraints in the velocity models. We show that after an expensive program of network training, repeated high-dimensional, probabilistic tomography is possible on timescales of the order of a second on a standard desktop computer.																	0941-0643	1433-3058				NOV	2020	32	22			SI		17077	17095		10.1007/s00521-020-04921-8		MAY 2020											
J								Modelling the vibration response of a gas turbine using machine learning	EXPERT SYSTEMS										gas turbine; genetic programming; machine learning; mechanical vibrations		This work deals with modelling the vibration response of a gas turbine obtained during the start-up process until reaching the nominal speed for power generation. Analysing the vibrations of a complex systems like a gas turbine is useful for the diagnostic of faults or damages in the internal mechanical components of the different stages that integrate a turbine. This work focuses on the study of the shaft vibrations of the bearing radial type mounted between the shaft and the bearing compressor associated with the speed of the turbine. This relationship is studied using experimental data collected from a particular gas turbine model. In particular, we propose a methodology to synthesize a computational model following a supervised learning approach implemented through different machine learning techniques, including a multi-layers perceptron network, support vector machine (SVM), random forest (RF) and genetic programming (GP) with local search. Results show that SVM, RF and GP perform very well in this task, producing accurate predictive models. Moreover, there are some interesting trade-offs between the methods, regarding generalization error, overfitting and model interpretability that are relevant for future applications and research.																	0266-4720	1468-0394				OCT	2020	37	5			SI				e12560	10.1111/exsy.12560		MAY 2020											
J								A new framework for predicting customer behavior in terms of RFM by considering the temporal aspect based on time series techniques	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Customer behavior; RFM model; Time series analysis; Forecasting	DATA MINING TECHNIQUES; SEGMENTATION; PATTERNS; IDENTIFICATION; LOYALTY	Data mining and in particular forecasting tools and techniques are being increasingly exploited by businesses to predict customer behavior and to formulate effective marketing programs. Conventionally, customer segmentation approaches are utilized when dealing with a large population of customers. Inspired by this idea, a new methodology is proposed in this study to perform segment-level customer behavior forecasting. To keep the dynamic nature of customer behavior, customer behavior is represented as a time series. Therefore, customer behavior forecasting is changed into a time series forecasting problem. The proposed methodology contains two main components i.e. clustering and forecasting. In the clustering phase, time series are clustered using time series clustering algorithms, and then, in the forecasting phase, the behavior of each segment is predicted via time series forecasting techniques. The main objective is to predict future behavior at segment level. The forecasting component also consists of a combined method exploiting the concept of forecast fusion. The combined method employs a pool of forecasters both from traditional time series forecasting and computational intelligence methods. To test the usefulness of the proposed method, a case study is carried out using the data of customers' point of sale (POS) in a bank. The results of the experiments demonstrate that the combined method outperforms all other individual forecasters in terms of symmetric mean absolute percentage error (SMAPE). The proposed methodology can be correspondingly applied in other areas and applications of time series forecasting.																	1868-5137	1868-5145															10.1007/s12652-020-02015-w		MAY 2020											
J								Monarch butterfly optimized control with robustness analysis for grid tied centralized and distributed power generations	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Intelligent optimizations; Modelling of modern power systems; Load frequency control; Heuristically tuned PID controllers	LOAD FREQUENCY CONTROL; DESIGN; SYSTEMS	Modern power systems have emerged as a distributed complex interconnection of conventional as well as renewable energy resources and varied loads. Intermittent and varied nature of wind power has further posed major challenges towards control of voltage profile variations and frequency deviations in such integrated distributed power systems. This is because, such integrated renewable energy resources result in overall reduced system inertia that introduces complexity to load frequency control problem. In this paper, recently proposed heuristic algorithm known as Monarch Butterfly based Optimization has been extended to address frequency control problem arising due to sudden source and load fluctuation in a three-area test power system. The considered test system consists of both distributed and centralized power generations using Wind, Hydro, Gas and Steam. The controllers optimized using the proposed approach were simulated on MATLAB(R) and Simulink(R) platform. Simulation results validate effectiveness of proposed strategy vis-a-vis other existing techniques using Moth Search Algorithm, Elephant Herding Optimization.																	1868-5137	1868-5145															10.1007/s12652-020-01992-2		MAY 2020											
J								A probabilistic logic approach to outcome prediction in team games using historical data and domain knowledge	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Parameter learning; MLE; Probabilistic logic; Prediction; Uncertainty; Historical data; Soccer match	INFERENCE; PROGRAMS	Relational data is structured and, in the real world, ambiguous. Logic can handle relations and probability can handle uncertainty. A probabilistic logic approach to learning can handle both relational structure and uncertainty in the data. Probabilistic logic approach works well with relational data. Incorporating domain knowledge in probabilistic logic approach further enhances learning, improving accuracy. A number of statistical techniques carry out predictive analytics based on historical data alone. Soccer, however, is a team game and the outcome of a soccer game depends on how well the team together and the players play against the opponent team. Thus, data about soccer games are better represented in relational form. In the present work, we propose to learn from soccer match data to predict their outcomes. We learn a model for the prediction of soccer game outcomes, taking into account the history of the matches played by the teams. We frame the background knowledge as rules in the logic program to enhance the prediction. Compared to the traditional machine learning approaches to soccer game outcome prediction, probabilistic logic approach is found to result in significant improvement in prediction accuracy.																	1868-5137	1868-5145															10.1007/s12652-020-01989-x		MAY 2020											
J								Counting frequent patterns in large labeled graphs: a hypergraph-based approach	DATA MINING AND KNOWLEDGE DISCOVERY										Data mining; Graph mining; Support measure; Hypergraph	EFFICIENT ALGORITHM; SUBGRAPH	In recent years, the popularity of graph databases has grown rapidly. This paper focuses on single-graph as an effective model to represent information and its related graph mining techniques. In frequent pattern mining in a single-graph setting, there are two main problems: support measure and search scheme. In this paper, we propose a novel framework for designing support measures that brings together existing minimum-image-based and overlap-graph-based support measures. Our framework is built on the concept of occurrence/instance hypergraphs. Based on such, we are able to design a series of new support measures: minimum instance (MI) measure, and minimum vertex cover (MVC) measure, that combine the advantages of existing measures. More importantly, we show that the existing minimum-image-based support measure is an upper bound of the MI measure, which is also linear-time computable and results in counts that are close to number of instances of a pattern. We show that not only most major existing support measures and new measures proposed in this paper can be mapped into the new framework, but also they occupy different locations of the frequency spectrum. By taking advantage of the new framework, we discover that MVC can be approximated to a constant factor (in terms of number of pattern nodes) in polynomial time. In contrast to common belief, we demonstrate that the state-of-the-art overlap-graph-based maximum independent set (MIS) measure also has constant approximation algorithms. We further show that using standard linear programming and semidefinite programming techniques, polynomial-time relaxations for both MVC and MIS measures can be developed and their counts stand between MVC and MIS. In addition, we point out that MVC, MIS, and their relaxations are bounded within constant factor. In summary, all major support measures are unified in the new hypergraph-based framework which helps reveal their bounding relations and hardness properties.																	1384-5810	1573-756X				JUL	2020	34	4					980	1021		10.1007/s10618-020-00686-9		MAY 2020											
J								Finding every car: a traffic surveillance multi-scale vehicle object detection method	APPLIED INTELLIGENCE										Traffic surveillance; Vehicle object detection; YOLOv3; Convolutional neural networks (CNNs)		According to the problem that the multi-scale vehicle objects in traffic surveillance video are difficult to detect and the overlapping objects are prone to missed detection, an improved vehicle object detection method based on YOLOv3 was proposed. In order to extract feature more efficiently, we first use the inverted residuals technique to improve the convolutional layer of YOLOv3. To solve the multi-scale vehicle object detection problem, three spatial pyramid pooling(SPP) modules are added before each YOLO layer to obtain multi-scale information. In order to cope with the overlapping of vehicles in traffic videos, soft non maximum suppression (Soft-NMS) is used to replace non maximum suppression (NMS), thereby reducing the missing of predicted boxes due to vehicle overlaps. Our experiment results in the Car dataset and the KITTI dataset confirm that the proposed method achieves good detection results for vehicle objects of various scales in various scenes. Our method can meet the needs of practical applications better.																	0924-669X	1573-7497				OCT	2020	50	10					3125	3136		10.1007/s10489-020-01704-5		MAY 2020											
J								A method to multi-attribute decision-making based on interval-valued q-rung dual hesitant linguistic Maclaurin symmetric mean operators	COMPLEX & INTELLIGENT SYSTEMS										Multi-attribute decision-making; Interval-valued q-rung dual hesitant fuzzy sets; Interval-valued q-rung dual hesitant linguistic sets; Maclaurin symmetric mean	ORTHOPAIR FUZZY INFORMATION; AGGREGATION OPERATORS; SETS	The aim of this paper is to propose a new multi-attribute decision-making (MADM) method to rank all feasible alternatives in complex decision-making scenarios and determine the optimal one. To this end, we first propose the notion of interval-valued q-rung dual hesitant linguistic sets (IVq-RDHLSs) by combining interval-valued q-rung dual hesitant fuzzy (IVq-RDHF) sets with linguistic terms set. The proposed IVq-RDHLSs utilize IVq-RDHF membership and non-membership degrees to assess linguistic terms, so that they can fully express decision-makers' evaluation information. Additionally, some related concepts such as the operational rules, score and accuracy functions, and ranking method of IVq-RDHLSs are presented. Considering the good performance of the classical Maclaurin symmetric mean (MSM) in integrating fuzzy information, we further generalize MSM into IVq-RDHLSs to propose the interval-valued q-rung dual hesitant linguistic MSM operator, the interval-valued q-rung dual hesitant linguistic dual MSM operator, as well as their weighted forms. Afterwards, we study the applications of IVq-RDHLSs and their aggregation operators in decision-making and propose a new MADM method. Some real decision-making problems in daily life are employed to prove the rightness of the proposed method. We also attempt to demonstrate the advantages and superiorities of our proposed method through comparing with some other methods in this paper.																	2199-4536	2198-6053				OCT	2020	6	3					447	468		10.1007/s40747-020-00141-8		MAY 2020											
J								View-independent representation with frame interpolation method for skeleton-based human action recognition	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Action recognition; View-independent representation; Frame interpolation; Transfer CNN; Self-attention mechanism	FEATURES; FUSION; RGB	Human action recognition is an important branch of computer vision science. It is a challenging task based on skeletal data because of joints' complex spatiotemporal information. In this work, we propose a method for action recognition, which consists of three parts: view-independent representation, frame interpolation, and combined model. First, the action sequence becomes view-independent representations independent of the view. Second, when judgment conditions are met, differentiated frame interpolations are used to expand the temporal dimensional information. Then, a combined model is adopted to extract these representation features and classify actions. Experimental results on two multi-view benchmark datasets Northwestern-UCLA and NTU RGB+D demonstrate the effectiveness of our complete method. Although using only one type of action feature and a simple architecture combined model, our complete method still outperforms most of the referential state-of-the-art methods and has strong robustness.																	1868-8071	1868-808X				DEC	2020	11	12					2625	2636		10.1007/s13042-020-01132-4		MAY 2020											
J								Application of RBF neural network optimal segmentation algorithm in credit rating	NEURAL COMPUTING & APPLICATIONS										Optimal segmentation method; Radial basis function neural network; Credit grading; Classification data	SCORING MODEL; RISK	Credit rating is an important part of bank credit risk management. Since the traditional radial basis function network model is more susceptible to outliers and cannot effectively process the classification data, it is very sensitive in terms of the initial center and class width of the selected model. This paper mainly studies the application of the radial basis function neural network model combined with the optimal segmentation algorithm in the personal loan credit rating model of banks or other financial institutions. The optimal segmentation algorithm is improved and applied to the training of RBF neural network parameters in this paper to increase the center and width of the class, and the center and width of the RBF network model are further improved. Finally, the adaptive selection of the number of hidden nodes is realized by using the differential objective function of the class to adjust dynamically the structure of the radial basis function network model, which is used to establish the credit rating model. The experimental results show that the improved model has higher precision when dealing with non-numeric data, and the robustness of the improved model has been improved.																	0941-0643	1433-3058															10.1007/s00521-020-04958-9		MAY 2020											
J								Two calibrated meta-heuristics to solve an integrated scheduling problem of production and air transportation with the interval due date	SOFT COMPUTING										Integrated production-distribution; Scheduling; Meta-heuristics; Air transportation; Time window	VARIABLE NEIGHBORHOOD SEARCH; VEHICLE-ROUTING PROBLEM; GENETIC ALGORITHM; COORDINATED PRODUCTION; DELIVERY; OPTIMIZATION; MODEL	Contrary to previous methods in production management, today's approaches mainly focus on the whole supply chain parties' considerations. Considering production planning and distribution, as the two main functions in supply chain (SC) management, in an integrated manner in order to enhance the SC advantages is one of today's main dilemma. Here, we have firstly proposed and investigated the integrated production and air transportation scheduling problem with time windows for the due date to minimize the total SC costs. Since the problem was NP-hard, two new coordinated and integrated solution procedures have been presented based on meta-heuristics. Four algorithms (i.e., simulated annealing (SA), genetic algorithm, particle swarm optimization/district PSO (PSO/DPSO), and hybrid variable neighborhood search-simulated annealing (H-VNS-SA)) have been developed in both procedures. For the first time in literature, we probe different encoding schemes in the proposed algorithms. In addition, by using Taguchi experimental design, the parameters of the algorithms have been tuned. Besides, to study the behavior of the algorithms, different problem sizes have been generated and the results of two procedures have been compared together and discussed. Finally, a comparison of the proposed algorithms with some state-of-art optimized algorithms has been presented to prove statistically better performance of the proposed algorithms in most cases.																	1432-7643	1433-7479				NOV	2020	24	21					16383	16411		10.1007/s00500-020-04948-y		MAY 2020											
J								Modeling and simulation to optimize direct power control of DFIG in variable-speed pumped-storage power plant using teaching-learning-based optimization technique	SOFT COMPUTING										Pumped-storage power plant; Doubly fed induction generator; Teaching-learning-based optimization; Imperialist competitive algorithm; Particle swarm optimization algorithm	WIND TURBINE; DESIGN OPTIMIZATION; CONTROL STRATEGIES; SYSTEM; DRIVEN	The issues related to the optimal control of large-scale storage systems in electric power systems such as pumped storage (PS) plant have turned into vital challenges in the way of integrating renewable energy sources into power systems to provide reliable and economical electric energy. In this regard, this paper uses the direct power control strategy to model and simulate a variable-speed PS plant, which includes a doubly fed induction generator (DFIG). The active and the reactive power of the stator would be able to be controlled, separately. This approach has a better dynamic performance compared to other methods, while it would be quite simple to implement. But there are some shortfalls with this method, such as high ripple relating to the active power as well as reactive power together with the current harmonics. In this respect, the space vector modulation (SVM) is applied to eliminate these shortfalls. In the proposed control technique, including SVM, the dynamic performance of the studied DFIG unit is controlled using the proportional-integral (PI) controller. It should be noted that the teaching-learning-based optimization (TLBO) method is employed to tune the PI controller for controlling the DFIG system in the PS plant. Finally, in order to validate the performance of the suggested framework, a comparison is made between the results obtained by the TLBO and the ones reported by other optimization methods. The obtained results using the TLBO algorithm indicate better performance of the PI controller to reduce the ripples of the active and reactive power of the stator as well as the harmonic power.																	1432-7643	1433-7479				NOV	2020	24	22					16895	16915		10.1007/s00500-020-04984-8		MAY 2020											
J								Composite quantile-based classifiers	STATISTICAL ANALYSIS AND DATA MINING										classification; high-dimensional data; quantile-based classifier; supervised learning	DISCRIMINANT-ANALYSIS; CENTROIDS	Accurate classification of high-dimensional data is important in many scientific applications. We propose a family of high-dimensional classification methods based upon a comparison of the component-wise distances of the feature vector of a sample to the within-class population quantiles. These methods are motivated by the fact that quantile classifiers based on these component-wise distances are the most powerful univariate classifiers for an optimal choice of the quantile level. A simple aggregation approach for constructing a multivariate classifier based upon these component-wise distances to the within-class quantiles is proposed. It is shown that this classifier is consistent with the asymptotically optimal classifier as the sample size increases. Our proposed classifiers result in simple piecewise-linear decision rule boundaries that can be efficiently trained. Numerical results are shown to demonstrate competitive performance for the proposed classifiers on both simulated data and a benchmark email spam application.																	1932-1864	1932-1872				AUG	2020	13	4					337	353		10.1002/sam.11460		MAY 2020											
J								Embodied cognition: dimensions, domains and applications	ADAPTIVE BEHAVIOR										Embodiment; cognitive science; cognitive psychology; human cognition; philosophy of mind	ENACTIVE APPROACH; INTELLIGENCE; VARIETIES; DYNAMICS; BRAINS; SKILLS; MINDS; BODY	This article is intended as a response to Goldinger et al. and to all those, an increasing minority in the sciences, who still belittle the contribution of embodied cognition to our understanding of human cognitive behaviour. In this article (section 1), I introduce the notion of embodiment and explain its dimensions and reach. I review (section 2) a range of embodied cognition theories and highlight the principles and criteria on which they rely or draw from. I focus (section 3) on three crucial empirical domains in which an embodied perspective has driven novel insights about the relationship between mind and cognition. I argue that embodiment is not just a philosophical mantra empty of empirical content. I draw attention (section 4) to some of the recent ways in which principles underlying embodied cognition have begun to be applied in different fields (contemporary psychology). I review some of these interventions and suggest that discussing these applications not only provides additional evidence against any poverty claim but can also help moving the field forward in important ways. Contra Goldinger et al., I therefore conclude (section 5) that embodied cognition is a very fruitful research programme for the empirical sciences and that can adequately explain many aspects of human cognitive behaviour.																	1059-7123	1741-2633														UNSP 1059712320912963	10.1177/1059712320912963		MAY 2020											
J								CIMAX: collective information maximization in robotic swarms using local communication	ADAPTIVE BEHAVIOR										Swarm robotics; collective decision making; bioinspiration; decentralized self-organization; autonomous robotic swarms	DECISION-MAKING; BEHAVIOR; WRONGS; MODELS	Robotic swarms and mobile sensor networks are used for environmental monitoring in various domains and areas of operation. Especially in otherwise inaccessible environments, decentralized robotic swarms can be advantageous due to their high spatial resolution of measurements and resilience to failure of individuals in the swarm. However, such robotic swarms might need to be able to compensate misplacement during deployment or adapt to dynamical changes in the environment. Reaching a collective decision in a swarm with limited communication abilities without a central entity serving as decision-maker can be a challenging task. Here, we present the CIMAX algorithm for collective decision-making for maximizing the information gathered by the swarm as a whole. Agents negotiate based on their individual sensor readings and ultimately make a decision for collectively moving in a particular direction so that the swarm as a whole increases the amount of relevant measurements and thus accessible information. We use both simulation and real robotic experiments for presenting, testing, and validating our algorithm. CIMAX is designed to be used in underwater swarm robots for troubleshooting an oxygen depletion phenomenon known as "anoxia."																	1059-7123	1741-2633														1059712320912021	10.1177/1059712320912021		MAY 2020											
J								Multi-edge optimized LSTM RNN for video summarization	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Discrete wavelet transformation (DWT); Edge detection; Long short-term memory (LSTM); Recurrent neural network (RNN); Video summarization		Video summarization is an inevitable process in this developed communication world. The improvements in digital communication and filmless video recording technologies triggered the tremendous growth of storing and sharing variety of videos. Video summarization is used to optimize the searching and organizing process of different types of videos. Precision, Recall, F-Score and Processing time are the primary evaluation metrics of a video summarization procedure. A frequency domain multi-edge detection process and Multi-Edge optimized Long Short-Term Memory (LSTM) Recurrent Neural Network (RNN) are proposed and integrated in this work. The frequency domain Multi-Edge detection is introduced to improve the precision, recall and F-Score whereas, Multi-Edge Optimized LSTM is used to reduce the processing time of the summarization process. Discrete Wavelet Transformation based multi-edge detection algorithm is introduced and integrated with the optimized LSTM to achieve the betterment of the summarization process. The proposed method named as Multi-Edge optimized LSTM RNN for Video Summarization (MOLRVS) is indented to perform the video summarization process in real time video streaming environments to reduce a significant amount of manual interventions.																	1868-5137	1868-5145															10.1007/s12652-020-02025-8		MAY 2020											
J								Optimal placement of UAVs of an aerial mesh network in an emergency situation	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Metaheuristic; Multi-objective optimization; Multi-objective Particle Swarm Optimization; Non-dominated Sorting Genetic Algorithm II; Strength Pareto Evolutionary Algorithm 2; Pareto Envelope-based Selection Algorithm II; Optimal non-dominant solutions	VEHICLE BASE STATION; SENSOR NETWORKS; 3-D PLACEMENT; COVERAGE; TARGET	There has been a rapid increase in usage of unmanned aerial vehicles (UAVs) in different application areas that are unfriendly to humans. These UAVs have been used in Aerial Mesh Networks (AMNs) that act as backbone network to support communication in a post-disaster scenario. However, there may be limited available number of UAV nodes that need to be utilized efficiently to improve the performance of such networks. Here, we consider three important objectives of the network i.e. target coverage, Quality of Service and energy consumption by the network that need to be optimized efficiently to improve the performance. Yet, it is a grueling task to optimize all of these conflicting objectives at the same time, which is affected by the height of UAVs. To optimize more than one conflicting objectives, we used metaheuristic based multi-objective optimization algorithms i.e. Multi-objective Particle Swarm Optimization (MOPSO), Non-dominated Sorting Genetic Algorithm II (NSGA-II), Strength Pareto Evolutionary Algorithm 2 (SPEA2) and Pareto Envelope-based Selection Algorithm II (PESA-II), which suggest the optimal placement of UAVs. These algorithms are compared based on four performance metrics i.e. generational distance, diversification metric, spread of non-dominant solutions and percentage of domination in three different scenarios. The rigorous experiments are performed by each algorithm in small, medium and large-scale scenarios to compare their results. The ANOVA's validation test suggests that SPEA2 performs better than others in small-scale scenarios while NSGA-II performs better than others in medium and large-scale scenarios. However, MOPSO has lowest average execution time, after that NSGA-II, then PESA-II and then SPEA2.																	1868-5137	1868-5145															10.1007/s12652-020-01976-2		MAY 2020											
J								Effective epileptic seizure detection based on the event-driven processing and machine learning for mobile healthcare	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Electroencephalogram; Epileptic seizure diagnosis; Event-driven processing; Compression; AR burg features extraction; Machine learning; Biomedical implants; Mobile healthcare		Mobile healthcare is a promising approach. It is realized by using the biomedical implants that are connected to the cloud. A framework for the precise and effective diagnosis of epileptic seizures is designed in this context. To achieve real-time compression and effective signal processing and transmission, it uses an intelligent event-driven electroencephalogram (EEG) signal acquisition. Experimental results show that grace of the event-driven nature an overall 3.3 fold compression and transmission bandwidth usage reduction is achieved by the devised method compared to the conventional counterparts. It promises a notable decrease in the post analysis and classification processing activity. The system performance is studied by using a standard three class EEG epileptic seizure dataset. The highest classification accuracy of 97.5% is secured for a mono-class. The best average classification accuracy of 96.4% is attained for three-classes. Comparison of the system with classical equivalents is made. Results demonstrate more than threefold and sevenfold of outperformance respectively in terms of compression gain and processing efficiency while confirming a comparable classification precision.																	1868-5137	1868-5145															10.1007/s12652-020-02024-9		MAY 2020											
J								Soft computing approach based energy and correlation aware cooperative data collection for wireless sensor network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Soft computing approach; Data compression; Energy balancing; Energy conservation; Link aware forwarding; Passive clustering	EFFICIENT DATA-COLLECTION; LIFETIME MAXIMIZATION; ROUTING PROTOCOL; DATA-COMPRESSION; ALGORITHM; PREDICTION	Energy plays the predominant constraint in wireless sensor network (WSN) hence, an energy and correlation aware cooperative data collection (ECCDC) method of soft computing approach has been discussed in this work. This work is an attempt to maximize the life span of the network by identifying and rectifying the major energy exploiting activities of WSN and it also defines a hierarchical network structure with balanced clusters and energy harvesting sensor nodes. These sensor nodes select the Cluster Heads (CHs) among themselves using a low overhead passive clustering approach. The cluster balancing has been performed by acknowledging only the most deserved sensor nodes as their members. The members co-operatively collect the data from the region of interest. During every time slot, the cluster members are chosen for reporting, based on the mutual correlation, link quality of the member with the CH and the residual energy of the member nodes. The fraction of data collected at the CH is then sent through the optimized inter-cluster paths to the sink, wherein the received data are used for estimation, based on the node's position and correlation pattern. This proposed system of optimize method reduces the energy consumed, due to the transmission of redundant data and the data loss by preferring optimized data paths.																	1868-5137	1868-5145															10.1007/s12652-020-02008-9		MAY 2020											
J								On Maximizing Manipulability Index while Solving a Kinematics Task	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Manipulability index; Inverse kinematics; QP-based IK solver; Collision avoidance	ROBOT	In this paper, we investigate the problem of maximizing the manipulability index while solving a general Inverse Kinematics (IK) problem of a redundant industrial manipulator. Manipulability index has been extensively studied in the robotics literature and several formulae have been developed, nevertheless, they mainly only exploit the robot redundancy. The general IK is formulated as a Quadratic Programming (QP) that can seamlessly incorporate inequality constraints, such as collision avoidance, and we propose two new formulae to integrate the manipulability index maximization into the QP-based IK solver. We then thoroughly analyze the performance of the proposed formulae in simulation and validate them on a real Baxter research robot. The experimental results revealed the outperformance of the proposed formulae in comparison with the classical formula in the literature. Hence, providing a way to improve the manipulability index of a recorded trajectory, e.g. by learning by demonstration, or an offline generated one by a motion planning algorithm.																	0921-0296	1573-0409				OCT	2020	100	1					3	13		10.1007/s10846-020-01171-7		MAY 2020											
J								Forecasting significant stock price changes using neural networks	NEURAL COMPUTING & APPLICATIONS										Stock price forecasting; Neural networks; CNN; LSTM; RSI	INDEX; DEEP; PREDICTION; MARKETS; MODEL	Stock price prediction is a rich research topic that has attracted interest from various areas of science. The recent success of machine learning in speech and image recognition has prompted researchers to apply these methods to asset price prediction. The majority of literature has been devoted to predicting either the actual asset price or the direction of price movement. In this paper, we study a hitherto little explored question of predicting significant changes in stock price based on previous changes using machine learning algorithms. We are particularly interested in the performance of neural network classifiers in the given context. To this end, we construct and test three neural network models including multilayer perceptron, convolutional net, and long short-term memory net. As benchmark models, we use random forest and relative strength index methods. The models are tested using 10-year daily stock price data of four major US public companies. Test results show that predicting significant changes in stock price can be accomplished with a high degree of accuracy. In particular, we obtain substantially better results than similar studies that forecast the direction of price change.																	0941-0643	1433-3058															10.1007/s00521-020-04942-3		MAY 2020											
J								Neural network control system of cooperative robot based on genetic algorithms	NEURAL COMPUTING & APPLICATIONS										Genetic algorithm; Network control system; Cooperative robot; Neural network		Attitude detection of cooperative robots can help robots recognize environment, understand tasks, and improve the safety, accuracy and efficiency of robot work. For robots with unknown configuration parameters, their configuration can be estimated; it is convenient for further kinematics and dynamics analysis. For an economical cooperative robot, its kinematic parameters can be corrected by attitude detection. An independent robot attitude detection system can be used as a secondary auxiliary system, and the fault diagnosis system of the robot is composed of the high-precision encoder system of the robot body. This paper studies the neural network control system of cooperative robot based on genetic algorithm. In this paper, the robot is taken as the research object. Aiming at its strong conjunction, non-linearity and multivariable characteristics, the problem of robot motion control based on neural network is mainly discussed. On this basis, the basic genetic algorithm and an improved genetic algorithm called messy are used to optimize the structure of the neural network, so as to better realize the motion control problem of the robot. The results of this study show that: After optimization with messy genetic algorithm, not only the tracking effect is better, but also the number of hidden layer nodes of the network is reduced from 12 to 7. This greatly simplifies the structure of the network and makes the design and training of the network relatively simple.																	0941-0643	1433-3058															10.1007/s00521-020-04952-1		MAY 2020											
J								LSTM-based indoor air temperature prediction framework for HVAC systems in smart buildings	NEURAL COMPUTING & APPLICATIONS										HVAC; LSTM; Sequence-to-sequence; Multi-step ahead predictions; VAV; CAV	THERMAL COMFORT; NEURAL-NETWORK; ENERGY; OPTIMIZATION; MODEL; PERFORMANCE; CONSUMPTION	Accurate indoor air temperature (IAT) predictions for heating, ventilation, and air conditioning (HVAC) systems are challenging, especially for multi-zone building and for different HVAC system types. Moreover, the nonlinearity of the buildings thermal dynamics makes the IAT prediction more difficult since it is affected by complex factors such as controlled and uncontrolled points, outside weather conditions and occupancy schedule. This paper presents a long short-term memory (LSTM) model to predict IAT for multi-zone building based on direct multi-step prediction with sequence-to-sequence approach. Two strategies, LSTM-MISO and LSTM-MIMO, are built for multi-input single-output and multi-input multi-output, respectively. The performance of these two strategies has been evaluated based on two case studies on real smart buildings using variable air volume (VAV) and constant air volume (CAV) systems. For both buildings, experimental results showed that the LSTM models outperform multilayer perceptron models by reducing the prediction error by 50%.																	0941-0643	1433-3058															10.1007/s00521-020-04926-3		MAY 2020											
J								Care2Vec: a hybrid autoencoder-based approach for the classification of self-care problems in physically disabled children	NEURAL COMPUTING & APPLICATIONS										Deep learning; Disability; ICF-CY; Self-care; Medical informatics; SCADI	NEURAL-NETWORKS; ICF-CY; SYSTEMS; DIAGNOSIS	Accurate classification of self-care problems in children who suffer from physical and motor affliction is an important problem in the healthcare industry. This is a difficult and a time-consuming process, and it needs the expertise of occupational therapists. In recent years, healthcare professionals have opened up to the idea of using expert systems and artificial intelligence in the diagnosis and classification of self-care problems. In this study, we propose a new deep learning-based approach named Care2Vec for solving the self-care classification problem. We use a real-world self-care activities dataset that is based on a conceptual framework designed by the World Health Organization. The conceptual framework is known as the International Classification of Functioning, Disability and Health for Children and Youth (ICF-CY), which is a widely used standard framework for analyzing self-care activity records. Deep learning is a form of representation learning, and in recent years, it has been very successful in various fields such as computer vision, speech processing and more. We propose a hybrid autoencoder-based method Care2Vec, where we use autoencoders and deep neural networks as a two-step modeling process. We also propose a variation of the Care2Vec method. We then compare Care2Vec with traditional methods reported in the literature for solving the self-care classification problem (such as deep neural networks and decision trees) in both multi-class classification and binary classification settings. We use k-fold cross-validation while applying the different methodologies. The evaluation metrics used were the mean accuracy and the mean area under the curve (AUC). We found that the Care2Vec method has a better prediction accuracy than the prevalent methods and so a recommended approach for the self-care classification problem. The adoption of Care2Vec can help expert therapists in making better diagnostic decisions and will thus lead to better treatment.																	0941-0643	1433-3058															10.1007/s00521-020-04943-2		MAY 2020											
J								SANgo: a storage infrastructure simulator with reinforcement learning support	PEERJ COMPUTER SCIENCE										Storage system simulation; Optimal control; Reinforcement learning; Storage array; Discrete event simulation	FAILURE; ELECTROMIGRATION	We introduce SANgo (Storage Area Network in the Go language)-a Go-based package for simulating the behavior of modern storage infrastructure. The software is based on the discrete-event modeling paradigm and captures the structure and dynamics of high-level storage system building blocks. The flexible structure of the package allows us to create a model of a real storage system with a configurable number of components. The granularity of the simulated system can be defined depending on the replicated patterns of actual system behavior. Accurate replication enables us to reach the primary goal of our simulator-to explore the stability boundaries of real storage systems. To meet this goal, SANgo offers a variety of interfaces for easy monitoring and tuning of the simulated model. These interfaces allow us to track the number of metrics of such components as storage controllers, network connections, and harddrives. Other interfaces allow altering the parameter values of the simulated system effectively in real-time, thus providing the possibility for training a realistic digital twin using, for example, the reinforcement learning (RL) approach. One can train an RL model to reduce discrepancies between simulated and real SAN data. The external control algorithm can adjust the simulator parameters to make the difference as small as possible. SANgo supports the standard OpenAI gym interface; thus, the software can serve as a benchmark for comparison of different learning algorithms.																	2376-5992					MAY 4	2020									e271	10.7717/peerj-cs.271													
J								Exact acceleration of complex real-time model checking based on overlapping cycle	PEERJ COMPUTER SCIENCE										Real-time model checking; Exact acceleration; Complex real-time system; Timed automata; Overlapping cycle	VERIFICATION	When real-time systems are modeled as timed automata, different time scales may lead to substantial fragmentation of the symbolic state space. Exact acceleration solves the fragmentation problem without changing system reachability. The relatively mature technology of exact acceleration has been used with an appended cycle or a parking cycle, which can be applied to the calculation of a single acceleratable cycle model. Using these two technologies to develop a complex real-time model requires additional states and consumes a large amount of time cost, thereby influencing acceleration efficiency. In this paper, a complex real-time exact acceleration method based on an overlapping cycle is proposed, which is an application scenario extension of the parking-cycle technique. By comprehensively analyzing the accelerating impacts of multiple acceleratable cycles, it is only necessary to add a single overlapping period with a fixed length without relying on the windows of acceleratable cycles. Experimental results show that the proposed timed automaton model is simple and effectively decreases the time costs of exact acceleration. For the complex real-time system model, the method based on an overlapping cycle can accelerate the large scale and concurrent states which cannot be solved by the original exact acceleration theory.																	2376-5992					MAY 4	2020									e272	10.7717/peerj-cs.272													
J								Contracting in Brazilian public administration: A machine learning approach	EXPERT SYSTEMS										credit risk; emerging economies; machine learning; public contracts; public sector; reputation systems	SUPPORT VECTOR MACHINES; DEVELOPING-COUNTRIES; CREDIT; PROCUREMENT; RISK; CLASSIFIERS; CORRUPTION; GOVERNANCE; REPUTATION; DESIGN	The risk of non-fulfilment of a contract can harm public administration or even interrupt public services. Therefore, models that assist manager decision making in the audit and control of contracts with a higher disqualification risk may be important tools, with economic and even social repercussions. In this article, public contracts are classified with respect to the risk of non-compliance with their terms of delivery. The quantitative tools used are statistical and machine learning models, similar to credit risk rating of loans. As dependent variables, the models use data found in electronic databases present in e-government implementations. A previously classified listing of suspended companies is used as a proxy for risky contracts, as it contains private companies which failed with their contractual obligations. The classification techniques utilized are logistic regression, k-nearest neighbours, discriminant analysis, support vector machine and random forests. Although the methods can be applied to any government with electronic procurement and contracts systems, Brazilian data is used to illustrate the benefits of contract governance for emerging economies. It is concluded that the credit rating techniques used directly apply to contractual risk in public administration. Considering real public administration contract data, the classification algorithm that generates the best performance is k-nearest neighbours.																	0266-4720	1468-0394				OCT	2020	37	5			SI				e12550	10.1111/exsy.12550		MAY 2020											
J								Attentive multi-view reinforcement learning	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Deep reinforcement learning; Function approximation; Multi-view learning; Representation learning	LEVEL; GAME; GO	The reinforcement learning process usually takes millions of steps from scratch, due to the limited observation experience. More precisely, the representation approximated by a single deep network is usually limited for reinforcement learning agents. In this paper, we propose a novel multi-view deep attention network (MvDAN), which introduces multi-view representation learning into the reinforcement learning framework for the first time. Based on the multi-view scheme of function approximation, the proposed model approximates multiple view-specific policy or value functions in parallel by estimating the middle-level representation and integrates these functions based on attention mechanisms to generate a comprehensive strategy. Furthermore, we develop the multi-view generalized policy improvement to jointly optimize all policies instead of a single one. Compared with the single-view function approximation scheme in reinforcement learning methods, experimental results on eight Atari benchmarks show that MvDAN outperforms the state-of-the-art methods and has faster convergence and training stability.																	1868-8071	1868-808X				NOV	2020	11	11					2461	2474		10.1007/s13042-020-01130-6		MAY 2020											
J								Sequential and orthogonalized PLS (SO-PLS) regression for path analysis: Order of blocks and relations between effects	JOURNAL OF CHEMOMETRICS										common components; graphical modelling; path analysis; SEM; SO-PLS	TOOL	This paper is about the use of the multiblock regression method sequential and orthogonalized partial least squares (SO-PLS) for path modeling. The paper is a follow up of previously published papers on the same topic and presents a number of new results for the method. First of all, the paper discusses more thoroughly the aspect of how to incorporate blocks in the models and relates this to standard concepts in the area of graphical modeling. Second, the paper defines the concept of direct and indirect effects more precisely in terms of population parameters and shows how they are related to the additional effect in SO-PLS modeling. The paper illustrates the theory by simple graphs, simulations, and a real example from process monitoring.																	0886-9383	1099-128X														e3243	10.1002/cem.3243		MAY 2020											
J								Clustering-Aided Multi-View Classification: A Case Study on Android Malware Detection	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Multi-view Learning; Classification; Clustering; Android Malware Detection; Android Application Static Analysis	SECURITY; ENSEMBLE; ALGORITHM	Recognizing malware before its installation plays a crucial role in keeping an android device safe. In this paper we describe a supervised method that is able to analyse multiple information (e.g. permissions, api calls and network addresses) that can be retrieved through a broad static analysis of android applications. In particular, we propose a novel multi-view machine learning approach to malware detection, which couples knowledge extracted via both clustering and classification. In an assessment, we evaluate the effectiveness of the proposed method using benchmark Android applications and established machine learning metrics.																	0925-9902	1573-7675				AUG	2020	55	1					1	26		10.1007/s10844-020-00598-6		MAY 2020											
J								Multimodal depression detection on instagram considering time interval of posts	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Depression detection; Deep learning; Social media		Depression is a common and serious mental disorder that causes a person to have sad or hopeless feelings in his/her daily life. With the rapid development of social media, people tend to express their thoughts or emotions on the social platform. Different social platforms have various formats of data presentation, which makes huge and diverse data available for analysis by researchers. In our study, we aim to detect users with depressive tendency on Instagram. We create a depression dictionary for automatically collecting data of depressive and non-depressive users. In terms of the prediction model, we construct a multimodal system, which utilizes image, text and behavior features to predict the aggregated depression score of each post on Instagram. Considering the time interval between posts, we propose a two-stage detection mechanism for detecting depressive users. Experimental results demonstrate that our proposed methods can achieve up to 0.835 F1-score for detecting depressive users. It can therefore serve as an early depression detector for a timely treatment before it becomes severe.																	0925-9902	1573-7675															10.1007/s10844-020-00599-5		MAY 2020											
J								Remerging Feigenbaum Trees, Coexisting Behaviors and Bursting Oscillations in a Novel 3D Generalized Hopfield Neural Network	NEURAL PROCESSING LETTERS										Generalized Hopfield neural network (GHNN); AC equilibrium points; Remerging Feigenbaum trees; Coexistence of multiple stable states; Bursting oscillations; Pspice simulations	MULTIPLE ATTRACTORS; ANTIMONOTONICITY; DYNAMICS; SPIKING; NEURONS	This paper, a novel 3D generalized Hopfield neural network is proposed and investigated in order to generate and highlight some unknown behaviors related to such type of neural network. This generalized model is constructed by exploiting the effect of an external stimulus on the dynamics of a simplest 3D autonomous Hopfield neural network reported to date. The stability of the model around its ac-equilibrium point is studied. We note that the model has three types of stability depending on the value of the external stimulus. The stable node-focus, the unstable saddle focus, and the stable node characterize the stability of the equilibrium point of the model. Traditional nonlinear analyses tools are used to highlight and support several complex phenomena such as remerging Feigenbaum trees, the coexistence of up to two, four and six disconnected stable states when the amplitude of the external stimulus is set to zero. Furthermore, for some values of the frequency and amplitudes of the external stimulus, bursting oscillations occur. This latter behavior is characterized by the fact that oscillations switch between quiescent states and spiking states, repetitively. The transformed phase portraits are used to support the bursting mode of oscillations found. Finally, PSpice simulations enable to support the results of the theoretical studies.																	1370-4621	1573-773X				AUG	2020	52	1			SI		267	289		10.1007/s11063-020-10264-1		MAY 2020											
J								Study on sampling-based discrete noniterative algorithms for centroid type-reduction of interval type-2 fuzzy logic systems	SOFT COMPUTING										Interval type-2 fuzzy logic systems; Type-reduction; Nagar-Bardini algorithms; Nie-Tan algorithms; Begian-Melek-Mendel algorithms; Sampling points	KARNIK-MENDEL ALGORITHMS; SETS; DEFUZZIFICATION; OPTIMIZATION; CONTROLLER; SPEED	As an emerging technology, interval type-2 fuzzy logic systems (IT2 FLSs) have drawn great attentions in the past decade years. However, the computational intensive and time consuming type-reduction (TR) block may hinder the real applications IT2 FLSs. Unlike the most popular Karnik-Mendel (KM) iterative algorithms, the noniterative algorithms decrease the computational cost greatly. The comparison between the discrete and continuous algorithms is still an open problem. This paper compares the sum operations in discrete noniterative algorithms and the integral operations in continuous noniterative algorithms, and discovers the inner relations between the discrete and continuous noniterative algorithms. Then, three types of noniterative algorithms are adopted for performing the centroid TR of IT2 FLSs. Three computer simulations show the calculation results of sampling based discrete noniterative algorithms can accurately approximate the corresponding continuous noniterative algorithms as varying the number of sampling of primary variable appropriately, in addition, the computational efficiencies of former are much higher than the latter, which provide the potential value for designing IT2 FLSs.																	1432-7643	1433-7479				AUG	2020	24	15			SI		11819	11828		10.1007/s00500-020-04998-2		MAY 2020											
J								HFACS-based FAHP implementation to identify critical factors influencing human error occurrence in nuclear plant control room	SOFT COMPUTING										Human factor analysis and classification system; Cognitive and organizational factor; Nuclear industry; Multicriteria decision making; Fuzzy analytical hierarchy process	HUMAN RELIABILITY-ANALYSIS; FUZZY AHP; METHODOLOGY; ACCIDENTS; FRAMEWORK; DESIGN; SYSTEM	Human factor is an inevitable element of safety-critical control room operations in nuclear power plants. Identifying human factors influencing operator error occurrence in such critical application is important to mitigate human errors. Conventional methods employed for human factor identification are often static, unable to deal with data and model uncertainty, and to consider independencies among failure modes. This study employs the integration of the soft computing technique, viz. fuzzy analytic hierarchy process (FAHP), into the human factors analysis and classifying system (HFACS) framework to identify the critical human factors that contribute to human errors in the nuclear control room application. Integration of FAHP improves the HFACS framework by providing an analytical foundation and group decision-making ability in order to ensure quantitative assessment of nuclear accidents. The proposed model has two phases. The first phase is study of 18 human performance-related events that occurred in Indian nuclear power plants, utilizing HFACS to analyze and determine the human and organizational factors (HOFs) responsible for such events. Existing HFACS can only be used to detect a wide range of human factors. The proposed model further explores the underlying causes of such a wide range of factors. The hierarchy of HOFs identified as adverse mental states and organizational process factors that contributed most failures causing accident in the first phase provides inputs for the critical human factor identification to the second phase, which is a quantitative analysis using FAHP. In the second phase, more than 40 adverse mental state and organizational process factors are identified from literature survey and ten subfactors are screened and selected by human reliability assessment and control room operation experts. The study was conducted by administering a questionnaire that is comprised of the screened factors. Data have been collected from 88 senior reactor operators in an operating nuclear power plant. The critical factors contributing to human errors identified from FAHP are attention, perception and memory under cognitive factor and decision making, training and communications under organizational factor. The study results reveal that the implementation of HFACS-based FAHP methodology enabled in determining intrinsic human factors that contribute to nuclear power plant control room operator performance.																	1432-7643	1433-7479				NOV	2020	24	21					16577	16591		10.1007/s00500-020-04961-1		MAY 2020											
J								Deep learning for estimating the channel in orthogonal frequency division multiplexing systems	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										OFDM; Channel estimation; Deep learning; 1D CNN; ANN; LS; MMSE	OFDM SYSTEMS; CLASSIFICATION; NETWORKS	The essential criteria of wireless communication system are accurate signal identification. The channel assessment and adjustment are the two most critical mechanisms for signal identification. Since orthogonal frequency division multiplexing (OFDM) used to decrease bit error rates in current wireless communication systems and to optimize spectral efficiency and compared to a single carrier network, secure channel compensation method. Nevertheless, the system's quality output is degraded depending on the channel state due to the incorrect channel approximation and noise amplification through the channel compensation procedure. In this paper, a 1D Convolutional Neural Network (CNN) deep learning model to estimate the channel provided and the equalized data also recovered. The Deep Neural Network (DNN) can not only use the channel variation features from previous estimates with correctly chosen outputs, but it can also derive additional features from the pilots and receiving signals. To prove the ability of this model the Bit Error Rate (BER) for the recovered data is compared with the conventional models like Minimum Mean Square Error (MMSE) and Least Square (LS) and compared with the Feed Forward Neural Network (FFNN) model in different digital modulation techniques.																	1868-5137	1868-5145															10.1007/s12652-020-02010-1		MAY 2020											
J								Implementation of thermal aware wireless sensor network clustering algorithm based on fuzzy and spider optimized cluster head selection	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING											ENERGY-EFFICIENT	In a modern approach, a smart grid requires an innovation on various fronts such as a wireless sensor network which is an important component of smart grid applicability. To fulfill the Quality of Service (QoS) needs in smart grid network, a structure of network and topology should be optimized especially in the urban areas. The clustering methodologies are useful technique for optimizing network topologies. In clustering, the clustering process consists of cluster head node selection and rotation which is based on Residual Energy, Distance of node from base station etc. However, the impact of temperature rise has not been considered so far. Based on the previous related works, this paper proposes a Thermal Aware solution based on combining Eigen Centrality Fuzzy Cluster size Control and Spider Optimization Algorithm. Furthermore, an influence of temperature can be realized through Received Signal Strength (RSS) and the number of packets received. The proposed algorithm is simulated in MATLAB and implemented in hardware test bed using Zigbee and PIC microcontroller. Consequently, the result confirms the impact of thermal heat on Cluster Head selection control and also the prediction of number of rounds.																	1868-5137	1868-5145															10.1007/s12652-020-02002-1		MAY 2020											
J								Aspect category detection using statistical and semantic association	COMPUTATIONAL INTELLIGENCE										aspect category detection; association rule; review analysis; semantic association; word-embeddings		Aspect category detection (ACD) is an important subtask of aspect-based sentiment analysis (ABSA). It is a challenging problem due to subjectivity involved in categorization, as well as the existence of overlapping classes. Among various approaches that have been applied to ACD include rule-based approaches along with other machine learning approaches, and most of them are statistical in nature. In this article, we have used an association rule-based approach. To deal with the statistical limitation of association rules, we proposed a hybridized rule-based approach that combines association rules with the semantic association. For semantic associations, we have used the notion of word-embeddings. Experiments were performed on SemEval dataset, a standard benchmark dataset for aspect categorization in the restaurant domain. We observed that semantic associations can complement statistical association and improve the accuracy of classification. The proposed method performs better than several state-of-the-art methods.																	0824-7935	1467-8640				AUG	2020	36	3					1161	1182		10.1111/coin.12327		MAY 2020											
J								Automatic traditional Chinese painting classification: A benchmarking analysis	COMPUTATIONAL INTELLIGENCE										classification; deep learning; instance segmentation; traditional Chinese painting		In the recent years, there is a growing trend toward digitization of cultural heritage for better accessibility and preservation. For instance, the development of image processing techniques in traditional Chinese painting (TCP) has begun to attract researchers' attention in the computer vision field. TCP is one of the representative of Chinese traditional arts. Evidenced by the successes of development in image processing techniques in various applications, this article aim to apply the deep learning approach on TCP for several purposes, which include automatic establishment of unified image library, facilitating update-to-date data in the database, reduction of cost required for image classification and retrieval. First, a unified database is established, that consists of more than a thousand of images from six major TCP themes. Then, several deep learning algorithms that are based on mathematical models are applied to examine the classification performance. In addition, the salient regions that denote significant features are identified, by adopting the instance segmentation technique. As a result, the modified pretrained neural network is capable to achieve 99.66% recognition accuracy. Qualitative results are also presented to demonstrate the effectiveness of the proposed method. We also note that this is the first work that performs multiclass classification on six categories in this domain. Furthermore, a 10-class classification result of 96% is obtained when performing on one of the painting types, namely, ghost-and-god.																	0824-7935	1467-8640				AUG	2020	36	3					1183	1199		10.1111/coin.12328		MAY 2020											
J								ELM-MC: multi-label classification framework based on extreme learning machine	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Multi-label classification; Extreme learning machine; Principle component analysis; Linear discriminant analysis	ALGORITHM	Multi-label classification methods aim to a class of application problems where each individual contains a single instance while associates with a set of labels simultaneously. In this paper, we formulate a novel multi-label classification method based on extreme learning machine framework, named ELM-MC algorithm. The essence of ELM-MC algorithm is to convert the multi-label classification problem into some single-label classifications, and fully considers the relationship among different labels. After the classification of one label, the associations with next label are applied to update the learning parameters in ELM-MC algorithm. In addition, we design a backup pool for the hidden nodes. It can help to select relatively suitable hidden nodes to the corresponding label classification case. In the simulation part, six famous databases are applied to demonstrate the satisfied classification accuracy of the proposed method.																	1868-8071	1868-808X				OCT	2020	11	10					2261	2274		10.1007/s13042-020-01114-6		MAY 2020											
J								Multiplication fusion of sparse and collaborative-competitive representation for image classification	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Representation based classification methods; Sparse representation; Collaborative representation; Collaborative-competitive representation based classification	LOW-RANK REPRESENTATION; CONSISTENT K-SVD; FACE-RECOGNITION; ROBUST	Representation based classification methods have become a hot research topic during the past few years, and the two most prominent approaches are sparse representation based classification (SRC) and collaborative representation based classification (CRC). CRC reveals that it is the collaborative representation rather than the sparsity that makes SRC successful. Nevertheless, the dense representation of CRC may not be discriminative which will degrade its performance for classification tasks. To alleviate this problem to some extent, we propose a new method called sparse and collaborative-competitive representation based classification (SCCRC) for image classification. Firstly, the coefficients of the test sample are obtained by SRC and CCRC, respectively. Then the fused coefficient is derived by multiplying the coefficients of SRC and CCRC. Finally, the test sample is designated to the class that has the minimum residual. Experimental results on several benchmark databases demonstrate the efficacy of our proposed SCCRC. The source code of SCCRC is accessible at .																	1868-8071	1868-808X				OCT	2020	11	10					2357	2369		10.1007/s13042-020-01123-5		MAY 2020											
J								Recognition of positive and negative valence states in children with autism spectrum disorder (ASD) using discrete wavelet transform (DWT) analysis of electrocardiogram signals (ECG)	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Autism spectrum disorder (ASD); Heart rate variability (HRV); Pan-Tompkins algorithm; K nearest neighbor (KNN)	EMOTION RECOGNITION; PARKINSONS-DISEASE; FACIAL EXPRESSION; YOUNG-CHILDREN; RESPONSES; AROUSAL; EEG; FEATURES; SYSTEM	Children with autism spectrum disorder (ASD) are deficit in communication, social skills, empathy, emotional responsiveness and have significant behavioral pattern. They have difficulty in understanding other feelings and their own emotions. This leads to the sudden emotional outburst and aggressive behavior in these children. Parents, caretakers and doctors find it very difficult to prevent such extreme behaviors. Learning the positive and negative valence leads in determining the early indications before the onset of emotional outbursts in children with ASD. The present study measures the psycho physiological electrocardiogram (ECG) signal from the typically developed (TD) children and children with ASD in the age group of 5-11 years. Personalized protocol was developed for every child with ASD to induce positive and negative valence and ECG data was collected using wearable Shimmer ECG device. The heart rate variability (HRV) and the QRS amplitude were derived from ECG signal using Pan-Tompkins algorithm and eleven features were extracted using DWT (db2, db4 and db8) mother wavelet. The significant features of ECG, HRV and QRS amplitude were classified using the K nearest neighbor (KNN), support vector machine (SVM) and ensemble classifier. Ensemble and KNN classifier achieved maximum accuracy of 81% and 76.2% for children with ASD and Ensemble and SVM classifiers obtained maximum accuracy of 87.4% and 83.8% for TD children using HRV data.																	1868-5137	1868-5145															10.1007/s12652-020-01985-1		MAY 2020											
J								Task scheduling based on swarm intelligence algorithms in high performance computing environment	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Task scheduling; Swarm intelligence; High performance computing; Average scheduling time; Resource utilization		The high-performance computing environment is a computing platform, which aggregates multiple distributed high-performance computers from indifferent organizations, providing users with unified access and usage patterns. Since the task scheduling strategy is lack of flexibility, an optimized task scheduling model in the high-performance computing environment is proposed in this paper, which introduces an improved swarm intelligence algorithm in task queues, refines the Core Scheduler for each task, and increases the configuration of task scheduling strategy. In core task scheduling, swarm intelligence algorithm is adopted to minimize the average scheduling time for completion tasks through optimal task allocation on each node. Simulation results show that the proposed scheduling algorithm is better than the traditional task scheduling algorithm. Therefore, according to the task scheduling strategy based on swarm intelligence algorithm, it can effectively reduce the task waiting, improve the system's throughput, the task response and system resource utilization has a better effect.																	1868-5137	1868-5145															10.1007/s12652-020-01994-0		MAY 2020											
J								A distributed fuzzy multicast routing protocol (DFMCRP) for maximizing the network lifetime in mobile ad-hoc networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Manet; Multicast; Fuzzy logic system; Stability; Delay; Energy; Bandwidth; Optimized local route repair	PREDICTION; WIRELESS	An ad hoc wireless network is formed whenever multiple devices or nodes that wish to make communication, connect among themselves wirelessly based on their radio range and distribute network information with each other without any central administrator. In a MANET, the devices are mobile in nature and multi-hop communication is carried out between the mobile devices through some intermediate mobile devices. Due to the mobile nature of nodes, unsteady links, and restricted resources, routing is a difficult issue in MANET. Several routing techniques are already in use and multicast routing is used to conserve the network resources. In this paper, the optimal path is selected for data transmission using fuzzy logic. Distributed fuzzy decision making is obtained by running fuzzy inference engine in every node to handle the uncertainties in the MANET environment. The proposed work also suggests an optimized local route repair to be used in case of route failure. The ultimate objectives of both fuzzy based optimal route selection and route repair result in increased network lifetime. The fuzzy rules are developed using MATLAB and they are used in the NS2 network simulation for route generation and performance analysis. The proposed protocol performs better and also gives good network lifetime compared with the existing protocol MAODV.																	1868-5137	1868-5145															10.1007/s12652-020-01936-w		MAY 2020											
J								GM-CPSO: A New Viewpoint to Chaotic Particle Swarm Optimization via Gauss Map	NEURAL PROCESSING LETTERS										Chaotic; Epileptic seizure; Gauss map; Hybrid classifier; Optimization; Pattern recognition		Chaos concept has been appealed in the recent optimization methods to achieve a convenient tradeoff between exploration and exploitation. Different chaotic maps have been considered to find out the appropriate one for the system dynamics. However, on particle swarm optimization (PSO), the usage of these maps has not been handled in an extensive manner, and the best fit one has not known yet. In this paper, ten chaotic maps are handled to reveal the best fit one for PSO, and to explore whether chaotic maps are necessary for PSO or not. Thirteen benchmark functions are used to perform a detailed evaluation at the first experiment. Chaotic PSO (CPSO) methods including different maps are tested on global function optimization. Concerning this, Gauss map based CPSO (GM-CPSO) has come to the forefront by achieving promising fitness values in all function evaluations and in comparison with the state-of-the-art methods. To test the efficiency of GM-CPSO on a different task, GM-CPSO is hybridized with neural network (NN) at the second experiment, and the epileptic seizure recognition is handled. Discrete wavelet transform (DWT) based features, GM-CPSO and NN are considered to design an efficient framework and to specify the type of electroencephalography signals. GM-CPSO-NN is compared with hybrid NNs including two state-of-the-art optimization methods so as to examine the efficiency of GM-CPSO. To accurately test the performances, twofold cross validation is realized on 11,500 instances, and four metrics [accuracy, area under ROC curve (AUC), sensitivity, specificity] are consulted for a detailed assessment beside of computational complexity analysis. In experiments, GM-CPSO including the necessary map, has provided remarkable fitness scores over the state-of-the-art optimization methods on optimization of various functions defined in different dimensions. Besides, the proposed framework including GM-CPSO-NN, has achieved remarkable performance by obtaining reliable accuracy (97.24%), AUC (95.67%), sensitivity (93.04%) and specificity (98.29%) scores, and by including less computational complexity than other algorithms. According to the results, GM-CPSO has arisen as the most convenient optimization method to be preferred in the formation of hybrid NNs. In addition to optimization and classification results, it's seen that the detail sub-bands of DWT comprise necessary information for seizure recognition. Consequently, it's revealed that GM-CPSO can be preferred on global function optimization for reliable convergence, and its usage can be extended to different disciplines like signal classification, pattern recognition or hybrid system design.																	1370-4621	1573-773X				AUG	2020	52	1			SI		241	266		10.1007/s11063-020-10247-2		MAY 2020											
J								Model for estimating power dissipation along the interconnect length in single on-chip topology	EVOLUTIONARY INTELLIGENCE										Thermal conductivity modeling; Laplace's equation; Finite-difference analogs; Microelectronics	HEAT SINK	Designing a typical microcircuit for multi-core chips includes the development of the overall system architecture as well as the creation of single-chip multiprocessor systems, which house a set of interrelated nodes. To evaluate the complexity and power consumption of these multiprocessor systems, adequate models and methods are required. A simplified model for calculating heat dissipation in conductors suits this purpose completely. The preliminary settlement of the heat dissipation parameter will allow engineers to evaluate the on-chip interconnect topology at the early stages of design so that these interconnects comply with the power dissipation and the power consumption variables. The novelty of this approach lies in the fact that it allows modeling thermal stresses that take place inside the chip. A solver applied here enables numerical solution to differential equations that is theoretically best, i.e., a direct numerical solution. In addition, it overcame the Gibbs phenomenon by incorporating more natural, not sharp boundary conditions into the model. Here, this approach was applied to the multiprocessor system design that implies the integration of processor cores in a single chip package.																	1864-5909	1864-5917															10.1007/s12065-020-00407-7		MAY 2020											
J								High precision and fast disparity estimation via parallel phase correlation hierarchical framework	JOURNAL OF REAL-TIME IMAGE PROCESSING										Hierarchical structure; Parallel phase correlation; Disparity estimation	STEREO MATCHING ALGORITHM; ACCURACY; REGISTRATION; VISION; WINDOW; 1D	When estimating the disparity of remote sensing images, known phase correlation (PC)-based disparity estimation methods are not fast and robust, such as hierarchical structure PC method and fixed window PC method. To tackle this problem, a parallel PC-based hierarchical framework is proposed, which includes two ideas: first, a weighted PC peak fitting algorithm is introduced for estimating the high precise disparity matrix efficiently and stably; second, a graphics processing unit-based parallel PC algorithm is integrated into the hierarchical framework for fast and robustly estimating high precise disparity map. Additionally, many stages of hierarchical framework, such as padding and reliable evaluation stages, are improved for improving the computational efficiency of disparity estimation system. In a large number of experiments, the results have shown that the efficiency of the proposed algorithm is on average 24 times faster than the compared state-of-the-art methods. Meanwhile, the precision of the proposed algorithm is also superior to or very close to the compared algorithms. The proposed algorithm has been successfully used in a unmanned aerial vehicle three-dimensional retrieval system, and the practice effect has also been verified.																	1861-8200	1861-8219															10.1007/s11554-020-00972-1		MAY 2020											
J								Analysis of zig-zag scan based modified feedback convolution algorithm against differential attacks and its application to image encryption	APPLIED INTELLIGENCE										Convolution; Differential cryptanalysis; Image encryption; Security	CHAOTIC SYSTEM; PERMUTATION; SCHEME; KEYS; MAP	In this paper, a novel zig-zag scan-based feedback convolution algorithm for image encryption against differential attacks is proposed. The two measures Number of Pixel Change Rate (NPCR) and Unified Average Changed Intensity (UACI) are commonly utilized for analyzing the differential attacks. From the study of the existing papers, even though high Number of Pixel Change Rate and Unified Average Changed Intensity values are obtained, a few values lie in the critical range of alpha-level significance which in turn increase the possibility of differential attacks. To overcome differential attacks, two aspects of scanning with different test cases are analyzed and from these analyses, it is concluded that zig-zag scan based feedback convolution in forward and reverse direction achieves good Number of Pixel Change Rate and Unified Average Changed Intensity without critical values. Zig-zag scan based feedback convolution in forward and reverse direction is enforced for key sequence generation and applied in diffusion process to achieve high level of security. Moreover, plain image related initial seed is also generated to overcome the chosen/known plain text attacks. Both numerical and theoretical analyses are performed to prove that the proposed encryption method is resistant to differential attacks. General security measures are carried out for the proposed method to validate its security level. From the simulations, it is shown that the proposed methodology has good keyspace, high key sensitivity, good randomness, and uniform distribution of cipher image pixels.																	0924-669X	1573-7497				OCT	2020	50	10					3101	3124		10.1007/s10489-020-01697-1		MAY 2020											
J								Evaluation of an interactive educational system in urban knowledge acquisition and representation based on students' profiles	EXPERT SYSTEMS										3D interactive learning environments; architecture education; urban design; virtual reality; visualization	VIRTUAL-REALITY; AUGMENTED REALITY; ACADEMIC-PERFORMANCE; VISUALIZATION; MOTIVATION; PARTICIPATION; METHODOLOGIES; PERCEPTION; IMPROVE; MODELS	The aim of this article is to study how students currently understand the conception of space through different media and how that understanding helps them to intervene in the space. Firstly, the process of teaching and learning as well as innovative supporting technologies are analysed, revealing the characteristics of the contemporary student profile and better ways of teaching according to it. Secondly, we describe the evaluation of an experiment with a virtual reality (VR) system for urban project design with students of architecture from two universities. The premise is that the technology used in VR is familiar to the current profile of students. This paper aims to study the advantages and disadvantages of this trend and to find a balance. To do so, we use a quantitative method to evaluate students' profiles and their level of satisfaction with the system. The results were obtained by a questionnaire and a survey, which show the role and use of technologies in the students' environment and the degree of satisfaction when using it in the educational processes. In line with our assumptions, the value of satisfaction in the use of an advanced visualization technology in the classroom reveals a high level of motivation, in general, with differentiation between students in their first and last phases of studies.																	0266-4720	1468-0394				OCT	2020	37	5			SI				e12570	10.1111/exsy.12570		MAY 2020											
J								A Comparative Analysis of Incremental and Disruptive Innovation Policies in the European Banking Sector with Hybrid Interval Type-2 Fuzzy Decision-Making Models	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Innovation; Europe; Banking sector; Interval type-2 fuzzy DEMATEL; Interval type-2 fuzzy TOPSIS; Interval type-2 fuzzy VIKOR	EXTENDED VIKOR METHOD; ECONOMIC-GROWTH; RISK-MANAGEMENT; CUSTOMER SATISFACTION; FINANCIAL PERFORMANCE; SERVICE INNOVATION; TECHNOLOGY INNOVATION; COMPETITIVE ADVANTAGE; ALLIANCE PORTFOLIOS; EMPIRICAL-ANALYSIS	Nowadays innovation is a key issue in all business sectors, maintaining a positive correlation with the countries' economic development. The banking sector stands out among the different economic sectors, as globalization has pushed banks into tough competition. Hence, banks within this competitive framework must innovate by developing new products to be competitive and survive. The innovation in banking products is usually sorted as incremental or disruptive. Therefore, this paper aims to evaluate the innovation policies for the European Banking Sector by analyzing incremental and disruptive innovation policies. The novelty of the paper is to propose a set of dimensions and criteria for the innovation policies of European banking industry and to construct a hybrid decision-making model based on interval type-2 fuzzy sets. Accordingly, a comparative analysis of the top five GDP European countries has been carried out using a multiple criteria decision model (MCDM). The MCDM defines different criteria for incremental and disruptive innovations according to the specialized literature. Interval type-2 fuzzy DEMATEL (IT2 FDEMATEL) is used for weighting factors, and interval type-2 fuzzy VIKOR (IT2 FVIKOR) and interval type-2 fuzzy TOPSIS (IT2 FTOPSIS) are considered for ranking alternatives in the integrated modeling. Eventually, the findings highlight the most important criteria in this analysis and the results demonstrate that the comparative analysis of IT2 FVIKOR and IT2 FTOPSIS provides comprehensive and coherent results to select the best country in the innovation policies. In addition, the need to redesign the European banking system with necessary regulations to contribute to the development of the innovations is pointed out.																	1562-2479	2199-3211				JUN	2020	22	4					1158	1176		10.1007/s40815-020-00851-8		MAY 2020											
J								Determination of Interaction Between Criteria and the Criteria Priorities in Laptop Selection Problem	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										MCDM; AHP; Fuzzy AHP; DEMATEL; Laptop selection	SCIENCE-AND-TECHNOLOGY; HUMAN-RESOURCE; FUZZY AHP; DEMATEL; MODEL	The statistic of global laptop shipment showed that 161.6 million laptops were sold, 65.23% more than desktop-PCs, and almost the same quantity as tablets in 2017. So, laptop demand is quite high and needs to be researched on what kind of criteria (properties) are crucial. Since there are more than one criteria, this study focuses on multi-criteria decision-making (MCDM) methods in selecting laptops. This study aims to determine the interaction between criteria, and the criteria priorities in laptop selection problem that can be added/changing new features every day and to present the findings/results to the researcher/decision-makers. DEMATEL method is used for determining the interactions which are not observed in laptop literature, and fuzzy AHP is for criteria priorities. The price and brand image of a laptop have more interrelated action than the other main criteria. According to results, the brand image is determined as the most important criteria. Along with the brand image, other/peripheral properties and price criteria have totally two-thirds priority among six main criteria. The brand image along with the price had a dominant effect on other criteria according to the interaction matrices/diagrams. This study also shows that criteria priority results should be considered along with interaction matrices/diagrams that may produce such following information: speed, storage, monitor and other/peripheral properties criteria have overall effecting feature but not strongly, while brand image is a transition criterion along with having the highest priority, and the price criterion is obviously being affected by all criteria.																	1562-2479	2199-3211				JUN	2020	22	4					1177	1190		10.1007/s40815-020-00857-2		MAY 2020											
J								Automatic extraction of named entities of cyber threats using a deep Bi-LSTM-CRF network	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Cybersecurity; Vulnerability; Cyber threat intelligence; Named entity recognition; Bidirectional long-short-term memory conditional random field		Countless cyber threat intelligence (CTI) reports are used by companies around the world on a daily basis for security reasons. To secure critical cybersecurity information, analysts and individuals should accordingly analyze information on threats and vulnerabilities. However, analyzing such overwhelming volumes of reports requires considerable time and effort. In this study, we propose a novel approach that automatically extracts core information from CTI reports using a named entity recognition (NER) system. During the process of constructing our proposed NER system, we defined meaningful keywords in the security domain as entities, including malware, domain/URL, IP address, Hash, and Common Vulnerabilities and Exposures. Furthermore, we linked these keywords with the words extracted from the text data of the report. To achieve a higher performance, we utilized the character-level feature vector as an input to bidirectional long-short-term memory using a conditional random field network. We finally achieved an average F1-score of 75.05%. We release 498,000 tag datasets created during our research.																	1868-8071	1868-808X				OCT	2020	11	10					2341	2355		10.1007/s13042-020-01122-6		MAY 2020											
J								Semi-supervised deep learning based framework for assessing manufacturability of cellular structures in direct metal laser sintering process	JOURNAL OF INTELLIGENT MANUFACTURING										Manufacturability analysis; Cellular structures; Design for additive manufacturing; Semi-supervised deep learning; Direct metal laser sintering	DESIGN; NETWORK; TRENDS; PART	In recent years, metal cellular structures have drawn attentions in various industrial sectors due to their design freedoms and abilities to achieve multi-functional mechanical properties. However, metal cellular structures are difficult to fabricate due to their complex geometries, even with modern additive manufacturing technologies such as the direct metal laser sintering (DMLS) process. Assessing the manufacturability of metal cellular structures via a DMLS process is a challenging task as the geometric features of the structures are complex. Besides, via a DMLS process, the manufacturability also depends on the cumulative deformation of the layers during the manufacturing process. Existing methods on Design for Additive Manufacturing (DFAM) provide design guidelines that are based on past successful printed designs. However, they are not effective in predicting the manufacturability of metal cellular structures. In this paper, we propose a semi-supervised deep learning based manufacturability assessment (SSDLMA) framework to assess whether a metal cellular structure can be successfully manufactured from a given DMLS process. To enable efficient learning, we represent the complex cellular structures as 3D binary arrays with a simple yet efficient voxelisation method. We then train a deep learning based classifier using only a small amount of experimental data by adopting a semi-supervised learning approach. By running real experiments and comparing with existing DFAM methods and machine learning models, we demonstrate the advantages of the proposed SSDLMA framework. The proposed framework can be extended to predict the manufacturability of various other complex geometries beyond cellular structure in a reliable way even with a small number of training data.																	0956-5515	1572-8145															10.1007/s10845-020-01575-0		MAY 2020											
J								Efficient Relative Pose Estimation for Cameras and Generalized Cameras in Case of Known Relative Rotation Angle	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Multi-view geometry; Relative pose estimation; Generalized cameras; Epipolar constraints; Relative rotation angle; Grobner basis	ALGORITHM	We propose two minimal solvers to the problem of relative pose estimation for a camera with known relative rotation angle. In practice, such angle can be derived from the readings of a 3D gyroscope. Different from other relative pose formulations fusing a camera and a gyroscope, the use of relative rotation angle does not require extrinsic calibration between the two sensors. The first proposed solver is formulated for a calibrated regular camera and requires four-point correspondences from two views. The second solver extends the problem to a generalized camera and requires five-point correspondences. We represent the rotation part of the motion in terms of unit quaternions in order to construct polynomial equations encoding the epipolar constraints. The Grobner basis technique is then used to efficiently derive the polynomial solutions. Our first solver for regular cameras significantly improves the existing state-of-the-art solution. The second solver for generalized cameras is novel. The presented minimal solvers can be used in a hypothesize-and-test architecture such as RANSAC for reliable pose estimation. Experiments on synthetic and real datasets confirm that our algorithms are numerically stable, fast, and robust.																	0924-9907	1573-7683				OCT	2020	62	8					1076	1086		10.1007/s10851-020-00958-5		MAY 2020											
J								A scalable and effective rough set theory-based approach for big data pre-processing	KNOWLEDGE AND INFORMATION SYSTEMS										Big data; Data pre-processing; Rough set theory; Distributed processing; Scalability; High-performance computing	FEATURE-SELECTION; CLASSIFICATION; REDUCTION; MAPREDUCE	A big challenge in the knowledge discovery process is to perform data pre-processing, specifically feature selection, on a large amount of data and high dimensional attribute set. A variety of techniques have been proposed in the literature to deal with this challenge with different degrees of success as most of these techniques need further information about the given input data for thresholding, need to specify noise levels or use some feature ranking procedures. To overcome these limitations, rough set theory (RST) can be used to discover the dependency within the data and reduce the number of attributes enclosed in an input data set while using the data alone and requiring no supplementary information. However, when it comes to massive data sets, RST reaches its limits as it is highly computationally expensive. In this paper, we propose a scalable and effective rough set theory-based approach for large-scale data pre-processing, specifically for feature selection, under the Spark framework. In our detailed experiments, data sets with up to 10,000 attributes have been considered, revealing that our proposed solution achieves a good speedup and performs its feature selection task well without sacrificing performance. Thus, making it relevant to big data.																	0219-1377	0219-3116				AUG	2020	62	8					3321	3386		10.1007/s10115-020-01467-y		MAY 2020											
J								A novel super-resolution CT image reconstruction via semi-supervised generative adversarial network	NEURAL COMPUTING & APPLICATIONS										Super-resolution; Computed tomography images; Residual blocks; Generative adversarial network	HASHING-BASED APPROACH; SERVICE RECOMMENDATION; RESOLUTION	Reconstruction of super-resolution CT images using deep learning requires a large number of high-resolution images. However, high-resolution images are often limited to access due to CT performance and operation factors. In this paper, a new semi-supervised generative adversarial network is presented to accurately recover high-resolution CT images from low-resolution counterparts. We use a deep unsupervised network of 16 residual blocks to design the generator and build a discriminator based on a supervised network. We also apply a parallel 1 x 1 convolution operation to reduce the dimensionality of each hidden layer's output. Four types of loss functions are presented to build a new one for enforcing the mappings between the generator and discriminator. The bulk specification layer in the commonly used residual network is removed to construct a new type of residual network. In terms of experiments, we conduct an objective and subjective comprehensive evaluation with several state-of-the-art methods. The comparison results show that our proposed network has better advantages in super-resolution image reconstruction.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14563	14578		10.1007/s00521-020-04905-8		MAY 2020											
J								Cooperative output regulation of heterogeneous linear multi-agent systems with edge-event triggered adaptive control under time-varying topologies	NEURAL COMPUTING & APPLICATIONS										Cooperative output regulation; Adaptive controller; Fully distributed control; Time-varying communication topologies	CONSENSUS; TRACKING	This paper discusses the cooperative output regulation (COR) problem for the heterogeneous linear multi-agent systems (MAS)s with time-varying communication topologies. Both full distributed state asynchronous edge-event triggered adaptive controller and dynamic output feedback asynchronous edge-event triggered adaptive controller are designed, which all can ensure that COR problem is achieved and Zeno behavior will not exist in the MASs. Different from other works about COR problem, one of the highlights of the paper is that agents in the MASs are divided into three groups, that is, exosystem, informed agents directly receive external signal and uninformed agents that cannot directly receive external signal. The communication between the exosystem and the informed agents is not continuous by designing the event-triggered mechanism or asynchronous edge-based mechanism in the link composed of the exosystem and the informed agent. Between the informed agent and the uninformed agent, the asynchronous edge-based mechanism is proposed to reduce communication in the MASs. The methods designed can effectively reduce the waste of system resources. Note that the triggering mechanisms and adaptive controllers are all not required global information of the networks.																	0941-0643	1433-3058				OCT	2020	32	19			SI		15573	15584		10.1007/s00521-020-04883-x		MAY 2020											
J								On time series representations for multi-label NILM	NEURAL COMPUTING & APPLICATIONS										Non-intrusive load monitoring; Multi-label NILM; Smart grids; Time series representation; Signal2Vec	CLASSIFICATION	Given only the main power consumption of a household, a non-intrusive load monitoring (NILM) system identifies which appliances are operating. With the rise of Internet of things, running energy disaggregation models on the edge is more and more essential for privacy concerns and economic reasons. However, current NILM solutions use data-hungry deep learning models that can recognize only one device and are impossible to run on a device with limited resources. This research investigates in-depth multi-label NILM systems and suggests a novel framework which enables a cost-effective solution. It can be deployed on an embedded device, and thus, privacy can be preserved. The proposed system leverages dimensionality reduction using Signal2Vec, is evaluated on two popular public datasets and outperforms another state-of-the-art multi-label NILM system.																	0941-0643	1433-3058															10.1007/s00521-020-04916-5		MAY 2020											
J								Indoor scene segmentation algorithm based on full convolutional neural network	NEURAL COMPUTING & APPLICATIONS										Indoor scene; Convolutional neural network; Deep learning; Image segmentation	IMAGE SEGMENTATION; CLASSIFICATION	With the leaps and bounds of computer performance and the advent of the era of big data, deep learning has drawn more and more attention from all walks of life. It can combine low-level features to form more abstract high-level features and describe the data more essentially. Therefore, it is widely used in various fields such as computer vision. Image segmentation is one of the most basic research topics in the field of computer vision. The main purpose is to extract regions of interest from images for later image processing research. In 3D reconstruction based on sequence images, the segmentation accuracy and speed of sequence images determine the quality and efficiency of target reconstruction. Therefore, when facing large-scale sequence images, the biggest problem is how to improve the segmentation speed while ensuring accuracy. Based on the above background, the research content of this article is an indoor scene segmentation algorithm based on full convolutional neural network. According to the characteristics of indoor application scenes, this paper proposes a fast convolutional neural network image segmentation method to segment the indoor scene image and construct the fast fully convolutional networks (FFCN) for indoor scene image segmentation uses inter-layer fusion to reduce the amount of network calculation parameters and avoid the loss of picture feature information by continuous convolution. In order to verify the effectiveness of the network, in this paper, a basic living object data set (XAUT data set) in an indoor environment is created. The XAUT data set is used to train the FFCN network under the Caffe framework to obtain an indoor scene segmentation model. In order to compare the effectiveness of the model, the structure of the worn FCN8s, FCN16s, and FCN32s models was fine-tuned, and the corresponding algorithm model for indoor scene segmentation was obtained by training with the XAUT data set. The experimental results show that the pixel recognition accuracy of all types of networks has reached 86%, and the mean IU ratio has reached more than 63%. The mean IU of the FCN8s network is the highest at 70.38%, but its segmentation speed is only 1/5 of FFCN. On the premise that other types of indicators are not much different, the average segmentation speed on FFCN fast segmentation convolutional neural network reaches 40 fps. It can be seen that the scale fusion technology can well avoid the loss of image feature information in the network convolution and reddening process. Compared with other FCN networks, it has a faster speed and is conducive to real-time image preprocessing.																	0941-0643	1433-3058															10.1007/s00521-020-04961-0		MAY 2020											
J								Hesitant fuzzy soft topology and its applications to multi-attribute group decision-making	SOFT COMPUTING										Hesitant fuzzy set (HF-set); Hesitant fuzzy soft set (HFS-set); HFS-topology; MAGDM	PYTHAGOREAN MEMBERSHIP GRADES; SET-THEORY; ALGORITHM; OPERATORS	The purpose of this research study is to extend the multi-attribute group decision-making (MAGDM) methods to hesitant fuzzy soft set (HFS-set), hesitant fuzzy soft topology (HFS-topology) and HFS-Hausdorff spaces in group decision-making environment, as HFS-set is more superior tool to capture vagueness, hesitancy and incompleteness in individual evaluations. In order to obtain optimal decisions in MAGDM, we present two algorithms based on hesitant fuzzy soft set and hesitant fuzzy soft topology. Lastly, we present MAGDM method by using HFS-Hausdorff space to deal with hesitancy and uncertainty. The developed methods have the ability to solve MADGM problems in which the assessment information on available alternatives, provided by the experts, is presented by hesitant fuzzy soft sets. Furthermore, the efficiency of proposed algorithms is shown by applying them to the real-world problems. We use reduct, optimum reduct, aggregate HFS-sets and weight vector according of given alternatives, priority of the attributes and customer demand for best MAGDM in the selection of car.																	1432-7643	1433-7479				NOV	2020	24	21					16269	16289		10.1007/s00500-020-04938-0		MAY 2020											
J								Design of NEWMA np control chart for monitoring neutrosophic nonconforming items	SOFT COMPUTING										Uncertainty; Neutrosophic statistics; Classical statistics; NEWMA shift; Attribute	AVERAGE CONTROL CHART; SAMPLE-SIZE; (X)OVER-BAR; DISPERSION; RULES	We will introduce a neutrosophic exponentially weighted moving average (NEWMA) statistic for the attribute data. We will use the proposed NEWMA to design an attribute control chart. We will introduce the neutrosophic Monte Carlo simulation to find the neutrosophic average run length (NARL). The comparative study shows the efficiency of the proposed NEWMA attribute. Two examples of having neutrosophic parameters will be given to explain the proposed control chart. We hope that the proposed chart will perform better under uncertainty.																	1432-7643	1433-7479				NOV	2020	24	21					16617	16626		10.1007/s00500-020-04964-y		MAY 2020											
J								Truss-sizing optimization attempts with CSA: a detailed evaluation	SOFT COMPUTING										Truss; Size optimization; Metaheuristics; CSA; Parameter tuning	PARTICLE SWARM OPTIMIZER; ANT COLONY OPTIMIZATION; CROW SEARCH ALGORITHM; SIZE OPTIMIZATION; OPTIMAL-DESIGN; DISCRETE OPTIMIZATION; HEURISTIC ALGORITHM; OPTIMUM DESIGN; STEEL TRUSSES; CONSTRAINTS	Thanks to the advent of powerful computers, searching for optimal solutions to engineering design problems becomes easier every day. Numerous researchers are still developing modern optimization algorithms, and the competition for "the most efficient optimization algorithm" continues apace. This study evaluates the performances of the Crow Search Algorithm (CSA) and a slightly modified variant (CSA(M)) in one of the most popular and controversial competitions in the structural optimization field for the first time. Unlike most of the works on structural optimization, this paper does not tell a success story. After days of computation to collect the sensitivity and convergence data, it is shown that both CSA and CSA(M) mostly fail compared to today's competitive algorithms. The findings of the study are discussed through tables and plots in detail to share the unfavorable experience on the truss optimization attempts, to review the difficulties of using parameter-controlled algorithms in structural optimization through CSA, and to save time for the researchers in the field.																	1432-7643	1433-7479				NOV	2020	24	22					16775	16801		10.1007/s00500-020-04972-y		MAY 2020											
J								Designing and deploying insurance recommender systems using machine learning	WILEY INTERDISCIPLINARY REVIEWS-DATA MINING AND KNOWLEDGE DISCOVERY										Bayesian networks; deep learning; deployed system; insurance domain; recommender systems; structure learning	BAYESIAN NETWORKS; DISEASE	Recommender systems have become extremely important to various types of industries where customer interaction and feedback is paramount to the success of the business. For companies that face changes that arise with ever-growing markets, providing product recommendations to new and existing customers is a challenge. Our goal is to give our customers personalized recommendations based on what other similar people with similar portfolios have, in order to make sure they are adequately covered for their needs. Our system uses customer characteristics in addition to customer portfolio data. Since the number of possible recommendable products is relatively small, compared to other recommender domains, and missing data is relatively frequent, we chose to use Bayesian Networks for modeling our systems. We also present a deep-learning-based approach to provide recommendations to prospects (potential customers) where only external marketing data is available at the time of prediction. This article is categorized under: Application Areas > Industry Specific Applications Algorithmic Development > Structure Discovery Algorithmic Development > Bayesian Models Technologies > Machine Learning																	1942-4787	1942-4795				JUL	2020	10	4							e1363	10.1002/widm.1363		MAY 2020											
J								Complex sequential understanding through the awareness of spatial and temporal concepts	NATURE MACHINE INTELLIGENCE												Understanding sequential information is a fundamental task for artificial intelligence. Current neural networks attempt to learn spatial and temporal information as a whole, limiting their abilities to represent large-scale spatial representations over long-range sequences. Here, we introduce a new modelling strategy-'semi-coupled structure' (SCS)-which consists of deep neural networks that decouple the complex spatial and temporal concepts during learning. SCS can learn to implicitly separate input information into independent parts and process these parts separately. Experiments demonstrate that SCS can successfully sequentially annotate the outline of an object in images and perform video action recognition. As an example of sequence-to-sequence problems, SCS can predict future meteorological radar echo images based on observed images. Taken together, our results demonstrate that SCS has the capacity to improve the performance of long short-term memory (LSTM)-like models on large-scale sequential tasks. Current neural networks attempt to learn spatial and temporal information as a whole, limiting their ability to process complex video data. Pang et al. improve performance by introducing a network structure which learns to implicitly decouple complex spatial and temporal concepts.																		2522-5839				MAY	2020	2	5					245	+		10.1038/s42256-020-0168-3													
J								Direct steering of de novo molecular generation with descriptor conditional recurrent neural networks	NATURE MACHINE INTELLIGENCE												Deep learning has acquired considerable momentum over the past couple of years in the domain of de novo drug design. Here, we propose a simple approach to the task of focused molecular generation for drug design purposes by constructing a conditional recurrent neural network (cRNN). We aggregate selected molecular descriptors and transform them into the initial memory state of the network before starting the generation of alphanumeric strings that describe molecules. We thus tackle the inverse design problem directly, as the cRNNs may generate molecules near the specified conditions. Moreover, we exemplify a novel way of assessing the focus of the conditional output of such a model using negative log-likelihood plots. The output is more focused than traditional unbiased RNNs, yet less focused than autoencoders, thus representing a novel method with intermediate output specificity between well-established methods. Conceptually, our architecture shows promise for the generalized problem of steering of sequential data generation with recurrent neural networks. The rise of deep neural networks allows for new ways to design molecules that interact with biological structures. An approach that uses conditional recurrent neural networks generates molecules with properties near specified conditions.																		2522-5839				MAY	2020	2	5					254	+		10.1038/s42256-020-0174-5													
J								Improving healthcare operations management with machine learning	NATURE MACHINE INTELLIGENCE											GENERALIZED ADDITIVE-MODELS; REGRESSION; TREES; TIMES; ROOM	Healthcare institutions need modern and powerful technology to provide high-quality, cost-effective care to patients. However, despite the considerable progress in the computerization and digitization of medicine, efficient and robust management tools have yet to materialize. One important reason for this is the extreme complexity and variability of healthcare operations, the needs of which have outgrown conventional management. Machine learning algorithms, scalable and adaptive to complex patterns, may be particularly well suited to solving these problems. Two major advantages of machine learning-the power of building strong models from a large number of weakly predictive features, and the ability to identify key factors in complex feature sets-have a particularly direct connection to the principal operational challenges. The main goal of this work was to study this relationship using two major types of operational problems: predicting operational events, and identifying key workflow drivers. Using practical examples, we demonstrate how machine learning can improve human ability to understand and manage healthcare operations, leading to more efficient healthcare. While computerization and digitization of medicine have advanced substantially, management tools in healthcare have not yet benefited much from these developments due to the extreme complexity and variability of healthcare operations. The ability of machine learning algorithms to build strong models from a large number of weakly predictive features, and to identify key factors in complex feature sets, is tested in operational problems involving hospital datasets on workflow and patient waiting time.																		2522-5839				MAY	2020	2	5					266	+		10.1038/s42256-020-0176-3													
J								A shallow convolutional neural network predicts prognosis of lung cancer patients in multi-institutional computed tomography image datasets	NATURE MACHINE INTELLIGENCE											PULMONARY NODULES; GENE-EXPRESSION; CT; RADIOMICS; SEGMENTATION; RADIOGENOMICS; SURVIVAL	Lung cancer is the most common fatal malignancy in adults worldwide, and non-small-cell lung cancer (NSCLC) accounts for 85% of lung cancer diagnoses. Computed tomography is routinely used in clinical practice to determine lung cancer treatment and assess prognosis. Here, we developed LungNet, a shallow convolutional neural network for predicting outcomes of patients with NSCLC. We trained and evaluated LungNet on four independent cohorts of patients with NSCLC from four medical centres: Stanford Hospital (n=129), H. Lee Moffitt Cancer Center and Research Institute (n=185), MAASTRO Clinic (n=311) and Charite - Universitatsmedizin, Berlin (n=84). We show that outcomes from LungNet are predictive of overall survival in all four independent survival cohorts as measured by concordance indices of 0.62, 0.62, 0.62 and 0.58 on cohorts 1, 2, 3 and 4, respectively. Furthermore, the survival model can be used, via transfer learning, for classifying benign versus malignant nodules on the Lung Image Database Consortium (n=1,010), with improved performance (AUC=0.85) versus training from scratch (AUC=0.82). LungNet can be used as a non-invasive predictor for prognosis in patients with NSCLC and can facilitate interpretation of computed tomography images for lung cancer stratification and prognostication. Predicting overall survival for patients with confirmed non-small-cell lung cancer is an important issue in clinical practice. The authors developed and validated in four independent patient cohorts a shallow convolutional neural network that can predict the outcomes of individuals using pre-treatment CT images. The authors further show that the survival model can be used, via transfer learning, for classifying benign versus malignant nodules.																		2522-5839				MAY	2020	2	5					274	282		10.1038/s42256-020-0173-6													
J								An interpretable mortality prediction model for COVID-19 patients	NATURE MACHINE INTELLIGENCE											CORONAVIRUS	The sudden increase in COVID-19 cases is putting high pressure on healthcare services worldwide. At this stage, fast, accurate and early clinical assessment of the disease severity is vital. To support decision making and logistical planning in healthcare systems, this study leverages a database of blood samples from 485 infected patients in the region of Wuhan, China, to identify crucial predictive biomarkers of disease mortality. For this purpose, machine learning tools selected three biomarkers that predict the mortality of individual patients more than 10 days in advance with more than 90% accuracy: lactic dehydrogenase (LDH), lymphocyte and high-sensitivity C-reactive protein (hs-CRP). In particular, relatively high levels of LDH alone seem to play a crucial role in distinguishing the vast majority of cases that require immediate medical attention. This finding is consistent with current medical knowledge that high LDH levels are associated with tissue breakdown occurring in various diseases, including pulmonary disorders such as pneumonia. Overall, this Article suggests a simple and operable decision rule to quickly predict patients at the highest risk, allowing them to be prioritized and potentially reducing the mortality rate. Early and accurate clinical assessment of disease severity in COVID-19 patients is essential for planning the allocation of scarce hospital resources. An explainable machine learning tool trained on blood sample data from 485 patients from Wuhan selected three biomarkers for predicting mortality of individual patients with high accuracy.																		2522-5839				MAY	2020	2	5					283	+		10.1038/s42256-020-0180-7													
J								How to Inspect and Measure Data Quality about Scientific Publications: Use Case of Wikipedia and CRIS Databases	ALGORITHMS										Wikipedia; current research information systems (CRIS); publications data; data quality; objective quality dimensions; research data processing; data management; data analysis; data measurement; completeness; consistency; correctness; timeliness; efficient decision-making	RESEARCH INFORMATION-SYSTEMS; METRICS	The quality assurance of publication data in collaborative knowledge bases and in current research information systems (CRIS) becomes more and more relevant by the use of freely available spatial information in different application scenarios. When integrating this data into CRIS, it is necessary to be able to recognize and assess their quality. Only then is it possible to compile a result from the available data that fulfills its purpose for the user, namely to deliver reliable data and information. This paper discussed the quality problems of source metadata in Wikipedia and CRIS. Based on real data from over 40 million Wikipedia articles in various languages, we performed preliminary quality analysis of the metadata of scientific publications using a data quality tool. So far, no data quality measurements have been programmed with Python to assess the quality of metadata from scientific publications in Wikipedia and CRIS. With this in mind, we programmed the methods and algorithms as code, but presented it in the form of pseudocode in this paper to measure the quality related to objective data quality dimensions such as completeness, correctness, consistency, and timeliness. This was prepared as a macro service so that the users can use the measurement results with the program code to make a statement about their scientific publications metadata so that the management can rely on high-quality data when making decisions.																		1999-4893				MAY	2020	13	5							107	10.3390/a13050107													
J								A Novel Data-Driven Magnetic Resonance Spectroscopy Signal Analysis Framework to Quantify Metabolite Concentration	ALGORITHMS										magnetic resonance spectroscopy; singular value decomposition; K-means clustering; metabolite concentration; neurodegenerative disorders	HUMAN BRAIN; IN-VIVO; AUTOMATED QUANTITATION; MR SPECTROSCOPY; NMR; WATER; TIME; QUANTIFICATION; ALGORITHM; SPECTRA	Developing tools for precise quantification of brain metabolites using magnetic resonance spectroscopy (MRS) is an active area of research with broad application in non-invasive neurodegenerative disease studies. The tools are mainly developed based on black box (data-driven), or basis sets approaches. In this study, we offer a multi-stage framework that integrates data-driven and basis sets methods. We first use truncated Hankel singular value decomposition (HSVD) to decompose free induction decay (FID) signals into single tone FIDs, as the data-driven stage. Subsequently, single tone FIDs are clustered into basis sets while using initialized K-means with prior knowledge of the metabolites, as the basis set stage. The generated basis sets are fitted with the magnetic resonance (MR) spectra while using a linear constrained least square, and then the metabolite concentration is calculated. Prior to using our proposed multi-stage approach, a sequence of preprocessing blocks: water peak removal, phase correction, and baseline correction (developed in house) are used.																		1999-4893				MAY	2020	13	5							120	10.3390/a13050120													
J								p-Refined Multilevel Quasi-Monte Carlo for Galerkin Finite Element Methods with Applications in Civil Engineering	ALGORITHMS										Multilevel Monte Carlo; Multilevel Quasi-Monte Carlo; h- and p-refinement; uncertainty quantification; structural engineering; geotechnical engineering	SIMULATION; EFFICIENT	Civil engineering applications are often characterized by a large uncertainty on the material parameters. Discretization of the underlying equations is typically done by means of the Galerkin Finite Element method. The uncertain material parameter can be expressed as a random field represented by, for example, a Karhunen-Loeve expansion. Computation of the stochastic responses, i.e., the expected value and variance of a chosen quantity of interest, remains very costly, even when state-of-the-art Multilevel Monte Carlo (MLMC) is used. A significant cost reduction can be achieved by using a recently developed multilevel method: p-refined Multilevel Quasi-Monte Carlo (p-MLQMC). This method is based on the idea of variance reduction by employing a hierarchical discretization of the problem based on a p-refinement scheme. It is combined with a rank-1 Quasi-Monte Carlo (QMC) lattice rule, which yields faster convergence compared to the use of random Monte Carlo points. In this work, we developed algorithms for the p-MLQMC method for two dimensional problems. The p-MLQMC method is first benchmarked on an academic beam problem. Finally, we use our algorithm for the assessment of the stability of slopes, a problem that arises in geotechnical engineering, and typically suffers from large parameter uncertainty. For both considered problems, we observe a very significant reduction in the amount of computational work with respect to MLMC.																		1999-4893				MAY	2020	13	5							110	10.3390/a13050110													
J								Forecasting Electricity Prices: A Machine Learning Approach	ALGORITHMS										energy sector; electricity prices; forecasting; machine learning; geometric semantic; based programming	ENERGY PRICES; INTELLIGENCE; MARKETS; DEMAND; CARBON; MODEL	The electricity market is a complex, evolutionary, and dynamic environment. Forecasting electricity prices is an important issue for all electricity market participants. In this study, we shed light on how to improve electricity price forecasting accuracy through the use of a machine learning technique-namely, a novel genetic programming approach. Drawing on empirical data from the largest EU energy markets, we propose a forecasting model that considers variables related to weather conditions, oil prices, and CO2 coupons and predicts energy prices 24 h ahead. We show that the proposed model provides more accurate predictions of future electricity prices than existing prediction methods. Our important findings will assist the electricity market participants in forecasting future price movements.																		1999-4893				MAY	2020	13	5							119	10.3390/a13050119													
J								PUB-SalNet: A Pre-Trained Unsupervised Self-Aware Backpropagation Network for Biomedical Salient Segmentation	ALGORITHMS										unsupervised learning; saliency segmentation; biomedical image processing; pre-trained methods	ATTENTION	Salient segmentation is a critical step in biomedical image analysis, aiming to cut out regions that are most interesting to humans. Recently, supervised methods have achieved promising results in biomedical areas, but they depend on annotated training data sets, which requires labor and proficiency in related background knowledge. In contrast, unsupervised learning makes data-driven decisions by obtaining insights directly from the data themselves. In this paper, we propose a completely unsupervised self-aware network based on pre-training and attentional backpropagation for biomedical salient segmentation, named as PUB-SalNet. Firstly, we aggregate a new biomedical data set from several simulated Cellular Electron Cryo-Tomography (CECT) data sets featuring rich salient objects, different SNR settings, and various resolutions, which is called SalSeg-CECT. Based on the SalSeg-CECT data set, we then pre-train a model specially designed for biomedical tasks as a backbone module to initialize network parameters. Next, we present a U-SalNet network to learn to selectively attend to salient objects. It includes two types of attention modules to facilitate learning saliency through global contrast and local similarity. Lastly, we jointly refine the salient regions together with feature representations from U-SalNet, with the parameters updated by self-aware attentional backpropagation. We apply PUB-SalNet for analysis of 2D simulated and real images and achieve state-of-the-art performance on simulated biomedical data sets. Furthermore, our proposed PUB-SalNet can be easily extended to 3D images. The experimental results on the 2d and 3d data sets also demonstrate the generalization ability and robustness of our method.																		1999-4893				MAY	2020	13	5							126	10.3390/a13050126													
J								The Effect of Different Deep Network Architectures upon CNN-Based Gaze Tracking	ALGORITHMS										gaze tracking; convolution neural network; batch normalization; global average pooling layer		In this paper, we explore the effect of using different convolutional layers, batch normalization and the global average pooling layer upon a convolutional neural network (CNN) based gaze tracking system. A novel method is proposed to label the participant's face images as gaze points retrieved from eye tracker while watching videos for building a training dataset that is closer to human visual behavior. The participants can swing their head freely; therefore, the most real and natural images can be obtained without too many restrictions. The labeled data are classified according to the coordinate of gaze and area of interest on the screen. Therefore, varied network architectures are applied to estimate and compare the effects including the number of convolutional layers, batch normalization (BN) and the global average pooling (GAP) layer instead of the fully connected layer. Three schemes, including the single eye image, double eyes image and facial image, with data augmentation are used to feed into neural network to train and evaluate the efficiency. The input image of the eye or face for an eye tracking system is mostly a small-sized image with relatively few features. The results show that BN and GAP are helpful in overcoming the problem to train models and in reducing the amount of network parameters. It is shown that the accuracy is significantly improved when using GAP and BN at the mean time. Overall, the face scheme has a highest accuracy of 0.883 when BN and GAP are used at the mean time. Additionally, comparing to the fully connected layer set to 512 cases, the number of parameters is reduced by less than 50% and the accuracy is improved by about 2%. A detection accuracy comparison of our model with the existing George and Routray methods shows that our proposed method achieves better prediction accuracy of more than 6%.																		1999-4893				MAY	2020	13	5							127	10.3390/a13050127													
J								Uncertainty Quantification Approach on Numerical Simulation for Supersonic Jets Performance	ALGORITHMS										uncertainty quantification; surrogate-based UQ; CFD; supersonic jets; convergent-divergent		One of the main issues addressed in any engineering design problem is to predict the performance of the component or system as accurately and realistically as possible, taking into account the variability of operating conditions or the uncertainty on input data (boundary conditions or geometry tolerance). In this paper, the propagation of uncertainty on boundary conditions through a numerical model of supersonic nozzle is investigated. The evaluation of the statistics of the problem response functions is performed following 'Surrogate-Based Uncertainty Quantification'. The approach involves: (a) the generation of a response surface starting from a DoE in order to approximate the convergent-divergent 'physical' model (expensive to simulate), (b) the application of the UQ technique based on the LHS to the meta-model. Probability Density Functions are introduced for the inlet boundary conditions in order to quantify their effects on the output nozzle performance. The physical problem considered is very relevant for the experimental tests on the UQ approach because of its high non-linearity. A small perturbation to the input data can drive the solution to a completely different output condition. The CFD simulations and the Uncertainty Quantification were performed by coupling the open source Dakota platform with the ANSYS Fluent (R) CFD commercial software: the process is automated through scripting. The procedure adopted in this work demonstrate the applicability of advanced simulation techniques (such as UQ analysis) to industrial technical problems. Moreover, the analysis highlights the practical use of the uncertainty quantification techniques in predicting the performance of a nozzle design affected by off-design conditions with fluid-dynamic complexity due to strong nonlinearity.																		1999-4893				MAY	2020	13	5							130	10.3390/a13050130													
J								Evolution of SOMs' Structure and Learning Algorithm: From Visualization of High-Dimensional Data to Clustering of Complex Data	ALGORITHMS										artificial intelligence; computational intelligence; artificial neural networks; self-organizing neural networks; self-organizing maps; high-dimensional data visualization; complex data clustering	SELF-ORGANIZING-MAP; NETWORK	In this paper, we briefly present several modifications and generalizations of the concept of self-organizing neural networks-usually referred to as self-organizing maps (SOMs)-to illustrate their advantages in applications that range from high-dimensional data visualization to complex data clustering. Starting from conventional SOMs, Growing SOMs (GSOMs), Growing Grid Networks (GGNs), Incremental Grid Growing (IGG) approach, Growing Neural Gas (GNG) method as well as our two original solutions, i.e., Generalized SOMs with 1-Dimensional Neighborhood (GeSOMs with 1DN also referred to as Dynamic SOMs (DSOMs)) and Generalized SOMs with Tree-Like Structures (GeSOMs with T-LSs) are discussed. They are characterized in terms of (i) the modification mechanisms used, (ii) the range of network modifications introduced, (iii) the structure regularity, and (iv) the data-visualization/data-clustering effectiveness. The performance of particular solutions is illustrated and compared by means of selected data sets. We also show that the proposed original solutions, i.e., GeSOMs with 1DN (DSOMs) and GeSOMS with T-LSs outperform alternative approaches in various complex clustering tasks by providing up to 20% increase in the clustering accuracy. The contribution of this work is threefold. First, algorithm-oriented original computer-implementations of particular SOM's generalizations are developed. Second, their detailed simulation results are presented and discussed. Third, the advantages of our earlier-mentioned original solutions are demonstrated.																		1999-4893				MAY	2020	13	5							109	10.3390/a13050109													
J								Distributional Reinforcement Learning with Ensembles	ALGORITHMS										distributional reinforcement learning; multiagent learning; ensembles; categorical reinforcement learning		It is well known that ensemble methods often provide enhanced performance in reinforcement learning. In this paper, we explore this concept further by using group-aided training within the distributional reinforcement learning paradigm. Specifically, we propose an extension to categorical reinforcement learning, where distributional learning targets are implicitly based on the total information gathered by an ensemble. We empirically show that this may lead to much more robust initial learning, a stronger individual performance level, and good efficiency on a per-sample basis.																		1999-4893				MAY	2020	13	5							118	10.3390/a13050118													
J								Ensemble Deep Learning Models for Forecasting Cryptocurrency Time-Series	ALGORITHMS										deep learning; ensemble learning; convolutional networks; long short-term memory; cryptocurrency; time-series	NEURAL-NETWORKS	Nowadays, cryptocurrency has infiltrated almost all financial transactions; thus, it is generally recognized as an alternative method for paying and exchanging currency. Cryptocurrency trade constitutes a constantly increasing financial market and a promising type of profitable investment; however, it is characterized by high volatility and strong fluctuations of prices over time. Therefore, the development of an intelligent forecasting model is considered essential for portfolio optimization and decision making. The main contribution of this research is the combination of three of the most widely employed ensemble learning strategies: ensemble-averaging, bagging and stacking with advanced deep learning models for forecasting major cryptocurrency hourly prices. The proposed ensemble models were evaluated utilizing state-of-the-art deep learning models as component learners, which were comprised by combinations of long short-term memory (LSTM), Bi-directional LSTM and convolutional layers. The ensemble models were evaluated on prediction of the cryptocurrency price on the following hour (regression) and also on the prediction if the price on the following hour will increase or decrease with respect to the current price (classification). Additionally, the reliability of each forecasting model and the efficiency of its predictions is evaluated by examining for autocorrelation of the errors. Our detailed experimental analysis indicates that ensemble learning and deep learning can be efficiently beneficial to each other, for developing strong, stable, and reliable forecasting models.																		1999-4893				MAY	2020	13	5							121	10.3390/a13050121													
J								Change-Point Detection in Autoregressive Processes via the Cross-Entropy Method	ALGORITHMS										time series; change-point; Minimum Description Length; Cross-Entropy algorithm	BIOLOGICAL SEQUENCES; SEGMENTATION; ALGORITHMS; PACKAGE	It is very often the case that at some moment a time series process abruptly changes its underlying structure and, therefore, it is very important to accurately detect such change-points. In this problem, which is called a change-point (or break-point) detection problem, we need to find a method that divides the original nonstationary time series into a piecewise stationary segments. In this paper, we develop a flexible method to estimate the unknown number and the locations of change-points in autoregressive time series. In order to find the optimal value of a performance function, which is based on the Minimum Description Length principle, we develop a Cross-Entropy algorithm for the combinatorial optimization problem. Our numerical experiments show that the proposed approach is very efficient in detecting multiple change-points when the underlying process has moderate to substantial variations in the mean and the autocorrelation coefficient. We also apply the proposed method to real data of daily AUD/CNY exchange rate series from 2 January 2018 to 24 March 2020.																		1999-4893				MAY	2020	13	5							128	10.3390/a13050128													
J								Image Resolution Enhancement of Highly Compressively Sensed CT/PET Signals	ALGORITHMS										CT; PET; super-resolution; compressed sensing	OF-INTEREST; RECONSTRUCTION; SCANNER; PET; CT	One of the most challenging aspects of medical modalities such as Computed Tomography (CT) as well hybrid techniques such as CT/PET (Computed Tomography/Positron emission tomography) and PET/MRI is finding a balance between examination time, radiation dose, and image quality. The need for a dense sampling grid is associated with two major factors: image resolution enhancement, which leads to a strengthening of human perception, and image features interpretation. All these aspects make an unsupervised image processing much easier. The presented algorithm employs super-resolution-reconstruction with high accuracy motion fields' estimation at its core for Computed Tomography/Positron Emission Tomography (CT/PET) images enhancement. The suggested method starts with processing compressively sensed input signals. This paper shows that it is possible to achieve higher image resolution while keeping the same radiation dose. The purpose of this paper is to propose a highly effective CT/PET image reconstruction strategy, allowing for simultaneous resolution enhancing and scanning time minimisation. The algorithm aims to overcome two major obstacles-image resolution limitation and algorithm reconstruction time efficiency-by combining a highly-sparse Ridgelet analysis based sampling pattern as well as PET signal sensing with super-resolution (SR) image enhancement. Due to the diverse nature of Computed Tomography, the applied Ridgelet analysis arguing its usability turned out to be efficient in reducing acquisition times in regard to maintaining satisfying scan quality. This paper presents a super-resolution image enhancement algorithm designed for handling highly sensitively compressed hybrid CT/PET scanners raw data. The presented technique allows for improving image resolution while reducing motion artefacts and keeping scanning times at pretty low levels.																		1999-4893				MAY	2020	13	5							129	10.3390/a13050129													
J								The Expected Utility Insurance Premium Principle with Fourth-Order Statistics: Does It Make a Difference?	ALGORITHMS										risk management; insurance; premium; expected utility; CARA	RISK-AVERSION	The expected utility principle is often used to compute the insurance premium through a second-order approximation of the expected value of the utility of losses. We investigate the impact of using a more accurate approximation based on the fourth-order statistics of the expected loss and derive the premium under this expectedly more accurate approximation. The comparison between the two approximation levels shows that the second-order-based premium is always lower (i.e., an underestimate of the correct one) for the commonest loss distributions encountered in insurance. The comparison is also carried out for real cases, considering the loss parameters values estimated in the literature. The increased risk of the insurer is assessed through the Value-at-Risk.																		1999-4893				MAY	2020	13	5							116	10.3390/a13050116													
J								Incremental FPT Delay	ALGORITHMS										parameterized complexity; function complexity; enumeration	PARAMETERIZED COMPLEXITY; DUALIZATION	In this paper, we study the relationship of parameterized enumeration complexity classes defined by Creignou et al. (MFCS 2013). Specifically, we introduce two hierarchies (IncFPTa and CapIncFPTa) of enumeration complexity classes for incremental fpt-time in terms of exponent slices and show how they interleave. Furthermore, we define several parameterized function classes and, in particular, introduce the parameterized counterpart of the class of nondeterministic multivalued functions with values that are polynomially verifiable and guaranteed to exist, TFNP, known from Megiddo and Papadimitriou (TCS 1991). We show that this class TF(para-NP), the restriction of the function variant of NP to total functions, collapsing to F(FPT), the function variant of FPT, is equivalent to the result that OutputFPT coincides with IncFPT. In addition, these collapses are shown to be equivalent to TFNP = FP, and also equivalent to P equals NP intersected with coNP. Finally, we show that these two collapses are equivalent to the collapse of IncP and OutputP in the classical setting. These results are the first direct connections of collapses in parameterized enumeration complexity to collapses in classical enumeration complexity, parameterized function complexity, classical function complexity, and computational complexity theory.																		1999-4893				MAY	2020	13	5							122	10.3390/a13050122													
J								Goal Oriented Time Adaptivity Using Local Error Estimates	ALGORITHMS										adaptivity; IVPs; goal oriented problems; error estimation	ADJOINT	We consider initial value problems (IVPs) where we are interested in a quantity of interest (QoI) that is the integral in time of a functional of the solution. For these, we analyze goal oriented time adaptive methods that use only local error estimates. A local error estimate and timestep controller for step-wise contributions to the QoI are derived. We prove convergence of the error in the QoI for tolerance to zero under a controllability assumption. By analyzing global error propagation with respect to the QoI, we can identify possible issues and make performance predictions. Numerical tests verify these results. We compare performance with classical local error based time-adaptivity and a posteriori based adaptivity using the dual-weighted residual (DWR) method. For dissipative problems, local error based methods show better performance than DWR and the goal oriented method shows good results in most examples, with significant speedups in some cases.																		1999-4893				MAY	2020	13	5							113	10.3390/a3050113													
J								A Novel Hybrid Metaheuristic Algorithm for Optimization of Construction Management Site Layout Planning	ALGORITHMS										algorithms; metaheuristic; optimization; symbiotic organisms search; construction; site layout planning	SYMBIOTIC ORGANISMS SEARCH; DIMENSIONAL SPACE ALLOCATION; PARTICLE SWARM OPTIMIZATION; GENETIC ALGORITHM; DESIGN; SINGLE	Symbiotic organisms search (SOS) is a promising metaheuristic algorithm that has been studied recently by numerous researchers due to its capability to solve various hard and complex optimization problems. SOS is a powerful optimization technique that mimics the simulation of the typical symbiotic interactions among organisms in an ecosystem. This study presents a new SOS-based hybrid algorithm for solving the challenging construction site layout planning (CSLP) discrete problems. A new algorithm called the hybrid symbiotic organisms search with local operators (HSOS-LO) represents a combination of the canonical SOS and several local search mechanisms aimed at increasing the searching capability in discrete-based solution space. In this study, three CSLP problems that consist of single and multi-floor facility layout problems are tested, and the obtained results were compared with other widely used metaheuristic algorithms. The results indicate the robust performance of the HSOS-LO algorithm in handling discrete-based CSLP problems.																		1999-4893				MAY	2020	13	5							117	10.3390/a13050117													
J								Diagnosis in Tennis Serving Technique	ALGORITHMS										rule-based expert systems; tennis hitting technique; computer algebra systems; Groebner bases; Boolean logic	EXPERT-SYSTEM; LOGIC; BASES; GEOMETRY; MODEL	Tennis is a sport with a very complex technique. Amateur tennis players have trainers and/or coaches, but are not usually accompanied by them to championships. Curiously, in this sport, the result of many matches can be changed by a small hint like 'hit the ball a little higher when serving'. However, the biomechanical of a tennis stroke is only clear to an expert. We, therefore, developed a prototype of a rule-based expert system (RBES) aimed at an amateur competition player that is not accompanied by his/her coach to a championship and is not serving as usual (the RBES is so far restricted to serving). The player has to answer a set of questions about how he/she is serving that day and his/her usual serving technique and the RBES obtains a diagnosis using logic inference about the possible reasons (according of the logic rules that have been previously given to the RBES). A certain knowledge of the tennis terminology and technique is required from the player, but that is something known at this level. The underlying logic is Boolean and the inference engine is algebraic (it uses Groebner bases).																		1999-4893				MAY	2020	13	5							106	10.3390/a13050106													
J								A Fuzzy-Based Decision Support Model for Risk Maturity Evaluation of Construction Organizations	ALGORITHMS										risk management; risk maturity evaluation; analytic network process (ANP); fuzzy set theory; construction organizations	MANAGEMENT; PROJECTS	Risk maturity evaluation is an efficient tool which can assist construction organizations in the identification of their strengths and weaknesses in risk management processes and in taking necessary actions for the improvement of these processes. The accuracy of its results relies heavily on the quality of responses provided by participants specialized in these processes across the organization. Risk maturity models reported in the literature gave equal importance to participants' responses during the model development, neglecting their level of authority in the organization as well as their level of expertise in risk management processes. Unlike the existing models, this paper presents a new risk maturity model that considers the relative importance of the responses provided by the participants in the model development. It considered their authority in the organization and their level of involvement in the risk management processes for calculating the relative weights associated with the risk maturity attributes. It employed an analytic network process (ANP) to model the interdependencies among the risk maturity attributes and utilizes the fuzzy set theory to incorporate the uncertainty associated with the ambiguity of the responses used in the model development. The developed model allows the construction organizations to have a more accurate and realistic view of their current performance in risk management processes. The application of the developed model was investigated by measuring the risk maturity level of an industrial partner working on civil infrastructure projects in Canada.																		1999-4893				MAY	2020	13	5							115	10.3390/a13050115													
J								Mining Sequential Patterns with VC-Dimension and Rademacher Complexity	ALGORITHMS										data mining; sequential patterns; sampling; VC-dimension; Rademacher complexity; statistical learning		Sequential pattern mining is a fundamental data mining task with application in several domains. We study two variants of this task-the first is the extraction of frequent sequential patterns, whose frequency in a dataset of sequential transactions is higher than a user-provided threshold; the second is the mining of true frequent sequential patterns, which appear with probability above a user-defined threshold in transactions drawn from the generative process underlying the data. We present the first sampling-based algorithm to mine, with high confidence, a rigorous approximation of the frequent sequential patterns from massive datasets. We also present the first algorithms to mine approximations of the true frequent sequential patterns with rigorous guarantees on the quality of the output. Our algorithms are based on novel applications of Vapnik-Chervonenkis dimension and Rademacher complexity, advanced tools from statistical learning theory, to sequential pattern mining. Our extensive experimental evaluation shows that our algorithms provide high-quality approximations for both problems we consider.																		1999-4893				MAY	2020	13	5							123	10.3390/a13050123													
J								Two NEH Heuristic Improvements for Flowshop Scheduling Problem with Makespan Criterion	ALGORITHMS										NEH; flowshop; scheduling; heuristic; methods; factorial basis decomposition	PERMUTATION FLOWSHOP; M-MACHINE; N-JOB; MINIMIZE MAKESPAN; ALGORITHM	Since its creation by Nawaz, Enscore, and Ham in 1983, NEH remains the best heuristic method to solve flowshop scheduling problems. In the large body of literature dealing with the application of this heuristic, it can be clearly noted that results differ from one paper to another. In this paper, two methods are proposed to improve the original NEH, based on the two points in the method where choices must be made, in case of equivalence between two job orders or partial sequences. When an equality occurs in a sorting method, two results are equivalent, but can lead to different final results. In order to propose the first improvement to NEH, the factorial basis decomposition method is introduced, which makes a number computationally correspond to a permutation. This method is very helpful for the first improvement, and allows testing of all the sequencing possibilities for problems counting up to 50 jobs. The second improvement is located where NEH keeps the best partial sequence. Similarly, a list of equivalent partial sequences is kept, rather than only one, to provide the global method a chance of better performance. The results obtained with the successive use of the two methods of improvement present an average improvement of 19% over the already effective results of the original NEH method.																		1999-4893				MAY	2020	13	5							112	10.3390/a13050112													
J								Investigation of the iCC Framework Performance for Solving Constrained LSGO Problems	ALGORITHMS										large-scale global optimization; evolution algorithms; differential evolution; cooperative coevolution; constrained optimization	COOPERATIVE COEVOLUTION; DIFFERENTIAL EVOLUTION; OPTIMIZATION; ALGORITHM	Many modern real-valued optimization tasks use "black-box" (BB) models for evaluating objective functions and they are high-dimensional and constrained. Using common classifications, we can identify them as constrained large-scale global optimization (cLSGO) tasks. Today, the IEEE Congress of Evolutionary Computation provides a special session and several benchmarks for LSGO. At the same time, cLSGO problems are not well studied yet. The majority of modern optimization techniques demonstrate insufficient performance when confronted with cLSGO tasks. The effectiveness of evolution algorithms (EAs) in solving constrained low-dimensional optimization problems has been proven in many scientific papers and studies. Moreover, the cooperative coevolution (CC) framework has been successfully applied for EA used to solve LSGO problems. In this paper, a new approach for solving cLSGO has been proposed. This approach is based on CC and a method that increases the size of groups of variables at the decomposition stage (iCC) when solving cLSGO tasks. A new algorithm has been proposed, which combined the success-history based parameter adaptation for differential evolution (SHADE) optimizer, iCC, and the "epsilon-constrained method (namely "epsilon-iCC-SHADE). We investigated the performance of the "-iCC-SHADE and compared it with the previously proposed "epsilon-CC-SHADE algorithm on scalable problems from the IEEE CEC 2017 Competition on constrained real-parameter optimization.																		1999-4893				MAY	2020	13	5							108	10.3390/a13050108													
J								Multi-Level Joint Feature Learning for Person Re-Identification	ALGORITHMS										deep learning; intelligent monitoring; person re-identification		In person re-identification, extracting image features is an important step when retrieving pedestrian images. Most of the current methods only extract global features or local features of pedestrian images. Some inconspicuous details are easily ignored when learning image features, which is not efficient or robust to for scenarios with large differences. In this paper, we propose a Multi-level Feature Fusion model that combines both global features and local features of images through deep learning networks to generate more discriminative pedestrian descriptors. Specifically, we extract local features from different depths of network by the Part-based Multi-level Net to fuse low-to-high level local features of pedestrian images. Global-Local Branches are used to extract the local features and global features at the highest level. The experiments have proved that our deep learning model based on multi-level feature fusion works well in person re-identification. The overall results outperform the state of the art with considerable margins on three widely-used datasets. For instance, we achieve 96% Rank-1 accuracy on the Market-1501 dataset and 76.1% mAP on the DukeMTMC-reID dataset, outperforming the existing works by a large margin (more than 6%).																		1999-4893				MAY	2020	13	5							111	10.3390/a13050111													
J								Automobile Fine-Grained Detection Algorithm Based on Multi-Improved YOLOv3 in Smart Streetlights	ALGORITHMS										smart streetlight; YOLOv3; multi-scale training; anchor clustering; label smoothing; mixup; IOU; GIOU; fine-grained classification of automobile		Upgrading ordinary streetlights to smart streetlights to help monitor traffic flow is a low-cost and pragmatic option for cities. Fine-grained classification of vehicles in the sight of smart streetlights is essential for intelligent transportation and smart cities. In order to improve the classification accuracy of distant cars, we propose a reformed YOLOv3 (You Only Look Once, version 3) algorithm to realize the detection of various types of automobiles, such as SUVs, sedans, taxis, commercial vehicles, small commercial vehicles, vans, buses, trucks and pickup trucks. Based on the dataset UA-DETRAC-LITE, manually labeled data is added to improve the data balance. First, data optimization for the vehicle target is performed to improve the generalization ability and position regression loss function of the model. The experimental results show that, within the range of 67 m, and through scale optimization (i.e., by introducing multi-scale training and anchor clustering), the classification accuracies of trucks and pickup trucks are raised by 26.98% and 16.54%, respectively, and the overall accuracy is increased by 8%. Secondly, label smoothing and mixup optimization is also performed to improve the generalization ability of the model. Compared with the original YOLO algorithm, the accuracy of the proposed algorithm is improved by 16.01%. By combining the optimization of the position regression loss function of GIOU (Generalized Intersection Over Union), the overall system accuracy can reach 92.7%, which improves the performance by 21.28% compared with the original YOLOv3 algorithm.																		1999-4893				MAY	2020	13	5							114	10.3390/a13050114													
J								A Novel Method for Inference of Chemical Compounds of Cycle Index Two with Desired Properties Based on Artificial Neural Networks and Integer Programming	ALGORITHMS										mixed integer linear programming; QSAR/QSPR; molecular design	DESIGN; GRAPH	Inference of chemical compounds with desired properties is important for drug design, chemo-informatics, and bioinformatics, to which various algorithmic and machine learning techniques have been applied. Recently, a novel method has been proposed for this inference problem using both artificial neural networks (ANN) and mixed integer linear programming (MILP). This method consists of the training phase and the inverse prediction phase. In the training phase, an ANN is trained so that the output of the ANN takes a value nearly equal to a given chemical property for each sample. In the inverse prediction phase, a chemical structure is inferred using MILP and enumeration so that the structure can have a desired output value for the trained ANN. However, the framework has been applied only to the case of acyclic and monocyclic chemical compounds so far. In this paper, we significantly extend the framework and present a new method for the inference problem for rank-2 chemical compounds (chemical graphs with cycle index 2). The results of computational experiments using such chemical properties as octanol/water partition coefficient, melting point, and boiling point suggest that the proposed method is much more useful than the previous method.																		1999-4893				MAY	2020	13	5							124	10.3390/a13050124													
J								Quality Control in Crowdsourcing Using Sequential Zero-Determinant Strategies	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Crowdsourcing; Games; Quality control; Task analysis; Process control; Gold; Economics; Crowdsourcing; quality control; sequential game; zero-determinant strategies	TIT-FOR-TAT	Quality control in crowdsourcing is challenging due to the heterogeneous nature of the workers. The state-of-the-art solutions attempt to address the issue from the technical perspective, which may be costly because they function as an additional procedure in crowdsourcing. In this paper, an economics based idea is adopted to embed quality control into the crowdsourcing process, where the requestor can take advantage of the market power to stimulate the workers for submitting high-quality jobs. Specifically, we employ two sequential games to model the interactions between the requestor and the workers, with one considering binary strategies while the other taking continuous strategies. Accordingly, two incentive algorithms for improving the job quality are proposed to tackle the sequential crowdsourcing dilemma problem. Both algorithms are based on a sequential zero-determinant (ZD) strategy modified from the classical ZD strategy. Such a revision not only provides a theoretical basis for designing our incentive algorithms, but also enlarges the application space of the classical ZD strategy itself. Our incentive algorithms have the following desired features: 1) they do not depend on any specific crowdsourcing scenario; 2) they leverage economics theory to train the workers to behave nicely for better job quality instead of filtering out the unprofessional workers; 3) no extra costs are incurred in a long run of crowdsourcing; and 4) fairness is realized as even the requestor (the ZD player), who dominates the game, cannot increase her utility by arbitrarily penalizing any innocent worker.																	1041-4347	1558-2191				MAY 1	2020	32	5					998	1009		10.1109/TKDE.2019.2896926													
J								Cluster Motion in a Two-Contour System with Priority Rule for Conflict Resolution	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL											SPECTRUM; FLOWS	A deterministic dynamical system representing the contour network is considered. The number of contours is two. At each contour there is a segment moving with a constant velocity which is called the cluster, because in the discrete variant of the system it corresponds to the cluster of particles, that is, to the group of particles occupying the adjacent cells and moving simultaneously. The lengths of contours and the lengths of clusters are prescribed. There is a common point named a node. Clusters cannot pass a node simultaneously. A cluster stops and waits for the node to empty if this cluster comes to the node at the instance when another cluster passes through the node. If clusters come to a node simultaneously, then precedence is given to the cluster considered the priority cluster (the priority rule of conflict resolution). The theorems on the average speed of cluster motion are proved taking delays in different types of the system's behavior into account. It is established that the average speed of motion of each cluster in the system is independent of the cluster position at the initial time instance in contrast to the analogous system with another rule of conflict resolution considered previously, where such dependence appears in the general case. The possible practical interpretation of the studied system is given. The presented system is referred to as the class of dynamical networks introduced and investigated by A.P. Buslaev. The results may be applied to solve questions on the automatization of motion of a continuous mass, simulate the motion of transport, and other areas.																	1064-2307	1555-6530				MAY	2020	59	3					311	321		10.1134/S1064230720030119													
J								Modeling and Analysis of Output Processes of Linear Continuous Stochastic Systems Based on Orthogonal Expansions of Random Functions	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL												The problem of modeling and analyzing output processes of linear continuous stochastic systems is considered and the method for its solution based on the spectral form of mathematical description of the control systems is proposed. The proposed approach provides an explicit representation of the output signal of the system in the form of functional series with random coefficients or in the form of a partial sum in the approximate solution, which distinguishes this method from other approaches when the result of the solution are the deterministic characteristics of the output signal: the first two moments or the probability density function. As an application, the problem of modeling the action of wind is considered using the Dryden shaping filter.																	1064-2307	1555-6530				MAY	2020	59	3					322	337		10.1134/S1064230720030156													
J								Optimization of Control with Anticipation and Delay for the Problem of the Shock Isolation of an Object on a Moving Base	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL												The problem of protecting an object on a moving base from transient shock impacts (disturbances) using an active shock isolator is considered. The maximum displacement of the object relative to the base is selected as the minimized quality criterion. Based on the anticipating optimal control obtained for a disturbance in the form of an instantaneous impact, a guaranteeing control is constructed with optimization of the moment the control starting instant, which can be chosen with anticipation or with a delay. The proposed optimization is compared in terms of the performance index with the optimization with respect to the anticipation time only and with the case where both the anticipation time and the control starting instant are optimized.																	1064-2307	1555-6530				MAY	2020	59	3					338	346		10.1134/S1064230720030077													
J								Analytical Quasi-Optimal Solution of the Slew Problem for an Axially Symmetric Rigid Body with a Combined Performance Index	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL											SPACECRAFT	The results of the analytical and numerical solution of the problem of controlling the rotational motion of an axially symmetric rigid body with a combined performance index of the control process are presented using quaternions. The performance index includes the duration of the control, impulse of the squared angular momentum magnitude, and impulse of the magnitude of the control torque applied to the body. The control must take an axisymmetric rigid body from a state of rest to another state of rest.																	1064-2307	1555-6530				MAY	2020	59	3					347	357		10.1134/S1064230720030107													
J								Management of a Dynamic Infrastructure of Complex Systems Under Conditions of Directed Cyber Attacks	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL												The problem of managing the dynamic infrastructure of complex systems in the context of targeted cyber attacks is considered. An approach to management is proposed, consisting of finding a self-regulation scenario in the minimum time that eliminates the destructive impact by finding an alternative route for implementing the objective function. It is proposed to characterize the process of self-regulation evaluating the system's resistance to cyber attacks. Approaches to assessing the stability for systems with centralized and decentralized control are proposed.																	1064-2307	1555-6530				MAY	2020	59	3					358	370		10.1134/S1064230720020124													
J								Suppression of Oscillations of Thin Plate by Bounded Control Acting to the Boundary	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL											CONTROLLABILITY	The problem of the exact bounded control of transverse vibrations of a thin plate is considered. Control actions are applied to the boundary of the plate, which fills a certain bounded domain on the plane. The purpose of the control is to completely stop oscillations in a finite time period.																	1064-2307	1555-6530				MAY	2020	59	3					371	380		10.1134/S1064230720030144													
J								Operations over k-Homogeneous Hypergraphs and Their Vectors of the Degrees of the Vertices	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL											FIXED PARAMETERS; NETWORK MODELS	The paper considers operations onk-homogeneous hypergraphs and finding the vectors of the degrees of the vertices of the result of the operation without constructing the hypergraphs' results themselves. It is proposed to express vectors from certain operations in terms of a vector from the intersection of two homogeneous hypergraphs. This will speed up the calculations and simplify the construction of the corresponding vectors.																	1064-2307	1555-6530				MAY	2020	59	3					381	386		10.1134/S1064230720030041													
J								Analysis of Two-Layer Resource Supply Flow Networks	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL											ENERGY SYSTEM CONTROL; FUEL	The problem of controlling resource flows in two-level networks with a tree structure simulated by a simplified radial structure at each level is considered. A resource flow vector to end consumers is taken as the maximized functional characteristic. Two flow control strategies are analyzed to obtain a Pareto optimal solution based on a weighted leximin rule. The first strategy, called the equal-share resource allocation, is based on the idea of equalizing the ratio of the delivered resource volume to the maximum possible resource volume for the given node for the given network capacity. The second strategy, which ensures that resources are allocated equally, is based on the idea of equalizing the ratio of the delivered resource volume to the required resource volume. In the case of unknown requirements, it is proposed to focus on the strategy that ensures equal allocation. Both strategies are described in two variants: optimization at the main level with the further additional optimization in each allocation network and direct optimization without an intermediate level. It is shown that the solutions are different. Their properties are studied. The possibility of combining different rules is investigated. A model example is presented.																	1064-2307	1555-6530				MAY	2020	59	3					387	399		10.1134/S1064230720030089													
J								Effect of Virtual Machine Deployment Policies on the Efficiency Data Processing Centers	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL											RESOURCE-ALLOCATION; ALGORITHM	The use of deployment policies with the development of virtual machines (VMs) allows improving the reliability of applications, reducing the latency in interactions between VMs, and simplifying the administration at data processing centers. Setting VM deployment policies places additional requirements on physical resources. This can result in reduced physical resource utilization and a lower percentage of deployed requests. The article proposes an algorithm for mapping requests (for creating virtual resources) for data centers' (DCs) physical resources with the optional consideration of VM deployment policies and presents the results of an experimental study of the effect of deployment policies on the load on the physical resources and the percentage of deployed requests.																	1064-2307	1555-6530				MAY	2020	59	3					400	405		10.1134/S1064230720030168													
J								Scheme-Parametric Optimization by the Criterion of Electric Consumption of Autonomous One-Rock Electrohydraulic Steering Actuators of Space Rocket Packs	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL												The paper presents the results of scheme-parametric optimization by the criterion of electric consumption of autonomous one-rock electrohydraulic steering actuators (SAs) with a negative overlap of throttle ports of their electrohydraulic amplifiers (EHAs) by spool plungers. The optimization rests on the results of investigating the effects of structural and regulatory parameters of EHAs on static characteristics of SAs and iterative methods of static analysis of SAs. It is substantiated that as the basic criterion for optimizing such SAs, we can take the parameter of their minimum electric consumption in providing all the requirements for static characteristics imposed on these SAs. As a result of the calculations of the static characteristics of SAs, it is shown that the final variants of SAs of both schemes have very close values of electric consumption. Therefore, for optimizing it is necessary to use additional nonformalized properties of the variants under consideration. As a result of the analysis of static characteristics, it is established that the variant of a SA with a four-throttle EHA does not possess static stability; this is a destabilizing factor, which can cause self-oscillations in a steering servo. The best variant is a SA with a two-throttle EHA.																	1064-2307	1555-6530				MAY	2020	59	3					406	414		10.1134/S1064230720030028													
J								Training a Classifier by Descriptors in the Space of the Radon Transform	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL											ORIENTED GRADIENTS; OPTIMIZATION; HISTOGRAMS; FEATURES; IMAGE; MODEL	The problem of detecting objects in images is solved by training the classifier by the descriptors constructed based on the local Radon transform of the gradient field of the image. The space of the Radon transform is considered as the Hough space accumulator in which projections are constructed. The set of local projections forms a descriptor of the region related to the object that is a generalization of the known histogram of oriented gradients (HOG) descriptor. The issues of the effect of the approximation of the Radon transform contribution function, the form of local normalization, and the number of directions in the projection histograms on the results of detecting pedestrians are investigated. The results produced by the proposed descriptor are compared with the results obtained using the HOG descriptor and convolutional neural networks (CNN) based on the ResNext and MobileNet architectures on the INRIA and CityScapes databases.																	1064-2307	1555-6530				MAY	2020	59	3					415	429		10.1134/S1064230720030053													
J								Determining the Orbit Onboard a Space Vehicle	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL												Methods and algorithms for determining the orbit of a space vehicle onboard using the measurements of global navigation satellite (NS) systems are considered. Methods and algorithms for obtaining radio navigation measurements in the case of the highly dynamic orbital motion are described. The celestial mechanics interpretation of measurements is given. Algorithms for the three-phase processing of trajectory measurements that improve the reliability and accuracy of determining the orbit are proposed. A mathematical model of a simulator of the satellite navigation signals is developed. The computational results are presented.																	1064-2307	1555-6530				MAY	2020	59	3					430	450		10.1134/S1064230720020112													
J								Application of Generalized Butterworth Polynomials for Stabilization of the Equilibrium Position of a Space Station	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL											COMMUNICATION SATELLITE; POLE-PLACEMENT; ATTITUDE; MOTION; ANGLE	The problem of searching for and holding the International Space Station in a position of dynamic equilibrium is considered. The equilibrium orientation of the International Space Station (ISS) is determined by the superposition of gravitational, gyroscopic, and aerodynamic moments acting on it. The system of powered gyroscopes installed on the space station plays the role of effectors in this task. Using this example as an example, we consider the possibility of applying the method of the sequential closure of the modes of motion to synthesize the multi-input multi-output (MIMO) control law, a multidimensional multiply connected dynamic system. It is proposed to use the generalized Butterworth polynomial as the reference polynomial determining the location of the poles of the closed system. Using the mathematical modeling of the control system in the search mode and maintaining the dynamic equilibrium, the advantage of using the generalized Butterworth polynomials in comparison with the classical Butterworth polynomials is demonstrated.																	1064-2307	1555-6530				MAY	2020	59	3					451	465		10.1134/S106423072003003X													
J								Stabilization of the Solar Orientation Mode of an Artificial Satellite of the Earth Without Accumulation of the Angular Momentum of the Gyro System	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL											MICROACCELERATIONS	The motion of an artificial Earth satellite in different variants of the solar orientation mode in the low Earth orbit is investigated. The satellite is close in shape to a cylinder with two solar arrays. Nonrotating solar arrays are located symmetrically relative to it along the satellite's longitudinal axis. In the solar mode normal to the plane of the satellite's solar arrays invariably directed toward the Sun, the longitudinal axis lies near the plane of the orbit and the absolute angular velocity of the satellite is very small. A gyro system (a set of reaction wheels or gyrodines) is used as the decision-making centers of the satellite's control system. Two versions of the law controlling the angular momentum of the gyro system are considered. The first variant provides only the attenuation of the disturbed motion of the satellite in the vicinity of the rest position at the required speed. The second variant further limits the growth of the accumulated angular momentum of the gyro system by controlling the angle of rotation of the satellite around the normal to the plane of the solar arrays.																	1064-2307	1555-6530				MAY	2020	59	3					466	478		10.1134/S1064230720030065													
J								Using evolutionary algorithms to select text features for mining design rationale	AI EDAM-ARTIFICIAL INTELLIGENCE FOR ENGINEERING DESIGN ANALYSIS AND MANUFACTURING										Ant colony optimization; design rationale; feature selection; genetic algorithms; text mining	COLONY; OPTIMIZATION	At its heart, design is a decision-making process. These decisions, and the reasons for making them, comprise the design rationale (DR) for the designed artifact. If available, DR provides a comprehensive record of the reasoning behind the decisions made during the design. Unfortunately, while this information is potentially quite valuable, it is usually not explicitly captured. Instead, it is often buried in other design and development artifacts. In this paper, we study how to identify rationale from text documents, specifically software bug reports and design discussion transcripts. The method we examined is statistical text mining where a model is built to use document features to classify sentences. Choosing which features are most likely to be good predictors is important. We studied two evolutionary algorithms to optimize feature selection - ant colony optimization and genetic algorithms. We found that for many types of rationale, models built with an optimized feature set outperformed those built using all the document features.																	0890-0604	1469-1760				MAY	2020	34	2			SI		132	146	PII S0890060420000037	10.1017/S0890060420000037													
J								Design characteristics and aesthetics in evolutionary design of architectural forms directed by fuzzy evaluation	AI EDAM-ARTIFICIAL INTELLIGENCE FOR ENGINEERING DESIGN ANALYSIS AND MANUFACTURING										Aesthetic measure; architectural form; design characteristics; evolutionary design; graph representation		This paper deals with design characteristics-oriented approach to architectural design based on the combination of three methods - recognition, generation, and evaluation. Design characteristics are understood as a set of specific features which constitute a discriminant of a class of architectural forms. The Biederman recognition-by-components theory is used to recognize the design structure. An evolutionary algorithm, which serves as a generative tool, is driven by the fuzzy evaluation based on Birkhoff's aesthetic measure. Phenotypes of architectural objects are seen as configurations of Biederman's basic components essential for visual perception. Genotypes of these objects are represented by graphs with bonds, where nodes represent object components, node bonds represent component surfaces, while graph edges represent relations between surfaces. Graph evolutionary operators, that is, crossover and mutation, are defined in such a way that they preserve characteristic features seen as design requirements specified for designed objects. The fitness function is determined by the fuzzy evaluation of designs based on Birkhoff's aesthetic measure for polygons adapted for three-dimensional solids. The approach is illustrated by examples of designing objects with the use of a fuzzy evaluation mechanism, which takes into account both aesthetic criteria and the degree to which design requirements corresponding to object characteristic features are satisfied.																	0890-0604	1469-1760				MAY	2020	34	2			SI		147	159	PII S0890060420000153	10.1017/S0890060420000153													
J								Enabling parametric design space exploration by non-designers	AI EDAM-ARTIFICIAL INTELLIGENCE FOR ENGINEERING DESIGN ANALYSIS AND MANUFACTURING										Barycentric interpolation; design space exploration; mass customization; parametric modeling; user testing	CUSTOMIZATION; LIMITS	In mass customization, software configurators enable novice end-users to design customized products and services according to their needs and preferences. However, traditional configurators hardly provide an engaging experience while avoiding the burden of choice. We propose a Design Participation Model to facilitate navigating the design space, based on two modules. Modeler enables designers to create customizable designs as parametric models, and Navigator subsequently permits novice end-users to explore these designs. While most parametric designs support direct manipulation of low-level features, we propose interpolation features to give customers more flexibility. In this paper, we focus on the implementation of such interpolation features into Navigator and its user interface. To assess our approach, we designed and performed user experiments to test and compare Modeler and Navigator, thus providing insights for further developments of our approach. Our results suggest that barycentric interpolation between qualitative parameters provides a more easily understandable interface that empowers novice customers to explore the design space expeditiously.																	0890-0604	1469-1760				MAY	2020	34	2			SI		160	175		10.1017/S0890060420000177													
J								Using agent-based simulation for public space design based on the Shanghai Bund waterfront crowd disaster	AI EDAM-ARTIFICIAL INTELLIGENCE FOR ENGINEERING DESIGN ANALYSIS AND MANUFACTURING										Agent-based simulation; computer-aided design; crowd disaster; Shanghai Bund waterfront; urban public space		With growing city density and mass gatherings held all over the world in urban spaces, crowd disasters have been happening each year. In considering the avoidance of crowd disasters and the reduction of fatalities, it is important to analyze the efficient spatial layout of the public space in situations of high crowd density. Compared with traditional empirical design methods, computational approaches have better abilities for quantitative analysis and are gradually being adopted in the planning and management of the urban public space. In this paper, we investigated the official documents, publicly available videos, and materials of the Shanghai waterfront crowd disaster which happened on December 31, 2014. Based on the investigation, a detailed site survey was conducted and pedestrian flow data were acquired. To test the influence of different spatial layouts, an agent-based simulator is built, following the ASPFver4.0 (Agent Simulator of Pedestrian Flow) pedestrian walking rules. With the surveyed pedestrian flow data, the original spatial layout of the Shanghai Bund waterfront together with five other comparison scenarios are tested, including both space design and crowd management improvements. In the simulation results, the efficiencies of different space design and crowd management solutions are compared. The results show that even simple crowd control measures such as capacity reserve and more proper route planning will allow for a positive improvement in crowd safety. The results also compare the efficiency of different spatial operations and give general suggestions to the problems urban public space designers should consider in high-density environments.																	0890-0604	1469-1760				MAY	2020	34	2			SI		176	190	PII S0890060420000049	10.1017/S0890060420000049													
J								A shape grammar for the building-type definition of the ancient Greek and Roman library and the evaluation of library plans	AI EDAM-ARTIFICIAL INTELLIGENCE FOR ENGINEERING DESIGN ANALYSIS AND MANUFACTURING										Ancient Greek libraries; ancient library plans; frequency analysis; parametric shape grammars; Roman libraries		This paper presents in detail a parametric shape grammar for the generation of ancient Greek and Roman library plans. Ancient Libraries showcase a significant variability of scale, monumentality, and design with the use of different architectural components, and researchers cannot identify them as easily as other building types. The grammar is in 2D and captures all the variability of libraries from the simpler forms of the Hellenistic period to the more monumental of the Roman period, creating a vast range of designs. The paper presents the grammar-generated plans of the known libraries based on the evidence in the archaeological record. A frequency analysis of the occurrence of the rules in the derivations ranks the different characteristics of libraries and the architectural forms with which they occur in the corpus, to determine the probability of specific rules and to interpret them as mandatory or optional. The contribution of this work is that the shape grammar summarizes the design principles of ancient libraries visually and helps establish the building type of the ancient library. The frequency analysis assists tremendously in the reconstruction of fragmentarily preserved ancient libraries by providing a system in the reconstruction of the missing parts, when ambiguity is involved.																	0890-0604	1469-1760				MAY	2020	34	2			SI		191	206	PII S0890060420000189	10.1017/S0890060420000189													
J								Reinforcement learning-based collision avoidance: impact of reward function and knowledge transfer	AI EDAM-ARTIFICIAL INTELLIGENCE FOR ENGINEERING DESIGN ANALYSIS AND MANUFACTURING										Agent-based systems; autonomous vehicle; collision avoidance; deep reinforcement learning; machine learning	TIME OBSTACLE AVOIDANCE; GAME; GO	Collision avoidance for robots and vehicles in unpredictable environments is a challenging task. Various control strategies have been developed for the agent (i.e., robots or vehicles) to sense the environment, assess the situation, and select the optimal actions to avoid collision and accomplish its mission. In our research on autonomous ships, we take a machine learning approach to collision avoidance. The lack of available ship steering data of human ship masters has made it necessary to acquire collision avoidance knowledge through reinforcement learning (RL). Given that the learned neural network tends to be a black box, it is desirable that a method is available which can be used to design an agent's behavior so that the desired knowledge can be captured. Furthermore, RL with complex tasks can be either time consuming or unfeasible. A multi-stage learning method is needed in which agents can learn from simple tasks and then transfer their learned knowledge to closely related but more complex tasks. In this paper, we explore the ways of designing agent behaviors through tuning reward functions and devise a transfer RL method for multi-stage knowledge acquisition. The computer simulation-based agent training results have shown that it is important to understand the roles of each component in a reward function and the various design parameters in transfer RL. The settings of these parameters are all dependent on the complexity of the tasks and the similarities between them.																	0890-0604	1469-1760				MAY	2020	34	2			SI		207	222	PII S0890060420000141	10.1017/S0890060420000141													
J								Framework for metaphor-based spatial configuration design: a case study of Japanese rock gardens	AI EDAM-ARTIFICIAL INTELLIGENCE FOR ENGINEERING DESIGN ANALYSIS AND MANUFACTURING										Computational design; Japanese rock gardens; knowledge representation; spatial configuration	AFFORDANCES	Metaphors are powerful tools for design, enabling designers to encapsulate sets of properties and relations as short verbal descriptions. This paper aims to clarify how simple spatial configurations may emerge from concise metaphoric descriptions at the conceptual design phase. To this aim, we propose a framework for a metaphor-based design process. As a basis for the framework, we introduce the concept of "complementary visual potential" - a property which ties the spatial configuration of the objects in the composition with their metaphoric roles. The framework is developed by studying the practice of metaphor-based spatial configuration design in Japanese rock gardens. Accordingly, it is implemented and tested in this context by attempting to generate alternative designs for an existing rock composition in the famous garden of Ryoan-ji. This is followed by a discussion of its possible implications and potential for generalization to other areas of design.																	0890-0604	1469-1760				MAY	2020	34	2			SI		223	232	PII S0890060419000295	10.1017/S0890060419000295													
J								Category, process, and recommendation of design in an interactive evolutionary computation interior design experiment: a data-driven study	AI EDAM-ARTIFICIAL INTELLIGENCE FOR ENGINEERING DESIGN ANALYSIS AND MANUFACTURING										Clustering; color combination; design experiment; interactive evolutionary computation; recommendation system	NEXT-GENERATION; SYSTEMS	Design is a complicated and sophisticated process with numerous existing theories trying to describe it. To verify theories and quantitatively describe the design process, design experiment, and data analysis are crucial and inevitable. However, applying data analysis in the design experiment is tricky and design data is not fully utilized in many aspects. To explore the potential of design experiment data, this paper introduces data-driven research based on an interior design experiment, aiming to reveal the category and process of design by conducting data analysis, visualization, and recommendation. We introduce an interactive evolutionary computation (IEC) design experiment that deals with a simplified interior design task and has already been tested on 230 subjects. Using the data gathered during the experiment, we conduct data analysis and visualization involving methods including Holistic color interval and K-means clustering to show categories and processes in design. Additionally, we train a content-based recommendation system with experiment data to capture user preference and make the IEC system more efficient and intelligent. The analysis and visualization show clear design categories and capture an evident trend towards the final design outcome. The application of the recommendation system brings a prominent improvement to the IEC system. This research shows the great potential of the various data-driven methods in design research.																	0890-0604	1469-1760				MAY	2020	34	2			SI		233	247	PII S0890060420000050	10.1017/S0890060420000050													
J								Demonstration, extension, and refinement of the re-proposed notion of design abduction	AI EDAM-ARTIFICIAL INTELLIGENCE FOR ENGINEERING DESIGN ANALYSIS AND MANUFACTURING										Abduction; design reasoning; dynamic abduction; model-based abduction; strategic abduction; Wright brothers	ENGINEERING DESIGN; THINKING	The original ideas on design abduction, inspired by treatments in philosophy of science, had a narrow conception on how novelty emerges in design, when looked at in terms of logic. The authors have previously presented a re-proposed notion of abduction in design, taking the differences between science and design into account. Now, in this article, the invention of the airplane by the Wright brothers is analyzed as a retrospective case study. Key parts of the re-proposed notion of design abduction are demonstrated, and two new types of design abduction are identified, namely strategic abduction and dynamic abduction. Perhaps even more importantly, a new hypothesis on the cognitive basis of design abduction is reached. While the importance of model-based abduction (and reasoning) is confirmed, the case also pinpoints the central role of verbalization and discussion in supporting design reasoning in general and especially abduction. All in all, it seems that an improved understanding of design abduction and its cognitive basis would be instrumental in promoting more effective and efficient designing.																	0890-0604	1469-1760				MAY	2020	34	2			SI		286	297	PII S0890060420000165	10.1017/S0890060420000165													
J								Cooperative Mechanism of SME Growth in the Mesoscopic Structure With Strategic and Nonstrategic Partners	IEEE INTELLIGENT SYSTEMS										Intelligent systems; Finance; Symbiosis; Mathematics; Economics; Industries; small and micro enterprise; tech-economic paradigm; mesoscopic structure; cooperative behavior	CREDIT RISK	As big data techniques are being improved to follow up the enterprise development, it has been widely observed that the small and microenterprises (SMEs) do not always work independently, but operate in groups and form a symbiotic mesoscopic structure in the network environment. In this study, we propose a coupled system subjected to the ratchet utility potential, to mimic the cooperative growth mechanism of the SMEs with strategic and nonstrategic partners under the tech-economic paradigm. By decoupling the system, we approximately obtain the average probability current to measure the mesoscopic performance in terms of growth ability and cooperation efficiency. Further, we systematically study the performance dependence on system parameters, and discuss the sensitivity to parameter changes. The results provide the latent support for regulating the growth behaviors, and further leads us to create useful decision rules in the SME management practices.																	1541-1672	1941-1294				MAY-JUN	2020	35	3					7	17		10.1109/MIS.2019.2935965													
J								Formal Analysis of Smart Contract Based on Colored Petri Nets	IEEE INTELLIGENT SYSTEMS										Contracts; Tools; Syntactics; Security; Petri nets; Virtual machining; Analytical models; Smart Contract; Formal Analysis; Colored Petri Net; Model Checking		Smart contracts increasingly cause attention for its ability to widen blockchain's application scope. However, the security of contracts is vital to its wide deployment. In this article, we propose a multilevel smart contract modeling solution to analyze the security of contract. We improve the program logic rules for bytecode and apply the Hoare condition to create a Colored Petri Net (CPN) model. The model detection method provided by the CPN tools can show the full-state space and the wrong execution path, which help us analyze the security of the contract from several perspectives. The example shows that the counter-example path given by the contract model is accord with our expected results based on code analysis, proving the correctness of the solution. In addition, we design a highly automated modeling method, introducing custom call libraries and a path derivation algorithm based on backtracking, which improves the efficiency and pertinence of the dynamic simulation of CPN models.																	1541-1672	1941-1294				MAY-JUN	2020	35	3					19	29		10.1109/MIS.2020.2977594													
J								Blockchain-Based Fair and Secure Electronic Double Auction Protocol	IEEE INTELLIGENT SYSTEMS										Protocols; Consumer electronics; Cryptography; Contracts; Companies; Blockchain; distributed systems; secure multi-party computation; distributed commercial transactions		Double auction is an auction in which multiple buyers and sellers looking for a price where supply and demand balance. Since the electronic double auction based on secure multiparty computation (DABSMPC) cannot guarantee its fairness, we propose a blockchain-based fair and secure electronic double auction protocol (BFSDA). BFSDA modifies the data input and distribution mechanism of participants in DABSMPC, which improves the security of the protocol. Then, the BFSDA introduces and improves the blockchain-based fair and secure multiparty computation protocol (BFSMPC) to ensure fairness while increasing the success rate of secret recovery. In addition, BFSDA uses a fairer and more efficient protocol for secure two-party comparing to obtain the final marketing clearing price. The schema analysis result of BFSDA shows that: first, the private input data will not be revealed as long as data owner is not compromised, second, honest participants can get the result or economic compensation, and third, participants only need to pay the deposit once and a large amount of complicated verification operations are carried out off the chain, which ensures the efficiency of the protocol.																	1541-1672	1941-1294				MAY-JUN	2020	35	3					31	40		10.1109/MIS.2020.2977896													
J								Domain Adaptation Learning Based on Structural Similarity Weighted Mean Discrepancy for Credit Risk Classification	IEEE INTELLIGENT SYSTEMS										Feature extraction; Kernel; Data models; Sparse matrices; Dimensionality reduction; Training; Credit Risk; Domain Adaptation Learning; Maximum Mean Discrepancy; Low Rank Representation	KERNEL	Domain adaptation learning is an effective method for leveraging knowledge from the source domain with labels to a target domain without any labels. However, most prior methods have neglected the contribution of each sample to the integral measure when adaptively matching either marginal distribution, conditional distribution, or both between domains. In this article, an improved algorithm based on mean discrepancy embedding with structural similarity is proposed, which aims at the contribution of each sample to the integral measure on the performance of target domain learning model by using labeled source samples and unlabeled target samples. The discrepancy between both marginal and conditional distribution are minimized with dimensionality reduction procedure to feature extraction with structural similarity weights for all samples from the source and target domains. The results of empirical analysis demonstrate that the proposed method has better performance over several state-of-the-art methods in credit risk classification.`																	1541-1672	1941-1294				MAY-JUN	2020	35	3					41	51		10.1109/MIS.2020.2972791													
J								XGBoost Model and Its Application to Personal Credit Evaluation	IEEE INTELLIGENT SYSTEMS										Predictive models; Decision trees; Data models; Analytical models; Prediction algorithms; Machine learning algorithms; Context modeling; XGB; Ensemble Learning; Feature Engineering; Credit Evaluation		This article investigates the application of the eXtreme Gradient Boosting (XGB) method to the credit evaluation problem based on big data. We first study the theoretical modeling of the credit classification problem using XGB algorithm, and then we apply the XGB model to the personal loan scenario based on the open data set from Lending Club Platform in USA. The empirical study shows that the XGB model has obvious advantages in both feature selection and classification performance compared to the logistic regression and the other three tree-based models.																	1541-1672	1941-1294				MAY-JUN	2020	35	3					52	61		10.1109/MIS.2020.2972533													
J								An AI Model for Oil Volatility Forecasting	IEEE INTELLIGENT SYSTEMS										Oils; Forecasting; Predictive models; Indexes; Intelligent systems; Genetic algorithms; Oil volatility; forecasting; genetic algorithm; AI; classifier system	CRUDE-OIL	By introducing a genetic algorithm learning with a classifier system, we construct an AI model for oil volatility forecasting on the basis of Internal Information and External Information. The model provides decision support for mark-to-market portfolio and risk management by forecasting whether 1-day-ahead volatility is above a given threshold. Moreover, we explore the dynamic influencing mechanism of different types of information through information usage frequency in the learning process. In particular, we find that the jump component of oil realized volatility is efficient only in bull market, and currency information contributes most rather than oil information in bear market. Therefore, this article provides an AI method to forecast oil volatility as well as to improve the information structure of forecasting models.																	1541-1672	1941-1294				MAY-JUN	2020	35	3					62	70		10.1109/MIS.2020.2972544													
J								Three Algorithms for Solving High-Dimensional Fully Coupled FBSDEs Through Deep Learning	IEEE INTELLIGENT SYSTEMS										Stochastic processes; Optimal control; Neural networks; Feedback control; Intelligent systems; Deep learning; Differential equations; deep learning; fully-coupled FBSDEs; high-dimensional equation; stochastic control	STOCHASTIC DIFFERENTIAL-EQUATIONS; MULTISTEP SCHEMES; BACKWARD	Recently, the deep learning method has been used for solving forward-backward stochastic differential equations (FBSDEs) and parabolic partial differential equations, as it has good accuracy and performance for high-dimensional problems. In this article, we mainly solve fully coupled FBSDEs through deep learning and provide three algorithms, and the numerical results show remarkable performance, especially for high-dimensional cases.																	1541-1672	1941-1294				MAY-JUN	2020	35	3					71	84		10.1109/MIS.2020.2971597													
J								A Data-Analytics Approach for Risk Evaluation in Peer-to-Peer Lending Platforms	IEEE INTELLIGENT SYSTEMS										Internet; Industries; Finance; Risk management; Peer-to-peer computing; Computer crashes; Peer-to-Peer lending; data-driven analytics; risk evaluation; crash		The goal of this article is to investigate the roles of individual behavior characteristics and Internet finance industry risk in the light of bank run theory for P2P. We know that risk evaluation is clearly important for peer-to-peer (P2P) lending platforms in China, as during the last two years, the industry has experienced thousands of platform crashes. Traditional approaches to evaluate enterprise risk are increasingly ineffective in this industry, due to the difficulty of assessing the real information. In addition, the Internet business model makes it possible to record new kinds of information. By applying a data-driven analytics method, we build an intelligent risk evaluation model for P2P platforms that have comparable targeting platforms. The case study shows that our risk evaluation method can generate early warning signals regarding platform or industry risk, which is able to provide effective supporting for P2P business in practice.																	1541-1672	1941-1294				MAY-JUN	2020	35	3					85	94		10.1109/MIS.2020.2971946													
J								Multiangle P2P Borrower Characterization Analytics by Attributes Partition Considering Business Process	IEEE INTELLIGENT SYSTEMS										Data mining; Risk management; Global Positioning System; Intelligent systems; Internet; Loans and mortgages; Internet; Mining methods and algorithms; Feature evaluation and selection; Data mining	RISK-ASSESSMENT; CREDIT RISK	In the research of P2P lending data, the study of borrower characteristics is of great value for the establishment of target customers and risk management. Because of high dimensionality, mixed attributes, different importance, and different generation time of information, P2P lending data often leads to the mining results unable to reflect the important borrower characteristics that affect the approval results and the approval loan amount. In this article, we are the first to propose the attributes partition of lending data considering the business process to classify variables into different types. Furthermore, we propose a multiangle data mining method for lending data by attributes partition considering the business process to discover the characteristics of P2P borrowers from multiple perspectives. Experimental results on the real dataset demonstrate that the method depicts the important characteristics of borrowers that affect the approval results and the loan amount, makes the research on P2P borrower characteristics more comprehensive and specific, and provides new ideas for the research on high-dimensional lending data.																	1541-1672	1941-1294				MAY-JUN	2020	35	3					96	105		10.1109/MIS.2020.2986973													
J								Cross-Lingual Sentiment Quantification	IEEE INTELLIGENT SYSTEMS												Sentiment Quantification is the task of estimating the relative frequency of sentiment-related classes-such as Positive and Negative-in a set of unlabeled documents. It is an important topic in sentiment analysis, as the study of sentiment-related quantities and trends across a population is often of higher interest than the analysis of individual instances. In this article, we propose a method for cross-lingual sentiment quantification, the task of performing sentiment quantification when training documents are available for a source language S, but not for the target language T, for which sentiment quantification needs to be performed. Cross-lingual sentiment quantification (and cross-lingual text quantification in general) has never been discussed before in the literature; we establish baseline results for the binary case by combining state-of-the-art quantification methods with methods capable of generating cross-lingual vectorial representations of the source and target documents involved. Experiments on publicly available datasets for crosslingual sentiment classification show that the presented method performs cross-lingual sentiment quantification with high accuracy.																	1541-1672	1941-1294				MAY-JUN	2020	35	3					106	113		10.1109/MIS.2020.2979203													
J								A New Index for TOPSIS based on Relative Distance to Best and Worst Points	INTERNATIONAL JOURNAL OF INFORMATION TECHNOLOGY & DECISION MAKING										Multiple criteria decision making (MCDM); TOPSIS; relative distance; best point; worst point	MULTIATTRIBUTE DECISION-MAKING; FUZZY TOPSIS; RANKING; MANAGEMENT; EFFICIENCY; EXTENSION; NETWORKS; MCDM; AHP	The technique for order performance by similarity to ideal solution (TOPSIS) is one of the most well-known methods in multiple criteria decision making (MCDM) problems. The classical TOPSIS method employs a similarity index to rank alternatives. However, the chosen alternative sometimes does not have the shortest distance to the positive ideal solution (PIS) and remotest distance from the negative ideal solution (NIS), simultaneously. Besides, in some cases, TOPSIS cannot assign a unique rank to alternatives. The purpose of this paper is to propose a new similarity TOPSIS index based on the relative distance to the best and worst points. In the proposed method, by treating the separations of an alternative from the PIS and the NIS as negative criterion and positive criterion, respectively, we reduce the original MCDM problem to a new one with two criteria. The proposed index, based on different weights, in optimistic, pessimistic, and apathetic cases, easily determines the score of each alternative. Finally, we illustrate the proposed index using four numerical examples. The results are compared with those published in the literature.																	0219-6220	1793-6845				MAY	2020	19	3					695	719		10.1142/S0219622020500145													
J								Multi-Scale Shapelets Discovery for Time-Series Classification	INTERNATIONAL JOURNAL OF INFORMATION TECHNOLOGY & DECISION MAKING										Time-series; shapelets; classification		Shapelets are subsequences of time-series that represent local patterns and can improve the accuracy and the interpretability of time-series classification. The major task of time-series classification using shapelets is to discover high quality shapelets. However, this is challenging since local patterns may have various scales/lengths rather than a unified scale. In this paper, we resolve this problem by discovering shapelets with multiple scales. We propose a novel Multi-Scale Shapelet Discovery (MSSD) algorithm to discover expressive multi-scale shapelets by extending initial single-scale shapelets (i.e., shapelets with a unified scale). MSSD adopts a bidirectional extension process and is robust to extend single-shapelets obtained by different methods. A supervised shapelet quality measurement is further developed to qualify the extension of shapelets. Comprehensive experiments conducted on 25 UCR time-series datasets show that multi-scale shapelets discovered by MSSD improve classification accuracy by around 10% (in average), compared with single-scale shapelets discovered by counterpart methods.																	0219-6220	1793-6845				MAY	2020	19	3					721	739		10.1142/S0219622020500133													
J								Sustainable Location Selection of Data Centers: Developing a Multi-Criteria Set-Covering Decision-Making Methodology	INTERNATIONAL JOURNAL OF INFORMATION TECHNOLOGY & DECISION MAKING										Data center; location selection; best-worst method (BWM); sustainability framework; linear set-covering model; conditional performance	SUPPLY CHAIN MANAGEMENT; OF-THE-ART; WORST METHOD; OIL; EFFICIENCY; PLACEMENT; FRAMEWORK; ISSUES	In this paper, a multi-criteria set-covering methodology is proposed to select suitable locations for a set of data centers. First, a framework of criteria, with social, economic and environmental dimensions, is presented. The framework is used to calculate the suitability of potential data center locations in Iran. To that end, a sample of specialists in Iran was asked to take part in an online questionnaire, based on best-worst method (BWM), to determine the weight of the criteria included in the proposed framework, after which a number of potential locations are evaluated on the basis of the criteria. The proposed model is evaluated under a number of settings. Using the proposed multi-criteria set-covering model, not only the utility of candidate places is evaluated by sustainability criteria but also all service applicants are covered by at least one data center with a specific coverage radius.																	0219-6220	1793-6845				MAY	2020	19	3					741	773		10.1142/S0219622020500157													
J								Novel Multiperspective Hiring Framework for the Selection of Software Programmer Applicants Based on AHP and Group TOPSIS Techniques	INTERNATIONAL JOURNAL OF INFORMATION TECHNOLOGY & DECISION MAKING										Multicriteria decision-making techniques; measurement criteria; MCDM; AHP; TOPSIS; software programmer	OF-THE-ART; MULTICRITERIA ANALYSIS; TRACKING CHANNELS; OPEN ISSUES; DECISION; BENCHMARKING; CRITERIA; MCDM; OPTIMIZATION; METHODOLOGY	The selection of software programmer applicants based on multiperspective evaluation criteria (grade point average (GPA) and soft skills of the applicants) is needed instead of an interview because an interview does not necessarily lead to hiring the best candidate amongst the applicants. The selection of a suitable software programmer is considered a challenging task owing to the following factors: (1) data variation, (2) multiple evaluation criteria and (3) criterion importance. A general framework for the selection of the best software programmer applicants is not available in the existing literature. The present study aims to propose a novel multiperspective hiring framework based on multicriteria analysis to select the best software programmer amongst several applicants. A decision matrix (DM) is constructed for the selection of the best programmer applicants according to multiple criteria, namely, structured programming, object-oriented programming, data structure, database system and courseware engineering. Each criterion includes two parameters, namely, GPA and soft skills, and these criteria cross over with programmer applicants as alternatives. The standard and expert opinion of the Software Engineering Body of Knowledge is used to distribute the criteria in the DM. The two commonly used techniques of multicriteria decision-making are analytic hierarchy process (AHP) for weighing the criteria and technique for order performance by similarity to ideal solution (TOPSIS) for ranking the alternatives (programmer applicants). The data used in this study include 60 software engineering students who graduated in 2016 from Universiti Pendidikan Sultan Idris. Results show that integrating multilayer analytic hierarchy process (MLAHP) and group TOPSIS are effective for solving applicant selection problems. Group TOPSIS uses different contexts - internal and external aggregation - and indicates similar results. Objective validation is used for the ranking of the results, which are equally divided into four parts. Furthermore, the applicants are systematically ranked. This study benefits application software, system software and computer programming tool companies by providing a method that improves software quality whilst reducing time and cost in the selection process.																	0219-6220	1793-6845				MAY	2020	19	3					775	847		10.1142/S0219622020500121													
J								A Lightweight Approach to Extract Interschema Properties from Structured, Semi-Structured and Unstructured Sources in a Big Data Scenario	INTERNATIONAL JOURNAL OF INFORMATION TECHNOLOGY & DECISION MAKING										Unstructured sources; interschema property derivation; structuring unstructured data; big data	METADATA QUALITY; DIGITAL REPOSITORIES; SIMILARITY; CLASSIFICATION; CONSTRUCTION; INTEGRATION; SYSTEM; MODEL; DIKE	The knowledge of interschema properties (e.g., synonymies, homonymies, hyponymies and subschema similarities) plays a key role for allowing decision-making in sources characterized by disparate formats. In the past, wide amount and variety of approaches to derive interschema properties from structured and semi-structured data have been proposed. However, currently, it is esteemed that more than 80% of data sources are unstructured. Furthermore, the number of sources generally involved in an interaction is much higher than in the past. As a consequence, the necessity arises of new approaches to address the interschema property derivation issue in this new scenario. In this paper, we aim at providing a contribution in this setting by proposing an approach capable of uniformly extracting interschema properties from a huge number of structured, semi-structured and unstructured sources.																	0219-6220	1793-6845				MAY	2020	19	3					849	889		10.1142/S0219622020500182													
J								A Concentration Ratio for Nonlinear Best Worst Method	INTERNATIONAL JOURNAL OF INFORMATION TECHNOLOGY & DECISION MAKING										Multi-criteria decision-making; best worst method (BWM); concentration; consistency	DECISION-MAKING; SUPPLIER SELECTION; CONCENTRATION PARAMETER; QUALITY ASSESSMENT; SUSTAINABILITY	Best Worst Method (BWM) is a multi-criteria decision-making method that is based on a structured pairwise comparison system. It uses two pairwise comparison vectors (best-to-others and others-to-worst) as input for an optimization model to get the optimal weights of the criteria (or alternatives). The original BWM involves a nonlinear model that sometimes results in multiple optimal weights meaning that the weight of each criterion is presented as an interval. The aim of this paper is to introduce a ratio, called concentration ratio, to check the concentration of the optimal intervals obtained from the nonlinear BWM. The relationship between the concentration ratio and the consistency ratio is investigated and it is found that the concentration ratio along with the consistency ratio of the model provides enhanced insights into the reliability and flexibility of the results of BWM.																	0219-6220	1793-6845				MAY	2020	19	3					891	907		10.1142/S0219622020500170													
J								A Novel Multi-Perspective Benchmarking Framework for Selecting Image Dehazing Intelligent Algorithms Based on BWM and Group VIKOR Techniques	INTERNATIONAL JOURNAL OF INFORMATION TECHNOLOGY & DECISION MAKING										Multi-criteria decision making; intelligent algorithm; image dehazing; benchmarking; BWM; VIKOR	FAST SINGLE-IMAGE; MULTICRITERIA ANALYSIS; QUALITY ASSESSMENT; TRACKING CHANNELS; HAZE; MCDM; OPTIMIZATION; METHODOLOGY; EFFICIENT; VIDEO	The increasing demand for image dehazing-based applications has raised the value of efficient evaluation and benchmarking for image dehazing algorithms. Several perspectives, such as inhomogeneous foggy, homogenous foggy, and dark foggy scenes, have been considered in multicriteria evaluation. The benchmarking for the selection of the best image dehazing intelligent algorithm based on multi-criteria perspectives is a challenging task owing to (a) multiple evaluation criteria, (b) criteria importance, (c) data variation, (d) criteria conflict, and (e) criteria tradeoff. A generally accepted framework for benchmarking image dehazing performance is unavailable in the existing literature. This study proposes a novel multi-perspective (i.e., an inhomogeneous foggy scene, a homogenous foggy scene, and a dark foggy scene) benchmarking framework for the selection of the best image dehazing intelligent algorithm based on multi-criteria analysis. Experiments were conducted in three stages. First was an evaluation experiment with five algorithms as part of matrix data. Second was a crossover between image dehazing intelligent algorithms and a set of target evaluation criteria to obtain matrix data. Third was the ranking of the image dehazing intelligent algorithms through integrated best-worst and VIseKriterijumska Optimizacija I Kompromisno Resenje methods. Individual and group decision-making contexts were applied to demonstrate the efficiency of the proposed framework. The mean was used to objectively validate the ranks given by group decision-making contexts. Checklist and benchmarking scenarios were provided to compare the proposed framework with an existing benchmark study. The proposed framework achieved a significant result in terms of selecting the best image dehazing algorithm.																	0219-6220	1793-6845				MAY	2020	19	3					909	957		10.1142/S0219622020500169													
J								An Approach to Software Maintenance: A Case Study in Small and Medium-Sized Businesses IT Organizations	INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING										Software maintenance; knowledge management; turnover	KNOWLEDGE MANAGEMENT; CHALLENGES; FRAMEWORK	Software maintenance is the task of modifying a running product previously delivered to the client, in order to correct defects, improve performance or adapt it to the environment. This task is a crucial activity for enterprises. Without it, existing systems would become rapidly out-of-date and inefficient. The purpose of this paper is to present a software maintenance approach used in small and medium-sized business (SMB) organizations in Brazil. Currently, these organizations represent 95.5% of the software companies in the country. The approach presented here indicates how SMB IT companies have improved their software maintenance processes. Multiple case studies were performed to validate this approach. The outcomes showed that strategies associated with managing users' knowledge and development/maintenance teams are relevant to increase the maintenance process effectiveness. This approach involves three aspects: users' knowledge management, maintenance team knowledge and the management and maintenance process. This improvement includes reducing time and also minimizing the number of tickets. The response time for tickets resolution to the end user has been reduced. In addition, IT organizations have minimized the effects associated with both staff and client turnovers.																	0218-1940	1793-6403				MAY	2020	30	5					603	630		10.1142/S0218194020500217													
J								Categorization of Multiple Documents Using Fuzzy Overlapping Clustering Based on Formal Concept Analysis	INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING										Overlapping clustering; Formal Concept Analysis; fuzzy logic		Most clustering algorithms build disjoint clusters. However, clusters might be overlapped because documents may belong to two or more categories in the real world. For example, a paper discussing the Apple Watch may be categorized into either 3C, Fashion, or even Clothing and Shoes. Therefore, overlapping clustering algorithms have been studied such that a resource can be assigned to one or more clusters. Formal Concept Analysis (FCA), which has many practical applications in information science, has been used in disjoin clustering, but has not been studied in overlapping clustering. To make overlapping clustering possible by using FCA, we propose an approach, including two types of transformation. From the experimental results, it shows that the proposed fuzzy overlapping clustering performed more efficiently than existing overlapping clustering methods. The positive results confirm the feasibility of the proposed scheme used in overlapping clustering. Also, it can be used in applications such as recommendation systems.																	0218-1940	1793-6403				MAY	2020	30	5					631	647		10.1142/S0218194020500229													
J								Deep Transfer Learning for Source Code Modeling	INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING										Transfer learning; deep neural language models; source code modeling; attention learning	NEURAL-NETWORKS; GENERATION	In recent years, deep learning models have shown great potential in source code modeling and analysis. Generally, deep learning-based approaches are problem-specific and data-hungry. A challenging issue of these approaches is that they require training from scratch for a different related problem. In this work, we propose a transfer learning-based approach that significantly improves the performance of deep learning-based source code models. In contrast to traditional learning paradigms, transfer learning can transfer the knowledge learned in solving one problem into another related problem. First, we present two recurrent neural network-based models RNN and GRU for the purpose of transfer learning in the domain of source code modeling. Next, via transfer learning, these pre-trained (RNN and GRU) models are used as feature extractors. Then, these extracted features are combined into attention learner for different downstream tasks. The attention learner leverages from the learned knowledge of pre-trained models and fine-tunes them for a specific downstream task. We evaluate the performance of the proposed approach with extensive experiments with the source code suggestion task. The results indicate that the proposed approach outperforms the state-of-the-art models in terms of accuracy, precision, recall and F-measure without training the models from scratch.																	0218-1940	1793-6403				MAY	2020	30	5					649	668		10.1142/S0218194020500230													
J								Concurrent Bug Finding Based on Bounded Model Checking	INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING										Software verification; automated bug finding; bounded model checking; parallel and concurrent approaches	SYMBOLIC EXECUTION; DEPTH-1ST SEARCH; PARALLEL; VERIFICATION	Automated and reliable software verification is of crucial importance for development of high-quality software. Formal methods can be used for finding different kinds of bugs without executing the software, for example, for finding possible run-time errors. The methods like model checking and symbolic execution offer very precise static analysis but on real world programs do not always scale well. One way to tackle the scalability problem is to apply new concurrent and sequential approaches to complex algorithms used in these kinds of software analysis. In this paper, we compare different variants of bounded model checking and propose two concurrent approaches: concurrency of intra-procedural analysis and concurrency of inter-procedural analysis. We implemented these approaches in a software verification tool LAV, a tool that is based on bounded model checking and symbolic execution. For assessing the improvements gained, we experimentally compared the concurrent approaches with the standard bounded model checking approach (where all correctness conditions are put into a single compound formula) and with a sequential approach (where correctness conditions are checked separately, one after the other). The results show that, in many cases, the proposed concurrent approaches give significant improvements.																	0218-1940	1793-6403				MAY	2020	30	5					669	694		10.1142/S0218194020500242													
J								Software Crucial Functions Ranking and Detection in Dynamic Execution Sequence Patterns	INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING										Sequence pattern; decision function; ranking; crucial functions	IDENTIFYING INFLUENTIAL NODES; CENTRALITY; NETWORKS; SPREADERS	Because of the sequence and number of calls of functions, software network cannot reflect the real execution of software. Thus, to detect crucial functions (DCF) based on software network is controversial. To address this issue, from the viewpoint of software dynamic execution, a novel approach to DCF is proposed in this paper. It firstly models, the dynamic execution process as an execution sequence by taking functions as nodes and tracing the stack changes occurring. Second, an algorithm for deleting repetitive patterns is designed to simplify execution sequence and construct software sequence pattern sets. Third, the crucial function detection algorithm is presented to identify the distribution law of the numbers of patterns at different levels and rank those functions so as to generate a decision-function-ranking-list (DFRL) by occurrence times. Finally, top-k discriminative functions in DFRL are chosen as crucial functions, and similarity the index of decision function sets is set up. Comparing with the results from Degree Centrality Ranking and Betweenness Centrality Ranking approaches, our approach can increase the node coverage to 80%, which is proven to be an effective and accurate one by combining advantages of the two classic algorithms in the experiments of different test cases on four open source software. The monitoring and protection on crucial functions can help increase the efficiency of software testing, strength software reliability and reduce software costs.																	0218-1940	1793-6403				MAY	2020	30	5					695	719		10.1142/S0218194020500254													
J								A Template-Based Sequential Algorithm for Online Clustering of Spikes in Extracellular Recordings	COGNITIVE COMPUTATION										Spike sorting; Extracellular recording; Clustering; Validation index	CLASSIFICATION; EFFICIENT; POTENTIALS	In order to discriminate different spikes in an extracellular recording, a multitude of successful spike sorting algorithms has been proposed up to now. However, new implantable neuroprosthetics containing a spike sorting block necessitate the use of a real-time and a preferably unsupervised method. The aim of this article is to propose a new unsupervised spike sorting algorithm which could work in real-time. As opposed to most traditional frameworks that consist of separate noise cancelation and feature extraction steps, here a sequential algorithm is proposed which makes use of noise statistics and uses data samples as features. For each detected spike, the difference between the detected spike and all the previously detected spike templates are calculated. If the output is a signal similar to noise, this indicates that the new spike is fired from a previously observed neuron. Two varieties of the general method are illustrated and a set of clustering indices which determine an optimal clustering is used to set the parameters. Clustering indices surpassed 0.90 (out of 1) for synthetic data with modest noise level. Experiments with our recorded signals showed satisfactory results in clustering and template identification. Spike sorting is an active field. A deficiency in conventional spike sorting algorithms is that most of them are either supervised or offline. Here, we present an online unsupervised algorithm which could be developed as a solution for current neuroprosthetics. Since the present method clustered real spikes data appropriately without a need for training data, the methodology could be adapted to be used in implantable devices.																	1866-9956	1866-9964				MAY	2020	12	3					542	552		10.1007/s12559-020-09711-x													
J								Quantum Aspects of High Dimensional Conceptual Space: a Model for Achieving Consciousness	COGNITIVE COMPUTATION										Cognition; Conceptual spaces; Consciousness; Formal concept analysis; Quantum theory; Uncertainty	COGNITION; BRAIN	Cognitive frameworks best represent cognitive information and knowledge. Several classical probability theory (CPT)-based cognitive frameworks were proposed in the literature. Recently, CPT has failed in explaining certain cognitive processes while quantum theories were successful in explaining the same. In this work, we integrate two cognitive frameworks namely conceptual spaces and 3-way formal concept analysis (3WFCA) to propose a high dimensional conceptual space (HDCS). Our new insights into the analysis of this proposal reveal its quantum characteristics. Among different cognitive processes that can be modelled, our interest is on phenomenal consciousness. Accordingly, we have proposed a formal method to achieve consciousness. We have also proposed an algorithm for the conceptual scaling of a cognitive scenario. HDCS represents a cognitive state using quality dimensions, attributes and their relations. Subsequently, the proposed model makes novel use of agent-environment interaction paradigm for guiding the interaction of HDCS with conceptually scaled cognitive scenario. Cognitive-state representation in HDCS is analogous to quantum state representation in a N-qubit system. Cognitive states are learnt through the parallel accumulation of evidences against all the attributes of multiple dimensions. The interaction between the HDCS and the scenario facilitates the identification and removal of uncertainties. The identified uncertainty coupled with its resolution time resembles Heisenberg-like uncertainty. The analogous of quantum two-slit self-interference would be comparison of evidence accumulation in HDCS with the preceding version of itself. Consequently, this research reveals that modelling consciousness requires a high dimensional space rather than set theoretic structures and support from quantum theories.																	1866-9956	1866-9964				MAY	2020	12	3					563	576		10.1007/s12559-020-09712-w													
J								Knowledge Fusion via Joint Tensor and Matrix Factorization	COGNITIVE COMPUTATION										Knowledge fusion; Connected components; Entity overlap; Tensor factorization; Word similarities	DECOMPOSITION; SIMILARITY	We consider the task of knowledge fusion, an important aspect of cognitive intelligence, with the goal of combiningpart-ofknowledge drawn from different sources. For this, entities and relations are cast into matrix-based representations. Unlike previous work on relation prediction, we consider the challenging setting of graphs with large amounts of completely separate connected components and no overlap between the training and test set entities. In order to address these challenges, we propose a novel cognitively inspired factorization method that jointly factorizes asubject-predicate-objecttensor via RESCAL and a similarity matrix via matrix factorization. Our experimental results show that our method significantly outperforms several strong baseline models, including RESCAL and several TransE-style models. The proposed joint factorization of asubject-predicate-objecttensor while applying matrix factorization to a similarity matrix obtains substantially higher average accuracy rates than previous approaches. This shows that it can successfully address the challenge of knowledge fusion of disconnected data.																	1866-9956	1866-9964				MAY	2020	12	3					642	653		10.1007/s12559-019-09686-4													
J								A Cognitively Inspired Knowledge-Based Decision-Making Methodology Employing Intuitionistic Fuzzy Sets	COGNITIVE COMPUTATION										Intuitionistic fuzzy set; Entropy measure; Knowledge measure; Multiple criteria decision making	ENTROPY WEIGHT	The differentiation procedure of intuitionistic fuzzy sets (IFSs) is very important in multiple criteria decision-making (MCDM). The aim here is to introduce a fruitful class of knowledge measures related to the information provided in terms of IFSs. We present a class of knowledge measures of IFSs that are based on the two notions: the fuzziness and the intuitionism of an IFS. An experimental problem is employed to illustrate the weight determination method based on the proposed knowledge measures.																	1866-9956	1866-9964				MAY	2020	12	3					667	678		10.1007/s12559-019-09702-7													
J								Generalized Geometric Aggregation Operators Based on T-Norm Operations for Complex Intuitionistic Fuzzy Sets and Their Application to Decision-making	COGNITIVE COMPUTATION										Complex intuitionistic fuzzy set; Geometric operators; Multicriteria decision-making; Archimedean t-cornom and t-norm; Aggregation operators	SOFT SETS; ALGORITHMS; DISTANCE; CONORM	Complex intuitionistic fuzzy set (CIFS) is a special intuitionistic fuzzy set where the membership and non-membership degrees are expressed by a complex-valued membership degree and can more easily describe the vagueness and uncertainty in the real world. Archimedean t-conorm and t-norm (ATT), as an important class of the t-norm (TN) and t-conorm (TC), have greater flexibility in the information fusion process. In this paper, we extend the ATT to CIFSs and present some generalized geometric aggregation operators, which can be used to handle the multiple criteria decision-making (MCDM) problems. For it, we firstly define some new operational laws of the CIFSs based on ATT, then some weighted geometric aggregation operators based on proposed operations are proposed. Further, some desirable properties and special cases of them are studied. Finally, a decision-making approach is developed for the MCDM problem with complex intuitionistic fuzzy information. A practical example is given to show the availability and advantages of the proposed method by comparison with some existing methods. The proposed aggregation operators are more generalized than the existing ones to utilize the uncertain and imprecise information. Several existing operators are considered as special cases of the proposed one. Finally, the proposed method will offer various choices to the decision-maker to access the finest alternatives.																	1866-9956	1866-9964				MAY	2020	12	3					679	698		10.1007/s12559-019-09678-4													
J								Early temporal prediction of Type 2 Diabetes Risk Condition from a General Practitioner Electronic Health Record: A Multiple Instance Boosting Approach	ARTIFICIAL INTELLIGENCE IN MEDICINE										Type 2 Diabetes; Machine Learning; Predictive Medicine; Temporal Analysis; Electronic Health Record; Clinical Decision Support System	INSULIN-RESISTANCE; FASTING GLUCOSE; CLASSIFICATION; TRIGLYCERIDES; ALGORITHMS; PRODUCT	Early prediction of target patients at high risk of developing Type 2 diabetes (T2D) plays a significant role in preventing the onset of overt disease and its associated comorbidities. Although fundamental in early phases of T2D natural history, insulin resistance is not usually quantified by General Practitioners (GPs). Triglyceride-glucose (TyG) index has been proven useful in clinical studies for quantifying insulin resistance and for the early identification of individuals at T2D risk but still not applied by GPs for diagnostic purposes. The aim of this study is to propose a multiple instance learning boosting algorithm (MIL-Boost) for creating a predictive model capable of early prediction of worsening insulin resistance (low vs high T2D risk) in terms of TyG index. The MIL-Boost is applied to past electronic health record (EHR) patients' information stored by a single GP. The proposed MIL-Boost algorithm proved to be effective in dealing with this task, by performing better than the other state-of-the-art ML competitors (Recall from 0.70 and up to 0.83). The proposed MIL-based approach is able to extract hidden patterns from past EHR temporal data, even not directly exploiting triglycerides and glucose measurements. The major advantages of our method can be found in its ability to model the temporal evolution of longitudinal EHR data while dealing with small sample size and variability in the observations (e.g., a small variable number of prescriptions for non-hospitalized patients). The proposed algorithm may represent the main core of a clinical decision support system.																	0933-3657	1873-2860				MAY	2020	105								101847	10.1016/j.artmed.2020.101847													
J								On the use of pairwise distance learning for brain signal classification with limited observations	ARTIFICIAL INTELLIGENCE IN MEDICINE										Pairwise learning; Schizophrenia; Classification; Electroencephalography	NEURAL-NETWORKS; EEG	The increasing access to brain signal data using electroencephalography creates new opportunities to study electrophysiological brain activity and perform ambulatory diagnoses of neurological disorders. This work proposes a pairwise distance learning approach for schizophrenia classification relying on the spectral properties of the signal. To be able to handle clinical trials with a limited number of observations (i.e. case and/or control individuals), we propose a Siamese neural network architecture to learn a discriminative feature space from pairwise combinations of observations per channel. In this way, the multivariate order of the signal is used as a form of data augmentation, further supporting the network generalization ability. Convolutional layers with parameters learned under a cosine contrastive loss are proposed to adequately explore spectral images derived from the brain signal. The proposed approach for schizophrenia diagnostic was tested on reference clinical trial data under resting-state protocol, achieving 0.95 +/- 0.05 accuracy, 0.98 +/- 0.02 sensitivity and 0.92 +/- 0.07 specificity. Results show that the features extracted using the proposed neural network are remarkably superior than baselines to diagnose schizophrenia (+20pp in accuracy and sensitivity), suggesting the existence of non-trivial electrophysiological brain patterns able to capture discriminative neuroplasticity profiles among individuals. The code is available on Github: https://github.com/DCalhas/siamese_schizophrenia_eeg.																	0933-3657	1873-2860				MAY	2020	105								101852	10.1016/j.artmed.2020.101852													
J								Breast cancer diagnosis from histopathological images using textural features and CBIR	ARTIFICIAL INTELLIGENCE IN MEDICINE										Breast cancer; Computer-aided diagnosis; Content-based image retrieval; Medical images; Histopathological images	CLASSIFICATION; HISTOLOGY; PERFORMANCE	Currently, breast cancer diagnosis is an extensively researched topic. An effective method to diagnose breast cancer is to use histopathological images. However, extracting features from these images is a challenging task. Thus, we propose a method that uses phylogenetic diversity indexes to characterize images for creating a model to classify histopathological breast images into four classes - invasive carcinoma, in situ carcinoma, normal tissue, and benign lesion. The classifiers used were the most robust ones according to the existing literature: XGBoost, random forest, multilayer perceptron, and support vector machine. Moreover, we performed content-based image retrieval to confirm the classification results and suggest a ranking for sets of images that were not labeled. The results obtained were considerably robust and proved to be effective for the composition of a CADx system to help specialists at large medical centers.																	0933-3657	1873-2860				MAY	2020	105								101845	10.1016/j.artmed.2020.101845													
J								Deep neural network for semi-automatic classification of term and preterm uterine recordings	ARTIFICIAL INTELLIGENCE IN MEDICINE										EHG and TOCO signals; sample entropy; wavelet entropy; stacked sparse autoencoder; softmax	ALGORITHM; ELECTROMYOGRAPHY	Pregnancy is a complex process, and the prediction of premature birth is uncertain. Many researchers are exploring non-invasive approaches to enhance its predictability. Currently, the ElectroHysteroGram (EHG) and Tocography (TOCO) signal are a real-time and non-invasive technology which can be employed to predict preterm birth. For this purpose, sparse autoencoder (SAE) based deep neural network (SAE-based DNN) is developed. The deep neural network has three layers including a stacked sparse autoencoder (SSAE) network with two hidden layers and one final softmax layer. To this end, the bursts of all 26 recordings of the publicly available TPEHGT DS database corresponding to uterine contraction intervals and non-contraction intervals (dummy intervals) were manually segmented. 20 features were extracted by two feature extraction algorithms including sample entropy and wavelet entropy. Afterwards, the SSAE network is adopted to learn high-level features from raw features by unsupervised learning. The softmax layer is added at the top of the SSAE network for classification. In order to verify the effectiveness of the proposed method, this study used 10-fold cross-validation and four indicators to evaluate classification performance. Experimental research results display that the performance of deep neural network can achieve Sensitivity of 98.2%, Specificity of 97.74%, and Accuracy of 97.9% in the publicly TPEHGT DS database. The performance of deep neural network outperforms the comparison models including deep belief networks (DBN) and hierarchical extreme learning machine (H-ELM). Finally, experimental research results reveal that the proposed method could be valid applied to semi-automatic identification of term and preterm uterine recordings.																	0933-3657	1873-2860				MAY	2020	105								101861	10.1016/j.artmed.2020.101861													
J								Mining post-surgical care processes in breast cancer patients	ARTIFICIAL INTELLIGENCE IN MEDICINE										Breast cancer; Process Mining; Topic Modelling; Latent Dirichlet Allocation; Temporal Data Analytics; Temporal Electronic Phenotyping; Electronic Health Records	CLINICAL PATHWAY PATTERNS	In this work we describe the application of a careflow mining algorithm to detect the most frequent patterns of care in a cohort of 3000 breast cancer patients. The applied method relies on longitudinal data extracted from electronic health records, recorded from the first surgical procedure after a breast cancer diagnosis. Careflows are mined from events data recorded for administrative purposes, including procedures from ICD9 - CM billing codes and chemotherapy treatments. Events data have been pre-processed with Topic Modelling to create composite events based on concurrent procedures. The results of the careflow mining algorithm allow the discovery of electronic temporal phenotypes across the studied population. These phenotypes are further characterized on the basis of clinical traits and tumour histopathology, as well as in terms of relapses, metastasis occurrence and 5-year survival rates. Results are highly significant from a clinical perspective, since phenotypes describe well characterized pathology classes, and the careflows are well matched with existing clinical guidelines. The analysis thus facilitates deriving real-world evidence that can inform clinicians as well as hospital decision makers.																	0933-3657	1873-2860				MAY	2020	105								101855	10.1016/j.artmed.2020.101855													
J								Fuzzy support vector machine-based personalizing method to address the inter-subject variance problem of physiological signals in a driver monitoring system	ARTIFICIAL INTELLIGENCE IN MEDICINE										Fuzzy support vector machine; Fuzzy membership; Personalization; Physiological signal; Driver monitoring system	DECISION-SUPPORT; NEURAL-NETWORK; STRESS; CLASSIFICATION; EXTRACTION; ALGORITHM; HEALTH; CAR	Physiological signals can be utilized to monitor conditions of a driver, but the inter-subject variance of physiological signals can degrade the classification accuracy of the monitoring system. Personalization of the system using the data of a tested subject, called local data, can be a solution, but the acquisition of sufficient local data may not be possible in real situations. Therefore, this paper proposes an effective personalizing method using small-sized local data. The proposed method utilizes a fuzzy support vector machine to allocate higher weight to the local data than to others, and a fuzzy membership is assigned to the training data by analyzing the importance of each datum. Three classification problems for a physiological signal-based driver monitoring system are introduced and utilized to validate the proposed method. The classification accuracy is compared with that of other personalizing methods, and the results show that the proposed method achieves a better accuracy on average, which is 3.46% higher than that of the simple approach using a basic support vector machine, thereby proving its effectiveness. The proposed method can train a personalized classifier with improved accuracy for a tested subject. The advantages of the proposed method can be utilized to develop a practical driver monitoring system.																	0933-3657	1873-2860				MAY	2020	105								101843	10.1016/j.artmed.2020.101843													
J								Explainable AI meets persuasiveness: Translating reasoning results into behavioral change advice	ARTIFICIAL INTELLIGENCE IN MEDICINE										Explainable AI; Explainable reasoning; Natural Language Generation; MHealth; Ontologies	MEDITERRANEAN DIET; PHYSICAL-ACTIVITY; GENERATION; FRAMEWORK; MODEL	Explainable AI aims at building intelligent systems that are able to provide a clear, and human understandable, justification of their decisions. This holds for both rule-based and data-driven methods. In management of chronic diseases, the users of such systems are patients that follow strict dietary rules to manage such diseases. After receiving the input of the intake food, the system performs reasoning to understand whether the users follow an unhealthy behavior. Successively, the system has to communicate the results in a clear and effective way, that is, the output message has to persuade users to follow the right dietary rules. In this paper, we address the main challenges to build such systems: (i) the Natural Language Generation of messages that explain the reasoner inconsistency; and, (ii) the effectiveness of such messages at persuading the users. Results prove that the persuasive explanations are able to reduce the unhealthy users' behaviors.																	0933-3657	1873-2860				MAY	2020	105								101840	10.1016/j.artmed.2020.101840													
J								Fuzzy inference model based on triaxial signals for pronation and supination assessment in Parkinson's disease patients	ARTIFICIAL INTELLIGENCE IN MEDICINE										Parkinson; Fuzzy inference; Feature extraction; Pronation supination	ANALYTICAL HIERARCHY PROCESS; QUANTIFICATION; TREMOR; BRADYKINESIA; IMPROVEMENT; LOGIC	Nowadays, the Unified Parkinson Disease Rating Scale supported by the Movement Disorder Society (MDS-UPDRS), is a standardized and widely accepted instrument to rate Parkinson's disease (PD). This work presents a thorough analysis of item 3.6 of the MDS-UPDRS scale which corresponds to the pronation and supination hand movements. The motivation for this work lies in the objective quantification of motor affectations not covered by the MDS-UPDRS scale such as unsteady oscillations and velocity decrements during the motor exploration. Overall, 12 different bio-mechanical features were quantified based on measurements performed by inertial measurement units (IMUs). After a feature selection process, the selected bio-mechanical features were used as inputs for a fuzzy inference model that predicts the stage of development of the disease in each patient. In addition to this model's output, the scores of three different expert examiners and the output of a fuzzy inference model which covers affectations strictly attached the MDS-UPDRS guidelines, were also considered to obtain an integrated computational model. The proposed integrated model was incorporated using the Analytic Hierarchy Process (AHP), which gives the novelty of a combined score that helps expert examiners to give a broader assessment of the disease that covers both affectations mentioned in the MDS-UPDRS guidelines and affectations not covered by it in an objective manner.																	0933-3657	1873-2860				MAY	2020	105								101873	10.1016/j.artmed.2020.101873													
J								Personalized risk prediction for breast cancer pre-screening using artificial intelligence and thermal radiomics	ARTIFICIAL INTELLIGENCE IN MEDICINE										Breast cancer; Thermography; Risk assessment; Machine learning; Thermalytix; Artificial intelligence	THERMOGRAPHY; ANGIOGENESIS	Motivation: Breast cancer is the leading cause of cancer deaths among women today. Survival rates in developing countries are around 50%-60% due to late detection. A personalized, accurate risk scoring method can help in targeting the right population for follow-up tests and enables early detection of breast abnormalities. Most of the available risk assessment tools use generic and weakly correlated features like age, weight, height etc. While a personalized risk scoring from screening modalities such as mammography and ultrasound could be helpful, these tests are limited to very few metropolitan hospitals in developing countries due to high capital cost, operational expenses and interpretation expertise needed for a large screening population. Methods: We propose and analyze a new personalized risk framework called Thermalytix Risk Score (TRS) to identify a high-risk target population for regular screening and enable early stage breast cancer detection at scale. This technique uses Artificial Intelligence (AI) over thermal images to automatically generate a breast health risk score. This risk score is mainly derived from two sub-scores namely, vascular score and hotspot score. A hotspot score signifies the abnormality seen from irregular asymmetric heat patterns seen on the skin surface, whereas vascular score predicts the presence of asymmetric vascular activity. These scores are generated using machine learning algorithms over medically interpretable parameters that describes the metabolic activity inside the breast tissue and indicate the presence of a possible malignancy even in asymptomatic women. Results: The proposed personalized risk score was tested on 769 subjects in four breast cancer screening facilities. The subjects' age ranged from 18 to 82 years with a median of around 45 years. Out of the 769 subjects, 185 subjects were diagnosed with a breast malignancy by an expert radiologist after mammography, ultrasound and/ or histopathology. Our personalized AI based risk score achieved an area under the receiver-operator curve (AUC) of 0.89 when compared to an age normalized risk score that showed an AUC of 0.68. We also found that if the computed risk score is used to place individuals into four risk groups, the likelihood of malignancy also increases monotonically with the risk grouping level. Conclusion: The proposed AI based personalized risk score uses breast thermal image patterns for risk computation and compares favorably to other generic risk estimation approaches. The proposed risk framework solution is automated, affordable, non-invasive, non-contact and radiation free and works for a wide age range of women from 18 to 82 years, including young women with dense breasts. The proposed score might be further used to assign subjects into one of the four risk groups and provide guidance on the periodicity of screening needed. In addition, the automatically annotated thermal images localizes the potential abnormal regions and might empower the physician to create a better personalized care.																	0933-3657	1873-2860				MAY	2020	105								101854	10.1016/j.artmed.2020.101854													
J								Reconstructing the patient's natural history from electronic health records	ARTIFICIAL INTELLIGENCE IN MEDICINE										Electronic Health Records; Natural Language Processing; Temporal Reasoning	CANCER; INFORMATION; SYSTEM	The automatic extraction of a patient's natural history from Electronic Health Records (EHRs) is a critical step towards building intelligent systems that can reason about clinical variables and support decision making. Although EHRs contain a large amount of valuable information about the patient's medical care, this information can only be fully understood when analyzed in a temporal context. Any intelligent system should then be able to extract medical concepts, date expressions, temporal relations and the temporal ordering of medical events from the free texts of EHRs; yet, this task is hard to tackle, due to the domain specific nature of EHRs, writing quality and lack of structure of these texts, and more generally the presence of redundant information. In this paper, we introduce a new Natural Language Processing (NLP) framework, capable of extracting the aforementioned elements from EHRs written in Spanish using rule-based methods. We focus on building medical timelines, which include disease diagnosis and its progression over time. By using a large dataset of EHRs comprising information about patients suffering from lung cancer, we show that our framework has an adequate level of performance by correctly building the timeline for 843 patients from a pool of 989 patients, achieving a precision of 0.852.																	0933-3657	1873-2860				MAY	2020	105								101860	10.1016/j.artmed.2020.101860													
J								Mining incomplete clinical data for the early assessment of Kawasaki disease based on feature clustering and convolutional neural networks	ARTIFICIAL INTELLIGENCE IN MEDICINE										Electronic health records; Clinical data mining; Medical decision making; Convolutional neural networks	RISK; IMPUTATION	Kawasaki disease (KD) is the leading cause of acquired heart disease in children. Its prompt treatment can effectively lower the risk of severe complications, such as coronary aneurysms. However, accurately diagnosing KD at its early stage is impracticable given its unknown pathogenesis and lack of pathognomonic features. In this study, we investigated data-driven approaches by using a cohort of 10,367 patients extracted from electronic health records for early KD assessment. The incompleteness of clinical data presents group-based missing patterns associated with different clinical assessment measures. To address this problem, we developed a method integrating feature clustering to enable matrix-based representation and convolutional neural networks (CNN) for feature extraction and fusion to explicitly exploit the multi-source data structure. Integrating missing data imputation methods with the proposed method demonstrated superior accuracy (an AUC of 0.97) compared with a number of benchmark methods. The present method shows potential to improve clinical data mining. Our study highlighted the feasible utilization of matrix-based feature representation and CNN-based feature extraction for incomplete clinical data mining to support medical decision-making.																	0933-3657	1873-2860				MAY	2020	105								101859	10.1016/j.artmed.2020.101859													
J								Vessel Structure Extraction using Constrained Minimal Path Propagation	ARTIFICIAL INTELLIGENCE IN MEDICINE										Minimal path approach; backtracking; vascular structures; segmentation	RETINAL IMAGES; BLOOD-VESSELS; MATCHED-FILTER; SEGMENTATION; CURVES; LEVEL	Minimal path method has been widely recognized as an efficient tool for extracting vascular structures in medical imaging. In a previous paper, a method termed minimal path propagation with backtracking (MPP-BT) was derived to deal with curve-like structures such as vessel centerlines. A robust approach termed CMPP (constrained minimal path propagation) is here proposed to extend this work. The proposed method utilizes another minimal path propagation procedure to extract the complete vessel lumen after the centerlines have been found. Moreover, a process named local MPP-BT is applied to handle structure missing caused by the socalled close loop problems. This approach is fast and unsupervised with only one roughly set start point required in the whole process to get the entire vascular structure. A variety of datasets, including 2D cardiac angiography, 2D retinal images and 3D kidney CT angiography, are used for validation. A quantitative evaluation, together with a comparison to recently reported methods, is performed on retinal images for which a ground truth is available. The proposed method leads to specificity (Sp) and sensitivity (Se) values equal to 0.9750 and 0.6591. This evaluation is also extended to 3D synthetic vascular datasets and shows that the specificity (Sp) and sensitivity (Se) values are higher than 0.99. Parameter setting and computation cost are analyzed in this paper.																	0933-3657	1873-2860				MAY	2020	105								101846	10.1016/j.artmed.2020.101846													
J								MetaChem: An Algebraic Framework for Artificial Chemistries	ARTIFICIAL LIFE										Artificial chemistry; swarm chemistry; nested chemistry; Jordan algebra; MetaChem; algebraic framework		We introduce MetaChem, a language for representing and implementing artificial chemistries. We motivate the need for modularization and standardization in representation of artificial chemistries. We describe a mathematical formalism for Static Graph MetaChem, a static-graph-based system. MetaChem supports different levels of description, and has a formal description; we illustrate these using StringCatChem, a toy artificial chemistry. We describe two existing artificial chemistries-Jordan Algebra AChem and Swarm Chemistry-in MetaChem, and demonstrate how they can be combined in several different configurations by using a MetaChem environmental link. MetaChem provides a route to standardization, reuse, and composition of artificial chemistries and their tools.																	1064-5462	1530-9185				MAY	2020	26	2					153	195		10.1162/artl_a_00315													
J								Alteration of (Frequency-Dependent) Fitness in Time-Shift Experiments Reveals Cryptic Coevolution and Uncoordinated Stasis in a Virtual Jurassic Park	ARTIFICIAL LIFE										Coevolution; digital evolution; evolutionary stasis; Red Queen; temporal intransitivity	RED-QUEEN; EXPERIMENTAL EVOLUTION; ENVIRONMENTAL-CHANGES; EVOLVING POPULATIONS; ADAPTIVE RADIATION; SPECIES-DIVERSITY; ESCHERICHIA-COLI; COURT JESTER; DYNAMICS; BIODIVERSITY	Among the major unresolved questions in ecosystem evolution are whether coevolving multispecies communities are dominated more by biotic or by abiotic factors, and whether evolutionary stasis affects performance as well as ecological profile; these issues remain difficult to address experimentally. Digital evolution, a computer-based instantiation of Darwinian evolution in which short self-replicating computer programs compete, mutate, and evolve, is an excellent platform for investigating such topics in a rigorous experimental manner. We evolved model communities with ecological interdependence among community members, which were subjected to two principal types of mass extinction: a pulse extinction that killed randomly, and a selective press extinction involving an alteration of the abiotic environment to which the communities had to adapt. These treatments were applied at two different strengths (Strong and Weak), along with unperturbed Control experiments. We performed several kinds of competition experiments using simplified versions of these communities to see whether long-term stability that was implied previously by ecological and phylogenetic metrics was also reflected in performance, namely, whether fitness was static over long periods of time. Results from Control and Weak treatment communities revealed almost completely transitive evolution, while Strong treatment communities showed higher incidences of temporal intransitivity, with pre-treatment ecotypes often able to displace some of their post-recovery successors. However, pre-treatment carryovers more often had lower fitness in mixed communities than in their own fully native conditions. Replacement and invasion experiments pitting single ecotypes against pre-treatment reference communities showed that many of the invading ecotypes could measurably alter the fitnesses of one or more residents, usually with depressive effects, and that the strength of these effects increased over time even in the most stable communities. However, invaders taken from Strong treatment communities often had little or no effect on resident performance. While we detected periods of time when the fitness of a particular evolving ecotype remained static, this stasis was not permanent and never affected an entire community at once. Our results lend support to the fitness-deterioration interpretation of the Red Queen hypothesis, and highlight community context dependence in determining fitness, the shaping of communities by both biotic factors and abiotic forcing, and the illusory nature of evolutionary stasis. Our results also demonstrate the potential of digital evolution studies to illuminate many aspects of evolution in interacting multispecies communities.																	1064-5462	1530-9185				MAY	2020	26	2					196	216		10.1162/artl_a_00305													
J								Evolving Connectionist Models to Capture Population Variability across Language Development: Modeling Children's Past Tense Formation	ARTIFICIAL LIFE										Neural network based modeling; behavioral genetics; class imbalance; quasi-regular mappings; English past tense; evolution; individual differences; development; genetic computing	NEURAL-NETWORKS; TRAJECTORIES; INTELLIGENCE; MORPHOLOGY; EVOLUTION; GENETICS	Children's acquisition of the English past tense has been widely studied as a testing ground for theories of language development, mostly because it comprises a set of quasi-regular mappings. English verbs are of two types: regular verbs, which form their past tense based on a productive rule, and irregular verbs, which form their past tenses through exceptions to that rule. Although many connectionist models exist for capturing language development, few consider individual differences. In this article, we explore the use of populations of artificial neural networks (ANNs) that evolve according to behavioral genetics principles in order to create computational models capable of capturing the population variability exhibited by children in acquiring English past tense verbs. Literature in the field of behavioral genetics views variability in children's learning in terms of genetic and environmental influences. In our model, the effects of genetic influences are simulated through variations in parameters controlling computational properties of ANNs, and the effects of environmental influences are simulated via a filter applied to the training set. This filter alters the quality of information available to the artificial learning system and creates a unique subsample of the training set for each simulated individual. Our approach uses a population of twins to disentangle genetic and environmental influences on past tense performance and to capture the wide range of variability exhibited by children as they learn English past tenses. We use a novel technique to create the population of ANN twins based on the biological processes of meiosis and fertilization. This approach allows modeling of both individual differences and development (within the lifespan of an individual) in a single framework. Finally, our approach permits the application of selection on developmental performance on the quasi-regular task across generations. Setting individual differences within an evolutionary framework is an important and novel contribution of our work. We present an experimental evaluation of this model, focusing on individual differences in performance. The experiments led to several novel findings, including: divergence of population attributes during selection to favor regular verbs, irregular verbs, or both; evidence of canalization, analogous to Waddington's developmental epigenetic landscape, once selection starts targeting a particular aspect of the task domain; and the limiting effect on the power of selection in the face of stochastic selection (roulette wheel), sexual reproduction, and a variable learning environment for each individual. Most notably, the heritability of traits showed an inverse relationship to optimization. Selected traits show lower heritability as the genetic variation of the population reduces. The simulations demonstrate the viability of linking concepts such as heritability of individual differences, cognitive development, and selection over generations within a single computational framework.																	1064-5462	1530-9185				MAY	2020	26	2					217	241		10.1162/artl_a_00316													
J								Decentralized Control for Swarm Robots That Can Effectively Execute Spatially Distributed Tasks	ARTIFICIAL LIFE										Swarm robot; decentralized control; self-organization; exploration and exploitation; task allocation	BEHAVIOR	A swarm robotic system is a system in which multiple robots cooperate to fulfill a macroscopic function. Many swarm robots have been developed for various purposes. This study aims to design swarm robots capable of executing spatially distributed tasks effectively, which can be potentially used for tasks such as search-and-rescue operation and gathering scattered garbage in rooms. We propose a simple decentralized control scheme for swarm robots by extending our previously proposed non-reciprocal-interaction-based model. Each robot has an internal state, called its workload. Each robot first moves randomly to find a task, and when it does, its workload increases, and then it attracts its neighboring robots to ask for their help. We demonstrate, via simulations, that the proposed control scheme enables the robots to effectively execute multiple tasks in parallel under various environments. Fault tolerance of the proposed system is also demonstrated.																	1064-5462	1530-9185				MAY	2020	26	2					242	259		10.1162/artl_a_00317													
J								The Surprising Creativity of Digital Evolution: A Collection of Anecdotes from the Evolutionary Computation and Artificial Life Research Communities	ARTIFICIAL LIFE										Surprise; creativity; digital evolution; experimental evolution; genetic algorithms; evolutionary computation	DESIGN; GENOME; STRATEGIES; ADAPTATION; ROBUSTNESS; COMPLEXITY; RADIATION; ROBOTS; TERM	Evolution provides a creative fount of complex and subtle adaptations that often surprise the scientists who discover them. However, the creativity of evolution is not limited to the natural world: Artificial organisms evolving in computational environments have also elicited surprise and wonder from the researchers studying them. The process of evolution is an algorithmic process that transcends the substrate in which it occurs. Indeed, many researchers in the field of digital evolution can provide examples of how their evolving algorithms and organisms have creatively subverted their expectations or intentions, exposed unrecognized bugs in their code, produced unexpectedly adaptations, or engaged in behaviors and outcomes, uncannily convergent with ones found in nature. Such stories routinely reveal surprise and creativity by evolution in these digital worlds, but they rarely fit into the standard scientific narrative. Instead they are often treated as mere obstacles to be overcome, rather than results that warrant study in their own right. Bugs are fixed, experiments are refocused, and one-off surprises are collapsed into a single data point. The stories themselves are traded among researchers through oral tradition, but that mode of information transmission is inefficient and prone to error and outright loss. Moreover, the fact that these stories tend to be shared only among practitioners means that many natural scientists do not realize how interesting and lifelike digital organisms are and how natural their evolution can be. To our knowledge, no collection of such anecdotes has been published before. This article is the crowd-sourced product of researchers in the fields of artificial life and evolutionary computation who have provided first-hand accounts of such cases. It thus serves as a written, fact-checked collection of scientifically important and even entertaining stories. In doing so we also present here substantial evidence that the existence and importance of evolutionary surprises extends beyond the natural world, and may indeed be a universal property of all complex evolving systems.																	1064-5462	1530-9185				MAY	2020	26	2					274	306		10.1162/artl_a_00319													
J								Report on Workshop: Planning the Future of Agent Simulation	ARTIFICIAL LIFE										Agent-based simulation; complex systems; principled simulation		In May 2019, a workshop on principled development of future agent-based simulations was held at Keele University. Participants spanned companies and academia, and a range of domains of interest, as well as participant career stages. This report summarizes the discussions and main outcomes from this workshop.																	1064-5462	1530-9185				MAY	2020	26	2					307	313		10.1162/artl_a_00320													
J								The Determination of Absorption and Reduced Scattering Coefficients of Optical Phantoms Using a Frequency-Domain Multi-Distance Method in a Non-contact Manner	ADVANCES IN ELECTRICAL AND COMPUTER ENGINEERING										absorption; biophotonics; optical sensors; optoelectronic devices; spectroscopy	NEAR-INFRARED SPECTROSCOPY; HEMOGLOBIN; MEDIA	A non-contact optical system has been designed to determine the absorption and reduced scattering coefficients of optical phantoms. The frequency-domain multi-distance method, which allows an estimation of optical properties in biological tissue uses the phase and intensity of radio frequency modulated light. The proposed design has been evaluated with optical phantoms. Estimated values for an absorption coefficient equal to 1 cm(-1) are 0.795, 0.690, 0.670 and 0.613 cm(-1) for wavelengths of 658 nm, 705 nm, 785 nm and 833 nm, respectively and for a reduced scattering coefficient equal to 22 cm(-1), the estimated values are 19.876, 18.845, 17.134 and 17.927 cm(-1). It has been concluded that this novel non-contact design can be used to determine the absorption and reduced scattering coefficients of optical phantoms. This system is the first step in medical equipment that may be used to measure absolute quantification of HbO, Hb, HbCO and HbMet concentrations in a contactless manner. Current oximeters with hemoglobin measurement capability require contact between the sensor and the skin. These oximeters have drawbacks when measuring child patients with asthma, bronchiolitis and bronchopneumonia. Currently it is not possible to assess oxygenation in open wounds. Therefore, it is worthwhile to develop a non-contact oximeter.																	1582-7445	1844-7600				MAY	2020	20	2					3	10		10.4316/AECE.2020.02001													
J								IoT Framework for Interoperability in the oneM2M Architecture	ADVANCES IN ELECTRICAL AND COMPUTER ENGINEERING										inference mechanisms; information science; internet of things; semantic web; standardization	SEMANTIC INTEROPERABILITY; INTERNET; THINGS	The IoT is expected that many devices and sensors can be interconnected and interact over the Internet. Conventional IoT solutions rely on vertically developed machine-to-machine solutions that yield limited interoperability. To ensure interoperability between IoT solutions, the oneM2M global initiative defines a horizontal M2M service layer. To provide more intelligent services, such as autonomous interaction services, semantic-level interoperability should be ensured. Previous studies have proposed solutions based on ontologies to realize semantic level interoperability. However, in dynamic environments such as IoT, where data generated by many devices must be processed, an ontology leads to a system performance degradation owing the overhead of the resource mapping mechanism. In this study, we propose a semantic IoT framework based on the Resource Description Framework graph extension scheme. We utilize an aggregator based on the oneM2M standard platform. All data are represented as an RDF graph, and reconfigured dynamically through semantic queries. The proposed semantic IoT gateway provides a user-based rule management mechanism via the Web, thereby enabling rule configuration to be dynamically tailored to user requirements. Finally, the performance is evaluated compared with a solution that utilizes an ontology in a real IoT system.																	1582-7445	1844-7600				MAY	2020	20	2					11	18		10.4316/AECE.2020.02002													
J								Optimal Transceiver Design for SWIPT in Interference Alignment Network	ADVANCES IN ELECTRICAL AND COMPUTER ENGINEERING										energy harvesting; interference channels; iterative methods; MIMO; optimization	SIMULTANEOUS WIRELESS INFORMATION	This paper studies simultaneous wireless information and power transfer in a K-user multiple-input multiple-output interference channel. A new scheme jointly designing interference alignment (IA) and wireless energy harvesting is proposed. Firstly, the character of each receiver is determined according to the maximum achievable rate and maximum harvested power. The receiving users are used as either energy harvesting (EH) users or information decoding (ID) users, where EH users are used to harvest energy, and ID users are used to transmit information. Secondly, the double-objective problem is established by maximizing both the total power harvested by EH users and the total signal-to-interference-noise ratio (SINR) of the ID users. The problem is solved by the weighted sum method. Moreover, the transmit precoding matrices and the receive interference suppression matrices are solved by iterative optimization algorithm based on a complete IA constraint (IOA-CIA). Simulation results show that the proposed IOA-CIA algorithm provides higher sum rate and harvested power than the existing algorithm.																	1582-7445	1844-7600				MAY	2020	20	2					19	26		10.4316/AECE.2020.02003													
J								Determination with Linear Form of Turkey's Energy Demand Forecasting by the Tree Seed Algorithm and the Modified Tree Seed Algorithm	ADVANCES IN ELECTRICAL AND COMPUTER ENGINEERING										algorithms; demand forecasting; energy optimization; heuristic algorithms	GENETIC ALGORITHM; SWARM INTELLIGENCE; OPTIMIZATION; CONSUMPTION; IMPROVEMENT; PREDICTION	Energy plays an important role in every stage of human life in different forms and variations. Along with the developments in economic, social and industrial fields, the amount of energy that countries need is increasing day by day. Therefore, it is significant to estimate the energy demand for a country's economic activities accurately. In this study, the energy demand forecast (EDF) application optimization problem of Turkey, one of the real-world optimization problems, was performed by MTSA (Modified Tree Seed Algorithm) and TSA (Tree Seed Algorithm) methods. From 1979 to 2005, gross domestic product (GDP), population, export and import values were used as parameter data. Thus, in the presence of three different possible scenarios, Turkey's energy demand from 2006 to 2025, which was estimated by MTSA and TSA methods. To demonstrate the success of MTSA and TSA in the problem of energy demand forecasting (EDF), they are compared with Ant Colony Algorithm (ACO), Particle Swarm Optimization (PSO), Bat Algorithm (BA), Differential Evolution Algorithm (DEA) and Artificial Algae Algorithm (AAA) methods which are in the literature. According to the results of the analysis, it was observed that the MTSA method was a successful estimation tool for energy demand.																	1582-7445	1844-7600				MAY	2020	20	2					27	34		10.4316/AECE.2020.02004													
J								Deep Learning Based Prediction Model for the Next Purchase	ADVANCES IN ELECTRICAL AND COMPUTER ENGINEERING										time series analysis; deep learning; prediction; e-commerce	NEURAL-NETWORK; ARIMA; ALGORITHM; DEMAND	Time series represent the consecutive measurements taken at equally spaced time intervals. Time series prediction uses the information in a time series to predict future values. The future value prediction is important for many business and administrative decision makers especially in e-commerce. To promote business, sales prediction and sensing of future consumer behavior can help business decision makers in marketing campaigns, budget and resource planning. In this study, deep learning based a new prediction model has been developed for the time of next purchase in e-commerce. The proposed model has been extensively tested and compared with RF, ARIMA, CNN and MLP using a retail market dataset. The experimental results show that the developed model has been more successful than RF, ARIMA, CNN and MLP to predict the time of the next purchase.																	1582-7445	1844-7600				MAY	2020	20	2					35	44		10.4316/AECE.2020.02005													
J								A Hybrid Model of 2d-DCT and 2d-Mycielski Algorithm for Hourly Global Solar Irradiation	ADVANCES IN ELECTRICAL AND COMPUTER ENGINEERING										discrete cosine transform; forecasting; modeling; prediction; solar energy	ARTIFICIAL NEURAL-NETWORK; PARTICLE SWARM OPTIMIZATION; SUPPORT VECTOR MACHINE; RADIATION; REGRESSION; FORECAST; DIFFUSE; POWER	In this work, hourly solar irradiation is defined as a two-dimensional discrete signal. This two-dimensional discrete signal is modeled by a novel hybrid approach, which includes both deterministic and stochastic processes. The variables of the two-dimensional model are the hour and the day of each solar irradiation measurement. The deterministic process is modeled by two-dimensional discrete cosine transform. The two-dimensional discrete cosine transform finds the coefficients of two-dimensional cosine harmonics of the solar irradiation data. In the proposed model, two-dimensional discrete cosine transform is applied at two levels for obtaining an accurate deterministic model. The stochastic process is modeled by two-dimensional Mycielski algorithm, which is developed for searching repeats of two-dimensional neighborhood pattern of the sample to be predicted in the data and making predictions which depend on the closest repeat of the largest neighborhood pattern. A novel model is obtained for hourly solar irradiation, which fits both deterministic and stochastic processes by the combination of two models. The proposed model is benchmarked with the selected distinguished methods in the literature. The obtained comparative results demonstrate success of the proposed model.																	1582-7445	1844-7600				MAY	2020	20	2					45	54		10.4316/AECE.2020.02006													
J								Firefly Algorithm Based Optimization Model for Planning of Optical Transport Networks	ADVANCES IN ELECTRICAL AND COMPUTER ENGINEERING										artificial intelligence; communication networks; genetic algorithms; optical fiber networks; optimization	DESIGN	The growth in data traffic is raising serious challenges for OTN in terms of improving their capacity efficiency in order to meet the new traffic requirements. Under these circumstances, the task of efficiently utilizing available resources opens opportunities for the development of a variety of techniques for network planning. This paper presents a decision support system for the optical transport network. It is considered the optical transport network planning problem where a traffic interest matrix between the demand nodes is specified. The network is modeled as a graph, through the arc-path approach. An Integer Linear Programming problem solved with a Firefly Algorithm is proposed for network planning, considering cost minimization. The main novelties of the proposed ILP model is that it accomplishes the optical network design with the possibility of multiple destinations of the traffic matrix and with dynamic allocation of the transmission system modularity. To solve the ILP optimization model the firefly algorithm, genetic algorithm and the exact method are used. Simulations are carried out to verify the performance of the bio-inspired algorithms in relation to the exact method. The results obtained with the firefly algorithm surpass those of the genetic algorithm and approximate the optimal result.																	1582-7445	1844-7600				MAY	2020	20	2					55	64		10.4316/AECE.2020.02007													
J								Performance Analysis of Tree Seed Algorithm for Small Dimension Optimization Functions	ADVANCES IN ELECTRICAL AND COMPUTER ENGINEERING										benchmark testing; algorithms; optimization; heuristic algorithms; optimization methods	SINE COSINE ALGORITHM; COLONY; TSA	Tree-Seed Algorithm (TSA) simulates the growth of trees and seeds on a land. TSA is a method proposed to solve continuous optimization problems. Trees and seeds indicate possible solutions in the search space for optimization problems. Trees are planted in the ground at the beginning of the search and each tree produces several seeds during iterations. While the trees were selected randomly during seed formation, the tournament selection method was used and also hybridized by adding the C parameter, which is the acceleration coefficient calculated according to the size of the problem. In this study, continuous optimization problem has been solved by the hybrid method. First, the performance analyses of the five best known numerical benchmark functions have been done, in both TSA and hybrid method TSA with 2, 3, 4 and 5 dimensions, and 10-50 population numbers. After that, well-known algorithms in the literature like Particle Swarm Optimization (PSO), TSA, Artificial Bee Colony (ABC), Harmony Search (HS), as well as hybrid method TSA (HTSA) have been applied to twenty-four numerical benchmark functions and the performance analyses of algorithms have been done. Hopeful and comparable conclusions based on solution quality and robustness can be obtained with the hybrid method.																	1582-7445	1844-7600				MAY	2020	20	2					65	72		10.4316/AECE.2020.02008													
J								Acquisition and Transmission of ECG Signals Through Stainless Steel Yarn Embroidered in Shirts	ADVANCES IN ELECTRICAL AND COMPUTER ENGINEERING										biomedical electrodes; biomedical telemetry; electrocardiography; telemedicine; wearable sensors	TEXTILE ELECTRODES; DRY ELECTRODES; SENSOR	A significant percent of all global deaths are caused by cardiovascular diseases (CVD). The diagnostic of the electrocardiogram (ECG) is a clinical practice widely adopted to evaluate the heart condition and identify CVD. For long-term ECG monitoring, a biopotential acquisition system integrated in common clothing is a viable solution for telemedicine. The electrodes and wires play a major role in the comfort and signal quality acquired from the patient. The paper presents a technical solution, where stainless steel yarn was used to create a Lead I Einthoven system consisting of 3 dry electrodes embroidered on a sports shirt. There are novel electrode materials and techniques that push further the state-of-the-art in ECG acquisition, but the authors focused on the currently available materials that are low-cost, widely available and easily integrable into common clothing, in order to seek a simple yet fully functional solution with the potential to become a truly ubiquitous ECG monitoring system.																	1582-7445	1844-7600				MAY	2020	20	2					73	78		10.4316/AECE.2020.02009													
J								Image Retrieval using One-Dimensional Color Histogram Created with Entropy	ADVANCES IN ELECTRICAL AND COMPUTER ENGINEERING										entropy; feature extraction; histograms; image retrieval; vector quantization	LBG; QUANTIZATION; ALGORITHM; FEATURES; SYSTEMS; PERFORMANCE; DESCRIPTOR	Image histograms are frequently used as a feature vector in content-based image retrieval (CBIR). The related methodology involves processing of a single channel histogram on gray level images while histograms of three channels must be processed in color images. Subsequently, there are two ways to process histograms of color images. In the first approach, the length of feature vector is extended by adding histogram data of each channel to create new feature vector. However, this kind of solution increases computational time and complexity. Second solution is to combine the histogram data obtained from each channel to establish a feature vector. In this study, a novel image retrieval approach, which uses a cluster-based one-dimensional histogram (ODH) for color images has been developed. Initially, multiple thresholds (MT) for each channel were calculated by means of Kapur entropy method. Then, the RGB color space was subdivided into sub-cubes or prisms. The numbers of pixels in each cluster and cluster index or class label have been used to construct a cluster-based one-dimensional histogram. Finally, image retrieval process has been implemented by using the one-dimensional color histogram (ODH) of images in database and query.																	1582-7445	1844-7600				MAY	2020	20	2					79	88		10.4316/AECE.2020.02010													
J								A Vision Based Crop Monitoring System Using Segmentation Techniques	ADVANCES IN ELECTRICAL AND COMPUTER ENGINEERING										agricultural engineering; crops; image processing; foldscope; image segmentation	ARABIDOPSIS-THALIANA; PLANT; IDENTIFICATION; PLATFORM; CAMERA	The characterization of health status for a plant using a non-destructive method is one of the challenging problems. In this study, the number of leaves and discoloration properties have been estimated using the images obtained from nine saplings of Solanum melongena (eggplant or brinjal) grown in the laboratory. The images were obtained using a mobile phone camera fitted on an automated device. A particle wave algorithm and contour grow technique was used for the segmentation of leaves which resulted in a segmentation accuracy of 89%. The defective percentage was estimated based on which saplings were ranked. Validation of healthy and defective regions was done by applying linear regression analysis on the estimated Normalized Green Red Difference Index (NGRDI) from images obtained using an automated device and a Foldscope (new paper-based microscope). The analysis resulted in R squared value and Least Mean Square Error (LMSE) of 0.86 and 0.1 respectively.																	1582-7445	1844-7600				MAY	2020	20	2					89	100		10.4316/AECE.2020.02011													
J								Generation of Visual Patterns from BoVW for Image Retrieval using modified Similarity Score Fusion	ADVANCES IN ELECTRICAL AND COMPUTER ENGINEERING										feature extraction; image fusion; image matching; image representation; supervised learning	BAG-OF-WORDS	The Bag of Visual Words (BoVW) turns up to be an efficient method to represent images for Content Based Image Retrieval (CBIR). Despite their significant usage, the traditional BoVW method has low discriminative power and fails to provide spatial information, which increases the false positive images and reduces the precision values. To address the first issue, a novel way of identifying a set of visual words unique for each category, named as Visual Patterns (VP) is proposed. Also, the weight for the respective VPs and a new way of score calculations for similarity matching with the database images are proposed. Then, to address the second issue of enhancing the spatial information, late fusion of Gabor filter features along with VP is proposed. As a consequence, VP provides better discriminative power and Gabor filtering, taking advantage of its complementary clue, provides spatial information. Hence, it helps to reduce the false matches and improves the precision values. Experiments are carried out on the popular datasets, namely, Caltech 256, Oxford 5K and Inria Holidays datasets along with Flickr 1M dataset. The proposed method is compared with other BoVW based models and proved that the MAP value is improved 0.50 times from the basic BoVW model.																	1582-7445	1844-7600				MAY	2020	20	2					101	112		10.4316/AECE.2020.02012													
J								Convolutional Neural Network Based Prediction of Conversion from Mild Cognitive Impairment to Alzheimer's Disease: A Technique using Hippocampus Extracted from MRI	ADVANCES IN ELECTRICAL AND COMPUTER ENGINEERING										artificial neural networks; computer aided diagnosis; image analysis; image classification; pattern recognition	GRADIENT NONLINEARITY CORRECTION; STRUCTURAL MRI; FEATURE RANKING; MCI; CLASSIFICATION; AD; DIAGNOSIS; ACCURACY; ENSEMBLE; IMAGES	Alzheimer's disease (AD) is an irreversible neurodegenerative disorder. Mild Cognitive Impairment (MCI) is a prodromal stage of AD and its identification is very crucial for early treatment. MCI to AD conversion is of imperative concern in current Alzheimer's research. In this study, we have investigated the conversion from MCI to AD using different types of features. The impact of structural changes in entire brain tissues captured through MRI, genetics, neuropsychological assessment scores and their combination are investigated. Computational cost can be significantly reduced by examining only the hippocampi region, atrophy of which is visible in the earliest stages of the disease. We proposed a CNN based deep learning approach for the prediction of conversion from MCI to AD using above mentioned features. Highest accuracy is achieved when left hippocampus is used as a region of interest (ROI). The proposed technique outperforms the other state of the art methods, while maintaining a low computational cost. The main contribution of the research lies in the fact that only a single slice based small region of MRI is used resulting in an outstanding performance. The accuracy, sensitivity and specificity achieved are 94%, 92% and 96% respectively.																	1582-7445	1844-7600				MAY	2020	20	2					113	122		10.4316/AECE.2020.02013													
J								Design of an Adaptive Flux Observer for Sensorless Switched Reluctance Motors Using Lyapunov Theory	ADVANCES IN ELECTRICAL AND COMPUTER ENGINEERING										AC machines; Lyapunov methods; motor drives; observers; state estimation	POSITION ESTIMATION; PULSE-INJECTION; ROTOR POSITION; SPEED; DRIVE; SIMULATION; 4-QUADRANT; MACHINES; SRM	This paper proposes an adaptive flux observer for a sensorless switched reluctance motor. The observer adaptive gains are designed using the Lyapunov theory to guarantee both the accuracy and stability of the sensorless control of a switched reluctance motor. A nonlinear inductance model is developed based on a finite element analysis data and used in the estimation algorithms for rotor position and speed. The adaptive flux observer estimates the rotor position at low, medium, and high speeds. A low-frequency ramp method is proposed to excite the switched reluctance motor during standstill where the voltage and current signals are unobservable. The proposed hybrid method is characterized by simplicity, accuracy, ease of implementation, and low real-time computation burden. Therefore, the sensorless control technique depends only on active phase measurements without extra hardware and memory storage for real-time implementation. Complete sensorless control of a three-phase 6/4-pole switched reluctance motor drive system is carried out using Matlab/Simulink. Also, it is implemented experimentally in real-time using the digital signal processor-DS1102 control board. The simulation and experimental results of the proposed sensorless scheme demonstrate the accurate estimation of both the speed and rotor position during the transient and steady states.																	1582-7445	1844-7600				MAY	2020	20	2					123	130		10.4316/AECE.2020.02014													
J								Comparison of Classification Algorithms for Detecting Patient Posture in Expandable Tumor Prostheses	ADVANCES IN ELECTRICAL AND COMPUTER ENGINEERING										biomedical measurement; machine learning; prosthetics; supervised learning; support vector machines	FALL DETECTION; RECOGNITION; SYSTEM; MOTION; MICROCALCIFICATIONS; ENDOPROSTHESIS; ACCELEROMETRY; SENSORS	Autonomous tumor prostheses are extended without the need of a clinic and of a medical supervision. It is necessary to make sure that the patient is not standing before extending these prostheses. This study aims to determine the posture of the patient for expandable tumor prostheses by employing oft-used three machine learning-based classification methods through comparing them all with each other. Patient posture is determined by using accelerometer and gyroscope data from inertial control unit placed in autonomous expandable tumor prosthesis. By using the created dataset, 48 features are extracted. Then, for optimization, with feature selection, the number of features is reduced to 10. The selected features are processed using the decision tree, the k-nearest neighborhood and support vector machine algorithms. These algorithms were compared with each other using machine learning performance parameters. Accuracy, recall, precision and F-score values are calculated and compared. Consequently, support vector machine is determined as the most successful technique. Then, the model is tested on the experimental setup developed within the scope of the study, and the posture is determined. It is found that with this system, in the presence of a load on the prosthesis, it can be accurately detected at a rate of 97.1% (the recall parameter).																	1582-7445	1844-7600				MAY	2020	20	2					131	138		10.4316/AECE.2020.02015													
J								US/MRI Guided Robotic System for the Interventional Treatment of Prostate	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Prostate intervention; robotic system; US; MRI; image fusion	IMAGE REGISTRATION; CANCER DETECTION; BIOPSY; US	Needle-based percutaneous prostate interventions include biopsy and brachytherapy and the former is the gold standard for the diagnosis of prostate cancer and the latter is often used in the treatment of prostate cancer. This paper introduces a novel robotic assistant system for prostate intervention and the system architecture and workflow are described, which is significant for the design of similar systems. In order to offer higher precision and better real-time performance, a Ultrasound (US)/Magnetic Resonance Imaging (MRI) fusion method is proposed to guide the procedures in this study. Moreover, image registration is a key step and a hot issue in image fusion, especially in multimodal image fusion. In this work, we adopt a novel registration method based on active demons and optic flow for prostate image fusion. To verify the availability of the system, we evaluate our approach of the US/MRI image fusion by using data acquired from six patients, and root mean square error (RMSE) for anatomical landmarks is 3.15 mm. In order to verify the accuracy and validity of the system developed in this paper, a system experimental platform was built and used for bionic tissue puncture of prostate under the guidance of MR and Transrectal Ultrasound (TRUS) fusion images. The experimental results show that the deviations of the final actual needle points of the three target points on the bionic tissue model measured in the laboratory environment are less than 2.5 mm.																	0218-0014	1793-6381				MAY	2020	34	5							2059014	10.1142/S0218001420590144													
J								Research on Remote Sensing Image Target Recognition Based on Deep Convolution Neural Network	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Remote sensing images; target recognition; deep convolution neural network; Faster R-CNN; SSD	OBJECT DETECTION	Target recognition is an important application in the time of high-resolution remote sensing images. However, the traditional target recognition method has the characteristics of artificial design, and the generalization ability is not strong, which makes it difficult to meet the requirement of the current mass data. Therefore, it is urgent to explore new methods for feature extraction and target recognition and location in remote sensing images. Convolutional neural network in deep learning can extract representative and discriminative multi-level features of typical features from images, so it can be used for multi-target recognition of remote sensing big data in complex scenes. In this study, NWPU VHR-10 data was selected, 50% was used for training, and the remainder was used for verification. The target recognition effects of two kinds of convolutional neural network models, Faster R-CNN and SSD, were studied and compared, and the mean average precision (mAP) was used for evaluation. The evaluation results show that the Faster R-CNN has three categories with an accuracy of more than 80%, and the SSD has seven categories with an accuracy of more than 80%, all of which show good results. The SSD model is particularly prominent in running time and recognition results, which proves convolutional neural networks have broad application prospects in the target recognition of remote sensing image data.																	0218-0014	1793-6381				MAY	2020	34	5							2054015	10.1142/S0218001420540154													
J								Diamond-Coated Mechanical Seal Remaining Useful Life Prediction Based on Convolution Neural Network	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Diamond-coated mechanical seal; remaining useful life (RUL); acoustic emission (AE); feature representation; convolutional neural network (CNN)	ACOUSTIC-EMISSION; FEATURE-EXTRACTION	Reliable remaining useful life (RUL) prediction of industrial equipment key components is of considerable importance in condition-based maintenance to avoid catastrophic failure, promote reliability and reduce cost during the production. Diamond-coated mechanical seal is one of the most critical wearing components in petroleum chemical, nuclear power and other process industries. Estimating the RUL is of critical importance. We consider the data-driven approaches for diamond-coated mechanical seal RUL estimation based on AE sensor data, since it is difficult to construct an explicit mathematical degradation model of seal. The challenges of this work are dealing with the noisy AE sensor data and modeling the degradation process with fluctuation. Faced with these challenges, we propose a pipeline method CDF-CNN to estimate the RUL for mechanical seal: WPD-KLD to raise the signal-to-noise ratio, novel CDF-based statistics to represent seal degradation process and CNN structure to estimate RUL. To acquire AE sensor data, several diamond-coated seals are tested from new to failure in three working conditions. Experimental results demonstrate that the proposed method can accurately predict the RUL of diamond-coated mechanical seal based on AE signals. The proposed prediction method can be generalized to other various mechanical assets.																	0218-0014	1793-6381				MAY	2020	34	5							2051007	10.1142/S0218001420510076													
J								Research on Pedestrian Detection and Vehicle Distance Algorithms of Electric Vehicle Based on Image Processing	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Image processing; electric vehicle; pedestrian detection; pedestrian-vehicle distance		With the economic growth of our country and the continuous improvement of people's living standards, cars have begun to enter thousands of households and become a necessity for people. However, the rapid growth of the number of automobiles has led to a sustained increase in carbon dioxide emissions and a significant decline in urban air quality, which seriously restricts the sustainable development of cities. With the introduction of the national air quality protection policy, electric vehicles will eventually replace the existing fuel vehicles and become a new generation of transportation for people to travel. At the same time, the large expansion of the number of cars has increased the hidden dangers of traffic accidents. In order to ensure the safety of pedestrians, drivers are given a more intelligent driving environment. This paper presents the research of pedestrian detection and pedestrian distance algorithm based on image processing. By comparing the performance of pedestrian detection algorithm based on SSD with traditional HOG+SVM pedestrian detection algorithm, the results of pedestrian-vehicle distance calculation are detected, and the feasibility and effectiveness of the algorithm are obtained. The results show that the proposed algorithm has good feasibility and practicability, and provide a good reference for the research of pedestrian detection algorithm for electric vehicles.																	0218-0014	1793-6381				MAY	2020	34	5							2054014	10.1142/S0218001420540142													
J								Research on Path Tracking Algorithm of Autopilot Vehicle Based on Image Processing	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Image processing; pattern recognition; automatic driving; trajectory tracking	FATIGUE	Smart cars are the result of the combination of the latest technological achievements in the fields of artificial intelligence, sensors, control science, computer, and network technology with the modern automobile industry. Intelligent cars usually have functions, such as automatic shifting, automatic driving, and automatic road condition recognition. The research of intelligent car technology involves many disciplines. This thesis focuses on the field of smart car visual navigation, focusing on image denoising, image information recognition, extraction, and pattern recognition control algorithms. The traditional trajectory tracking algorithm is mainly used in industrial computer or high-performance computer. The computational complexity leads to poor real-time control, and it is easily interfered by external complex terrain environment and internal disordered electromagnetic environment during vehicle driving. In general, on a regular basis, by the image analysis of the driver or the driver information, the image information is proposed using way trace processing technology, vehicle tracking control method and automatic driving rules. The simulation and experimental results show that the proposed control methods and rules used to carry out automatic driving vehicle are feasible. The algorithm reduces the complexity of the algorithm, improves the real-time and stability of the control and finally achieves a good trajectory tracking effect of the car on high-speed automatic driving.																	0218-0014	1793-6381				MAY	2020	34	5							2054013	10.1142/S0218001420540130													
J								Human Action Pattern Recognition and Semantic Research Based on Embodied Cognition Theory	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Cognition; motion recognition; pattern recognition; semantics	COMPUTER VISION SYSTEM; UNIFYING CONCEPTUAL-FRAMEWORK; ARTIFICIAL-INTELLIGENCE; PHYSICAL-ACTIVITY; OBJECT KNOWLEDGE; SELECTIVE USE; VAN SCHIE; DISEASE; ELK	Human body motion pattern recognition in video images is an important research direction in the field of pattern recognition. It has a very broad application prospect in many fields such as intelligent video surveillance, human-computer interaction, motion analysis, video retrieval, etc. Research has also received extensive attention from scholars at home and abroad. Pattern recognition is essentially a branch of artificial intelligence. It has its unique role in the field of artificial intelligence. Accurate recognition of human body motion patterns in video images is of great help in image classification, retrieval, human tracking and video surveillance. Based on the human visual perception mechanism, this paper proposes a human behavior recognition algorithm based on semantic saliency map. Through the combination of sliding window and similarity measure, the behavioral region that best exhibits the semantic features of the image is found, which is the semantically significant region. The semantic significant region and the original image are used as the dual input source to study the human behavior recognition, and the image is enhanced. The utilization of significant regional information better reveals the identifiable area of the image and contributes to the recognition of human behavior.																	0218-0014	1793-6381				MAY	2020	34	5							2055013	10.1142/S0218001420550137													
J								Parallel Spine Design and CPG Motion Test of Quadruped Robot	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Quadruped robot; parallel mechanism; active spine; CPG	MODEL; GAIT	The spine of mammals aids in the stability of locomotion. Central Pattern Generators (CPGs) located in spinal cord can rapidly provide a rhythmic output signal during loss of sensory feedback on the basis of a simulated quadruped agent. In this paper, active spine of quadruped robot is shown to be extremely effective in motion. An active spine model based on the Parallel Kinematic Mechanism (PKM) system and biological phenomena is described. The general principles involved in constructing a neural network coupled with limbs and spine to solve specific problems are discussed. A CPG mathematical model based on Hopf nonlinear oscillators produces rhythmic signal during locomotion is described, where many parameters to be solved must be formulated in terms of desired stability, often subject to vertical stability analysis. Our simulations demonstrate that active spine with setting reasonable CPG parameters can reduce unnecessary lateral displacement during trot gait, improving the stability of quadruped robot. In addition, we demonstrate that physical prototype mechanism provides a framework which shows correctness of simulation, and stability can thus be easily embodied within locomotion.																	0218-0014	1793-6381				MAY	2020	34	5							2059013	10.1142/S0218001420590132													
J								Correlation Between Error Factors, Visual Perception, and Interface Layout - Taking Digital Instrument Control Equipment of Nuclear Power Safety Injection System as an Optimization Example	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Error factor; visual perception; interface layout; correlation effect; nuclear power safety injection system		One of the effective ways to improve task performance is to trace the sources of the task failures, that is, error factors, and take them into consideration for the optimization design of the system interactive interface. In this study, the digital instrument control equipment of a nuclear power safety injection system plant was taken as an example to construct an error factor-visual perception-interface layout (EF-VP-IL) correlation model. The visual perception intensity (VPI) levels in the functional sections of the interface were obtained by extracting the error factors and performing weight analysis on each functional section. A set of optimal solutions of the interface layout was obtained using the genetic algorithm, and the interface layout was optimized accordingly. The calculation results showed that the proposed method increased the visual communication index (VCI) of the original interface from Z = 11.9662 to Z = 14.761 after optimization, which is an improvement of 23.36%.																	0218-0014	1793-6381				MAY	2020	34	5							2055012	10.1142/S0218001420550125													
J								Texture Analysis and Genetic Algorithms for Osteoporosis Diagnosis	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Texture analysis; genetic algorithms; trabecular bone; osteoporosis	BONE TEXTURE; TOOL	Early diagnosis of osteoporosis can efficiently predict fracture risk. There is a great demand to prevent this disease. The goal of this study was to distinguish osteoporotic cases from healthy controls on 2D bone radiograph images, using texture analysis and genetic algorithms (GAs). Gray Level Co-occurrence Matrix (GLCM), Rim length Matrix (RLM) and Binarized Statistical Image Features (BSIF) were used for texture analysis. Features are numerous and parameter-dependent. The related experts can pick out the useful input features for the classifier. It however remains a difficult task and may be inefficient or even harmful as the data pattern is not clear. In this paper, GAs were used to optimize the two parameters of the co-occurrence matrix (distance parameter or pixel separation, orientation or direction) and the number of gray levels used in the preprocessing quantification step. GAs were also used to select the best combination of features extracted from GLCM and RLM matrices. Experiments were conducted on two populations composed of Osteoporotic Patients and Control Subjects. Results show that GAs combined with GLCM and BSIF features can improve the classification rates (ACC = 87.50%) obtained using GLCM (ACC = 77.8%) alone.																	0218-0014	1793-6381				MAY	2020	34	5							2057002	10.1142/S0218001420570025													
J								Feature-Based Online Representation Algorithm for Streaming Time Series Similarity Search	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Pattern recognition; streaming time series; feature representation; similarity search	RECURRENT NEURAL-NETWORKS; MOTIFS	With the rapid development of information technology, we have already access to the era of big data. Time series is a sequence of data points associated with numerical values and successive timestamps. Time series not only has the traditional big data features, but also can be continuously generated in a high speed. Therefore, it is very time- and resource-consuming to directly apply the traditional time series similarity search methods on the raw time series data. In this paper, we propose a novel online segmenting algorithm for streaming time series, which has a relatively high performance on feature representation and similarity search. Extensive experimental results on different typical time series datasets have demonstrated the superiority of our method.																	0218-0014	1793-6381				MAY	2020	34	5							2050010	10.1142/S021800142050010X													
J								An Improved Maximum a Posterior-Based Estimation Method Coping with Capture Effect for RFID Tags Identification	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Dynamic framed-slotted ALOHA; maximum a posterior; capture effect; radio frequency identification	ANTICOLLISION ALGORITHM; FRAME LENGTH; AWARE ESTIMATION	Capture effect is a very common phenomenon in wireless channel. By using capture effect, the collision tags can be identified in a certain probability, so the identification efficiency and speed of the system can be improved. In this paper, we extend the Maximum a Posterior probability method to capture effect environment and propose a novel tag anti-collision algorithm MAPCE for RFID tags identification. The proposed algorithm can not only estimate the number of tags, but also estimate the capture effect probability. Based on the estimation, we can derive the optimal frame length in the tag identification systems. The experimental results show that when the tags' number is larger than 600 and less than 1000, the proposed algorithm has lower computational complexity and lower estimation error rate.																	0218-0014	1793-6381				MAY	2020	34	5							2050011	10.1142/S0218001420500111													
J								A Novel Neutrosophic Image Segmentation Based on Improved Fuzzy C-Means Algorithm (NIS-IFCM)	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Image segmentation; FCM; PSO; neutrosophic image	SET	Image segmentation is a classical problem in the field of computer vision. Fuzzy c-means algorithm (FCM) is often used in image segmentation. However, when there is noise in the image, it easily falls into the local optimum, which results in poor image boundary segmentation effect. A novel method is proposed to solve this problem. In the proposed method, first, the image is transformed into a neutrosophic image. In order to improve the ability of global search, a combined FCM based on particle swarm optimization (PSO) is proposed. Finally, the proposed algorithm is applied to the neutrosophic image segmentation. The results of experiments show that the novel algorithm can eliminate image noise more effectively than the FCM algorithm, and make the boundary of the segmentation area clearer.																	0218-0014	1793-6381				MAY	2020	34	5							2055011	10.1142/S0218001420550113													
J								Research on Human Movement Target Recognition Algorithm in Complex Traffic Environment	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Intelligent transportation; moving target recognition; Adaboost algorithm; support vector machine		With the increase in the total population of the society and the continuous increase in the number of trips, the traffic pressures faced by people are increasing. With the development and advancement of computer technology, the emergence of intelligent transportation provides a better way to solve the problem of effectively alleviating traffic pressure and reducing the incidence of traffic accidents. In recent years, intelligent traffic monitoring system, as one of the important branches in the field of intelligent transportation, has also received more and more attention. Among them, video-based moving target recognition technology involves theoretical knowledge in various fields such as artificial intelligence, image processing, pattern recognition and computer vision. It is an important means to realize "safe city" and "smart city" and a key technology for intelligent monitoring. Therefore, the research on human motion target recognition algorithm in complex traffic environment has important theoretical and practical value. In the field of intelligent traffic monitoring, the moving target detection and recognition effect of video images will have certain influence on the classification and behavior understanding of subsequent moving targets. In this paper, the commonly used moving target detection methods are studied first, and the convergence problem of the traditional Adaboost algorithm is improved. An Adaboost algorithm based on adaptive weight update is proposed, and then the support vector machine (SVM) is used. The algorithm identifies the detected moving target. Finally, through simulation experiments on the acquired video images, the results show that the proposed human motion target recognition algorithm based on adaptive weight update Adaboost and SVM has good feasibility and rationality.																	0218-0014	1793-6381				MAY	2020	34	5							2050012	10.1142/S0218001420500123													
J								Quantifying the regularity of a 3D set of points on the surface of an ellipsoidal object	PATTERN RECOGNITION LETTERS										3D Regularity; Compound structure; Ellipsoid hexagonal tilling	CELL IRREGULARITY	Several natural and artificial structures, such as human skin and mammals cortices, exhibit a compound organization, with basic elements being distributed along a surface. The problem of quantifying the geometrical uniformity of this type of biological and physical compound structures is addressed in this work. This required the solution of several problems, including the detection, along the surface, of the borders of the compound system, defining the adjacency between the elements in the 3D space, and obtaining a reference of uniformity for calculating the polygonality. Specific approaches were devised and applied to address each of these difficulties, including connectivity criteria ensuring the adjacency to remain within the considered surface as well as the extension of the polygonality, originally suggested for 2D structures, to 3D compound systems. The potential of the so-obtained method is illustrated with respect to compound eyes of fungus gnats (small, forest dwelling flies), and interesting results are reported and discussed, including the fact that the uniformity tends to increase toward the center of the system, and the absence of correlation with two measurements traditionally used for characterizing this type of eyes. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAY	2020	133						1	7		10.1016/j.patrec.2020.02.012													
J								Nom document digitalization by deep convolution neural networks	PATTERN RECOGNITION LETTERS												Nom is an ancient script used in Vietnam until the current Latin-based Vietnamese alphabet became common, and a large number of ancient Nom documents are in existence. Due to the gradual degradation of Nom documents and a decrease in the number of scholars who can understand them, a system to digitalize Nom documents is urgently necessary. This paper presents a segmentation-based method for digitalizing Nom documents using deep convolution neural networks. Nom pages are preprocessed, segmented into isolated characters, and then recognized by a single-character OCR. The structure of the U-Net is applied to create segmentation maps and extract character regions from them. Subsequently, we propose coarse and fine combined classifiers to recognize each character pattern. The results by the best classifier are revised by a decoder using a langue model. The decoder is the same as the connectionist temporal classification decoder used in end-to-end text recognition systems. Compared with the traditional segmentation method using projection profiles and the Voronoi diagram (IoU = 81.23%), the segmentation method using the deep convolution neural network produces a better result (IoU = 92.08%) for detecting character regions. The proposed CNN models for recognizing segmented character patterns outperforms the traditional models using the modified quadratic discriminant function and the learning vector quantization with the recognition rate of 85.07%. The combination of coarse and fine classifiers, the training dataset with salt and pepper noises, and the attention layer are the key factors in the recognition rate improvement. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAY	2020	133						8	16		10.1016/j.patrec.2020.02.015													
J								Egocentric visitor localization and artwork detection in cultural sites using synthetic data	PATTERN RECOGNITION LETTERS										Image-based localization; Artwork detection; Similarity search; Simulated data		Computer vision and machine learning can be used in cultural heritage to augment the experience of visitors during the exploration of the cultural site, as well as to assist its management. To achieve such goals, two fundamental tasks should be addressed, i.e., localizing visitors and recognizing the observed artworks. Wearable cameras offer a convenient setting to address both tasks through the analysis of images acquired from the visitors' points of view. However, the engineering of approaches to address such tasks generally requires large amounts of labeled data. We propose a tool which can be used to collect and automatically label synthetic visual data suitable to study image-based localization and artwork detection. The tool simulates a virtual agent navigating the 3D model of a real cultural site and automatically captures video frames along with the related ground truth camera poses and semantic masks indicating the position of artworks. We generate a dataset of synthetic images starting from the 3D model of a museum located in Siracusa, Italy. The experiments suggest that the proposed tool allows to drastically reduce the effort needed to collect and label data, providing a means to generate large-scale datasets suitable to study localization and artwork detection in cultural sites. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAY	2020	133						17	24		10.1016/j.patrec.2020.02.014													
J								Text-to-image via mask anchor points	PATTERN RECOGNITION LETTERS										Text-to-image; Mask dataset; Image synthesis; Anchor points		Text-to-image is a process of generating an image from the input text. It has a variety of applications in art generation, computer-aided design, and data synthesis. In this paper, we propose a new framework which leverages mask anchor points to incorporate two major steps in the image synthesis. In the first step, the mask image is generated from the input text and the mask dataset. In the second step, the mask image is fed into the state-of-the-art mask-to-image generator. Note that the mask image captures the semantic information and the location relationship via the anchor points. We also developed a userfriendly interface which helps parse the input text into the meaningful semantic objects. As a result, our framework is able to produce clear, reasonable, and more realistic images. The experiments on the most challenging COCO-stuff dataset illustrate the superiority of our proposed approach over the previous state of the arts. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAY	2020	133						25	32		10.1016/j.patrec.2020.02.013													
J								Association between work-related features and coronary artery disease: A heterogeneous hybrid feature selection integrated with balancing approach	PATTERN RECOGNITION LETTERS										Machine learning; Data mining; Heart disease; Coronary artery disease; Feature selection	DATA MINING TECHNIQUES; HEART-DISEASE; NEURAL-NETWORK; DIAGNOSIS; CLASSIFICATION; FRAMEWORK; STRESS	Coronary artery disease (CAD) is a leading cause of death worldwide and is associated with high health-care expenditure. Researchers are motivated to apply machine learning (ML) for quick and accurate detection of CAD. The performance of the automated systems depends on the quality of features used. Clinical CAD datasets contain different features with varying degrees of association with CAD. To extract such features, we developed a novel hybrid feature selection algorithm called heterogeneous hybrid feature selection (2HFS). In this work, we used Nasarian CAD dataset, in which work place and environmental features are also considered, in addition to other clinical features. Synthetic minority over-sampling technique (SMOTE) and Adaptive synthetic (ADASYN) are used to handle the imbalance in the dataset. Decision tree (DT), Gaussian Naive Bayes (GNB), Random Forest (RF), and XGBoost classifiers are used. 2HFS-selected features are then input into these classifier algorithms. Our results show that, the proposed feature selection method has yielded the classification accuracy of 81.23% with SMOTE and XGBoost classifier. We have also tested our approach with other well-known CAD datasets: Hungarian dataset, Long-beach-va dataset, and Z-Alizadeh Sani dataset. We have obtained 83.94%, 81.58% and 92.58% for Hungarian dataset, Long-beach-va dataset, and Z-Alizadeh Sani dataset, respectively. Hence, our experimental results confirm the effectiveness of our proposed feature selection algorithm as compared to the existing state-of-the-art techniques which yielded outstanding results for the development of automated CAD systems. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAY	2020	133						33	40		10.1016/j.patrec.2020.02.010													
J								Measuring user relevance in online debates through an argumentative model	PATTERN RECOGNITION LETTERS										Social networks; Weighted bipartite graph; Valued argumentation; Relevance measures; User profiles		Online debating forums are important social media for people to voice their opinions and engage in debates with each other. Measuring user relevance on these forums can be useful to identify different user profiles or behaviors in online debates, for example, users that tend to participate at the beginning of a debate and whose comments trigger participation, or users that post relevant comments but are not replied too much. To help users to distinguish such different user profiles, we propose graded measures based on users' influence, the controversy that they generate throughout the debates, their contribution to the polarization of the debates, and their social acceptance, that we extract by analyzing the debates in which the users participate. Our approach is based on an argumentation-based analysis that represents a debate as a valued argumentation framework, in which comments of a debate are arguments, the attack relation between arguments models disagreement between comments, and values for arguments represent the overall support of users for comments. Finally, we test our measures with a sample of users from Reddit debates, identifying four main groups of users, from users with almost no impact on the debate to very active ones with decisive comments for the outcome of the debate. (C) 2020 The Authors. Published by Elsevier B.V.																	0167-8655	1872-7344				MAY	2020	133						41	47		10.1016/j.patrec.2020.02.008													
J								Learning deep face representation with long-tail data: An aggregate-and-disperse approach	PATTERN RECOGNITION LETTERS										Face recognition; Deep representation learning; Long-tail distribution; Aggregate-and-disperse		In this work, we study the problem of deep representation learning on a large face dataset with long-tail distribution. Training convolutional neural networks on such dataset with conventional strategy suffers from imbalance problem which results in biased classification boundary, and the few-shot classes lying in tail parts further make the model prone to overfitting. Aiming to learn more discriminative CNN model from long-tail data, we propose a novel aggregate-and-disperse training schema. Firstly, our proposed method aggregates similar classes in tail part to avoid imbalance problem. Based on the aggregated super classes and those original head classes, a model is pre-trained to capture accurate discrimination in head classes as well as coarse discrinimation in tail classes. Secondly, we selectively disperses those aggregated super classes to learn precise inter-class variations and refine the representation for better generalization. We perform extensive experiments on MS-Celeb-1M, BLUFR and MegaFace. Compared with baselines and existing methods, our method achieves better performance of face recognition, demonstrating its effectiveness of handling long-tail distribution. (C) 2020 Published by Elsevier B.V																	0167-8655	1872-7344				MAY	2020	133						48	54		10.1016/j.patrec.2020.02.007													
J								Validating the robustness of an internet of things based atrial fibrillation detection system	PATTERN RECOGNITION LETTERS										Intelligent internet of things; Deep learning; Atrial fibrillation; Heart rate; Blindfold validation	SIGNALS	This paper describes the validation of a deep learning model for Internet of Things (IoT) based health care applications. As such, the deep learning model was created to detect episodes of Atrial Fibrillation (AF) using Heart Rate (HR) signals. The initial Long Short-Term Memory (LSTM) model was developed using 20 data sets, from distinct subjects, obtained from the AFDB database on PhysioNet. This model achieved an AF detection accuracy of 98.51% with ten fold cross validation. In this study, we validated the initial results by testing the developed deep learning model with unknown data. To be specific, we fed the data from 82 subjects to the deep learning system and compared the classification results with the diagnosis results indicated by human practitioners. The validation results show 94% accuracy with an area under the Receiver Operating Characteristic (ROC) curve of 96.58. These results indicate that the LSTM model is able to extract the feature maps from the unknown data and hence detect the AF periods accurately. With this blindfold validation testing we violated a well known design rule for learning systems which states that more data should be used for training than for testing. By doing so, we have established that our deep learning system is fit for practical deployment, because in a practical situation the diagnosis support system must apply the knowledge, extracted from a limited training data set, to a HR trace from a patient. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAY	2020	133						55	61		10.1016/j.patrec.2020.02.005													
J								GPU based parallel optimization for real time panoramic video stitching	PATTERN RECOGNITION LETTERS										Panoramic video; Image stitching; Embedded GPU; Local ORB; Stream parallel optimization		Panoramic video is a sort of video recorded at the same point of view to record the full scene. With the development of video surveillance and the requirement for 3D converged video surveillance in smart cities, CPU and GPU are required to possess strong processing abilities to make panoramic video. The traditional panoramic products depend on post processing, which results in high power consumption, low stability and unsatisfying performance in real time. In order to solve these problems, we propose a real-time panoramic video stitching framework. The framework we propose mainly consists of three algorithms, L-ORB image feature extraction algorithm, feature point matching algorithm based on LSH and GPU parallel video stitching algorithm based on CUDA. The experiment results show that the algorithm mentioned can improve the performance in the stages of feature extraction of images stitching and matching, the running speed of which is 11.3 times than that of the traditional ORB algorithm and 641 times than that of the traditional SIFT algorithm. Based on analyzing the GPU resources occupancy rate of each resolution image stitching, we further propose a stream parallel strategy to maximize the utilization of GPU resources. Compared with the L-ORB algorithm, the efficiency of this strategy is improved by 1.6-2.5 times, and it can make full use of GPU resources. The performance of the system accomplished in the paper is 29.2 times than that of the former embedded one, while the power dissipation is reduced to 10 W. (C) 2019 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAY	2020	133						62	69		10.1016/j.patrec.2019.06.018													
J								Image-to-video person re-identification with cross-modal embeddings	PATTERN RECOGNITION LETTERS										Person re-identification; Model transfer; Verification-identification		Despite the great progress achieved, image-to-video person re-identification is still challenging in the cross-modal scenario. Currently, state-of-the-art approaches mainly concentrate on the task-specific data, neglecting the extra information from the different but related tasks. In this paper, we propose an end-to-end neural network framework for image-to-video person re-identification with cross-modal embeddings learned from extra information. Concretely speaking, cross-modal embedding layers from image captioning and video captioning models, are incorporated to learn common latent embeddings for multiple modalities. The learned multimodal embeddings are expected to focus on person's prominent distinctions, due to textual descriptive information generally paying close attention to person's explicit characteristics. Apart from that, our proposed framework resorts to CNNs and LSTMs for extracting visual and spatiotemporal features, and combines the strengths of identification and verification model to improve the discriminative ability of the learned features. The experimental results demonstrate the effectiveness of our framework on narrowing down the gap between heterogeneous data and obtaining observable improvement in the image-to-video person re-identification task. (C) 2019 Published by Elsevier B.V.																	0167-8655	1872-7344				MAY	2020	133						70	76		10.1016/j.patrec.2019.03.003													
J								Diagnosis of carpal tunnel syndrome: A comparative study of shear wave elastography, morphometry and artificial intelligence techniques	PATTERN RECOGNITION LETTERS										Carpal tunnel syndrome; Ultrasonography; Elasticity imaging techniques; Deep learning; Machine learning (According to MESH)	MEDIAN NERVE; ULTRASOUND ELASTOGRAPHY; PRACTICE PARAMETER; GRAY-SCALE; DOPPLER; ULTRASONOGRAPHY; SONOGRAPHY; SEVERITY; ACCURACY; RATIO	Ultrasonography is an acceptable modality to evaluate median nerve (MN) in patients with carpal tunnel syndrome (CTS). Additional investigations are needed to evaluate sonographic parameters and compare their performances with artificial intelligence (AI) methods. The aim of this study is to compare the performance of shear wave elastography, morphometry, and AI techniques to predict MN entrapment accurately. 200 wrists including 100 CTS and 100 control wrists were included. Twelve morphological and five elasticity parameters were measured from each MN. Two AI techniques namely, support vector machine (SVM), and convolutional neural network (CNN) were used to diagnose CTS. MN area with area under receiver-operating characteristic curve (AUC) of 0.949 and mean elasticity with AUC of 0.942 showed the highest performance to differentiate CTS from control wrists among morphological and elasticity parameters, respectively. The CNN achieved the best performance with AUC of 0.980, while SVM obtained AUC of 0.943 in testing dataset to diagnose CTC. MN is larger, stiffer, more irregular and extended in CTS patients. Deep learning technique yielded the highest performance in diagnosing CTS automatically. AI methods have vast potential to be implemented in clinical practice as an auxiliary tool for the assessment of CTS with high accuracy. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAY	2020	133						77	85		10.1016/j.patrec.2020.02.020													
J								A permutational-based Differential Evolution algorithm for feature subset selection	PATTERN RECOGNITION LETTERS										Machine learning; Evolutionary algorithms; Wrapper scheme	OPTIMIZATION	This paper describes a permutational-based Differential Evolution algorithm implemented in a wrapper scheme to find a feature subset to be applied in the construction of a near-optimal classifier. In this approach, the relevance of a feature chosen to build a better classifier is represented through its relative position in an integer-valued vector, and by using a permutational-based mutation operator, it is possible to create new feasible candidate solutions only. Furthermore, to provide a controlled diversity rate in the population, a straightforward repair-based recombination operator is utilized to evolve a population of candidate solutions. Unlike the other approaches in the existing literature using integer-valued vectors and requiring a predefined subset size, in this approach, this size is determined by an additional element included in the encoding scheme, allowing to find an adequate feature subset size to each specific dataset. Experimental results show that this approach is an effective way to create more accurate classifiers as they are compared with those obtained by other similar approaches. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAY	2020	133						86	93		10.1016/j.patrec.2020.02.021													
J								Graph based semi-supervised classification with probabilistic nearest neighbors	PATTERN RECOGNITION LETTERS										Graph construction; Probabilistic nearest neighbors; Probability transition matrix; Semi-supervised classification	FACE RECOGNITION; CONSTRUCTION	Label propagation (LP) is one of the state-of-the-art graph based semi-supervised learning (GSSL) algorithm. Probability transition matrix (PTM) is the key for LP to propagate label information among samples. Conventionally, PTM is calculated based on the graph constructed in advance, and graph construction independent of PTM calculation. It leads to complex steps for acquiring PTM, and more importantly, brings about the lack of correlation between graph construction and inference. Based on adaptive neighbors-based method, probabilistic nearest neighbors (PNN) based graph construction algorithm is proposed for effective l2 norm optimization, and the solving process of the objective function is optimized by incorporating min-max normalization. The derived PNN matrix is more discriminative and directly serve as PTM for LP. It makes PTM computation more conveniently and more applicable for classification task. In addition, number of neighbors is adaptively determined on the premise of its preset value. Experimental results show that the proposed PNN algorithm specializes in reflecting probability differences of neighboring nodes in a graph, and positive results are achieved in semi-supervised classification. The average classification accuracy on synthetic data sets is 84.24%, and that on image data sets achieves 89.08%. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAY	2020	133						94	101		10.1016/j.patrec.2020.01.021													
J								Machine Learning for Cultural Heritage: A Survey	PATTERN RECOGNITION LETTERS										Artificial Intelligence; Machine Learning; Cultural Heritage; Digital Humanities	ARCHAEOLOGICAL SITE; CLASSIFICATION; RECOGNITION; REGRESSION	The application of Machine Learning (ML) to Cultural Heritage (CH) has evolved since basic statistical approaches such as Linear Regression to complex Deep Learning models. The question remains how much of this actively improves on the underlying algorithm versus using it within a 'black box' setting. We survey across ML and CH literature to identify the theoretical changes which contribute to the algorithm and in turn them suitable for CH applications. Alternatively, and most commonly, when there are no changes, we review the CH applications, features and pre/post-processing which make the algorithm suitable for its use. We analyse the dominant divides within ML, Supervised, Semi-supervised and Unsupervised, and reflect on a variety of algorithms that have been extensively used. From such an analysis, we give a critical look at the use of ML in CH and consider why CH has only limited adoption of ML. (C) 2020 The Authors. Published by Elsevier B.V.																	0167-8655	1872-7344				MAY	2020	133						102	108		10.1016/j.patrec.2020.02.017													
J								Text alignment in early printed books combining deep learning and dynamic programming	PATTERN RECOGNITION LETTERS										Dynamic programming; Early printed books; Faster R-CNN; Object detection		We describe a technique for transcript alignment in early printed books by using deep models in combination with dynamic programming algorithms. Two object detection models, based on Faster R-CNN, are trained to locate words. We first train an initial model to recognize generic words and hyphens by using information about the number of words in text lines. Using the model prediction on pages with a line-by-line ground-truth annotation is available, we train a second model able to detect landmark words. The alignment is then based on the identification of landmark words in pages where we only know the text corresponding to zones in the page. The proposed technique is evaluated on a publicly available digitization of the Gutenberg Bible while the transcription is based on the Vulgata, a late 4th century Latin translation of the Bible. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAY	2020	133						109	115		10.1016/j.patrec.2020.02.016													
J								MCENN: A variant of extended nearest neighbor method for pattern recognition	PATTERN RECOGNITION LETTERS										Extended nearest neighbor; Distance metric learning; Intra-class coherence; Classification	CLASSIFICATION	Recent studies have shown that extended nearest neighbor (ENN) method is able to improve the classification performance over traditional k-nearest neighbor (KNN) methods, due to its novel "two-way communication" decision making process. The ENN method classifies a test data sample by maximizing the intra-class coherence gain over the whole data set. However, the original ENN method, like KNN, is a type of instance-based learning algorithm without a training stage. This paper presents a variant of ENN method, called Maximum intra-class Coherence Extended Nearest Neighbor (MCENN), which incorporates distances between individual data sample and its nearest neighbors into the decision making process and introduces a novel distance metric learning algorithm as a training process to learn an optimal linear transformation that maximizes the overall intra-class coherence of training data. We demonstrate that the proposed MCENN approach is able to improve the discriminative performance for pattern recognition. Experimental results on real-life data sets demonstrate the effectiveness of the proposed approaches. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAY	2020	133						116	122		10.1016/j.patrec.2020.01.015													
J								Fitting local, low-dimensional parameterizations of optical turbulence modeled from optimal transport velocity vectors	PATTERN RECOGNITION LETTERS										Turbulence; Optimal transport; Velocity fields; Geometric optics	ATMOSPHERIC-TURBULENCE; GEOMETRICAL-OPTICS; NUMERICAL-METHOD; PROPAGATION; WAVES	This work exploits a connection between optimal transport theory and the physics of image propagation to yield a locally low-dimensional model of turbulence-corrupted imagery. Optimal transport produces an invertible, pixel-wise linear trajectories to approximate the globally nonlinear turbulence between a clean and turbulence corrupted image pair. We use the low-dimensional model to fit subsets of the optimal transport vector fields and stitch the local models into a surrogate for the global map to be used for image cleaning. Experiments are performed on laboratory generated data of beam propagation using different values of the Fried parameter (a scale measuring turbulence coherence) as well as a toy data set. The results suggest this is a fruitful direction, and first step, towards using multiple realizations of turbulence corrupted images to learn a blind surrogate for the optimal transport vector field for image cleaning. (C) 2019 Published by Elsevier B.V.																	0167-8655	1872-7344				MAY	2020	133						123	128		10.1016/j.patrec.2019.10.023													
J								Multi-scale superpatch matching using dual superpixel descriptors	PATTERN RECOGNITION LETTERS										Superpixels; Superpatch dual descriptor; Multi-scale non-local matching	ALGORITHM	Over-segmentation into superpixels is a very effective dimensionality reduction strategy, enabling fast dense image processing. The main issue of this approach is the inherent irregularity of the image decomposition compared to standard hierarchical multi-resolution schemes, especially when searching for similar neighboring patterns. Several works have attended to overcome this issue by taking into account the region irregularity into their comparison model. Nevertheless, they remain sub-optimal to provide robust and accurate superpixel neighborhood descriptors, since they only compute features within each region, poorly capturing contour information at superpixel borders. In this work, we address these limitations by introducing the dual superpatch, a novel superpixel neighborhood descriptor. This structure contains features computed in reduced superpixel regions, as well as at the interfaces of multiple superpixels to explicitly capture contour structure information. A fast multi-scale non-local matching framework is also introduced for the search of similar descriptors at different resolution levels in an image dataset. The proposed dual superpatch enables to more accurately capture similar structured patterns at different scales, and we demonstrate the robustness and performance of this new strategy on matching and supervised labeling applications. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAY	2020	133						129	136		10.1016/j.patrec.2020.02.018													
J								Using persistent homology to quantify a diurnal cycle in hurricanes	PATTERN RECOGNITION LETTERS										Topological Data Analysis; Atmospheric science; Diurnal cycle; Image processing	ALGORITHMS; SIGNALS; PULSES	The diurnal cycle of tropical cyclones (TCs) is a daily cycle in clouds that appears in satellite images and may have implications for TC structure and intensity. The diurnal pattern can be seen in infrared (IR) satellite imagery as cyclical pulses in the cloud field that propagate radially outward from the center of nearly all Atlantic-basin TCs. These diurnal pulses, a distinguishing characteristic of this diurnal cycle, begin forming in the storm's inner core near sunset each day, appearing as a region of cooling cloud-top temperatures. The area of cooling takes on a ring-like appearance as cloud-top warming occurs on its inside edge and the cooling moves away from the storm overnight, reaching several hundred kilometers from the circulation center by the following afternoon. The state-of-the-art TC diurnal cycle measurement in IR satellite imagery has a limited ability to analyze the behavior beyond qualitative observations. We present a method for quantifying the TC diurnal cycle using one-dimensional persistent homology, a tool from Topological Data Analysis, by tracking maximum persistence and quantifying the cycle using the discrete Fourier transform. Using Geostationary Operational Environmental Satellite IR imagery from Hurricanes Felix and Ivan, our method is able to detect an approximate daily cycle. (C) 2020 The Authors. Published by Elsevier B.V.																	0167-8655	1872-7344				MAY	2020	133						137	143		10.1016/j.patrec.2020.02.022													
J								Syntactic pattern recognition-based diagnostics of fetal palates	PATTERN RECOGNITION LETTERS										Syntactic pattern recognition; Ultrasound image analysis; Fetal cleft palate detection; Medical diagnosis	SEGMENTATION; MODEL; INFERENCE; TOOL	Analyzing ultrasound images of relatively small fetal structures is a difficult task. This difficulty particularly applies to diagnosing congenital defects of the fetus, including one of the most common defects, which is the cleft palate. To date, no methods have been developed for visualizing the fetal palate. Therefore, there is a need to improve the effectiveness of diagnostics in this area by developing appropriate image analysis and recognition methods using computer-based techniques. At the same time, relatively fast algorithms are being sought that can be a part of the software of an ultrasound device. The contribution of the paper consists in defining a new computer method satisfying all the requirements mentioned above. The method applies syntactic pattern recognition approach to the analysis of the image of the fetus's palate. It is based on extracting of a sequence of images on multiple binarization thresholds (in the preprocessing phase), defining picture primitives on the basis of the histogram analysis, and applying a parser for a GDPLL(k) string grammar (Generalized Dynamically Programmed LL(k) grammar) for classifying abnormalities (in the recognition phase). The implementation of the method is computationally efficient and it can be a helpful tool for supporting doctors in the diagnostic process (as it has been verified in practice). The computer recognition of fetal defects on basis of the analysis of the structure of the fetus's palate is a novel achievement. Such results have never been reported before. (C) 2020 Published by Elsevier B.V.																	0167-8655	1872-7344				MAY	2020	133						144	150		10.1016/j.patrec.2020.02.023													
J								Modernizing historical documents: A user Study	PATTERN RECOGNITION LETTERS										Cultural heritage; Historical documents modernization; Machine translation; Human evaluation; User study		Accessibility to historical documents is mostly limited to scholars. This is due to the language barrier inherent in human language and the linguistic properties of these documents. Given a historical document, modernization aims to generate a new version of it, written in the modern version of the document's language. Its goal is to tackle the language barrier, decreasing the comprehension difficulty and making historical documents accessible to a broader audience. In this work, we proposed a new neural machine translation approach that profits from modern documents to enrich its systems. We tested this approach with both automatic and human evaluation, and conducted a user study. Results showed that modernization is successfully reaching its goal, although it still has room for improvement. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAY	2020	133						151	157		10.1016/j.patrec.2020.02.027													
J								Controllable digital restoration of ancient paintings using convolutional neural network and nearest neighbor	PATTERN RECOGNITION LETTERS										Image restoration; Ancient paintings; Deep generative networks; Nearest neighbor		Ancient paintings are valuable culture legacy which can help archaeologists and culture researchers to study history and humanity. Most ancient artworks have damage problems, such as degradation, flaking and cracking. This work presents a novel controllable image inpainting framework with capability of incorporating suggestions from experts, which can help artists envisage how the ancient painting may have looked after a restoration. The framework leverages the content prediction power of deep convolutional neural network (CNN) and the nearest neighbor based pixel matching, where a deep CNN is designed to produce a coarse estimation of complete paintings by filling in missing regions and nearest neighbor based pixel matching is designed to map a mid-frequency estimation obtained from the deep CNN to high quality outputs in a controllable manner. In addition, we design a pixel descriptor using multi-scale neural features from different layers of a pre-trained deep network to capture different amounts of spatial context. Experimental results demonstrate that the proposed approach successfully predicts information in large missing regions and generates controllable high-frequency photo-realistic inpainting results. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAY	2020	133						158	164		10.1016/j.patrec.2020.02.033													
J								Improving FastText with inverse document frequency of subwords	PATTERN RECOGNITION LETTERS										Word embedding; FastText; Inverse document frequency; Word2vec		Word embedding is important in natural language processing, and word2vec is known as a representative algorithm. However, word2vec and many other dictionary-based word embedding algorithms create word vectors only for words that appear in the training data, ignoring morphological features of these words. The FastText algorithm was previously proposed to solve this problem: it creates a word vector from subword vectors, making it possible to create word embeddings even for words never seen during the training. Because of morphological features, FastText is strong in syntactic tasks but weak in semantic tasks, compared with word2vec. In this paper, we propose a method of improving FastText by using the inverse document frequency of subwords. Our approach is intended to overcome the weakness of FastText in semantic tasks. According to our experiments, the proposed method shows improved results in semantic tests with a little loss in syntactic tests. Our method can be applied to any word embedding algorithm that uses subwords. We additionally tested probabilistic FastText, an algorithm designed to distinguish multiple-meaning words, by adding the inverse document frequency, and the results confirmed an improved performance. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAY	2020	133						165	172		10.1016/j.patrec.2020.03.003													
J								A framework for multidimensional skyline queries over streaming data	DATA & KNOWLEDGE ENGINEERING										Multidimensional skylines; Streaming data; Index structure; Query optimization	COMPUTATION	Skyline query has attracted a great deal of interest during last years because of its ability to help decision makers when multi-criteria objectives are to be handled. Several authors have pointed the interest of multidimensional skylines, i.e., the set of criteria become a parameter of the query. In order to efficiently evaluate these queries, index structures have been proposed. In this paper, we address the problem of efficiently handling multidimensional skyline queries in the context of streaming data. The appended records have a validity time interval after which they become outdated and hence, can be discarded. To that end, we propose a framework that handles an index structure periodically updated. Then the queries consider just the indexed data. This is the price we pay to deal with the streaming nature of the data we consider. Through extensive experiments, we demonstrate our framework's ability to handle multidimensional skyline queries with challenging streaming data. The main criteria we consider to assess the performance of our solution are query execution time and both index structure maintenance time and its memory consumption.																	0169-023X	1872-6933				MAY	2020	127								101792	10.1016/j.datak.2020.101792													
J								Integrating Cuckoo search-Grey wolf optimization and Correlative Naive Bayes classifier with Map Reduce model for big data classification	DATA & KNOWLEDGE ENGINEERING										Big data classification; CNB classifier; GWO; CS algorithm; MapReduce model	MAPREDUCE; MACHINE	Big data is progressively being used in various areas, such as industry, financial dealing, medicine, and so on, as it can handle the challenges in processing large amounts of data. One of the data mining techniques used widely and effectively to classify big data is the MapReduce model. In this paper, an approach for the classification of big data is developed using Cuckoo-Grey wolf based Correlative Naive Bayes classifier and MapReduce Model (CGCNB-MRM). Accordingly, a novel classifier, named Cuckoo-Grey wolf based Correlative Naive Bayes classifier (CG-CNB), is designed by modifying CNB classifier with a newly developed optimization algorithm, Cuckoo-Grey Wolf based Optimization (CGWO). CGWO algorithm is designed by the effective integration of Cuckoo Search (CS) Algorithm into Grey Wolf Optimizer (GWO), to optimize the CNB model by the optimal selection of the model parameters. Finally, the proposed CGCNB-MRM approach performs the classification for each data samples based on the probability index table and the posterior probability of the data. Three metrics, such as accuracy, sensitivity, and specificity, are utilized for the performance evaluation of the proposed CGCNB-MRM approach, where it could achieve 80.7% accuracy with 84.5% sensitivity and 76.9% specificity and thus, prove its effectiveness in big data classification.																	0169-023X	1872-6933				MAY	2020	127								101788	10.1016/j.datak.2019.101788													
J								Content-based Node2Vec for representation of papers in the scientific literature	DATA & KNOWLEDGE ENGINEERING										Distributed representation; Artificial neural networks; Node2Vec; Link prediction		Lower-dimensional representation of scientific text has attracted much attention among researchers due to its impact on many data mining and recommendation tasks. This paper studies two main research streams in scientific literature representation. First, both local and distributed representation viewpoints are reviewed and their advantages and disadvantages in lower dimensional representation are discussed. The paper then proposes a novel hybrid distributed technique for text representation. Using scientific articles as the major source of textual information, both the article's content and citation network are used to build a distributed and universal lower dimensional representation. The superiority of the new technique to the traditional methods is then justified in predicting the existence of links in large citation graphs.																	0169-023X	1872-6933				MAY	2020	127								101794	10.1016/j.datak.2020.101794													
J								Semi-automated development of conceptual models from natural language text	DATA & KNOWLEDGE ENGINEERING										Conceptual modelling; Information extraction; Natural language processing; Ontologies; Semi-structured data	ONTOLOGIES; DESIGN	The process of converting natural language specifications into conceptual models requires detailed analysis of natural language text, and designers frequently make mistakes when undertaking this transformation manually. Although many approaches have been used to partly automate this process, one of the main limitations is the lack of a domain-independent ontology that can be used as a repository for entities and relationships, thus guiding the transformation process. In this paper, a semi-automated system for mapping natural language text into conceptual models is proposed. The system, called SACMES, combines a linguistic approach with an ontological approach and human intervention to achieve the task. SACMES learns from the natural language specifications that it processes and stores the information that is learnt in a conceptual model ontology and a user history knowledge database. It then uses the stored information to improve performance and reduce the need for human intervention. The evaluation conducted on SACMES demonstrates that (1) by using the system, precision and recall for users identifying entities of conceptual models is increased by 6% and 13%, respectively, while for relationships, increases are even higher, 14% for precision and 23% for recall; (2) the performance of the system is improved by processing more natural language requirements, and thus, the need for human intervention is decreased.																	0169-023X	1872-6933				MAY	2020	127								101796	10.1016/j.datak.2020.101796													
J								Top-k user-specified preferred answers in massive graph databases	DATA & KNOWLEDGE ENGINEERING										Graph databases; Top-k querying; Preferred answers	PERFORMANCE; TOOL	There are numerous applications where users wish to identify subsets of vertices in a social network or graph database that are of interest to them. They may specify sets of patterns and vertex properties, and each of these confers a score to a subgraph. The users want to find the subgraphs with top-k highest scores. Examples in the real world where such subgraphs involve custom scoring methods include: techniques to identify sets of coordinated influence boss on Twitter, methods to identify suspicious subgraphs of nodes involved in nuclear proliferation networks, and sets of sockpuppet accounts seeking to illicitly influence star ratings on e-commerce platforms. All of these types of applications have numerous custom scoring methods. This motivates the concept of Scoring Queries presented in this paper - unlike past work, an important aspect of scoring queries is that the users get to choose the scoring mechanism, not the system. We present the Advanced top-k (ATK) algorithm and show that it intelligently leverages graph indexes from the past but also presents novel pruning opportunities. We present an implementation of ATK showing that it beats out a baseline algorithm that builds on advanced subgraph matching methods with multiple graph database backends including Jena and GraphDB. We show that ATK scales well on real world graph databases from YouTube, Flickr, IMDb, and CiteSeerX.																	0169-023X	1872-6933				MAY	2020	127								101798	10.1016/j.datak.2020.101798													
J								A multi-view similarity measure framework for trouble ticket mining	DATA & KNOWLEDGE ENGINEERING										Trouble ticket; Similarity measure; Semantic similarity; Machine learning		Text similarity measures play a very important role in several text mining applications. Although there is an extensive literature on measuring the similarity between long texts, there is less work related to the measurement of similarity between short texts. And most of these works on short text similarity are based on adaptations of long-text similarity methods. Unfortunately, the description of a trouble ticket is just a kind of short texts. Thus, ticket mining applications such as ticket classification, ticket clustering, and ticket resolution recommendation often suffer from poor performance because of tickets' particular characteristics of unstructured, short free-text with large vocabulary size, large volume, non-English dictionary words, and so on. Therefore, the ability to accurately measure the similarity between two tickets is critical to the performance of ticket mining. To address this performance issue, this paper proposes a multi-view similarity measure framework that easily integrates several kinds of existing similarity measures including surface matching based measures, semantic similarity measures and syntax based measures. Further, in order to make full use of the strengths of different similarity measures, our framework adopts four different policies to combine them. In particular, we consider a machine learning based policy that can be applied to integrate various similarity measures in a more general way, which makes our framework flexible and extensible. To demonstrate the effectiveness of measures generated from our framework, we empirically validate them on a publicly available short text data set and apply them to a real-world ticket data set from a large enterprise IT infrastructure. Some important findings obtained via the result analysis will be helpful to further improve performance.																	0169-023X	1872-6933				MAY	2020	127								101800	10.1016/j.datak.2020.101800													
J								Hierarchy construction and classification of heterogeneous information networks based on RSDAEf	DATA & KNOWLEDGE ENGINEERING										Heterogeneous information networks; Relax strategy; Stacked denoising auto encoder; Hierarchy construction		Heterogeneous information networks (HINs) composed of multiple types of nodes and links, play increasingly important roles in real life applications. Classification of the related data is an essential work in network analysis. Existing methods can effectively solve these classification tasks when they are applied to homogeneous information networks and simple data, but not for the noisy and sparse data. To address the problem, we propose Stacked Denoising Auto Encoder (SDAE) with sparse factors to learn features of nodes in heterogeneous networks. In particular, sparse factors are added in each hidden layer of the proposed stacked denoising auto-encoder to efficiently extract features from noisy and sparse data. Moreover, a relax strategy is employed to construct class hierarchy with high-quality based. Finally, nodes of the heterogeneous information network can be classified. Our proposed framework Relax strategy on Stacked Denoising Auto Encoder with sparse factors (RSDAEf) comparison with several existing methods clearly indicates RSDAEf outperforms the existing methods and achieves a classification precision of 88.3% on DBLP dataset.																	0169-023X	1872-6933				MAY	2020	127								101790	10.1016/j.datak.2020.101790													
J								The classification of gliomas based on a Pyramid dilated convolution resnet model	PATTERN RECOGNITION LETTERS										Gliomas; Classification; Deep learning; ResNet; Dilated convolution	RECOGNITION; ALGORITHM; TUMORS	Gliomas are characterized by high morbidity and high mortality in primary tumors. The identification of glioma type is helpful for radiologists to facilitate correct medical judgments and better prognosis for patients. In order to avoid harm to patients caused by a biopsy, radiologists attempt to classify Magnetic Resonance Images(MRI) using deep learning methods. In the present paper, we propose a deep learning convolutional neural network ResNet based on the pyramid dilated convolution for Gliomas classification. The pyramid dilated convolution is integrated into the bottom of Resnet to increase the receptive field of the original network and improve the classification accuracy. After adding the pyramid dilated convolution model, the receptive field of the original network underlying convolution was improved. A clinical dataset is used to test the pyramid dilated convolution ResNet neural network model proposed in this paper. The experimental results demonstrate that the proposed method can effectively improve glioma classification performance. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAY	2020	133						173	179		10.1016/j.patrec.2020.03.007													
J								A lightweight face detector by integrating the convolutional neural network with the image pyramid	PATTERN RECOGNITION LETTERS										Face detection; Lightweight network; Real-time detection		Recently, significant improvements on face detection have been achieved by convolutional neural networks. However, the speed of a CNN-based face detector is hampered by the large computational complexity and the huge number of parameters. It is still challenging to achieve real-time face detection as well as maintain high accuracy. In this work, we introduce a single-stage face detector with an extremely lightweight CNN to achieve fast and accurate detection. Specifically, our method has a structure to integrate the network with the image pyramid for fully utilizing the calculated features. Benefiting from weight sharing, the network size still can keep small. We also analysis the detection capability of anchors with various scales, and reserve the most effective anchors in our model. Besides, to avoid the too difficult training samples which the small network can't learn, each ground truth face is assigned with a 0-or-1 weight during training. When tested on WIDERFACE and FDDB, it outperforms existing lightweight face detectors on accuracy with the smallest model size. The outstanding detection performance and lightweight model size signify its effectiveness and practicability. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAY	2020	133						180	187		10.1016/j.patrec.2020.03.002													
J								Automated detection of abnormal EEG signals using localized wavelet filter banks	PATTERN RECOGNITION LETTERS										EEG; Energy localization; Support vector machine (SVM); Wavelet-based features	TIME-FREQUENCY LOCALIZATION; EPILEPTIC SEIZURE DETECTION; ALZHEIMERS-DISEASE; DIAGNOSIS; CLASSIFICATION; DESIGN; PREDICTION; TRANSFORM	Epilepsy is a neural disorder that is associated with the central nervous system (CNS) in which the brain activity sometimes becomes abnormal, which may lead to seizures, loss of awareness, unusual sensations, and behavior. Electroencephalograms (EEG) are widely used to detect epilepsy accurately. However, the interpretation of a particular type of abnormality using the EEG signal is a subjective affair and may vary from clinician-to-clinician. Visual inspection of the EEG signal by observing a change in frequency or amplitude in long-duration signals is an arduous task for the clinicians. It may lead to an erroneous classification of EEGs. The proposed methodology focuses on automated detection of epilepsy using a novel stop-band energy (SBE) minimized orthogonal wavelet filter bank. Using the wavelet decomposition, we obtain subbands (SBs) of EEG signals. Subsequently, fuzzy entropy, logarithmic of the squared norm, and fractal dimension are computed for each SB. The different combinations of the extracted features were supplied to various classifiers for the classification of normal and abnormal EEG signals. In the proposed method, we have used a single-channel EEG dataset of Temple University Hospital. The dataset is the most substantial EEG data publicly available, which contains an EEG recording of 2130 distinct subjects. Our proposed system obtained the highest classification accuracy (CACC) of 78.4% and 79.34% during training and evaluation using the SVM classifier. We achieved the highest F1-score of 0.88. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAY	2020	133						188	194		10.1016/j.patrec.2020.03.009													
J								Deep hard modality alignment for visible thermal person re-identification	PATTERN RECOGNITION LETTERS										Visible thermal erson Re-identification; Deep modality alignment; Hard subspace mining	FACE	Visible Thermal Person Re-Identification (VTReID) is essentially a cross-modality problem and widely encountered in real night-time surveillance scenarios, which is still in need of vigorous performance improvement. In this work, we design a simple but effective Hard Modality Alignment Network (HMAN) framework to learn modality-robust features. Since current VTReID works do not consider the cross-modality discrepancy imbalance, their models are likely to suffer from the selective alignment behavior. To solve this problem, we propose a novel Hard Modality Alignment (HMA) loss to simultaneously balance and reduce the modality discrepancies. Specifically, we mine the hard feature subspace with large modality discrepancies and abandon the easy feature subspace with small modality discrepancies to make the modality distributions more distinguishable. For mitigating the discrepancy imbalance, we pay more attention on reducing the modality discrepancies of the hard feature subspace than that of the easy feature subspace. Furthermore, we propose to jointly relieve the modality heterogeneity of global and local visual semantics to further boost the cross-modality retrieval performance. This paper experimentally demonstrates the effectiveness of the proposed method, achieving superior performance over the state-of-the-art methods on RegDB and SYSU-MM01 datasets. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAY	2020	133						195	201		10.1016/j.patrec.2020.03.012													
J								Feature extraction from EEG spectrograms for epileptic seizure detection	PATTERN RECOGNITION LETTERS										EEG signals; Short-time Fourier Transform; Spectrograms; Feature Extraction; Seizure Classification	CLASSIFICATION	Identification of EEG signals is currently an open problem where performance analysis in terms of accuracy is relevant in several fields, such as biomedicine and brain computer interfaces. Nevertheless, performance depends on the feature extraction phase, where the aim is to find relevant patterns related to different mental activities. Thus, in this work, an approach to extract features from EEG signals is proposed based on spectrograms: Firstly, STFT is applied to EEG to obtain time-frequency representations, where parameters such as window length and type are experimented based on the EEG signal frequency. After that, spectral peaks are found to be used as reference in order to obtain descriptors per spectrogram. Three ways for extracting features from EEG are presented, the first based on frequency and surfaces, the second using K-means to extract features and the adaptation of local ternary pattern, and finally, a third using maximum peaks. The extracted descriptors are evaluated by means of a multilayer perceptron, support vector machines, and k-nearest neighbors. The proposed approach was evaluated using the dataset from Bonn University, identifying a healthy person and an epileptic attack classes as main task. According to the experimental results, the proposed method obtains acceptable accuracy (100%) in several cases by considering fewer features than those extracted by other related works. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAY	2020	133						202	209		10.1016/j.patrec.2020.03.006													
J								Automated detection and classification of fundus diabetic retinopathy images using synergic deep learning model	PATTERN RECOGNITION LETTERS										Deep learning; Classification; Diabetic retinopathy; Messidor dataset; Synergic deep learning		In recent days, the incidence of Diabetic Retinopathy (DR)has become high, affecting the eyes because of drastic increase in the glucose level in blood. Globally, almost half of the people under the age of 70 gets severely affected by diabetes. In the absence of earlier recognition and proper medication, the DR patients tend to lose their vision. When the warning signs are tracked down, the severity level of the disease has to be validated so to take decisions regarding appropriate treatment further. The current research paper focuses on the concept of classification of DR fundus images on the basis of severity level using a deep learning model. This paper proposes a deep learning-based automated detection and classification model for fundus DR images. The proposed method involves various processes namely preprocessing, segmentation and classification. The methods begins with preprocessing stage in which unnecessary noise that exists in the edges is removed. Next, histogram-based segmentation takes place to extract the useful regions from the image. Then, Synergic Deep Learning (SDL) model was applied to classify the DR fundus images to various severity levels. The justification for the presented SDL model was carried out on Messidor DR dataset. The experimentation results indicated that the presented SDL model offers better classification over the existing models. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAY	2020	133						210	216		10.1016/j.patrec.2020.02.026													
J								Adjusting the imbalance ratio by the dimensionality of imbalanced data	PATTERN RECOGNITION LETTERS										Imbalanced data; Imbalance extent; Imbalanced learning; Imbalance ratio; Pearson correlation test		Class-imbalance extent metrics measure how imbalanced the data are. In pattern classification, it is usually expected that the higher the imbalance extent, the worse the classification performance, and thus an appropriate imbalance extent metric should show a negative correlation with the classification performance. Existing metrics, such as the popular imbalance ratio (IR), only consider the effect of the sample sizes of different classes. However, we note that the dimensionality of imbalanced data also affects the classification performance. Datasets with the same IR can present distinct classification performances when their dimensionalities are different, making IR suboptimal to reflect the imbalance extent for classification. We also observe that the classification performance becomes better with more discriminative features. Inspired by these observations, we propose a new imbalance extent metric, the adjusted IR, by adding a penalty term of the number of discriminative features that is effectively determined by the Pearson correlation test. The adjusted IR adaptively revises the IR when the number of discriminative features varies. The empirical studies demonstrate the effectiveness of the adjusted IR, in terms of its better negative correlation with the classification performance. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAY	2020	133						217	223		10.1016/j.patrec.2020.03.004													
J								Supervised learning of the next-best-view for 3d object reconstruction	PATTERN RECOGNITION LETTERS										Next-best-view; 3D-CNN; 3D reconstruction		Motivated by the advances in 3D sensing technology and the spreading of low-cost robotic platforms, 3D object reconstruction has become a common task in many areas. Nevertheless, the selection of the optimal sensor pose that maximizes the reconstructed surface is a problem that remains open. It is known in the literature as the next-best-view planning problem. In this paper, we propose a novel next-best-view planning scheme based on supervised deep learning. The scheme contains an algorithm for automatic generation of datasets and an original three-dimensional convolutional neural network (3D-CNN) used to learn the next-best-view. Unlike previous work where the problem is addressed as a search, the trained 3D-CNN directly predicts the sensor pose. We present an experimental comparison of the proposed architecture against two alternative networks; we also compare it with state-of-the-art next-best-view methods in the reconstruction of several unknown objects. Our method is faster and reaches high coverage. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAY	2020	133						224	231		10.1016/j.patrec.2020.02.024													
J								Automated invasive ductal carcinoma detection based using deep transfer learning with whole-slide images	PATTERN RECOGNITION LETTERS										Invasive ductal carcinoma; Whole slide images; Deep learning; Transfer learning	CONVOLUTIONAL NEURAL-NETWORKS; CANCER STATISTICS; CLASSIFICATION; FEATURES; SEGMENTATION	Advances in artificial intelligence technologies have made it possible to obtain more accurate and reliable results using digital images. Due to the advances in digital histopathological images obtained using whole slide image (WSI) scanners, automated analysis of digital images by computer support systems has become interesting. In particular, deep learning architectures, are one of the preferred approaches in the analysis of digital histopathology images. The deeper networks trained on large amounts of image data are adapted for different tasks using transfer learning technique. In this study, automated detection of invasive ductal carcinoma (IDC), which is the most common subtype of breast cancers, is proposed using deep transfer learning technique. We have used deep learning pre-trained models, ResNet-50 and DenseNet-161 for the IDC detection task. The public histopathology dataset containing 277,524 image patches were used in our experimental studies. As a result of training on the last layers of pre-trained deep networks, DenseNet-161 model has yielded F-sore of 92.38% and balanced accuracy value of 91.57%. Similarly, we have obtained F-score of 94.11% and balanced accuracy value of 90.96% using ResNet-50 architecture. In addition, our developed model is validated using the publicly available BreakHis breast cancer dataset and obtained promising results in classifying magnification independent histopathology images into benign and malignant classes. Our developed system obtained the highest classification performance as compared to the state-of-art techniques and is ready to be tested with more diverse huge databases. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAY	2020	133						232	239		10.1016/j.patrec.2020.03.011													
J								Generating (co)homological information using boundary scale	PATTERN RECOGNITION LETTERS										Geometric cell complex; Algebraic-topological model; Scale-space model; Homology groups; Hierarchical graph	ALGORITHMS; HOMOLOGY; COMPLEXES; SPACE	In this paper we develop a new computational technique called boundary scale-space theory. This technique is based on the topol(1) ogical paradigm consisting of representing a geometric subdivided object K using a one-parameter family of geometric objects {K-i}(i >= 1) all of them having the same number of closed pieces than K. Each piece of K-i (for all i >= 1) presents the same interior part than the corresponding one in K, and a different boundary part depending on the scale i. Working with coefficients in a field, a scale is installed for the algebraic boundary of each piece and a new invariant for cell complex isomorphisms is given in terms of the Betti numbers of the generated boundary-scale-space cell complexes. Moreover, the so called homology boundary scale-space model of K (hbss-model for short) is introduced here. This model consists of a hierarchical graph whose nodes are the homology generators of the different boundary scale levels and whose edges are specified by homology generators of consecutive boundary scale indices linked by (hbss-transition maps) preserving homology classes. Various codes for each connected subgraph of an hbss-model are defined, which besides being fast and efficient similarity measures for cellular structures, they are as well relevant interpretive tools for the hbss-model. Finally, experimentations mainly aimed at clarifying and understanding the notion of hbss-model, as well as conjecturing about new graph isomorphism invariants (seeing graphs as a 1-dimensional cell complexes), are performed. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAY	2020	133						240	246		10.1016/j.patrec.2020.02.028													
J								Multiple Instance Learning with Genetic Pooling for medical data analysis	PATTERN RECOGNITION LETTERS										Multiple Instance Learning (MIL); Genetic Algorithm (GA); Pooling; Neural network		Multiple Instance Learning is a weakly supervised learning technique which is particularly well suited for medical data analysis as the class labels are often not available at desired granularity. Multiple Instance Learning through Deep Neural Networks is relatively a new paradigm in machine learning. The most important part of Multiple Instance Learning through Deep Neural Networks is designing a trainable pooling function which determines the instance-to bag relationship. In this paper, we propose a Multiple Instance pooling technique based on Genetic Algorithm called Genetic Pooling. In this technique, instance labels inside a bag are optimized by minimizing bag-level losses. The main contribution of the paper is that the bag level pooling layer for generating attention weights for bag instances are replaced by random initialization of attention weights and finding the optimized attention weights through Genetic Algorithm. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAY	2020	133						247	255		10.1016/j.patrec.2020.02.025													
J								Multiscale summarization and action ranking in egocentric videos	PATTERN RECOGNITION LETTERS										Egocentric video analysis; Hierarchical clustering; Multiscale summary; Action ranking		Useful information extraction from egocentric videos has evolved as an important research problem for both computer vision and multimedia communities. In this paper, we have addressed two problems, namely (i) generating multiscale summaries, i.e., multiple summaries of different lengths and (ii) priority-based ranking of various actions present in egocentric videos. A new algorithm, termed as Multiscale Egocentric Video Summarization and Action Ranking (MEVSAR), with agglomerative clustering as its backbone, is proposed to solve the above problems. Importantly, the MEVSAR algorithm follows an "analyze once, generate many" principle to generate multiple summaries in a single run and subsequently rank actions from the generated summaries. Experimental evaluation on two well-known publicly available datasets clearly demonstrate the merits of the proposed approach. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAY	2020	133						256	263		10.1016/j.patrec.2020.02.029													
J								Image segmentation based on ultimate levelings: From attribute filters to machine learning strategies	PATTERN RECOGNITION LETTERS										Residual morphological operator; Ultimate levelings; Ultimate attribute openings; Ultimate grain filters; Morphological trees		Ultimate levelings are operators that extract important image contrast information from a scale-space based on levelings. Along with the residual extraction process, some residues usually come from undesirable regions, and they should be filtered out. For this, some strategies can be applied to filter these undesirable residues. In this paper, we introduce a new approach to detect desirable regions from ultimate levelings through a new hierarchical structure called residual tree. From this structure, we extract attribute vectors to build a machine learning model which gives a matching value between ground truth regions and residual tree nodes. Thus, from the selected residual tree nodes, we present a new approach to choose the best disjoint residual nodes which gives the regions of the ultimate levelings. Finally, from the ultimate levelings, we use its partition associated function to solve the segmentation problem. In order to evaluate our new approach, some experiments were carried out with a plant dataset and results report the state-of-the-art performance in plant segmentation. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAY	2020	133						264	271		10.1016/j.patrec.2020.03.013													
J								A novel fitness function in genetic programming to handle unbalanced emotion recognition data	PATTERN RECOGNITION LETTERS										Emotion recognition; Fitness function; Genetic programming; EEG; Fast Fourier transformation		In the area of behavioral psychology, real-time emotion recognition by using physiological stimuli is an active topic of interest. This research considers the recognition of two class of emotions i.e., positive and negative emotions using EEG signals in response to happy, horror, sad, and neutral genres. In a noise-free framework for data acquisition of 50 participants, NeuroSky MindWave 2 is used. The dataset collected is unbalanced i.e., there are more instances of positive classes than negative ones. Therefore, accuracy is not a useful metric to assess the results of the unbalanced dataset because of biased results. So, the primary goal of this research is to address the issue of unbalanced emotion recognition dataset classification, for which we are proposing a novel fitness function known as Gap score (G score), which learns about both the classes by giving them equal importance and being unbiased. The genetic programming (GP) framework in which we implemented G score is named as G-score GP (GGP). The second goal is to assess how distinct genres affect human emotion recognition process and to identify an age group that is more active emotionally when their emotions are elicited. Experiments were conducted on EEG data acquired with a single-channel EEG device. We have compared the performance of GGP for the classification of emotions with state-of-the-art methods. The analysis shows that GGP provides 87.61% classification accuracy by using EEG. In compliance with the self-reported feelings, brain signals of 26 to 35 years of age group provided the highest emotion recognition rate. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAY	2020	133						272	279		10.1016/j.patrec.2020.03.005													
J								Sample imbalance disease classification model based on association rule feature selection	PATTERN RECOGNITION LETTERS										Association rules; Feature selection; Integrated learning; Sample imbalance	CONVOLUTIONAL NEURAL-NETWORK	In the research of computer-aided diagnosis, the shortage of disease feature dimension curse and the imbalance of medical samples have always been the focus of research on diagnostic decision support systems. For these two problems, we propose a feature selection algorithm based on association rules and an integrated classification algorithm based on random equilibrium sampling. We extracted and cleaned the electronic medical record text obtained from the hospital to obtain a diabetes data set. The proposed algorithm was verified in this data set and the public data set UCI. Experimental results show that the feature selection algorithm based on association rules is better than the CART, ReliefF and RFE-SVM algorithms in terms of feature dimension and classification accuracy. The proposed integrated classification algorithm based on random equalization sampling is superior to the comparative SMOTE-Boost and SMOTE-RF algorithms in macro precision, macro-full rate and macro F1 value, which embodies the robustness of the algorithm. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAY	2020	133						280	286		10.1016/j.patrec.2020.03.016													
J								Learning to realign hierarchy for image segmentation	PATTERN RECOGNITION LETTERS										Hierarchical image segmentation; Alignment of hierarchy; Regression; Random forest; Neural network		A hierarchical image segmentation is a set of image segmentations at different detail levels. However, objects (or even parts of the same object) may appear at different scales due to their size differences or to their distinct distances from the camera. One possible solution to cope with that is to realign the hierarchy such that every region containing an object (or its parts) is at the same level. In this work, we have explored the use of regression models to predict score values for regions belonging to a hierarchy of partitions, which are used to realign it. We have also proposed a new score calculation and a new assessment strategy considering all user-defined segmentations that exist in the ground-truth. Experimental results have pointed out that the use of new proposed score was able to improve final segmentation results. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAY	2020	133						287	294		10.1016/j.patrec.2020.03.010													
J								Special issue on pattern recognition and cognitive assistants	PATTERN RECOGNITION LETTERS										Pattern recognition; Cognitive assistants; Cognitive systems; Cognitive modelling; Machine learning; Virtual tutors		This special issue proposes a space for researchers to discuss the challenges of bridging pattern recognition and cognitive assistants. This bridge was travelled in both sides. From one side, how pattern recognition methods can help intelligent systems to become more cognitive. And from the other side, how cognitive models can improve pattern recognition methods so that the final system can better assist users. So, this special issue presents research work on these topics, aiming to observe their interrelations in order to create theoretical approaches, methodologies and computational tools to advance work on cognitive assistants and pattern recognition. (C) 2020 Published by Elsevier B.V.																	0167-8655	1872-7344				MAY	2020	133						295	297		10.1016/j.patrec.2020.03.017													
J								Metric Learning from Imbalanced Data with Generalization Guarantees	PATTERN RECOGNITION LETTERS										Imbalanced Data; Classification; Metric Learning; Statistical Machine Learning; Uniform Stability	SMOTE	Since many machine learning algorithms require a distance metric to capture dis/similarities between data points, metric learning has received much attention during the past decade. Surprisingly, very few methods have focused on learning a metric in an imbalanced scenario where the number of positive examples is much smaller than the negatives, and even fewer derived theoretical guarantees in this setting. Here, we address this difficult task and design a new Mahalanobis metric learning algorithm (IML) which deals with class imbalance. We further prove a generalization bound involving the proportion of positive examples using the uniform stability framework. The empirical study performed on a wide range of datasets shows the efficiency of IML. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAY	2020	133						298	304		10.1016/j.patrec.2020.03.008													
J								Video captioning with text -based dynamic attention and step-by-step learning	PATTERN RECOGNITION LETTERS										Dynamic attention; Context semantic information; Video captioning		Automatically describing video content with natural language has been attracting much attention in computer vision and natural language processing communities. Most existing methods predict one word at a time, and by feeding the last generated word back as input at the next time, while the other generated words are not fully exploited. Furthermore, traditional methods optimize the model using all the training samples in each epoch without considering their learning situations, which leads to a lot of unnecessary training and can not target the difficult samples. To address these issues, we propose a text-based dynamic attention model named TDAM, which imposes a dynamic attention mechanism on all the generated words with the motivation to improve the context semantic information and enhance the overall control of the whole sentence. Moreover, the text-based dynamic attention mechanism and the visual attention mechanism are linked together to focus on the important words. They can benefit from each other during training. In addition, the model is trained through two steps: "starting from scratch" and "checking for gaps". The former uses all the samples to optimize the model, while the latter only trains for samples with poor control. Experimental results on the popular datasets MSVD and MSR-VTT demonstrate that our non-ensemble model outperforms the state-of-the-art video captioning benchmarks. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAY	2020	133						305	312		10.1016/j.patrec.2020.03.001													
J								Error-tolerant approximate graph matching utilizing node centrality information	PATTERN RECOGNITION LETTERS										Centrality measures; Graph edit distance; Graph matching; Structural pattern recognition	ATTRIBUTED RELATIONAL GRAPHS; EDIT DISTANCE; RECOGNITION; COMPUTATION; SIMILARITY; SEARCH	Graph matching is the task of finding the similarity between the two graphs. Error-tolerant graph matching is the process of computing the similarity between the two graphs, where some flexibility to noise or error is allowed to approximate the value of graph matching. In this paper, we present a framework for graph matching by utilizing the centrality measures to ignore the least central nodes of the graphs. Experimental evaluation shows that this approach can be useful to reduce the overall matching time and it can provide time versus accuracy trade-off. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAY	2020	133						313	319		10.1016/j.patrec.2020.03.019													
J								Research on real-time analysis technology of urban land use based on support vector machine	PATTERN RECOGNITION LETTERS										Support vector machine; Data processing; Data analysis; Web mining; Text analysis	CLASSIFICATION; MODEL; ALGORITHM	One of the main problems that traditional support vector machine (SVM) has to solve is how to dynamically determine the kernel parameters and penalty parameters of the kernel function in time, along with the increasing amount of data and the changing data structure and characteristics. A new method is proposed for dynamic acquisition of SVM parameters by fruit fly optimization algorithm (FOA) based on the analysis of the classification and aggregation of land use data in urban industry. FOA-SVM aims at the relationship between feature words in the classification process and the core words of different activity semantics in context. In an incomplete date set of initial feature words, FOA-SVM can extract new feature words from the semantic association of feature words to improve the feature word date set. The dynamic parameters of SVM can be obtained through continuous training with FOA, and the accuracy of classification can be improved. The experimental results showed that FOA-SVM can process multi-feature synchronous classification according to different activity semantics and efficiently control the operation of the whole classification process, so as to obtain higher classification accuracy and stronger robustness in multi-source web date categorization. The efficiency of land use real-time analysis is improved. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAY	2020	133						320	326		10.1016/j.patrec.2020.03.022													
J								Feature fusion network based on attention mechanism for 3D semantic segmentation of point clouds	PATTERN RECOGNITION LETTERS										3D Semantic segmentation; Point clouds; Feature fusion; Attention mechanism		3D scene parsing has always been a hot topic and point clouds are efficient data format to represent scenes. The semantic segmentation of point clouds is critical to the 3D scene, which is a challenging problem due to the unordered structure of point clouds. The max-pooling operation is typically used to obtain the order invariant features, while the point-wise features are destroyed after the max-pooling operation. In this paper, we propose a feature fusion network that fuses point-wise features and local features by attention mechanism to compensate for the loss caused by max-pooling operation. By incorporating point-wise features into local features, the point-wise variation is preserved to obtain a refined segmentation accuracy, and the attention mechanism is used to measure the importance of the pointwise features and local features for each 3D point. Extensive experiments show that our method achieves better performances than other prestigious methods. (C) 2020 Published by Elsevier B.V.																	0167-8655	1872-7344				MAY	2020	133						327	333		10.1016/j.patrec.2020.03.021													
J								Visual question answering with attention transfer and a cross-modal gating mechanism	PATTERN RECOGNITION LETTERS										Attention; Visual question answering; Gating		Visual question answering (VQA) is challenging since it requires to understand both language information and corresponding visual contents. A lot of efforts have been made to capture single-step language and visual interactions. However, answering complex questions requires multiple steps of reasoning which gradually adjusts the region of interest to the most relevant part of the given image, which has not been well investigated. To integrate question related object relations into attention mechanism, we propose a multi-step attention architecture to facilitate the modeling of multi-modal correlations. Firstly, an attention transfer mechanism is integrated to gradually adjust the region of interest considering reasoning representation of questions. Secondly, we propose a cross-modal gating strategy to filter out irrelevant information based on multi-modal correlations. Finally, we achieve the state-of-the-art performance on the VQA 1.0 dataset and favorable results on the VQA 2.0 dataset, which verifies the effectiveness of our proposed method. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAY	2020	133						334	340		10.1016/j.patrec.2020.02.031													
J								Genetic algorithm based key sequence generation for cipher system	PATTERN RECOGNITION LETTERS										Feedback shift register; Genetic algorithm; Stream cipher system; Pseudo random number generator; Cross over; Mutation	IMAGES; NETWORK; SCHEME; TUMOR	Stream cipher system seems to be one of the best alternatives in order to provide confidentiality and security in an on line and high-speed transmission. Cryptography is required to secure the secret information transmitting over the communication channel. Day by day the importance of security increases due to increase of online transaction processing and e-commerce. As well as encryption / decryption algorithm, randomness characteristics of key sequences prove the strength of the stream cipher. In this work the Linear Feedback Shift Register (LFSR) is used to produce non binary pseudo random key sequence. The length of the sequence has been enhanced by designing hybrid model using LFSR and Genetic Algorithm (GA). Achieving the length more than the maximum length of LFSR is the primary intention of this work. The statistical tests is conducted to assess the randomness of key sequence generated from hybrid model. Generated key sequences are used as key sequences in cryptographic applications and results are analyzed. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAY	2020	133						341	348		10.1016/j.patrec.2020.03.015													
J								Multi-criteria online frame-subset selection for autonomous vehicle videos	PATTERN RECOGNITION LETTERS										Active learning; Video frame selection; Autonomous driving; Semantic segmentation		Data Subset selection for training learning models for a variety of tasks, has been widely studied in the literature of batch mode active learning. Recent works attempt to utilize the model specific signals in the deep learning context for computer vision tasks. Companies, in their bid to create safe autonomous driving models, train and test their models on billions of miles of driving data; not all of which may be valuable for a training task. In this paper, we study the problem of frame-subset selection from autonomous vehicle driving data, for the problem of semantic segmentation - which is a crucial component of the perception module in an autonomous driving system. We find that state of the art methods for deep active learning do not utilize pairwise similarity between incoming and existing frames. We explore both active learning settings, where labels for incoming points are not available, as well as frame selection settings and find that our method selects more valuable frames than only score-based frame subset selection, or frame subset selection without label information. We demonstrate the effectiveness of our method using DeeplabV3+ model on both benchmark as well as datasets generated by driving simulators. Our generated dataset and code will be made publicly available. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAY	2020	133						349	355		10.1016/j.patrec.2020.03.031													
J								Classification of origin with feature selection and network construction for folk tunes	PATTERN RECOGNITION LETTERS										Melody feature extraction; Random forest; Similarity networks	MELODIC SIMILARITY; MUSIC	We address the question to what extent origin of folk songs can be predicted, and construct a song similarity network. For this we use feature selection and train a random forest classifier on extracted melody n-grams and rhythm grams of songs from 7 different groups of origin, 80 from each. We use its importances to reduce the feature space dimension for the construction of an informative network, which we visualized in a interactive web application. These tools have vast application in large-scale exploration in digitized music databases and for specific questions in musicology. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAY	2020	133						356	364		10.1016/j.patrec.2020.03.023													
J								BEBLID: Boosted efficient binary local image descriptor	PATTERN RECOGNITION LETTERS										Local image descriptors; Binary descriptors; Real-time; Efficient matching; Boosting		Efficient matching of local image features is a fundamental task in many computer vision applications. However, the real-time performance of top matching algorithms is compromised in computationally limited devices, such as mobile phones or drones, due to the simplicity of their hardware and their finite energy supply. In this paper we introduce BEBLID, an efficient learned binary image descriptor. It improves our previous real-valued descriptor, BELID, making it both more efficient for matching and more accurate. To this end we use AdaBoost with an improved weak-learner training scheme that produces better local descriptions. Further, we binarize our descriptor by forcing all weak-learners to have the same weight in the strong learner combination and train it in an unbalanced data set to address the asymmetries arising in matching and retrieval tasks. In our experiments BEBLID achieves an accuracy close to SIFT and better computational efficiency than ORB, the fastest algorithm in the literature. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAY	2020	133						366	372		10.1016/j.patrec.2020.04.005													
J								Understanding the decisions of CNNs: An in-model approach	PATTERN RECOGNITION LETTERS										Explainable AI; Explainability; Interpretability; Deep Llearning; Convolutional Nneural Nnetworks		With the outstanding predictive performance of Convolutional Neural Networks on different tasks and their widespread use in real-world scenarios, it is essential to understand and trust these black-box models. While most of the literature focuses on post-model methods, we propose a novel in-model joint architecture, composed by an explainer and a classifier. This architecture outputs not only a class label, but also a visual explanation of such decision, without the need for additional labelled data to train the explainer besides the image class. The model is trained end-to-end, with the classifier taking as input an image and the explainer's resulting explanation, thus allowing for the classifier to focus on the relevant areas of such explanation. Moreover, this approach can be employed with any classifier, provided that the necessary connections to the explainer are made. We also propose a three-phase training process and two alternative custom loss functions that regularise the produced explanations and encourage desired properties, such as sparsity and spatial contiguity. The architecture was validated in two datasets (a subset of ImageNet and a cervical cancer dataset) and the obtained results show that it is able to produce meaningful image- and class-dependent visual explanations, without direct supervision, aligned with intuitive visual features associated with the data. Quantitative assessment of explanation quality was conducted through iterative perturbation of the input image according to the explanation heatmaps. The impact on classification performance is studied in terms of average function value and AOPC (Area Over the MoRF (Most Relevant First) Curve). For further evaluation, we propose POMPOM (Percentage of Meaningful Pixels Outside the Mask) as another measurable criteria of explanation goodness. These analyses showed that the proposed method outperformed state-of-the-art post-model methods, such as LRP (Layer-wise Relevance Propagation). (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				MAY	2020	133						373	380		10.1016/j.patrec.2020.04.004													
J								Saddle: Fast and repeatable features with good coverage	IMAGE AND VISION COMPUTING										Interest points; Fast detectors; Image matching	SCALE	A novel similarity-covariant feature detector that extracts points whose neighborhoods, when treated as a 3D intensity surface, have a saddle-like intensity profile is presented. The saddle condition is verified efficiently by intensity comparisons on two concentric rings that must have exactly two dark-to-bright and two bright-to-dark transitions satisfying certain geometric constraints. Saddle is a fast approximation of Hessian detector as ORB, that implements the FAST detector, is for Harris detector. We propose to use the matching strategy called the first geometric inconsistent with binary descriptors that is suitable for our feature detector, including experiments with fix point descriptors hand-crafted and learned. Experiments show that the Saddle features are general, evenly spread and appearing in high density in a range of images. The Saddle detector is among the fastest proposed. In comparison with detector with similar speed, the Saddle features show superior matching performance on number of challenging datasets. Compared to recently proposed deep-learning based interest point detectors and popular hand-crafted keypoint detectors, evaluated for repeatability in the ApolloScape dataset Huang et al. (2018), the Saddle detectors shows the best performance in most of the street-level view sequences a.k.a. traversals. (C) 2019 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				MAY	2020	97								103807	10.1016/j.imavis.2019.08.011													
J								Co-occurrence of deep convolutional features for image search	IMAGE AND VISION COMPUTING										Co-occurrence; Image retrieval; Feature aggregation; Pooling		Image search can be tackled using deep features from pre-trained Convolutional Neural Networks (CNN). The feature map from the last convolutional layer of a CNN encodes descriptive information from which a discriminative global descriptor can be obtained. We propose a new representation of co-occurrences from deep convolutional features to extract additional relevant information from this last convolutional layer. Combining this co-occurrence map with the feature map, we achieve an improved image representation. We present two different methods to get the co-occurrence representation, the first one based on direct aggregation of activations, and the second one, based on a trainable co-occurrence representation. The image descriptors derived from our methodology improve the performance in very well-known image retrieval datasets as we prove in the experiments. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				MAY	2020	97								103909	10.1016/j.imavis.2020.103909													
J								IoU-aware single-stage object detector for accurate localization	IMAGE AND VISION COMPUTING										IoU prediction; IoU-aware detector; Accurate localization; Single-stage object detector		Single-stage object detectors have beenwidely applied inmany computer vision applications due to their simpleness and high efficiency. However, the low correlation between the classification score and localization accuracy in detection results severely hurts the average precision of the detection model. To solve this problem, an IoU-aware single-stage object detector is proposed in this paper. Specifically, IoU-aware single-stage object detector predicts the IoU for each detected box. Then the predicted IoU is multiplied by the classification score to compute the final detection confidence, which is more correlated with the localization accuracy. The detection confidence is then used as the input of the subsequent NMS and COCO AP computation, which substantially improves the localization accuracy of model. Sufficient experiments on COCO and PASCOL VOC datasets demonstrate the effectiveness of IoU-aware single-stage object detector on improving model's localization accuracy. Without whistles and bells, the proposed method can substantially improve AP by 1.7%-1.9% and AP75 by 2.2%-2.5% on COCO test-dev. And it can also substantially improve AP by 2.9%-4.4% and AP80, AP90 by 4.6%-10.2% on PASCAL VOC. The source code will be made publicly available. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				MAY	2020	97								103911	10.1016/j.imavis.2020.103911													
J								Digital video intrusion intelligent detection method based on narrowband Internet of Things and its application	IMAGE AND VISION COMPUTING										NB-IoT; Video intrusion detection; Support vector machines	FEATURE-EXTRACTION; FACE RECOGNITION; SYSTEM; TECHNOLOGY; ALGORITHM; NETWORK	This paper proposes a digital video intrusion detection method based on Narrow Band Internet of Things (NB-IoT), and establishes a digital video intrusion detection system based on NB-IoT network and SVM intelligent classification algorithm. Firstly, the image is preprocessed by gradation processing and threshold transformation to extract the HOG feature extraction of human intrusion behavior in digital video frame images. Then, combined with the human intrusion HOG feature data, the SVM intelligent algorithm is used to classify the human intrusion behavior, so as to accurately classify the movements of walking, jumping, running and waving in video surveillance. Finally, the performance analysis of the algorithm finds that the classification time, classification accuracy and classification false positive rate of the model are tested. The classification time is 40.8 s, the shortest is 27 s, the classification accuracy is 87.65%, and the lowest is 83.7%. The false detection rate is up to 15%, both of which are less than 20%, indicating that the classification method has good accuracy and stability. Comparing the algorithm with other algorithms, the intrusion sensitivity, intrusion specificity and training speed of the model are 93.6%, 94.3%, and 19 s, respectively, which are better than other methods, which indicates that the model has good detection performance in the experimental stage. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				MAY	2020	97								103914	10.1016/j.imavis.2020.103914													
J								A novel hybrid particle swarm optimization and gravitational search algorithm for multi-objective optimization of text mining	APPLIED SOFT COMPUTING										Data text mining; Big-data; Swarm intelligence; GSA; PSO; Normal boundary intersection	FRAMEWORK	Big-data is one of the milestones on the web especially on social media (SM). Due to the widespread popularity of SM on the web, it is a painful task to capture the essence of SM. In this study, mining big social media data is re-formulated into a multi-objective optimization (MOO) task for an extractive summary. A Gravitational Search Algorithm (GSA) is utilized for optimizing several expressive objectives for generating a concise summary of SM. Moreover, particle swarm optimization (PSO) is mixed with GSA in a new shape to strengthen a local search ability and slow convergence speed in standard GSA. Whereas some users may demand the brief at any moment, several groups are constituted for incremental updating process during real-time based on naive Bayes algorithm. From experimental results, the proposed approach outperformed other notable and state-of-art comparative methods. (c) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106189	10.1016/j.asoc.2020.106189													
J								Influence Mechanism of Ambidextrous Innovation on Technological Catch-Up in Latecomer Enterprises of Technical Standards Alliance	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										ambidextrous innovation; technological catch-up; latecomer; technical standards alliance	ORGANIZATIONAL AMBIDEXTERITY; PRODUCT INNOVATION; PROCESS MANAGEMENT; MODERATING ROLE; EXPLOITATION; EXPLORATION; PERFORMANCE; KNOWLEDGE; FIRMS; INTERNATIONALIZATION	This study analyzes the mechanism through which ambidextrous innovation influences technological catch-up in latecomer enterprises. It first proposes a conceptual model of the influence of ambidextrous innovation on technological catch-up in latecomer enterprises and introduces the two regulating variables of alliance governance model and absorptive capacity. It then empirically studies the main effect and adjustment effect using hierarchical regression analysis based on a large-sample questionnaire survey, and finally, determines the effect and conditions of ambidextrous innovation on technological catch-up in latecomer enterprises.																	1343-0130	1883-8014				MAY	2020	24	3					251	259		10.20965/jaciii.2020.p0251													
J								A Predictive Model for Fertility Behavior of Women of Childbearing Age: Based on the Apriori Algorithm and Smooth Cut-Point Calculation	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										Apriori; smooth cut-points calculation; fertility behavior; model prediction	MIGRATION; HYPOTHESES	We combine the Apriori data mining algorithm and smooth cut-point calculation to build a model that uses microscopic individual data to predict fertility behavior. The data of China's migrant population from 2013 to 2015 are used to predict the reproductive behavior of migrant women. The accuracy of the prediction results is over 84%. The model also quantifies the extent to which the existing characteristics of individuals influence their reproductive behavior. The government can regulate individual fertility behavior based on the quantified scores.																	1343-0130	1883-8014				MAY	2020	24	3					260	264															
J								Forecasting Stock Index Futures Intraday Returns: Functional Time Series Model	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										functional time series analysis; dynamic forecasting; stock index futures; intraday returns	PREDICTION	It is of great significance to forecast the intraday returns of stock index futures. As the data sampling frequency increases, the functional characteristics of data become more obvious. Based on the functional principal component analysis, the functional principal component score was predicted by BM, OLS, RR, PLS, and other methods, and the dynamic forecasting curve was reconstructed by the predicted value. The traditional forecasting methods mainly focus on "point" prediction, while the functional time series forecasting method can avoid the point forecasting limitation, and realize "line" prediction and dynamic forecasting, which is superior to the traditional analysis method. In this paper, the empirical analysis uses the 5-minute closing price data of the stock index futures contract (IF1812). The results show that the BM prediction method performed the best. In this paper, data are considered as a functional time series analysis object, and the interference caused by overnight information is removed so that it can better explore the intraday volatility law, which is conducive to further understanding of market microstructure.																	1343-0130	1883-8014				MAY	2020	24	3					265	271															
J								A Study of the Impact of Innovation on Industrial Upgrading in China: A Spatial Econometric Analysis Based on China's Provincial Panel Data	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										innovation; industrial structure upgrading; spatial Durbin model; spatial spillover effect		This study provides an in-depth analysis of the impact of innovation on industrial upgrading from theoretical and practical perspectives. In terms of theory, based on the endogenous growth theory model, a multi-sectoral growth model is constructed to portray the inherent logical connection between innovation and industrial structure upgrading. The results show that innovation has an important impact on industrial structure upgrading. Industrial structure upgrading depends on differences in innovation level among industries and the substitution relationship of finished products in the industry. From an empirical perspective, based on the panel data of 31 provinces in China from 2005 to 2017, the spatial effect of innovation on industrial upgrading was analyzed using the spatial Durbin model. The results show that innovation and industrial structure upgrading has significant spatial correlation effects, and regional innovation can drive China's industrial upgrading. Meanwhile, the space spillover effect is an important factor that cannot be ignored in industrial upgrading.																	1343-0130	1883-8014				MAY	2020	24	3					272	281		10.20965/jaciii.2020.p0272													
J								Asymmetric Multifractal Analysis of Rebar Futures and Spot Market in China	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										wavelet denoising; overlapping smooth window; A-MMDCCA; asymmetric multifractal; rebar market	CROSS-CORRELATION ANALYSIS; DETRENDED FLUCTUATION ANALYSIS	In this study, a wavelet denoising method is first used to eliminate the influence of noise. Then, an overlapping smooth window technique is introduced into the asymmetric multifractal detrended cross-correlation analysis method, which was combined with the multiscale multifractal analysis method, resulting in the proposed asymmetric multiscale multifractal detrended cross-correlation analysis method. This method not only remedies the pseudo-fluctuation defect of the traditional method, but also explores the asymmetric multifractal cross-correlation between China's rebar futures and spot markets at different scales. The results show the existence of an asymmetric multifractal cross-correlation between rebar futures and spot markets with upward and downward trends at different scales. This cross-correlation is highly complex at the small-scale, and more pronounced when the futures market is in an uptrend.																	1343-0130	1883-8014				MAY	2020	24	3					282	292															
J								Human Body Dynamics Detection of Shock Absorption and Rebound Ability of Specialized Athletic Shoes	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										fencing-like shoes; functional athletic shoes; shock absorption capacity; fencing; lunge		For those who love sports, not only appearance but also functionality are important considerations in the design of athletic shoes. This is a study done on 12 subjects on a college fencing team and other sports teams. The subjects wear experimental specialized athletic shoes, including fencing shoes and other similar athletic shoes. Five kinds of shoes, fencing shoes plus four other similar types of athletic shoes, were examined for their shock absorption and rebound capacities. No significant difference between was found among them in the lunge test. However, there were significant differences between the two types of shoes in the 15 cm and 35 cm jump-down tests and extra heel support silicone gaskets in the fencing-like shoes tests. The fencing shoes proved to be the worst in terms of rebound ability in the 35 cm jump-down test. The fencing shoes had the best shock absorption but the worst rebound ability. In terms of overall performance, fencing shoes had the best shock absorption capacity. It is advisable for fencing shoes to be required during fencing training and at tournaments, but the severity of bounces, jumps, and squats should be lowered to avoid damage to the heel.																	1343-0130	1883-8014				MAY	2020	24	3					293	298															
J								Forecasting Realized Volatility Based on Sentiment Index and GRU Model	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										sentiment index; realized volatility; GRU	VARIANCE	Numerous studies have proven that news media sentiment has an impact on stock market volatility, making topics such as how to quantify news media sentiment and use it to predict stock market volatility increasingly relevant. In this paper, a Chinese financial sentiment lexicon was constructed to quantify the emotions in the news media as a sentiment index to be added to the model and establish new prediction models HAR-RV-AI and GRU-AI. To compare the prediction ability of the models, we consider the loss function and model confidence set (MCS) test as the evaluation criterion and employ the rolling window strategy for out-of-sample forecasting. The prediction results of the GRU model are found to be better than the HAR-RV model, and the prediction effect of the model improved after the addition of the news media sentiment index.																	1343-0130	1883-8014				MAY	2020	24	3					299	306															
J								Discussing Psychological Changes in College Students Who Participate in Physical Education Using Structural Equation Modeling	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										fuzzy questionnaires; well-being; passion; self-esteem; psychological states	SELF-ESTEEM; INTRINSIC MOTIVATION; SATISFACTION; PASSION; PERFORMANCE; HAPPINESS; STRESS; QUEST	Objectives: To examine self-esteem in college students who participate in physical education (PE), the influence of psychological trends on self-esteem, and the possible influence of passion. Motivations: This study aims to determine whether passion affects participants' self-esteem and psychological well-being. Methods: Subjects included 183 students enrolled in university PE classes (46 men and 137 women) with an average age of 19.58 (SD = 6.19). Self-esteem, passion, and psychological well-being were measured using a fuzzy questionnaire and descriptive statistics and structural equation modeling were performed through fuzzy statistical analysis. Findings: 1. The self-esteem, passion, and psychological well-being scales had good reliability and validity. 2. The model's dimensional goodness of fit was satisfactory (chi(2) = 281.601(182)= 1.952, GFI = 0.863, AGFI = 0.821, RMSEA = 0.072, SRMR = 0.649, TLI [NNFI] = 0.918, IFI = 0.925, CFI = 0.942). 3. Passion had a direct influence on well-being and an indirect influence on self-esteem. Well-being had a direct influence on self-esteem. Innovations: The passion scale and psychological well-being scale were confirmed to have good reliability and validity. The new method of fuzzy statistical analysis used in this study provides new research techniques for investigation and research into psychological trends in the field of sports. Value: In the teaching process, PE teachers should encourage students' passion for participation to better their psychological well-being and self-esteem.																	1343-0130	1883-8014				MAY	2020	24	3					307	315		10.20965/jaciii.2020.p0307													
J								Estimation of Search Intents from Query to Context Search Engine	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										search engine; time series data; search intent; learning to rank		This paper estimates users' search intents when using the context search engine (CSE) by analyzing submitted queries. Recently, due to the increase in the amount of information on the Web and the diversification of information needs, the gap between user's information needs and a basic search function provided by existing web search engines becomes larger. As a solution to this problem, the CSE that limits its tasks to answer questions about temporal trends has been proposed. It provides three primitive search functions, which users can use in accordance with their purposes. Furthermore, if the system can estimate users' search intents, it can provide more user-friendly services that contribute the improvement of search efficiency. Aiming at estimating users' search intents only from submitted queries, this paper analyzes the characteristics of queries in terms of typical search intents when using CSE, and defines classification rules. To show the potential use of the estimated search intents, this paper introduces a learning to rank into CSE. Experimental results show that MAP (mean average precision) is improved by learning rank models separately for different search intents.																	1343-0130	1883-8014				MAY	2020	24	3					316	325		10.20965/jaciii.2020.p0316													
J								HEVC Video Encryption Algorithm Based on Integer Dynamic Coupling Tent Mapping	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										HEVC encryption; dynamic tent mapping; coupled map lattice; multi-core parallelization	SELECTIVE ENCRYPTION; RC6	A selective encryption algorithm is proposed to improve the efficiency of high efficiency video coding (HEVC) video encryption and ensure the security of HEVC videos. The algorithm adopts the integer dynamic coupling tent mapping optimization model as the pseudo-random sequence generator, and multi-core parallelization is used as the sequence generation mechanism. The binstrings during the process of context adaptive binary arithmetic coding are selected for encryption, which conforms to the features of invariable binstream and compatible format in terms of video encryption. Performance tests for six types of standard videos with different resolutions were performed. The results indicated that the encryption algorithm has a large key space and benefits from a high encryption effect.																	1343-0130	1883-8014				MAY	2020	24	3					335	345		10.20965/jaciii.2020.p0335													
J								Robot Vision System for Human Detection and Action Recognition	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										robot vision; generic object recognition; real-time image processing; CNN; optical flow		Mobile robots equipped with camera sensors are required to perceive humans and their actions for safe autonomous navigation. For simultaneous human detection and action recognition, the real-time performance of the robot vision is an important issue. In this paper, we propose a robot vision system in which original images captured by a camera sensor are described by the optical flow. These images are then used as inputs for the human and action classifications. For the image inputs, two classifiers based on convolutional neural networks are developed. Moreover, we describe a novel detector (a local search window) for clipping partial images around the target human from the original image. Since the camera sensor moves together with the robot, the camera movement has an influence on the calculation of optical flow in the image, which we address by further modifying the optical flow for changes caused by the camera movement. Through the experiments, we show that the robot vision system can detect humans and recognize the action in real time. Furthermore, we show that a moving robot can achieve human detection and action recognition by modifying the optical flow.																	1343-0130	1883-8014				MAY	2020	24	3					346	356															
J								Improved Model-Free Adaptive Control of Pneumatic Gravity Compensation System	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										pneumatic gravity compensation system; pressure control; frictionless cylinder; improved model-free adaptive control; pseudo gradient identification	TRAJECTORY TRACKING; ROBOTS	A pneumatic gravity compensation system is typically nonlinear in behavior. It is difficult to establish an accurate mathematical model for it, and it is particularly difficult to realize high-precision pressure control. A pneumatic gravity compensation system driven by a frictionless cylinder is built. Considering that the traditional model-free adaptive control is slow for pseudo-gradient identification, an improved model-free adaptive control is proposed to predict the changes in the pseudo gradient and accelerate the process of pseudo gradient identification. The static and dynamic gravity compensation of the pneumatic gravity compensation system is realized. Finally, the experimental results show that the steady-error of step response of the improved model-free adaptive controller is less than 200 Pa, and the rise time is approximately 13 seconds. The sinusoidal tracking error (0.04 Hz) is approximately 1.94 KPa.																	1343-0130	1883-8014				MAY	2020	24	3					357	365		10.20965/jaciii.2020.p0357													
J								Modular Neural Network for Learning Visual Features, Routes, and Operation Through Human Driving Data Toward Automatic Driving System	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										deep learning; automatic driving; modular neural networks; supervised learning		This paper proposes an automatic driving system based on a combination of modular neural networks processing human driving data. Research on automatic driving vehicles has been actively conducted in recent years. Machine learning techniques are often utilized to realize an automatic driving system capable of imitating human driving operations. Almost all of them adopt a large monolithic learning module, as typified by deep learning. However, it is inefficient to use a monolithic deep learning module to learn human driving operations (accelerating, braking, and steering) using the visual information obtained from a human driving a vehicle. We propose combining a series of modular neural networks that independently learn visual feature quantities, routes, and driving maneuvers from human driving data, thereby imitating human driving operations and efficiently learning a plurality of routes. This paper demonstrates the effectiveness of the proposed method through experiments using a small vehicle.																	1343-0130	1883-8014				MAY	2020	24	3					368	376		10.20965/jaciii.2020.p0368													
J								Support System for Teachers in Communication with Educational Support Robot	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										robot service network protocol (RSNP); support system; educational support robot; internet		With the growth of robot technology, more educational-support robots, which support learning, are paid attention to. For example, one robot supports the school life of students. Another robot helps students to learn English better. Most researches have focused on robot behavior and investigating the effect. Previous research has reported that a society in which robots and humans learn together will soon be a reality. If a society where robots and humans learn side by side is realized, children will be in houses where they will learn alongside multiple unspecified robots. We think that perspectives of third parties, such as educators and guardians, are important for further improvements in the field. Thus, we think that a system is necessary wherein third parties can direct robots to provide suitable learning support to learners. This paper proposes a system for teachers that can direct robots to provide suitable learning support to learners, who simultaneously can grasp their learning conditions as they study alongside robots.																	1343-0130	1883-8014				MAY	2020	24	3					377	385		10.20965/jaciii.2020.p0377													
J								Operation Skill Acquisition and Fuzzy-Rule Extraction for Drone Control Based on Visual Information Using Deep Learning	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										operation skill; drone control; deep learning; CNN; visual information		In recent years, much research on the unmanned control of a moving vehicle has been conducted, and various robots and motor vehicles moving automatically are being used. However, the more complicated the environment is, the more difficult it is for the autonomous vehicles to move automatically. Even in such a challenging environment, however, an expert with the necessary operation skill can sometimes perform the appropriate control of the moving vehicle. In this research, a method for learning a human's operation skill using a convolutional neural network (CNN) and setting visual information for input is proposed for learning more complicated environmental information. A CNN is a kind of deep-learning network, and it exhibits high performance in the field of image recognition. In this experiment, the operation knowledge was also visualized using a fuzzy neural network with obtained input-output maps to create fuzzy rules. To verify the effectiveness of this method, an experiment involving operation skill acquisition by some subjects using a drone control simulator was conducted.																	1343-0130	1883-8014				MAY	2020	24	3					386	395		10.20965/jaciii.2020.p0386													
J								Learning Effect of Collaborative Learning with Robots Speaking a Compliment	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										human-robot-interaction; robot; collaborative learning; compliment		With advances in robotic technology, more robots are being designed to support learning. Most studies have focused on robot behavior and investigated their effects. However, few have studied the compliments given by robots. It is not known how such compliments affect learning and motivation. Therefore, this study investigates the effects of collaborative learning with a robot that delivers compliments. We conducted an experiment to compare the learning effects across three groups. In one group, the learners studied with a robot that praised them using onomatopoeias. In the second group, the learners studied with a robot that praised them using adjectives. In the last group, the learners learned with a robot that praised them without using onomatopoeias or adjectives (original text). The results of this study suggest that collaborative learning with a robot that encourages learners using the original text or onomatopoeias is more effective.																	1343-0130	1883-8014				MAY	2020	24	3					396	403		10.20965/jaciii.2020.p0396													
J								Do You Forgive Past Mistakes of Animated Agents? A Study of Instances of Assistance by Animated Agents	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										animated agent; human-agent interaction; user interfaces; appearance; order effect	PRIMACY-RECENCY; LAST DANCE; ORDER; APPEARANCE; DECISION; IMPRESSIONS; COPRESENCE; BEHAVIOR; IMPACT; SAVE	Many studies on human-computer interaction have demonstrated that the visual appearance of an agent or a robot significantly influences people's perceptions and behaviors. Several studies on the appearance of agents/robots have concluded that consistency between expectations from an agent's or a robot's appearance and performances was an important factor to the continuous use of these agents/robots. This is because users would stop interacting with the agents/robots when predictions are not met by actual experiences. However, previous studies mainly focused on the consistency between an initial expectation and a performance of a single instance of a task. The influence of the orders of successes or failures for more than one instance of a task has not been examined in detail. Therefore, in this study, we investigate the order effects of how the timing of sufficient or insufficient results of animated agents affects user evaluation. This will lead to the contribution to fill the lack of studies regarding more than one task in the field of human-computer interaction and to realize the continuous use of agents/robots as long as possible and to avoid stopping to use the agents/robots owing to their successful design. We create a simulated retrieval website and conduct an experiment using retrieval assistant agents that show both sufficient and insufficient results for more than one instance of retrieval tasks. The experimental results demonstrated a recency effect wherein the users significantly revised their evaluations of the animated agents based on new information more than that based on initial evaluations. The investigation of the case of repeated instances of a task and the influence of successes or failures is important for designing intelligent agents that may show incomplete results in intelligent tasks. Furthermore, the result of this study will contribute to build strategies to design behaviors of agents/robots that have a high or low evaluation based on their appearance in advance to prevent users from stopping use of the agents/robots.																	1343-0130	1883-8014				MAY	2020	24	3					404	412															
J								Fuzzy-Integral Based Estimate of Vertical-Direction Error Caused by Pointing Fingers at Objects	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										fuzzy integral; inclusion-exclusion integral; regression analysis; finger pointing; human symbiotic system	ROBOT	There have been recent attempts to control home electric appliances and devices using robots. Information can be shared with robots by using finger pointing. Finger pointing is used as a means of communication with people around. However, when a person points at an object with a finger, position of the object cannot be indicated accurately. In this work, we studied the error between a target point, which a person tries to point at with a finger, and an observation point, which is actually pointed at. We also proposed an error estimation model using a fuzzy integral to estimate and correct the error at the observation point.																	1343-0130	1883-8014				MAY	2020	24	3					413	421		10.20965/jaciii.2020.p0413													
J								Study on Development of Humor Discriminator for Dialogue System	JOURNAL OF ADVANCED COMPUTATIONAL INTELLIGENCE AND INTELLIGENT INFORMATICS										automatic dialogue system; non task oriented; humor discriminator		Studies on automatic dialogue systems, which allow people and computers to communicate with each other using natural language, have been attracting attention. In particular, the main objective of a non-task-oriented dialogue system is not to achieve a specific task but to amuse users through chat and free dialogue. For this type of dialogue system, continuity of the dialogue is important because users can easily get tired if the dialogue is monotonous. On the other hand, preceding studies have shown that speech with humorous expressions is effective in improving the continuity of a dialogue. In this study, we developed a computer-based humor discriminator to perform user- or situation-independent objective discrimination of humor. Using the humor discriminator, we also developed an automatic humor generation system and conducted an evaluation experiment with human subjects to test the generated jokes. A t-test on the evaluation scores revealed a significant difference (P value: 3.5x10(-5)) between the proposed and existing methods of joke generation.																	1343-0130	1883-8014				MAY	2020	24	3					422	435															
J								Multi-robot information driven path planning under communication constraints	AUTONOMOUS ROBOTS										Communications constraints; Information-driven path planning; Multi-robot informed path planning	EXPLORATION	We consider the problem of simultaneous exploration and information collection in an initially unknown environment by multiple autonomous robots when the communication between robots is unreliable and intermittent. We propose a novel algorithm where decisions to select locations for exploration and information collection are guided by a utility function that combines Gaussian Process-based distributions for information entropy and communication signal strength, along with a distributed coordination protocol to avoid path conflicts between robots and repeated exploration and information collection from the same region by different robots. Our proposed algorithm was experimentally validated in simulation and on hardware Clearpath Jackal robots while using a realistic signal loss model from the literature. Our results show that our approach plans it samples such that it receives up to 25 dBm more signal strength throughout navigation compared to approaches that do not consider communications when selecting locations to sample from while accomplishing similar levels of model error.																	0929-5593	1573-7527				MAY	2020	44	5					721	737		10.1007/s10514-019-09890-z													
J								On-line object detection: a robotics challenge	AUTONOMOUS ROBOTS										Computer vision; Robot vision; Object detection; Deep learning; Humanoid robots	FOCAL LOSS	Object detection is a fundamental ability for robots interacting within an environment. While stunningly effective, state-of-the-art deep learning methods require huge amounts of labeled images and hours of training which does not favour such scenarios. This work presents a novel pipeline resulting from integrating (Maiettini et al. in 2017 IEEE-RAS 17th international conference on humanoid robotics (Humanoids), 2017) and (Maiettini et al. in 2018 IEEE/RSJ international conference on intelligent robots and systems (IROS), 2018), which naturally trains a robot to detect novel objects in few seconds. Moreover, we report on an extended empirical evaluation of the learning method, justifying that the proposed hybrid architecture is key in leveraging powerful deep representations while maintaining fast training time of large scale Kernel methods. We validate our approach on the Pascal VOC benchmark (Everingham et al. in Int J Comput Vis 88(2): 303-338, 2010), and on a challenging robotic scenario (iCubWorld Transformations (Pasquale et al. in Rob Auton Syst 112:260-281, 2019). We address real world use-cases and show how to tune the method for different speed/accuracy trades-off. Lastly, we discuss limitations and directions for future development.																	0929-5593	1573-7527				MAY	2020	44	5					739	757		10.1007/s10514-019-09894-9													
J								Adaptive force and velocity control based on intrinsic contact sensing during surface exploration of dynamic objects	AUTONOMOUS ROBOTS										Haptic; Autonomous control; Fuzzy logic	HAPTIC EXPLORATION; PERCEPTION	Haptic exploration is a process of using haptic feedback to interact and perceive an unknown object. It is an essential approach to understand the physical and geometrical properties of the object. While numerous research has been carried out for haptic exploration on static objects, haptic exploration on objects with dynamic movements has not been reported. It is due to the significant challenges to achieve robust force and velocity control when the object is nonstationary. In this work, a novel adaptive force and velocity control algorithm based on intrinsic contact sensing (ICS) for haptic surface exploration of dynamic objects is presented. A fuzzy-logic control framework making use of the information obtained from ICS has been developed. To validate the proposed control algorithm, extensive surface exploration experiments have been carried out on objects with different surface properties, geometries, stiffness, and concave or convex patterns. The validation results demonstrate the high accuracy and robustness of the proposed algorithm using different experimental platforms.																	0929-5593	1573-7527				MAY	2020	44	5					773	790		10.1007/s10514-019-09896-7													
J								Joint optimization based on direct sparse stereo visual-inertial odometry	AUTONOMOUS ROBOTS										Direct sparse odometry; IMU pre-integration; Sliding window optimization; 3D reconstruction		This paper proposes a novel fusion of an inertial measurement unit (IMU) and stereo camera method based on direct sparse odometry (DSO) and stereo DSO. It jointly optimizes all model parameters within a sliding window, including the inverse depth of all selected pixels and the internal or external camera parameters of all keyframes. The vision part uses a photometric error function that optimizes 3D geometry and camera pose in a combined energy functional. The proposed algorithm uses image blocks to extract neighboring image features and directly forms measurement residuals in the image intensity space. A fixed-baseline stereo camera solves scale drift. IMU information is accumulated between several frames using manifold pre-integration and is inserted into the optimization as additional constraints between keyframes. The scale and gravity inserted are incorporated into the stereo visual inertial odometry model and are optimized together with other variables such as poses. The experimental results show that the tracking accuracy and robustness of the proposed method are superior to those of the state-of-the-art fused IMU method. In addition, compared with previous semi-dense direct methods, the proposed method displays a higher reconstruction density and scene recovery.																	0929-5593	1573-7527				MAY	2020	44	5					791	809		10.1007/s10514-019-09897-6													
J								Relative navigation of autonomous GPS-degraded micro air vehicles	AUTONOMOUS ROBOTS										Aerial robotics; GPS-denied; Navigation; GPS-degraded; Observable	POSE GRAPHS; LARGE-SCALE; VISION; INDOOR; ENVIRONMENTS; NETWORKS; FUSION; FLIGHT; TIME	Unlike many current navigation approaches for micro air vehicles, the relative navigation (RN) framework presented in this paper ensures that the filter state remains observable in GPS-denied environments by working with respect to a local reference frame. By subtly restructuring the problem, RN ensures that the filter uncertainty remains bounded, consistent, and normally-distributed, and insulates flight-critical estimation and control processes from large global updates. This paper thoroughly outlines the RN framework and demonstrates its practicality with several long flight tests in unknown GPS-denied and GPS-degraded environments. The relative front end is shown to produce low-drift estimates and smooth, stable control while leveraging off-the-shelf algorithms. The system runs in real time with onboard processing, fuses a variety of vision sensors, and works indoors and outdoors without requiring special tuning for particular sensors or environments. RN is shown to produce globally-consistent, metric, and localized maps by incorporating loop closures and intermittent GPS measurements.																	0929-5593	1573-7527				MAY	2020	44	5					811	830		10.1007/s10514-019-09899-4													
J								Graph search of a moving ground target by a UAV aided by ground sensors with local information	AUTONOMOUS ROBOTS										Pursuit evasion; Partial information; Graph search; Brand and bound	ROBBER	The optimal control of a UAV searching for a target moving, with known constant speed, on a road network and heading toward one of many goal locations is considered. To aid the UAV, some roads in the network are instrumented with unattended ground sensors (UGSs) that detect the target's motion and record the time it passes by the UGS. When the UAV flies over an UGS location, this time stamped information, if available, is communicated to it. At time 0, the target enters the road network and selects a path leading to one of the exit nodes. The UAV also arrives at the same entry UGS after some delay and is thus informed about the presence of the target in the network. The UAV has no on-board sensing capability and so, capture entails the UAV and target being colocated at an UGS location. If this happens, the UGS is triggered and this information is instantaneously relayed to the UAV, thereby enabling capture. On the other hand, if the target reaches an exit node without being captured, he is deemed to have escaped. We transform the road network, which is restricted to a directed acyclic graph, into a time tree whose node is a tuple comprising the UGS location and evader arrival time at that location. For a given initial delay, we present a recursive forward search method that computes the minimum capture time UAV pursuit policy, under worst-case target action. The recursion scales poorly in the problem parameters, i.e., number of nodes in the time tree and number of evader paths. We present a novel branch and bound technique and a pre-processing step that is experimentally shown to reduce the computational burden by at least two orders of magnitude. We illustrate the applicability of the proposed pruning methods, which result in no loss in optimality, on a realistic example road network.																	0929-5593	1573-7527				MAY	2020	44	5					831	843		10.1007/s10514-019-09900-0													
J								High-dimensional model recovery from random sketched data by exploring intrinsic sparsity	MACHINE LEARNING										Classification; Regression; Large-scale; High dimension; Sparsity; Randomized reduction; JL transform	JOHNSON-LINDENSTRAUSS; RANDOM PROJECTIONS; RISK PREDICTION; SELECTION; CLASSIFICATION; KERNELS; ALGORITHMS; REGRESSION; MARGINS; LASSO	Learning from large-scale and high-dimensional data still remains a computationally challenging problem, though it has received increasing interest recently. To address this issue, randomized reduction methods have been developed by either reducing the dimensionality or reducing the number of training instances to obtain a small sketch of the original data. In this paper, we focus on recovering a high-dimensional classification/regression model from random sketched data. We propose to exploit the intrinsic sparsity of optimal solutions and develop novel methods by increasing the regularization parameter before the sparse regularizer. In particular, (i) for high-dimensional classification problems, we leverage randomized reduction methods to reduce the dimensionality of data and solve a dual formulation on the random sketched data with an introduced sparse regularizer on the dual solution; (ii) for high-dimensional sparse least-squares regression problems, we employ randomized reduction methods to reduce the scale of data and solve a formulation on the random sketched data with an increased regularization parameter before the sparse regularizer. For both classes of problems, by exploiting the intrinsic sparsity of the optimal dual solution or the optimal primal solution we provide formal theoretical guarantee on the recovery error of learned models in comparison with the optimal models that are learned from the original data. Compared with previous studies on randomized reduction for machine learning, the present work enjoy several advantages: (i) the proposed formulations enjoys intuitive geometric explanations; (ii) the theoretical guarantee does not rely on any stringent assumptions about the original data (e.g., low-rankness of the data matrix or the data are linearly separable); (iii) the theory covers both smooth and non-smooth loss functions for classification; (iv) the analysis is applicable to a broad class of randomized reduction methods as long as the reduction matrices admit the Johnson-Lindenstrauss type of lemma. We also present empirical studies to support the proposed methods and the presented theory.																	0885-6125	1573-0565				MAY	2020	109	5					899	938		10.1007/s10994-019-05865-4													
J								Detecting anomalous packets in network transfers: investigations using PCA, autoencoder and isolation forest in TCP	MACHINE LEARNING										PCA; Autoencoders; Isolation forest; Network traffic		Large-scale scientific workflows rely heavily on high-performance file transfers. These transfers require strict quality parameters such as guaranteed bandwidth, no packet loss or data duplication. To have successful file transfers, methods such as predetermined thresholds and statistical analysis need to be done to determine abnormal patterns. Network administrators routinely monitor and analyze network data for diagnosing and alleviating these, making decisions based on their experience. However, as networks grow and become complex, monitoring large data files and quickly processing them, makes it improbable to identify errors and rectify these. Abnormal file transfers have been classified by simply setting alert thresholds, via tools such as PerfSonar and TCP statistics (Tstat). This paper investigates the feasibility of unsupervised feature extraction methods for identifying network anomaly patterns with three unsupervised classification methods-principal component analysis, autoencoder and isolation forest. We collect file transfer statistics from two experiment sets-synthetic iPerf generated traffic and 1000 Genome workflow runs, with synthetically introduced anomalies. Our results show that while PCA and a simple autoencoder finds it difficult to detect clusters, the tree-variant isolation forest is able to identify anomalous packets by breaking down TCP traces into tree classes early.																	0885-6125	1573-0565				MAY	2020	109	5					1127	1143		10.1007/s10994-020-05870-y													
J								CEREBRUM: a fast and fully-volumetric Convolutional Encoder-decodeR for weakly-supervised sEgmentation of BRain strUctures from out-of-the-scanner MRI	MEDICAL IMAGE ANALYSIS										Brain MRI segmentation; Convolutional neural networks; Weakly supervised learning; 3D Image analysis	DEEP; IMAGES	Many functional and structural neuroimaging studies call for accurate morphometric segmentation of different brain structures starting from image intensity values of MRI scans. Current automatic (multi-) atlas-based segmentation strategies often lack accuracy on difficult-to-segment brain structures and, since these methods rely on atlas-to-scan alignment, they may take long processing times. Alternatively, recent methods deploying solutions based on Convolutional Neural Networks (CNNs) are enabling the direct analysis of out-of-the-scanner data. However, current CNN-based solutions partition the test volume into 2D or 3D patches, which are processed independently. This process entails a loss of global contextual information, thereby negatively impacting the segmentation accuracy. In this work, we design and test an optimised end-to-end CNN architecture that makes the exploitation of global spatial information computationally tractable, allowing to process a whole MRI volume at once. We adopt a weakly supervised learning strategy by exploiting a large dataset composed of 947 out-of-the-scanner (3 Tesla T1-weighted 1 mm isotropic MP-RAGE 3D sequences) MR Images. The resulting model is able to produce accurate multi-structure segmentation results in only a few seconds. Different quantitative measures demonstrate an improved accuracy of our solution when compared to state-of-the-art techniques. More-over, through a randomised survey involving expert neuroscientists, we show that subjective judgements favour our solution with respect to widely adopted atlas-based software. (C) 2020 The Authors. Published by Elsevier B.V.																	1361-8415	1361-8423				MAY	2020	62								101688	10.1016/j.media.2020.101688													
J								Automatic labeling of cortical sulci using patch- or CNN-based segmentation techniques combined with bottom-up geometric constraints	MEDICAL IMAGE ANALYSIS										Convolutional neural network; Multi-atlas segmentation; Cortical sulci labeling	OPTIMIZED PATCHMATCH; NEURAL-NETWORKS; REGISTRATION; RECOGNITION	The extreme variability of the folding pattern of the human cortex makes the recognition of cortical sulci, both automatic and manual, particularly challenging. Reliable identification of the human cortical sulci in its entirety, is extremely difficult and is practiced by only a few experts. Moreover, these sulci correspond to more than a hundred different structures, which makes manual labeling long and fastidious and therefore limits access to large labeled databases to train machine learning. Here, we seek to improve the current model proposed in the Morphologist toolbox, a widely used sulcus recognition toolbox included in the BrainVISA package. Two novel approaches are proposed: patch-based multi-atlas segmentation (MAS) techniques and convolutional neural network (CNN)-based approaches. Both are currently applied for anatomical segmentations because they embed much better representations of inter-subject variability than approaches based on a single template atlas. However, these methods typically focus on voxel-wise labeling, disregarding certain geometrical and topological properties of interest for sulcus morphometry. Therefore, we propose to refine these approaches with domain specific bottom-up geometric constraints provided by the Morphologist toolbox. These constraints are utilized to provide a single sulcus label to each topologically elementary fold, the building blocks of the pattern recognition problem. To eliminate the shortcomings associated with the Morphologist's pre-segmentation into elementary folds, we complement this regularization scheme using a top-down perspective which triggers an additional cleavage of the elementary folds when required. All the newly proposed models outperform the current Morphologist model, the most efficient being a CNN U-Net-based approach which carries out sulcus recognition within a few seconds. (C) 2020 The Authors. Published by Elsevier B.V.																	1361-8415	1361-8423				MAY	2020	62								101651	10.1016/j.media.2020.101651													
J								Deep learning uncertainty and confidence calibration for the five-class polyp classification from colonoscopy	MEDICAL IMAGE ANALYSIS										Polyp classification; Deep learning; Model calibration; Classification uncertainty; Bayesian learning; Bayesian inference	SOCIETY TASK-FORCE; COLORECTAL-CANCER; VALIDATION; DIAGNOSIS; SYSTEM	There are two challenges associated with the interpretability of deep learning models in medical image analysis applications that need to be addressed: confidence calibration and classification uncertainty. Confidence calibration associates the classification probability with the likelihood that it is actually correct - hence, a sample that is classified with confidence X% has a chance of X% of being correctly classified. Classification uncertainty estimates the noise present in the classification process, where such noise estimate can be used to assess the reliability of a particular classification result. Both confidence calibration and classification uncertainty are considered to be helpful in the interpretation of a classification result produced by a deep learning model, but it is unclear how much they affect classification accuracy and calibration, and how they interact. In this paper, we study the roles of confidence calibration (via post-process temperature scaling) and classification uncertainty (computed either from classification entropy or the predicted variance produced by Bayesian methods) in deep learning models. Results suggest that calibration and uncertainty improve classification interpretation and accuracy. This motivates us to propose a new Bayesian deep learning method that relies both on calibration and uncertainty to improve classification accuracy and model interpretability. Experiments are conducted on a recently proposed five-class polyp classification problem, using a data set containing 940 high-quality images of colorectal polyps, and results indicate that our proposed method holds the state-of-the-art results in terms of confidence calibration and classification accuracy. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				MAY	2020	62								101653	10.1016/j.media.2020.101653													
J								Embedding high-dimensional Bayesian optimization via generative modeling: Parameter personalization of cardiac electrophysiological models	MEDICAL IMAGE ANALYSIS										High-dimensional Bayesian optimization; variational autoencoder; personalized modeling	HEART; ABLATION	The estimation of patient-specific tissue properties in the form of model parameters is important for personalized physiological models. Because tissue properties are spatially varying across the underlying geometrical model, it presents a significant challenge of high-dimensional (HD) optimization at the presence of limited measurement data. A common solution to reduce the dimension of the parameter space is to explicitly partition the geometrical mesh. In this paper, we present a novel concept that uses a generative variational auto-encoder (VAE) to embed HD Bayesian optimization into a low-dimensional (LD) latent space that represents the generative code of HD parameters. We further utilize VAE-encoded knowledge about the generative code to guide the exploration of the search space. The presented method is applied to estimating tissue excitability in a cardiac electrophysiological model in a range of synthetic and real-data experiments, through which we demonstrate its improved accuracy and substantially reduced computational cost in comparison to existing methods that rely on geometry-based reduction of the HD parameter space. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				MAY	2020	62								101670	10.1016/j.media.2020.101670													
J								Stochastic Gradient Langevin dynamics for joint parameterization of tracer kinetic models, input functions, and T1 relaxation-times from undersampled k-space DCE-MRI	MEDICAL IMAGE ANALYSIS										Stochastic optimization; Model-based reconstruction; Dynamic contrast enhanced magnetic resonance imaging	CONTRAST-ENHANCED MRI; PARAMETRIC-ESTIMATION; LINEAR-MODELS; SELECTION; REPRODUCIBILITY; RECONSTRUCTION; DECOMPOSITION; ACCURACY; MOTION	Dynamic Contrast Enhanced (DCE) Magnetic Resonance Imaging (MRI) is an important diagnostic technique that can quantify the structure and function of microvasculature processes, using T1 relaxation times and tracer kinetic maps. However, a series of methodological limitations affect both the accuracy and standardisation of the quantified maps, and consequently their diagnostic ability. The main methodological challenge in the quantification of tracer kinetics is a multi-parameter optimization, with correlated parameters that have different scales, which results in local minima particularly when measurements are highly undersampled. This work suggests a novel data driven optimization scheme, based on a variation of the Stochastic Gradient Langevin dynamics (SGLD) Markov chain Monte Carlo algorithm, which combines stochastic gradient descent and Langevin dynamics. The proposed SGDL algorithm avoided local minima and accurately quantified proton density, T1 relaxation times and tracer kinetics. Joint direct parameterization significantly benefited the quantification of proton density, T1 relaxation times, and the selection of a suitable tracer kinetic model per tissue type. Model based arterial and portal vein input functions were automatically determined during the joint direct parameterization. Observations made on simulated fully and highly undersampled liver DCE MRI data were confirmed on acquired clinical data. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				MAY	2020	62								101690	10.1016/j.media.2020.101690													
J								An integrated deep learning framework for joint segmentation of blood pool and myocardium	MEDICAL IMAGE ANALYSIS										Blood pool; Myocardium; Magnetic resonance image; Segmentation	MR-IMAGES; MODEL; SHAPE; NETWORK	Simultaneous and automatic segmentation of the blood pool and myocardium is an important precondition for early diagnosis and pre-operative planning in patients with complex congenital heart disease. However, due to the high diversity of cardiovascular structures and changes in mechanical properties caused by cardiac defects, the segmentation task still faces great challenges. To overcome these challenges, in this study we propose an integrated multi-task deep learning framework based on the dilated residual and hybrid pyramid pooling network (DRHPPN) for joint segmentation of the blood pool and myocardium. The framework consists of three closely connected progressive sub-networks. An inception module is used to realize the initial multi-level feature representation of cardiovascular images. A dilated residual network (DRN), as the main body of feature extraction and pixel classification, preliminary predicts segmentation regions. A hybrid pyramid pooling network (HPPN) is designed for facilitating the aggregation of local information to global information, which complements DRN. Extensive experiments on three-dimensional cardiovascular magnetic resonance (CMR) images (the available dataset of the MICCAI 2016 HVSMR challenge) demonstrate that our approach can accurately segment the blood pool and myocardium and achieve competitive performance compared with state-of-the-art segmentation methods. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				MAY	2020	62								101685	10.1016/j.media.2020.101685													
J								Synthesized 7T MRI from 3T MRI via deep learning in spatial and wavelet domains	MEDICAL IMAGE ANALYSIS										Image synthesis; Magnetic resonance imaging (MRI); Spatial and wavelet domains	SINGLE-IMAGE SUPERRESOLUTION; CONVOLUTIONAL NEURAL-NETWORK; REGISTRATION; ENHANCEMENT	Ultra-high field 7T MRI scanners, while producing images with exceptional anatomical details, are cost prohibitive and hence highly inaccessible. In this paper, we introduce a novel deep learning network that fuses complementary information from spatial and wavelet domains to synthesize 7T T1-weighted images from their 3T counterparts. Our deep learning network leverages wavelet transformation to facilitate effective multi-scale reconstruction, taking into account both low-frequency tissue contrast and high-frequency anatomical details. Our network utilizes a novel wavelet-based affine transformation (WAT) layer, which modulates feature maps from the spatial domain with information from the wavelet domain. Extensive experimental results demonstrate the capability of the proposed method in synthesizing high-quality 7T images with better tissue contrast and greater details, outperforming state-of-the-art methods. (C) 2020 Published by Elsevier B.V.																	1361-8415	1361-8423				MAY	2020	62								101663	10.1016/j.media.2020.101663													
J								Trophectoderm segmentation in human embryo images via inceptioned U-Net	MEDICAL IMAGE ANALYSIS										Trophectoderm segmentation; Deep learning; Medical image analysis; IVF; Human embryo	AUTOMATIC SEGMENTATION; QUALITY ASSESSMENT	Trophectoderm (TE) is one of the main components of a day-5 human embryo (blastocyst) that correlates with the embryo's quality. Precise segmentation of TE is an important step toward achieving automatic human embryo quality assessment based on morphological image features. Automatic segmentation of TE, however, is a challenging task and previous work on this is quite limited. In this paper, four fully convolutional deep models are proposed for accurate segmentation of trophectoderm in microscopic images of the human blastocyst. In addition, a multi-scaled ensembling method is proposed that aggregates five models trained at various scales offering trade-offs between the quantity and quality of the spatial information. Furthermore, synthetic embryo images are generated for the first time to address the lack of data in training deep learning models. These synthetically generated images are proven to be effective to fill the generalization gap in deep learning when limited data is available for training. Experimental results confirm that the proposed models are capable of segmenting TE regions with an average Precision, Recall, Accuracy, Dice Coefficient and Jaccard Index of 83.8%, 90.1%, 96.9%, 86.61% and 76.71%, respectively. Particularly, the proposed Inceptioned U-Net model outperforms state-of-the-art by 10.3% in Accuracy, 9.3% in Dice Coefficient and 13.7% in Jaccard Index. Further experiments are conducted to highlight the effectiveness of the proposed models compared to some recent deep learning based segmentation methods. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				MAY	2020	62								101612	10.1016/j.media.2019.101612													
J								Joint PET-MRI image reconstruction using a patch-based joint-dictionary prior	MEDICAL IMAGE ANALYSIS										Simultaneous PET-MRI; Joint reconstruction; Joint dictionary; Sparsity; Undersampled k-space; Markov random field; Expectation maximization	INFORMATION; QUALITY	For simultaneous positron-emission-tomography and magnetic-resonance-imaging (PET-MRI) systems, while early methods relied on independently reconstructing PET and MRI images, recent works have demonstrated improvement in image reconstructions of both PET and MRI using joint reconstruction methods. The current state-of-the-art joint reconstruction priors rely on fine-scale PET-MRI dependencies through the image gradients at corresponding spatial locations in the PET and MRI images. In the general context of image restoration, compared to gradient-based models, patch-based models (e.g., sparse dictionaries) have demonstrated better performance by modeling image texture better. Thus, we propose a novel joint PET-MRI patch-based dictionary prior that learns inter-modality higher-order dependencies together with intra-modality textural patterns in the images. We model the joint-dictionary prior as a Markov random field and propose a novel Bayesian framework for joint reconstruction of PET and accelerated-MRI images, using expectation maximization for inference. We evaluate all methods on simulated brain datasets as well as on in vivo datasets. We compare our joint dictionary prior with the recently proposed joint priors based on image gradients, as well as independently applied patch-based priors. Our method demonstrates qualitative and quantitative improvement over the state of the art in both PET and MRI reconstructions. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				MAY	2020	62								101669	10.1016/j.media.2020.101669													
J								SDAE-GAN: Enable high-dimensional pathological images in liver cancer survival prediction with a policy gradient based data augmentation method	MEDICAL IMAGE ANALYSIS										Computer-aided diagnosis; Imaging and non-Imaging biomarkers; Integration; Cancer survival prediction	CELLS	High-dimensional pathological images produced by Immunohistochemistry (IHC) methods consist of many pathological indexes, which play critical roles in cancer treatment planning. However, these indexes currently cannot be utilized in survival prediction because joining them with patients' clinicopathological features (e.g., age and tumor size) is challenging due to their high dimension and sparse features. To address this problem, we propose a novel two-stage survival prediction model named ICSPM to join the IHC images and clinicopathological features. For the first stage, our proposed SDAE-GAN compresses high-dimensional IHC images to flat, compact and representative feature vectors by compressing and reconstructing them. For the first time, SDAE-GAN integrates dense blocks, the stacked auto-encoder and the GAN architecture to maximize the ability to detect patterns in IHC images. In addition, we propose a novel policy gradient based data augmentation method to involve the diversity in IHC images without breaking patterns inside them. For the second stage, ICSPM adopts a DenseNet to join feature vectors and clinicopathological features for survival prediction. Experimental results demonstrate that ICSPM reached a state-of-the-art prediction accuracy of 0.72 on the five-year survival. ICSPM is the first work to enable high-dimensional IHC images in cancer survival prediction. We prove that high-dimensional IHC images and clinicopathological features provide valuable and complementary information in survival prediction. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				MAY	2020	62								101640	10.1016/j.media.2020.101640													
J								Contrast agent-free synthesis and segmentation of ischemic heart disease images using progressive sequential causal GANs	MEDICAL IMAGE ANALYSIS										Gadolinium contrast agents; Synthesis; Sequential learning; Ischemic heart disease; Progressive framework	CARDIOVASCULAR MAGNETIC-RESONANCE; MYOCARDIAL-INFARCTION; ENHANCEMENT; MOTION; MEDIA	The elimination of gadolinium contrast agent (CA) injections and manual segmentation are crucial for ischemic heart disease (IHD) diagnosis and treatment. In the clinic, CA-based late gadolinium enhancement (LGE) imaging and manual segmentation remain subject to concerns about potential toxicity, interobserver variability, and ineffectiveness. In this study, progressive sequential causal GANs (PSCGAN) are proposed. This is the first one-stop CA-free IHD technology that can simultaneously synthesize an LGE-equivalent image and segment diagnosis-related tissues (i.e., scars, healthy myocardium, blood pools, and other pixels) from cine MR images. To this end, the PSCGAN offer three unique properties: 1) a progressive framework that cascades three phases (i.e., priori generation, conditional synthesis, and enhanced segmentation) for divide-and-conquer training synthesis and segmentation of images. Importantly, this framework leverages the output of the previous phase as a priori condition to input the next phase and guides its training for enhancing performance, 2) a sequential causal learning network (SCLN) that creates a multi-scale, two-stream pathway and a multi-attention weighing unit to extract spatial and temporal dependencies from cine MR images and effectively select task-specific dependence. It also integrates the GAN architecture to leverage adversarial training to further facilitate the learning of interest dependencies of the latent space of cine MR images in all phases; and 3) two specifically designed self-learning loss terms: a synthetic regularization loss term leverages the spare regularization to avoid noise during synthesis, and a segmentation auxiliary loss term leverages the number of pixels for each tissue to compensate for discrimination during segmentation. Thus, the PSCGAN gain unprecedented performance while stably training in both synthesis and segmentation. By training and testing a total of 280 clinical subjects, our PSCGAN yield a synthetic normalization root-mean-squared-error of 0.14 and an overall segmentation accuracy of 97.17%. It also produces a 0.96 correlation coefficient for the scar ratio in a real diagnostic metric evaluation. These results proved that our method is able to offer significant assistance in the standardized assessment of cardiac disease. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				MAY	2020	62								101668	10.1016/j.media.2020.101668													
J								Unsupervised X-ray image segmentation with task driven generative adversarial networks	MEDICAL IMAGE ANALYSIS										Unsupervised domain adaptation; X-Ray image segmentation; Generative adversarial networks; Task driven modeling; Image-to-image networks	CHEST RADIOGRAPHS	Semantic parsing of anatomical structures in X-ray images is a critical task in many clinical applications. Modern methods leverage deep convolutional networks, and generally require a large amount of labeled data for model training. However, obtaining accurate pixel-wise labels on X-ray images is very challenging due to the appearance of anatomy overlaps and complex texture patterns. In comparison, labeled CT data are more accessible since organs in 3D CT scans preserve clearer structures and thus can be easily delineated. In this paper, we propose a model framework for learning automatic X-ray image parsing from labeled 3D CT scans. Specifically, a Deep Image-to-Image network (DI2I) for multi-organ segmentation is first trained on X-ray like Digitally Reconstructed Radiographs (DRRs) rendered from 3D CT volumes. Then we build a Task Driven Generative Adversarial Network (TD-GAN) to achieve simultaneous synthesis and parsing for unseen real X-ray images. The entire model pipeline does not require any annotations from the X-ray image domain. In the numerical experiments, we validate the proposed model on over 800 DRRs and 300 topograms. While the vanilla DI2I trained on DRRs without any adaptation fails completely on segmenting the topograms, the proposed model does not require any topogram labels and is able to provide a promising average dice of 86% which achieves the same level of accuracy as results from supervised training (89%). Furthermore, we also demonstrate the generality of TD-GAN through quantatitive and qualitative study on widely used public dataset. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				MAY	2020	62								101664	10.1016/j.media.2020.101664													
J								CoEvil: A Coevolutionary Model for Crime Inference Based on Fuzzy Rough Feature Selection	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Coevolutionary model (COEVIL); crime inference; fuzzy rough set; large-scale heterogeneous data; spatiotemporal	POINT-PROCESSES; REDUCTION	Millions of crimes arise each year, which threatens public safety and harms the victims. Precise crime inference is of great significance in preventing crimes. The sharply increasing large-scale heterogeneous data provide a chance to reveal the patterns and trends in crimes. Several approaches employing feature-based regression or spatiotemporal distribution fitting are proposed but lack of some considerations: 1) ignore the dynamic mutual influences among crimes and locations; and 2) overlook the large scale, incompleteness, uncertainty, and vagueness in the heterogeneous data. This article comprehensively investigates the reliability and applicability of proposing a coevolutionary model to formulate the interaction pattern among the crimes and locations and develops a fuzzy-rough-set-based feature selection method to discover the distinctiveness and permanence properties of the crimes and locations with different latent features. Extensive experiments show that our algorithm achieves the mean absolute error of 1.529 (hour) in the crime time inference and the accuracy of 0.653 and 0.633 in the crime type and location inference, which surpass the state of the arts more than 6.5, 1.9, and 1.8 times, respectively. Additional experiments on different parameter settings of our model are provided to further explore its effectiveness and scalability.																	1063-6706	1941-0034				MAY	2020	28	5					806	817		10.1109/TFUZZ.2019.2939957													
J								Fuzzy Rough Attribute Reduction for Categorical Data	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Rough sets; Data models; Computational modeling; Uncertainty; Numerical models; Approximation algorithms; Feature extraction; Attribute reduction; categorical data; fuzzy rough set (FFRS); rough approximation	UNCERTAINTY MEASURES; DECISION-MAKING; SET; APPROXIMATION; SELECTION	Classical rough set theory is considered a useful tool for dealing with the uncertainty of categorical data. The major deficiency of this model is that the classical rough set model is sensitive to noise in classification learning due to the stringent condition of equivalence relation. Thus, a class of fuzzy similarity relations was introduced to describe the similarity between samples with categorical attributes. However, these kinds of similarity relations also have deficiencies when they are used in fuzzy rough computation. In this article, we propose a new fuzzy-rough-set model for categorical data by introducing a variable parameter to control the similarity of samples. This model employs the iterative computation strategy to define fuzzy rough approximations and dependence functions. It is proved that the proposed rough dependence function is monotonic. Finally, the proposed model is applied to the attribute reduction of categorical data. The experimental results indicate that the proposed model is more effective for categorical data than some existing algorithms.																	1063-6706	1941-0034				MAY	2020	28	5					818	830		10.1109/TFUZZ.2019.2949765													
J								Hypotheses Analysis and Assessment in Counterterrorism Activities: A Method Based on OWA and Fuzzy Probabilistic Rough Sets	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Rough sets; Terrorism; Probabilistic logic; Open wireless architecture; Probability distribution; Weapons; Bayes methods; Fuzzy probabilistic rough sets; GrC; ordered weighted averaging (OWA); three-way decisions	AGGREGATION; UNCERTAINTY	This article presents a new interactive method to analyze and assess hypotheses, and its application to terrorism events. The method combines probability, fuzzy, and rough set theories and supports decision makers and analysts of counterterrorism in the analysis of intelligence information by using behavioral models of known terrorist groups. Starting from intelligence information about possible attack patterns, the proposed method uses two parameters allowing derivation and analysis of a wide range of hypotheses, and their assessment on the basis of different support levels of evidence. The evaluation of results has been done on real data relating to five years (2012-2016) of terrorist activities extracted from the Global Terrorism Database.																	1063-6706	1941-0034				MAY	2020	28	5					831	845		10.1109/TFUZZ.2019.2955047													
J								Distributed Feature Selection for Big Data Using Fuzzy Rough Sets	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Rough sets; Feature extraction; Big Data; Servers; Distributed databases; Cloud computing; Heuristic algorithms; Big data; distributed feature selection; dynamic data decomposition; fuzzy rough sets	ATTRIBUTE REDUCTION; APPROXIMATIONS	Fuzzy rough-set-based feature selection is an important technique for big data analysis. However, the classic fuzzy rough set algorithm takes all the data correlations into account, which leads to the centralized computing mode, requiring high computing and memory space resources. With the increasing amount of data in the big data era, the centralized server cannot afford the computation of fuzzy rough set. To enable the fuzzy rough set for big data analysis, in this article, we propose the novel distributed fuzzy rough set (DFRS)-based feature selection, which separates and assigns the tasks to multiple nodes for parallel computing. The key challenge is to maintain the global information on each distributed node without conserving the entire fuzzy relation matrix. We tackle this challenge by a dynamic data decomposition algorithm and a data summarization process on each distributed node. Extensive experiments based on multiple real datasets demonstrate that DFRS significantly improves the runtime, and its feature selection accuracy is nearly the same as the traditional centralized computing.																	1063-6706	1941-0034				MAY	2020	28	5					846	857		10.1109/TFUZZ.2019.2955894													
J								Novel Incremental Algorithms for Attribute Reduction From Dynamic Decision Tables Using Hybrid Filter-Wrapper With Fuzzy Partition Distance	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Rough sets; Filtering algorithms; Partitioning algorithms; Classification algorithms; Heuristic algorithms; Filtering theory; Fuzzy sets; Attribute reduction; decision tables; fuzzy distances; fuzzy rough sets; incremental algorithms	ROUGH SET APPROACH; PROBABILISTIC APPROXIMATION SPACES; FEATURE-SELECTION; KNOWLEDGE REDUCTION; SYSTEMS	Attribute reduction from decision tables has been much focused in recent years in which the incremental methods of the tradition rough set and extended models are mostly used for adding, removing, or updating the object or attribute set. However, when dealing with the dynamic decision tables, the existing incremental methods do not recalculate information which has been added into the decision table. In this article, we propose some new incremental methods using the hybrid filter-wrapper with fuzzy partition distance on fuzzy rough set. Experimental results indicate that the proposed algorithms decrease significantly the cardinality of reduct as well as achieve higher accuracy than the other filter incremental methods such as IV-FS-FRS-2, IARM, ASS-IAR, IFSA, and IFSD.																	1063-6706	1941-0034				MAY	2020	28	5					858	873		10.1109/TFUZZ.2019.2948586													
J								Fast and Scalable Approaches to Accelerate the Fuzzy k-Nearest Neighbors Classifier for Big Data	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Apache spark; big data; classification; fuzzy sets; k-nearest neighbors (kNN); MapReduce	ALGORITHMS	One of the best-known and most effective methods in supervised classification is the k-nearest neighbors algorithm (kNN). Several approaches have been proposed to improve its accuracy, where fuzzy approaches prove to be among the most successful, highlighting the classical fuzzy k-nearest neighbors (FkNN). However, these traditional algorithms fail to tackle the large amounts of data that are available today. There are multiple alternatives to enable kNN classification in big datasets, spotlighting the approximate version of kNN called hybrid spill tree. Nevertheless, the existing proposals of FkNN for big data problems are not fully scalable, because a high computational load is required to obtain the same behavior as the original FkNN algorithm. This article proposes global approximate hybrid spill tree FkNN and local hybrid spill tree FkNN, two approximate approaches that speed up runtime without losing quality in the classification process. The experimentation compares various FkNN approaches for big data with datasets of up to 11 million instances. The results show an improvement in runtime and accuracy over literature algorithms.																	1063-6706	1941-0034				MAY	2020	28	5					874	886		10.1109/TFUZZ.2019.2936356													
J								Fusing Fuzzy Monotonic Decision Trees	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Rough sets; Decision trees; Heuristic algorithms; Task analysis; Uncertainty; Big Data; Entropy; Attribute reduction; discernibility matrix; ensemble learning; fuzzy dominance rough set; ordinal classification	ATTRIBUTE REDUCTION; ROUGH; ENTROPY; APPROXIMATION; ACCELERATOR	Ordinal classification is an important classification task, in which there exists a monotonic constraint between features and the decision class. In this article, we aim to develop a method of fusing ordinal decision trees with fuzzy rough-set-based attribute reduction. Most of the existing attribute reduction methods for ordinal decision tables are based on the dominance rough set theory or significance measures. However, the crisp dominance relation is difficult in making full use of the information of attribute values; and the reducts based on significance measures are poor in interpretability and may contain unnecessary attributes. In this article, we first define a discernibility matrix with fuzzy dominance rough set. With this discernibility matrix, multiple reducts can be found, which provide multiple complementary feature subspaces with original information. Then, diverse ordinal trees can be established from these feature subspaces, and finally, the trees are fused by majority voting. The experimental results show that the proposed fusion method performs significantly better than other fusion methods using dominance rough set or significance measures.																	1063-6706	1941-0034				MAY	2020	28	5					887	900		10.1109/TFUZZ.2019.2953024													
J								Active Incremental Feature Selection Using a Fuzzy-Rough-Set-Based Information Entropy	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Fuzzy rough sets; incremental feature selection; information entropy; representative instance	ATTRIBUTE REDUCTION APPROACH; KNOWLEDGE GRANULARITY; DYNAMIC DATA; ALGORITHM; GRANULATION	Feature selection is a popular technique of preprocessing data. In order to deal with dynamic or large data, incremental feature selection has been developed, in which the features selected from existing data are integrated with those mined from both existing and dynamic data in the manner of incremental computation. Fuzzy rough set theory is powerful in handling uncertainty in real-valued data or even mixed data, and one of its most important applications is feature selection. Nevertheless, not much work has been found on fuzzy-rough-set-based incremental feature selection. Therefore, in this article, we investigate the incremental feature selection using a fuzzy-rough-set-based information entropy with incoming instances. Specifically, the representative instances from the incoming ones are first selected according to the information coverage of fuzzy granules generated by fuzzy rough sets. Then, the incremental mechanism of the fuzzy-rough-set-based information entropy is formulated by adding newcome instances. Finally, an incremental feature selection procedure, which we call active incremental feature selection, is proposed. Furthermore, some numerical experiments are conducted to assess the performance of the proposed feature selection algorithm, and the results show that our algorithm is of a prominent advantage in terms of computational time, especially for a dataset with large number of instances.																	1063-6706	1941-0034				MAY	2020	28	5					901	915		10.1109/TFUZZ.2019.2959995													
J								A Hopping Umbrella for Fuzzy Joining Data Streams From IoT Devices in the Cloud and on the Edge	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Cloud computing; Monitoring; Sensor systems; Logic gates; Temperature sensors; Data centers; Big Data; Big Data; Cloud computing; data streams; fuzzy join; fuzzy sets; Internet of Things (IoT); sensors	IMPLEMENTATION; SYSTEMS; ALGORITHM; SELECTION	Internet of Things (IoT) is a new technology that changes the image of the current world, yielding new possibilities, but also proliferating data. IoT devices may constantly produce enormous amounts of data as data streams that can be analyzed in real time and also collected for further exploration in data lakes in huge data centers. Due to their scaling capabilities, these data centers are frequently located in the Cloud. However, recent rapid growth in the number of IoT devices and their applications in manufacturing, transport, and health care motivates moving the burden of data processing and analysis to the Edge. One of the phases of data processing is combining data streams from two (or more) IoT devices that monitor the same object and work asynchronously. Since they generate sensor readings at various moments of time, their data streams must be properly combined in order to obtain a complete image of the monitored object or process. In this article, we present the idea of a hopping umbrella which fuzzifies timestamps from sensor readings while joining data streams from asynchronous IoT devices in a flexible way. In contrast to processing data at rest, the hopping umbrella implements the fuzzy join operation in time windows for data streams (data in motion). By using fuzzy sets, the hopping umbrella not only allows combining asynchronous events from multiple sensors, but also facilitates evaluation of the degree of matching of the combined sensor readings, and consequently allows for reduction of the output stream size. Our experiments performed in Cloud and on Edge devices proved that with the use of this idea, we are able to properly join the best matching sensor readings and in some scenarios, reduce the number of data transferred to the Cloud data center without significant overhead in resource utilization of stream processing units.																	1063-6706	1941-0034				MAY	2020	28	5					916	928		10.1109/TFUZZ.2019.2955056													
J								Scalable Approximate FRNN-OWA Classification	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Open wireless architecture; Rough sets; Training; Time complexity; Approximation algorithms; Machine learning algorithms; Fuzzy systems; Big data applications; classification algorithms; fuzzy rough sets; nearest neighbor searches; scalability	FEATURE-SELECTION; ROUGH	Fuzzy rough nearest neighbor classification with ordered weighted averaging operators (FRNN-OWA) is an algorithm that classifies unseen instances according to their membership in the fuzzy upper and lower approximations of the decision classes. Previous research has shown that the use of OWA operators increases the robustness of this model. However, calculating membership in an approximation requires a nearest neighbor search. In practice, the query time complexity of exact nearest neighbor search algorithms in more than a handful of dimensions is near linear, which limits the scalability of FRNN-OWA. Therefore, we propose approximate FRNN-OWA, a modified model that calculates upper and lower approximations of decision classes using the approximate nearest neighbors returned by hierarchical navigable small worlds (HNSW), a recent approximative nearest neighbor search algorithm with logarithmic query time complexity at constant near-100% accuracy. We demonstrate that approximate FRNN-OWA is sufficiently robust to match the classification accuracy of exact FRNN-OWA while scaling much more efficiently. We test four parameter configurations of HNSW and evaluate their performance by measuring classification accuracy and construction and query times for samples of various sizes from three large datasets. We find that with two of the parameter configurations, approximate FRNN-OWA achieves near-identical accuracy to exact FRNN-OWA for most sample sizes within query times that are up to several orders of magnitude faster.																	1063-6706	1941-0034				MAY	2020	28	5					929	938		10.1109/TFUZZ.2019.2949769													
J								Multiobjective Evolution of Fuzzy Rough Neural Network via Distributed Parallelism for Stock Prediction	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Distributed parallelism; evolutionary neural network; fuzzy rough neural network (FRNN); multiobjective evolution; stock price prediction	RULE GENERATION; ALGORITHM; OPTIMIZATION; MLP	Fuzzy rough theory can describe real-world situations in a mathematically effective and interpretable way, while evolutionary neural networks can be utilized to solve complex problems. Combining them with these complementary capabilities may lead to evolutionary fuzzy rough neural network with the interpretability and prediction capability. In this article, we propose modifications to the existing models of fuzzy rough neural network and then develop a powerful evolutionary framework for fuzzy rough neural networks by inheriting the merits of both the aforementioned systems. We first introduce rough neurons and enhance the consequence nodes, and further integrate the interval type-2 fuzzy set into the existing fuzzy rough neural network model. Thus, several modified fuzzy rough neural network models are proposed. While simultaneously considering the objectives of prediction precision and network simplicity, each model is transformed into a multiobjective optimization problem by encoding the structure, membership functions, and the parameters of the network. To solve these optimization problems, distributed parallel multiobjective evolutionary algorithms are proposed. We enhance the optimization processes with several measures including optimizer replacement and parameter adaption. In the distributed parallel environment, the tedious and time-consuming neural network optimization can be alleviated by numerous computational resources, significantly reducing the computational time. Through experimental verification on complex stock time series prediction tasks, the proposed optimization algorithms and the modified fuzzy rough neural network models exhibit significant improvements the existing fuzzy rough neural network and the long short-term memory network.																	1063-6706	1941-0034				MAY	2020	28	5					939	952		10.1109/TFUZZ.2020.2972207													
J								Scalable Fuzzy Rough Set Reduct Computation Using Fuzzy Min-Max Neural Network Preprocessing	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Discernibility matrix; feature subset selection; fuzzy min-max neural network (FMNN); fuzzy rough sets (FRSs); granular computing; hyperbox; reduct; rough sets	FEATURE-SELECTION; ATTRIBUTE REDUCTION; OPTIMIZATION; ALGORITHM	A fuzzy rough set (FRS) is a hybridization of rough sets and fuzzy sets and provides a framework for reduct (feature subset selection) computation for hybrid decision systems. However, the existing FRS-based feature selection approaches are intractable for large decision systems due to the space complexity of the FRS methodology. We propose a novel fuzzy min-max neural network (FMNN)-FRS reduct computation approach utilizing the FMNN to enhance the scalability of FRS approaches. The FMNN provides a single pass epoch learning of arriving at granules of objects in the form of fuzzy hyperboxes for multiple decision classes. In the proposed approach, the FMNN model is used to reconstruct the object-based decision system into a fuzzy hyperbox-based interval-valued decision system. Then, a novel way of constructing the fuzzy discernibility matrix (FDM) from the interval-valued decision system is introduced. A fuzzy rough approximate reduct computation algorithm is developed with the induced FDM. The FMNN-FRS approach reduces the space complexity of FRS reduct computation significantly and results in enhanced scalability. Comparative experimental analysis has been done with the existing FRS reduct approaches on benchmark hybrid decision systems and established the relevance of the FMNN-FRS approach. The FMNN-FRS approach obtained the exact reduct in most of the datasets in much lesser computational time than existing FRS approaches while preserving similar classification accuracy. The FMNN-FRS method achieved enhanced scalability to such large decision systems, at which it is not possible to obtain reduct by existing FRS approaches.																	1063-6706	1941-0034				MAY	2020	28	5					953	964		10.1109/TFUZZ.2020.2965899													
J								Medical Image Segmentation by Partitioning Spatially Constrained Fuzzy Approximation Spaces	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Medical imaging; rough-fuzzy clustering; rough sets; segmentation; spatial information	MAGNETIC-RESONANCE IMAGES; MARKOV RANDOM-FIELD; C-MEANS ALGORITHM; K-MEANS ALGORITHM; MR-IMAGES; CLUSTERING-ALGORITHM; LOCAL INFORMATION; CLASSIFICATION; MODEL	Image segmentation is an important prerequisite step for any automatic clinical analysis technique. It assists in visualization of human tissues, as accurate delineation of medical images requires involvement of expert practitioners, which is also time consuming. In this background, the rough-fuzzy clustering algorithm provides an effective approach for image segmentation. It handles uncertainties arising due to overlapping classes and incompleteness in class definition by partitioning the fuzzy approximation spaces. However, the existing rough-fuzzy clustering algorithms do not consider the spatial distribution of the image. They depend only on the distribution of pixels to determine their class labels. In this regard, this article introduces a new algorithm, termed as spatially constrained rough-fuzzy c-means (sRFCM) for medical image segmentation. The proposed sRFCM algorithm combines wisely the merits of rough-fuzzy clustering and local neighborhood information. In the proposed algorithm, the labels of local neighbors influence in the determination of the label of center pixel. The effect of local neighbors acts as a regularizer. Moreover, the proposed sRFCM algorithm partitions each cluster in possibilistic lower approximation or core region and probabilistic boundary region. The cluster centroid depends on the core and boundary regions, weight parameter, and neighborhood regularizer. A novel segmentation validity index, termed as neighborhood Silhouette, is proposed to find out the optimum values of regularizer and weight parameter, controlling the performance of the sRFCM. The efficacy of the proposed sRFCM algorithm, as well as several existing segmentation algorithms, is demonstrated on four brain MR volume databases and one HEp-2 cell image data.																	1063-6706	1941-0034				MAY	2020	28	5					965	977		10.1109/TFUZZ.2020.2965896													
J								Shadowed Neighborhoods Based on Fuzzy Rough Transformation for Three-Way Classification	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Fuzzy rough transformation; shadowed neighborhood; three-way classification; uncertain data analysis	NEAREST-NEIGHBOR; DECISION-SUPPORT; K-MEANS; SETS; APPROXIMATIONS; GRANULATION; SYSTEMS	Neighborhoods form a set-level approximation of data distribution for learning tasks. Due to the advantages of data generalization and nonparametric property, neighborhood models have been widely used for data classification. However, the existing neighborhood-based classification methods rigidly assign a certain class label to each data instance and lack the strategies to handle the uncertain instances. The far-fetched certain classification of uncertain instances may suffer serious risks. To tackle this problem, in this article, we propose a novel shadowed set to construct shadowed neighborhoods for uncertain data classification. For the fuzzy-rough transformation in the proposed shadowed set, a step function is utilized to map fuzzy neighborhood memberships to the set of triple typical values and thereby partition a neighborhood into certain regions and uncertain boundary (neighborhood shadow). The threshold parameter in the step function for constructing shadowed neighborhoods is optimized through minimizing the membership loss in the mapping of shadowed sets. Based on the constructed shadowed neighborhoods, we implement a three-way classification algorithm to distinguish data instances into certain classes and uncertain case. Experiments validate the proposed three-way classification method with shadowed neighborhoods is effective in handling uncertain data and reducing the classification risk.																	1063-6706	1941-0034				MAY	2020	28	5					978	991		10.1109/TFUZZ.2020.2979365													
J								A Hierarchical Clustering Approach to Fuzzy Semantic Representation of Rare Words in Neural Machine Translation	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Fuzzy semantic representation (FSR); hierarchical clustering; neural network; neural machine translation (NMT)		Rare words are usually replaced with a single token in the current encoder-decoder style of neural machine translation, challenging the translation modeling by an obscured context. In this article, we propose to build a fuzzy semantic representation (FSR) method for rare words through a hierarchical clustering method to group rare words together, and integrate it into the encoder-decoder framework. This hierarchical structure can compensate for the semantic information in both source and target sides, and providing fuzzy context information to capture the semantic of rare words. The introduced FSR can also alleviate the data sparseness, which is the bottleneck in dealing with rare words in neural machine translation. In particular, our method is easily extended to the transformer-based neural machine translation model and learns the FSRs of all in-vocabulary words to enhance the sentence representations in addition to rare words. Our experiments on Chinese-to-English translation tasks confirm a significant improvement in the translation quality brought by the proposed method.																	1063-6706	1941-0034				MAY	2020	28	5					992	1002		10.1109/TFUZZ.2020.2969399													
J								Optimize TSK Fuzzy Systems for Regression Problems: Minibatch Gradient Descent With Regularization, DropRule, and AdaBound (MBGD-RDA)	IEEE TRANSACTIONS ON FUZZY SYSTEMS										AdaBound; Adaptive-network-based fuzzy inference system (ANFIS); DropRule; fuzzy systems; minibatch gradient descent (MBGD); regularization	INTERVAL TYPE-2; BIG DATA	Takagi-Sugeno-Kang (TSK) fuzzy systems are very useful machine learning models for regression problems. However, to our knowledge, there has not existed an efficient and effective training algorithm that ensures their generalization performance and also enables them to deal with big data. Inspired by the connections between TSK fuzzy systems and neural networks, we extend three powerful neural network optimization techniques, i.e., minibatch gradient descent (MBGD), regularization, and AdaBound, to TSK fuzzy systems, and also propose three novel techniques (DropRule, DropMF, and DropMembership) specifically for training TSK fuzzy systems. Our final algorithm, MBGD with regularization, DropRule, and AdaBound, can achieve fast convergence in training TSK fuzzy systems, and also superior generalization performance in testing. It can be used for training TSK fuzzy systems on datasets of any size; however, it is particularly useful for big datasets, on which currently no other efficient training algorithms exist.																	1063-6706	1941-0034				MAY	2020	28	5					1003	1015		10.1109/TFUZZ.2019.2958559													
J								Online Incremental Classification Resonance Network and Its Application to Human-Robot Interaction	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Robots; Subspace constraints; Real-time systems; Face; Learning systems; Convolutional neural networks; Feature extraction; Concept vectors; face identification; human-robot interaction (HRI); incremental class learning; online incremental classification resonance network (OICRN)	MACHINE	In human-robot interaction (HRI), classification is one of the most important problems, and it is essential particularly when the robot recognizes the surroundings and chooses a reaction based on a certain situation. Each interaction is different since new people appear or the environment changes, and the robot should be able to adapt to different situations during a brief interaction. Thus, it is imperative that the classification is performed incrementally in real time. In this sense, we propose an online incremental classification resonance network (OICRN) that enables incremental class learning in multi-class classification with high performance online. In OICRN, a scale-preserving projection process is introduced to use the raw input vectors online without a normalization process in advance. The integrated network of the convolutional neural network (CNN) for feature extraction and the OICRN for classification is applied to a robotic system that learns human identities through HRIs. To demonstrate the effectiveness of our network, experiments are carried out on benchmark data sets and on a humanoid robot, Mybot, developed in the Robot Intelligence Technology Laboratory, KAIST.																	2162-237X	2162-2388				MAY	2020	31	5					1426	1436		10.1109/TNNLS.2019.2920158													
J								Multimodal Deep Network Embedding With Integrated Structure and Attribute Information	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Task analysis; Correlation; Learning systems; Feature extraction; Data mining; Social networking (online); Deep learning; Deep learning; multimodal learning; network analysis; network embedding	DIMENSIONALITY REDUCTION; LAPLACIAN EIGENMAPS; AUTOENCODERS	Network embedding is the process of learning low-dimensional representations for nodes in a network while preserving node features. Existing studies only leverage network structure information and emphasize the preservation of structural features. However, nodes in real-world networks often have a rich set of attributes providing extra semantic information. It has been demonstrated that both structural and attribute features are important for network analysis tasks. To preserve both features, we investigate the problem of integrating structure and attribute information to perform network embedding and propose a multimodal deep network embedding (MDNE) method. MDNE captures the non-linear network structures and the complex interactions among structures and attributes using a deep model consisting of multiple layers of non-linear functions. Since structures and attributes are two different types of information, a multimodal learning method is adopted to pre-process them and help the model to better capture the correlations between node structure and attribute information. We define the loss function employing structural and attribute proximities to preserve the respective features, and the representations are obtained by minimizing the loss function. Results of extensive experiments on four real-world data sets show that the proposed method performs significantly better than baselines on a variety of tasks, which demonstrates the effectiveness and generality of our method.																	2162-237X	2162-2388				MAY	2020	31	5					1437	1449		10.1109/TNNLS.2019.2920267													
J								Finite-Horizon H-infinity State Estimation for Periodic Neural Networks Over Fading Channels	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Fading channel; finite-horizon; H-infinity state estimation; periodic neural networks (PNNs)	SAMPLED-DATA CONTROL; STABILITY ANALYSIS; TIME; DISCRETE; SYSTEMS; STABILIZATION; EXISTENCE; DESIGN	The problem of finite-horizon H-infinity state estimator design for periodic neural networks over multiple fading channels is studied in this paper. To characterize the measurement signals transmitted through different channels experiencing channel fading, a multiple fading channels model is considered. For investigating the situation of correlated fading channels, a set of correlated random variables is introduced. Specifically, the channel coefficients are described by white noise processes and are assumed to be correlated. Two sufficient criteria are provided, by utilizing a stochastic analysis approach, to guarantee that the estimation error system is stochastically stable and achieves the prescribed H-infinity performance. Then, the parameters of the estimator are derived by solving recursive linear matrix inequalities. Finally, some simulation results are shown to illustrate the effectiveness of the proposed method.																	2162-237X	2162-2388				MAY	2020	31	5					1450	1460		10.1109/TNNLS.2019.2920368													
J								Skip-Connected Covariance Network for Remote Sensing Scene Classification	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Feature extraction; Learning systems; Neural networks; Training; Aggregates; Computational modeling; Remote sensing; Covariance pooling; deep neural network; multi-layer feature; scene recognition	CONVOLUTIONAL NEURAL-NETWORKS; FEATURES; RECOGNITION	This paper proposes a novel end-to-end learning model, called skip-connected covariance (SCCov) network, for remote sensing scene classification (RSSC). The innovative contribution of this paper is to embed two novel modules into the traditional convolutional neural network (CNN) model, i.e., skip connections and covariance pooling. The advantages of newly developed SCCov are twofold. First, by means of the skip connections, the multi-resolution feature maps produced by the CNN are combined together, which provides important benefits to address the presence of large-scale variance in RSSC data sets. Second, by using covariance pooling, we can fully exploit the second-order information contained in such multi-resolution feature maps. This allows the CNN to achieve more representative feature learning when dealing with RSSC problems. Experimental results, conducted using three large-scale benchmark data sets, demonstrate that our newly proposed SCCov network exhibits very competitive or superior classification performance when compared with the current state-of-the-art RSSC techniques, using a much lower amount of parameters. Specifically, our SCCov only needs 10% of the parameters used by its counterparts.																	2162-237X	2162-2388				MAY	2020	31	5					1461	1474		10.1109/TNNLS.2019.2920374													
J								A Novel Neural Network for Solving Nonsmooth Nonconvex Optimization Problems	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Recurrent neural networks; Linear programming; Convex functions; Learning systems; Real-time systems; Quadratic programming; Convergence within a limited time; nonsmooth nonconvex optimization; recurrent neural network (RNN); unbounded feasible region	PROGRAMMING-PROBLEMS	In this paper, a novel recurrent neural network (RNN) is presented to deal with a kind of nonsmooth nonconvex optimization problem in which the objective function may be nonsmooth and nonconvex, and the constraints include linear equations and convex inequations. Under certain suitable assumptions, from an arbitrary initial state, each solution to the proposed RNN exists globally and is bounded, and it enters the feasible region within a limited time. Moreover, the solution to the RNN with an arbitrary initial state can converge to the critical point set of the optimization problem. In particular, the RNN does not need the following: 1) abounded feasible region; 2) the computation of an exact penalty parameter; or 3) the initial state being chosen from a given bounded set. Numerical experiments are provided to show the effectiveness and advantages of the RNN.																	2162-237X	2162-2388				MAY	2020	31	5					1475	1488		10.1109/TNNLS.2019.2920408													
J								Estimation of Domain of Attraction for Aperiodic Sampled-Data Switched Delayed Neural Networks Subject to Actuator Saturation	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Switches; Actuators; Symmetric matrices; Synchronization; Artificial neural networks; Control theory; Actuator saturation; aperiodic sampled-data switched delayed neural networks (ASDSDNNs); domain of attraction; time-scheduled Lyapunov functionals	EXTENDED DISSIPATIVE ANALYSIS; STABILITY ANALYSIS; EXPONENTIAL STABILIZATION; SYSTEMS; SYNCHRONIZATION; PARAMETERS; DESIGN	In this paper, for the case of the asynchronous switching caused by that subsystem's switching occuring during a sampling interval, the domain of attraction estimation problem is investigated for aperiodic sampled-data switched delayed neural networks (ASDSDNNs) subject to actuator saturation. A parameters-dependent time-scheduled Lyapunov functional consisting of a novel looped-functional is constructed using segmentation technology and linear interpolation. By employing this novel functional and using an average dwell time (ADT) approach, exponential stability criteria are proposed for polytopic uncertain ASDSDNNs subject to actuator saturation. And a relationship between ADT and sampling period is revealed for ASDSDNNs. As a corollary, exponential stability criteria are proposed for nominal ASDSDNNs subject to actuator saturation. Furthermore, by describing the domain of attraction as a time-varying ellipsoid determined by the time-scheduled Lyapunov matrix, the proposed theoretical conditions are transformed into a linear matrix inequality (LMI)-based multi-objective optimization problem. The dynamic estimates of the domain of attraction for ASDSDNNs are solved. Numerical simulation examples are provided to illustrate the effectiveness of the proposed method.																	2162-237X	2162-2388				MAY	2020	31	5					1489	1503		10.1109/TNNLS.2019.2920665													
J								New Criteria for Stability of Neutral-Type Neural Networks With Multiple Time Delays	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Stability criteria; Biological neural networks; Delay effects; Neurons; Mathematical model; Delays; Lyapunov functionals; multiple delays; neutral-type neural networks; stability analysis	GLOBAL EXPONENTIAL STABILITY; ROBUST STABILITY; DEPENDENT STABILITY; SYSTEMS; DISCRETE	This research work studies stability problems for more general models of neutral-type neural systems where both neuron states and the time derivative of neuron states involve multiple delays. Some new sufficient criterion is presented, which guarantee the existence, uniqueness, and global asymptotic stability of equilibrium points of the considered neural network model. These obtained stability conditions, which can be applied to some larger classes of general neural network models, are based on the analysis of a new and improved suitable Lyapunov functional. The proposed conditions are independent of time delay parameters and can be easily justified by examining some certain relationships among the relevant neural network parameters. This paper also shows that the obtained stability criteria can be considered as the generalization of some previously reported corresponding stability conditions for neural networks, including multiple time delay parameters.																	2162-237X	2162-2388				MAY	2020	31	5					1504	1513		10.1109/TNNLS.2019.2920672													
J								Deep Inception-Residual Laplacian Pyramid Networks for Accurate Single-Image Super-Resolution	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Image reconstruction; Training; Laplace equations; Learning systems; Convolutional neural networks; Convolutional neural networks (CNNs); Laplacian pyramid framework; local rank space; single-image super-resolution (SR)	SPARSE REPRESENTATION; RECONSTRUCTION; INTERPOLATION	With exploiting contextual information over large image regions in an efficient way, the deep convolutional neural network has shown an impressive performance for single-image super-resolution (SR). In this paper, we propose a new deep convolutional network by cascading multiple well-designed inception-residual blocks within the deep Laplacian pyramid framework to progressively restore the missing high-frequency details in the low-resolution images. By optimizing our network structure, the trainable depth of our proposed network gains a significant improvement, which in turn improves super-resolving accuracy. However, the saturation and degradation of training accuracy remains a critical problem. With regard to this, we propose an effective two-stage training strategy, in which we first use the images downsampled from the ground-truth high-resolution (HR) images to pretrain the inception-residual blocks on each pyramid level with an extremely high learning rate enabled by gradient clipping, and then the original ground-truth HR images are used to fine-tune all the pretrained inception-residual blocks for obtaining our final SR models. Furthermore, we present a new loss function operating in both image space and local rank space to optimize our network for exploiting the contextual information among different output components. Extensive experiments on benchmark data sets validate that the proposed method outperforms the existing state-of-the-art SR methods in terms of the objective evaluation as well as the visual quality.																	2162-237X	2162-2388				MAY	2020	31	5					1514	1528		10.1109/TNNLS.2019.2920852													
J								Hyperspectral Pansharpening With Deep Priors	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Bayes methods; Hyperspectral sensors; High frequency; Spatial resolution; Fuses; Imaging; Deep priors; high frequency; hyperspectral (HS) pansharpening; structure tensor (ST); sylvester equation	IMAGE SUPERRESOLUTION; FUSION; RESOLUTION; MS	Hyperspectral (HS) image can describe subtle differences in the spectral signatures of materials, but it has low spatial resolution limited by the existing technical and budget constraints. In this paper, we propose a promising HS pansharpening method with deep priors (HPDP) to fuse a low-resolution (LR) HS image with a high-resolution (HR) panchromatic (PAN) image. Different from the existing methods, we redefine the spectral response function (SRF) based on the larger eigenvalue of structure tensor (ST) matrix for the first time that is more in line with the characteristics of HS imaging. Then, we introduce HFNet to capture deep residual mapping of high frequency across the upsampled HS image and the PAN image in a band-by-band manner. Specifically, the learned residual mapping of high frequency is injected into the structural transformed HS images, which are the extracted deep priors served as additional constraint in a Sylvester equation to estimate the final HR HS image. Comparative analyses validate that the proposed HPDP method presents the superior pansharpening performance by ensuring higher quality both in spatial and spectral domains for all types of data sets. In addition, the HFNet is trained in the high-frequency domain based on multispectral (MS) images, which overcomes the sensitivity of deep neural network (DNN) to data sets acquired by different sensors and the difficulty of insufficient training samples for HS pansharpening.																	2162-237X	2162-2388				MAY	2020	31	5					1529	1543		10.1109/TNNLS.2019.2920857													
J								A Training Data Set Cleaning Method by Classification Ability Ranking for the k-Nearest Neighbor Classifier	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Training; Training data; Testing; Cleaning; Noise measurement; Learning systems; Classification ability ranking (CAR); k-nearest neighbor (KNN); leave-one-out (LV1) strategy; pattern classification; training data set cleaning (TDC)	SELECTION; ALGORITHMS; RULE	The $k$ -nearest neighbor (KNN) rule is a successful technique in pattern classification due to its simplicity and effectiveness. As a supervised classifier, KNN classification performance usually suffers from low-quality samples in the training data set. Thus, training data set cleaning (TDC) methods are needed for enhancing the classification accuracy by cleaning out noisy, or even wrong, samples in the original training data set. In this paper, we propose a classification ability ranking (CAR)-based TDC method to improve the performance of a KNN classifier, namely CAR-based TDC method. The proposed classification ability function ranks a training sample in terms of its contribution to correctly classify other training samples as a KNN through the leave-one-out (LV1) strategy in the cleaning stage. The training sample that likely misclassifies the other samples during the KNN classifications according to the LV1 strategy is considered to have lower classification ability and will be cleaned out from the original training data set. Extensive experiments, based on ten real-world data sets, show that the proposed CAR-based TDC method can significantly reduce the classification error rates of KNN-based classifiers, while reducing computational complexity thanks to a smaller cleaned training data set.																	2162-237X	2162-2388				MAY	2020	31	5					1544	1556		10.1109/TNNLS.2019.2920864													
J								Neural Network-Based Information Transfer for Dynamic Optimization	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Artificial neural networks; Sociology; Statistics; Optimization; Learning systems; Predictive models; Computer science; Dynamic optimization problem (DOP); information transfer; neural network (NN)	EVOLUTIONARY ALGORITHMS; DIFFERENTIAL EVOLUTION; FEEDFORWARD NETWORKS; ASSOCIATIVE MEMORY; ENVIRONMENTS; SEARCH; OPTIMA	In dynamic optimization problems (DOPs), as the environment changes through time, the optima also dynamically change. How to adapt to the dynamic environment and quickly find the optima in all environments is a challenging issue in solving DOPs. Usually, a new environment is strongly relevant to its previous environment. If we know how it changes from the previous environment to the new one, then we can transfer the information of the previous environment, e.g., past solutions, to get new promising information of the new environment, e.g., new high-quality solutions. Thus, in this paper, we propose a neural network (NN)-based information transfer method, named NNIT, to learn the transfer model of environment changes by NN and then use the learned model to reuse the past solutions. When the environment changes, NNIT first collects the solutions from both the previous environment and the new environment and then uses an NN to learn the transfer model from these solutions. After that, the NN is used to transfer the past solutions to new promising solutions for assisting the optimization in the new environment. The proposed NNIT can be incorporated into population-based evolutionary algorithms (EAs) to solve DOPs. Several typical state-of-the-art EAs for DOPs are selected for comprehensive study and evaluated using the widely used moving peaks benchmark. The experimental results show that the proposed NNIT is promising and can accelerate algorithm convergence.																	2162-237X	2162-2388				MAY	2020	31	5					1557	1570		10.1109/TNNLS.2019.2920887													
J								Distributed Synchronization Control of Nonaffine Multiagent Systems With Guaranteed Performance	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Multi-agent systems; Synchronization; Couplings; Lyapunov methods; Transient analysis; Steady-state; Adaptive neural control; cooperative control; guaranteed performance; nonaffine multiagent system	ADAPTIVE NEURAL-CONTROL; COOPERATIVE TRACKING CONTROL; NONLINEAR-SYSTEMS; NN CONTROL; CONSENSUS	This paper deals with the synchronization control problem in the leader-follower format of a class of high-order nonaffine nonlinear multiagent systems under a directed communication protocol. A novel adaptive neural distributed synchronization scheme with guaranteed performance is proposed. The main contribution lies in the fact that both nonaffine agent dynamics, which basically makes most existing agent dynamics as special cases, and guaranteed synchronization performance are taken into account. The difficulty lies mainly in the nonaffine terms and coupling terms due to the interactions of agents. To overcome this challenge, an augmented quadratic Lyapunov function by incorporating the lower bounds of control gains is proposed. The problems resulting from the nonaffine dynamics and the coupling terms among agents are solved by incorporating the special property of radial basis function neural network into the derivative of the augmented quadratic Lyapunov function. The unknown nonaffine terms are addressed by using an indirected neural network approach. A nonlinear mapping is built to relate the local consensus error to a new one, which is subsequently stabilized via Lyapunov synthesis. As a result, the proposed approach can ensure the outputs of all follower agents to track the outputs of the leader, while the synchronization performance bounds can be quantified on both transient and steady-state stages. All other signals in the closed loop are ensured to be semiglobally, uniformly, and ultimately bounded. Finally, the effectiveness of the proposed controller is verified through a heterogeneous four-agent example.																	2162-237X	2162-2388				MAY	2020	31	5					1571	1580		10.1109/TNNLS.2019.2920892													
J								Fault Diagnosis of Complex Processes Using Sparse Kernel Local Fisher Discriminant Analysis	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Fault diagnosis; Input variables; Kernel; Optimization; Fault detection; Principal component analysis; Sparse matrices; Elastic net algorithm; fault diagnosis; Fisher discriminant analysis (FDA); local FDA (LFDA); nonlinear characteristic; variable selection	VARIABLE SELECTION; ALGORITHM; PCA	As an outstanding discriminant analysis technique, Fisher discriminant analysis (FDA) gained extensive attention in supervised dimensionality reduction and fault diagnosis fields. However, it typically ignores the multimodality within the measured data, which may cause infeasibility in practice. In addition, it generally incorporates all process variables without emphasizing the key faulty ones when modeling the complex process, thus leading to degraded fault classification capability and poor model interpretability. To ease the above two drawbacks of conventional FDA, this brief presents an advantageously sparse local FDA (SLFDA) model, it first preserves the within-class multimodality by introducing local weighting factors into scatter matrix. Then, the responsible faulty variables are identified automatically through the elastic net algorithm, and the current optimization problem is subsequently settled through the feasible gradient direction method. Since then, the local data structure characteristics are exploited from both the sample dimension and variable dimension so that the fault diagnosis performance and model interpretability are significantly enhanced. In addition, we naturally extend SLFDA model to nonlinear variant (i.e., sparse kernel local FDA) by the kernel trick, which is substantially more resistant to strong nonlinearity. The simulation studies on Tennessee Eastman (TE) benchmark process and real-world diesel engine working process both validate that the novel diagnosis strategy is more accurate and reliable than the existing state-of-the-art methods.																	2162-237X	2162-2388				MAY	2020	31	5					1581	1591		10.1109/TNNLS.2019.2920903													
J								Person Reidentification via Multi-Feature Fusion With Adaptive Graph Learning	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Dictionaries; Machine learning; Measurement; Learning systems; Cameras; Adaptation models; Optimization; Adaptive graph learning; feature representation learning; multi-feature fusion; person reidentification (Re-ID)		The goal of person reidentification (Re-ID) is to identify a given pedestrian from a network of nonoverlapping surveillance cameras. Most existing works follow the supervised learning paradigm which requires pairwise labeled training data for each pair of cameras. However, this limits their scalability to real-world applications where abundant unlabeled data are available. To address this issue, we propose a multi-feature fusion with adaptive graph learning model for unsupervised Re-ID. Our model aims to negotiate comprehensive assessment on the consistent graph structure of pedestrians with the help of special information of feature descriptors. Specifically, we incorporate multi-feature dictionary learning and adaptive multi-feature graph learning into a unified learning model such that the learned dictionaries are discriminative and the subsequent graph structure learning is accurate. An alternating optimization algorithm with proved convergence is developed to solve the final optimization objective. Extensive experiments on four benchmark data sets demonstrate the superiority and effectiveness of the proposed method.																	2162-237X	2162-2388				MAY	2020	31	5					1592	1601		10.1109/TNNLS.2019.2920905													
J								Multidimensional Gains for Stochastic Approximation	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Convergence; Jacobian matrices; Noise measurement; Covariance matrices; Approximation algorithms; Estimation; Optimization; Gradient descent; Newton's method; noisy function measurements; root finding; Rosenbrock function; stochastic approximation (SA); stochastic optimization; variance reduction	NEURAL-NETWORKS; PID CONTROLLER; GRADIENT; ALGORITHM; OPTIMIZATION	This paper deals with iterative Jacobian-based recursion technique for the root-finding problem of the vector-valued function, whose evaluations are contaminated by noise. Instead of a scalar step size, we use an iterate-dependent matrix gain to effectively weigh the different elements associated with the noisy observations. The analytical development of the matrix gain is built on an iterative-dependent linear function interfered by additive zero-mean white noise, where the dimension of the function is $ {M\ge 1}$ and the dimension of the unknown variable is $ {N\ge 1}$ . Necessary and sufficient conditions for $ {M\ge N}$ algorithms are presented pertaining to algorithm stability and convergence of the estimate error covariance matrix. Two algorithms are proposed: one for the case where $ {M\ge N}$ and the second one for the antithesis. The two algorithms assume full knowledge of the Jacobian. The recursive algorithms are proposed for generating the optimal iterative-dependent matrix gain. The proposed algorithms here aim for per-iteration minimization of the mean square estimate error. We show that the proposed algorithm satisfies the presented conditions for stability and convergence of the covariance. In addition, the convergence rate of the estimation error covariance is shown to be inversely proportional to the number of iterations. For the antithesis $ {M< N}$ , contraction of the error covariance is guaranteed. This underdetermined system of equations can be helpful in training neural networks. Numerical examples are presented to illustrate the performance capabilities of the proposed multidimensional gain while considering nonlinear functions.																	2162-237X	2162-2388				MAY	2020	31	5					1602	1615		10.1109/TNNLS.2019.2920930													
J								Stabilization of Mode-Dependent Impulsive Hybrid Systems Driven by DFA With Mixed-Mode Effects	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Switches; Automata; Delays; Switched systems; Sun; Automation; Symmetric matrices; Deterministic finite automaton (DFA); impulsive effects; mixed mode; semi-tensor product (STP); stabilization	SEMI-TENSOR PRODUCT; LINEAR SWITCHED SYSTEMS; EXPONENTIAL SYNCHRONIZATION; STABILITY ANALYSIS; FINITE AUTOMATA; NEURAL-NETWORKS; STABILIZABILITY; DYNAMICS	This paper is concerned with mode-dependent impulsive hybrid systems driven by deterministic finite automaton (DFA) with mixed-mode effects. In the hybrid systems, a complex phenomenon called mixed mode, caused in time-varying delay switching systems, is considered explicitly. Furthermore, mode-dependent impulses, which can exist not only at the instants coinciding with mode switching but also at the instants when there is no system switching, are also taken into consideration. First, we establish a rigorous mathematical equation expression of this class of hybrid systems. Then, several criteria of stabilization of this class of hybrid systems are presented based on semi-tensor product (STP) techniques, multiple Lyapunov-Krasovskii functionals, as well as the average dwell time approach. Finally, an example is simulated to illustrate the effectiveness of the obtained results.																	2162-237X	2162-2388				MAY	2020	31	5					1616	1625		10.1109/TNNLS.2019.2921020													
J								Minimization of Fraction Function Penalty in Compressed Sensing	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Closed-form thresholding functions; compressed sensing; fraction function minimization; iterative FP thresholding algorithm; non-convex optimization	SPARSE RECOVERY; ALGORITHMS; SIGNALS; REGULARIZATION; REPRESENTATION; EQUATIONS	In this paper, we study the minimization problem of a non-convex sparsity-promoting penalty function, i.e., fraction function, in compressed sensing. First, we discuss the equivalence of $\ell _{0}$ minimization and fraction function minimization. It is proved that the optimal solution to fraction function minimization solves $\ell _{0}$ minimization and the optimal solution to the regularization problem also solves fraction function minimization if the certain conditions are satisfied, which is similar to the regularization problem in a convex optimization theory. Second, we study the properties of the optimal solution to the regularization problem, including the first-order and second-order optimality conditions and the lower and upper bounds of the absolute value for its nonzero entries. Finally, we derive the closed-form representation of the optimal solution to the regularization problem and propose an iterative $FP$ thresholding algorithm to solve the regularization problem. We also provide a series of experiments to assess the performance of the $FP$ algorithm, and the experimental results show that the $FP$ algorithm performs well in sparse signal recovery with and without measurement noise.																	2162-237X	2162-2388				MAY	2020	31	5					1626	1637		10.1109/TNNLS.2019.2921404													
J								Cascade Superpixel Regularized Gabor Feature Fusion for Hyperspectral Image Classification	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Hyperspectral imaging; Feature extraction; Transforms; Image analysis; Machine learning; Learning systems; Cascade superpixel regularization (SR); feature fusion; hyperspectral image classification; image segmentation	SPATIAL CLASSIFICATION; REPRESENTATION; INFORMATION; FRAMEWORK; PROFILES	A 3-D Gabor wavelet provides an effective way to obtain the spectral-spatial-fused features for hyperspectral image, which has shown advantageous performance for material classification and recognition. In this paper, instead of separately employing the Gabor magnitude and phase features, which, respectively, reflect the intensity and variation of surface materials in local area, a cascade superpixel regularized Gabor feature fusion (CSRGFF) approach has been proposed. First, the Gabor filters with particular orientation are utilized to obtain Gabor features (including magnitude and phase) from the original hyperspectral image. Second, a support vector machine (SVM)-based probability representation strategy is developed to fully exploit the decision information in SVM output, and the achieved confidence score can make the following fusion with Gabor phase more effective. Meanwhile, the quadrant bit coding and Hamming distance metric are applied to encode the Gabor phase features and measure sample similarity in sequence. Third, the carefully defined characteristics of two kinds of features are directly combined together without any weighting operation to describe the weight of samples belonging to each class. Finally, a series of superpixel graphs extracted from the raw hyperspectral image with different numbers of superpixels are employed to successively regularize the weighting cube from over-segmentation to under-segmentation, and the classification performance gradually improves with the decrease in the number of superpixels in the regularization procedure. Four widely used real hyperspectral images have been conducted, and the experimental results constantly demonstrate the superiority of our CSRGFF approach over several state-of-the-art methods.																	2162-237X	2162-2388				MAY	2020	31	5					1638	1652		10.1109/TNNLS.2019.2921564													
J								Knowledge-Driven Deep Unrolling for Robust Image Layer Separation	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Task analysis; Rain; Deep learning; Image edge detection; Lighting; Learning systems; Visualization; Deep unrolling; image enhancement; knowledge-driven; single-image layer separation	QUALITY ASSESSMENT; RETINEX; DECOMPOSITION; ILLUMINATION; ENHANCEMENT; ALGORITHM; MODEL	Single-image layer separation targets to decompose the observed image into two independent components in terms of different application demands. It is known that many vision and multimedia applications can be (re)formulated as a separation problem. Due to the fundamentally ill-posed natural of these separations, existing methods are inclined to investigate model priors on the separated components elaborately. Nevertheless, it is knotty to optimize the cost function with complicated model regularizations. Effectiveness is greatly conceded by the settled iteration mechanism, and the adaption cannot be guaranteed due to the poor data fitting. What is more, for a universal framework, the most taxing point is that one type of visual cue cannot be shared with different tasks. To partly overcome the weaknesses mentioned earlier, we delve into a generic optimization unrolling technique to incorporate deep architectures into iterations for adaptive image layer separation. First, we propose a general energy model with implicit priors, which is based on maximum a posterior, and employ the extensively accepted alternating direction method of multiplier to determine our elementary iteration mechanism. By unrolling with one general residual architecture prior and one task-specific prior, we attain a straightforward, flexible, and data-dependent image separation framework successfully. We apply our method to four different tasks, including single-image-rain streak removal, high-dynamic-range tone mapping, low-light image enhancement, and single-image reflection removal. Extensive experiments demonstrate that the proposed method is applicable to multiple tasks and outperforms the state of the arts by a large margin qualitatively and quantitatively.																	2162-237X	2162-2388				MAY	2020	31	5					1653	1666		10.1109/TNNLS.2019.2921597													
J								An Efficient Entropy-Based Causal Discovery Method for Linear Structural Equation Models With IID Noise Variables	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Measurement errors; Mathematical model; Distributed databases; Entropy; Markov processes; Data models; Estimation; Causal discovery; entropy; exogenous variable; linear non-Gaussian acyclic model (LiNGAM); measurement error	GRAPH	The discovery of causal relationships from the observational data is an important task. To identify the unique causal structure belonging to a Markov equivalence class, a number of algorithms, such as the linear non-Gaussian acyclic model (LiNGAM), have been proposed. However, two challenges remain to be met: 1) these algorithms fail to work on the data which follow linear structural equation model with Gaussian noise and 2) they misjudge the causal direction when the data contain additional measurement errors. In this paper, we propose an entropy-based two-phase iterative algorithm for arbitrary distribution data with additional measurement errors under some mild assumptions. In the first phase of the algorithm, based on the property that entropy can measure the amount of information behind the data with arbitrary distribution, we design a general approach for the identification of exogenous variable on both Gaussian and non-Gaussian data, and we give the corresponding theoretical derivation. In the second phase, to eliminate the effects of measurement errors, we revise the value of the exogenous variable by removing its measurement error and further use the revised value to remove its effect on the remaining variables. Experimental results on real-world causal structures are presented to demonstrate the effectiveness and stability of our method. We also apply the proposed algorithm on the mobile-base-station data with measurement errors, and the results further prove the effectiveness of our algorithm.																	2162-237X	2162-2388				MAY	2020	31	5					1667	1680		10.1109/TNNLS.2019.2921613													
J								Deep Class-Wise Hashing: Semantics-Preserving Hashing via Class-Wise Loss	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Training; Measurement; Semantics; Visualization; Optimization; Binary codes; Image retrieval; Deep convolutional neural network (CNN); deep supervised hashing; large-scale image retrieval; learn to hashing	REPRESENTATION	Deep supervised hashing has emerged as an effective solution to large-scale semantic image retrieval problems in computer vision. Convolutional neural network-based hashing methods typically seek pairwise or triplet labels to conduct similarity-preserving learning. However, complex semantic concepts of visual contents are hard to capture by similar/dissimilar labels, which limits the retrieval performance. Generally, pairwise or triplet losses not only suffer from expensive training costs but also lack sufficient semantic information. In this paper, we propose a novel deep supervised hashing model to learn more compact class-level similarity-preserving binary codes. Our model is motivated by deep metric learning that directly takes semantic labels as supervised information in training and generates corresponding discriminant hashing code. Specifically, a novel cubic constraint loss function based on Gaussian distribution is proposed, which preserves semantic variations while penalizes the overlapping part of different classes in the embedding space. To address the discrete optimization problem introduced by binary codes, a two-step optimization strategy is proposed to provide efficient training and avoid the problem of gradient vanishing. Extensive experiments on five large-scale benchmark databases show that our model can achieve the state-of-the-art retrieval performance.																	2162-237X	2162-2388				MAY	2020	31	5					1681	1695		10.1109/TNNLS.2019.2921805													
J								Energy Disaggregation via Deep Temporal Dictionary Learning	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Dictionaries; Hidden Markov models; Signal processing algorithms; Machine learning; Home appliances; Optimization; Computational modeling; Deep learning; dictionary learning (DL); energy disaggregation (ED); long short-term memory autoencoder (LSTM-AE)	ARCHITECTURE; POWER	This paper presents a novel nonlinear dictionary learning (DL) model to address the energy disaggregation (ED) problem, i.e., decomposing the electricity signal of a home to its operating devices. First, ED is modeled as a new temporal DL problem where a set of dictionary atoms is learned to capture the most representative temporal features of electricity signals. The sparse codes corresponding to these atoms show the contribution of each device in the total electricity consumption. To learn powerful atoms, a novel deep temporal DL (DTDL) model is proposed that computes complex nonlinear dictionaries in the latent space of a long short-term memory autoencoder (LSTM-AE). While the LSTM-AE captures the deep temporal manifold of electricity signals, the DTDL model finds the most representative atoms inside this manifold. To simultaneously optimize the dictionary and the deep temporal manifold, a new optimization algorithm is proposed that alternates between finding the optimal LSTM-AE and the optimal dictionary. To the best of authors' knowledge, DTDL is the only DL model that understands the deep temporal structures of the data. Experiments on the Reference ED Data Set show an outstanding performance compared with the recent state-of-the-art algorithms in terms of precision, recall, accuracy, and F-score.																	2162-237X	2162-2388				MAY	2020	31	5					1696	1709		10.1109/TNNLS.2019.2921952													
J								SVRG-MKL: A Fast and Scalable Multiple Kernel Learning Solution for Features Combination in Multi-Class Classification Problems	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Kernel; Scalability; Task analysis; Complexity theory; Memory management; Optimization; Convergence; Compact features combination; multiple kernel learning (MKL); scalability; stochastic variance reduced gradient (SVRG); variance reduction	DESCRIPTORS; MACHINE; MATRIX	In this paper, we present a novel strategy to combine a set of compact descriptors to leverage an associated recognition task. We formulate the problem from a multiple kernel learning (MKL) perspective and solve it following a stochastic variance reduced gradient (SVRG) approach to address its scalability, currently an open issue. MKL models are ideal candidates to jointly learn the optimal combination of features along with its associated predictor. However, they are unable to scale beyond a dozen thousand of samples due to high computational and memory requirements, which severely limits their applicability. We propose SVRG-MKL, an MKL solution with inherent scalability properties that can optimally combine multiple descriptors involving millions of samples. Our solution takes place directly in the primal to avoid Gram matrices computation and memory allocation, whereas the optimization is performed with a proposed algorithm of linear complexity and hence computationally efficient. Our proposition builds upon recent progress in SVRG with the distinction that each kernel is treated differently during optimization, which results in a faster convergence than applying off-the-shelf SVRG into MKL. Extensive experimental validation conducted on several benchmarking data sets confirms a higher accuracy and a significant speedup of our solution. Our technique can be extended to other MKL problems, including visual search and transfer learning, as well as other formulations, such as group-sensitive (GMKL) and localized MKL (LMKL) in convex settings.																	2162-237X	2162-2388				MAY	2020	31	5					1710	1723		10.1109/TNNLS.2019.2922123													
J								Cooperative Adaptive Output Regulation for Lower Triangular Nonlinear Multi-Agent Systems Subject to Jointly Connected Switching Networks	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Multi-agent systems; Switches; Regulation; Adaptation models; Observers; Eigenvalues and eigenfunctions; Adaptive distributed observer; cooperative output regulation; learning control; nonlinear multi-agent systems; switching networks	CONSENSUS; TRACKING; SYNCHRONIZATION; AGENTS	The cooperative global robust output regulation problem for multi-agent systems is a generalization of the leader-following consensus problem. The problem has been studied for various multi-agent systems over connected static networks and for some special classes of nonlinear multi-agent systems over jointly connected switching networks. In this paper, we further consider the same problem for a class of heterogeneous lower triangular nonlinear multi-agent systems over jointly connected switching networks. This class of systems is quite general in that it contains inverse dynamics, is of any order, and its subsystems can have different relative degrees. We will integrate the adaptive distributed observer and the distributed internal model approach to come up with a recursive approach to deal with our problem. We will also apply our approach to a leader-following consensus problem for a group of hyperchaotic Lorenz systems.																	2162-237X	2162-2388				MAY	2020	31	5					1724	1734		10.1109/TNNLS.2019.2922174													
J								Disturbance Observer-Based Neural Network Control of Cooperative Multiple Manipulators With Input Saturation	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Manipulator dynamics; Robot kinematics; Neural networks; Kinematics; Force; Adaptive neural network control; distubance observer; input saturation; multi-manipulator collaborative control; robot	HUMAN-ROBOT INTERACTION; NONLINEAR-SYSTEMS; IMPEDANCE CONTROL; VIBRATION CONTROL; ADAPTIVE-CONTROL; FEEDBACK-CONTROL; CONTROL DESIGN; ROBUST-CONTROL; CONTROL SCHEME; EXOSKELETON	In this paper, the complex problems of internal forces and position control are studied simultaneously and a disturbance observer-based radial basis function neural network (RBFNN) control scheme is proposed to: 1) estimate the unknown parameters accurately; 2) approximate the disturbance experienced by the system due to input saturation; and 3) simultaneously improve the robustness of the system. More specifically, the proposed scheme utilizes disturbance observers, neural network (NN) collaborative control with an adaptive law, and full state feedback. Utilizing Lyapunov stability principles, it is shown that semiglobally uniformly bounded stability is guaranteed for all controlled signals of the closed-loop system. The effectiveness of the proposed controller as predicted by the theoretical analysis is verified by comparative experimental studies.																	2162-237X	2162-2388				MAY	2020	31	5					1735	1746		10.1109/TNNLS.2019.2923241													
J								A Semisupervised Recurrent Convolutional Attention Model for Human Activity Recognition	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Training; Data models; Semisupervised learning; Training data; Labeling; Activity recognition; Deep learning; Attention; class imbalance; human activity recognition (HAR); semisupervised learning	ALGORITHMS	Recent years have witnessed the success of deep learning methods in human activity recognition (HAR). The longstanding shortage of labeled activity data inherently calls for a plethora of semisupervised learning methods, and one of the most challenging and common issues with semisupervised learning is the imbalanced distribution of labeled data over classes. Although the problem has long existed in broad real-world HAR applications, it is rarely explored in the literature. In this paper, we propose a semisupervised deep model for imbalanced activity recognition from multimodal wearable sensory data. We aim to address not only the challenges of multimodal sensor data (e.g., interperson variability and interclass similarity) but also the limited labeled data and class-imbalance issues simultaneously. In particular, we propose a pattern-balanced semisupervised framework to extract and preserve diverse latent patterns of activities. Furthermore, we exploit the independence of multi-modalities of sensory data and attentively identify salient regions that are indicative of human activities from inputs by our recurrent convolutional attention networks. Our experimental results demonstrate that the proposed model achieves a competitive performance compared to a multitude of state-of-the-art methods, both semisupervised and supervised ones, with 10% labeled training data. The results also show the robustness of our method over imbalanced, small training data sets.																	2162-237X	2162-2388				MAY	2020	31	5					1747	1756		10.1109/TNNLS.2019.2927224													
J								Finite-Time Consensus of Second-Order Switched Nonlinear Multi-Agent Systems	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Switches; Protocols; Nonlinear systems; Artificial neural networks; Learning systems; Finite-time consensus (FTC); multi-agent systems (MASs); neural networks (NNs); nonlinear systems; switched systems	DISTRIBUTED CONSENSUS; STABILIZATION; NETWORKS; FEEDBACK	In this brief, the practical finite-time consensus (FTC) problem is investigated for the second-order heterogeneous switched nonlinear multi-agent systems (MASs), where the subsystems and the switching signal for each agent are different. Mainly due to that agents' dynamics are switched and the unknown nonlinearities in the systems are more general, the practical FTC problem of the MASs is rather difficult to be solved by existing methods. As such, a new protocol design framework for the FTC problem is developed. Then, a novel adaptive protocol is proposed for the switched nonlinear MASs based on the developed design framework and the neural network method. The sufficient conditions for the practical FTC of nonlinear MASs under arbitrary switching are given. Finally, a numerical example is presented to demonstrate the effectiveness of the proposed control scheme.																	2162-237X	2162-2388				MAY	2020	31	5					1757	1762		10.1109/TNNLS.2019.2920880													
J								Direct Error-Driven Learning for Deep Neural Networks With Applications to Big Data	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Artificial neural networks; Noise measurement; Indexes; Big Data; Data models; Cost function; Error-driven; exploratory learning; generalization error; neural network	CHALLENGES	In this brief, heterogeneity and noise in big data are shown to increase the generalization error for a traditional learning regime utilized for deep neural networks (deep NNs). To reduce this error, while overcoming the issue of vanishing gradients, a direct error-driven learning (EDL) scheme is proposed. First, to reduce the impact of heterogeneity and data noise, the concept of a neighborhood is introduced. Using this neighborhood, an approximation of generalization error is obtained and an overall error, comprised of learning and the approximate generalization errors, is defined. A novel NN weight-tuning law is obtained through a layer-wise performance measure enabling the direct use of overall error for learning. Additional constraints are introduced into the layer-wise performance measure to guide and improve the learning process in the presence of noisy dimensions. The proposed direct EDL scheme effectively addresses the issue of heterogeneity and noise while mitigating vanishing gradients and noisy dimensions. A comprehensive simulation study is presented where the proposed approach is shown to mitigate the vanishing gradient problem while improving generalization by 6%.																	2162-237X	2162-2388				MAY	2020	31	5					1763	1770		10.1109/TNNLS.2019.2920964													
J								Distributed Training for Multi-Layer Neural Networks by Consensus	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Training; Neural networks; Consensus algorithm; Distributed databases; Topology; Convergence; Network topology; Backpropagation; consensus; distributed training; graph theory; Lyapunov		Over the past decade, there has been a growing interest in large-scale and privacy-concerned machine learning, especially in the situation where the data cannot be shared due to privacy protection or cannot be centralized due to computational limitations. Parallel computation has been proposed to circumvent these limitations, usually based on the master-slave and decentralized topologies, and the comparison study shows that a decentralized graph could avoid the possible communication jam on the central agent but incur extra communication cost. In this brief, a consensus algorithm is designed to allow all agents over the decentralized graph to converge to each other, and the distributed neural networks with enough consensus steps could have nearly the same performance as the centralized training model. Through the analysis of convergence, it is proved that all agents over an undirected graph could converge to the same optimal model even with only a single consensus step, and this can significantly reduce the communication cost. Simulation studies demonstrate that the proposed distributed training algorithm for multi-layer neural networks without data exchange could exhibit comparable or even better performance than the centralized training model.																	2162-237X	2162-2388				MAY	2020	31	5					1771	1778		10.1109/TNNLS.2019.2921926													
J								Aberrant Connectivity During Pilocarpine-Induced Status Epilepticus	INTERNATIONAL JOURNAL OF NEURAL SYSTEMS										Status epilepticus; aberrant connectivity; information flow mode; hippocampus; thalamus	GRAPH-THEORETICAL ANALYSIS; COGNITIVE IMPAIRMENT; BRAIN NETWORKS; ANIMAL-MODELS; DEFAULT MODE; EPILEPSY; EEG; METHODOLOGY; SEIZURES; MEMORY	Status epilepticus (SE) is a common, life-threatening neurological disorder that may lead to permanent brain damage. In rodent models, SE is an acute phase of seizures that could be reproduced by injecting with pilocarpine and then induce chronic temporal lobe epilepsy (TLE) seizures. However, how SE disrupts brain activity, especially communications among brain regions, is still unclear. In this study, we aimed to identify the characteristic abnormalities of network connections among the frontal cortex, hippocampus and thalamus during the SE episodes in a pilocarpine model with functional and effective connectivity measurements. We showed that the coherence connectivity among these regions increased significantly during the SE episodes in almost all frequency bands (except the alpha band) and that the frequency band with enhanced connections was specific to different stages of SE episodes. Moreover, with the effective analysis, we revealed a closed neural circuit of bidirectional effective interactions between the frontal regions and the hippocampus and thalamus in both ictal and post-ictal stages, implying aberrant enhancement of communication across these brain regions during the SE episodes. Furthermore, an effective connection from the hippocampus to the thalamus was detected in the delta band during the pre-ictal stage, which shifted in an inverse direction during the ictal stage in the theta band and in the theta, alpha, beta and low-gamma bands during the post-ictal stage. This specificity of the effective connection between the hippocampus and thalamus illustrated that the hippocampal structure is critical for the initiation of SE discharges, while the thalamus is important for the propagation of SE discharges. Overall, our results demonstrated enhanced interaction among the frontal cortex, hippocampus and thalamus during the SE episodes and suggested the modes of information flow across these structures for the initiation and propagation of SE discharges. These findings may reveal an underlying mechanism of aberrant network communication during pilocarpine-induced SE discharges and deepen our knowledge of TLE seizures.																	0129-0657	1793-6462				MAY	2020	30	5							1950029	10.1142/S0129065719500291													
J								Parsimonious Minimal Learning Machine via Multiresponse Sparse Regression	INTERNATIONAL JOURNAL OF NEURAL SYSTEMS										Minimal learning machine; reference points; multiresponse sparse regression	ALGORITHMS; APPROXIMATION; SELECTION	The training procedure of the minimal learning machine (MLM) requires the selection of two sets of patterns from the training dataset. These sets are called input reference points (IRP) and output reference points (ORP), which are used to build a mapping between the input geometric configurations and their corresponding outputs. In the original MLM, the number of input reference points is the hyper-parameter and the patterns are chosen at random. Therefore, the conventional proposal does not consider which patterns will belong to each reference point group, since the model does not implement an appropriate way of selecting the most suitable patterns as reference points. Such an approach can impact on the decision function in terms of smoothness, resulting in high complexity models. 'This paper introduces a new approach to select IRP for MLM applied to classification tasks. The optimally selected minimal learning machine (OS-MLM) relies on the multiresponse sparse regression (MRSR) ranking method and the leave-one-out (LOO) criterion to sort the patterns in terms of relevance and select an appropriate number of input reference points, respectively. The experimental assessment conducted on UCI datasets reports the proposal was able to produce sparser models and achieve competitive performance when compared to the regular strategy of selecting MLM input RPs.																	0129-0657	1793-6462				MAY	2020	30	5							2050023	10.1142/S0129065720500239													
J								Alternative Diagnosis of Epilepsy in Children Without Epileptiform Discharges Using Deep Convolutional Neural Networks	INTERNATIONAL JOURNAL OF NEURAL SYSTEMS										Nonepileptic event; epilepsy; epileptiform discharge; EEG; deep convolutional neural network	LEAGUE-AGAINST-EPILEPSY; SEIZURES; CLASSIFICATION; MISDIAGNOSIS; MOBILE	Numerous nonepileptic paroxysmal events, such as syncope and psychogenic nonepileptic seizures, may imitate seizures and impede diagnosis. Misdiagnosis can lead to mistreatment, affecting patients' lives considerably. Electroencephalography is commonly used for diagnosing epilepsy. Although on electroencephalograms (EEGs), epileptiform discharges (ED) specifically indicate epilepsy, only approximately 50% of patients with epilepsy have ED in their first EEG. In this study, we developed a deep convolutional neural network (ConvNet)-based classifier to distinguish EEG between patients with epilepsy without ED and controls. Overall, 25 patients with epilepsy without ED in their EEGs and 25 age-matched patients with Tourette syndrome or syncope were enrolled. Their EEGs were classified using the deep ConvNet. When the EEG data without overlapping were used, the accuracy, sensitivity, and specificity were 65.00%, 48.00%, and 82.00%, respectively. The performance measures improved when the input EEG data were augmented through overlapping. With 95% EEG data overlapping, the accuracy, sensitivity, and specificity increased to 80.00%, 70.00%, and 90.00%, respectively. The proposed method could be regarded as a pilot study to demonstrate a proof of concept of a potential diagnostic value of deep ConvNet in patients with epilepsy without ED. Further studies are needed to assist neurologists in distinguishing nonepileptic paroxysmal events from epilepsy.																	0129-0657	1793-6462				MAY	2020	30	5							1850060	10.1142/S0129065718500600													
J								Modeling Hippocampal CA1 Gabaergic Synapses of Audiogenic Rats	INTERNATIONAL JOURNAL OF NEURAL SYSTEMS										Wistar audiogenic rats; CA1 pyramidal cells; inhibitory postsynaptic currents; interevent interval; release frequency	AMACRINE AII CELLS; FUNCTIONAL-PROPERTIES; TRANSMITTER RELEASE; IN-VITRO; INHIBITION; RECEPTORS; CHANNELS; NETWORKS; NUMBER; RECRUITMENT	Wistar Audiogenic Rats (WARs) are genetically susceptible to sound-induced seizures that start in the brainstem and, in response to repetitive stimulation, spread to limbic areas, such as hippocampus. Analysis of the distribution of interevent intervals of GABAergic inhibitory postsynaptic currents (IPSCs) in CA1 pyramidal cells showed a monoexponential trend in Wistar rats, suggestive of a homogeneous population of synapses, but a biexponential trend in WARs. Based on this, we hypothesize that there are two populations of GABAergic synaptic release sites in CA1 pyramidal neurons from WARs. To address this hypothesis, we used a well-established neuronal computational model of a CA1 pyramidal neuron previously developed to replicate physiological properties of these cells. Our simulations replicated the biexponential trend only when we decreased the release frequency of synaptic currents by a factor of six in at least 40% of distal synapses. Our results suggest that almost half of the CABAergic synapses of WARS have a drastically reduced spontaneous release frequency. The computational model was able to reproduce the temporal dynamics of GABAergic inhibition that could underlie susceptibility to the spread of seizures.																	0129-0657	1793-6462				MAY	2020	30	5							2050022	10.1142/S0129065720500227													
J								Cortical Asymmetries and Connectivity Patterns in the Valence Dimension of the Emotional Brain	INTERNATIONAL JOURNAL OF NEURAL SYSTEMS										EEG; asymmetries; differential entropy; coherence; emotions; valence dimension	FUNCTIONAL CONNECTIVITY; CEREBRAL ASYMMETRY; EEG; COHERENCE; SYNCHRONIZATION; NEUROANATOMY; METAANALYSIS; COMPLEXITY; ATTENTION	Understanding the neurophysiology of emotions, the neuronal structures involved in processing emotional information and the circuits by which they act, is key to designing applications in the field of affective neuroscience, to advance both new treatments and applications of brain-computer interactions. However, efforts have focused on developing computational models capable of emotion classification instead of on studying the neural substrates involved in the emotional process. In this context, we have carried out a study of cortical asymmetries and functional cortical connectivity based on the electroencephalographic signal of 24 subjects stimulated with videos of positive and negative emotional content to bring some light to the neurobiology behind emotional processes. Our results show opposite interhemispheric asymmetry patterns throughout the cortex for both emotional categories and specific connectivity patterns regarding each of the studied emotional categories. However, in general, the same key areas, such as the right hemisphere and more anterior cortical regions, presented higher levels of activity during the processing of both valence emotional categories. These results suggest a common neural pathway for processing positive and negative emotions, but with different activation patterns. These preliminary results are encouraging for elucidating the neuronal circuits of the emotional valence dimension.																	0129-0657	1793-6462				MAY	2020	30	5							2050021	10.1142/S0129065720500215													
J								A Training Data-Driven Canonical Correlation Analysis Algorithm for Designing Spatial Filters to Enhance Performance of SSVEP-Based BCIs	INTERNATIONAL JOURNAL OF NEURAL SYSTEMS										Brain computer interface; steady-state visual evoked potential; canonical correlation analysis; task-related component analysis; training data	BRAIN-COMPUTER INTERFACE; VISUAL-EVOKED POTENTIALS; FREQUENCY RECOGNITION; COMPONENT ANALYSIS; MENTAL PROSTHESIS; SPEED; P300; COMMUNICATION	Canonical correlation analysis (CCA.) is an effective spatial filtering algorithm widely used in steady-state visual evoked potential (SSVEP)-based brain-computer interfaces (BCIs). In existing CCA methods, training data are used for constructing templates of stimulus targets and the spatial filters are created between the template signals and a single-trial testing signal. The fact that spatial filters rely on testing data, however, results in low classification performance of CCA compared to other state-of-the-art algorithms such as task-related component analysis (TRCA). In this study, we proposed a novel CCA method in which spatial filters are estimated using training data only. This is achieved by using observed EEG training data and their SSVEP components as the two inputs of CCA and the objective function is optimized by averaging multiple training trials. In this case, we proved in theory that the two spatial filters estimated by the CCA are equivalent, and that the CCA and TRCA are also equivalent under certain hypotheses. A benchmark SSVEP data set from 35 subjects was used to compare the performance of the two algorithms according to different lengths of data, numbers of channels and numbers of training trials. In addition, the CCA was also compared with power spectral density analysis (PSDA). The experimental results suggest that the CCA is equivalent to TRCA if the signal-to-noise ratio of training data is high enough; otherwise, the CCA outperforms TRCA in terms of classification accuracy. The CCA is much faster than PSDA in detecting time of targets. The robustness of the training data-driven CCA to noise gives it greater potential in practical applications.																	0129-0657	1793-6462				MAY	2020	30	5							2050020	10.1142/S0129065720500203													
J								An efficient truthfulness privacy-preserving tendering framework for vehicular fog computing	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Vehicular fog computing; Tendering; Incentive mechanism; Privacy preservation	AGGREGATE; SECURITY; VEHICLE; AUCTION	This paper presents a proposal for a tendering-based incentive framework in order to encourage vehicle owners to join in announced tasks in the vehicular fog computing. The truthfulness of users is ensured by using the incentive mechanism that also assists a fog node server to choose suitable resources for the task. An illustrative language, which is a novel approach to guaranteeing fairness amongst vehicles, is designed based on heterogeneous vehicular resource types. The signcryption technique and a homomorphic concept are integrated in the proposed framework in order to preserve vehicles privacy. Moreover, a detailed performance analysis demonstrates that the communication and computational overheads of this privacy-preserving scheme are significantly more efficient than the available alternatives.																	0952-1976	1873-6769				MAY	2020	91								103583	10.1016/j.engappai.2020.103583													
J								Improved K2 algorithm for Bayesian network structure learning	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Bayesian network; Structure learning; Score-based algorithms; Constraint-based algorithm	COMPUTATIONAL INTELLIGENCE; SEARCH ALGORITHM; SPACE; TESTS	In this paper, we study the problem of learning the structure of Bayesian networks from data, which takes a dataset and outputs a directed acyclic graph. This problem is known to be NP-hard. Almost most of the existing algorithms for structure learning can be classified into three categories: constraint-based, score-based, and hybrid methods. The K2 algorithm, as a score-based algorithm, takes a random order of variables as input and its efficiency is strongly dependent on this ordering. Incorrect order of variables can lead to learning an incorrect structure. Therefore, the main challenge of this algorithm is strongly dependency of output quality on the initial order of variables. The main contribution of this paper is to derive a significant order of variables from the given dataset. Also, one of the significant challenges of structure learning is to find a practical structure learning approach to learn an optimal structure from complex and high-dimensional datasets in a reasonable time. We propose a new fast and straightforward algorithm for addressing this problem in a reasonable time. The proposed algorithm is based on an ordering by extracting strongly connected components of the graph built from data. We reduce the super-exponential search space of structures to the smaller space of nodes ordering. We evaluated the proposed algorithm using some standard benchmark datasets and compare the results with the results obtained from some state of the art algorithms. Finally, we show that the proposed algorithm is competitive with some algorithms for structure learning.																	0952-1976	1873-6769				MAY	2020	91								103617	10.1016/j.engappai.2020.103617													
J								Two-level principal-agent model for schedule risk control of IT outsourcing project based on genetic algorithm	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										IT outsourcing project; Schedule risk; Risk control; Principal-agent theory; Genetic algorithm	DECISION-MAKING; CROSSOVER OPERATOR; MANAGEMENT; IMPACT	With increasing developments in the Information Technology (IT) outsourcing industry, many enterprises outsource IT services to reduce costs. However, the schedule risk of IT outsourcing (ITO) projects may result in enormous economic losses for an enterprise. In this paper, the principal-agent theory is used to control the schedule risk of ITO projects. A two-level mathematical model is built to describe the decision process of the client and vendors. With an increase to the number of subprojects and activities, the scale of the problem will become very large. The resulting optimization is an NP hard problem with continuous domain. Therefore, a genetic algorithm (GA) is designed to solve the proposed model. Experiments are performed to test the ability of the proposed algorithm. Some insights from simulation analysis - the principal-agent theory and two-level mathematical model - are suitable for describing the cooperative relationship between principle and agent. By comparing with ant colony optimization and simulated annealing, the proposed GA shows strong optimization abilities for convergence, reliability, and efficiency, which is a good tool for this kind of optimization problem. The near-optimal plan reduced the schedule risk of the project remarkably, which is the scientific quantitative proposal for the decision maker. This study provides practitioners insights on relationships of schedule risk and ITO projects, and the design model and algorithms of this paper provides practitioners effective potential method to reduce the schedule risk of ITO projects in their operations. However, the uncertain characteristics of key and multiple factors should be considered in future work. Stochastic Programming and the Monte Carlo Simulation Method are two potential tools for dealing with uncertain factors. Additionally, the proposed GA could potentially be improved in terms of convergence. The advantages of other intelligent algorithms could be applied to the GA in order to improve its searching ability, such as the Taboo mechanism.																	0952-1976	1873-6769				MAY	2020	91								103584	10.1016/j.engappai.2020.103584													
J								Multi-label Log-Loss function using L-BFGS for document categorization	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Multi-label classification; Text mining; Quasi-Newton method; Holy Quran; Corpus analysis; BFGS; Scikit-learn; Artificial neural networks; Text classification		Text mining, which fundamentally involves quantitative tactics to analyze textual data, can be used for discovering knowledge and to achieve scholarly research goals. For large-scale data such as corpus text, intelligent learning methods have been effectively approached. In this paper, an artificial neural network with a quasi-Newton updating procedure is presented for multi-label multi-class text classification. This numerical unconstrained training technique, the Multi-Label extension of Log-Loss function using in Limited-memory Broyden-Fletcher-Goldfarb-Shanno algorithm (ML4BFGS), provides a noteworthy opportunity for text mining and leads to a significant improvement in text classification performances. The ML4BFGS training approach is applied to allocate some (one or multi) of the classes to each corresponding sentence from different available labels. We evaluate this method on English translations of the Holy Quran. These religious texts have been chosen for experiments of this manuscript because each verse (sentence) usually has multiple labels (topics) and different translations of each verse should have the same labels. Experimental results show that ML4BFGS is talented for multi-label multi-class classification in the Quranic corpus. Evaluation criteria of some advanced updating methods such as ITCG, BFGS, L-BFGS-B, L3BFGS as well as some other multi-label approaches such as ML-k-NN, and well-known SVM are compared with the proposed ML4BFGS and the outcomes are fullydescribed in this study. The performance measures including the Hamming loss, recall, precision, and F1 score show that the ML4BFGS achieves the best results in extracting related classes for each verse, while the proposed network takes the least epochs compared to the other training approach for completing learning or training phase. Simultaneously, the elapsed time for ML4BFGS is just 78% (in seconds) of the best experience of this term. Compared with the applicability of some state-of-the-art algorithms, ML4BFGS has a less computational cost, faster convergence rate, and much accuracy in corpus analysis.																	0952-1976	1873-6769				MAY	2020	91								103623	10.1016/j.engappai.2020.103623													
J								STDnet: Exploiting high resolution feature maps for small object detection	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Small object detection; Convolution neural networks (ConvNets); Deep learning	SEGMENTATION	The accuracy of small object detection with convolutional neural networks (ConvNets) lags behind that of larger objects. This can be observed in popular contests like MS COCO. This is in part caused by the lack of specific architectures and datasets with a sufficiently large number of small objects. Our work aims at these two issues. First, this paper introduces STDnet, a convolutional neural network focused on the detection of small objects that we defined as those under 16 x 16 pixels. The high performance of STDnet is built on a novel early visual attention mechanism, called Region Context Network (RCN), to choose the most promising regions, while discarding the rest of the input image. Processing only specific areas allows STDnet to keep high resolution feature maps in deeper layers providing low memory overhead and higher frame rates. High resolution feature maps were proved to be key to increasing localization accuracy in such small objects. Second, we also present USC-GRAD-STDdb, a video dataset with more than 56,000 annotated small objects in challenging scenarios. Experimental results over USC-GRAD-STDdb show that STDnet improves the AP(@. 5) of the best state-of-the-art object detectors for small target detection from 50.8% to 57.4%. Performance has also been tested in MS COCO for objects under 16 x 16 pixels. In addition, a spatio-temporal baseline network, STDnet-bST, has been proposed to make use of the information of successive frames, increasing the AP(@. 5) of STDnet in 2.3%. Finally, optimizations have been carried out to be fit on embedded devices such as Jetson TX2.																	0952-1976	1873-6769				MAY	2020	91								103615	10.1016/j.engappai.2020.103615													
J								An extended MULTIMOORA method based on OWGA operator and Choquet integral for risk prioritization identification of failure modes	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Failure mode and effects analysis; Trapezoidal fuzzy number; OWGA operator; Choquet integral; MULTIMOORA	DECISION-MAKING; AGGREGATION OPERATORS; BAYESIAN NETWORKS; FUZZY; FMEA; SYSTEM; TOPSIS; UNCERTAINTY; FRAMEWORK; MATRIX	Failure mode and effects analysis (FMEA) is one of the important methods for risk analysis, and has been used in various fields to improve the reliability of systems. However, in the fuzzy environment, the aggregation, weight calculation, risk evaluation and prioritization of evaluation information limit its wide application. Therefore, this paper proposes an extended multi-objective optimization by ratio analysis plus full multiplicative form (MULTIMOORA) method based on the ordered weighted geometric averaging (OWGA) operator and Choquet integral for FMEA. Firstly, trapezoidal fuzzy numbers (TrFNs) are used for describing the fuzzy ratings of failure modes. Secondly, considering the uncertainty preferences of decision-makers, the OWGA operator is adopted for aggregating the evaluation information given by multiple decision-makers. Thirdly, the TrFNs ranking method based on the relative preference relation is combined with Choquet integral to model the interactions among risk factors, thus capturing their importance weights. On this basis, an extended MULTIMOORA method is proposed to determine the risk priority of failure modes. Finally, two case studies are provided to illustrate the effectiveness and practicability of the proposed method.																	0952-1976	1873-6769				MAY	2020	91								103605	10.1016/j.engappai.2020.103605													
J								Purifying real images with an attention-guided style transfer network for gaze estimation	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Gaze estimation; Style transfer; Attention-guided style transfer network; Learning-by-synthesis	REGRESSION; APPEARANCE	Recently, the progress of learning-by-synthesis has proposed a training model for synthetic images, which can effectively reduce the cost of human and material resources. Image synthesis has been widely accepted as a cost effective way to learn models because it provides training sets that are large, diverse and accurately labeled. However, the realism of the synthetic image is not enough, this affects generalization on naturalistic test image. In an attempt to address this issue, previous methods learn a model to improve the realism of synthetic image. Different from previous methods, we take the first step towards purifying the real image to weaken the influence of light and convert the distribution of an outdoor naturalistic image through a real-time style transfer task to that of indoor synthetic image. In this paper, we first introduce the segmentation masks to construct Red, Green, and Blue-mask (RGB-mask) pairs as inputs, then we design an attention-guided style transfer network to learn style features separately from the attention and background regions, learn content features from full and attention regions. Moreover, we propose a novel region-level task-guided loss to restrain the features learnt from style and content. Experiments were performed using a mixed research (qualitative and quantitative) method to demonstrate the possibility of purifying real images in complex directions. We evaluate the proposed method on three public datasets, including Labeled pupils in the wild (LPW), Common Objects in COntext (COCO) and MPIIGaze. Extensive experimental results show that the proposed method is effective and achieves the state-of-the-art results.																	0952-1976	1873-6769				MAY	2020	91								103609	10.1016/j.engappai.2020.103609													
J								An encoder-decoder approach to mine conditions for engineering textual data	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Condition mining; Natural language processing; Neural networks	SENTIMENT ANALYSIS; EXTRACTION; SYSTEM	Data engineering seeks to support artificial intelligence processes that extract knowledge from raw data. Many such data are rendered in natural language from which entity-relation extractors extract facts and opinion miners extract opinions; the goal of condition mining is to mine the conditions that have an influence on them. In this article, a new condition mining method is proposed. It relies on a deep neural network and attempts to overcome the limitations of existing methods for condition mining that we reviewed. The materials used include readily-available software components for natural language processing and a large multi-lingual, multi-topic dataset. The common information retrieval performance measures were used to assess the results, namely: precision, which is the fraction of correct conditions to the mined ones, recall, which is the fraction of correct conditions that have been mined to the total number of correct conditions, and the.. 1 score, which is the harmonic mean of precision and recall. The results of the experimental analysis prove that the new proposal can attain an.. 1 score that is significantly greater than with existing methods. Furthermore, a comprehensive analysis of the dataset was performed, which revealed two key findings: the connectives follows a long-tail distribution and the conditions are quite dissimilar from a semantic point of view.																	0952-1976	1873-6769				MAY	2020	91								103568	10.1016/j.engappai.2020.103568													
J								A novel Gait-Appearance-based Multi-Scale Video Covariance Approach for pedestrian (re)-identification	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Pedestrian (re)-identification; Video surveillance Data Mining; Gait; Unsupervised learning; Multi-scale covariance features; Video-Tree Structure	PERSON REIDENTIFICATION; RECOGNITION; DESCRIPTOR; SPARSE	In order to handle the complex databases of acquired images in the security area, a robust and adaptive framework for Video Surveillance Data Mining as well as for multi-shot pedestrian (re)-identification is required. The pedestrian's signature must be invariant and robust against the noise and uncontrolled variation. In this paper a new fast Gait-Appearance-based Multi-Scale Video Covariance (GAMS-ViCov) unsupervised approach was proposed to efficiently describe any image-sequence, on streaming or stored in the database, of a pedestrian into a compact and fixed size signature while exploiting the whole relevant spatiotemporal information. The proposed model is based on multi-scale features extracted from a novel data structure called 'Two-Half-Video-Tree' (THVT) which represents the pedestrians and allows discarding the uncontrolled variations. THVT can efficiently model the gait and appearance of the upper and lower parts of the person's silhouette into trees of multi-scale features. THVT can thus model the video data to new structured forms through a fast algorithm. Furthermore, GAMS-ViCov approach can also be competitive as a technique of dynamic video summarization using k-means clustering to model the signatures extracted from the image-sequences of each person into a cluster center. For each person's cluster, the image-sequence that its signature is nearest to the centroid is selected and stored as the key image-sequence of this person. The proposed approach was evaluated for the person (re)-identification with i-LIDS and PRID databases. The experimental results show that GAMS-ViCov outperforms the most of unsupervised approaches.																	0952-1976	1873-6769				MAY	2020	91								103566	10.1016/j.engappai.2020.103566													
J								Fault diagnosis using novel AdaBoost based discriminant locality preserving projection with resamples	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Fault diagnosis; Discriminant locality preserving projection; AdaBoost; Resample; Process industry	PARTIAL LEAST-SQUARES; FACE RECOGNITION; PCA; DECOMPOSITION	Fault diagnosis plays a pivotal role in ensuring the safety of process industries. However, due to the diversity of process faults and the high coupling of fault data, it becomes very difficult to achieve high accuracy in the fault diagnosis of complex industrial processes. To address this concern, in this article, a novel AdaBoost-based discriminant locality preserving projection (DLPP) with resamples (A-DLPPR) model is proposed. The proposed A-DLPPR model has two features: to address the problem of matrix decomposition in DLPP, the bootstrap method is utilized to generate groups of resample data, and to obtain high classification accuracy, the AdaBoost-based classification technique is adopted. Finally, an effective fault diagnosis model using the proposed A-DLPPR model can be established. To validate the effectiveness of the proposed A-DLPPR model, the Tennessee Eastman process (TEP) is selected, and case studies using different kinds of TEP faults are conducted. The simulation results indicate that the proposed A-DLPPR model can achieve higher fault diagnosis accuracy than some other models, which verifies that in the field of complex industrial processes, the proposed A-DLPPR method can be used as an effective model for fault diagnosis.																	0952-1976	1873-6769				MAY	2020	91								103631	10.1016/j.engappai.2020.103631													
J								Interpretable policies for reinforcement learning by empirical fuzzy sets	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Interpretable fuzzy systems; Reinforcement learning; Probability distribution learning; Autonomous learning systems; AnYa type fuzzy systems; Empirical Fuzzy Sets	SYSTEMS; CONTROLLERS	This paper proposes a method and an algorithm to implement interpretable fuzzy reinforcement learning (IFRL). It provides alternative solutions to common problems in RL, like function approximation and continuous action space. The learning process resembles that of human beings by clustering the encountered states, developing experiences for each of the typical cases, and making decisions fuzzily. The learned policy can be expressed as human-intelligible IF-THEN rules, which facilitates further investigation and improvement. It adopts the actor-critic architecture whereas being different from mainstream policy gradient methods. The value function is approximated through the fuzzy system AnYa. The state-action space is discretized into a static grid with nodes. Each node is treated as one prototype and corresponds to one fuzzy rule, with the value of the node being the consequent. Values of consequents are updated using the Sarsa(lambda) algorithm. Probability distribution of optimal actions regarding different states is estimated through Empirical Data Analytics (EDA), Autonomous Learning Multi-Model Systems (ALMMo), and Empirical Fuzzy Sets (epsilon FS). The fuzzy kernel of IFRL avoids the lack of interpretability in other methods based on neural networks. Simulation results with four problems, namely Mountain Car, Continuous Gridworld, Pendulum Position, and Tank Level Control, are presented as a proof of the proposed concept.																	0952-1976	1873-6769				MAY	2020	91								103559	10.1016/j.engappai.2020.103559													
J								Emotional, affective and biometrical states analytics of a built environment	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Emotional; Affective and biometrical states; Neuro-decision and neuro-correlation matrices; Built environment; Multiple-criteria analysis; VINERS method; VINERS circumplex model of affect	AIR-POLLUTION; TRACKING TECHNOLOGIES; SENSOR INFORMATION; HAPPINESS EVIDENCE; CIRCUMPLEX MODEL; TOURIST ACTIVITY; PUBLIC SPACES; RISK-FACTOR; TIME; SATISFACTION	Personal interests constitute the emphasis of client-centered, personalized marketing, which leads to personalized client fulfillment. Current shoppers are interested in more than simply buying products and services; shoppers are also interested in the surroundings of the shopping site. Everywhere in the world, an analysis of marketing value, with rare exceptions, does not integrate criteria relevant to the emotional, affective and biometrical states, valence and arousal of potential buyers. Such parameters require assessment for implementing an accurate and more effective, client-centered marketing process. This research, which required developing the Emotional, Affective and Biometrical States Analytics of the Built Environment (VINERS) Method, provides a "big picture'' of built environment neuromarketing. A multiple-criteria analysis integrated the emotional, affective and biometrical states of potential buyers and the surrounding environment (its physical, economic, social and environmental criteria). Neuro-decision and neuro-correlation matrices analysis constituted its basis. This research involved the accumulation and analysis of over 350 million remote data points, which aimed to ascertain the development of the biometrical, affective and emotional maps and sought to determine over 35,000 of average and strong correlations. The obtained dependencies constituted the basis for calculating and graphically submitting the VINERS circumplex model of affect, which the authors of this article had developed. This model is similar to Russell's circumplex model of affect. However, now, the VINERS Method has provided supplements offering new opportunities. Determination of an integrated emotional market rental (IEMR) value, provision of digital tips and optimization of the IEMR value are made possible by the VINERS Method.																	0952-1976	1873-6769				MAY	2020	91								103621	10.1016/j.engappai.2020.103621													
J								Hydrophobicity classification of composite insulators based on convolutional neural networks	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Composite insulators; Convolutional neural networks; Hydrophobicity classification; Image processing; Insulator testing; Spray method		This paper discusses the classification of composite insulators in hydrophobicity classes, according to the spray method of IEC Standard 62073, using convolutional neural networks. By applying the spray method, about 4500 photos were collected and are available online, from all hydrophobicity classes using distilled water-ethyl alcohol as spraying solution. Convolutional neural networks were trained, validated and tested, in order to determine the hydrophobicity class of composite insulators and to eliminate the operator's subjectivity, which is the main problem in this measurement. Various configuration setups of convolutional neural networks are applied and compared for their appropriateness in accurately classifying the composite insulators. The proposed methodology is a useful tool for the classification of composite insulators in hydrophobicity classes restricting the subjectivity of human judgment. The experiments showed this method gives almost 98% accuracy in this classification task. Therefore, the proposed methodology is helpful in maintaining of composite insulators.																	0952-1976	1873-6769				MAY	2020	91								103613	10.1016/j.engappai.2020.103613													
J								Interpretation and modeling of emotions in the management of autonomous robots using a control paradigm based on a scheduling variable	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Emotions; Decision-making systems; Cognitive modeling; Human mind; Computational models; Fuzzy approach; Intelligent systems; Autonomous agents	INTELLIGENT SYSTEM; GAIN; BEHAVIOR	The paper presents a technical introduction to psychological theories of emotions. It highlights a usable idea implemented in a number of recently developed computational systems of emotions, and the hypothesis that emotion can play the role of a scheduling variable in controlling autonomous robots. In the main part of this study, we outline our own computational system of emotion - xEmotion - designed as a key structural element in the developed target device, being an Intelligent System of Decision-making (ISD) for autonomous and robotic units. The ISD system has a cognitive architecture based on the principles of human psychology. The main purpose of building such a system is to prepare a framework for autonomous units used in system engineering (Kowalczuk and Czubenko, 2011; Czubenko et al., 2015). In particular, ISD is based on the concepts of cognitive psychology (in information processing) and motivation theory, which includes the system of needs (for decision-making). The xEmotion subsystem, however, focuses on modeling an alternative approach based on emotion. The xEmotion implementation covers aspects of somatic, appraisal and evolutionary theories of emotions using fuzzy sets. In this article, we also illustrate the core emotional behavior of the ISD system using simulation. The first application is a user interface for identifying emotions and predicting human behavior. The second is an eSailor simulation, which illustrates the possible behavior of the xEmotion subsystem. The last is an xDriver simulation experiment, which is to prove the validity of the concept of using emotion-based systems, according to the SVC principle. In summary, we also discuss other possible applications of the xEmotion system.																	0952-1976	1873-6769				MAY	2020	91								103562	10.1016/j.engappai.2020.103562													
J								Ensemble-based active learning using fuzzy-rough approach for cancer sample classification	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Ensemble learning; Active learning; Cancer classification; Gene expression data; Fuzzy set; Rough set	GENE-EXPRESSION DATA; TUMOR CLASSIFICATION; CLUSTER-ANALYSIS; PREDICTION; ALGORITHM; SELECTION; SVM	Background and Objective: Classification of cancer from gene expression data is one of the major research areas in the field of machine learning and medical science. Generally, conventional supervised methods are not able to produce desired classification accuracy due to inadequate training samples present in gene expression data to train the system. Ensemble-based active learning technique in this situation can be effective as it determines few informative samples by all the base classifiers and ensemble the decisions of all the base classifiers to get the most informative samples. Most informative samples are labeled by the subject experts and those are added to the training set, which can improve the classification accuracy. Method: We propose a novel ensemble-based active learning using fuzzy-rough approach for cancer sample classification from microarray gene expression data. The proposed method is able to deal with the uncertainty, overlap and indiscernibility usually present in the subtype classes of the gene expression data and can improve the accuracy of the individual base classifier in presence of limited training samples. Results: The proposed method is validated using eight microarray gene expression datasets. The performance of the proposed method in terms of classification accuracy, precision, recall, F-1-measures and kappa is compared with six other methods. The improvements in accuracy achieved by the proposed method compared to its nearest competitive methods are 2.96%, 9.34%, 0.93%, 3.69%, 7.2% and 4.53% respectively for Colon cancer, Prostate cancer, SRBCT, Ovarian cancer, DLBCL and Central nervous system datasets. Results of the paired t-test justify the statistical relevance of the results in favor of the proposed method for most of the datasets. Conclusion: The proposed method is an effective general purpose ensemble-based active learning adopting the fuzzy-rough concept and therefore can be applied for other classification problem in future.																	0952-1976	1873-6769				MAY	2020	91								103591	10.1016/j.engappai.2020.103591													
J								Learning on robot skills: Motion adjustment and smooth concatenation of motion blocks	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Kinesthetic teaching; Motion block; Motion adjustment; Smooth motion concatenation	REPRODUCTION; GENERATION; MODEL	Previous studies involving robot skill learning have focused on learning to encode and regenerate a simple motion trajectory. Few studies have been conducted on robot skill learning of complex tasks. In this study, kinesthetic teaching was adopted to train basic robot motion blocks, which were adjusted according to the environmental situation and concatenated to accomplish a specified complex task. The difficulties were adjusting and concatenating motion blocks to render the robot motion smooth and natural. Regarding motion adjustment, the Cartesian trajectories of motion blocks were translated, rotated, and scaled to a target position and their joint trajectories were optimized to be similar to those in a human demonstration. Regarding smooth motion concatenation, a weighting function was proposed to smoothly connect two successive adjusted motion blocks. A humanoid robot arm was used to validate the proposed skill learning method. The experiment results indicated that the robot was able to generate smooth Cartesian and joint trajectories to reach to a cup, grasp it, and pour water.																	0952-1976	1873-6769				MAY	2020	91								103619	10.1016/j.engappai.2020.103619													
J								A hybrid DEMATEL-FRACTAL method of handling dependent evidences	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Dempster-Shafer evidence theory; Dependence; DEMATEL; Discounting evidence; Fractal; Decision-making	MULTISENSOR DATA FUSION; DEMPSTER-SHAFER THEORY; FUZZY DEMATEL; BELIEF; COMBINATION; FRAMEWORK	Dempster combination rule in evidence theory is widely used in data fusion system. One assumption of Dempster combination rule is the independence among different evidences. However, it is difficult to satisfy this requirement due to many influencing factors. The main contribution of this paper is to propose a systematic model to deal with dependence evidences in evidence theory. The core of the model can be divided into two parts: handling inner dependence and handling outer dependence. For the inner dependence, we use DEMATEL model to establish the relationships between different components in the system and get their relative weights considering their influences. For the outer dependence, we use BPAs inherent fractals features to deal with the uncertainty in the outer environment as well as the dependence among each collected evidence. After that we combine these two aspects of dependence and make decisions. By discounting evidences we avoid redundant calculation thus obtain useful information as much as possible. A case study of transportation project selection problem is used to illustrate our proposed method.																	0952-1976	1873-6769				MAY	2020	91								103543	10.1016/j.engappai.2020.103543													
J								Alternatives selection for produced water management: A network-based methodology	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Produced water management; Visibility graph; Decision making; Power average operator; Complex networks	VISIBILITY GRAPH; COMPLEX NETWORKS; DECISION-MAKING; TIME-SERIES; OPERATOR	Products and process strategies selection has attracted much attention, especially for produced water management. In order to select best alternatives, it is necessary to develop a comprehensive decision-making methodology applicable to complex and uncertain systems. In this paper, we propose a new network-based methodology of decision-making. Firstly, the discrete quantitative and qualitative input data are mapped into networks based on ordered visibility graph. Then we take advantages of network topologies, which are node degree and node distance, to establish the relationships among the input data. Finally, the data are aggregated into a single value to make a decision. The case study indicates that our method is advantageous to make decisions objectively while inheriting the characteristics of network structures.																	0952-1976	1873-6769				MAY	2020	91								103556	10.1016/j.engappai.2020.103556													
J								Development and implementation of induction motor drive using sliding-mode based simplified neuro-fuzzy control	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Induction motor; Neuro-fuzzy sliding-mode control (NFSMC); Feedback linearization	DIRECT TORQUE CONTROL; FEEDBACK LINEARIZATION CONTROL; PERFORMANCE; SYSTEMS; ANFIS	This paper develops a sliding-mode based simplified structure of neuro-fuzzy speed and torque compensator incorporated with an induction motor (IM) drive deploying feedback linearization (FBL). The intuitive linearization technique with the proposed simplified structure neuro-fuzzy sliding-mode control (NFSMC) considerably improves the torque and speed responses under system uncertainty and outer load disturbance, giving optimal system performance. This proposed technique also has high computational efficiency due to single error input over conventional one and thus can easily be applied for industrial uses. The parameter tuning of the simplified neuro-fuzzy control (NFC) is done by sliding-mode control (SMC) based adaptive mechanism. The proposed simplified method based linearized drive is simulated as well as experimentally investigated using low-cost DSP2812. The responses prove that the drive system performance characteristics using proposed simplified NFSMC is well-preserved compared to that of conventional one. Additionally, it provides optimal dynamic performance and is robust in terms of parameter variations and peripheral load disturbance.																	0952-1976	1873-6769				MAY	2020	91								103593	10.1016/j.engappai.2020.103593													
J								Minimizing the maximum receiver interference in wireless sensor networks using probabilistic interference model	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Minimizing maximum receiver interference; Probabilistic interference model; Linear power assignment; Wireless sensor networks; Genetic algorithm; Prufer code; Edge-set; Edge-window-decoder	AD-HOC; SPANNING TREE; ALGORITHMS; TOPOLOGY; CAPACITY	Now in the era of the Internet of Energy (IoE), researchers are more focused on optimal energy utilization. In the wireless sensor network, maximization of battery lifetime or network lifetime is one of the primary research objectives. There are many techniques available to enhance the network lifetime, and interference minimization is one of them. Interference minimization leads to less transmission power consumption in wireless sensor networks and thus enhances the network lifetime. The interference minimization is proven to be NP-Hard problem. In this paper, we have proposed a method to minimize receiver interference using different encoding schemes and genetic algorithm. We have used more realistic probabilistic interference model to calculate receiver interference instead of the graph-based model used in most of the literature. The Genetic Algorithm used to minimize receiver interference uses three different chromosome representation schemes, namely Prufer code, Edge-set, and Edge-window-decoder. We have used benchmark data sets and special cases like exponential node chain, two exponential node chain, spiral model, one cluster, and two-cluster for experimental simulations. Our proposed algorithm outperforms other algorithms available in the literature like MI-S (Minimizing Interference in Sensor networks), MinMax-RIP (Minimizing Maximum Receiver Interference Problem), and MST (Minimum Spanning Tree: Prim's algorithm) in terms of minimizing maximum receiver interference in the network.																	0952-1976	1873-6769				MAY	2020	91								103563	10.1016/j.engappai.2020.103563													
J								A new Exponentially Expanded Robust Random Vector Functional Link Network based MPPT model for Local Energy Management of PV-Battery Energy Storage Integrated Microgrid	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Distributed adaptive droop; Exponentially Expanded Robust Random; Vector functional Link Network; Local Energy Management; Maximum power point tracking; Photovoltaic systems	IMPLEMENTATION; SYSTEM	In this paper a new Maximum Power Point Tracking (MPPT) model is presented for Local Energy Management (LEM) of a multiple Photovoltaic (PV) based microgrid. To detect accurate MPP references under local uncertainties, a non-iterative Linear Recurrence Relationship (LRR) based PV model is incorporated with PV penetration index. A robust, accurate and fast Exponentially Expanded Robust Random Vector Functional Link network (EE-RRVFLN) based MPPT algorithm is constructed with an exponentially expansion unit to address positive dynamic volatility and a direct link relationship to address null vs. positive volatility in PV data. The robustness is further incorporated by a maximum likelihood estimator using Huber's cost function, where both input and output weights are optimally estimated by targeting reduction in MPP tracking error. An Assessment Index (i.e. MPPT error related) based Distributed Adaptive Droop (DAD) mechanism is suggested as Primary Controller (PC) for effective power sharing among multiple PVs. A detailed case study is presented to evaluate the accuracy of the proposed model in MATLAB simulation, as well as in dSPACE 1104 based Hardware-inLoop (HIL) platform. Historical data for different intervals/seasons, partial shading, improved LEM validations (simulation and HIL) are considered as different cases to establish the excellence of the proposed approach, as compared with conventional Functional Link Neural Network (FLNN) and Random Vector Functional Link Neural Network (RVFLNN).																	0952-1976	1873-6769				MAY	2020	91								103633	10.1016/j.engappai.2020.103633													
J								Deep reinforcement one-shot learning for artificially intelligent classification in expert aided systems	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Deep reinforcement learning; One-shot learning; Network optimization; Online classification	ANOMALY DETECTION	In recent years there has been a sharp rise in applications, in which significant events need to be classified but only a few training instances are available. These are known as cases of one-shot learning. To handle this challenging task, organizations often use human analysts to classify events under high uncertainty. Existing algorithms use a threshold-based mechanism to decide whether to classify an object automatically or send it to an analyst for deeper inspection. However, this approach leads to a significant waste of resources since it does not take the practical temporal constraints of system resources into account. By contrast, the focus in this paper is on rigorously optimizing the resource consumption in the system which applies to broad application domains, and is of a significant interest for academic research, industrial developments, as well as society and citizens benefit. The contribution of this paper is threefold. First, a novel Deep Reinforcement One-shot Learning (DeROL) framework is developed to address this challenge. The basic idea of the DeROL algorithm is to train a deep-Q network to obtain a policy which is oblivious to the unseen classes in the testing data. Then, in real-time, DeROL maps the current state of the one-shot learning process to operational actions based on the trained deep-Q network, to maximize the objective function. Second, the first open-source software for practical artificially intelligent one-shot classification systems with limited resources is developed for the benefit of researchers and developers in related fields. Third, an extensive experimental study is presented using the OMNIGLOT dataset for computer vision tasks, the UNSW-NB15 dataset for intrusion detection tasks, and the Cleveland Heart Disease Dataset for medical monitoring tasks that demonstrates the versatility and efficiency of the DeROL framework.																	0952-1976	1873-6769				MAY	2020	91								103589	10.1016/j.engappai.2020.103589													
J								Incorporating domain knowledge into reinforcement learning to expedite welding sequence optimization	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Welding sequence optimization; FEA based welding simulation; Reinforcement learning; Structural deformation; Residual stress; Artificial intelligence; Machine learning	DISTORTION; FABRICATION; PARAMETERS; ROBOT	Welding Sequence Optimization (WSO) is very effective to minimize the structural deformation, however selecting proper welding sequence leads to a combinatorial optimization problem. State-of-the-art algorithms could take more than one week to compute the best sequence for an assembly of eight weld beads which is unrealistic for the early stages of Product Delivery Process (PDP). In this article, we develop and implement a novel Reinforcement Q-learning algorithm for WSO where structural deformation is used to compute reward function. We utilize a thermo-mechanical Finite Element Analysis (FEA) to predict deformation. The exploration-exploitation dilemma has been tackled by domain knowledge driven epsilon-greedy algorithm into Q-RL which helps to expedite the WSO and we call this novel algorithm as DKQRL. We run welding simulation experiment using well-known Simufact (R) software on a typical widely used mounting bracket which contains eight welding beads. DKQRL allows the reduction of structural deformation up to similar to 71% and it substantially speeds up the computational time over Modified Lowest Cost Search (MLCS), Genetic Algorithm (GA), exhaustive search, and standard RL algorithm. Results of welding simulation demonstrate a reasonable agreement with real experiment in terms of structural deformation.																	0952-1976	1873-6769				MAY	2020	91								103612	10.1016/j.engappai.2020.103612													
J								Coalition formation with dynamically changing externalities	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Coalition formation; Coalitions externalities; Dynamically changing externalities; Task dependencies; Multi-agent negotiation	STRUCTURE GENERATION; COOPERATIVE GAMES; PLAN COORDINATION; ALGORITHM; AGENTS; NEGOTIATION; STABILITY; EFFICIENT	We consider multiple self-interested bounded-rational agents each of which has a goal it needs to achieve. Goals are achievable by executing a set of interdependent tasks. Some tasks exhibit time dependencies and may require sequential execution. For each agent, there may be several alternative sets of tasks that can achieve the goal. Execution of alternatives, may be more beneficial when done by a group of agents and not by a single agent. To jointly achieve goals, agents may form interdependent coalitions. Such coalition formation is computationally intractable. We nevertheless seek a practical solution that is not necessarily optimal yet acceptable by the agents. A solution where agents examine only coalitions in which they are members is inapplicable, as externalities are a major factor given task interdependencies. In this paper we study this coalition formation problem. We describe the problem and introduce a novel Multi-lateral Negotiation Protocol (MNP) that solves it by forming interdependent coalitions. We allow agents to heuristically make gradual concessions, revise their proposals and converge on specific alternatives, and nevertheless increase their expected gains.																	0952-1976	1873-6769				MAY	2020	91								103577	10.1016/j.engappai.2020.103577													
J								Directional adversarial training for cost sensitive deep learning classification applications	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Adversarial training; Artificial intelligence; Cost-sensitive; Deep learning; Image classification; Optimal transport; Wasserstein		In many real-world applications of Machine Learning it is of paramount importance not only to provide accurate predictions, but also to ensure certain levels of robustness. Adversarial Training is a training procedure aiming at providing models that are robust to worst-case perturbations around predefined points. Unfortunately, one of the main issues in adversarial training is that robustness w.r.t. gradient-based attackers is always achieved at the cost of prediction accuracy. In this paper, a new algorithm, called Wasserstein Projected Gradient Descent (WPGD), for adversarial training is proposed. WPGD provides a simple way to obtain cost-sensitive robustness, resulting in a finer control of the robustness-accuracy trade-off. Moreover, WPGD solves an optimal transport problem on the output space of the network and it can efficiently discover directions where robustness is required, allowing to control the directional trade-off between accuracy and robustness. The proposed WPGD is validated in this work on image recognition tasks with different benchmark datasets and architectures. Moreover, real world-like datasets are often imbalanced: this paper shows that when dealing with such type of datasets, the performance of adversarial training are mainly affected in term of standard accuracy.																	0952-1976	1873-6769				MAY	2020	91								103550	10.1016/j.engappai.2020.103550													
J								Short-term wind speed prediction based on LMD and improved FA optimized combined kernel function LSSVM	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Wind speed prediction; Local mean decomposition; Kernel function; LSSVM; Firefly algorithm	EMPIRICAL MODE DECOMPOSITION; SINGULAR SPECTRUM ANALYSIS; LOCAL MEAN DECOMPOSITION; NEURAL-NETWORK; HYBRID; MULTISTEP; REGRESSION; ALGORITHM; STRATEGY; MACHINE	Accurate prediction of wind speed is of great significance to the operation and maintenance of wind farms, the optimal scheduling of turbines, and the safe and stable operation of power grids. A new prediction method for short-term wind speed based on local mean decomposition (LMD) and combined kernel function least squares support vector machine (LSSVM) is proposed. The short-term wind speed time series is decomposed into some components by the LMD algorithm. Based on LSSVM, radial basis function and the Polynomial function are used to generate the combined kernel function. The combined kernel function LSSVM combines the advantages of the radial basis function and the Polynomial function, which can achieve better prediction accuracy. The decomposed wind speed time series are predicted separately by the combined kernel function LSSVM model. At the same time, an improved firefly algorithm is proposed to optimize the parameters of the combined kernel function LSSVM. The final predictive value can be obtained by superimposing the predicted value of each combined kernel function LSSVM prediction model. The actual collected short-term wind speed data is chosen as the research object, the simulation experiments with four prediction horizons have been implemented. Compared with state-of-the-art prediction methods, through the comparison result curve between the prediction and actual wind speed, the box-plot results of predictive error distribution, the comparison results of the relative prediction error, the performance indicators, the Pearson's test, the DM test and the Taylor diagram results show that the proposed prediction method has higher prediction accuracy and is able to reflect the laws of wind speed correctly. Furthermore, the simulation results of four new datasets and adding noise to the input data of training set show that the proposed prediction method has strong robustness.																	0952-1976	1873-6769				MAY	2020	91								103573	10.1016/j.engappai.2020.103573													
J								Artificial intelligence techniques empowered edge-cloud architecture for brain CT image analysis	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Stroke classification; Feature extractor; Computed tomography; Edge computing	TEXTURE CLASSIFICATION; PATTERN RECOGNITION; MOBILE EDGE; STROKE; FEATURES; DISEASE	Strokes are one of the leading causes of death in the world. Despite the high mortality rate, chances of recovery are high when an accurate diagnosis is made quickly, and appropriate treatment is provided. Several types of neuroimaging techniques are used to detect strokes, and computed tomography (CT) and magnetic resonance imaging are the main ones. Although magnetic resonance imaging shows clearer results, CT is, in most cases, the most viable alternative, due to the reduced examination time and low cost. Several computeraided diagnostic systems have been developed in recent years with a focus on the Internet of Things (IoT). These systems, which establish rapid communication between the IoT devices, provide greater integration between specialists and patients, and consequently, a better medical follow up. However, stroke detection and classification techniques in IoT devices require that these methods developed methods have low computational and low storage costs. Thus, Edge computing devices have been attracting attention for their excellent processing capabilities, providing a layer between the IoT device and the cloud. This work proposes a new feature extractor for brain CT images based on an Adaptive Analysis of Brain Tissue Densities. The proposed method presented promising results of accuracy and F1-score, reaching 98.13% and 97.83%, respectively, surpassing several state-of-the-art methods. Furthermore, the proposed method presents low computational cost, with an average extraction time of 0.087s per image, and is, thus, a viable option for integration in IoT and Edge Computing devices, by providing rapid detection and classification of strokes.																	0952-1976	1873-6769				MAY	2020	91								103585	10.1016/j.engappai.2020.103585													
J								Robust support vector data description for novelty detection with contaminated data	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Outlier detection; Support vector data description; Stahel-Donoho outlyingness; Robustness	IDENTIFICATION; OUTLIERS	Support vector data description (SVDD) is a widely used novelty detection algorithm. It provides excellent predictions even in the absence of negative samples and retains the mathematical elegance of Support Vector Machines. The decision boundary can be very flexible due to the incorporation of kernel functions. However, SVDD can suffer a lot from contaminated data containing, for example, outliers or mislabeled observations. Although several weighting schemes have been proposed to find a more reliable description of the target data, the calculation of the weight are themselves affected by the outliers and does not provide much insight in the data. The masked outliers fail to receive lower weight values. The Stahel-Donoho (SD) outlyingness from multivariate statistics is a very robust measure to expose the outliers. To avoid the masking effect, we propose to assign weight to each observation based on the SD outlyingness in an arbitrary kernel space. A robust SVDD is defined down-weighting the samples with large outlyingness. The experimental results demonstrate superiority of the proposed method in terms of AUC for contaminated data.																	0952-1976	1873-6769				MAY	2020	91								103554	10.1016/j.engappai.2020.103554													
J								A CLSTM-TMN for marketing intention detection	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Text classification; Marketing intention; Topic memory; News	INTERACTIVE EVOLUTIONARY COMPUTATION; LEARNING-METHOD; FUZZY-LOGIC; TEXT	In recent years, neural network-based models such as machine learning and deep learning have achieved excellent results in text classification. On the research of marketing intention detection, classification measures are adopted to identify news with marketing intent. However, most of current news appears in the form of dialogs. There are some challenges to find potential relevance between news sentences to determine the latent semantics. In order to address this issue, this paper has proposed a CLSTM-based topic memory network (called CLSTM-TMN for short) for marketing intention detection. A ReLU-Neuro Topic Model (RNTM) is proposed. A hidden layer is constructed to efficiently capture the subject document representation, Potential variables are applied to enhance the granularity of subject model learning. We have changed the structure of current Neural Topic Model (NTM) to add CLSTM classifier. This method is a new combination ensemble both long and short term memory (LSTM) and convolution neural network (CNN). The CLSTM structure has the ability to find relationships from a sequence of text input, and the ability to extract local and dense features through convolution operations. The effectiveness of the method for marketing intention detection is illustrated in the experiments. Our detection model has a more significant improvement in F1 (7%) than other compared models.																	0952-1976	1873-6769				MAY	2020	91								103595	10.1016/j.engappai.2020.103595													
J								Long short-term memory neural network with weight amplification and its application into gear remaining useful life prediction	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Residual life prediction; Recurrent neural network; Feature fusion; Attention mechanism; Gear vibration signal	FAULT-DIAGNOSIS; LSTM; FEATURES	As an important component of industrial equipment, once gears have failures, they may cause serious catastrophes. Thus, the prediction of gear remaining life is of great significance. The health indicator of gears is first generated by fusing time-domain and frequency-domain features of gears vibration signals via the isometric mapping algorithm. Then a new type of long-short-term memory neural network with weight amplification (LSTMP-A) is proposed for accurately predicting gear remaining life. Compared with traditional LSTMs, LSTMP-A amplifies the input weights and the recurrent weights of the hidden layer to different degrees by the attention mechanism according to the contribution degree of the corresponding data, and a projection layer is added into the network. With LSTMP-A, we can predict the health characteristics of gears based on historical fusion features. With the monitoring data of a gear life cycle test, the comparative experiments show that the proposed gear remaining life prediction method has higher prediction accuracy than the conventional prediction methods.																	0952-1976	1873-6769				MAY	2020	91								103587	10.1016/j.engappai.2020.103587													
J								Demand coverage diversity based ant colony optimization for dynamic vehicle routing problems	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Dynamic vehicle routing; Demand coverage diversity; Ant colony optimization	ALGORITHM; SEARCH; MANAGEMENT; SYSTEM	Dynamic vehicle routing problem (DVRP) has attracted increasing attention due to its wide applications in logistics. Compared with the static vehicle routing problem, DVRP is characterized by the prior unknown customer requests dynamically appearing in route execution. Nevertheless, the newly appeared customers pose a great challenge to route optimizer, since the optimized route may be contrarily of bad quality when including the new customers that are far from planned routes in route planning. To address this issue, in this paper we propose a demand coverage diversity based metaheuristic, termed ACO-CD, in the framework of ant colony algorithm. In ACO-CD, a demand coverage diversity adaptation method is suggested to maintain the diversity of covered customers in routes so that the optimizer can effectively response to the newly appeared customer requests. Experimental results on 27 DVRP test instances demonstrate the effectiveness of the proposed demand coverage diversity adaptation method and the superiority of the proposed ACO-CD over four state-of-the-art DVRP algorithms in terms of solution quality.																	0952-1976	1873-6769				MAY	2020	91								103582	10.1016/j.engappai.2020.103582													
J								Joint DBN and Fuzzy C-Means unsupervised deep clustering for lung cancer patient stratification	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Deep clustering; Deep belief network; Fuzzy C-Means; Patient stratification	MOLECULAR SUBTYPE	Patient stratification has made a great contribution to efficient and personalized medicine. An important task in patient stratification is to discover quite distinct disease subtypes for effective treatment. In this paper, we propose a new deep learning and clustering model which combines Deep Belief Network (DBN) and Fuzzy C-Means(FCM), called Unsupervised Deep Fuzzy C-Means clustering Network(UDFCMN), to cluster lung cancer patients from lung CT images. In our deep clustering network, images after preprocessing are first encoded into multiple layers of hidden variables to extract hierarchical features and feature distribution and form the high-level representations. Here, to solve the problem of feature homogenization in DBN, we introduce the Winner-Take-All (WTA) idea to meliorate the traditional DBN structure, called WTADBN. Then FCM is used to produce the initial cluster labels with the new representations learnt by stacked WTARBM. Therefore, the FCM-generated cluster labels are used for the fine-tuning of the DBN as ground-truth labels. And an unsupervised image clustering and patient stratification process is completed by cross iteration. We tested our deep FCM clustering algorithm to do experiment on both public dataset from the internet and private dataset from cooperate hospital. For the latter one, the clinical and biological verification was also performed. Experimental results reveal outperformance of UDFCMN as compared to the state-of-the-art unsupervised classification methods. These results also indicate that our approach may have practical applications in lung cancer pathogenesis studies and provide useful guidelines for personalized cancer therapy.																	0952-1976	1873-6769				MAY	2020	91								103571	10.1016/j.engappai.2020.103571													
J								Online prediction of noisy time series: Dynamic adaptive sparse kernel recursive least squares from sparse and adaptive tracking perspective	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Online prediction; Dynamic sparse; Kernel recursive least squares; Online vector projection	FEATURE-EXTRACTION; ALGORITHM; SYSTEMS; FILTER	With the deepening of the research on kernel recursive least squares (KRLS), significant researches have been applied to time series online prediction. However, it usually ignores the extraneous and redundant factors in the raw data, which can cause bias in the prediction. In addition, it usually contains both noise and non-stationary characteristics, resulting in deteriorated prediction accuracy and reduced model efficiency. To ease the above two drawbacks of conventional KRLS, this brief presents a dynamic adaptive sparse kernel recursive least squares (DASKRLS) filtering algorithm. It first uses the online vector projection standard and the approximate linear dependence criterion to effectively constrain kernel matrix dimension, and reduce the computational complexity of the model. After that, the regularized maximum correlation entropy criterion to significant process noise-containing data from the perspective of improving generalization ability. Moreover, the adaptive update mechanism can dynamically track the real-time weight of non-stationary signals. The dynamic sparse process is essentially equivalent to a feature selection process that maintains low-dimensional manifold information. Lorenz benchmarking experiments and real data experiments show that DASKRLS achieves better prediction performance in complex systems with noise and nonstationary.																	0952-1976	1873-6769				MAY	2020	91								103547	10.1016/j.engappai.2020.103547													
J								Stabilization of stochastic time-varying coupled systems with delays and Levy noise on networks based on aperiodically intermittent control	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Levy noise; Time-varying coupling structure; Aperiodically intermittent control; Stabilization	NEURAL-NETWORKS; DIFFERENTIAL-EQUATIONS; STABILITY ANALYSIS; STATE-FEEDBACK; COMPUTATIONAL INTELLIGENCE; EXPONENTIAL STABILITY; SYNCHRONIZATION; MODEL; SIMULATION; LEAKAGE	The aim of this paper is to research the stabilization of stochastic time-varying coupled systems with delays and Levy noise on networks (STSDLN) via aperiodically intermittent control. It is worth pointing out that the timevarying coupling is considered into Levy noise systems in the first time. Then, by means of a graph-theoretic approach, Lyapunov method and some techniques of inequalities, some stabilization criteria are obtained to guarantee exponentially stability in mean square for STSDLN. Therein, we weaken the sufficient condition for dealing with the time-varying coupling compared to the existing literature, which can reduce the conservation of the conclusions. Additionally, the intensity of control is closely related to the perturbed intensity of noise and the time-varying coupling strength. In particular, as a practical application of our theoretical results, the stabilization of stochastic time-varying coupled oscillators with delays and Levy noise on networks is studied. Finally, a numerical example is provided to illustrate the validity of the results obtained.																	0952-1976	1873-6769				MAY	2020	91								103576	10.1016/j.engappai.2020.103576													
J								Support vector machine classifier with huberized pinball loss	ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE										Support vector machine; Huberized pinball loss; Proximal gradient; Wilcoxon signed rank test; Friedman test; ROC curve; AUC	REGRESSION; SELECTION; INPUT	The original support vector machine (SVM) uses the hinge loss function, which is non-differentiable and makes the problem difficult to solve in particular for regularized SVM, such as with..1-regularized. On the other hand, the hinge loss is sensitive to noise. To circumvent these drawbacks, a huberized pinball loss function is proposed. It is less sensitive to noise, similar to the pinball loss which is related to the quantile distance. The proposed loss function is differentiable everywhere and this differentiability can significantly reduce the computational cost for the SVM algorithm. The elastic net penalty is applied to the SVM and the support vector machine classifier with huberized pinball loss (HPSVM) is proposed. Due to the continuous differentiability of the huberized pinball loss function, the Proximal Gradient method is used to solve the proposed model. The numerical experiments on synthetic data, real world datasets confirm the robustness and effectiveness of the proposed method. Statistical comparison is performed to show the significant difference between the proposed method and other compered ones.																	0952-1976	1873-6769				MAY	2020	91								103635	10.1016/j.engappai.2020.103635													
J								Refractive Two-View Reconstruction for Underwater 3D Vision	INTERNATIONAL JOURNAL OF COMPUTER VISION										Underwater imaging; Two-view Refractive Structure-from-Motion; Flat refractive geometry	CALIBRATION; GEOMETRY; CAMERAS; MOTION; MODEL	Recovering 3D geometry from cameras in underwater applications involves the Refractive Structure-from-Motion problem where the non-linear distortion of light induced by a change of medium density invalidates the single viewpoint assumption. The pinhole-plus-distortion camera projection model suffers from a systematic geometric bias since refractive distortion depends on object distance. This leads to inaccurate camera pose and 3D shape estimation. To account for refraction, it is possible to use the axial camera model or to explicitly consider one or multiple parallel refractive interfaces whose orientations and positions with respect to the camera can be calibrated. Although it has been demonstrated that the refractive camera model is well-suited for underwater imaging, Refractive Structure-from-Motion remains particularly difficult to use in practice when considering the seldom studied case of a camera with a flat refractive interface. Our method applies to the case of underwater imaging systems whose entrance lens is in direct contact with the external medium. By adopting the refractive camera model, we provide a succinct derivation and expression for the refractive fundamental matrix and use this as the basis for a novel two-view reconstruction method for underwater imaging. For validation we use synthetic data to show the numerical properties of our method and we provide results on real data to demonstrate its practical application within laboratory settings and for medical applications in fluid-immersed endoscopy. We demonstrate our approach outperforms classic two-view Structure-from-Motion method relying on the pinhole-plus-distortion camera model.																	0920-5691	1573-1405				MAY	2020	128	5					1101	1117		10.1007/s11263-019-01218-9													
J								Learning 3D Shape Completion Under Weak Supervision	INTERNATIONAL JOURNAL OF COMPUTER VISION										3D shape completion; 3D reconstruction; Weakly-supervised learning; Amortized inference; Benchmark		We address the problem of 3D shape completion from sparse and noisy point clouds, a fundamental problem in computer vision and robotics. Recent approaches are either data-driven or learning-based: Data-driven approaches rely on a shape model whose parameters are optimized to fit the observations; Learning-based approaches, in contrast, avoid the expensive optimization step by learning to directly predict complete shapes from incomplete observations in a fully-supervised setting. However, full supervision is often not available in practice. In this work, we propose a weakly-supervised learning-based approach to 3D shape completion which neither requires slow optimization nor direct supervision. While we also learn a shape prior on synthetic data, we amortize, i.e., learn, maximum likelihood fitting using deep neural networks resulting in efficient shape completion without sacrificing accuracy. On synthetic benchmarks based on ShapeNet (Chang et al. Shapenet: an information-rich 3d model repository, 2015. ) and ModelNet (Wu et al., in: Proceedings of IEEE conference on computer vision and pattern recognition (CVPR), 2015) as well as on real robotics data from KITTI (Geiger et al., in: Proceedings of IEEE conference on computer vision and pattern recognition (CVPR), 2012) and Kinect (Yang et al., 3d object dense reconstruction from a single depth view, 2018. ), we demonstrate that the proposed amortized maximum likelihood approach is able to compete with the fully supervised baseline of Dai et al. (in: Proceedings of IEEE conference on computer vision and pattern recognition (CVPR), 2017) and outperforms the data-driven approach of Engelmann et al. (in: Proceedings of the German conference on pattern recognition (GCPR), 2016), while requiring less supervision and being significantly faster.																	0920-5691	1573-1405				MAY	2020	128	5					1162	1181		10.1007/s11263-018-1126-y													
J								Curriculum Model Adaptation with Synthetic and Real Data for Semantic Foggy Scene Understanding	INTERNATIONAL JOURNAL OF COMPUTER VISION										Semantic foggy scene understanding; Fog simulation; Learning with synthetic and real data; Curriculum model adaptation; Network distillation; Adverse weather conditions	CONTRAST RESTORATION; VISION	This work addresses the problem of semantic scene understanding under fog. Although marked progress has been made in semantic scene understanding, it is mainly concentrated on clear-weather scenes. Extending semantic segmentation methods to adverse weather conditions such as fog is crucial for outdoor applications. In this paper, we propose a novel method, named Curriculum Model Adaptation (CMAda), which gradually adapts a semantic segmentation model from light synthetic fog to dense real fog in multiple steps, using both labeled synthetic foggy data and unlabeled real foggy data. The method is based on the fact that the results of semantic segmentation in moderately adverse conditions (light fog) can be bootstrapped to solve the same problem in highly adverse conditions (dense fog). CMAda is extensible to other adverse conditions and provides a new paradigm for learning with synthetic data and unlabeled real data. In addition, we present four other main stand-alone contributions: (1) a novel method to add synthetic fog to real, clear-weather scenes using semantic input; (2) a new fog density estimator; (3) a novel fog densification method for real foggy scenes without known depth; and (4) the Foggy Zurich dataset comprising 3808 real foggy images, with pixel-level semantic annotations for 40 images with dense fog. Our experiments show that (1) our fog simulation and fog density estimator outperform their state-of-the-art counterparts with respect to the task of semantic foggy scene understanding (SFSU); (2) CMAda improves the performance of state-of-the-art models for SFSU significantly, benefiting both from our synthetic and real foggy data. The foggy datasets and code are publicly available.																	0920-5691	1573-1405				MAY	2020	128	5					1182	1204		10.1007/s11263-019-01182-4													
J								Image-Based Geo-Localization Using Satellite Imagery	INTERNATIONAL JOURNAL OF COMPUTER VISION										Geo-localization; Markov localization; Cross-view localization; Convolutional Neural Network; NetVLAD		The problem of localization on a geo-referenced satellite map given a query ground view image is useful yet remains challenging due to the drastic change in viewpoint. To this end, in this paper we work on the extension of our earlier work on the Cross-View Matching Network (CVM-Net) (Hu et al. in IEEE conference on computer vision and pattern recognition (CVPR), 2018) for the ground-to-aerial image matching task since the traditional image descriptors fail due to the drastic viewpoint change. In particular, we show more extensive experimental results and analyses of the network architecture on our CVM-Net. Furthermore, we propose a Markov localization framework that enforces the temporal consistency between image frames to enhance the geo-localization results in the case where a video stream of ground view images is available. Experimental results show that our proposed Markov localization framework can continuously localize the vehicle within a small error on our Singapore dataset.																	0920-5691	1573-1405				MAY	2020	128	5					1205	1219		10.1007/s11263-019-01186-0													
J								Semi-supervised Semantic Mapping Through Label Propagation with Semantic Texture Meshes	INTERNATIONAL JOURNAL OF COMPUTER VISION										Semantic mapping; Label propagation; Semantic textured mesh	3D RECONSTRUCTION; SEGMENTATION; OCTOMAP	Scene understanding is an important capability for robots acting in unstructured environments. While most SLAM approaches provide a geometrical representation of the scene, a semantic map is necessary for more complex interactions with the surroundings. Current methods treat the semantic map as part of the geometry which limits scalability and accuracy. We propose to represent the semantic map as a geometrical mesh and a semantic texture coupled at independent resolution. The key idea is that in many environments the geometry can be greatly simplified without loosing fidelity, while semantic information can be stored at a higher resolution, independent of the mesh. We construct a mesh from depth sensors to represent the scene geometry and fuse information into the semantic texture from segmentations of individual RGB views of the scene. Making the semantics persistent in a global mesh enables us to enforce temporal and spatial consistency of the individual view predictions. For this, we propose an efficient method of establishing consensus between individual segmentations by iteratively retraining semantic segmentation with the information stored within the map and using the retrained segmentation to re-fuse the semantics. We demonstrate the accuracy and scalability of our approach by reconstructing semantic maps of scenes from NYUv2 and a scene spanning large buildings.																	0920-5691	1573-1405				MAY	2020	128	5					1220	1238		10.1007/s11263-019-01187-z													
J								Self-Supervised Model Adaptation for Multimodal Semantic Segmentation	INTERNATIONAL JOURNAL OF COMPUTER VISION										Semantic segmentation; Multimodal fusion; Scene understanding; Model adaptation; Deep learning	NETWORKS; RGB	Learning to reliably perceive and understand the scene is an integral enabler for robots to operate in the real-world. This problem is inherently challenging due to the multitude of object types as well as appearance changes caused by varying illumination and weather conditions. Leveraging complementary modalities can enable learning of semantically richer representations that are resilient to such perturbations. Despite the tremendous progress in recent years, most multimodal convolutional neural network approaches directly concatenate feature maps from individual modality streams rendering the model incapable of focusing only on the relevant complementary information for fusion. To address this limitation, we propose a mutimodal semantic segmentation framework that dynamically adapts the fusion of modality-specific features while being sensitive to the object category, spatial location and scene context in a self-supervised manner. Specifically, we propose an architecture consisting of two modality-specific encoder streams that fuse intermediate encoder representations into a single decoder using our proposed self-supervised model adaptation fusion mechanism which optimally combines complementary features. As intermediate representations are not aligned across modalities, we introduce an attention scheme for better correlation. In addition, we propose a computationally efficient unimodal segmentation architecture termed AdapNet++ that incorporates a new encoder with multiscale residual units and an efficient atrous spatial pyramid pooling that has a larger effective receptive field with more than 10x\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$10\,\times $$\end{document} fewer parameters, complemented with a strong decoder with a multi-resolution supervision scheme that recovers high-resolution details. Comprehensive empirical evaluations on Cityscapes, Synthia, SUN RGB-D, ScanNet and Freiburg Forest benchmarks demonstrate that both our unimodal and multimodal architectures achieve state-of-the-art performance while simultaneously being efficient in terms of parameters and inference time as well as demonstrating substantial robustness in adverse perceptual conditions.																	0920-5691	1573-1405				MAY	2020	128	5					1239	1285		10.1007/s11263-019-01188-y													
J								SeDAR: Reading Floorplans Like a Human-Using Deep Learning to Enable Human-Inspired Localisation	INTERNATIONAL JOURNAL OF COMPUTER VISION										Robotics; Localisation; Deep Learning; Semantic; MCL; Monte-Carlo; Turtlebot; ROS; Human-level; Segmentation; Indoor	MONTE-CARLO LOCALIZATION	The use of human-level semantic information to aid robotic tasks has recently become an important area for both Computer Vision and Robotics. This has been enabled by advances in Deep Learning that allow consistent and robust semantic understanding. Leveraging this semantic vision of the world has allowed human-level understanding to naturally emerge from many different approaches. Particularly, the use of semantic information to aid in localisation and reconstruction has been at the forefront of both fields. Like robots, humans also require the ability to localise within a structure. To aid this, humans have designed high-level semantic maps of our structures called floorplans. We are extremely good at localising in them, even with limited access to the depth information used by robots. This is because we focus on the distribution of semantic elements, rather than geometric ones. Evidence of this is that humans are normally able to localise in a floorplan that has not been scaled properly. In order to grant this ability to robots, it is necessary to use localisation approaches that leverage the same semantic information humans use. In this paper, we present a novel method for semantically enabled global localisation. Our approach relies on the semantic labels present in the floorplan. Deep Learning is leveraged to extract semantic labels from RGB images, which are compared to the floorplan for localisation. While our approach is able to use range measurements if available, we demonstrate that they are unnecessary as we can achieve results comparable to state-of-the-art without them.																	0920-5691	1573-1405				MAY	2020	128	5					1286	1310		10.1007/s11263-019-01239-4													
J								Cognitive Mapping and Planning for Visual Navigation	INTERNATIONAL JOURNAL OF COMPUTER VISION										Visual navigation; Spatial representations; Learning for navigation	PERCEPTION; WORLD	We introduce a neural architecture for navigation in novel environments. Our proposed architecture learns to map from first-person views and plans a sequence of actions towards goals in the environment. The Cognitive Mapper and Planner (CMP) is based on two key ideas: (a) a unified joint architecture for mapping and planning, such that the mapping is driven by the needs of the task, and (b) a spatial memory with the ability to plan given an incomplete set of observations about the world. CMP constructs a top-down belief map of the world and applies a differentiable neural net planner to produce the next action at each time step. The accumulated belief of the world enables the agent to track visited regions of the environment. We train and test CMP on navigation problems in simulation environments derived from scans of real world buildings. Our experiments demonstrate that CMP outperforms alternate learning-based architectures, as well as, classical mapping and path planning approaches in many cases. Furthermore, it naturally extends to semantically specified goals, such as "going to a chair". We also deploy CMP on physical robots in indoor environments, where it achieves reasonable performance, even though it is trained entirely in simulation.																	0920-5691	1573-1405				MAY	2020	128	5					1311	1330		10.1007/s11263-019-01236-7													
J								Adversarial Framework for Unsupervised Learning of Motion Dynamics in Videos	INTERNATIONAL JOURNAL OF COMPUTER VISION										Generative adversarial networks; Video generation; Unsupervised learning; Video object segmentation		Human behavior understanding in videos is a complex, still unsolved problem and requires to accurately model motion at both the local (pixel-wise dense prediction) and global (aggregation of motion cues) levels. Current approaches based on supervised learning require large amounts of annotated data, whose scarce availability is one of the main limiting factors to the development of general solutions. Unsupervised learning can instead leverage the vast amount of videos available on the web and it is a promising solution for overcoming the existing limitations. In this paper, we propose an adversarial GAN-based framework that learns video representations and dynamics through a self-supervision mechanism in order to perform dense and global prediction in videos. Our approach synthesizes videos by (1) factorizing the process into the generation of static visual content and motion, (2) learning a suitable representation of a motion latent space in order to enforce spatio-temporal coherency of object trajectories, and (3) incorporating motion estimation and pixel-wise dense prediction into the training procedure. Self-supervision is enforced by using motion masks produced by the generator, as a co-product of its generation process, to supervise the discriminator network in performing dense prediction. Performance evaluation, carried out on standard benchmarks, shows that our approach is able to learn, in an unsupervised way, both local and global video dynamics. The learned representations, then, support the training of video object segmentation methods with sensibly less (about 50%) annotations, giving performance comparable to the state of the art. Furthermore, the proposed method achieves promising performance in generating realistic videos, outperforming state-of-the-art approaches especially on motion-related metrics.																	0920-5691	1573-1405				MAY	2020	128	5					1378	1397		10.1007/s11263-019-01246-5													
J								Weighted quasi-arithmetic mean based score level fusion for multi-biometric systems	IET BIOMETRICS										sensor fusion; support vector machines; biometrics (access control); statistical analysis; NIST-BSSR1 Multimodal; NIST-BSSR1 Fingerprint; multialgorithm systems; score fusion rules; weighted quasiarithmetic mean based score level fusion; multibiometric systems; mobile user authentication; high-security scenarios; score-level fusion; WQAM fusion algorithm; NIST-BSSR1 Face	LOCAL BINARY PATTERNS; FINGERPRINT; FACE; VERIFICATION	Biometrics is now being principally employed in many daily applications ranging from the border crossing to mobile user authentication. In the high-security scenarios, biometrics require stringent accuracy and performance criteria. Towards this aim, multi-biometric systems that fuse the evidences from multiple sources of biometric have exhibited to diminish the error rates and alleviate inherent frailties of the individual biometric systems. In this article, a novel scheme for score-level fusion based on weighted quasi-arithmetic mean (WQAM) has been proposed. Specifically, WQAMs are estimated via different trigonometric functions. The proposed fusion scheme encompasses properties of both weighted mean and quasi-arithmetic mean. Moreover, it does not require any leaning process. Experimental results on three publicly available data sets (i.e. NIST-BSSR1 Multimodal, NIST-BSSR1 Fingerprint and NIST-BSSR1 Face) for multi-modal, multi-unit and multi-algorithm systems show that presented WQAM fusion algorithm outperforms the previously proposed score fusion rules based on transformation (e.g. t-norms), classification (e.g. support vector machines) and density estimation (e.g. likelihood ratio) methods.																	2047-4938	2047-4946				MAY	2020	9	3					91	99		10.1049/iet-bmt.2018.5265													
J								3D face mask presentation attack detection based on intrinsic image analysis	IET BIOMETRICS										neural nets; feature extraction; face recognition; image texture; reflectance image; face mask; intrinsic image analysis; face presentation attacks; face recognition systems; image reflectance; face image; intrinsic image decomposition algorithm	LIVENESS DETECTION	Face presentation attacks have become a major threat against face recognition systems and many countermeasures have been proposed over the past decade. However, most of them are devoted to 2D face presentation attack detection, rather than 3D face masks. Unlike the real face, the 3D face mask is usually made of resin materials and has a smooth surface, resulting in reflectance differences. Therefore, in this study, the authors propose a novel 3D face mask presentation attack detection method based on analysis of image reflectance. In the proposed method, the face image is first processed with intrinsic image decomposition algorithm to compute its reflectance image. Then, the intensity distribution histograms are extracted from three orthogonal planes to represent the intensity differences of reflectance images between the real face and 3D face mask. After that, given that the reflectance image of a smooth surface is more sensitive to illumination changes, 1D convolutional neural network is used to characterise how different materials or surfaces react differently to illumination changes. Extensive experiments with the public available 3DMAD database demonstrate the effectiveness of the proposed method for distinguishing a face mask from the real one and show that the detection performance outperforms other state-of-the-art methods.																	2047-4938	2047-4946				MAY	2020	9	3					100	108		10.1049/iet-bmt.2019.0155													
J								Deep learning for face recognition on mobile devices	IET BIOMETRICS										face recognition; Java; mobile computing; learning (artificial intelligence); interesting choice; automatic face recognition; training stage; mobile devices; small-size deep-learning model; low capacity devices; public datasets; private datasets; mobile scenarios; publicly available models; template matching; face detection stages; mobile phone; great variability; capturing conditions; facial features; deep learning solutions	CLASSIFICATION	Mobility implies a great variability of capturing conditions, which is not easy to control and directly affects to face detection and the extraction of facial features. Deep learning solutions seem to be the most interesting choice for automatic face recognition, but they are highly dependent on the model generated during the training stage. In addition, the size of the models makes it difficult for their integration into applications oriented to mobile devices, particularly when the model must be embedded. In this work, a small-size deep-learning model was trained for face recognition on low capacity devices and evaluated in terms of accuracy, size and timings to provide quantitative data. This evaluation is aimed to cover as many scenarios as possible, so different databases were employed, including public and private datasets specifically oriented to recreate the complexity of mobile scenarios. Also, publicly available models and traditional approaches were included in the evaluation to carry out a fair comparison. Moreover, given the relevance of template matching and face detection stages, the assessment is complemented with different classifiers and detectors. Finally, a JAVA-Android implementation of the system was developed and evaluated to obtain performance data of the whole system integrated on a mobile phone.																	2047-4938	2047-4946				MAY	2020	9	3					109	117		10.1049/iet-bmt.2019.0093													
J								Cost-effective and accurate palm vein recognition system based on multiframe super-resolution algorithms	IET BIOMETRICS										feature extraction; vein recognition; image resolution; medical image processing; cryptography; reliability; image denoising; image restoration; sophisticated acquisition devices; high-resolution imaging; PVR system; low-resolution imaging; multiframe super-resolution algorithms; query imaging; low-resolution imaging devices; palm vein recognition system; contactless biometric identification method; image acquisition; feature extraction; pre-processing component; authentication method; informative palm vein patterns; noise suppression	PERONA-MALIK MODEL; IMAGE SUPERRESOLUTION; RECONSTRUCTION; FUSION	Palm vein recognition (PVR) refers to the contactless biometric identification method that uses palm vein patterns to confirm the identity of a person. Compared with other methods, PVR has received a wide attention because it provides more secure results. The veins, being located inside the human body, make PVR robust against tempering and changes in morphology of body features. Most PVR systems integrate four stages: image acquisition, pre-processing, feature extraction, and decision. The first two stages determine accuracy of the final identification results. Focusing on the pre-processing component, we discovered that the available approaches fail to generate more informative vein patterns by simultaneously suppressing noise and blur, and also by recovering semantically useful features (edges, contours, and lines) from the acquired images. This weakness calls for sophisticated acquisition devices that make PVR systems costly. In this work, we have proposed multiframe super-resolution (MSR) as a pre-processing stage to improve performance of the traditional PVR systems. MSR exploits information from multiple images of the same scene to reconstruct a high-resolution image. This technique signals the possibility of using inexpensive low-resolution imaging devices demanded by the traditional PVR systems. Experiments show that our method outperforms most classical methods.																	2047-4938	2047-4946				MAY	2020	9	3					118	125		10.1049/iet-bmt.2019.0016													
J								Online writer identification system using adaptive sparse representation framework	IET BIOMETRICS										feature extraction; handwritten character recognition; learning (artificial intelligence); image classification; handwriting recognition; support vector machines; image representation; adapted saliency values; sparse codes; online writer identification system; adaptive sparse representation framework; adaptive sparse representation approach; dictionary atom; given writer; derived components; saliency measure; sum-pooled sparse coefficients; enrolled writers; sub-stroke based feature vectors; support vector machines		This study explores an adaptive sparse representation approach for online writer identification. The main focus is on employing prior information that quantifies the degree of importance of a dictionary atom concerning a given writer. This information is proposed by a fusion of two derived components. The first component is a saliency measure obtained from the sum-pooled sparse coefficients corresponding to the sub-strokes of a set of enrolled writers. The second component is a similarity score, computed for each dictionary atom with regards to a given writer, that is related to the reconstruction error of the sub-stroke based feature vectors. The proposed identification is accomplished with an ensemble of support vector machines (SVMs), wherein the input to the SVM trained for a writer is obtained by incorporating the adapted saliency values of that writer on the document descriptor obtained via average pooling of sparse codes. Experiments performed on the IAM and IBM-UB1 online handwriting databases demonstrate the efficacy of the proposed scheme.																	2047-4938	2047-4946				MAY	2020	9	3					126	133		10.1049/iet-bmt.2019.0147													
J								A new re-encoding ECOC using reject option	APPLIED INTELLIGENCE										Error-correcting output codes; Cost-sensitive; Reject option; Receive operating characteristics	CORRECTING OUTPUT CODES; MULTICLASS; ENSEMBLE	When training base classifier by ternary Error Correcting Output Codes (ECOC), it is well know that some classes are ignored. On this account, a non-competent classifier emerges when it classify an instance whose real label does not belong to the meta-subclasses. Meanwhile, the classic ECOC dichotomizers can only produce binary outputs and have no capability of rejection for classification. To overcome the non-competence problem and better model the multi-class problem for reducing the classification cost, we embed reject option to ECOC and present a new variant of ECOC algorithm called as Reject-Option-based Re-encoding ECOC (ROECOC). The cost-sensitive classification model and cost-loss function based on Receiver Operating Characteristic (ROC) curve are built respectively. The optimal reject threshold values are obtained by combing the condition to be met for minimizing the loss function and the ROC convex hull. In so doing, reject option (t(1), t(2)) provides a three-symbol output to make dichotomizers more competent and ROECOC more universal and practical for cost-sensitive classification issue. Experimental results on two kinds of datasets show that our scheme with low-degree freedom of initialized ECOC can effectively enhance accuracy and reduce cost.																	0924-669X	1573-7497				OCT	2020	50	10					3090	3100		10.1007/s10489-020-01642-2		MAY 2020											
J								KASRA: A Kriging-based Adaptive Space Reduction Algorithm for global optimization of computationally expensive black-box constrained problems	APPLIED SOFT COMPUTING										Surrogate-based optimization; Efficient global optimization; Expensive black-box problems; Kriging model; Space reduction approaches	SURROGATE-BASED OPTIMIZATION; INFILL SAMPLING CRITERIA; DESIGN; STRATEGIES; HYBRID	Efficient Global Optimization (EGO) methodology over the entire design space can be considerably time-consuming as much as the expensive simulation computer codes on High-multimodal and computationally Expensive Black-box (HEB) constrained problems. This paper introduces a strategy specifically, the Kriging-based Adaptive Space Reduction Algorithm, named KASRA, to enhance the performance of EGO for HEB constrained optimization problems. A new measure is proposed according to the activity of the decision variables to adaptively reduce the size of design intervals centered at the current best solution. The shrunken intervals gradually are expanded to decrease the risk of missing the desirable region. The design sub-spaces are explored based on the weighed constrained expected improvement criterion. The weighting coefficients of exploration and exploitation dynamically are regulated according to the volume ratio of the current hyper-box-shaped region and the original one. The sequential quadratic programming and exponential tunneling algorithms as two local and global optimizers are employed on Kriging-based functions to achieve a more accurate solution at the end of the procedure if necessary. The genetic algorithm with different tuning strategies is used to defeat the extreme time challenge of constructing Kriging-based surrogates. The proposed algorithm is applicable even if there is no feasible point in the initial samples. The efficiency of KASRA is demonstrated on twenty-two mathematical and ten classical engineering benchmark problems. Experimental results and comparative studies confirm that the proposed approach has a promising performance to deal with HEB constrained optimization problems and generally performs better than the competitor methods on most of the benchmark problems. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106154	10.1016/j.asoc.2020.106154													
J								Learning to recommend third-party library migration opportunities at the API level	APPLIED SOFT COMPUTING										Application programming interface; Library migration; Machine learning; Software quality; Mining software repositories	ALGORITHM	The manual migration between different third-party libraries represents a challenge for software developers. Developers typically need to explore both libraries Application Programming Interfaces, along with reading their documentation, in order to locate the suitable mappings between replacing and replaced methods. In this paper, we introduce RAPIM, a machine learning model that recommends mappings between methods from two different libraries. Our model learns from previous migrations, manually performed in mined software systems, and extracts a set of features related to the similarity between method signatures and method textual documentations. We evaluate our model using 8 popular migrations, collected from 57,447 open-source Java projects. Results show that RAPIM is able to recommend relevant library API mappings with an average accuracy score of 87%. Finally, we provide the community with an API recommendation web service that could be used to support the migration process. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106140	10.1016/j.asoc.2020.106140													
J								Requirement ambiguity and fuzziness in large-scale projects: The problem and potential solutions	APPLIED SOFT COMPUTING										Requirement specification; Fuzzy logic; Public sector projects; Game theory		In large-scale projects, it often occurs that the product the purchaser ends up receiving - possibly from projects extending over many years - differs from what they expected. The provider usually defends its delivered product and may blame the imprecision and ambiguity of the requirements, defined by the purchaser, as the primary reason for misinterpretation of requirements and resulting deficiencies. This letter relies on game theory to explain this problem including both intentional and unintentional misinterpretation of requirements. The letter also highlights the practical and scientific significance of the problem using two real-world cases and suggests potential tools and techniques from soft computing in order to develop decision support systems to address the problem. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106148	10.1016/j.asoc.2020.106148													
J								A multi-objective evolutionary approach for planning and optimal condition restoration of secondary distribution networks	APPLIED SOFT COMPUTING										Distribution network planning; Distribution network reconfiguration; Multi-objective optimization; Nondominated sorting genetic algorithm; Particle swarm optimization	RADIAL-DISTRIBUTION NETWORK; DISTRIBUTION-SYSTEMS; GENETIC ALGORITHM; OPTIMAL PLACEMENT; RECONFIGURATION; OPTIMIZATION; GENERATION; MINIMIZATION; REDUCTION	A secondary distribution network (SDN), corresponding to the final user low voltage distribution circuit, is continuously growing due to a persistent increase in load demand. Consequently, the performance of any optimized design will inevitably degrade over time. To avoid the associated repercussions such as faults, congestion, voltage drops, and other major quality issues, we are eventually prompted to redesign this part of the grid. To do so, we propose a Two-Stage Multi-Objective Evolutionary Approach (TS-MOEAP), which is able to find a new optimal network configuration, circumventing the associated quality issues. The proposed approach is oriented to improve the performance of SDNs by combining the concepts of network reconfiguration (NR) and optimal placement of distribution transformers (DTs). Due to the large and complex topology of SDNs, we deal with a hard combinatorial, non-convex, and nonlinear optimization problem. Consequently, to facilitate the resolution of the problem, the proposal is divided into two stages: (1) optimal placement and sizing of distribution transformers, as well as conductor sizing and branch routing, and (2) optimal network reconfiguration. For the first stage, an improved particle swarm optimization technique (IPSO) combined with a greedy algorithm is used, and for the second stage, an improved nondominated sorting genetic algorithm with a heuristic mutation operator (NSGA-HO) is implemented. The approach redesigns SDNs by minimizing total power loss and investment costs while satisfying quality issues and technical constraints. The proposed approach is validated by improving a real-life SDN with critical quality and technical issues. We also compare the results with respect to other state-of-the-art algorithms. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106182	10.1016/j.asoc.2020.106182													
J								Ensemble of machine learning algorithms for cryptocurrency investment with different data resampling methods	APPLIED SOFT COMPUTING										Financial markets; Cryptocurrencies; Technical analysis; Machine learning; Ensemble classification; Financial data resampling	NEURAL-NETWORKS; MARKETS	This work proposes a system based on machine learning aimed at creating an investment strategy capable of trading on the cryptocurrency exchange markets. Additionally, with the goal of generating investments with higher returns and lower risk, rather than investing on predictions based on time sampled financial series, a novel method for resampling financial series was developed and employed in this work. For this purpose, the originally time sampled financial series are resampled according to a closing value threshold, thus creating a series prone to obtaining higher returns and lower risk than the original series. Out of these resampled series as well as the original, technical indicators are calculated and fed as inputs to four machine learning algorithms: Logistic Regression, Random Forest, Support Vector Classifier, and Gradient Tree Boosting. Each of these algorithms is responsible for generating a transaction signal. Afterwards, a fifth transaction signal is generated by simply calculating the unweighted average of the four trading signals outputted from the previous algorithms, to improve on their results. In the end, the investment results obtained with the resampled series are compared to the commonly utilized fixed time interval sampling. This work demonstrates that independently of using or not a resampling method, all learning algorithms outperform the Buy and Hold (B&H) strategy in the overwhelming majority of the 100 markets tested. Nevertheless, out of the learning algorithms, the unweighted average obtains the best overall results, namely accuracies up to 59.26% for time resampled series. But most importantly, it is concluded that both alternative resampling methods tested are capable of generating far greater returns and with lower risk relatively to time resampled data. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106187	10.1016/j.asoc.2020.106187													
J								A case-based reasoning system for recommendation of data cleaning algorithms in classification and regression tasks	APPLIED SOFT COMPUTING										Case-based reasoning; Classification; Regression	CONCEPTUAL-FRAMEWORK; KNOWLEDGE DISCOVERY; SUPPORT; SIMILARITY; SELECTION; CBR	Recently, advances in Information Technologies (social networks, mobile applications, Internet of Things, etc.) generate a deluge of digital data; but to convert these data into useful information for business decisions is a growing challenge. Exploiting the massive amount of data through knowledge discovery (KD) process includes identifying valid, novel, potentially useful and understandable patterns from a huge volume of data. However, to prepare the data is a non-trivial refinement task that requires technical expertise in methods and algorithms for data cleaning. Consequently, the use of a suitable data analysis technique is a headache for inexpert users. To address these problems, we propose a case-based reasoning system (CBR) to recommend data cleaning algorithms for classification and regression tasks. In our approach, we represent the problem space by the meta-features of the dataset, its attributes, and the target variable. The solution space contains the algorithms of data cleaning used for each dataset. We represent the cases through a Data Cleaning Ontology. The case retrieval mechanism is composed of a filter and similarity phases. In the first phase, we defined two filter approaches based on clustering and quartile analysis. These filters retrieve a reduced number of relevant cases. The second phase computes a ranking of the retrieved cases by filter approaches, and it scores a similarity between a new case and the retrieved cases. The retrieval mechanism proposed was evaluated through a set of judges. The panel of judges scores the similarity between a query case against all cases of the case-base (ground truth). The results of the retrieval mechanism reach an average precision on judges ranking of 94.5% in top 3 (P@3), for top 7 (P@7) 84.55%, while in top 10 (P@10) 78.35%. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106180	10.1016/j.asoc.2020.106180													
J								Investigating classification supervised learning approaches for the identification of critical patients' posts in a healthcare social network	APPLIED SOFT COMPUTING										Machine Learning; Natural language processing; Healthcare; Social network; Classification; Regression	USER SIMILARITY; MEDIA; PROFESSIONALS	Nowadays, Healthcare Social Networks (HSNs) offer the possibility to enhance patient care and education. However, they also present potential risks for patients due to the possible distribution of poor-quality or wrong information along with their bad interpretation. On one hand doctors and practitioners want to promote the exchange of information among patients about a specific disease, but on the other hand they do not have enough time to read patients' posts and moderate them when required. In this paper, we investigate and compare different supervised learning classifiers that we adopted for the classification of critical patients' posts who can trigger the intervention of the medical personnel. In particular, by considering different Bayesian, Linear and Support Vector Machine (SVM) classifiers we analyze their accuracy considering different n-grams datasets preparation approaches in order to identify the best approach for the identification of critical patients' posts in a Healthcare Social Network. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106155	10.1016/j.asoc.2020.106155													
J								NPrSVM: Nonparallel sparse projection support vector machine with efficient algorithm	APPLIED SOFT COMPUTING										Support vector machine; Projection twin support vector machine; Nonparallel projections; Sparseness; Classification	CLASSIFICATION; DIAGNOSIS; SVM	The recently proposed projection twin support vector machine (PTSVM) is an excellent nonparallel classifier. However, PTSVM employs the least-squares loss function to measure its within-class empirical risk, resulting in several drawbacks, such as non-sparseness for decision, sensitivity to outliers, expensive matrix inversion, and inconsistency in the linear and nonlinear models. To alleviate these issues, in this paper, we propose a novel nonparallel sparse projection support vector machine (NPrSVM). Different from the original PTSVM that squeezes the projected values of within-class instances to its own class center, NPrSVM aims to cluster them as much as possible within an insensitive tube. Specifically, our NPrSVM owns the following attractive merits: (i) Benefiting from the L1-norm symmetric Hinge loss function, NPrSVM not only enjoys sparseness for decision but also improves robustness to outliers. (ii) The elegant formulation of dual problems in NPrSVM no longer involves the matrix inversion during the training procedure. This greatly saves the computing time compared to PTSVM. (iii) While the nonlinear formulation of PTSVM is not the direct extension of linear PTSVM, the linear and nonlinear versions of our NPrSVM are consistent. (iv) An efficient dual coordinate descent algorithm is further designed for NPrSVM to handle large-scale classification. Finally, the feasibility and effectiveness of NPrSVM are validated by extensive experiments on both synthetic and real-world datasets. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106142	10.1016/j.asoc.2020.106142													
J								An empirical assessment of autonomicity for autonomic query optimizers using fuzzy-AHP technique	APPLIED SOFT COMPUTING										Fuzzy-Analytic Hierarchy Process; Autonomicity; Weighted matrix; Query optimizer; Autonomic software system		Quality assurance and evaluation has always been a key cause of concern for software developers. This problem has been further aggravated by the complete dependence of business enterprises, financial institutions and stock markets on computer hardware and software. It is therefore needed to propose and develop such software evaluation and quality assurance techniques that can fit into the business model's domain and satisfy the customer needs and aspirations. Autonomic computation is an artificial intelligent based approach used to design and develop software systems which can fit into business model and also satisfy customer needs. These systems are built with self-managed policy system. To guarantee their customers a Total Quality Assurance on the business applications being developed, the paper presents some key aspects of domain-specific software and its quality estimation parameter. In this paper, the authors have analyzed the various aspects of quality metrics of autonomic computation suggested by enhanced ISO 9126 quality model. A universally acceptable approach to assure quality for autonomic computing system would be to measure the Autonomicity of a system to determine whether it is autonomic or not. If it is autonomic then "to what extent'' is the next question? Autonomicity is an excellent indicator to assure quality of the autonomic software. The approach taken to measure the subjective attribute of Autonomicity is fuzzy theory with Analytic Hierarchy Process (AHP) integrated in it. Human assessment is qualitative and fuzzy technique is best candidate to quantify their opinions. For empirical analysis, three different query optimizers are examined to measure autonomicity. The result of the empirical analysis will be validated using the already proposed results of the research studies. The present study will provide a base for further research in terms of development of applications with autonomic features. It will also help in proposing new metrics for quality characteristics to estimate the overall quality of such application. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106137	10.1016/j.asoc.2020.106137													
J								Multi-scale patch based representation feature learning for low-resolution face recognition	APPLIED SOFT COMPUTING										Face recognition; Low-resolution; Feature learning; Multi-scale patch	IMAGE SUPERRESOLUTION; HALLUCINATION; POSE	In practical video surveillance, the quality of facial regions of interest is usually affected by the large distances between the objects and surveillance cameras, which undoubtedly degrade the recognition performance. Existing methods usually consider the holistic representations, while neglecting the complementary information from different patch scales. To tackle this problem, this paper proposes a multi-scale patch based representation feature learning (MSPRFL) scheme for low-resolution face recognition problem. Specifically, the proposed MSPRFL approach first exploits multi-level information to learn more accurate resolution-robust representation features of each patch with the help of a training dataset. Then, we exploit these learned resolution-robust representation features to reduce the resolution discrepancy by integrating the recognition results from all patches. Finally, by considering the complementary discriminative ability from different patch scales, we try to fuse the multi-scale outputs by learning scale weights via an ensemble optimization model. We further verify the efficiency of the proposed MSPRFL on low-resolution face recognition by the comparison experiments on several commonly used face datasets. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106183	10.1016/j.asoc.2020.106183													
J								Dynamic and multi-objective reconfiguration of distribution network using a novel hybrid algorithm with parallel processing capability	APPLIED SOFT COMPUTING										Dynamic reconfiguration; Parallel processing method; Reliability assessment; Adaptive population size; Distribution network	DISTRIBUTION-SYSTEMS; RELIABILITY IMPROVEMENT; EVOLUTIONARY ALGORITHM; GENETIC ALGORITHM; LOSS REDUCTION; OPTIMIZATION; EXCHANGE; SOLVE	The configuration of the networks leads to significant effect on the power quality factors like power loss, voltage profile, reliability, and networks resiliency. Due to intermittent nature of system parameters, the network configuration should be adjusted adaptively and dynamically; this process needs a fast and precise reconfiguration algorithm. The main challenge of existing algorithms is the efficient tradeoff between speed and accuracy of reconfiguration. Most of methods use wide-area searching algorithms. Hence, their response to dynamic deviations is low in the term of computational speed. In this regard, the reconfiguration methods that have been presented for dynamic purposes, have weak optimization structure and low accuracy. In this paper, a novel hybrid algorithm is proposed for dynamic and multi-objective reconfiguration of the distribution networks by using the parallel processing method and adaptive population approach. The combination of the exchange market algorithm (EMA) and wild goats algorithm (WGA) is implemented, in parallel pools, for enhancing the accuracy and speed of the reconfiguration simultaneously. The adaptive updating of the population size of parallel algorithms increases the convergence speed of the hybrid method and also offers a fast responding approach for dynamic reconfiguration of network. The objective functions intended for reconfiguration are active power loss and reliability indexes. The conducted research proposes an applicable architecture called as improved loop matrix for eliminating defects of the conventional loop matrix method which leads to ensuring the radial structure of network. The proposed method is tested on IEEE 15, 33, 69 and 85-bus standard test systems and the results are compared with literature and base mode of network. The analysis of the comparisons illuminates the superiority of the proposed method in terms of convergence speed, accuracy and processing time. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106146	10.1016/j.asoc.2020.106146													
J								A novel vSLAM framework with unsupervised semantic segmentation based on adversarial transfer learning	APPLIED SOFT COMPUTING										Semantic SLAM; Dynamic environments; Dynamic object segmentation	MONOCULAR SLAM	Significant progress has been made in the field of visual Simultaneous Localization and Mapping (vSLAM) systems. However, the localization accuracy of vSLAM can be significantly reduced in dynamic applications with mobile robots or passengers. In this paper, a novel semantic SLAM framework in dynamic environments is proposed to improve the localization accuracy. We incorporate a semantic segmentation model into the Oriented FAST and Rotated BRIEF-SLAM2 (ORB-SLAM2) system to filter out dynamic feature points, but we encounter one main challenge, i.e. the performance of a segmentation network well-trained with labeled datasets may decrease seriously in a real application without any labeled data due to the inconsistency between the source domain and the target domain. Therefore, we proposed an unsupervised semantic segmentation model with a Residual Neural Network (ResNet) structure, which is trained by the adversarial transfer learning method in the multi-level feature spaces. This work may be the first to perform multi-level feature space adversarial transfer learning for the semantic SLAM task in dynamic environments. In order to evaluate our method, images of indoor scenes from three datasets are used as the source domain, and the dynamic sequences of the TUM dataset are used as the target domain. The extensive experimental results show favorable performance against the state-of-the-art methods in terms of the absolute trajectory accuracy and image semantic segmentation quality. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106153	10.1016/j.asoc.2020.106153													
J								Emended salp swarm algorithm for multiobjective electric power dispatch problem	APPLIED SOFT COMPUTING										Exploration; Economic-emission load dispatch; Salp swarm algorithm; Exploitation	IMPROVED DIFFERENTIAL EVOLUTION; GREY WOLF OPTIMIZATION; BEE COLONY ALGORITHM; ECONOMIC-DISPATCH; SEARCH ALGORITHM; HYBRID	This paper presents an emended salp swarm algorithm (ESSA) which is basically the extension of the basic salp swarm algorithm (SSA) to solve multiobjective electric power load dispatch problem. The main inspiration behind ESSA is the swarming behavior and reproduction cycle of salps. Salps, in the chain, move across a multi-dimensional search space to aim for the food source (global solution). Owing to the searching behavior of SSA that makes the algorithm prone to premature convergence, the solitary and colonial reproduction phase of salp has been introduced to improve the convergence behavior along with their swarming behavior The multiobjective optimization problem is firstly converted into scalar objective exploiting fuzzy set theory and the conflicting nature of objectives is resolved by cardinal priority ranking. The variable elimination method with exterior penalty is used to handle the physical and operational constraints of generating units. The validation of the proposed ESSA has been examined on the standard benchmark functions and seven EcLD test systems including both scalar and multiple objectives. The statistical analysis based on Wilcoxon sign rank test, supports that results achieved by the algorithm are superior to the other competing algorithms. So, the proposed algorithm (ESSA) is found a promising algorithm. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106172	10.1016/j.asoc.2020.106172													
J								Implementation of optimized dynamic trajectory modification algorithm to avoid obstacles for secure navigation of UAV	APPLIED SOFT COMPUTING										Collision avoidance; Six Degrees of Freedom (6-DOF); Obstacle detection; Particle Swarm Optimization (PSO); Unmanned Aerial Vehicle (UAV)	UNMANNED AERIAL VEHICLE; DIFFERENTIAL EVOLUTION; PLANNER	To develop escape manoeuvre from obstacles and to find new waypoints for dynamic trajectory modification of UAV, a novel Particle Swarm Optimization based Collision Avoidance algorithm (PSO-CA) is presented in this paper. The proposed "obstacle sense and avoid'' algorithm and the logical decision-making system aids the Unmanned Aerial Vehicle (UAV) to re-route its current path to a safer flight course when an obstacle pops up along its intended path. A radar with 10 km range identifies obstacles and the UAV manoeuvres based on radar data, making it suitable for any unknown environment. The proposed system would manoeuvre the UAV autonomously along optimized alternate path to avoid the conflicting traffic. New waypoints are identified and the waypoint list is modified dynamically to avoid collision with stationary threats like hill, tree and moving intruders like other UAVs. The proposed algorithm steers the vehicle safely along alternate path with less change in intended trajectory while avoiding all potential threats. As the PSO-CA algorithm detects obstacles and identifies alternate path well in advance for unknown pop up threats, the UAV is safe and is suitable for real time environment. The proposed algorithm has considered obstacles with different positions, different sizes and random motion. Experimental results conducted on 6-DOF model UAV with different obstacles clearly justify the efficiency of the proposed method in comparison with other planners. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106168	10.1016/j.asoc.2020.106168													
J								Understanding the problem space in single-objective numerical optimization using exploratory landscape analysis	APPLIED SOFT COMPUTING										Benchmarking; Exploratory landscape analysis; Numerical optimization; Single objective problems	PERFORMANCE; ALGORITHMS	In benchmarking theory, creating a comprehensive and uniformly distributed set of problems is a crucial first step to designing a good benchmark. However, this step is also one of the hardest, as it can be difficult to determine how to evaluate the quality of the chosen problem set. In this article, we evaluate if the field of exploratory landscape analysis can be used to develop a generalized method of visualizing a set of arbitrary optimization functions. We present a method for visually determining the distribution of problems within a benchmark set using exploratory landscape analysis combined with clustering and t-sne visualization, and evaluate and explain the visualization this methodology produces. The proposed method is evaluated on a set of benchmark problems taken from two well known state-of-the-art real-parameter single objective optimization benchmarks: the CEC Special Sessions and Competitions on Real-Parameter Single Objective optimization, and the GECCO Black-Box Optimization Benchmark workshops. The main goal of this paper is to present an analysis of how exploratory landscape analysis can be used to visualize a benchmark problem set. We show that this method can provide a clear visualization of a benchmark problem set and shows the similarities of the problems in it by placing similar problems visually close together. We also show that the problem sets of the above benchmarks have a somewhat distinct set of problems that do not overlap. In addition, by applying feature selection approaches we show that a number of landscape features provided by state-of-the-art exploratory landscape analysis libraries are redundant and that a large amount of them are not invariant to simple transforms like scaling and shifting, at least when analyzing these two datasets. (C) 2020 The Author(s). Published by Elsevier B.V.																	1568-4946	1872-9681				MAY	2020	90								106138	10.1016/j.asoc.2020.106138													
J								A new secondary decomposition-ensemble approach with cuckoo search optimization for air cargo forecasting	APPLIED SOFT COMPUTING										Air cargo forecasting; Secondary decomposition-ensemble learning; Elman neural networks; Cuckoo search algorithm; Phase space reconstruction	EMPIRICAL MODE DECOMPOSITION; PHASE-SPACE RECONSTRUCTION; GREY RELATIONAL ANALYSIS; ALGORITHM; SELECTION; DEMAND; INVESTMENT; DELAYS; MARKET; EMD	The accurate forecast of air cargo demand is essential for infrastructure construction planning and daily operation management. Evidently, it is extremely difficult to capture the dynamics of time series impacted by distinct sources. To reduce the complexity of data, the current popular method is to decompose the original data into several modal branches with different characteristic attributes. But the new problem is that the components generated by decomposition are still irregular and unstable, and there is no unified method to predict them. In this paper, a new secondary decomposition-ensemble (SDE) approach with a cuckoo search algorithm (CSA) is proposed for air cargo forecasting. More specifically, the original air cargo time series is decomposed into several components by an enhanced decomposition formwork, which consists of variational mode decomposition (VMD), sample entropy (SE) and empirical mode decomposition (EMD). Subsequently, the ARIMA and the Elman neural networks (ENN) optimized by CSA are respectively applied to forecast the trend component and the low-frequency components, during which the phase space reconstruction (PSR) is conducted to determine the input structure of neural networks. The final forecasting results are obtained by integrating the predicted values of each component. Besides, the air cargo series from three different airports in China are adopted to validate the performance of our proposed approach and the empirical results show that it is superior to all other benchmark models in terms of the robustness and accuracy. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106161	10.1016/j.asoc.2020.106161													
J								Extracting core answers using the grey wolf optimizer in community question answering	APPLIED SOFT COMPUTING										Community question answering; Grey wolf optimizer; Core answers	SCHEDULING PROBLEM; QUALITY; ALGORITHM; SERVICES	A rapidly increasing number of users in community question answering has led to an explosive growth of answers. Thus, it is becoming increasingly more difficult to browse all the answers. Choosing a subset of answers arbitrarily will likely lead to cognitive bias and even poor decisions. Reading a set of core answers that can cover most topics of all the answers is a novel method to overcome information overload and facilitates information retrieval. In this paper, the method named AnsExt for extracting core answers using a kind of swarm intelligence algorithm, named the grey wolf optimizer (GWO), is proposed. First, answers are modeled with the biterm topic model, which fits both short and long texts. Then, factors including quality, coverage, redundancy and number in extracting core answers are defined. Two scenarios are modeled with the requirements of quality, coverage and redundancy: extracting the least number of answers and extracting a predefined number of answers. To extract the minimum number of core questions, a binary GWO is used to resolve the single-objective optimal model. The binary multi-objective GWO is constructed to resolve the optimal model, which is used to extract a predefined number of core answers. Extensive experiments are conducted on real datasets. The results show that the proposed method is feasible and performs well. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106125	10.1016/j.asoc.2020.106125													
J								Soft sensor modeling of industrial process data using kernel latent variables-based relevance vector machine	APPLIED SOFT COMPUTING										Latent variable modeling; Kernel partial least squares; Relevance vector machine; Indoor air quality; Wastewater treatment processes	FAULT-DIAGNOSIS; QUALITY; PLS; REGRESSION; PREDICTION	A composite model integrating latent variables of kernel partial least squares with relevance vector machine (KPLS-RVM) has been proposed to improve the prediction performance of conventional soft sensors when facing industrial processes. First, the latent variables are extracted to cope with the high dimensionality and complex collinearity of nonlinear process data by using KPLS projection. Then, the probabilistic method RVM is used to develop predictive function between latent variables and the output variable. The performance of the proposed method is evaluated through two case studies based on subway indoor air quality (IAQ) data and wastewater treatment processes (WWTP) data, respectively. The results show the superiority of KPLS-RVM in prediction performance over the other counterparts including least squares support vector machine (LSSVM), PLS-LSSVM, PLS-RVM, and KPLS-LSSVM. For the prediction of effluent chemical oxygen demand in WWTP data, the coefficient of determination value of KPLS-RVM has been improved by approximately 7.30-19.65% in comparison with the other methods. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106149	10.1016/j.asoc.2020.106149													
J								A spatially explicit evolutionary algorithm for the spatial partitioning problem	APPLIED SOFT COMPUTING										Combinatorial optimization; Evolutionary algorithm; Spatial optimization; Heuristics; Parallel computing	GENETIC ALGORITHMS; SCATTER SEARCH; LAND; OPTIMIZATION; CONTIGUITY; ALLOCATION; CONSTRAINTS; MODEL; LOCATION	Spatial optimization seeks optimal allocation or arrangement of spatial units under constraints such as distance, adjacency, contiguity, and pattern. Evolutionary Algorithms (EAs) are well-known optimization heuristics. However, classic EAs, based on a binary problem encoding and bit-operation-based offspring operators, are spatially unaware and do not capture topological and geometric relationships. Unsurprisingly when spatial characteristics are not explicitly considered in the design of EA operators, that EA becomes ineffective because satisfying spatial constraints is computationally expensive. We design and develop novel spatially explicit EA recombination operators, inspired by the path relinking and ejection chain heuristic strategies, that implement crossover and mutation using intelligently guided strategies in a spatially constrained decision space. Our spatial EA approach is general and slots well into the foundational theory of evolutionary algorithms for spatial optimization. We demonstrate improved solution quality and computational performance with a large-scale spatial partitioning application. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106129	10.1016/j.asoc.2020.106129													
J								A new incomplete pattern belief classification method with multiple estimations based on KNN	APPLIED SOFT COMPUTING										Missing data; K-nearest neighbors; Possibility distance; Belief functions theory; Uncertainty	K-NEAREST NEIGHBORS; MISSING DATA; IMPUTATION METHOD; SELECTION; MACHINE; VALUES	The classification of missing data is a challenging task, because the lack of pattern attributes may bring uncertainty to the classification results and most classification methods produce only one estimation, which may have a risk of misclassification. A new incomplete pattern belief classification (PBC) method with multiple estimations based on K-nearest neighbors (KNNs) is proposed to deal with missing data. PBC preliminarily classifies the incomplete pattern using its KNNs obtained by the known attributes. The pattern whose KNNs contain only one class information can be directly divided into this class. If not, the p (p <= c) estimations will be computed according to the different KNNs in different classes when p classes are included in the KNNs of the pattern and it will yield p pieces of classification results by the chosen classifier. Then, a weighted possibility distance method is used to further divide the p classification results with their KNNs' classification information. The pattern with similar possibility distances in different classes will be reasonably classified into a proper meta-class under the framework of belief functions theory, which truly reflects the uncertainty of the pattern caused by missing values and effectively reduces the error rate. Experiments on both artificial and real data sets show that PBC is effective for dealing with missing data. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106175	10.1016/j.asoc.2020.106175													
J								A novel fuzzy clustering algorithm by minimizing global and spatially constrained likelihood-based local entropies for noisy 3D brain MR image segmentation	APPLIED SOFT COMPUTING										3D brain MR image segmentation; Clustering; Entropy; Image segmentation	C-MEANS ALGORITHM; INTENSITY INHOMOGENEITY; INFORMATION	In this paper, we propose a novel fuzzy clustering algorithm by minimizing global and spatially constrained likelihood-based local entropies (FCMGsLE) for segmenting noisy 3D brain magnetic resonance (MR) image volumes. For each voxel, in order to measure uncertainties that arise while identifying its class, two different entropies are defined. In particular, they measure the amount of uncertainties in terms of global entropy using fuzzifier weighted global membership function and spatially constrained likelihood-based local entropy using fuzzifier weighted local membership function. To mitigate the effect of noise and intensity inhomogeneity (IIH) or radio frequency (RF) inhomogeneity, the local membership function is induced by spatially constrained likelihood measure. These entropies are minimized through a fuzzy objective function to obtain the cluster prototypes and membership functions. The final membership function is obtained by integrating these global and local membership functions using weighted parameters. The algorithm is assessed both qualitatively and quantitatively on ten 3D volumes of simulated and clinical brain MR image data having high levels of noise and intensity inhomogeneity and a synthetic 3D image volume with Rician noise. The simulation results reveal that the proposed algorithm outperforms several state-of-the-art algorithms devised in recent past when evaluated in terms of segmentation accuracy, Dice similarity coefficient, partition coefficient, and partition entropy (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106171	10.1016/j.asoc.2020.106171													
J								Markov network versus recurrent neural network in forming herd behavior based on sight and simple sound communication	APPLIED SOFT COMPUTING										Herd behavior; Sound communication; Markov network; Recurrent neural network; Genetic algorithm	SELFISH HERD; FLOCK; EVOLUTION	Sound emission based on information received from the environment, including messages made by other individuals, enables communication between organisms of a given type (e.g., victims). Sound is the main form of communication for animals that they can incorporate into the decision-making process. In this paper, we describe conducted experiments to observe the role of sound communication in forming herd behavior. During the simulation, we investigated prey and predator organisms steered by a controller in the virtual world. We consider two types of agent controllers. The first one is developed using a Markov Network, the second one - a Recurrent Neural Network. The controller, based on information received in the form of environmental stimuli or states of own memory, makes decisions to change the position or, optionally, to make a sound that can then be picked up by nearby individuals. To find the parameters of the controllers, they are evolved by a genetic algorithm. In each generation, genotypes are decoded to the recurrent neural network or Markov Network, then some steps of simulations in a unique artificial environment, modeling the real world, are performed. On this basis, the evaluation of individuals is calculated. The main research element in this work was examining the impact of simple sound communication on forming herd behavior under the predator pressure. A comparison of controllers, i.e., Markov Network and Recurrent Neural Network, was the second goal of our research. (C) 2020 Published by Elsevier B.V.																	1568-4946	1872-9681				MAY	2020	90								106177	10.1016/j.asoc.2020.106177													
J								Optimising Convolutional Neural Networks using a Hybrid Statistically-driven Coral Reef Optimisation algorithm	APPLIED SOFT COMPUTING										Neuroevolution; Convolutional Neural Networks; Optimisation; Hybridisation; Coral Reef Optimisation		Convolutional Neural Networks stands at the front of many solutions which deal with computer vision related tasks. The use and the applications of these models are growing unceasingly, as well as the complexity required to deal with bigger and highly complex problems. However, hitting the most suitable model for solving a specific task is not trivial. A very manually intensive and time consuming trial-and-error experimentation is needed in order to find an architecture, hyperparameters and parameters which reach a certain level of performance. Moreover, this process leads to oversized models, diminishing their generalisation capacity. In this paper, we leverage a metaheuristic and a hybridisation process to optimise the reasoning block of CNN models, composed by fully connected and dropout layers, conducting a full reconstruction that leads to lighter models with better performance. Our approach is architecture-independent and operates at the topology, hyperparameters and parameters (connection weights) levels. For that purpose, we have implemented the Hybrid Statistically-driven Coral Reef Optimisation (HSCRO) algorithm as an extension of SCRO, a metaheuristic which does not require to adjust any parameter since they are automatically and dynamically chosen based on the statistical characteristics of the evolution. In addition, a hybridisation process employs the backpropagation algorithm to make a final fine-grained weights adjustment. In the experiments, the VGG-16 model is successfully optimised in two different scenarios (the CIFAR-10 and the CINIC-10 datasets), resulting in a lighter architecture, with an 88% reduction of the connection weights, but without losing its generalisation performance. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106144	10.1016/j.asoc.2020.106144													
J								A new metaheuristic optimization algorithm inspired by human dynasties with an application to the wind turbine micrositing problem	APPLIED SOFT COMPUTING										Soft Computing; Genetic Algorithm; Differential Evolution Algorithm; Wind turbine Micrositing; Engineering optimization	DIFFERENTIAL EVOLUTION; ENERGY-CONSERVATION; CONFIDENCE-BOUNDS; PLACEMENT; FARM; MOTORS	Optimization is an art that is best performed by a well-tuned algorithm. Nature - instead of being fully deterministic - is evolutionary, vibrant and resourceful. The nature-inspired algorithms use the best combination and evolution strategy in a given situation. In this work, a new metaheuristic algorithm is developed by using social behavior in human dynasties. The motivation, conceptual framework, mathematical model, pseudocode and working of the algorithm are described in this paper and the adjoining papers. The proposed dynastic optimization algorithm (DOA) has evolved with the wind turbine micrositing (WTM) problem in mind. The proposed DOA has been successfully applied to the traditional WTM and encouraging results have been obtained. It is demonstrated that the proposed approach is equally viable as other existing algorithms, like the Genetic algorithm (GA) and Differential evolution algorithm (DEA). The main advantage of the proposed DOA is that it is simple, unique, fast, unbiased and versatile in comparison with others. The validation of results has been made with respect to a few other mainstream algorithms in the literature, besides statistical sensitivity analysis is also performed. The 95% confidence interval forecasts for the power enhancement and cost reduction by using DOA against GA and DEA are encouraging and guarantee an adequate amount of mean increase in power output and a considerable average cost reduction. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106176	10.1016/j.asoc.2020.106176													
J								Masked Conditional Neural Networks for sound classification	APPLIED SOFT COMPUTING										Restricted Boltzmann Machine; RBM; Conditional Restricted Boltzmann Machine; CRBM; Music Information Retrieval; MIR; Environmental Sound Recognition; ESR; Conditional Neural Networks; CLNN; Masked Conditional Neural Networks; MCLNN; Deep Neural Networks	GENRE CLASSIFICATION; MUSIC; RECOGNITION; FEATURES	The remarkable success of deep convolutional neural networks in image-related applications has led to their adoption also for sound processing. Typically the input is a time-frequency representation such as a spectrogram, and in some cases this is treated as a two-dimensional image. However, spectrogram properties are very different to those of natural images. Instead of an object occupying a contiguous region in a natural image, frequencies of a sound are scattered about the frequency axis of a spectrogram in a pattern unique to that particular sound. Applying conventional convolution neural networks has therefore required extensive hand-tuning, and presented the need to find an architecture better suited to the time-frequency properties of audio. We introduce the ConditionaL Neural Network (CLNN)(1) and its extension, the Masked ConditionaL Neural Network (MCLNN) designed to exploit the nature of sound in a time-frequency representation. The CLNN is, broadly speaking, linear across frequencies but non-linear across time: it conditions its inference at a particular time based on preceding and succeeding time slices, and the MCLNN use a controlled systematic sparseness that embeds a filterbank-like behavior within the network. Additionally, the MCLNN automates the concurrent exploration of several feature combinations analogous to hand-crafting the optimum combination of features for a recognition task. We have applied the MCLNN to the problem of music genre classification, and environmental sound recognition on several music (Ballroom, GTZAN, ISMIR2004, and Homburg), and environmental sound (Urbansound8K, ESC-10, and ESC-50) datasets. The classification accuracy of the MCLNN surpasses neural networks based architectures including state-of-the-art Convolutional Neural Networks and several hand-crafted attempts. (C) 2020 The Authors. Published by Elsevier B.V.																	1568-4946	1872-9681				MAY	2020	90								106073	10.1016/j.asoc.2020.106073													
J								Scalable and customizable benchmark problems for many-objective optimization	APPLIED SOFT COMPUTING										Benchmark functions; Scalable test functions; Many-objective optimization; Evolutionary algorithms	NONDOMINATED SORTING APPROACH; MULTIOBJECTIVE OPTIMIZATION; EVOLUTIONARY ALGORITHMS; INTELLIGENCE; TESTS; SUITE	Solving many-objective problems (MaOPs) is still a significant challenge in the multi-objective optimization (MOO) field. One way to measure algorithm performance is through the use of benchmark functions (also called test functions or test suites), which are artificial problems with a well-defined mathematical formulation, known solutions and a variety of features and difficulties. In this paper we propose a parameterized generator of scalable and customizable benchmark problems for MaOPs. It is able to generate problems that reproduce features present in other benchmarks and also problems with some new features. We propose here the concept of generative benchmarking, in which one can generate an infinite number of MOO problems, by varying parameters that control specific features that the problem should have: scalability in the number of variables and objectives, bias, deceptiveness, multimodality, robust and non-robust solutions, shape of the Pareto front, and constraints. The proposed Generalized Position-Distance (GPD) tunable benchmark generator uses the position-distance paradigm, a basic approach to building test functions, used in other benchmarks such as Deb, Thiele, Laumanns and Zitzler (DTLZ), Walking Fish Group (WFG) and others. It includes scalable problems in any number of variables and objectives and it presents Pareto fronts with different characteristics. The resulting functions are easy to understand and visualize, easy to implement, fast to compute and their Pareto optimal solutions are known. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106139	10.1016/j.asoc.2020.106139													
J								Surrogate models for high performance control systems in wind-excited tall buildings	APPLIED SOFT COMPUTING												High-performance control systems (HPCSs), including active, semi-active, and hybrid systems, have been demonstrated as promising methods to mitigate a variety of excitations. However, their deployment in the field is still very limited, attributable to reliability concerns in the closed loop configuration. A solution to promote their applicability is the development of an uncertainty-based design procedure, but such solution comes at a high computational cost due to the large number of possible scenarios to consider on both the closed-loop configuration and external load sides. To alleviate the computational demand of such analysis, this paper investigates the use of data-driven surrogate assisted techniques for uncertainty quantification of HPCSs deployed in wind-excited tall buildings. Both a Kriging surrogate and an adaptive wavelet network (AWN) are investigated and compared to map the unknown relationship between structural inputs and responses. The Kriging model exploits an offline batch learning process while the AWN uses an online sequential process. The surrogate models are applied to a 39-story building equipped with semi-active friction devices exposed to wind load and are compared in terms of accuracy and computational time. Two applications of the surrogate models for uncertainty analysis of the case study building are presented. One is for uncertainty quantification and the other for identification of the most influential uncertain variables. Results show that Kriging provides a more accurate representation to map uncertainties to the system response and to quantify the uncertain performance of HPCSs, but that the AWN provides a significantly faster computational alternative. In particular, for a case containing 17 uncertain variables, Kriging found a representation in 3h20, while the AWN converged in 37 min. Under 41 uncertain variables, these metrics increased to 16h20 and 3h22 for Kriging and the AWN, respectively. These representations were leveraged to identify and remove the uncertainties from three key variables yielding high variance in structural response. Results showed that variables identified under Kriging yielded a 34.9% decrease in variance under 17 uncertain inputs and a 22.9% decrease in variance under 41 uncertain inputs, while AWN yielded a 29.0% and 19.8% decrease, respectively. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106133	10.1016/j.asoc.2020.106133													
J								Strongly-typed genetic programming and fuzzy inference system: An embedded approach to model and generate trading rules	APPLIED SOFT COMPUTING										Strongly typed genetic programming; Stock market predictions; Recommendation system; Trading rules; Fuzzy inference system	TECHNICAL ANALYSIS	Generating trading signals is an interesting topic and a hard problem to solve. This work uses fuzzy inference system (FIS) and strongly typed genetic programming (STGP) to generate trading rules for the US stock market, a framework that we call FISTGP. The two embedded models have not been widely evaluated in financial applications, and according to the literature, their combination could improve forecasting performance. The fitness function used to train the STGP model is based on accuracy, optimizing the buy and sell signals, taking a different approach to the classic optimization of return-risk ratio. The rules are generated in a FIS framework, and the final signal depends on the amount of information that the investor relies on. The model is suited to each investor as a recommendation of when to change portfolio composition according to his or her particular criteria. Ternary rules are generated based on an economic interpretation, considering the risk-free rate as a part of more demanding rules. The model is applied to 90 of the most traded and active stocks in the US stock market. This approach generates important recommendations and delivers useful information to investors. The results show that the proposed model outperforms the Buy and Hold (B&H) strategy by 28.62% in the test period, considering excesses of return, with almost the same risk (1.28% higher). The other base models underperform in comparison to the B&H, with the proposed model also outperforming them. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106169	10.1016/j.asoc.2020.106169													
J								An integration of a QFD model with Fuzzy-ANP approach for determining the importance weights for engineering characteristics of the proposed wheelchair design	APPLIED SOFT COMPUTING										Ergonomics; Customer needs; Triangular fuzzy numbers; Fuzzy extent	QUALITY FUNCTION DEPLOYMENT; EXTENT ANALYSIS METHOD	A wheelchair design with a nested seat back and hand-rest was proposed and ergonomically analyzed, with the objective of decreasing the likelihood of poor and awkward body postures for both the disabled user and his/her companion. The proposed design was validated by integrating a Quality Function Deployment (QFD) framework with a Fuzzy Analytic Network Process (FANP) to determine the degree of importance of the engineering characteristics. In this study, the influence of this integration on determining weights that prioritize engineering characteristics (ECs) was highlighted by taking into consideration the mutual dependence between customer needs (CNs) and ECs and the inner dependence amongst them. This study focused on utilizing FANP methodology, in which triangular fuzzy numbers (TFNs) were used to represent the degree of importance of CNs and ECs, since human judgments of the intensity of preference for these attributes are subjective, vague, and uncertain. Regarding the importance weights for engineering characteristics of the proposed wheelchair design, it has been found that the quality of material scores the highest weight compared to other ECs, with an overall importance weight of 0.43. This result differed from that one obtained using the QFD model without integration, in which the method of design came in first position. The integrated approach proved to be a promising tool in solving fuzzy decision-making problems in different fields and in several applications such as product development and design for ergonomics. As the FANP approach is not popular in the product development/selection field, this study will expand its employment by decision makers in facilitating the evaluation process where there are interrelated factors under an uncertain environment. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106136	10.1016/j.asoc.2020.106136													
J								Combining information from thresholding techniques through an evolutionary Bayesian network algorithm	APPLIED SOFT COMPUTING										Evolutionary algorithms; Bayesian networks; Image segmentation; Thresholding; Digital image processing	MINIMUM CROSS-ENTROPY; PARTICLE SWARM OPTIMIZATION; COLOR IMAGE SEGMENTATION; CUCKOO SEARCH ALGORITHM; DIFFERENTIAL EVOLUTION; EFFICIENT METHOD; MODEL; SET	Segmentation is an important task in image processing because it could affect the performance of other steps in image analysis. One of the most used methods for segmentation is thresholding which can be formulated as an optimization problem, and evolutionary algorithms (EAs) are alternatives commonly applied to solve it. Estimation of Distribution Algorithms (EDAs) is a branch of EAs that explores the search space by building a probabilistic model, such as Bayesian Networks (BNs). In this article is proposed a BN-based EDA for multilevel image segmentation called BNMTH. The proposed approach iteratively selects the combination of thresholding techniques that permits to find the best configuration of thresholds for a digital image, exploring the inter-dependencies between the decision variables (thresholds) and the different techniques. BNMTH is applied over a set of benchmark images and the results of the segmentation are qualitatively analyzed by using the Peak Signal-to-Noise Ratio (PSNR), the Structure Similarity Index (SSIM) and the Feature Similarity Index (FSIM). Besides, a statistical analysis is provided to compare BNMTH with other state-of-the-art optimization algorithms. The results show that BNMTH is a competitive approach for image segmentation, providing accurate results in almost all the cases. Moreover, the segmented images and the histograms show that the classes are accurately generated even in complex conditions. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106147	10.1016/j.asoc.2020.106147													
J								An integrated multi-criteria decision making model with Self-Organizing Maps for the assessment of the performance of publicly traded banks in Borsa Istanbul	APPLIED SOFT COMPUTING										Self-Organizing Maps; EDAS; MOORA; OCRA; TOPSIS	EFFICIENCY; TOPSIS; SELECTION	The major role of banks is to play financial intermediation function between funding needs and funding surpluses. Any problem in the banking system directly influences not only the stakeholders but also the general economy. There have been many attempts to measure the performance of banks by using Multi-Criteria Decision Making Tools (MCDM). This study aims to assess the performance of the publicly traded banks in Borsa Istanbul operating in the Turkish banking sector for the four quarter of 2018. Contribution of this study can be summarized as follow. Dataset is consisted of three dimensions: (i) financial ratios, (ii) branch and personnel network, (iii) daily stock market returns and standard deviation of daily returns. Using a multi-dimensional dataset compiled from three different data sources enabled an objective assessment of bank performance. Features are selected from the financial ratio dataset by using Self-Organizing Maps technique. Instead of using single weight set, a million weight combinations are calculated to monitor the score distributions of MCDM tools. EDAS, MOORA, OCRA and TOPSIS techniques are selected due to their similarity in calculation steps. It is found that OCRA technique produced consistent rankings for different periods. Highest correlation coefficients observed between OCRA and TOPSIS techniques. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106166	10.1016/j.asoc.2020.106166													
J								Constrained evolutionary algorithms for epidemic spreading curing policy	APPLIED SOFT COMPUTING										Epidemic spreading; Complex networks; NIMFA model; Differential evolution; Genetic algorithms; Simulated binary crossover	DIFFERENTIAL EVOLUTION; GENETIC ALGORITHM; RANKING	The design and developments of policies aiming to control and contain spreading processes when resources are limited is an important problem in many application domains dealing with resource allocation, such as public health and network security. This problem, referred as Optimal Curing Policy (OCP) problem, can be formalized as a constrained minimization problem by relying on the approximated heterogeneous N-Intertwined Mean-Field Approximation (NIMFA) model of the SIS spreading process. In this paper, an approach which combines Differential Evolution and Genetic Algorithms is proposed to solve the OCP problem. The hybridization leverages the best characteristics of the two methods to produce high quality solutions in an efficient and effective way. An extensive experimentation on both real-world and synthetic networks shows that the approach is able to outperform a standard solver for semidefinite programming. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106173	10.1016/j.asoc.2020.106173													
J								Multi-label feature selection based on label distribution and feature complementarity	APPLIED SOFT COMPUTING										Multi-label feature selection; Neighborhood rough set; Feature complementarity; Label distribution; Mutual information; Multi-label data	RECOMMENDER SYSTEM; MUTUAL INFORMATION; MISSING LABELS; CLASSIFICATION; DEPENDENCY	In the real-world, data in various domains usually tend to be high-dimensional, which may result in considerable time complexity and poor performance for multi-label classification problems. Multi-label feature selection is an important preprocessing step in machine learning, which can effectively solve the so-called "curse of dimensionality'' by removing irrelevant and redundant features. Nevertheless, the significance of related labels for each instance is generally different, which is an issue that most of the existing multi-label feature selection algorithms have not addressed. Hence, in this paper, we integrate label-distribution learning into multi-label feature selection from the perspective of granular computing with considering multiple feature correlations. Then, a novel multi-label feature selection algorithm based on label distribution and feature complementarity is developed. In addition, the proposed algorithm consists of two primary parts: first, the different significances of related labels for each instance in the multi-label data are obtained based on granular computing; second, the feature complementarity is estimated based on neighborhood mutual information without discretization. Moreover, the superiority of our proposed method over other state-of-the-art methods is demonstrated by conducting comprehensive experiments with 10 publicly available multi-label datasets on six widely-used metrics. Finally, the proposed method can significantly improve the performance of the classifier while reducing the dimension of the original data. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106167	10.1016/j.asoc.2020.106167													
J								Adaptive inertia weight Bat algorithm with Sugeno-Function fuzzy search	APPLIED SOFT COMPUTING										Bat algorithm; Adaptive inertia weight; Neural networks; Sugeno-Function	DIFFERENTIAL EVOLUTION; OPTIMIZATION ALGORITHM; INSPIRED ALGORITHM; GENETIC ALGORITHM; NEURAL-NETWORKS; CLASSIFICATION; DISCRETE	Bat algorithm (BA) turns into the most generally utilized meta-heuristic algorithm to solve the different sort of global optimization problems. In the optimization of continuous data, BA experiences one of the prominent difficulties called premature convergence. In order to tackle premature convergence, this study exhibits a new version of BA called Adaptive inertia weight Bat algorithm with Sugeno-Function Fuzzy Search (ASF-BA). The proposed algorithm ASF-BA brings two major adjustments in the standard BA. Firstly, we incorporated an adaptive inertia weight to boost up the velocity rate of bats effectively. Secondly, we replaced the random searching method of standard BA with Sugeno-Function fuzzy search, which used Sugeno-Function decline curves to dynamically adjust the fitness of each bat according to their own experience and experience of their neighbour bats. We compared ASF-BA with several old fashioned and new fashioned optimization algorithms. ASF-BA is also tested against top hybridized and enhanced version of DE algorithms. The CEC 2017 benchmark (30 real parameter numerical optimization problems), CEC 2017 (28 constrained optimization problems) and 19 additional benchmark problems have been used to examine and compare the performance of ASF-BA with other state of the art variants. Contrasted with the existing BA and other leading variants of BA, DE, and PSO on CEC 2017 constrained and numerical benchmarks, the ASF-BA is excellent to the state-of-art variants of BA, DE, and PSO in terms of stability, convergence speed and solution quality. The ASF-BA sets stable support for resolving optimization problems of intelligent and expert systems. Furthermore, we also examined the performance of proposed ASF-BA for the weight optimization of Feed Forward Neural Networks (FFNN) and compared ASF-BA with Back Propagation Algorithm (BPA), BA and PSO. ASF-BA achieved 94 % of maximum accuracy. The experimental outcomes reveal that the suggested algorithm executed especially reliable and effective than the existing state of the art variants. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106159	10.1016/j.asoc.2020.106159													
J								Momentum method powered by swarm approaches for topology optimization	APPLIED SOFT COMPUTING										Topology optimization; Momentum gradients; Metaheuristic algorithms; Mathematical programming	ALGORITHM; DESIGN	This paper presents the momentum algorithm for topology optimization problems. Since topology optimization problems are multimodal, the momentum may be trapped into a local optimum. To achieve a better solution, this approach is combined with an enhanced particle swarm optimization (PSOG). The constrained problems are converted into unconstrained ones by an external penalty function. To illustrate the effectiveness of the proposed method, five examples with static, multiple and self-weight loadings are examined. The problems are investigated by the proposed method and the results are compared with optimality criteria algorithm (OC), method of moving asymptotes (MMA), sequential linear programming (SLP), momentum, PSO and PSOG. The numerical results show the superiority of the proposed approach. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106174	10.1016/j.asoc.2020.106174													
J								Adaptive repair method for constraint handling in multi-objective genetic algorithm based on relationship between constraints and variables	APPLIED SOFT COMPUTING										Evolutionary optimization; Constraint handling; Multi-objective; Genetic algorithm; Global optimization	EVOLUTIONARY ALGORITHMS; OPTIMIZATION	While evolutionary algorithms are known among the best methods for solving both theoretical and real-world optimization problems, constraint handling is still one of the major concerns. Common constraint handling methods reject or devalue infeasible solutions depending on their distance from the feasible space, even if they dominate feasible solutions. Alternatively, repair methods aim to overcome infeasibility, but they are currently limited to specific types of problems. In this paper, we propose a more generic repair approach to improve efficiency of constraint handling in non-dominance based genetic algorithm. We start by identifying variables which influence each constraint. This information is used to replace variable values that caused constraint violation, using other solutions in the current generation. Repairing is carried out on the solutions that dominate all feasible members of the population, or have the smallest constraint violation. The repair approach is implemented into NSGA-II and tested on one optimization test case and an engineering optimization problem. The latter focuses on structural design of a ship hull girder, involving two conflicting objectives, 94 decision variables and 376 nonlinear constraints. The proposed repairing approach reduces drastically the number of function evaluations needed to find the feasible space, and it leads to faster convergence and better spread of the non-dominated front. Starting from different random populations, the new algorithm finds feasible solutions within one generation, while the original algorithm takes between 7 and 72 generations. Effectiveness of the optimization is analyzed in terms of the hypervolume performance metric. The repairing algorithm obtains significantly better hypervolume values throughout the optimization run. The highest improvements are achieved in the initial phase of the optimization, which is important for the practical design. The new algorithm performs better than two constraint handling approaches from the literature. It also outperforms MOEA/D algorithm in the engineering problem. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106143	10.1016/j.asoc.2020.106143													
J								Wind power prediction using a three stage genetic ensemble and auxiliary predictor	APPLIED SOFT COMPUTING										Wind power prediction; Computational intelligence; Artificial neural networks (ANN); Genetic programming (GP); Radial basis function (RBF); Relevance vector machine	SPEED; REGRESSION; MODELS	This paper presents a novel method for accurate wind power prediction by applying computational intelligence approaches while exploiting Auxiliary Predictor (AxP) and Genetic Programming (GP) based ensemble of Neural Networks (AxP-GPNN). The inherent fluctuations in the power generated by wind mills may affect their optimal integration in the electric grid and therefore, accurate prediction is highly desired. To cater these fluctuations and highly nonlinear mapping, we present an ensemble approach, where the auxiliary predictor is constructed with Radial Basis Function (RBF) network and Relevance Vector Machine (RVM) and various neural networks are then employed as base regressors. Use of RVM is based on its established advantages for robust prediction on unseen data to address the overfitting issue in training phase. AxP is used for suitable weight initialization to base predictors and provides initial decision space to base learners. Further, an ensemble of neural networks based on GP is developed which utilizes the base predictions of neural networks as well as the auxiliary information generated by AxP. The GP ensemble based forecasting engine is thus robust to minor variations in the data as compared to the individual base regressors. We also employ informationtheoretic feature selection on physical measurements of the wind mills. Results have been extracted in the form of statistical performance indices including mean absolute error, standard deviation error and mean square error. These error measures are compared with the other existing wind power prediction techniques. These results present better wind power estimates and reduced prediction error. Paired t-test for the proposed model with other machine learning based models is carried out for further evaluation. Overall, these comparisons validate the importance of auxiliary predictor in ensemble model of GP and ANNs. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106151	10.1016/j.asoc.2020.106151													
J								A novel image encryption scheme using both pixel level and bit level permutation with chaotic map	APPLIED SOFT COMPUTING										Hilbert curve; Cyclic shift; Henon map; Security analysis; Image encryption	SIGNAL SECURITY SYSTEM; CRYPTANALYSIS; CRYPTOGRAPHY; CIPHERS	In cryptography, chaotic cryptosystem is one of the methods to carry out encryption and decryption of images. This paper introduces a new symmetric key encryption technique based on chaotic map, scan method and cyclic shift operation. The confusion and diffusion techniques are implemented using Hilbert curve and Henon map. To ensure image scrambling, both pixel level and bit level permutations are performed. A novel method is adopted for bit level permutation using cyclic shift operation. The key streams for cyclic shift and diffusion operations are generated from the Henon map. Final encrypted image is generated from the double scrambled image. The performance of the proposed method has been analyzed using various analyses like statistical analysis, entropy analysis, differential attack analysis, key sensitivity analysis and known plain text attack analysis. Experimental results show that the proposed image encryption technique resists various attacks and ensures high security. It also provides better performance when compared with several traditional and state-of-the-art image encryption methods. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106162	10.1016/j.asoc.2020.106162													
J								Taxonomical classification of barriers for scaling agile methods in global software development environment using fuzzy analytic hierarchy process	APPLIED SOFT COMPUTING										Global software development; Agile development; Scaling barriers; Fuzzy-analytic hierarchical process	CRITICAL SUCCESS FACTORS; DISTRIBUTED AGILE; KNOWLEDGE MANAGEMENT; PROCESS IMPROVEMENT; DECISION-MAKING; AHP; PRIORITIZATION; COMMUNICATION; CHALLENGES; SELECTION	Increasingly, software development organizations are scaling agile practices in the global software development (GSD) environment in order to meet the requirements of the quickly changing and regularly developing business environment. The main objectives of this study are to investigate the key barriers and develop a prioritization-based taxonomy of the barriers for scaling agile development in the GSD environment. Total twenty-two barriers were extracted from the available literature and categorized into five categories, i.e. "human resources management'', 'coordination'', "technology'', "project management'', and "software methodology''. In the next phase, the identified barriers and their categories were further validated using the questionnaire survey. In the final phase, fuzzy-AHP method, a multi-criterion decision making (MCDM) technique, was applied to prioritize and taxonomy of identified barriers and their related categories was designed. The contribution of this study is not limited to investigate the barriers, but it also provides the roadmap to tackle the issues related to the scaling agile methods in the GSD environment. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106122	10.1016/j.asoc.2020.106122													
J								A novel foraging algorithm for swarm robotics based on virtual pheromones and neural network	APPLIED SOFT COMPUTING										Swarm robotics; Cooperative foraging; Virtual pheromone; Self-organizing; Neural network	TASK ALLOCATION; CONFLICT-RESOLUTION; ARCHITECTURE; MODEL	Swarm robotics is an emerging interdisciplinary field that has many potential real-world applications. Swarm robotics aims to produce robust, scalable, and flexible self-organizing behaviors through local interactions from a large number of simple robots. In this paper, a novel pheromone model of swarm foraging behavior is developed based on a neural network. The output of a single neuron corresponds to the density of a pheromone, which diffuses to neighboring neurons through their local connections. A neural network is updated based on the proposed evaporation model. Neural networks can often mimic the dynamics and features of pheromones. Therefore, in this work, we develop an optimization method to determine the key parameters of cooperative foraging based on mathematical modeling. The differential equation variables represent the number of foraging robots assigned different tasks. The solutions of the differential equations represent the dynamics of the foraging behavior. The key parameters that affect task allocation are determined to make optimal decision rules. Simulation experiments are conducted under different foraging scenarios. The experimental results demonstrate the effectiveness of the proposed pheromone model. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106156	10.1016/j.asoc.2020.106156													
