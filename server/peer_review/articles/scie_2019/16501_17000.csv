PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	RP	EM	RI	OI	FU	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	D2	EA	PG	WC	SC	GA	UT	PM	OA	HC	HP	DA
J								Coal and rock interface identification based on wavelet packet decomposition and fuzzy neural network	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Multi-sensor; coal-rock interface identification; fuzzy neural network; wavelet packet decomposition		The coal-rock interface identification function enables the shearer to automatically identify the coal-rock interface and demonstrates outstanding advantages in improving economic efficiency and safe operation. It can improve the recovery rate of coal seam, reduce the content of rock, ash and sulfur in coal, improve the efficiency of coal mining operation and reduce equipment wear. It is one of the key equipments to realize coal mining automation. At present, there are more and more researchers on the research of coal rock interface identification technology. A common method is to use a single sensor to establish a coal rock identification system, and use the neural network algorithm as the core algorithm of the system. Therefore, this paper proposes a recognition system based on wavelet packet decomposition and fuzzy neural network. A variety of sensors are used to collect the response signal of the shearer, and then the multi-signal feature extraction and data fusion of the coal-rock interface identification method are realized, thereby improving the recognition rate. On the basis of the physical simulation system of coal and rock interface, a large number of tests were carried out, and a large amount of test data was collected through experiments. In view of the many advantages of wavelet analysis, this paper uses wavelet packet technology to extract signal features. An energy allocation method based on wavelet packet decomposition can determine the sensitive frequency band of each sensor signal and extract each feature value. The wavelet packet energy method is used for feature extraction, which completes the conversion from mode space to feature space, and provides reliable and accurate feature level data for data fusion. The results show that neural networks and genetic neural networks can be trained and simulated using experimental data. Data fusion based on genetic neural network can perform state recognition and has high recognition accuracy. Multi-sensor data fusion technology based on genetic neural network is feasible in coal-rock interface identification.																	1064-1246	1875-8967					2020	38	4			SI		3949	3959		10.3233/JIFS-179620													
J								Multiple sensor data fusion algorithm based on fuzzy sets and statistical theory	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Multiple sensor data fusion; weight distribution; fusion precision; convergence speed		Multiple sensor information fusion technology originated from the military. It has developed into a very active and popular field of defense research. It is also a high-level universal key technology, which has attracted attention in many disciplines and fields. Many countries, including China, have listed them on the research list. The purpose of this paper is to study multiple sensor data fusion through fuzzy sets and statistical theory. According to the scholars' research results at home and abroad, proposed a new evidence synthesis algorithm. The algorithm combines the advantages of modifying the original evidence and modifying the comprehensive rules. By comparing the accuracy of 100 sensor data in multiple sensors and single sensors, the consistency information and conflict information between the evidences are, mined and comprehensively considered the consistency information and conflict information between the evidences, and analyzed the evidence empirically. Considered ENCE in the weight distribution of conflict evidence fully. By comparing the results of multiple sets of experiments, established multiple sensor data fusion algorithm based on fuzzy sets and statistical theory. Record experimental data and analyze the experimental results. The experimental results show that compared with other methods, the method can reflect the credibility of the evidence more objectively, the convergence speed is faster, and the fusion result is more in line with the actual situation. The experimental results show that the fusion algorithm based on the best fusion set and the new integrated method of conflict evidence can be used as the core algorithm of local fusion center and global fusion center in the fusion system respectively, and can also be used as the secondary fusion model of the fusion system.																	1064-1246	1875-8967					2020	38	4			SI		3961	3970		10.3233/JIFS-179621													
J								Knowledge fusion method based on fuzzy set theory	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Knowledge management; information fusion; fuzzy set theory; fuzzy petri net; knowledge update	MANAGEMENT; RISK	The discussion of knowledge management and technological innovation has never stopped, and the discussion of the relationship between the two has not only important practical significance but also profound theoretical significance. The purpose of this paper is to study a new method of knowledge fusion from the perspective of science and technology philosophy. From a newperspective, this paper analyzes defects of subjective tendencies, decision-dependent partial attributes of knowledge source existed in practical applications of management field and proposed a knowledge fusion method based on fuzzy set theory. This paper firstly explains the characteristics of knowledge sources, and transforms knowledge into a new knowledge layer through combining multi-source knowledge, then improves the connotation, level and self-confidence of knowledge, finally improves the ability of the system to accomplish tasks and goals. Then combine the fuzzy set theory with the knowledge fusion algorithm reasonably and effectively, and obtain the results of knowledge fusion by using evidence synthesis and decision rules, so as to make up for the lack and defects in the knowledge fusion process and solve the uncertainty problem in knowledge reasoning. Finally, through the practical example, merged the fuzzy set theory proposed in this paper into knowledge fusion to deal it, obtain a kind of processing of fuzzy set theory, forming a knowledge fusion method based on fuzzy set theory. Based on fuzzy set theory, obtain the observation results of knowledge fusion algorithm combined with the various warning models, then to discuss and analyze the enterprise warning problem deeply. Therefore, the examples and simulation results show that the advantages in practicality and versatility of knowledge fusion method proposed through fuzzy set theory is higher than the common knowledge fusion method. The method used in the production of manufacturing products can help manufacturing companies improve development quality of product and shorten development cycle of product. Moreover, in the product design industry, it has verified that knowledge fusion can promote the dissemination of knowledge in the field of knowledge management, which helps to share and reuse design knowledge, reduce difficulty of development and improve efficiency of development.																	1064-1246	1875-8967					2020	38	4			SI		3971	3979		10.3233/JIFS-179622													
J								Intelligent recognition method of infrared imaging target of unmanned autonomous ship based on fuzzy mathematical model	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy mathematical model; unmanned autonomous ship; infrared imaging; intelligent target recognition	BREAD	In view of the objectively ambiguous feature of infrared image of unmanned autonomous ship, this paper presents a quantitative method to deal with the ambiguity problem in infrared image by using the fuzzy mathematical model to realize the purpose of intelligent recognition of infrared imaging target. In order to simplify the computation of target recognition and improve the response time and accuracy in the selection of target features in infrared images, three features of target location, radiation distribution and shape are selected for analysis in this paper. The membership functions of these three features are weighted to calculate the confidence, and the classification and recognition are realized according to the confidence. Finally, the simulation results show that the recognition method proposed in this paper can effectively identify the target, and the recognition rate is very high. Compared with the recognition methods based on neural network and SVM, the recognition distance of this method is longer than that of the latter two methods.																	1064-1246	1875-8967					2020	38	4			SI		3981	3989		10.3233/JIFS-179623													
J								Fuzzy clustering algorithm for time series based on adaptive incremental learning	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Network data; adaptive incremental learning; time series; fuzzy clustering algorithm	CLASSIFICATION; SELECTION	With the development of Internet technology, the growth of network services is accelerating. For more and more network service requests, how to ensure the response speed and query accuracy required by users is a huge challenge. In order to realize fast clustering of large data business request data and improve the accuracy of clustering. This paper presents a data fuzzy clustering algorithm based on Adaptive Incremental learning time series. The algorithm defines large data clustering in time series, and the incremental time series clustering method is used. Firstly, the complexity of network data is reduced by data compression, and then time series data clustering based on service time similarity is carried out. In this paper, the time series fuzzy clustering algorithm based on Adaptive Incremental Learning inherits the clustering structure information obtained by previous clustering. Initialize the current clustering process, and then search the outlier samples in the current data block adaptively without setting parameters. Automatically create new clusters from outlier samples, and finally check empty cluster recognition. Identification determines whether certain clusters need to be deleted to ensure the efficiency of subsequent cluster processes. The experimental results show that the algorithm has good clustering accuracy and efficiency for isochronous and unequal time series.																	1064-1246	1875-8967					2020	38	4			SI		3991	3998		10.3233/JIFS-179624													
J								Some partitioned heronian mean aggregation operators based on intuitionistic linguistic information and their application to decision-making	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Intuitionistic linguistic information; PHM; MAGDM	FUZZY INFORMATION; ATTRIBUTES	The intuitionistic linguistic (IL) variable (ILV) can express the vague and uncertain information in a better way, and the partitioned Heronian mean (PHM) operator can group attributes that have relationships with each other into one zone and the independent attributes are in different zones, so in this paper, we will propose some new PHM operators for IL information (ILI) and then apply them to multiple attribute group decision-making (MAGDM). Firstly, the some improved operational rules for ILVs are developed, which can provide a more accurate result and avoid the loss of information, then we extend the PHM operator to the IL environment and propose the intuitionistic linguistic partitioned Heronian mean (ILPHM) operator and the intuitionistic linguistic weighted partitioned Heronian mean (ILWPHM) operator, which can fully consider the advantages of the ILI and the PHM operator. Meanwhile, we discuss some desirable properties and special cases of the two operators. Further, we develop the MAGDM approach with ILI based on the developed operators. Lastly, a numerical instance is given to verify the feasibility and the superiority of the proposed method.																	1064-1246	1875-8967					2020	38	4			SI		4001	4029		10.3233/JIFS-181175													
J								Multiattribute social network matching with unknown weight and different risk preference	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Multiattribute matching decision making; social network analysis; optimization model; risk preference	GROUP DECISION-MAKING; CONSENSUS MODEL; MISINFORMATION	In multiattribute decision making problem, it is usually that the opinions of agent may be influenced by close friends or people with similar interests among his social network. This situation could act on the evaluation process of agent toward candidates, thereby affecting the overall two-sided matching results. This paper proposes a framework based on social network analysis. It can effectively achieve multiattribute social network matching that the relative weights of influencers are unknown before. In the proposed framework, the complete trust assessment matrix can be obtained through depicting the trust propagation relationship between agent and influencers. According to the trust assessment result, a nonlinear optimization model is developed to calculate the unknown weights of influencers, and then the obtained weights of influencers are integrated into the two-sided evaluation process. Furthermore, two multi-objective optimization models are constructed to manage the rational agents and conservative agents. Based on the multi-objective optimization models, we generate the optimal matching pairs. Finally, detailed comparison analysis and discussion are presented to check the effectiveness of the proposed social network matching method.																	1064-1246	1875-8967					2020	38	4			SI		4031	4048		10.3233/JIFS-182535													
J								L-fuzzy pre-proximities and application to L-fuzzy topologies	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Complete residuated lattice; L-fuzzy pre-proximity; L-fuzzy interior operators; Galois correspondence	SYNTOPOGENOUS STRUCTURES; ROUGH SETS; SPACES; APPROXIMATION; UNIFORMITIES; CATEGORIES; OPERATORS	In this paper, we investigate the relations between the L-fuzzy pre-proximities, L-fuzzy interior operators and L-fuzzy topological spaces in complete residuated lattices. In addition, degrees of L-fuzzy continuity, L-fuzzy proximity and L-fuzzy interior mappings are proposed and their connections are studied. Also, we showthat there is a Galois correspondence between the category of separated L-fuzzy interior spaces and that of separated L-fuzzy pre-proximity spaces. Finally, we give their examples.																	1064-1246	1875-8967					2020	38	4			SI		4049	4060		10.3233/JIFS-182652													
J								Interval-valued Pythagorean Fuzzy EDAS method: An Application to Car Selection Problem	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										EDAS; pythagorean; fuzzy; car selection	GROUP DECISION-MAKING; NEUTROSOPHIC SOFT SET; INTUITIONISTIC FUZZY; TOPSIS; OPERATIONS; EXTENSION; TODIM; VIKOR	Classical multi-criteria decision making (MCDM) methods have been extended to their fuzzy versions under uncertainty in the literature. Besides, these ordinary fuzzy MCDM methods have been further extended to their new versions through the recently developed types of ordinary fuzzy sets. This study extends the evaluation based on distance from the average solution (EDAS) method by using interval-valued Pythagorean fuzzy numbers to solve fuzzy multi-criteria group decision-making problems with a larger membership domain providing more flexibility. An illustrative example of the car selection problem is given to show the effectiveness and applicability of the proposed model and results are compared with intuitionistic interval-valued fuzzy EDAS method. A sensitivity analysis is also performed to reveal the effect of the weights on alternative rankings.																	1064-1246	1875-8967					2020	38	4			SI		4061	4077		10.3233/JIFS-182667													
J								Goodwin economic cycle via p-fuzzy system	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										P-fuzzy systems; Goodwin model; fuzzy rules; fuzzy numbers; fuzzy c-means	MODEL	In this manuscript, we present numerical solutions of a p-fuzzy system that models the Goodwin economic cycle. This model describes the dynamic interaction between labour share and the employment rate in an economy. One can use p-fuzzy systems to obtain solutions for differential equations whose the vector field are uncertain and partially known. The proposed p-fuzzy system is based on a fuzzy rule-based systems whose fuzzy rules represent economic premises described in the Goodwin model. We use two approaches to adjust the fuzzy terms of the corresponding fuzzy rule base from the economical data of a given country. The first approach is based on statical measures and the second one uses fuzzy c-means clustering method. Finally, we test our proposal to estimate the growth cycles of the labour share and the employment rate of Germany and we compare the obtained results with the historical data.																	1064-1246	1875-8967					2020	38	4			SI		4079	4090		10.3233/JIFS-182762													
J								Research on transfer learning algorithm based on support vector machine	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Machine learning; support vector machine; transfer learning; classification	CONVEX	Transfer learning is a new machine learning algorithm. It solves problems in different but related target domains by utilizing the knowledge in existing data. Based on the classical SVM algorithm and transfer learning, a selective transfer learning support vector machine (STL-SVM) algorithm is proposed in this paper. First, STL-SVM uses the maximum mean discrepancy to measure the weight vector of the source domain samples relative to the target domain, and selects samples from the source domain according to each weight to avoid negative transfer. Then, the knowledge in the source domain is learned by the approximate extreme point support vector at the minimum training data cost. Finally, the object function is constructed by the obtained knowledge and the soft-margin SVM. In the constraint conditions of the objective function, the learned knowledge that is highly correlated with the target domain is selected, and further, the phenomenon of negative transfer is avoided in principle. STL-SVM solves the problem of negative transfer, and has considerable advantages in training time efficiency compared with the existing algorithms. The experimental results on artificial and real datasets show the effectiveness of the proposed algorithm.																	1064-1246	1875-8967					2020	38	4			SI		4091	4106		10.3233/JIFS-190055													
J								Priority fuzzy database management system implementation based on extensions to the XQuery language	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy XQuery; XQuery Interpreter; XQuery; XML Database	CONSTRAINT SATISFACTION PROBLEMS; XML; INFORMATION; XPATH	In recent years, there has been an increased interest in processing fuzzy queries over XML data that is also possibly fuzzy. Attention has been paid to various extensions of XML query languages that introduce concepts of fuzzy theory. We propose an extension of the XQuery query language in an attempt to handle flexible queries that provide priority, threshold, and fuzzy expression as well as fuzzy linguistic labels allowing users great flexibility in customizing query constraints. We give a detailed description of an advanced query processing software system developed using GPFCSP (Generalized Prioritized Fuzzy Constraint Satisfaction Problem) as the theoretical background. The software, called FXI (Fuzzy XQuery Interpreter), was developed as a web application using Java, AngularJS, and eXist-db-an open source native XML database and it incorporates various advanced features such as fuzzy ordering operations and fuzzy compatibility calculations that includes priorities. The paper presents its design, the most important considerations related to implementation, as well as testing using realistic scenarios.																	1064-1246	1875-8967					2020	38	4			SI		4107	4118		10.3233/JIFS-190202													
J								Size relation of uncertain sets with application to clustering	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Uncertainty theory; uncertain set; membership function; size relation; clustering	CROSS-ENTROPY; DISTANCE	Along with increasing and extension of uncertain set theory in decision making, ordering and ranking of uncertain sets have significantly become relevant, that is, an uncertain set needs to be evaluated and compared with the others. This paper introduces the size relation between two uncertain sets. It derives a formula to calculate the uncertain measure that an uncertain set is less than or equal to another one. Moreover, some examples are given to illustrate the size relation of uncertain sets. Finally, clustering is proposed as an application.																	1064-1246	1875-8967					2020	38	4			SI		4119	4125		10.3233/JIFS-190342													
J								An algorithm for computing the generalized interaction index for k-maxitive fuzzy measures	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy measures; Shapley index; interaction index; k-maxitive measures		Fuzzy measures are used for modeling interactions between a set of elements. Simplified fuzzy measures, as k-maxitive measures, were proposed in the literature for complexity and semantic considerations. In order to analyze the importance of a coalition in the fuzzy measure, the use of indices is required. This work focuses on the generalized interaction index, gindex. Its computation requires many resources in both time and space. Following the efforts to reduce the complexity of fuzzy measure identification, this work presents two algorithms to compute the gindex for k-maxitive measures. The structure of k-maxitive measures makes possible to compute the gindex considering the coalitions at level k and, for each of them, the number of coalitions sharing the same coefficient (called inheritors). The first algorithm deals with the space complexity and the second one also optimizes the runtime by not generating, but only counting, the number of inheritors. While counting the number of descendants is easy, this is not the case for the number of inheritors due to all the inheritors of previous considered coalitions have to be taken into account. The two proposed algorithms are tested with synthetic k-maxitive measures showing that the second algorithm is around 4 times faster than the first one.																	1064-1246	1875-8967					2020	38	4			SI		4127	4137		10.3233/JIFS-190403													
J								A multi-objective reverse logistics network design model for after-sale services and a tabu search based methodology	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Reverse logistics; network design; tabu search; weighted max-min method	SUPPLY CHAIN NETWORK; MAX-MIN MODEL; OPTIMIZATION MODEL; STOCHASTIC-MODEL; SELECTION; PRODUCTS; RETURNS; COSTS	Reverse logistics has become one of the emerging research and application areas since the early 1990s due to several driving forces such as economic benefits, governmental regulations, increasing levels of environmental problems, etc. As an essential part of reverse logistics (RL) literature, Reverse Logistics Network Design (RLND) efforts can provide economic and ecological advantages to the supply chains. In this study, we consider a reverse distribution channel and try to determine the locations of the collection centers (CCs) and repair centers (RCs) and the amount of product flow between the customers-CCs and CCs-RCs. Considering the multi-objective nature of the RLND problems, total cost and total delay minimization and total average capacity utilization maximization objectives are taken into account. This problem is formulated as a multi-objective mixed integer nonlinear programming model and is solved by two approaches: a heuristic and a tabu search based meta-heuristic in which both of the solution approaches are integrated with weighted max-min approach. Numerical analyses have been carried out to show the effectiveness of the proposed approaches and the solutions obtained from each approach have been compared.																	1064-1246	1875-8967					2020	38	4			SI		4139	4157		10.3233/JIFS-190431													
J								Multiple criteria decision making with hesitant interval-valued fuzzy sets based on hesitance degree and least common multiple principle	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Hesitance degree; hesitant interval-valued fuzzy set; the least common multiple principle; TOPSIS	LINGUISTIC TERM SETS; SIMILARITY MEASURES; DISTANCE MEASURES	An important feature of hesitant interval-valued fuzzy set(HIVFS) is that it can describe the hesitant situations flexibly. In this paper, we first introduce the hesitance degree of HIVFS, then some distance measures between hesitant interval-valued fuzzy sets(HIVFSs) based on the hesitancy degree and least common multiple principle are proposed, which can not only describe the degree of the decision makers' hesitance in decision making process but also measure the relationship between HIVFS more objectively. Furthermore, we develop the TOPSIS method to the proposed distance measures with different preferences for the hesitancy degree and membership values. Finally, a numerical example is given to illustrate the feasibility of the proposed method, and the effectiveness of the method is verified by some comparative analysis. In order to understand the effect of the preference parameter alpha and the distance parameter tau in decision making process, we make the sensitivity analysis of parameters by evaluating the example about commercialization of possible emerging technology enterprises.																	1064-1246	1875-8967					2020	38	4			SI		4159	4172		10.3233/JIFS-190445													
J								Distributed adaptive output-feedback fault tolerant control for nonlinear systems with sensor faults	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fault tolerant control; adaptive control; multiagent systems	S FUZZY-SYSTEMS; TRACKING CONTROL; SYNCHRONIZATION; PERFORMANCE; DIAGNOSIS; VEHICLE	This paper studies the distributed adaptive output-feedback fault tolerant control problem for leader-following multiagent systems with sensor faults. By using the approximation theory of neural networks, an unknown continuous function is approximated, and the problem that the neural network output deviates from the true approximation of the unknown function due to the faults of neural network input is solved. A filter observer is adopted to estimate the unmeasurable states. Based on the adaptive backstepping technique and fault tolerant control technique, a distributed adaptive neural output-feedback control scheme is proposed to guarantee the output consensus of all nodes under directed communication graphs. Based on graph theory and Lyapunov stability theory, it is proved that the proposed adaptive neural control scheme guarantees the uniformly ultimate boundness of the closed-loop systems, and the tracking errors converge to a small adjustable neighborhood of the origin. The simulation results demonstrate the effectiveness of the control approach in this paper.																	1064-1246	1875-8967					2020	38	4			SI		4173	4190		10.3233/JIFS-190531													
J								Monitoring the process mean using a synthetic (X)over-bar control chart with two sampling intervals	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Statistical process control; synthetic (X)over-bar control chart; variable sampling interval; adjusted average time to signal	DESIGN; PARAMETERS	The traditional fixed sampling interval (FSI) synthetic (X) over bar control chart is sensitive to large and moderate process shifts, but insensitive to small process shifts. This paper proposes an improved variable sampling interval (VSI) synthetic (X) over bar control chart with two sampling interval lengths for monitoring the process mean. A Markov chain method is used to compute the adjusted average time to signal (AATS) of the proposed chart. The statistical design of the VSI chart is then formulated and solved with a specific genetic algorithm. Comparisons are made to show the capability of the VSI chart in yielding average performance boost of 16.4%, in the AATS of the VSI chart scheme compared to the situation in which the corresponding FSI chart is used.																	1064-1246	1875-8967					2020	38	4			SI		4191	4203		10.3233/JIFS-190600													
J								Secure multi-party collision resolution protocol for air traffic control	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Privacy; collision detection and resolution; line segment intersection algorithm; exact and inexact collision	AIRCRAFT CONFLICT DETECTION	In this paper, we present a protocol for secure resolution of collision between aircraft, while their corresponding trajectories are kept private. For privacy reasons, we assume that each aircraftaL (TM) s position, heading and velocity are not available to all aircraft involved in the protocol. Our new secure protocol is based on an algorithm, which is for collision resolution of aircraft, without considering the privacy requirement. Our approach is Secure Multi-party Computation. To present our Secure Multi-party Collision Resolution Protocol, we propose a new Secure Sorting Protocol and a Modified Distributed Oblivious Transfer Protocol. We also take advantage of employing a Secure Multi-party Collision Detection Protocol, a Homomorphic Encryption, a Secure Comparison Protocol, and a Secure Maximum Finding Protocol. To prove the security of our proposed protocol and its sub-protocols, we use the Ideal/ Real Simulation Paradigm. The communication complexity of our proposed protocol is in O(n(2)), and its computation complexity is in O(n). We have also done a simulation for the execution of our proposed protocol for multiple moving aircraft, to show its practicality.																	1064-1246	1875-8967					2020	38	4			SI		4205	4221		10.3233/JIFS-190675													
J								Multi-scale feature extraction and recognition of slope damage in high fill channel based on Gabor-SVM method	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Middle route of South-to-North Water Diversion Project; Feature extraction of multiple scales and directions; damage recognition; Gabor wavelet; support vector machine		In order to make the feature data more linearly separable for support vector machine(SVM) classifier, a set of different scale and direction parameter was proposed for improving the recognition effect of cement slope damage in high fill channels. The Gabor wavelet was used to extract the multi-scale and multi-directional features of the high-fill channel's abrupt features. Then SVM algorithm was utilized to perform damage classification and level recognition. To compare the recognition effect of the Gabor-SVM method, histogram-SVM, grayscale symbiotic matrix -SVM, canny-SVM algorithm were adopted to identify the damage degree of cement surface in the same environment, and these damage recognition rates are compared with Gabor-SVM's. The experimental results show that the damage recognition model, based on Gabor-SVM, tends to better stable value when the wavelet takes 6 scale and 12 directions. The recognition rate of the normal slope is 0.98, while the recognition rate of the crack, hole, and broken slope are 0.63, 0.88 and 0.90, respectively. Overall, the damage recognition model, based on Gabor-SVM, has better recognition effect, and it will provide technical support for finding potential leakage hazards in the high fill channel of South-to-North Water Diversion Project.																	1064-1246	1875-8967					2020	38	4			SI		4237	4246		10.3233/JIFS-190767													
J								Analysis of green supply chain considering green degree and sales effort with uncertain demand	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Green supply chain management; pricing decision; uncertain demand; uncertainty theory	PRICE	With resource exhaustion and environmental problems becoming more and more serious, green supply chain management attracts the attention of enterprises and scholars. For a new type of environmental-friendly product in this supply chain, we have no enough historical data to refer the demand. Hence, uncertainty theory is introduced as a tool to characterize the uncertainty of demand. Then this paper investigates the pricing, green degree and sales effort decisions in a green supply chain with a manufacturer and a retailer, where the demand of the product is described by uncertain variable. Considering the different power structures of supply chain, we build three expected value models and obtain equilibrium solutions, respectively. Then we analyze the influences of uncertain parameters and different power structures on decisions and profits. The results show that the green degree coefficient and sales effort coefficient have positive influences on the pricing, green degree and sales effort. Moreover, the results also show that decision leaders do not necessarily make more profits, which is up to the weights of green degree and sales effort. In addition, numerical analyses are given to show the correctness of results.																	1064-1246	1875-8967					2020	38	4			SI		4247	4264		10.3233/JIFS-190783													
J								A fixed point approach to approximate quadratic functional equation in non-Archimedean L*-fuzzy normed spaces	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										L*-fuzzy metric and normed space; Hyers-Ulam stability; quadratic functional equation; non-Archimedean L*-fuzzy normed space	STABILITY	We establish some stability results concerning the 2-dimensional vector variable quadratic functional equation f(x + y, z + w) + f(x - y, z - w) = 2f(x, z) + 2f(y, w) in non-Archimedean L*-fuzzy normed spaces.																	1064-1246	1875-8967					2020	38	4			SI		4265	4272		10.3233/JIFS-190802													
J								Fuzzy SVM based pre-processing technique for infrared (IR) thermal images	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										IR thermal images; fuzzy membership; fuzzy inference system; support vector machine; directional weighted median filter	IMPULSE NOISE REMOVAL; DIRECTIONAL MEDIAN FILTER; PEPPER NOISE; REDUCTION; SALT	This paper proposes a noise removal algorithm to improve the quality of images captured by IR thermal cameras. Due to atmospheric disturbances and the temperature reflected from the surrounding objects, IR images are generally noisy. For removal of noise in IR thermal images the proposed algorithm has two stages. In the first stage, a support vector machine incorporated with fuzzy logic (FSVM) is used to obtain noise pixels from the thermal image and in the second stage an adaptive directional weighted median filter (ADWMF) is applied to restore noisy pixels. Our proposed algorithm is compared with another fuzzy-logic based noise classification technique, namely the adaptive fuzzy inference based directional median filter (AFIDMF). Simulation results indicate that the performance of the proposed filter is better, compared to AFIDMF based on the measures such as peak signal to noise ratio (PSNR) and structural similarity index (SSIM).																	1064-1246	1875-8967					2020	38	4			SI		4273	4286		10.3233/JIFS-190860													
J								Crime rate detection using social media of different crime locations and Twitter part-of-speech tagger with Brown clustering	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Crime detection; Twitter data; social media; sentiment analysis; Brown clustering		Nowadays, the crime rate increases dramatically in every country. Therefore, it is an urgent need for governments and social associations to produce persistent solutions and disincentive penalties to prevent crime issues. Specifically, social media plays an important role in crime rate detection; thus, reducing crime rates significantly. It would be a good medium for the desired task. In this paper, we analyze Twitter data collected from Twitter accounts for seven different locations (Ghaziabad, Chennai, Bangaluru, Chandigarh, Jammu, Gujarat, and Hyderabad) from January 2014 to November 2018 in a case study of India, which is opted to illustrate the efficiency of the proposed work. Sentiment analysis has been used to analyze users' behavior and psychology through the tweets of people to track crime actions. Twitter part-of-speech tagger, which is a Markov Model of first-order entropy, has been used for part-of-speech in online conversational text. Brown clustering is used for a long set of unlabeled tweets. Comparisons are verified with real crime rates from an authorized source of information according to different locations. We also measure the latest crime trends for the highest (Ghaziabad, Uttar Pradesh) and lowest crime cities (Jammu) in India. It has been found that the latest crime trends have been recorded for the time duration of 7 days (23, January 2019 to 30, January 2019). The analyses demonstrate that the obtained results match with the real crime rate data. We believe that these types of studies will help to detect the real-time crime rate for different locations and detect the crime pattern easily.																	1064-1246	1875-8967					2020	38	4			SI		4287	4299		10.3233/JIFS-190870													
J								Determination of the optimal production plan by using fuzzy AHP and fuzzy linear programming	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Production plan; fuzzy set; fuzzy analytic hierarchy process; fuzzy linear programming	GROUP DECISION-MAKING; SUPPLY CHAIN; RANKING; MODEL	In this paper, we consider the problem of determining the optimal production quantity under uncertainties. The treated problem has a critical effect on the achievement of business goals, which further propagates to the competitive advantage of manufacturing company. The considered problem is stated as the fuzzy linear programming task. The objective function is defined as the maximization of all profit over the time period. The set of constraints consists of constraints deriving from the enterprise (available capacities) and from the market (demand). The fuzzy rating coefficient values of objective function of each pair of the considered products are described by linguistic expressions which are modelled by triangular fuzzy numbers. Handling of uncertainties of the stated fuzzy pair-wise comparison matrix is performed by using the extent analysis. The right-hand-side values of the capacity constraints are determined by decision makers and modelled by the triangular fuzzy numbers. The market constraint values are given according to evidence data. Determination of the optimal quantities of considered products for each time period is based on the concept of equal possibilities. The proposed model is illustrated by an example with real-life data.																	1064-1246	1875-8967					2020	38	4			SI		4315	4325		10.3233/JIFS-190913													
J								Using the fuzzy weighted association rule mining approach to develop a customer satisfaction product form	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Customer satisfaction; Kansei engineering; Fuzzy Delphi method; Fuzzy weighted association rule mining; Exercise bike	DELPHI METHOD; AFFECTIVE RESPONSES; KANO MODEL; DESIGN; SYSTEM; QUALITY; CLASSIFIER; TECHNOLOGY; MANAGEMENT; KNOWLEDGE	Customer satisfaction is an important indicator of user preferences for products which directly influence customers' purchase intentions. It is also an essential Kansei factor for enterprises to successfully develop products. Therefore, the objective of this study is to apply the fuzzy weighted association rule mining (FWARM) approach to extract the significant association between customer satisfaction and product form features, thus providing specific parameter guidelines for the enterprise's business decisions. In previous research, the fuzzy association rule mining (FARM) approach has proved to be a promising way forwards. However, the absence of consideration of the weight of items is always criticized as creating an uninteresting rule with high frequency and low importance. Therefore, in this study the fuzzy Delphi method (FDM) is used to calculate the weight of each item in the early stage of data mining, and filters out items with a high degree of consensus. Then, the FARM method extracts the fuzzy weighted association rule. Taking the household exercise bike as an example, the authors find that handlebar, LCD screen, rack, main outline and pedestal are vital item features. Subsequently, the method is used to identify 14 rules to inform the development of the exercise bike's form to achieve high customer satisfaction; valuable knowledge support is provided for manufacturers and designers in the initial stage of new product development, potentially improving customer satisfaction and reducing the risk of product failure.																	1064-1246	1875-8967					2020	38	4			SI		4343	4357		10.3233/JIFS-190957													
J								Decision making based on interval-valued complex single-valued neutrosophic hesitant fuzzy generalized hybrid weighted averaging operators	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Complex neutrosophic set; interval complex neutrosophic set; hesitant fuzzy set; interval-valued complex single-valued neutrosophic hesitant fuzzy set	AGGREGATION OPERATORS; SIMILARITY MEASURES; INFORMATION AGGREGATION; SETS; ENTROPY; NUMBERS	The interval complex single-valued neutrosophic fuzzy set (ICSVNFS) and hesitant fuzzy set (HFS) are two important different generalizations of the fuzzy set (FS) to cope with unreliable and unpredictable information in the real world. The ICSVNHFS is characterized by interval complex-valued membership, interval complex-valued abstinence, and interval complex-valued non-membership grades, whose ranges are restricted to a unit disc in a complex plane instead of real numbers. In this paper, the notions of interval-valued complex single-valued neutrosophic hesitant fuzzy sets (IVCSVNHFSs) are initiated and also their operational laws are described with examples. Further, based on IVCSVNHFSs, we develop the notions of the interval-valued complex single-valued neutrosophic fuzzy generalized weighted (IVCSVNHFGW) operator, interval-valued complex single-valued neutrosophic fuzzy generalized ordered weighted (IVCSVNHFGOW) operator, and interval-valued complex single-valued neutrosophic fuzzy generalized hybrid weighted (IVCSVNHFGHW) operator to cope with decision information and also study their properties. Further, a new multi-attribute group decision making (MAGDM) problem is initiated based on the proposed operators. Finally, we provide some numerical examples to illustrate the reliability and superiority of the proposed methods by comparison with other existing methods.																	1064-1246	1875-8967					2020	38	4			SI		4359	4401		10.3233/JIFS-191005													
J								A private entity matching approach for multiple databases	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Entity matching; privacy; numerical records; multiple databases	SECURITY	Matching similar records from different databases to prevent duplication in a private manner has attracted plenty of attention, which is referred to as Private Entity Matching (PEM). In spite of various approaches having been proposed to solve this problem, private linking numerical data such as integer (e.g. age), floating point (e.g. body mass index) from multiple databases is an urgent gap, which is commonly required in health domain, statistical departments and more. Hence, this paper targets at solving the problem of linking numerical data from three or more sources in an efficient and secure way. Firstly, we introduce a novel homomorphic encryption method constrained similar modul, which provides strong privacy to encrypt numerical data in the range of real numbers. Then, to avoid frequent decryptions in the homomorphic encryption schema, we draw an inference about the encryption keys. Finally, an accelerated algorithm is proposed to reduce the complexity of multi-party numerical records matching. Our approach is considered absolute safety that no party learns any sensitive information of the others in the absence of collusion. Experiments on two real-world health information databases of patient records validate our approach with regards to the efficiency improvement and at the same time, at no the sacrifice of linkage quality.																	1064-1246	1875-8967					2020	38	4			SI		4403	4414		10.3233/JIFS-191064													
J								Fuzzy weak hyper deductive systems of hyper equality algebras and their measures	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Hyper equality algebra; fuzzy weak hyper deductive system; R-implication; fuzzy weak hyper deductive system degree	FILTERS	The concepts of fuzzy weak hyper deductive systems and fuzzy sup-weak hyper deductive systems of hyper equality algebras as generalizations of fuzzy deductive systems are initiated in the paper. Moreover, several related properties and characterizations of fuzzy weak hyper deductive systems and fuzzy sup-weak hyper deductive systems are investigated, and the relationships between fuzzy sup-weak hyper deductive systems of homomorphic hyper equality algebras are also presented based on the introduced homomorphism in hyper equality algebras. It is known that a fuzzy weak hyper deductive system is a fuzzy set, while the converse is not true in general. In order to measure the degree of a fuzzy set becoming a fuzzy weak hyper deductive system, we introduce the notion of fuzzy weak hyper deductive system degrees, and put forward some equivalent characterizations of the fuzzy weak hyper deductive system.																	1064-1246	1875-8967					2020	38	4			SI		4415	4429		10.3233/JIFS-191112													
J								Some theoretical results on the stability of uncertain pantograph differential equations	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Uncertain pantograph differential equation; stability in measure; stability in pth moment; uncertainty theory	SURE STABILITY; TH MOMENT; CONVERGENCE; THEOREMS; MODEL	Uncertain pantograph differential equation is a new type of differential equations in uncertainty theory. In this paper, we first give the concept of uncertain pantograph differential equation. In addition, this paper aims at the stability of this type of differential equations. The concepts of stability in measure and stability in pth moment for uncertain pantograph differential equations are presented and the sufficient conditions for uncertain pantograph differential equations being stable in measure and in pth moment are derived. Finally, this paper also discusses the relationship between these two types of stability.																	1064-1246	1875-8967					2020	38	4			SI		4431	4439		10.3233/JIFS-191148													
J								On distributive laws of overlap and grouping functions over uninorms	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Overlap functions; grouping functions; uninorms; distributive laws		Recently, Qiao [8] investigated the distributive laws of uninorms over overlap and grouping functions when the uninorms were one of the usual classes (e.g., U-min, U-max, the family of idempotent uninorms, representable uninorms or uninorms continuous on inverted left perpendicular 0, 1 inverted right perpendicular(2)) and obtained some characterizations. As an addendum to [8], in this paper, we investigate the distributive laws of overlap and grouping functions over uninorms. In addition, we show some characterizations when the uninorm U belongs to one certain class.																	1064-1246	1875-8967					2020	38	4			SI		4441	4446		10.3233/JIFS-191168													
J								Grey relational bidirectional projection method based on trapezoidal type-2 intuitionistic fuzzy numbers	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Trapezoidal type-2 intuitionistic fuzzy numbers; Arithmetic operations; Score function; Hausdorff distance; Grey relational bidirectional projection method	MULTICRITERIA DECISION-MAKING; FAULT-TREE ANALYSIS; SETS; OPERATORS; ENTROPY; SYSTEMS	Intuitionistic fuzzy number, which is an extension of fuzzy number, has been applied in many fields as it considers membership degree and non-membership degree. However, in some circumstances, intuitionistic fuzzy number does not express uncertainty and vagueness well. In order to deal with this problem, a new concept of trapezoidal type-2 intuitionistic fuzzy number(TrT2IFN) is proposed in this paper. Meanwhile, the arithmetical operations of TrT2IFNs are defined. Then, a novel distance measures are proposed by taking advantage of the Hausdorff distance. Additionally, the multi-attributes decision making problem of the TrT2IFN is solved by the grey relational bidirectional projection method. Finally, the applicability and availability of the proposed method are demonstrated by a numerical example, and the final ranking outcome of alternatives is obtained. This paper provides an effective solution for solving multi-attribute decision making in TrT2IFN environment.																	1064-1246	1875-8967					2020	38	4			SI		4447	4457		10.3233/JIFS-191174													
J								A new ranking method for TOPSIS and VIKOR under interval valued intuitionistic fuzzy sets and possibility measures	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Interval-valued intuitionistic fuzzy sets; possibility measure; multi-criteria decision making; necessity measure; TOPSIS; VIKOR	GROUP DECISION-MAKING; SIMILARITY; SELECTION; ENTROPY	In this work we propose an approach of multi-criteria decision making (MCDM) using TOPSIS and VIKOR methods under interval-valued intuitionistic fuzzy (IVIF) sets and possibility theory. We are interested to study positive and negative ideal solutions to propose new formulas. Indeed, these solutions are presented with many formulas in literature [1, 2] which could cause ambiguity [3]. Due to the importance of possibility theory in resolution of many problems, we propose to use possibility measure for positive ideal solution and necessity measure for negative ideal solution under interval valued intuitionistic fuzzy sets. According to this, TOPSIS and VIKOR are modified to obtain new approaches. The latter are applied to an example from literature using IVIF data. This example permits to assess the investment projects problem for ranking different projects. The found results showed different solutions from that existing in literature which can give more choice to decision makers with additional information due to use of possibility measures.																	1064-1246	1875-8967					2020	38	4			SI		4459	4469		10.3233/JIFS-191223													
J								User and item profile expansion for dealing with cold start problem	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Recommender system; hybrid recommender system; ontology; profile expansion; KNN	RECOMMENDER SYSTEMS; ITERATIVE FUSION; ENSEMBLE; ALLEVIATE; RETRIEVAL; DIVERSITY; QUALITY	Recommender Systems (RS) are expected to suggest the accurate goods to the consumers. Cold start is the most important challenge for RSs. Recent hybrid RSs combine ConF and ColF. We introduce an ontological hybrid RS where the ontology has been employed in its ConF part while improving the ontology structure by its ColF part. In this paper, a new hybrid approach is proposed based on the combination of demographic similarity and cosine similarity between users in order to solve the cold start problem of new user type. Also, a new approach is proposed based on the combination of ontological similarity and cosine similarity between items in order to solve the cold start problem of new item type. The main idea of the proposed method is to expand user/item profiles based on different strategies to build higher-performing profiles for users/items. The proposed method has been evaluated on a real dataset and the experimentations indicate the proposed method has the better performance comparing with the state of the art RS methods, especially in the case of the cold start.																	1064-1246	1875-8967					2020	38	4			SI		4471	4483		10.3233/JIFS-191225													
J								Quadrotor stabilization by Fuzzy Kalman Filter	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Kalman filtering; quadrotor; fuzzy systems stabilization; control systems		In this work, state vector estimation by means of the Fuzzy Kalman Filter (FKF) is used to generate a control signal that stabilizes an unmanned quadrotor aircraft. The framework for fuzzy Kalman Filter methodology has been successfully developed, and in this sense, the FKF is implemented and compared with Kalman Filter (KF) and extended Kalman Filter (EKF). It will be proved that the fuzzy version gives some advantages such as a smaller processing time and a smaller Mean Squared Error (MSE). Finally, these results are shown in graphics and tables.																	1064-1246	1875-8967					2020	38	4			SI		4485	4494		10.3233/JIFS-191251													
J								Spatial possibilistic fuzzy C-Mean segmentation method integrated with brain Mid-Sagittal Surface information extracted by an evolutionary algorithm	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										3D brain MR segmentation; Mid-Sagittal Surface; Fuzzy C-Mean; genetic algorithm; fractal dimension; possibilistic information; spatial information	IMAGE SEGMENTATION; CLUSTERING-ALGORITHM; SYMMETRY; PLANE	A normal human brain holds a high level of bilateral reflection symmetry. On the sagittal view, the brain can be separated into the left and the right hemispheres with approximately identical anatomical properties, so that symmetrical mirror pixels are almost similar. As a result, the symmetry information can be used to enhance results of brain segmentation methods. In this paper, I introduced a new version of the Fuzzy C-Mean (FCM) segmentation method which is called Genetic Spatial Possibilistic Fuzzy C-Mean (GSPFCM). GSPFCM integrates symmetry information with SPFCM. It is an extension of Possibilistic Fuzzy C-Mean (PFCM) on 3D Magnetic Resonance (MR) images. GSPFCM uses the spatial information and fuzzy membership values. Spatial and possibilistic information were added in order to solve the noise sensibility defect of FCM. To integrate the symmetry information, I first extracted the Mid-Sagittal Surface using a proposed genetic algorithm. According to this algorithm, inside each axial slice, a Thin-Plate Spline (TPS) surface was constructed and a genetic algorithm was applied to fit this TPS surface to the brain data. Then, the symmetry degree of each symmetry pair voxels was calculated. Finally, the membership values in SPFCM were updated based on the corresponding symmetrical values. The efficiency of GSPFCM, was evaluated using both simulated and real Magnetic Resonance Images (MRI), and was compared to the state-of-the-art methods. My results showed images with different degrees of Intensity Non-Uniformity (INU) and different levels of noise were segmented efficiently by the GSPFCM.																	1064-1246	1875-8967					2020	38	4			SI		4495	4510		10.3233/JIFS-191258													
J								Iris center localization using energy map synthesis based on gradient and isophote	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Iris center localization; energy map synthesis; gradient; isophote; post-processing correction	EYE CENTER LOCATION; GAZE TRACKING	Gaze tracking has wide applications such as in driver fatigue detection, virtual reality, and human-computer interaction. The performance of gaze tracking depends largely on the accuracy of iris center localization. However, most of the existing gaze tracking products are intrusive or require additional equipment with a high cost. Therefore, precise localization methods of iris center in low quality images captured in a non-contact way with visible light need to be investigated. This paper proposes a novel localization method of iris center using energy map synthesis based on image gradient, isophote and midpoint of eye ROI (Region of interest). This method combines the advantages of higher localization accuracy based on gradient, invariance to the rotation and linear transformation of light based on isophote, and iris center close to the midpoint of eye ROI. Moreover, a post-processing correction method for the closed eyes and for other large deviations of iris center position is adopted to further improve the localization accuracy. The algorithm is verified on the BioID, Talking Face Video and MUCT Face databases, and the results show that the localization accuracy in the paper has outperformed the listed state-of-the-art methods in varying illuminations.																	1064-1246	1875-8967					2020	38	4			SI		4511	4523		10.3233/JIFS-191281													
J								A novel privacy-preserving matrix factorization recommendation system based on random perturbation	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Recommender system; matrix factorization; privacy protection; random perturbation; sparse data		With the popularity of networks and the increasing number of online users, recommender systems have suffered from the privacy leakage of sensitive information. While people enjoy recommender services, their information is exposed to the networks. To protect the privacy of users when using the recommender services, we propose a multi-level combined privacy-preserving model that maintains high accuracy of recommendation with privacy protection and alleviates the data sparsity problem. Our scheme contains two steps of recommendation. First, a multi-level combined random perturbation (MCRP) model is proposed on the client side. Our model dynamically divides multiple disturbance levels and adds noise of different ranges to the rating matrix according to Gaussian and uniform mixed disturbances. Second, on the server side, we propose a pseudo rating prediction filling (PRPF) algorithm based on the matrix factorization model. Combining the PRPF algorithm with the MCRP method significantly improves the recommender accuracy and effectively increases privacy security. Sensitive analysis and comparison experiments show that the proposed privacy method has certain advantages in security and recommender accuracy by using three publicly available datasets.																	1064-1246	1875-8967					2020	38	4			SI		4525	4535		10.3233/JIFS-191287													
J								Fuzzy regression model of goal difference of the Korean National Football Team based on ELO rating and dividend	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										ELO rating; dividend; path analysis; fuzzy partition; regression analysis	ODDS	Soccer recently has become an intellectual game by predicting the outcome of games. In this study, we use path analysis to find the variable that affects the outcome of the Korea National Football Team (KNFT) matches the most, and consequently the odds of victory, defeat, or a draw, as announced by the betting company. We will also investigate the influence of the variables inferred from the path analysis and Korea's ELO Rating on the difference between scoring and losing points of the KNFT. We will represent the dividend and the difference between scoring and losing points as fuzzy numbers using the fuzzy decomposition, and then infer the fuzzy regression model for the result of the KNFT's match. For this purpose, we use data on 113 games of the KNFT from September 2011 to June 2019 and the dividend rate of the KNFT obtained from Wise Toto company.																	1064-1246	1875-8967					2020	38	4			SI		4537	4543		10.3233/JIFS-191288													
J								An inventory model for linearly time-dependent deteriorating rate and time-varying demand with shortages partially backlogged	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Inventory; power demand pattern; deterioration; linearly; shortages	EOQ MODEL	In this paper, a deterministic inventory model for deteriorating items with linear deterioration rate is proposed. Demand follows a power pattern. Shortages are permitted and partially backlogged. An optimal solution is derived to minimizes the total average cost. Numerical examples are given, and sensitivity analysis carried out to show how the optimal decisions are affected by changes in different parameters in the model. Graphical representation of the convexity of the total cost against the decision variables shows the efficiency and reliability of the model.																	1064-1246	1875-8967					2020	38	4			SI		4545	4557		10.3233/JIFS-191323													
J								A novel internal cluster validity index	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Cluster validity index; number of clusters; affinity propagation; hierarchical clustering	VALIDATION	It is critical to determine the optimal number of clusters (NC) in cluster analysis. Many cluster validity indices have been proposed, such as the Silhouette index and In-group proportion index. However, these validity indices have more time complexity. From the viewpoint of sample geometry, a new internal cluster validity index for determining the optimal NC is proposed. The new index can evaluate the clustering quality of a certain clustering algorithm and determine the optimal NC for many kinds of data sets, including synthetic data sets, benchmark data sets, and real data sets. Compared with many well-known validity indices, the proposed index is more effective and efficient. Theoretical analysis and experimental results show the effectiveness and high efficiency of the new index.																	1064-1246	1875-8967					2020	38	4			SI		4559	4571		10.3233/JIFS-191361													
J								Improved African buffalo optimization algorithm for the green flexible job shop scheduling problem considering energy consumption	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Green flexible job shop; improved African buffalo optimization; modified individual learning mechanism; aging-based re-initialization mechanism; discrete individual updating method	SINGLE-MACHINE; GENETIC ALGORITHM; TIME; TARDINESS; MAKESPAN	The conventional production scheduling problem has mainly emphasized the time-related metrics, such as makespan, machine workload and tardiness/earliness, and so on. With the advent of the sustainable manufacturing, the green scheduling problem has been received more and more attention from scholars and researchers. In this paper, we investigate a green flexible job shop scheduling problem (GFJSP) with the consideration of environmental factors. To formulate the GFJSP problem, a mathematical model is first established to minimize the amount of total energy-consumption. To solve the model, a kind of improved African buffalo optimization (IABO) algorithm is proposed based on the characteristics of the problem. In the proposed IABO, a two-vector solution representation method is first designed, and a population initialization method is adopted to generate the initial solutions with certain quality and diversity. Based on the original ABO, several improvement strategies are introduced to enhance the performance of the algorithm, i.e., the modified individual learning mechanism and the aging-based re-initializaiton mechanism. In addition, in order to adapt our algorithm to the scheduling problem, a discrete individual updating method is developed to ensure the algorithm search directly in a discrete domain. Finally, a number of experiments have been conducted to test the performance of the proposed IABO algorithm. The simulation data demonstrate the effectiveness of the proposed IABO for the considered GFJSP.																	1064-1246	1875-8967					2020	38	4			SI		4573	4589		10.3233/JIFS-191370													
J								Accident risk prediction and avoidance in intelligent semi-autonomous vehicles based on road safety data and driver biological behaviours(1)	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Internet of things; cyber-physical systems; real-time systems; virtualisation; virtual object	AUTOMATION	Autonomous vehicles technology is an emerging area and has attracted lots of recognition in recent times. Accidents-free driving has always been the focal point of autonomous vehicles. Autonomous vehicles have the potential to eliminate human errors while driving, which has been argued as the predominant cause of traffic accidents. In autonomous vehicles technologies, a variety of efforts have been made to eliminate human drivers. The full elimination of humans is not possible at this moment, but some of the tasks can be automated to facilitate the drivers. In this paper, we investigate the leading causes of accidents based on UK vehicle safety data of 2017-2018. We analyze the data and investigate the leading factors which cause traffic crashes. Based on the leading features in the dataset, we then run different prediction algorithms to predict the severity of accidents under a given input feature set. The accuracy of the model with Decision Tree classifier, Random Forest, and Logistic Regression are compared, and it has been found that Random Forest performs best among others with 95% accuracy. The trained random forest model is deployed on the Internet of Things server based on Arduino, and a lightweight application is developed to get the vital data from the driver. The data is applied to the trained model to predict the risk index of driving. This application is lightweight but yet provide a significant contribution in terms of safety in autonomous vehicles.																	1064-1246	1875-8967					2020	38	4			SI		4591	4601		10.3233/JIFS-191375													
J								Seated pregnant subject biodynamics response enhancement against road irregularities using adaptive NeuroFuzzy control	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Adaptive NeuroFuzzy control; integrated vehicle-pregnant subject; seated pregnant subject; vibration attenuation; whole body	BACKSTEPPING CONTROL; BIOMECHANICAL MODEL; VIBRATION; VEHICLE; SYSTEMS; BODY	Human-beings in general and pregnant subjects (characterized by a sophisticated dynamic system) in particular are sensitive to whole-body vibrations (WBV) in sitting position while driving. The published literature is thickly populated with the investigation of WBV effects on non-pregnant subjects but sparsely populated with the biodynamic response enhancement of the pregnant subjects in driving conditions. In this paper, the biodynamic response analysis of eleven degrees of freedom (11-DoF) seated pregnant subject at the driver position is carried out while being exposed to vibrations induced by inherent road irregularities. An advanced adaptive NeuroFuzzy (AdaptNeuroFuzzy) control strategy for active suspensions of nineteen degrees of freedom (19-DoF) integrated vehicle-pregnant subject model is designed to attenuate harmful vibrations and protect the seated pregnant subject and fetus against the risk of damages. Matlab/simulink is employed to carry out simulations. Performance validation of the proposed advanced intelligent control strategy based suspension system is accomplished through comparison with the passive, PID, adaptive PID (AdapPID) and adaptive fuzzy logic control (AdapFLC) via standard performance indices, using an ISO-classified standard random road profile.																	1064-1246	1875-8967					2020	38	4			SI		4603	4617		10.3233/JIFS-191376													
J								An MINLP model for network layout of underground natural gas storage	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Layout; pipeline network; GA; underground natural gas storage; MINLP	OIL-FIELD; GENETIC ALGORITHMS; OPTIMAL-DESIGN; OPTIMIZATION; PRESSURE; COST	Underground natural gas storage (UNGS), usually regarded as one of the most important gas storing and peak shaving method today, has been widely used in various parts of the world. The pipeline gathering system plays a key role in UNGS surface engineering. Thus, optimization of the whole system is crucial to lower the total investment. However, we cannot find that any scholars have published related papers on the gathering pipeline network for UNGS at present. This paper focuses on the two-level star gas field gathering pipeline network construction, establishes a mixed integer nonlinear programming (MINLP) model with considering the injection and withdrawal process of UNGS. Minimizing pipeline network investment is the object of this model. Constraints of connection mode, platform, pipe length, flow rate, node pressure, pipe diameter are also taken into consideration in this model. A special genetic algorithm is proposed to figure out the optimal topological structure, location of platform and central station, pipe diameter, gas velocity along each pipe of this model. Last, two typical real cases are taken to test the applicability of the proposed model and the accuracy of the special GA. The optimal results indicate the mathematical model can lower the total investment and the corresponding GA can solve it efficiently.																	1064-1246	1875-8967					2020	38	4			SI		4619	4642		10.3233/JIFS-191383													
J								Linguistic interval-valued intuitionistic fuzzy Archimedean prioritised aggregation operators for multi-criteria decision making	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Prioritised and operator; prioritised or operator; linguistic interval-valued intuitionistic fuzzy set; Archimedean t-norm and t-conorm; multi-criteria decision making	PROGRAMMING METHODOLOGY; TODIM METHOD; NUMBERS; SETS	Two key steps in multi-criteria decision making (MCDM) are to quantify the considered criteria and to fuse the quantified criterion information to sort all alternatives. One of the most recent and important tools for the first step is linguistic interval-valued intuitionistic fuzzy number (LIVIFN) and one of the most common and effective ways for the second step is aggregation operator (AO). So far, a number of AOs of LIVIFNs have been presented. Each AO can work well under certain conditions. But there is not yet an AO of LIVIFNs that can deal with the situation where the weights of the considered criteria are unknown and the criteria are in different priority levels and concurrently provide satisfying generality and flexibility in the aggregation of criterion information. To this end, a linguistic interval-valued intuitionistic fuzzy Archimedean prioritised and (LIVIFAPA) operator and a linguistic interval-valued intuitionistic fuzzy Archimedean prioritised or (LIVIFAPO) operator, which have such capabilities, are presented in this paper. The formal definitions and generalised expressions of the two AOs are firstly provided. Then their properties are explored and proved and specific expressions are established. After that, a new method for solving the LIVIFNs based MCDM problems is proposed on the basis of the presented AOs. Finally, the proposed method is illustrated via an example about additive manufacturing machine selection and is evaluated via a comparison with existing methods. The major contribution of the paper is the development of the LIVIFAPA and LIVIFAPO operators for MCDM, which can make up for the above shortcoming of the existing AOs of LIVIFNs.																	1064-1246	1875-8967					2020	38	4			SI		4643	4666		10.3233/JIFS-191385													
J								A note on L-fuzzy up-sets by using closure operator	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Complete lattice; L-fuzzy up-set; closure operator; cut	CUT SETS	This paper deals with L-fuzzy up-sets by using terminologies of closure operators. It first gives a condition that a family of some subsets of a nonempty set can be represented by L-fuzzy up-sets, and it finally discusses the L-fuzzy sets on quotient sets under closure operators.																	1064-1246	1875-8967					2020	38	4			SI		4667	4673		10.3233/JIFS-191388													
J								A novel risk assessment model based on failure mode and effect analysis and probabilistic linguistic ELECTRE II method	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS											MULTICRITERIA DECISION-MAKING; TERM SETS; VIKOR METHOD; OPERATORS; DISTANCE; SYSTEM; TOPSIS																		1064-1246	1875-8967					2020	38	4			SI		4675	4691		10.3233/JIFS-191398													
J								Location selection for logistics center with fuzzy SWARA and CoCoSo methods	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										CoCoSo; fuzzy SWARA; GIS; logistics center; location selection; MCDM	MULTIPLE CRITERIA; SUPPLY CHAIN; DECISION; TOPSIS; VIKOR; AHP	Logistics centers are home to many and varied facilities, such as storage, transportation of goods, handling, reassembling, clearing, disassembling, quality control, social services and providing accommodation, so on. Providing logistical activities from one location can provide some macro advantages, as well as regional development in developing countries. For the micro level, logistics center selection has an effective role in increasing the operational efficiency and decreasing the costs of the firms. While the wrong location selection for logistics center affects the operations and costs of the companies negatively, the optimal location selection increases the performance, competitiveness, profitability of the firms and reduces the costs of the firms. Since many different qualitative and quantitative criteria are considered in the selection of the logistics center, this selection problem is an MCDM problem. A new integrated MCDM model is proposed to solve this problem for Sivas province in Turkey. This study presents two contributions to the literature. Firstly, the number of studies related to CoCoSo method is limited in the literature, therefore, the CoCoSo method is proposed in this study. Secondly, a new integrated GIS-based MCDM model comprising fuzzy SWARA and CoCoSo is introduced to literature to address the location selection problem for a logistics center. In this study, the results of CoCoSo method and the resulfts of other MCDM methods (COPRAS, VIKOR, ARAS, MOORA, and MABAC) are compared to test the accuracy of results obtained by CoCoSo. Besides, the criteria weights are changed and the possible changes in the results are tracked.																	1064-1246	1875-8967					2020	38	4			SI		4693	4709		10.3233/JIFS-191400													
J								The fuzzy inference approach to solve multi-objective constrained shortest path problem	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Constrained shortest path problem; multi-objective optimization; fuzzy inference system	SEARCH ALGORITHM; NETWORK; OPTIMIZATION	The multi-objective constrained shortest path problem is one of the most significant and well-known problems in the field of network optimization which due to its many applications in routing, telecommunication, transportation, scheduling, etc., has attracted the attention of many researchers. In this paper, the mathematical model of the constrained shortest path problem with three objectives of cost, time and risk is formulated, where the constraint is on the path length. The aim is to find the most desirable path to move commodities from origin to destination based on three factors of cost, time, and risk which the length of path does not exceed a predetermined value. The approach proposed for solving the problem under investigation is to use fuzzy inference system which finds optimal solution in comparison to linear programming and genetic algorithm approaches in less time. The proposed algorithm is implemented on a network of 27 nodes and 52 arcs. The implementation results of the proposed algorithm show that it is capable of finding the optimal solution.																	1064-1246	1875-8967					2020	38	4			SI		4711	4720		10.3233/JIFS-191413													
J								Probabilistic linguistic GRA method for multiple attribute group decision making	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Multiple attribute group decision making; probabilistic linguistic term sets (PLTSs); GRA method; CRITIC method; site selection; electric vehicle charging stations	GREY RELATIONAL ANALYSIS; PYTHAGOREAN FUZZY-SET; AGGREGATION OPERATORS; MEAN OPERATORS; REPRESENTATION MODEL; PREFERENCE RELATIONS; RISK-ASSESSMENT; TODIM METHOD; TERM SETS; INFORMATION	In practical multiple attribute group decision making (MAGDM) issues, uncertain and fuzzy cognitive decision information is well-depicted by linguistic term sets (LTSs). These LTSs are easily shifted into probabilistic linguistic sets (PLTSs). In such paper, a grey relational analysis (GRA) method is investigated to tackle probabilistic linguistic MAGDM with completely unknown attribute weights. Firstly, the definition of score function is then employed to objectively obtain the attribute weights based on the CRITIC method. Then, the optimal alternative is chosen through calculating largest relative relational degree from the probabilistic linguistic positive ideal solution (PLPIS) which considers both the largest grey relational coefficient from the PLPIS and the smallest grey relational coefficient form probabilistic linguistic negative ideal solution (PLNIS). This proposed method extends the applications range of the classical GRA method. Finally, a numerical case for site selection of electric vehicle charging stations (EVCS) is employed to illustrate the proposed method. The effectiveness of the proposed method is also verified by some comparative studies.																	1064-1246	1875-8967					2020	38	4			SI		4721	4732		10.3233/JIFS-191416													
J								Stability of a more general cubic functional equation in Felbin's type fuzzy normed linear spaces	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fixed point method; Felbin's type fuzzy normed linear space; fuzzy real number; generalized Hyers-Ulam stability	ULAM-RASSIAS STABILITY; COMPLETION	Let k >= 2 be an integer. The purpose of this paper is first to introduce the notation of Felbin's type fuzzy normed linear spaces, and then by virtue of this notation to study some stability results concerning the more general cubic functional equation of the form f (x + ky) + f (x - ky) + f (kx) = k(2) f (x + y) + k(2) f (x - y) + (k(3) - 2k(2) + 2) f (x) in the setting of Felbin's type fuzzy normed linear spaces by employing the direct and fixed point methods. Then some applications of our results for the stability of the cubic functional equation from a real normed space to a Banach space will be demonstrated. Furthermore, the interdisciplinary relation between the theory of Felbin's type fuzzy spaces and the theory of functional equations are also presented in this paper.																	1064-1246	1875-8967					2020	38	4			SI		4733	4742		10.3233/JIFS-191418													
J								Design of S-control chart for neutrosophic data: An application to manufacturing industry	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Quality control; fuzzy charts; neutrosophic measures; S-control chart	FUZZY PROCESS-CONTROL; CONSTRUCTION; BAR	The Shewart S-control chart is commonly used as one of the statistical tools for monitoring the process variability. The existing design of S-control is based on the assumption that inspected quality of the observed process is, an exact and clearly specified quantitative quantity. If however, measured data involve some vague and imprecise observations, the conventional approach of the S-control, cannot be practiced. Designing of a generalized neutrosophic S-control chart which could support the indeterminate values in the processing data is originally developed in this article. The associated properties of proposed design under neutrosophic environment have been established in this study. The proposed chart represents a general design of the existence structure of the S-chart. Using neutrosophic average run length (ARL(n)) as a performance measure, a comparative study of the proposed chart with the conventional approach of S-control under vague parameter values is evaluated. Findings both from analytical and simulation studies indicate that proposed design of S-control leads to efficient and more flexible approach over the traditional S-control. A real data example has been provided for demonstrating the implementation procedure of the proposed design.																	1064-1246	1875-8967					2020	38	4			SI		4743	4751		10.3233/JIFS-191439													
J								Energy demand forecasting of buildings using random neural networks	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Buildings; energy demand; random neural network; prediction	PREDICTION	Energy uncertainty and ecological pressures have contributed to a high volatility in energy demand and consumption. The building sector accounts for 30 to 40% of the total global energy consumption. There is a high demand for novel techniques and viable energy strategies for reducing energy consumption in this domain. Energy prediction models have the potential to play a pivotal role in optimising energy consumption. The proposed work presents a new and accurate Energy Demand Prediction (EDP) model for large buildings. This approach leverages the Random Neural Network (RNN) prediction methodology. The proposed RNN-based EDP is compared with traditional Artificial Neural Network (ANN), Support Vector Machine (SVM) and linear regression models. A large building is modelled and simulated for one year in the Integrated Environment Solutions Virtual Environment (IES-VE). Several data inputs such as air temperature, internal gain and the number of people (occupancy) are calculated from IES-VE model and provided to traditional ANN and the proposed RNN predictor. A number of test parameters such as Root Mean Square (RMSE), Normalized Root Mean Square (N-RMSE), Mean Absolute Percentage Error (MAPE) and R provide the proposed RNN model with higher accuracy over the traditional ANN, SVM and linear regression. The proposed RNN predictor provides approximately half of the error of the ANN model. The traditional ANN model gives higher error values of 2.07x, 1.83x and 2.35x for RMSE, NRMSE and MAPE, respectively as compared to the proposed RNN model. Furthermore, the error values of SVM and linear regression were also higher than the proposed EDP scheme.																	1064-1246	1875-8967					2020	38	4			SI		4753	4765		10.3233/JIFS-191458													
J								Hyers-Ulam-Rassias Fuzzy Stability of Bi-additive theta-random operator inequalities: A fixed point technique	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Hyers-Ulam-Rassias stability; bi-additive theta-random operator inequality; fuzzy sets	CAUCHY FUNCTIONAL-EQUATION; SPACES; THEOREM	We attempt to solve some bi-additive theta-random operator inequalities and use the fixed point technique to prove the fuzzy version of Hyers-Ulam-Rassias stability of them.																	1064-1246	1875-8967					2020	38	4			SI		4767	4777		10.3233/JIFS-191482													
J								Research on bilateral matching decision method considering attribute association in heterogeneous information environment	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Bilateral matching decision; attribute association; heterogeneous information; prospect theory	STABLE MARRIAGE; COLLEGE ADMISSIONS; PROSPECT-THEORY; STABILITY	One of the critical activities for bilateral matching decision is matching accuracy, which may be regarded as a type of bilateral matching decision problem with heterogeneous information and attribute association. This paper aims to develop a new fuzzy linear programming method to address such problems. In the proposed method, the multiple attributes are expressed as exact numbers, interval numbers, triangular fuzzy numbers, intuitionistic fuzzy numbers, linguistic terms, and neutrosophic numbers. Firstly, the distance of heterogeneous data and fuzzy measures are introduced; meanwhile, heterogeneous information attribute weights are calculated based on the Choquet integral. Then based on the psychological characteristics of matching participants' loss avoidance and superiority maximization, the lexicographical method is used to solve the multi-objective linear programming model to obtain the optimal bilateral transaction matching pair. Finally, an example of second-hand housing online rental-sales matching problems is analyzed to demonstrate the implementation process and applicability of the method proposed in this paper.																	1064-1246	1875-8967					2020	38	4			SI		4779	4792		10.3233/JIFS-191495													
J								Multidimensional situational information fusion method for energy saving on campus	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Smart campus; energy saving; situational information fusion; intelligent control	CONSERVATION; CONSUMPTION; EFFICIENCY; BUILDINGS; STRATEGIES; SUPPORT; SYSTEM	In order to solve the problem of resource waste in colleges and universities, this paper proposes a multidimensional situational information fusion method, which can be used to normalize, analyze and predict the multi-source data such as natural, humanistic and spatio-temporal data on campus so as to meet the application requirements for high-level decision-making. With this method, firstly, the event object model is used to normalize multi-source data. Then, the multidimensional situational information fusion mechanism of twice reasoning is used to obtain the real-time situation and equipment control scheme of the campus so that real-time intelligent semantic understanding is realized. In the process of reasoning, the improved KNN prediction model is used to predict situational trends, and the prediction information is used to continue deep reasoning and mining. Finally, the real-time energy-saving regulation is carried out through control instructions. In addition, through simulation verification, experimental results show that this method can quickly identify, integrate, and predict the current real-time situation and generate reasoning results, and finally achieve the goal of intelligent control for energy saving on campus.																	1064-1246	1875-8967					2020	38	4			SI		4793	4807		10.3233/JIFS-191513													
J								Partial kernel PCA-based GLRT for fault diagnosis of nonlinear processes	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Partial kernel principal component analysis (PKPCA); fault detection and isolation (FDI); generalized likelihood ratio test (GLRT); continuous stirred tank reactor (CSTR); air quality monitoring networks (AQMN)	COMPONENT ANALYSIS; SENSOR; MODELS	In this paper, a novel fault detection and isolation (FDI) framework based on kernel PCA (KPCA) and generalized likelihood ratio test (GLRT) that is capable of detecting and identifying faults is developed. Specifically, three main objectives are addressed. First, system model identification and residuals generation are addressed using KPCA model. Second, KPCA-based GLRT method is proposed to detect different types of faults in the systems. Third, partial KPCA (PKPCA)-based GLRT is developed for fault isolation. The proposed approach aims to apply a structured PKPCA-based GLRT to a set of sub-models. The fault detection and isolation performances using PKPCA-based GLRT are illustrated through two examples: a simulated continuous stirred tank reactor (CSTR) data and an air quality monitoring network data. The obtained results demonstrate the effectiveness of the partial KPCA-based GLRT method over the partial PCA-based GLRT method.																	1064-1246	1875-8967					2020	38	4			SI		4829	4843		10.3233/JIFS-191525													
J								A revision of sufficient and necessary condition of uncertainty distribution	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Uncertainty theory; uncertain variable; uncertainty distribution	ENTROPY	Nowadays, uncertainty theory has become a branch of axiomatic mathematics and has been studied by many researchers. In particular, uncertainty distribution is one of the most important tools to deal with indeterminate quantity in uncertainty theory. Peng and Iwamura (2010) presented a sufficient and necessary condition of a function being an uncertainty distribution. This paper gives a counterexample to illustrate this condition is not appropriate. A revision of the sufficient and necessary condition is also provided in this paper.																	1064-1246	1875-8967					2020	38	4			SI		4845	4854		10.3233/JIFS-191535													
J								An ELECTRE TRI-based outranking approach for multi-attribute group decision making with picture fuzzy sets	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Multi-attribute group decision making; picture fuzzy sets; ELECTRE TRI; supplier selection; picture fuzzy normalized Bonferroni mean	AGGREGATION OPERATORS; SIMILARITY MEASURES; HESITANT FUZZY; TOPSIS; NUMBERS; MODEL	The aim of this paper is to develop a multi-attribute group decision making (MAGDM) with picture fuzzy sets based on ELECTRE TRI method, i.e., an ELECTRE TRI based group decision making with picture fuzzy information is given. The MAGDM with picture fuzzy information based on picture fuzzy ELECTRE TRI outranking method is divided into three stages, i.e., the group decision information aggregation stage, determination of parameters and ELECTRE TRI outranking based outranking stage. A novel comparison law for picture fuzzy sets is introduced. In the group decision information aggregation stage, the concept of picture fuzzy normalized weighted Bonferroni mean (PFNWBM) is developed. The developed decision procedure is further applied to the assessment of energy security. The numerical example shows that the developed group decision procedure is feasible and valid.																	1064-1246	1875-8967					2020	38	4			SI		4855	4868		10.3233/JIFS-191540													
J								Fuzzy pricing of binary option based on the long memory property of financial markets	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Binary option; fuzzy option pricing; fractional brownian motion; asset-or-nothing option	JUMP-DIFFUSION MODEL	In order to introduce the long memory property of financial markets into the study of binary option pricing under fuzzy environment, the fractional Brownian motion is used to describe the dynamics of the stock price. This paper develops a new framework for pricing the binary option by using fuzzy set theory based on the long memory property of financial markets. The fuzzy price of the binary option is obtained by using a risk-neutral pricing principle and quasi-conditional expectation. To better understand the pricing model, some Greeks of this pricing model are given. In addition, the influence of the Hurst parameter H, a measure of long memory in the financial market, on binary option pricing is analyzed. Finally, the study provides an example that study binary option by fuzzifying the maturity value of the stock price using the triangular fuzzy number. The numerical experiment demonstrates the fuzzy pricing model proposed is rational and practicable.																	1064-1246	1875-8967					2020	38	4			SI		4889	4900		10.3233/JIFS-191551													
J								An unknown Protocol improved k-means clustering algorithm based on Pearson distance	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Protocol identification; unknown binary protocol; Pearson distance; determine cluster center		In order to solve the clustering problem of unknown binary protocols, an improved k-means unknown binary protocol clustering method is proposed, which determines the initial clustering center and improves the clustering distance. Firstly, the k value is determined and the clustering center is extracted by using DCBP (Determine the initial clustering center of binary Protocol) algorithm and the change rate of error square, and then the data are clustered by improving the k-means algorithm of distance function. The unknown binary protocol bit stream is divided into different subsets of binary protocols. By improving the k-means algorithm, the Pearson distance improves the accuracy of binary protocol clustering from 96% to 98.9%. The DCBP algorithm helps us to determine the k value accurately. The k value determined in this paper is 5, and the clustering accuracy is 98.9%. The clustering accuracy is 80% when k is 4 and 92.2% when k is 6. And the operation speed of the improved k-means algorithm is better than that of the AGNES algorithm. The algorithm is better adapted to the clustering of unknown binary protocols, and improves the accuracy of clustering and the speed of operation.																	1064-1246	1875-8967					2020	38	4			SI		4901	4913		10.3233/JIFS-191561													
J								A parallel neural network structure for sentiment classification of MOOCs discussion forums	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Parallel neural network; sentiment classification; MOOCs; learners' sentiment		Forum posts in Massive Open Online Courses (MOOCs) support an important way for online learners to interact with each other and with instructors. Instructors explore the sentiment from posts in MOOCs to detect learners' trending opinions towards the course so that they can improve MOOCs. However, it is unrealistic to expect instructors to adequately track learners' sentiment under the large number of messages exchanged on the forums. Fortunately, sentiment classification can automatically analyze learners' emotion on the course of MOOCs from posts. Traditional classifiers based on machine learning algorithm, which often depend on human-designed features and have data sparsity problem. In contrast to traditional approaches, we develop a novel neural network model called parallel neural network (PNNs) for sentiment classification of MOOCs discussion forum to alleviate the aforementioned problems. In our model, we design a parallel neural network structure to replace the popular serial neural network structure so that PNNs can preserve the validity of features as far as possible when neural network model training. Meanwhile, we also introduce Self-attention mechanism that automatically identifies which features play key roles in sentiment classification to obtain the important components in posts. We experiment on a public MOOCs dataset and two common sentiment classification datasets, and achieve a good performance. That means PNNs is a substantially reliable classification model for identifying the sentiment polarity of posts. The study has great potential application value on the platform of large scale courses, which can help instructors to gain the emotional tendency of learners for the course content in real time, so that timely intervention to support learning and may reduce the dropout rates.																	1064-1246	1875-8967					2020	38	4			SI		4915	4927		10.3233/JIFS-191572													
J								Decision tree matrix algorithm for detecting contextual faults in unmanned aerial vehicles	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										UAV; decision tree; anomaly detection; abnormal; classification; system failure; sensor faults; contextual faults; supervised algorithm	CLASSIFIER; DIAGNOSIS	The importance of detecting faults in Unmanned Aerial Vehicles motivated researchers to work in this area over recent years. Complex relationships among UAV attributes (Sensor readings, and Commands) make the task a bit challenging. Many known algorithms consider detecting the faults by spotting data anomalies in the values of each attribute without concern for their context, which leaves an opportunity for potential improvement. The contextual faults occur when a defected sensor shows an invalid value concerning other attributes. Our contribution is a novel matrix platform for detecting the potential contextual faults. This platform consists of multiple small Decision Trees, instead of using one huge single Decision Tree, which could be difficult and time-consuming to produce, particularly in the case of a large dataset with too many attributes. We propose to use the C4.5 decision tree algorithm to build each decision tree. The Decision Tree is a machine learning technique, which is an effective supervised method used for classification. It is computationally inexpensive and capable of dealing with noisy data. Besides, our approach uses a sliding window technique during training and testing phases, which brings into consideration the effect of the previous state of the system on the process of detecting the contextual faults. The algorithm starts by collecting the attributes of the UAV into a table of pairs, where each pair consists of two attributes; then, it defines the Decision Tree matrix by assigning one Decision Tree for each pair of attributes. The Training step includes constructing training sub-datasets using the values of sliding windows. The C4.5 algorithm uses each constructed training sub-dataset to induce one Decision Tree in the matrix. Finally, the testing step is responsible for reading the values of the sliding windows and using the concerned Decision Tree to detect the contextual faults. We evaluated our approach using Detection Rate, False Alarm Rate, Precision, and F1-score indicators. Moreover, we made a comparison with other broadly used algorithms, such as K-Means and One-Class SVM. Our approach showed superior results in detecting different types of faults (sensor-offset, sensor-stuck, sensor-drift, and sensor-cut). The DT-Matrix performance was neither affected by the small values of the outliers, nor by the number of the outliers, and this caused the DT-Matrix to work better in most of the experiments compared to the other algorithms.																	1064-1246	1875-8967					2020	38	4			SI		4929	4939		10.3233/JIFS-191575													
J								Fuzzy classification involved in fusion of existing decision and pre-known task applied for integrated input space	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy classifier; deep learning structure; existing decision information; interpretability; classification performance	LEARNING ALGORITHM; SYSTEM; PREDICTION; PERFORMANCE; DESIGN	Although the Takagi-Sugeno-Kang (TSK) fuzzy classifier has achieved great success, how to further improve its classification performance and enhance its interpretability is still one of the most difficult challenges. Involved with the fusion of existing decision information and pre-known classification task, a newly proposed deep/hierarchical TSK fuzzy classifier (EDIPK-TSK) with interpretable fuzzy rules makes full use of the classification advantages of each base classifier to construct a multi-layer deep learning structure. This study first considers that the existing decision information of each training sub-block is sequentially projected into the subsequent sub-blocks for training. Undoubtedly, the existing decision information has played a guiding role in the current learning process to some extent. Simultaneously, the pre-known classification task is fused into the decision information for fine-tuning of it, which can significantly improve the efficiency of guidance and accelerate the fitting speed of the model. In each layer, the use of interpretable integration input space guarantees that EDIPK-TSK is not a black box. The proposed deep classifier can realize learning by using short fuzzy rules, which ensures the satisfactory interpretability of the classifier. The final experimental results also verify that EDIPK-TSK has strong classification advantages and interpretability.																	1064-1246	1875-8967					2020	38	4			SI		4941	4957		10.3233/JIFS-191579													
J								Robust fuzzy control for nonlinear discrete-time systems with internal and external noises subject to multi-variance constraints and pole location constraints	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Robust fuzzy control; discrete-time Takagi-Sugeno fuzzy model; variance constraints and pole location constraints	D-STABILITY; MODEL; ASSIGNMENT; PLACEMENT; DESIGN; ROUTER; ENERGY	A novel robust fuzzy controller design problem subject to multi-variance constraints and pole location constraints for nonlinear discrete-time systems with internal and external noises is studied in this paper. Based on the Takagi-Sugeno fuzzy model, the nonlinear discrete-time systems are represented by blending many linear subsystems. The control performances considered in this paper include stability requirement, pole location constraint, individual state variance constraint, and minimum output variance. Applying the Lyapunov theory, a discrete-time robust fuzzy controller is designed based on parallel distributed compensation technology and the relevant conditions are deduced in the form of linear matrix inequalities. By solving these conditions, a discrete-time robust fuzzy controller can be obtained to satisfy the above performance constraints. At last, some simulations for controlling a nonlinear inverted pendulum system and a nonlinear ship steering system are provided to show the feasibility and applicability of the proposed robust fuzzy control method.																	1064-1246	1875-8967					2020	38	4			SI		4959	4975		10.3233/JIFS-191600													
J								Pythagorean fuzzy soft graphs with applications	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS											DECISION-MAKING; MEMBERSHIP GRADES; TOPSIS; SETS																		1064-1246	1875-8967					2020	38	4			SI		4977	4991															
J								Adaptive strategy for fault detection, isolation and reconstruction of aircraft actuators and sensors	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Actuators; sensors; fault detection and isolation; aircraft; neural networks; nonlinear systems	NONLINEAR-SYSTEMS; IDENTIFICATION; DIAGNOSIS	An online fault detection, isolation, and reconstruction strategy is proposed for actuators and sensors fault detection of an aircraft. For increasing the fault detection capabilities, the Extended Kalman Filter (EKF) is used for the weight updating parameters of multi-layer perceptron (MLP) neural network. The main purpose of using the EKF is to make the weight updating parameters of MLP adaptive in order to increase the fault detection, isolation and reconstruction preciseness, efficiency and rapidness compared to the conventional MLP where the fixed learning rate due to which it has slow response to faults occurrence. Because of the online adaptation of weighting parameters of MLP, the preciseness of the faults detection is increased. For testing and validation of the proposed strategy, the nonlinear dynamics of Boeing 747 100/200 are used. Results demonstrate that the proposed strategy has better accuracy and rapid response to fault detection compared to convention multi-layer perceptron neural network based faults detection schemes.																	1064-1246	1875-8967					2020	38	4			SI		4993	5012		10.3233/JIFS-191627													
J								Three-way decisions based on multi-granulation support intuitionistic fuzzy probabilistic rough sets	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Support intuitionistic fuzzy sets; rough sets; support intuitionistic fuzzy probabilistic; multi-granulation rough sets; three-way decisions	APPROXIMATION OPERATORS; SELECTION; SPACES	Three-way decisions have become a representative of the models dealing with decision-making problems with uncertainty and fuzziness. However, most of the current models are single granular structures that cannot meet the needs of complex fuzzy environmental decision-making. Multi-granulation rough sets can better deal with fuzzy problems of multiple granularity structures. Therefore, three-way decisions will be a more reasonable decision-making model to address uncertain decision problems in the context of multiple granularity structures. In this paper, firstly we propose the four different conditional probabilities based on support intuitionistic fuzzy sets, which are referred to as support intuitionistic fuzzy probability. Then, a multi-granulation support intuitionistic fuzzy probabilistic approximation space is defined. Secondly, we calculate the thresholds a and beta by the Bayesian theory, and construct four different types of multi-granulation support intuitionistic fuzzy probabilistic rough sets models in multi-granulation support intuitionistic fuzzy probabilistic approximation space. Moreover, some properties of lower and upper approximation operators of these models are discussed. Thirdly, by combining these proposed models with three-way decision theory, the corresponding three-way decision models are constructed and three-way decision rules are derived. Finally, an example of person-job fit procedure is given to prove and compare the validity of these proposed models.																	1064-1246	1875-8967					2020	38	4			SI		5013	5031		10.3233/JIFS-191657													
J								A new mixed-signal CMOS fuzzy logic controller in current mode	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										A/D converter; CMOS fuzzy controller; current mode circuits; defuzzifier; fuzzifier; fuzzy logic	DESIGN; MULTIPLIER/DIVIDER; IMPLEMENTATION	In this paper, the design and simulation results of a general-purpose fuzzy logic controller (FLC) with mixed-signal (analog and digital) inputs and digital outputs are presented. Based on a new strategy, it provides simplicity and high speed from the analog prospective and a total digital system advantages with unchanged digital system properties. A novel and reliable structure with respect to other topologies for the fuzzifier section is designed which enhances the accuracy and the velocity. In order to detect minimum and maximum of the input currents at the same time, an inference engine consisting of a min & max circuit is an addition. The benchmark for the defuzzifier in the proposed design is simplicity and through a simple approach, the center of area (COA) is attributed to the defuzzifier. The proposed controller circuit consists of two inputs, sixteen rules and one output designed in 0.35 mu m CMOS standard technology and simulated with MATLAB systematically. The total controller circuit is simulated with HSPICE simulator (BSIM3v3 parameters) and the layouts were extracted with Cadence Virtuoso v 5.1. The inference speed of the controller is about 41.3 MFLIPS (fuzzy logic inference per second) and power consumption is 3.2 mW.																	1064-1246	1875-8967					2020	38	4			SI		5033	5044		10.3233/JIFS-191672													
J								A fuzzy rule based effective feature selection approach for augmented reality	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Image classification; augmented reality; virtual reality; fuzzy systems; robot motion	STEREO VISION; SYSTEM; ALGORITHM; TRACKING; MODEL	A tele-operated robot stereo vision system is used for stretching out the operator's eye-hand motion and its distance based co-ordination with experts. The major challenge is the reduction of communication delay by using effective decisions to avoid tele-operation instability. This problem can be handled effectively by using the principles of Augmented Reality which provides facilities for superimposing virtual objects onto the real video images of the workspace to create a simulation plan in the client system. In this paper, we propose a new feature selection algorithm called Fuzzy Rules and Information Gain Ratio based Feature Selection Algorithm for selecting the optimal number of features from the full set of available features. Also, a new Fuzzy Rule based Neuro-Genetic Classification Algorithm is proposed in this paper for classifying the augmented images more accurately. The main advantages of the proposed model are reduction in classification and communication time and increase in decision accuracy.																	1064-1246	1875-8967					2020	38	4			SI		5045	5054		10.3233/JIFS-191674													
J								Extremal solutions to fuzzy relation equations and inequalities with three unknowns	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy relation; fuzzy relation equation; fuzzy relation inequality; complete residuated lattice; complete meet-continuous lattice	WEAKLY LINEAR-SYSTEMS; BISIMULATIONS; SOLVABILITY; ALGORITHM; EQUIVALENCE; AUTOMATA; SETS	A large number of studies have investigated the systems of fuzzy relation equations and inequalities, which have a much wider field of application. In this paper, we study several types of systems of fuzzy relation equations and inequalities consisting of a given family of k-ary fuzzy relations, where natural number k = 2, and three unknown fuzzy relations over complete residuated lattices and meet-continuous lattices. Their solutions are triples of fuzzy relations. For the systems of fuzzy relation inequalities, we give the greatest solutions contained in a given triple of fuzzy relations, the least solutions containing a given triple of fuzzy relations, or give maximal solutions contained in or containing a given triple of fuzzy relations, or belonging to a given interval of triples of fuzzy relations over complete residuated lattices and complete meet-continuous lattices. For the systems of fuzzy relation equations, we present a method of computing maximal solutions contained in a given triple of fuzzy relations and a method of computing minimal solutions containing a given triple of fuzzy relations. Furthermore, we provide some conditions under which there exist the greatest solutions contained in and the least solutions containing a given triple of fuzzy relations.																	1064-1246	1875-8967					2020	38	4			SI		5055	5076		10.3233/JIFS-191695													
J								Clever backstepping control using two adaptive laws, a RRFNN and a compensated controller of SPCRIM drive system	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Backstepping control; copper rotor induction motor; Lyapunov stability; particle swarm optimization; recurrent fuzzy neural network	BARRIER LYAPUNOV FUNCTIONS; NONLINEAR-SYSTEMS; NEURAL-NETWORKS; PARTICLE SWARM; DESIGN	A six phase copper rotor induction motor (SPCRIM) drive system still exists in lots of nonlinear characteristics such as the added load torque, the Stribeck effect torque, the the cogging torque, the coulomb friction torque and the parameters variations. Due to some uncertainties effects, the using linear controller can not achieve better control performance for the SPCRIM drive system. To obtain better performance, a clever backstepping control system using two adaptive laws and a hitting function is proposed for controlling the SPCRIM drive system. To improve larger chattering phenomenon under uncertainties affects for aforementioned control system, the clever backstepping control system using two adaptive laws, a revised recurrent fuzzy neural network (RRFNN) and a compensated controller is proposed to estimate the required lumped uncertainty and to compensate the minimum reconstructed error of the estimated law. Furthermore, the corrected particle swarm optimization (CPSO) algorithm by using variable dynamic inertia weight and variable dynamic constriction factor with segment regulation mechanics that is the innovativeness for using the CPSO algorithm is adopted to regulate four variable learning rates of the weights in the RRFNN to speed-up parameter's convergence. Finally, comparative performances through some experimental results are verified that the clever backstepping control system using two adaptive laws, a RRFNN and a compensated controller has better control performances than those of the proposed methods for the SPCRIM drive system.																	1064-1246	1875-8967					2020	38	4			SI		5077	5093		10.3233/JIFS-191712													
J								Molecular topological description of bacterial hypertrees	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Biological networks; molecular structure descriptor; eccentric atom bond connectivity; eccentric connectivity polynomial; binary tree; hyper binary trees networks	INTUITIONISTIC FUZZY GRAPHS; ECCENTRIC CONNECTIVITY INDEX; MAXIMUM ABC INDEX; INFINITE FAMILY; DOMINATION; PRODUCT; VERSION	The topological descriptor are numerical parameters of a graph which characterize its topology and are usually graph invariants. Nowadays, Biological science is an energizing and quickly creating branch of knowledge together with the topological descriptor. In recent years, the investigation of living things has experienced huge extension. All the living things are composed of a fundamental unit of life called cells. The microbiology is a science that deals with the living creatures that can not be seen by naked eyes like bacteria, viruses. In recent years, eccentricity based topological indices gain a lot of importance in many disciplines like chemistry, computer science, integrated circuits, electric circuits, communication networks, biological networks. In a connected graph G, the vertex set V(G) shows the bacteria and the edge set E(G) shows the relationship between two bacterium. Mostly, the reproduction of bacteria and other microorganisms occur by binary fission process. The topological indices play a vital and useful role in indicating and analyzing physical, chemical and biological properties of any molecular graph. In this paper, we have computed eccentric polynomial and eccentric atom bond connectivity index of hyper binary trees networks (k-level) and relate these networks to biological networks. Also discuss how biological activities of these networks work in daily life.																	1064-1246	1875-8967					2020	38	4			SI		5095	5105		10.3233/JIFS-191714													
J								Imperfect competition models in economic market structure with q-rung picture fuzzy information	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										q-Rung picture fuzzy competition graphs; q-rung picture fuzzy economic competition graphs; imperfect competition models; duopoly; oligopoly; monopolistic competition	AGGREGATION OPERATORS; GRAPHS; SET	The imperfect competition models are equipped by fuzzy set theory with direct assessments of uncertainty. An appropriate point of departure for origination of a system with potentially broader coverage can be provided in view of fuzzy sets. In this way, several extensions of fuzzy set have been introduced to deal with uncertain and ambiguous information including relationships between objects. The q-rung picture fuzzy (q-RPF) model, which inherits the virtues of q-rung orthopair fuzzy set and picture fuzzy set, is one of the convenient way to represent such information. In order to exhibit interactions in various economic structures the conception of q-RPF economic competition graphs can be employed. Thus the intention of present study is to deal with q-rung picture fuzzy competition graphs (q-RPFCGs) and in particular, q-rung picture fuzzy economic competition graphs (q-RPFECGs) with its generalizations: q-RPF k-economic competition graphs; p-economic competition q-RPFGs; and m-step q-RPFECGs through several important results. Furthermore, this paper offers a brief review for perfect and imperfect competition in competitive market structures and sketch q-RPFECGs to represent duopoly, oligopoly, and monopolistic competitions in graph theoretic approach. Also, it designs an algorithm to calculate the strength of economic competition among buyers and sellers in imperfect competitive markets with q-RPF information.																	1064-1246	1875-8967					2020	38	4			SI		5107	5126		10.3233/JIFS-191726													
J								Fuzzy fractional integral equations involving the kernel psi-functions	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										delta-Ulam-Hyers-Rassias; kernel psi-functions; Fuzzy fractional integral equations	DIFFERENTIAL-EQUATIONS; STABILITY	In this work, a new class of generalized fractional integral equations involving the kernel psi-function in the fuzzy setting is introduced. With this problem, we can recover a wide class of fractional fuzzy integral equations by choosing the kernel psi-function. In this sense, we provide sufficient conditions for the existence, uniqueness of solutions and delta-Ulam-Hyers-Rassias stability of the given problems. Some examples are given to illustrate our main results.																	1064-1246	1875-8967					2020	38	4			SI		5127	5141		10.3233/JIFS-191743													
J								Extremal solutions of fuzzy fractional Volterra integral equations involving the generalized kernel functions by the monotone iterative technique	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy fractional integral equations; generalized kernel functions; extremal solutions; monotone iterative technique	DIFFERENTIAL-EQUATIONS; INTERVAL; EXISTENCE	In this work, we consider a new form of fuzzy fractional Volterra integral equations (FFVIEs) involving the generalized kernel functions. By using the monotone iterative technique (MIT) combined with the method of lower and upper solutions, the existence of extremal solutions of FFVIEs is established. Some examples are given to illustrate our main results.																	1064-1246	1875-8967					2020	38	4			SI		5143	5155		10.3233/JIFS-191746													
J								HSE risk prioritization of molybdenum operation process using extended FMEA approach based on Fuzzy BWM and Z-WASPAS	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS											FAILURE MODE; OCCUPATIONAL-HEALTH; INDUSTRY; SAFETY; AHP; SYSTEM; MOORA																		1064-1246	1875-8967					2020	38	4			SI		5157	5173															
J								DEA cross-efficiency and fuzzy preference relation based on semi-disposability of undesirable outputs for environmental assessments	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										DEA cross-efficiency; semi-disposability; performance evaluation; preference relations; undesirable outputs	ENERGY EFFICIENCY; CHINA; MODEL; PERFORMANCE; WEAK	Considering the influence of undesirable outputs in environmental assessments of the practical production process, this paper proposes a DEA cross-efficiency method and fuzzy preference relation based on semi-disposability of undesirable outputs for environmental assessments, which combines self-evaluation with peer-evaluation and avoids potentially unrealistic weighting scheme. We first develop a new input-oriented DEA model based on semi-disposability of undesirable outputs. Then, the corresponding DEA cross-efficiency method is proposed to evaluate the homogeneous decision-making units (DMUs) and the additive fuzzy preference relation is constructed using the DEA cross-efficiency scores. An output-oriented DEA model is adopted to derive the priority vector from the constructed additive fuzzy preference relation. Furthermore, we summarize the specific procedures for environmental assessments based on DEA cross-efficiency and fuzzy preference relation. Finally, the practical example of industrial environment assessment of the Yangtze River Economic Belt is provided to show the applicability and effectiveness of the proposed method.																	1064-1246	1875-8967					2020	38	4			SI		5191	5201		10.3233/JIFS-191777													
J								A novel model based on similarity measure and quality function deployment on interval neutrosophic sets for evaluation and selection market segments	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS											QFD APPROACH; FUZZY; AHP																		1064-1246	1875-8967					2020	38	4			SI		5203	5214															
J								A DEMATEL and consensus based MCGDM approach for with multi -granularity hesitant fuzzy linguistic term set	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Consensus; MCGDM; G-DEMATEL; HFLTS; relative projection; similarity degree	GROUP DECISION-MAKING; INFORMATION; MECHANISM; MODELS; TRUST	This article proposes a novel multi-criteria group decision making (MCGDM) approach with multi-granularity hesitant fuzzy linguistic term set (HFLTS). It consists three aspects: (1) The processing algorithms for multi-granularity HFLTS; (2) G-DEMATEL model based on HFLTS; (3) A consensus model with the feedback mechanism. To do that, the relative projection model for multi-granular hesitant fuzzy language information is presented and the similarity degree between individual decision matrices based on relative projection is defined. On the basis, the similarity degree is used to determine the expert's weight vectors, and the SD-MGHIOWA operator is defined to aggregate the experts opinions. The traditional G-DEMATEL model is improved by the multi-granular hesitant fuzzy language and a newmodel is built to analyze the correlation between criterion and weight vector. Furthermore, consensus degree is defined from three levels to identify the inconsistent experts, and a feedback mechanism is activated to generate recommendation advice for the inconsistent experts to increase consensus degree. After that, a comprehensive score mechanism of alternatives is designed to select the most appropriate alternative after the consensus is reached. The main characteristics of the proposed MCGDM is that it not only considers the correlation between criterions but also provides the consensus model with the feedback mechanism in the context of hesitant fuzzy language. Finally, an example is provided to illustrate the feasibility and effectiveness of the developed method, which are then compared to the existing methods.																	1064-1246	1875-8967					2020	38	4			SI		5215	5229		10.3233/JIFS-191805													
J								Fixed point results for intuitionistic fuzzy mappings and an application	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Intuitionistic fuzzy mappings; intuitionistic fuzzy fixed point; nonlinear F-contractions; delay integral equations	COINCIDENCE; THEOREMS; EXISTENCE; EQUATIONS; SETS	In this paper, we prove a fixed point theorem for intuitionistic fuzzy mappings concerning F-contractions without using the Pompeiu - Hausdorff distance between cut sets of intuitionistic fuzzy mappings. This result and its consequences extend and generalize several results in the literature. An application to the existence for solutions of a delay integral equation of Volterra type and an example are given to illustrate the usability of our results.																	1064-1246	1875-8967					2020	38	4			SI		5231	5240		10.3233/JIFS-191806													
J								Convergence of numerical methods for fuzzy differential equations	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Credibility; fuzzy differential equations; Liu process; numerical methods; convergence		In this paper, semi-implicit and implicit Euler schemes for homogeneous fuzzy differential equations with perturbation term reflected by Liu process are introduced. As to the application with implicit term in numerical scheme, we must make the result gradually explicit by iterative method. In order to obtain numerical method with higher accuracy than fuzzy Euler method, fuzzy trapezoidal scheme is derived. Fuzzy trapezoidal scheme is an implicit formation, which is complicated and cumbersome in computational processing. For the sake of this problem, fuzzy Euler-trapezoidal method is proposed to simplify the algorithm. Furthermore, the convergence properties are investigated for numerical methods. At last, local convergence is proved better than global convergence.																	1064-1246	1875-8967					2020	38	4			SI		5257	5266		10.3233/JIFS-191856													
J								Stability in p-th moment of multi-dimensional uncertain differential equation	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Uncertainty theory; multi-dimensional uncertain differential equation; moment stability	SURE STABILITY	Uncertain differential equation plays an important role in dealing with dynamical systems with uncertainty. Multidimensional uncertain differential equation is a type of differential equation driven by multi-dimensional Liu processes. Stability analysis of a multi-dimensional means insensitivity of the state of a system to small changes in the initial state. This paper focuses on the stability in p-th moment for multi-dimensional uncertain differential equation. The concept of stability in p-th moment for multi-dimensional uncertain differential equation is presented. Some stability theorems for the solution of multi-dimensional uncertain differential equation are given, in which some sufficient conditions for a multidimensional uncertain differential equation being stable in p-th moment and a sufficient and necessary condition for a linear multi-dimensional uncertain differential equation being stable in p-th moment are provided. In addition, this paper discusses the relationships among stability in p-th moment, stability in measure and stability in mean.																	1064-1246	1875-8967					2020	38	4			SI		5267	5277		10.3233/JIFS-191880													
J								A framework to estimate dwell time of BRT systems using fuzzy regression	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy regression; BRT systems; headway control; dwell time; GDP	PRIORITY CONTROL; MODEL	This paper presents adjustments and validation of a model for the process of passenger boarding and alighting of BRT (Bus Rapid Transport) and express bus systems. Such models are of fundamental importance for the implementation of strategies to control the operation of those systems, which allow for quality and efficiency gains in the service. First, a generalized disjunctive programming model is presented in order to adapt a bus headway control formulation according to the number of onboard passengers. Later, as a way to improve the model, real data were collected from the boarding/alighting times of passengers in the Trunk line 10 system in the city of Blumenau, Brazil. These data were used in a two-stage fuzzy regression to take into account the uncertainties associated with the dwell time of the passengers. The actual data collected were used in simulations of the enhanced bus headway control system, showing that even in the presence of disturbances, the system can still operate properly without bunching.																	1064-1246	1875-8967					2020	38	4			SI		5279	5293		10.3233/JIFS-191904													
J								Fuzzy linear systems with the two-dimension fuzzy data	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy numbers; two-dimension fuzzy numbers; fuzzy linear system; fuzzy system of linear equations	DECOMPOSITION METHOD; COMPLEX SYSTEM; EQUATIONS	As a key theoretical basis of fuzzy analysis, a new representation of the two-dimension fuzzy numbers is proposed in this paper. We also present a new method for solving the fuzzy system of linear equations with the two-dimension fuzzy data based on the new representation of the two-dimension fuzzy numbers, which may translate a fuzzy system of linear equations into two real systems of linear equations, one is an n x n real system of linear equations and the other is a (2n) x (2n) real system of linear equations, where the coefficients, the right hand term and the unknown term of the (2n) x (2n) real systems are all positive real numbers. Accordingly, we compare the classical methods with the technique on the basis of the study of the two-dimensional fuzzy linear system proposed in this paper for an one-dimensional fuzzy linear system, and the results shows that they have the same solutions. Finally, the conditions of the existence of the solutions for a fuzzy system of linear equations are discussed, and the examples are given to show the efficiency and effectiveness of the method investigated in this article.																	1064-1246	1875-8967					2020	38	4			SI		5295	5315		10.3233/JIFS-191927													
J								Personalized Image Enhancement Using Neural Spline Color Transforms	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image enhancement; deep learning; data-driven methods; convolutional neural networks	ADJUSTMENT	In this work we present SpliNet, a novel CNN-based method that estimates a global color transform for the enhancement of raw images. The method is designed to improve the perceived quality of the images by reproducing the ability of an expert in the field of photo editing. The transformation applied to the input image is found by a convolutional neural network specifically trained for this purpose. More precisely, the network takes as input a raw image and produces as output one set of control points for each of the three color channels. Then, the control points are interpolated with natural cubic splines and the resulting functions are globally applied to the values of the input pixels to produce the output image. Experimental results compare favorably against recent methods in the state of the art on the MIT-Adobe FiveK dataset. Furthermore, we also propose an extension of the SpliNet in which a single neural network is used to model the style of multiple reference retouchers by embedding them into a user space. The style of new users can be reproduced without retraining the network, after a quick modeling stage in which they are positioned in the user space on the basis of their preferences on a very small set of retouched images.																	1057-7149	1941-0042					2020	29						6223	6236		10.1109/TIP.2020.2989584													
J								Rate Control for Video-Based Point Cloud Compression	IEEE TRANSACTIONS ON IMAGE PROCESSING										Bit allocation; high efficiency video coding; point cloud compression; rate control; video-based point cloud compression	RATE CONTROL ALGORITHM; OPTIMUM BIT ALLOCATION; DELAY RATE CONTROL; OPTIMIZATION; H.264	Rate control is a necessary tool for video-based point cloud compression (V-PCC). However, there is no solution specified on this topic yet. In this paper, we propose the first rate control algorithm for V-PCC. Generally, a rate control algorithm is divided into two processes: bit allocation and bitrate control. In V-PCC, the total bits are composed of three parts: the header information including the auxiliary information and occupancy map, the geometry video, and the attribute video. The bit allocation aims to assign the total bits to these three parts. Since the auxiliary information and occupancy map are encoded losslessly, the bit cost of the header information is fixed. Therefore, we only need to assign bits between the geometry and attribute videos. Our first key contribution is the proposed video-level bit allocation algorithm between the geometry and attribute videos to optimize the overall reconstructed point cloud quality. Then we assign geometry and attribute video bits to each group of pictures (GOP), each frame, and each basic unit (BU). Our second key contribution is that we assign zero bits to the BUs with only unoccupied pixels. The unoccupied pixels are useless for the reconstructed quality of the point cloud and therefore should be assigned zero bits. In the bitrate control process, the encoding parameters are determined, and the model parameters are updated for each frame and BU to achieve the target bits. Our third key contribution is that we propose a BU-level model updating scheme to handle the case where various patches may be placed in different positions in neighboring frames. We use the auxiliary information to find the corresponding BU in the previous frame and apply its model parameters to the current BU. The proposed algorithms are implemented in the V-PCC and High Efficiency Video Coding (HEVC) reference software. The experimental results show that the proposed rate control algorithm can achieve significant bitrate savings compared with the state-of-the-art method.																	1057-7149	1941-0042					2020	29						6237	6250		10.1109/TIP.2020.2989576													
J								Deblurring Face Images Using Uncertainty Guided Multi-Stream Semantic Networks	IEEE TRANSACTIONS ON IMAGE PROCESSING										Facial image deblurring; semantic masks; confidence scores		We propose a novel multi-stream architecture and training methodology that exploits semantic labels for facial image deblurring. The proposed Uncertainty Guided Multi-Stream Semantic Network (UMSN) processes regions belonging to each semantic class independently and learns to combine their outputs into the final deblurred result. Pixel-wise semantic labels are obtained using a segmentation network. A predicted confidence measure is used during training to guide the network towards the challenging regions of the human face such as the eyes and nose. The entire network is trained in an end-to-end fashion. Comprehensive experiments on three different face datasets demonstrate that the proposed method achieves significant improvements over the recent state-of-the-art face deblurring methods. Code is available at: https://github.com/rajeevyasarla/UMSN-Face-Deblurring																	1057-7149	1941-0042					2020	29						6251	6263		10.1109/TIP.2020.2990354													
J								Day and Night-Time Dehazing by Local Airlight Estimation	IEEE TRANSACTIONS ON IMAGE PROCESSING										Local airlight; haze; dehazing; night-time; fusion	IMAGE; VISIBILITY; FRAMEWORK; WEATHER	We introduce an effective fusion-based technique to enhance both day-time and night-time hazy scenes. When inverting the Koschmieder light transmission model, and by contrast with the common implementation of the popular dark-channel [1], we estimate the airlight on image patches and not on the entire image. Local airlight estimation is adopted because, under night-time conditions, the lighting generally arises from multiple localized artificial sources, and is thus intrinsically non-uniform. Selecting the sizes of the patches is, however, non-trivial. Small patches are desirable to achieve fine spatial adaptation to the atmospheric light, but large patches help improve the airlight estimation accuracy by increasing the possibility of capturing pixels with airlight appearance (due to severe haze). For this reason, multiple patch sizes are considered to generate several images, that are then merged together. The discrete Laplacian of the original image is provided as an additional input to the fusion process to reduce the glowing effect and to emphasize the finest image details. Similarly, for day-time scenes we apply the same principle but use a larger patch size. For each input, a set of weight maps are derived so as to assign higher weights to regions of high contrast, high saliency and small saturation. Finally the derived inputs and the normalized weight maps are blended in a multi-scale fashion using a Laplacian pyramid decomposition. Extensive experimental results demonstrate the effectiveness of our approach as compared with recent techniques, both in terms of computational efficiency and the quality of the outputs.																	1057-7149	1941-0042					2020	29						6264	6275		10.1109/TIP.2020.2988203													
J								LFNet: Light Field Fusion Network for Salient Object Detection	IEEE TRANSACTIONS ON IMAGE PROCESSING										Light field; salient object detection; convolutional neural networks; fusion network		In this work, we propose a novel light field fusion network-LFNet, a CNNs-based light field saliency model using 4D light field data containing abundant spatial and contextual information. The proposed method can reliably locate and identify salient objects even in a complex scene. Our LFNet contains a light field refinement module (LFRM) and a light field integration module (LFIM) which can fully refine and integrate focusness, depths and objectness cues from light field image. The LFRM learns the light field residual between light field and RGB images for refining features with useful light field cues, and then the LFIM weights each refined light field feature and learns spatial correlation between them to predict saliency maps. Our method can take full advantage of light field information and achieve excellent performance especially in complex scenes, e.g., similar foreground and background, multiple or transparent objects and low-contrast environment. Experiments show our method outperforms the state-of-the-art 2D, 3D and 4D methods across three light field datasets.																	1057-7149	1941-0042					2020	29						6276	6287		10.1109/TIP.2020.2990341													
J								Fuzzy testing decision-making model for intelligent manufacturing process with Taguchi capability index	JOURNAL OF INTELLIGENT & FUZZY SYSTEMS										Fuzzy testing decision-making model; intelligent manufacturing; Taguchi capability index; membership function of fuzzy number; alpha-cuts	PERFORMANCE	As the Internet-of-Things matures, technologies for the measurement and collection of production data are also improving. What is needed next is an effective information analysis model to aid in the timely adjustment of manufacturing parameters to optimize production. Production data analysis models must be continuously and properly utilized to monitor and maintain process quality. Improving product quality helps to lengthen service life, decrease scrap and rework, and reduce the social losses caused by malfunctions and maintenance. The Taguchi capability index C-pm can fully reflect the losses and yield of processes and is a convenient and effective tool to evaluate and analyze process data in the industry. As it contains unknown parameters, we derived the upper confidence limit (UCL) of C-pm based on collected production data. Due to the fuzzy uncertainties that are common in measurement data, we then used the UCL of the index to construct a fuzzy membership function and propose a fuzzy testing decision-making model to determine whether processes are in need of improvement. Before the proposed fuzzy test methods became full-fledged, we used the concept of sample size and the rules of statistical testing to explain the motivation underlying those methods. In fact, the sample size influences the risk of misjudgment in UCL, and in practice, sample sizes are rarely large due to cost and time considerations, thereby they produce larger UCL with a corresponding decrease in accuracy and increase in risk of misjudgment. The fuzzy test methods proposed in this study are based on statistical inference, and judgement is aided by expertise, thus are capable of solving the problems associated with larger UCL. Therefore, the theoretical foundation of this fuzzy testing decision-making model is the UCL, it can lower the chance of misjudgment caused by sampling errors and increase evaluation accuracy.																	1064-1246	1875-8967					2020	38	2			SI		2129	2139		10.3233/JIFS-190865													
J								Aligning Discriminative and Representative Features: An Unsupervised Domain Adaptation Method for Building Damage Assessment	IEEE TRANSACTIONS ON IMAGE PROCESSING										Building damage assessment; domain adaptation; MMD; transfer learning; variational autoencoder	CLASSIFICATION	Building assessment is highly prioritized during rescue operations and damage relief after hurricane disasters. Although machine learning has made remarkable improvement in building damage classification, it remains challenging because classifiers must be trained using a massive amount of labeled data. Furthermore, data labeling is labor intensive, costly, and unavailable after a disaster. To address this issue, we propose an unsupervised domain adaptation method with aligned discriminative and representative features (ADRF), which leverage a substantial amount of labeled data of relevant disaster scenes for new classification tasks. The remote sensing imageries of different disasters are collected using different sensors, viewpoints, times, even at various places. Compared with the public datasets used in the domain adaptation community, the remote sensing imageries are more complicated which exhibit characteristics of lower discrimination between categories and higher diversity within categories. As a result, pursuing domain invariance is a huge challenge. To achieve this goal, we build a framework with ADRF to improve the discriminative and representative capability of the extracted features to facilitate the classification task. The ADRF framework consists of three pipelines: a classifier for the labeled data of the source domain and one autoencoder each for the source and target domains. The latent variables of autoencoders are forced to observe unit Gaussian distributions by minimizing the maximum mean discrepancy (MMD), whereas the marginal distributions of both domains are aligned via the MMD. As a case study, two challenging transfer tasks using the hurricane Sandy, Maria, and Irma datasets are investigated. Experimental results demonstrate that ADRF achieves overall accuracy of 71.6% and 84.1% in the transfer tasks from dataset Sandy to dataset Maria and dataset Irma, respectively.																	1057-7149	1941-0042					2020	29						6110	6122		10.1109/TIP.2020.2988175													
J								Real-Time Correlation Tracking Via Joint Model Compression and Transfer	IEEE TRANSACTIONS ON IMAGE PROCESSING										Correlation tracking; model transfer; knowledge distillation; real-time tracking	OBJECT TRACKING	Correlation filters (CF) have received considerable attention in visual tracking because of their computational efficiency. Leveraging deep features via off-the-shelf CNN models (e.g., VGG), CF trackers achieve state-of-the-art performance while consuming a large number of computing resources. This limits deep CF trackers to be deployed to many mobile platforms on which only a single-core CPU is available. In this paper, we propose to jointly compress and transfer off-the-shelf CNN models within a knowledge distillation framework. We formulate a CNN model pretrained from the image classification task as a teacher network, and distill this teacher network into a lightweight student network as the feature extractor to speed up CF trackers. In the distillation process, we propose a fidelity loss to enable the student network to maintain the representation capability of the teacher network. Meanwhile, we design a tracking loss to adapt the objective of the student network from object recognition to visual tracking. The distillation process is performed offline on multiple layers and adaptively updates the student network using a background-aware online learning scheme. The online adaptation stage exploits the background contents to improve the feature discrimination of the student network. Extensive experiments on six standard datasets demonstrate that the lightweight student network accelerates the speed of state-of-the-art deep CF trackers to real-time on a single-core CPU while maintaining almost the same tracking accuracy.																	1057-7149	1941-0042					2020	29						6123	6135		10.1109/TIP.2020.2989544													
J								Fast Depth and Mode Decision in Intra Prediction for Quality SHVC	IEEE TRANSACTIONS ON IMAGE PROCESSING										SHVC; depth decision; ILR mode; intra prediction; early termination	CU SIZE DECISION; ALGORITHM; SKIP	Scalable High Efficiency Video Coding (SHVC) is the extension of High Efficiency Video Coding (HEVC). In intra prediction for quality SHVC, a Coding Unit (CU) is recursively divided into a quadtree-based structure from the largest CU to the smallest CU, in which 35 intra prediction modes and Inter-Layer Reference (ILR) mode are checked to determine the best possible mode. This leads to very high coding efficiency but also results in an extremely high coding complexity. To improve coding speed while maintaining coding efficiency, in this paper, we propose a new efficient algorithm for fast intra prediction for enhancement layer in SHVC. First, temporal and spatial correlations, as well as their correlation degrees, are combined in a Naive Bayes classifier to predict depth probabilities and skip depths with low likelihood. Second, for a given depth candidate, we combine ILR mode probability with Partial Zero Blocks (PZBs) based on the Sum of Squared Differences (SSD) to determine whether the ILR mode is the best one. In that case, we can skip intra prediction, which requires very high complexity. Third, initial Intra Modes (IMs) are obtained through Sobel operator, and are combined with the relationship between IMs and their corresponding Hadamard Cost (HC) values to predict candidate IMs in Rough Mode Decision (RMD). Then, an analytical criterion of early termination is developed based on the HC values of two neighboring IMs in the Rate-Distortion Optimization (RDO) process. Finally, we combine depth probabilities and the distribution of residual coefficients at the current depth to early terminate depth selection. The proposed scheme can significantly decrease the complexity of depth determination while reducing the complexity of mode decision for a depth candidate. Our experimental results demonstrate that the proposed scheme can achieve a speed up gain of more than 80% in average, while maintaining coding efficiency.																	1057-7149	1941-0042					2020	29						6136	6150		10.1109/TIP.2020.2988167													
J								Joint Angular Refinement and Reconstruction for Single-Particle Cryo-EM	IEEE TRANSACTIONS ON IMAGE PROCESSING										Single-particle cryo-EM; joint reconstruction; continuous angular refinement; ADMM; gradient descent	3-DIMENSIONAL RECONSTRUCTION; ELECTRON-MICROSCOPY; MAXIMUM-LIKELIHOOD; PROJECTIONS; RESOLUTION	Single-particle cryo-electron microscopy (cryo-EM) reconstructs the three-dimensional (3D) structure of bio-molecules from a large set of 2D projection images with random and unknown orientations. A crucial step in the single-particle cryo-EM pipeline is 3D refinement, which resolves a high-resolution 3D structure from an initial approximate volume by refining the estimation of the orientation of each projection. In this work, we propose a new approach that refines the projection angles on the continuum. We formulate the optimization problem over the density map and the orientations jointly. The density map is updated using the efficient alternating-direction method of multipliers, while the orientations are updated through a semi-coordinate-wise gradient descent for which we provide an explicit derivation of the gradient. Our method eliminates the requirement for a fine discretization of the orientation space and does away with the classical but computationally expensive template-matching step. Numerical results demonstrate the feasibility and performance of our approach compared to several baselines.																	1057-7149	1941-0042					2020	29						6151	6163		10.1109/TIP.2020.2984313													
J								Back-Projection Based Fidelity Term for Ill-Posed Linear Inverse Problems	IEEE TRANSACTIONS ON IMAGE PROCESSING										Inverse problems; image restoration; image deblurring; image super-resolution; compressed sensing; total variation; non-convex priors; BM3D; deep generative models	THRESHOLDING ALGORITHM; IMAGE; SPARSE; SUPERRESOLUTION; REMOVAL	Ill-posed linear inverse problems appear in many image processing applications, such as deblurring, super-resolution and compressed sensing. Many restoration strategies involve minimizing a cost function, which is composed of fidelity and prior terms, balanced by a regularization parameter. While a vast amount of research has been focused on different prior models, the fidelity term is almost always chosen to be the least squares (LS) objective, that encourages fitting the linearly transformed optimization variable to the observations. In this paper, we examine a different fidelity term, which has been implicitly used by the recently proposed iterative denoising and backward projections (IDBP) framework. This term encourages agreement between the projection of the optimization variable onto the row space of the linear operator and the pseudo-inverse of the linear operator ("back-projection") applied on the observations. We analytically examine the difference between the two fidelity terms for Tikhonov regularization and identify cases (such as a badly conditioned linear operator) where the new term has an advantage over the standard LS one. Moreover, we demonstrate empirically that the behavior of the two induced cost functions for sophisticated convex and non-convex priors, such as total-variation, BM3D, and deep generative models, correlates with the obtained theoretical analysis.																	1057-7149	1941-0042					2020	29						6164	6179		10.1109/TIP.2020.2988779													
J								Characterizing Generalized Rate-Distortion Performance of Video Coding: An Eigen Analysis Approach	IEEE TRANSACTIONS ON IMAGE PROCESSING										Rate-distortion function; video quality assessment; quadratic programming	IMAGE QUALITY ASSESSMENT; EFFICIENCY; MODEL	Rate-distortion (RD) theory is at the heart of lossy data compression. Here we aim to model the generalized RD (GRD) trade-off between the visual quality of a compressed video and its encoding profiles (e.g., bitrate and spatial resolution). We first define the theoretical functional space W of the GRD function by analyzing its mathematical properties. We show that W is a convex set in a Hilbert space, inspiring a computational model of the GRD function, and a method of estimating model parameters from sparse measurements. To demonstrate the feasibility of our idea, we collect a large-scale database of real-world GRD functions, which turn out to live in a low-dimensional subspace of W. Combining the GRD reconstruction framework and the learned low-dimensional space, we create a low-parameter eigen GRD method to accurately estimate the GRD function of a source video content from only a few queries. Experimental results on the database show that the learned GRD method significantly outperforms state-of-the-art empirical RD estimation methods both in accuracy and efficiency. Last, we demonstrate the promise of the proposed model in video codec comparison.																	1057-7149	1941-0042					2020	29						6180	6193		10.1109/TIP.2020.2988437													
J								Dual-Branch Network With a Subtle Motion Detector for Microaction Recognition in Videos	IEEE TRANSACTIONS ON IMAGE PROCESSING										Action recognition; microactions; dual-branch network; subtle motion detector; attention	REPRESENTATION; HISTOGRAMS; FLOW	By involving only subtle motions of body parts, video-based microaction recognition is a very important but challenging problem. Most existing action recognition methods are developed for general actions, and the current state-of-the-art methods usually largely rely on high-layer features learned from convolutional neural networks (CNNs). High-layer CNN features usually contain more semantic information but less detailed information. However, detailed information can be important for microactions due to the motion subtleness of such actions. In this paper, we propose to more effectively learn midlayer CNN features for enhancing microaction recognition. More specifically, we develop a new dual-branch network for microaction recognition: one branch uses the high-layer CNN features for classification, and the second branch further explores the midlayer CNN features for classification. In the second branch, we introduce a novel subtle motion detector consisting of three modules: 1) a discriminative spatial-temporal feature learning module, which further learns the subtle motion features corresponding to the discriminative spatial-temporal regions, 2) a parallel multiplier attention module, which further refines the features learned in channels and spatial-temporal domains, and 3) an activation fusion module, which fuses the max and average activations from midlayer CNN features for classification. In the experiments, we build a new microaction video dataset, where the micromotions of interest are mixed with other larger general motions such as walking. Comprehensive experimental results verify that the proposed method yields new state-of-the-art performance in two microaction video datasets, while its performance on two general-action video datasets is also very promising.																	1057-7149	1941-0042					2020	29						6194	6208		10.1109/TIP.2020.2989864													
J								Video Captioning With Object-Aware Spatio-Temporal Correlation and Aggregation	IEEE TRANSACTIONS ON IMAGE PROCESSING										Video captioning; spatio-temporal graph; bidirectional temporal graph; spatial relation graph; object-aware feature aggregation		Video captioning is a significant challenging task in computer vision and natural language processing, aiming to automatically describe video content by natural language sentences. Comprehensive understanding of video is the key for accurate video captioning, which needs to not only capture the global content and salient objects in video, but also understand the spatio-temporal relations of objects, including their temporal trajectories and spatial relationships. Thus, it is important for video captioning to capture the objects' relationships both within and across frames. Therefore, in this paper, we propose an object-aware spatio-temporal graph (OSTG) approach for video captioning. It constructs spatio-temporal graphs to depict objects with their relations, where the temporal graphs represent objects' inter-frame dynamics, and the spatial graphs represent objects' intra-frame interactive relationships. The main novelties and advantages are: (1) Bidirectional temporal alignment: Bidirectional temporal graph is constructed along and reversely along the temporal order to perform bidirectional temporal alignment for objects across different frames, which provides complementary clues to capture the inter-frame temporal trajectories for each salient object. (2) Graph based spatial relation learning: Spatial relation graph is constructed among objects in each frame by considering their relative spatial locations and semantic correlations, which is exploited to learn relation features that encode intra-frame relationships for salient objects. (3) Object-aware feature aggregation: Trainable VLAD (vector of locally aggregated descriptors) models are deployed to perform object-aware feature aggregation on objects' local features, which learn discriminative aggregated representations for better video captioning. A hierarchical attention mechanism is also developed to distinguish contributions of different object instances. Experiments on two widely-used datasets, MSR-VTT and MSVD, demonstrate our proposed approach achieves state-of-the-art performances in terms of BLEU@4, METEOR and CIDEr metrics.																	1057-7149	1941-0042					2020	29						6209	6222		10.1109/TIP.2020.2988435													
J								ASNets: Deep Learning for Generalised Planning	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH												In this paper, we discuss the learning of generalised policies for probabilistic and classical planning problems using Action Schema Networks (ASNets). The ASNet is a neural network architecture that exploits the relational structure of (P)PDDL planning problems to learn a common set of weights that can be applied to any problem in a domain. By mimicking the actions chosen by a traditional, non-learning planner on a handful of small problems in a domain, ASNets are able to learn a generalised reactive policy that can quickly solve much larger instances from the domain. This work extends the ASNet architecture to make it more expressive, while still remaining invariant to a range of symmetries that exist in PPDDL problems. We also present a thorough experimental evaluation of ASNets, including a comparison with heuristic search planners on seven probabilistic and deterministic domains, an extended evaluation on over 18,000 Blocksworld instances, and an ablation study. Finally, we show that sparsity-inducing regularisation can produce ASNets that are compact enough for humans to understand, yielding insights into how the structure of ASNets allows them to generalise across a domain.																	1076-9757	1943-5037					2020	68						1	68															
J								Vocabulary Alignment in Openly Specified Interactions	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH											MODEL	The problem of achieving common understanding between agents that use different vocabularies has been mainly addressed by techniques that assume the existence of shared external elements, such as a meta-language or a physical environment. In this article, we consider agents that use different vocabularies and only share knowledge of how to perform a task, given by the specification of an interaction protocol. We present a framework that lets agents learn a vocabulary alignment from the experience of interacting. Unlike previous work in this direction, we use open protocols that constrain possible actions instead of defining procedures, making our approach more general. We present two techniques that can be used either to learn an alignment from scratch or to repair an existent one, and we evaluate their performance experimentally.																	1076-9757	1943-5037					2020	68						69	107															
J								Variational Bayes In Private Settings (VIPS)	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH											INFERENCE; NOISE	Many applications of Bayesian data analysis involve sensitive information such as personal documents or medical records, motivating methods which ensure that privacy is protected. We introduce a general privacy-preserving framework for Variational Bayes (VB), a widely used optimization-based Bayesian inference method. Our framework respects differential privacy, the gold-standard privacy criterion, and encompasses a large class of probabilistic models, called the Conjugate Exponential (CE) family. We observe that we can straightforwardly privatise VB's approximate posterior distributions for models in the CE family, by perturbing the expected sufficient statistics of the complete-data likelihood. For a broadly-used class of non-CE models, those with binomial likelihoods, we show how to bring such models into the CE family, such that inferences in the modified model resemble the private variational Bayes algorithm as closely as possible, using the Polya-Gamma data augmentation scheme. The iterative nature of variational Bayes presents a further challenge since iterations increase the amount of noise needed. We overcome this by combining: (1) an improved composition method for differential privacy, called the moments accountant, which provides a tight bound on the privacy cost of multiple VB iterations and thus significantly decreases the amount of additive noise; and (2) the privacy amplification effect of subsampling mini-batches from large-scale data in stochastic learning. We empirically demonstrate the effectiveness of our method in CE and non-CE models including latent Dirichlet allocation, Bayesian logistic regression, and sigmoid belief networks, evaluated on real-world datasets.																	1076-9757	1943-5037					2020	68						109	157															
J								Towards Knowledgeable Supervised Lifelong Learning Systems	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH											SUPPORT; MODEL; SVM	Learning a sequence of tasks is a long-standing challenge in machine learning. This setting applies to learning systems that observe examples of a range of tasks at different points in time. A learning system should become more knowledgeable as more related tasks are learned. Although the problem of learning sequentially was acknowledged for the first time decades ago, the research in this area has been rather limited. Research in transfer learning, multitask learning, metalearning and deep learning has studied some challenges of these kinds of systems. Recent research in lifelong machine learning and continual learning has revived interest in this problem. We propose Proficiente, a full framework for long-term learning systems. Proficiente relies on knowledge transferred between hypotheses learned with Support Vector Machines. The first component of the framework is focused on transferring forward selectively from a set of existing hypotheses or functions representing knowledge acquired during previous tasks to a new target task. A second component of Proficiente is focused on transferring backward, a novel ability of long-term learning systems that aim to exploit knowledge derived from recent tasks to encourage refinement of existing knowledge. We propose a method that transfers selectively from a task learned recently to existing hypotheses representing previous tasks. The method encourages retention of existing knowledge whilst refining. We analyse the theoretical properties of the proposed framework. Proficiente is accompanied by an agnostic metric that can be used to determine if a long-term learning system is becoming more knowledgeable. We evaluate Proficiente in both synthetic and real-world datasets, and demonstrate scenarios where knowledgeable supervised learning systems can be achieved by means of transfer.																	1076-9757	1943-5037					2020	68						159	224															
J								Non-Lambertian Photometric Stereo Network Based on Inverse Reflectance Model With Collocated Light	IEEE TRANSACTIONS ON IMAGE PROCESSING										Reflectivity; Estimation; Surface treatment; Surface fitting; Machine learning; Feature extraction; Mathematical model; Inverse reflectance model; collocated light; non-Lambertian photometric stereo network	SHAPE; IMAGES	Current non-Lambertian photometric stereo methods generally require a large number of images to ensure accurate surface normal estimation. To achieve accurate surface normal recovery under a sparse set of lights, this paper proposes a non-Lambertian photometric stereo network based on a derived inverse reflectance model with collocated light. The model is deduced using monotonicity of isotropic reflectance and the univariate property of collocated light to decouple the surface normal from the reflectance function. Thus, the surface normal can be estimated by three steps, i.e., model fitting, shadow rejection, and normal estimation. We leverage a supervised deep learning technique to enhance the shadow rejection ability and the flexibility of the inverse reflectance model. Shadows are handled through max-pooling. Information from a neighborhood image patch is utilized to improve the flexibility to various reflectances. Experiments using both synthetic and real images demonstrate that the proposed method achieves state-of-the-art accuracy in surface normal estimation.																	1057-7149	1941-0042					2020	29						6032	6042		10.1109/TIP.2020.2987176													
J								An Optimized Quantization Constraints Set for Image Restoration and its GPU Implementation	IEEE TRANSACTIONS ON IMAGE PROCESSING										Discrete cosine transforms; Image restoration; Quantization (signal); Graphics processing units; Signal to noise ratio; Optimization; Image coding; Image restoration; optimized quantization constraints set; GPU parallelism	JPEG DECOMPRESSION; BLOCKING ARTIFACTS; REDUCING ARTIFACTS; CODED IMAGES; REDUCTION; PROJECTION; NOISE	This paper presents a novel optimized quantization constraint set, acting as an add-on to existing DCT-based image restoration algorithms. The constraint set is created based on generalized Gaussian distribution which is more accurate than the commonly used uniform, Gaussian or Laplacian distributions when modeling DCT coefficients. More importantly, the proposed constraint set is optimized for individual input images and thus it is able to enhance image quality significantly in terms of signal-to-noise ratio. Experimental results indicate that the signal-to-noise ratio is improved by at least 6.78% on top of the existing state-of-the-art methods, with a corresponding expense of only 0.38% in processing time. The proposed algorithm has also been implemented in GPU, and the processing speed increases further by 20 times over that of CPU implementation. This makes the algorithm well suited for fast image retrieval in security and quality monitoring system.																	1057-7149	1941-0042					2020	29						6043	6053		10.1109/TIP.2020.2988131													
J								Study of Subjective and Objective Quality Assessment of Audio-Visual Signals	IEEE TRANSACTIONS ON IMAGE PROCESSING										Streaming media; Distortion; Databases; Quality assessment; Quality of experience; Video recording; Predictive models; Quality assessment; audio-visual quality; video quality; audio quality; multimodal fusion	AUDIO QUALITY; VIDEO QUALITY; STRUCTURAL SIMILARITY; PERCEPTUAL IMAGE; MASKING; SPEECH	The topics of visual and audio quality assessment (QA) have been widely researched for decades, yet nearly all of this prior work has focused only on single-mode visual or audio signals. However, visual signals rarely are presented without accompanying audio, including heavy-bandwidth video streaming applications. Moreover, the distortions that may separately (or conjointly) afflict the visual and audio signals collectively shape user-perceived quality of experience (QoE). This motivated us to conduct a subjective study of audio and video (A/V) quality, which we then used to compare and develop A/V quality measurement models and algorithms. The new LIVE-SJTU Audio and Video Quality Assessment (A/V-QA) Database includes 336 A/V sequences that were generated from 14 original source contents by applying 24 different A/V distortion combinations on them. We then conducted a subjective A/V quality perception study on the database towards attaining a better understanding of how humans perceive the overall combined quality of A/V signals. We also designed four different families of objective A/V quality prediction models, using a multimodal fusion strategy. The different types of A/V quality models differ in both the unimodal audio and video quality prediction models comprising the direct signal measurements and in the way that the two perceptual signal modes are combined. The objective models are built using both existing state-of-the-art audio and video quality prediction models and some new prediction models, as well as quality-predictive features delivered by a deep neural network. The methods of fusing audio and video quality predictions that are considered include simple product combinations as well as learned mappings. Using the new subjective A/V database as a tool, we validated and tested all of the objective A/V quality prediction models. We will make the database publicly available to facilitate further research.																	1057-7149	1941-0042					2020	29						6054	6068		10.1109/TIP.2020.2988148													
J								Steerable ePCA: Rotationally Invariant Exponential Family PCA	IEEE TRANSACTIONS ON IMAGE PROCESSING										Poisson noise; X-ray free electron laser; steerable PCA; eigenvalue shrinkage; autocorrelation analysis; image denoising	PRINCIPAL-COMPONENTS; FACE RECOGNITION; ROTATED IMAGES; RECONSTRUCTION; APPROXIMATION; EIGENVALUES; DIFFRACTION; CONSISTENCY	In photon-limited imaging, the pixel intensities are affected by photon count noise. Many applications require an accurate estimation of the covariance of the underlying 2-D clean images. For example, in X-ray free electron laser (XFEL) single molecule imaging, the covariance matrix of 2-D diffraction images is used to reconstruct the 3-D molecular structure. Accurate estimation of the covariance from low-photon-count images must take into account that pixel intensities are Poisson distributed, hence the classical sample covariance estimator is highly biased. Moreover, in single molecule imaging, including in-plane rotated copies of all images could further improve the accuracy of covariance estimation. In this paper we introduce an efficient and accurate algorithm for covariance matrix estimation of count noise 2-D images, including their uniform planar rotations and possibly reflections. Our procedure, steerable $e$ PCA, combines in a novel way two recently introduced innovations. The first is a methodology for principal component analysis (PCA) for Poisson distributions, and more generally, exponential family distributions, called $e$ PCA. The second is steerable PCA, a fast and accurate procedure for including all planar rotations when performing PCA. The resulting principal components are invariant to the rotation and reflection of the input images. We demonstrate the efficiency and accuracy of steerable $e$ PCA in numerical experiments involving simulated XFEL datasets and rotated face images from Yale Face Database B.																	1057-7149	1941-0042					2020	29						6069	6081		10.1109/TIP.2020.2988139													
J								Gaussian Lifting for Fast Bilateral and Nonlocal Means Filtering	IEEE TRANSACTIONS ON IMAGE PROCESSING										Kernel; Lattices; Pipelines; Acceleration; Manifolds; Iron; Convolution; Nonlinear filtering; inverse problems; Gaussian pyramids; wavelet transforms	IMAGE; PHOTOGRAPHY; ENHANCEMENT; ALGORITHM; FLASH; ORDER	Recently, many fast implementations of the bilateral and the nonlocal filters were proposed based on lattice and vector quantization, e.g. clustering, in higher dimensions. However, these approaches can still be inefficient owing to the complexities in the resampling process or in filtering the high-dimensional resampled signal. In contrast, simply scalar resampling the high-dimensional signal after decorrelation presents the opportunity to filter signals using multi-rate signal processing techniques. This work proposes the Gaussian lifting framework for efficient and accurate bilateral and nonlocal means filtering, appealing to the similarities between separable wavelet transforms and Gaussian pyramids. Accurately implementing the filter is important not only for image processing applications, but also for a number of recently proposed bilateral-regularized inverse problems, where the accuracy of the solutions depends ultimately on accurate filter implementations. We show that our Gaussian lifting approach filters images more accurately and efficiently across many filter scales. Adaptive lifting schemes for bilateral and nonlocal means filtering are also explored.																	1057-7149	1941-0042					2020	29						6082	6095		10.1109/TIP.2020.2984357													
J								Receptive Multi-Granularity Representation for Person Re-Identification	IEEE TRANSACTIONS ON IMAGE PROCESSING										Feature extraction; Task analysis; Robustness; Neurons; Machine learning; Semantics; Adaptation models; Person re-identification; multiple granularity learning; local feature learning; convolutional neural networks	ATTRIBUTE; NETWORK	A key for person re-identification is achieving consistent local details for discriminative representation across variable environments. Current stripe-based feature learning approaches have delivered impressive accuracy, but do not make a proper trade-off between diversity, locality, and robustness, which easily suffers from part semantic inconsistency for the conflict between rigid partition and misalignment. This paper proposes a receptive multi-granularity learning approach to facilitate stripe-based feature learning. This approach performs local partition on the intermediate representations to operate receptive region ranges, rather than current approaches on input images or output features, thus can enhance the representation of locality while remaining proper local association. Toward this end, the local partitions are adaptively pooled by using significance-balanced activations for uniform stripes. Random shifting augmentation is further introduced for a higher variance of person appearing regions within bounding boxes to ease misalignment. By two-branch network architecture, different scales of discriminative identity representation can be learned. In this way, our model can provide a more comprehensive and efficient feature representation without larger model storage costs. Extensive experiments on intra-dataset and cross-dataset evaluations demonstrate the effectiveness of the proposed approach. Especially, our approach achieves a state-of-the-art accuracy of 96.2%@Rank-1 or 90.0%@mAP on the challenging Market-1501 benchmark.																	1057-7149	1941-0042					2020	29						6096	6109		10.1109/TIP.2020.2986878													
J								High-Quality Proposals for Weakly Supervised Object Detection	IEEE TRANSACTIONS ON IMAGE PROCESSING										Proposals; Training; Detectors; Object detection; Search problems; Task analysis; Convolutional neural networks; Weakly supervised object detection (WSOD); proposal generation; proposal selection; convolutional neural networks (CNNs)	LOCALIZATION; NETWORKS	Despite significant efforts made so far for Weakly Supervised Object Detection (WSOD), proposal generation and proposal selection are still two major challenges. In this paper, we focus on addressing the two challenges by generating and selecting high-quality proposals. To be specific, for proposal generation, we combine selective search and a Gradient-weighted Class Activation Mapping (Grad-CAM) based technique to generate more proposals having higher Intersection-Over-Union (IOU) with ground truth boxes than those obtained by greedy search approaches, which can better envelop the entire objects. As regards proposal selection, for each object class, we choose as many confident positive proposals as possible and meanwhile only select class-specific hard negatives to focus training on more discriminative negative proposals by up-weighting their losses, which can make training more effective. The proposed proposal generation and proposal selection approaches are generic and thus can be broadly applied to many WSOD methods. In this work, we unify them into the framework of Online Instance Classifier Refinement (OICR). Experimental results on the PASCAL VOC 2007 and 2012 datasets and MS COCO dataset demonstrate that our method significantly improves the baseline method OICR by large margins (13.4% mAP and 11.6% CorLoc gains on the VOC 2007 dataset, 15.0% mAP and 8.9% CorLoc gains on the VOC 2012 dataset, and 6.4% mAP and 5.0% CorLoc gains on the COCO dataset) and achieves the state-of-the-art results compared with existing methods.																	1057-7149	1941-0042					2020	29						5794	5804		10.1109/TIP.2020.2987161													
J								Fast Multi-Scale Structural Patch Decomposition for Multi-Exposure Image Fusion	IEEE TRANSACTIONS ON IMAGE PROCESSING										Multi-exposure fusion; high dynamic range imaging; computational photography	QUALITY ASSESSMENT	Exposure bracketing is crucial to high dynamic range imaging, but it is prone to halos for static scenes and ghosting artifacts for dynamic scenes. The recently proposed structural patch decomposition for multi-exposure fusion (SPD-MEF) has achieved reliable performance in deghosting, but suffers from visible halo artifacts and is computationally expensive. In addition, its relationship to other MEF methods is unclear. We show that without explicitly performing structural patch decomposition, we arrive at an unnormalized version of SPD-MEF, which enjoys an order of $30\times $ speed-up, and is closely related to pixel-level MEF methods as well as the standard two-layer decomposition method for MEF. Moreover, we develop a fast multi-scale SPD-MEF method, which can effectively reduce halo artifacts. Experimental results demonstrate the effectiveness of the proposed MEF method in terms of speed and quality.																	1057-7149	1941-0042					2020	29						5805	5816		10.1109/TIP.2020.2987133													
J								A Joint Label Space for Generalized Zero-Shot Classification	IEEE TRANSACTIONS ON IMAGE PROCESSING										Visualization; Semantics; Prototypes; Correlation; Training; Testing; Cats; Projection learning; generalized zero-shot learning; label space		The fundamental problem of Zero-Shot Learning (ZSL) is that the one-hot label space is discrete, which leads to a complete loss of the relationships between seen and unseen classes. Conventional approaches rely on using semantic auxiliary information, e.g. attributes, to re-encode each class so as to preserve the inter-class associations. However, existing learning algorithms only focus on unifying visual and semantic spaces without jointly considering the label space. More importantly, because the final classification is conducted in the label space through a compatibility function, the gap between attribute and label spaces leads to significant performance degradation. Therefore, this paper proposes a novel pathway that uses the label space to jointly reconcile visual and semantic spaces directly, which is named Attributing Label Space (ALS). In the training phase, one-hot labels of seen classes are directly used as prototypes in a common space, where both images and attributes are mapped. Since mappings can be optimized independently, the computational complexity is extremely low. In addition, the correlation between semantic attributes has less influence on visual embedding training because features are mapped into labels instead of attributes. In the testing phase, the discrete condition of label space is removed, and priori one-hot labels are used to denote seen classes and further compose labels of unseen classes. Therefore, the label space is very discriminative for the Generalized ZSL (GZSL), which is more reasonable and challenging for real-world applications. Extensive experiments on five benchmarks manifest improved performance over all of compared state-of-the-art methods.																	1057-7149	1941-0042					2020	29						5817	5831		10.1109/TIP.2020.2986892													
J								An Exact and Fast CBCT Reconstruction via Pseudo-Polar Fourier Transform-Based Discrete Grangeat's Formula	IEEE TRANSACTIONS ON IMAGE PROCESSING										Cone beam; radon; pseudo polar fast Fourier transform; cone to radon; Grangeat's formula	CONE-BEAM PROJECTIONS; IMAGE-RECONSTRUCTION; INVERSION	The recent application of Fourier Based Iterative Reconstruction Method (FIRM) has made it possible to achieve high-quality 2D images from a fan beam Computed Tomography (CT) scan with a limited number of projections in a fast manner. The proposed methodology in this article is designed to provide 3D Radon space in linogram fashion to facilitate the use of FIRM with cone beam projections (CBP) for the reconstruction of 3D images in a sparse view angles Cone Beam CT (CBCT). For this reason, in the first phase, the 3D Radon space is generated using CBP data after discretization and optimization of the famous Grangeat's formula. The method used in this process involves fast Pseudo Polar Fourier transform (PPFT) based on 2D and 3D Discrete Radon Transformation (DRT) algorithms with no wraparound effects. In the second phase, we describe reconstruction of the objects with available Radon values, using direct inverse of 3D PPFT. The method presented in this section eliminates noises caused by interpolation from polar to Cartesian space and exhibits no thorn, V-shaped and wrinkle artifacts. This method reduces the complexity to O(n(3) log n) for images of size $ {{\mathrm {n}}\times {\mathrm {n}} \times {\mathrm {n}}}$ . The Cone to Radon conversion (Cone2Radon) Toolbox in the first phase and MATLAB/Python toolbox in the second phase were tested on three digital phantoms and experiments demonstrate fast and accurate cone beam image reconstruction due to proposed modifications in all three stages of Grangeat's method.																	1057-7149	1941-0042					2020	29						5832	5847		10.1109/TIP.2020.2985874													
J								Spatially-Variant CNN-Based Point Spread Function Estimation for Blind Deconvolution and Depth Estimation in Optical Microscopy	IEEE TRANSACTIONS ON IMAGE PROCESSING										Microscopy; point spread function estimation; convolutional neural networks; blind deconvolution; depth from focus	IMAGE; RECONSTRUCTION; APPROXIMATIONS; CONTRAST; MODEL	Optical microscopy is an essential tool in biology and medicine. Imaging thin, yet non-flat objects in a single shot (without relying on more sophisticated sectioning setups) remains challenging as the shallow depth of field that comes with high-resolution microscopes leads to unsharp image regions and makes depth localization and quantitative image interpretation difficult. Here, we present a method that improves the resolution of light microscopy images of such objects by locally estimating image distortion while jointly estimating object distance to the focal plane. Specifically, we estimate the parameters of a spatially-variant Point Spread Function (PSF) model using a Convolutional Neural Network (CNN), which does not require instrument- or object-specific calibration. Our method recovers PSF parameters from the image itself with up to a squared Pearson correlation coefficient of 0.99 in ideal conditions, while remaining robust to object rotation, illumination variations, or photon noise. When the recovered PSFs are used with a spatially-variant and regularized Richardson-Lucy (RL) deconvolution algorithm, we observed up to 2.1 dB better Signal-to-Noise Ratio (SNR) compared to other Blind Deconvolution (BD) techniques. Following microscope-specific calibration, we further demonstrate that the recovered PSF model parameters permit estimating surface depth with a precision of $\mathrm {2 \mu \text {m} }$ and over an extended range when using engineered PSFs. Our method opens up multiple possibilities for enhancing images of non-flat objects with minimal need for a priori knowledge about the optical setup.																	1057-7149	1941-0042					2020	29						5848	5861		10.1109/TIP.2020.2986880													
J								LR3M: Robust Low-Light Enhancement via Low-Rank Regularized Retinex Model	IEEE TRANSACTIONS ON IMAGE PROCESSING										Lighting; Robustness; Noise reduction; Histograms; Minimization; Visualization; Estimation; Low-light enhancement; denoising; retinex model; low-rank decomposition	MATRIX COMPLETION; ALGORITHM; ILLUMINATION	Noise causes unpleasant visual effects in low-light image/video enhancement. In this paper, we aim to make the enhancement model and method aware of noise in the whole process. To deal with heavy noise which is not handled in previous methods, we introduce a robust low-light enhancement approach, aiming at well enhancing low-light images/videos and suppressing intensive noise jointly. Our method is based on the proposed Low-Rank Regularized Retinex Model (LR3M), which is the first to inject low-rank prior into a Retinex decomposition process to suppress noise in the reflectance map. Our method estimates a piece-wise smoothed illumination and a noise-suppressed reflectance sequentially, avoiding remaining noise in the illumination and reflectance maps which are usually presented in alternative decomposition methods. After getting the estimated illumination and reflectance, we adjust the illumination layer and generate our enhancement result. Furthermore, we apply our LR3M to video low-light enhancement. We consider inter-frame coherence of illumination maps and find similar patches through reflectance maps of successive frames to form the low-rank prior to make use of temporal correspondence. Our method performs well for a wide variety of images and videos, and achieves better quality both in enhancing and denoising, compared with the state-of-the-art methods.																	1057-7149	1941-0042					2020	29						5862	5876		10.1109/TIP.2020.2984098													
J								Scene Recognition With Prototype-Agnostic Scene Layout	IEEE TRANSACTIONS ON IMAGE PROCESSING										Layout; Semantics; Prototypes; Image recognition; Convolution; Neural networks; Deformable models; Scene classification; convolution neural networks; graph neural networks; scene layout	NETWORK	Exploiting the spatial structure in scene images is a key research direction for scene recognition. Due to the large intra-class structural diversity, building and modeling flexible structural layout to adapt various image characteristics is a challenge. Existing structural modeling methods in scene recognition either focus on predefined grids or rely on learned prototypes, which all have limited representative ability. In this paper, we propose Prototype-agnostic Scene Layout (PaSL) construction method to build the spatial structure for each image without conforming to any prototype. Our PaSL can flexibly capture the diverse spatial characteristic of scene images and have considerable generalization capability. Given a PaSL, we build Layout Graph Network (LGN) where regions in PaSL are defined as nodes and two kinds of independent relations between regions are encoded as edges. The LGN aims to incorporate two topological structures (formed in spatial and semantic similarity dimensions) into image representations through graph convolution. Extensive experiments show that our approach achieves state-of-the-art results on widely recognized MIT67 and SUN397 datasets without multi-model or multi-scale fusion. Moreover, we also conduct the experiments on one of the largest scale datasets, Places365. The results demonstrate the proposed method can be well generalized and obtains competitive performance.																	1057-7149	1941-0042					2020	29						5877	5888		10.1109/TIP.2020.2986599													
J								Query-Biased Self-Attentive Network for Query-Focused Video Summarization	IEEE TRANSACTIONS ON IMAGE PROCESSING										Task analysis; Semantics; Visualization; Computational modeling; Generators; Benchmark testing; Instruments; Video summarization; vision and language; self-attention mechanism	EGOCENTRIC VIDEO	This paper addresses the task of query-focused video summarization, which takes user queries and long videos as inputs and generates query-focused video summaries. Compared to video summarization, which mainly concentrates on finding the most diverse and representative visual contents as a summary, the task of query-focused video summarization considers the user's intent and the semantic meaning of generated summary. In this paper, we propose a method, named query-biased self-attentive network (QSAN) to tackle this challenge. Our key idea is to utilize the semantic information from video descriptions to generate a generic summary and then to combine the information from the query to generate a query-focused summary. Specifically, we first propose a hierarchical self-attentive network to model the relative relationship at three levels, which are different frames from a segment, different segments of the same video, textual information of video description and its related visual contents. We train the model on video caption dataset and employ a reinforced caption generator to generate a video description, which can help us locate important frames or shots. Then we build a query-aware scoring module to compute the query-relevant score for each shot and generate the query-focused summary. Extensive experiments on the benchmark dataset demonstrate the competitive performance of our approach compared to some methods.																	1057-7149	1941-0042					2020	29						5889	5899		10.1109/TIP.2020.2985868													
J								Efficient and Effective Context-Based Convolutional Entropy Modeling for Image Compression	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image coding; Entropy; Computational modeling; Transforms; Convolutional codes; Context modeling; Convolution; Context-based convolutional networks; entropy modeling; image compression		Precise estimation of the probabilistic structure of natural images plays an essential role in image compression. Despite the recent remarkable success of end-to-end optimized image compression, the latent codes are usually assumed to be fully statistically factorized in order to simplify entropy modeling. However, this assumption generally does not hold true and may hinder compression performance. Here we present context-based convolutional networks (CCNs) for efficient and effective entropy modeling. In particular, a 3D zigzag scanning order and a 3D code dividing technique are introduced to define proper coding contexts for parallel entropy decoding, both of which boil down to place translation-invariant binary masks on convolution filters of CCNs. We demonstrate the promise of CCNs for entropy modeling in both lossless and lossy image compression. For the former, we directly apply a CCN to the binarized representation of an image to compute the Bernoulli distribution of each code for entropy estimation. For the latter, the categorical distribution of each code is represented by a discretized mixture of Gaussian distributions, whose parameters are estimated by three CCNs. We then jointly optimize the CCN-based entropy model along with analysis and synthesis transforms for rate-distortion performance. Experiments on the Kodak and Tecnick datasets show that our methods powered by the proposed CCNs generally achieve comparable compression performance to the state-of-the-art while being much faster.																	1057-7149	1941-0042					2020	29						5900	5911		10.1109/TIP.2020.2985225													
J								Deep Recognition of Vanishing-Point-Constrained Building Planes in Urban Street Views	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image segmentation; Visualization; Buildings; Cameras; Logic gates; Geometry; Layout; Image segmentation; plane reconstruction; augmented reality; geometric reasoning; vanishing point; street view	AUGMENTED REALITY; LAYOUT; GENERATION	This paper presents a new approach to recognizing vanishing-point-constrained building planes from a single image of street view. We first design a novel convolutional neural network (CNN) architecture that generates geometric segmentation of per-pixel orientations from a single street-view image. The network combines two-stream features of general visual cues and surface normals in gated convolution layers, and employs a deeply supervised loss that encapsulates multi-scale convolutional features. Our experiments on a new benchmark with fine-grained plane segmentations of real-world street views show that our network outperforms state-of-the-arts methods of both semantic and geometric segmentation. The pixel-wise segmentation exhibits coarse boundaries and discontinuities. We then propose to rectify the pixel-wise segmentation into perspectively-projected quads based on spatial proximity between the segmentation masks and exterior line segments detected through an image processing. We demonstrate how the results can be utilized to perspectively overlay images and icons on building planes in input photos, and provide visual cues for various applications.																	1057-7149	1941-0042					2020	29						5912	5923		10.1109/TIP.2020.2986894													
J								ASTS: A Unified Framework for Arbitrary Shape Text Spotting	IEEE TRANSACTIONS ON IMAGE PROCESSING										Scene text spotting; arbitrary shape; Mask R-CNN; weak supervision	NEURAL-NETWORK; LOCALIZATION	Arbitrary shape text spotting remains a challenging computer vision task. In this paper, we propose an end-to-end trainable unified framework for arbitrary shape text spotting to overcome the limitations inherent in the existing methods. Specifically, we propose to perceive and understand text based on different levels of semantics, $i.e$ ., holistic-, pixel- and sequence-level semantics, and then unify the recognized semantics for robust text spotting. To implement the framework, we customize the detection and mask branches of Mask R-CNN to explore both holistic- and pixel-level semantics for text recognition. According to the recognition results, the text spotting task can then be formulated in the two-dimensional feature space. Then, by feeding the two-dimensional feature maps into an additional text recognition branch, our framework further delivers one-dimensional sequence-level semantics for text recognition based on an attention-based sequence-to-sequence network. Finally, the results from all the three levels of semantics are merged as the final result. Therefore, our framework is capable of simultaneously recognizing texts from both the one- and two-dimensional perspectives, achieving highly comprehensive text recognition. In addition, because some existing datasets lack character-level annotations, the extensive descriptions of texts from our framework further allow us to use only word-level annotations as weak supervision for training a robust text spotting model. Experiments on ICDAR 2013, ICDAR 2015, and Total-Text show that our framework achieves state-of-the-art performance for both detection and recognition.																	1057-7149	1941-0042					2020	29						5924	5936		10.1109/TIP.2020.2984082													
J								The Performance of Quality Metrics in Assessing Error-Concealed Video Quality	IEEE TRANSACTIONS ON IMAGE PROCESSING										Error; loss concealment; video quality assessment; image quality assessment	TO-NOISE RATIO; PERCEPTUAL IMAGE; SIMILARITY; PREDICTION; PSNR	In highly-interactive video streaming applications such as video conferencing, tele-presence, or tele-operation, retransmission is typically not used, due to the tight deadline of the application. In such cases, the lost or erroneous data must be concealed. While various error concealment techniques exist, there is no defined rule to compare their perceived quality. In this paper, the performance of 16 existing image and video quality metrics (PSNR, SSIM, VQM, etc.) evaluating error-concealed video quality is studied. The encoded video is subjected to packet loss and the loss is concealed using various error concealment techniques. We show that the subjective quality of the video cannot be necessarily predicted from the visual quality of the error-concealed frame alone. We then apply the metrics to the error-concealed images/videos and evaluate their success in predicting the scores reported by human subjects. The error-concealed videos are judged by image quality metrics applied on the lossy frame, or by video quality metrics applied on the video clip containing that lossy frame; this way, the impact of error propagation is also considered by the objective metrics. The measurement and comparison of the results show that, mostly though not always, measuring the objective quality of the video is a better way to judge the error concealment performance. Moreover, our experiments show that when the objective quality metrics are used for the assessment of the performance of an error concealment technique, they do not behave as they would for general quality assessment. In fact, some newly developed metrics show the correct decision only about 60% of the time, leading to an unacceptable error rate of as much as 40%. Our analysis shows which specific quality metrics are relatively more suitable for error-concealed videos.																	1057-7149	1941-0042					2020	29						5937	5952		10.1109/TIP.2020.2984356													
J								Online Tensor Sparsifying Transform Based on Temporal Superpixels From Compressive Spectral Video Measurements	IEEE TRANSACTIONS ON IMAGE PROCESSING										Sparse representation; online learning; compressive spectral video sensing; tensor sparsifying transform; tensor decomposition; temporal superpixels; spectral imaging	CODED-APERTURE DESIGN; IMAGES; FLOW	Spectral videos contain highly redundant information across spatial, spectral and temporal axes which can be exploited through a temporal-data-learned sparsifying basis. However, in compressive spectral video acquisition, tackling dictionary learning is time-consuming since it increases the computational complexity and presents drawbacks for real-time processing, where offline learning is required. This paper introduces a tensor-decomposition learning (TenDL) framework for simultaneous online sparsifying and recovering the spatial-spectral-temporal information of a spectral video performed on several temporal superpixels (TSP-TenDL) for time processing reduction. The framework is composed of two main stages: preprocessing and joint estimation. The preprocessing stage includes a strategy for a grayscale approximation of the video to provide a suitable initialization of the sparsifying basis to be learned. To fully exploit the high signal correlation, a set of temporal superpixels is estimated from the grayscale approximation, reducing the reconstruction time of the large-scale data. Then, the outcome of the first stage is used to estimate the basis and the signal coefficients, where an optimization problem is solved to learn and reconstruct the basis and the signal, respectively, following a block-descent coordinate strategy. The proposed approach is compared from simulations with an offline-learned based method, traditional matrix-based recovery algorithms and the tensor-based recovery, the two latter using a fixed basis, where TSP-TenDL exhibits higher image quality results and lower computation time. Specifically, our methodology gains up to 7dB in terms of PSNR and a speedup of up to $6.6\times $ compared with state-of-the-art counterparts.																	1057-7149	1941-0042					2020	29						5953	5963		10.1109/TIP.2020.2985871													
J								Quality Prediction on Deep Generative Images	IEEE TRANSACTIONS ON IMAGE PROCESSING										Gallium nitride; Image coding; Image quality; Generative adversarial networks; Image databases; Machine learning; Image quality assessment; GAN; SVD; the generative image database; subjective test	INFORMATION; SIMILARITY	In recent years, deep neural networks have been utilized in a wide variety of applications including image generation. In particular, generative adversarial networks (GANs) are able to produce highly realistic pictures as part of tasks such as image compression. As with standard compression, it is desirable to be able to automatically assess the perceptual quality of generative images to monitor and control the encode process. However, existing image quality algorithms are ineffective on GAN generated content, especially on textured regions and at high compressions. Here we propose a new "naturalness"-based image quality predictor for generative images. Our new GAN picture quality predictor is built using a multi-stage parallel boosting system based on structural similarity features and measurements of statistical similarity. To enable model development and testing, we also constructed a subjective GAN image quality database containing (distorted) GAN images and collected human opinions of them. Our experimental results indicate that our proposed GAN IQA model delivers superior quality predictions on the generative image datasets, as well as on traditional image quality datasets.																	1057-7149	1941-0042					2020	29						5964	5979		10.1109/TIP.2020.2987180													
J								PBR-Net: Imitating Physically Based Rendering Using Deep Neural Network	IEEE TRANSACTIONS ON IMAGE PROCESSING										Rendering (computer graphics); Lighting; Machine learning; Neural networks; Two dimensional displays; Cameras; Light sources; Physically based rendering; intrinsic image; stacked neural network; shading; modified perceptual loss	RETINEX	Physically based rendering has been widely used to generate photo-realistic images, which greatly impacts industry by providing appealing rendering, such as for entertainment and augmented reality, and academia by serving large scale high-fidelity synthetic training data for data hungry methods like deep learning. However, physically based rendering heavily relies on ray-tracing, which can be computational expensive in complicated environment and hard to parallelize. In this paper, we propose an end-to-end deep learning based approach to generate physically based rendering efficiently. Our system consists of two stacked neural networks, which effectively simulates the physical behavior of the rendering process and produces photo-realistic images. The first network, namely shading network, is designed to predict the optimal shading image from surface normal, depth and illumination; the second network, namely composition network, learns to combine the predicted shading image with the reflectance to generate the final result. Our approach is inspired by intrinsic image decomposition, and thus it is more physically reasonable to have shading as intermediate supervision. Extensive experiments show that our approach is robust to noise thanks to a modified perceptual loss and even outperforms the physically based rendering systems in complex scenes given a reasonable time budget.																	1057-7149	1941-0042					2020	29						5980	5992		10.1109/TIP.2020.2987169													
J								High-ISO Long-Exposure Image Denoising Based on Quantitative Blob Characterization	IEEE TRANSACTIONS ON IMAGE PROCESSING										Noise reduction; Kernel; Blob detection; Image denoising; Task analysis; Image reconstruction; Noise measurement; Image denoising; real-world noise; high-ISO long-exposure images; blob detection; blob characterization; second-order Gaussian kernel	GENERALIZED LAPLACIAN; NOISE-REDUCTION; EDGE-DETECTION; DECOMPOSITION	Blob detection and image denoising are fundamental, sometimes related tasks in computer vision. In this paper, we present a computational method to quantitatively measure blob characteristics using normalized unilateral second-order Gaussian kernels. This method suppresses non-blob structures while yielding a quantitative measurement of the position, prominence and scale of blobs, which can facilitate the tasks of blob reconstruction and blob reduction. Subsequently, we propose a denoising scheme to address high-ISO long-exposure noise, which sometimes spatially shows a blob appearance, employing a blob reduction procedure as a cheap preprocessing for conventional denoising methods. We apply the proposed denoising methods to real-world noisy images as well as standard images that are corrupted by real noise. The experimental results demonstrate the superiority of the proposed methods over state-of-the-art denoising methods.																	1057-7149	1941-0042					2020	29						5993	6005		10.1109/TIP.2020.2986687													
J								Two-Dimensional Autofocus Technique Based on Spatial Frequency Domain Fragmentation	IEEE TRANSACTIONS ON IMAGE PROCESSING										Azimuth; Estimation; Synthetic aperture radar; Apertures; Signal to noise ratio; Electronics packaging; Synthetic aperture radar (SAR); azimuth phase error (APE); residual range cell migration (RCM); two-dimensional autofocus; phase gradient autofocus (PGA); polar format algorithm (PFA); low signal-to-noise ratio (SNR)	PHASE-GRADIENT AUTOFOCUS; SAR IMAGE AUTOFOCUS; MOTION COMPENSATION; ALGORITHM; ERROR	Existing two-dimensional (2D) autofocus algorithms, exploiting the known structure of 2D phase error, lack the error estimation accuracy in the case of a low signal-to-noise ratio (SNR) due to the non-exhaustive use of the echo signal. In this paper, we propose a novel 2D autofocus algorithm, thoroughly utilizing all the available data and therefore achieving superior estimation performance. Via analytical study, we show that the partial derivative of the 2D error with respect to the azimuth frequency is approximable as a function of single argument, after appropriate change of variable. The established property enables a scheme for the azimuth phase error (APE) measurement, where the polar formatted data are fragmented along the range frequency and then aligned along the azimuth frequency, in order to equalize phase gradients in different fragments. On the one hand, such a scheme avoids the necessity to divide the full-aperture signal into subapertures, while on the other hand, it involves the whole signal support. Improved accuracy of the resulting estimate is achieved through the joint inter-fragment estimation of the APE gradient. The proposed algorithm, based on the mentioned scheme, was validated via computer simulations. The conducted experiments confirmed its preference against the existing techniques. The preference is particularly distinct for low SNR imagery.																	1057-7149	1941-0042					2020	29						6006	6016		10.1109/TIP.2020.2988143													
J								Confidence-Guided Self Refinement for Action Prediction in Untrimmed Videos	IEEE TRANSACTIONS ON IMAGE PROCESSING										Videos; Predictive models; Task analysis; Training; Hybrid power systems; Psychology; Machine learning; Action prediction; decision confidence; hybrid networks; attention mechanism	DECISION-MAKING; REPRESENTATION; TRACKING; NETWORK	Many existing methods formulate the action prediction task as recognizing early parts of actions in trimmed videos. In this paper, we focus on predicting actions from ongoing untrimmed videos where actions might not happen at the very beginning of videos. It is extremely challenging to predict actions in such untrimmed videos due to ambiguous or even no information of actions in the early parts of videos. To address this problem, we propose a prediction confidence that assesses the decision quality of a prediction model. Guided by the confidence, the model continuously refines the prediction results by itself with the increasing observed video frames. Specifically, we build a Self Prediction Refining Network (SPR-Net) which incrementally learns the confidence for action prediction. SPR-Net consists of three modules: a temporal hybrid network, an incremental confidence learner, and a self-refining Gumbel softmax sampler. The temporal hybrid network generates the action category distributions by integrating static scene and dynamic motion information. The incremental confidence learner calculates the confidence in an incremental manner, judging the extent to which the temporal hybrid network should believe its prediction result. The self-refining Gumbel softmax sampler models the mutual relationship between the prediction confidence and the category distribution, which enables them to be jointly learned in an end-to-end fashion. We also present a sparse self-attention mechanism to encode local spatio-temporal features into the frame-level motion representation to further improve the prediction performance. Extensive experiments on five datasets (i.e., UT-Interaction, BIT-Interaction, UCF101, THUMOS14, and ActivityNet) validate the effectiveness of the proposed method.																	1057-7149	1941-0042					2020	29						6017	6031		10.1109/TIP.2020.2987425													
J								Domain sentiment dictionary construction and optimization based on multi -source information fusion	INTELLIGENT DATA ANALYSIS										Sentiment lexicon; lexicon construction; domain lexicon; sentiment classification; ADMM	CLASSIFICATION; LEXICON	Sentiment analysis of text data, such as reviews, can help users and merchants make more favorable decisions. It is difficult to use the popular supervised learning method to complete the sentiment classification task because marking data manually is time-consuming and laborious. Unsupervised sentiment classification methods are mostly based on sentiment lexicons. The existing sentiment lexicons are simply not capable of domain sentiment classification, it still requires to construct a domain sentiment lexicon. There are still many problems with the advanced domain sentiment lexicon construction methods, e.g., rely heavily on labeled data, poor accuracy. We propose a labeled data extension idea to reduce the dependence of supervised learning methods on labeled data. In order to solve the problems of domain sentiment lexicon construction, we proposed a novel framework based on multi-source information fusion (MSIF) for learning. We extracted four kinds of emotional information, which are lexicon emotional information, emotional word co-occurrence information, emotional word polarity information and polarity relationship information of emotional word pair. When extracting the co-occurrence information, a novel method based on the data extension idea is proposed to enhance its accuracy and coverage. In order to accelerate the solution of the fusion model, an optimization method based on the ADMM algorithm is applied. Experimental results on five Amazon product review datasets show that the sentiment dictionary constructed by the proposed method can significantly improve the performance of review sentiment classification compared with the current popular baseline and the state-of-the-art methods.																	1088-467X	1571-4128					2020	24	2					229	251		10.3233/IDA-184426													
J								A cross-lingual sentiment topic model evolution over time	INTELLIGENT DATA ANALYSIS										Cross-Lingual sentiment analysis; Joint sentiment topic models; Cross-Lingual-Time-aware topic-sentiment models		Sentiment analysis in various languages has been a hot research topic with several applications. Most of the existing models have been reported to work well with widely used language. Were the lass directly applying these models to poor-quality corpora often leads to low results. Thus, to deal with these shortcoming we propose a cross-lingual sentiment topic model evolution over time (CLSTOT) which jointly models time with topic and sentiment. In CLSTOT, we consider the mapping between sentiment-aware topics under different cultures and analyze their evolution over time. The topic-specific sentiment is extracted using the entire data and not for each single document. As long as providing sentiment-topic, we can predict the timestamps for each test document by finding its most likely location over the timeline. This is achieved by using inference algorithm which is based on Gibbs Sampling. The experimental results on Chinese and English newsreader dataset; Chinese from SinaNews2, and English from Yahoo1, show that CLSTOT achieves significant improvement over the state-of-the-art.																	1088-467X	1571-4128					2020	24	2					253	266		10.3233/IDA-184449													
J								A comparison study on nonlinear dimension reduction methods with kernel variations: Visualization, optimization and classification	INTELLIGENT DATA ANALYSIS										Dimension reduction; PCA; LDA; FDA; KPCA; KFDA; SKPCA; SVM; parameter optimization; gender classification; MORPH-II	LOCAL BINARY PATTERNS; GENDER CLASSIFICATION; DISCRIMINANT-ANALYSIS; RECOGNITION; AGE; EIGENFACES; PREDICTION; KNOWLEDGE; PCA	Because of high dimensionality, correlation among covariates, and noise contained in data, dimension reduction (DR) techniques are often employed to the application of machine learning algorithms. Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), and their kernel variants (KPCA, KLDA) are among the most popular DR methods. Recently, Supervised Kernel Principal Component Analysis (SKPCA) has been shown as another successful alternative. In this paper, brief reviews of these popular techniques are presented first. We then conduct a comparative performance study based on three simulated datasets, after which the performance of the techniques are evaluated through application to a pattern recognition problem in face image analysis. The gender classification problem is considered on MORPH-II and FG-NET, two popular longitudinal face aging databases. Several feature extraction methods are used, including biologically-inspired features (BIF), local binary patterns (LBP), histogram of oriented gradients (HOG), and the Active Appearance Model (AAM). After applications of DR methods, a linear support vector machine (SVM) is deployed with gender classification accuracy rates exceeding 95% on MORPH-II, competitive with benchmark results. A parallel computational approach is also proposed, attaining faster processing speeds and similar recognition rates on MORPH-II. Our computational approach can be applied to practical gender classification systems and generalized to other face analysis tasks, such as race classification and age prediction.																	1088-467X	1571-4128					2020	24	2					267	290		10.3233/IDA-194486													
J								Hybridization of population -based ant colony optimization via data mining />	INTELLIGENT DATA ANALYSIS										Data Mining; population-based ant colony optimization; hybrid metaheuristics; travelling salesman problem; quadratic assignment problem; sequence mining	METAHEURISTICS; ALGORITHM; GRASP	We propose a hybrid application of Population Based Ant Colony Optimization that uses a data mining procedure to wisely initialize the pheromone entries. Hybridization of metaheuristics with data mining techniques has been studied by several researchers in recent years. In this line of research, frequent patterns in a number of initial high-quality solutions are extracted to guide the subsequent iterations of an algorithm, which results in an improvement in solution quality and computational time. Our proposal possesses certain differences from and contributions to existing literature. Instead of one single run that incorporates both the main metaheuristic and the data mining module inside, we propose to carry out independent runs and collect elite sets over these trials. Another contribution is the way we use the knowledge gained from the application of the data mining module. The extracted knowledge is used to initialize the memory model in the algorithm rather than to construct new initial solutions. One additional contribution is the use of a path mining algorithm (a specific sequence mining algorithm) rather than Apriori-like association mining algorithms. Computational experiments, conducted both on symmetric Travelling Salesman Problem and symmetric/asymmetric Quadratic Assignment Problem instances, showed that our proposal produces significantly better results, and is more robust than pure applications of population-based ant colony optimization.																	1088-467X	1571-4128					2020	24	2					291	307		10.3233/IDA-184431													
J								An improved opposition based learning firefly algorithm with dragonfly algorithm for solving continuous optimization problems	INTELLIGENT DATA ANALYSIS										Opposition based learning; dragonfly algorithm; firefly algorithm; continuous optimization problem; optimization	MULTIOBJECTIVE EVOLUTIONARY ALGORITHM; PATTERN SEARCH TECHNIQUE; GENETIC ALGORITHM; HYBRID	Nowadays, the existence of continuous optimization problems has led researchers to come up with a variety of methods to solve continues optimization problems. The metaheuristic algorithms are one of the most popular and common ways to solve continuous optimization problems. Firefly Algorithm (FA) is a successful metaheuristic algorithm for solving continuous optimization problems; however, although this algorithm performs very well in local search, it has weaknesses and disadvantages in finding solution in global search. This problem has caused this algorithm to be trapped locally and the balance between exploration and exploitation cannot be well maintained. In this paper, three different approaches based on the Dragonfly Algorithm (DA) processes and the OBL method are proposed to improve exploration, performance, efficiency and information-sharing of the FA and to avoid the FA getting stuck in local trap. In the first proposed method (FADA), the robust processes of DA are used to improve the exploration, performance and efficiency of the FA; and the second proposed method (OFA) uses an Opposition-Based Learning (OBL) algorithm to accelerate the convergence and exploration of the FA. Finally, in the third approach, which is referred to as OFADA in this paper, a hybridization of the hybrid FADA and the OBL method is used to improve the convergence and accuracy of the FA. The three proposed methods were implemented on functions with 2, 4, 10, and 30 dimensions. The results of the implementation of these three proposed methods showed that OFADA approach outperformed the other two proposed methods and other compared metaheuristic algorithms in different dimensions. In addition, all the three proposed methods provided better results compared with other metaheuristic algorithms on smalldimensional functions. However, performance of many metaheuristic algorithms decreased with increasing the dimensions of the functions. While the three proposed methods, in particular the OFADA approach, have been able to make better converge with the higher-dimensional optimization functions toward the target in comparison with other metaheuristic algorithms, and to show a high performance.																	1088-467X	1571-4128					2020	24	2					309	338		10.3233/IDA-194485													
J								Mining sequences in activities for time use analysis	INTELLIGENT DATA ANALYSIS										Time use; sequence mining analysis; data	SHORT-SLEEP DURATION; ALLOCATION; LEISURE; CLASSIFICATION; TRAVEL; BEHAVIOR; MODEL; WORK; RESTRICTION; ASSOCIATION	By providing a complete record of time use for a given population, time use studies enable investigators to test various hypotheses concerning that behavior. However, the large number and variety of activity combinations that are relevant in time allocation choices and, therefore, time use analysis, makes measuring or even fully identifying all of them impossible without the proper data mining tools. In this paper, we propose a framework for mining sequences of activities to capture more complex patterns than those currently available on how individuals organize their days. The proposed framework was applied to the American Time Use Surveys (ATUS) dataset to explore individual time allocation behavior, identifying sequences of activities that are frequent. For example, patterns such as the preferred activities that are performed before and after specific activities (such as paid work or leisure) are discussed in terms of their frequency. Such patterns are not easy to reveal using traditional descriptive analysis.																	1088-467X	1571-4128					2020	24	2					339	362		10.3233/IDA-184361													
J								An intrusion detection method based on active transfer learning	INTELLIGENT DATA ANALYSIS										Transfer learning; active learning; machine learning; intrusion detection; network security		Intrusion detection plays a very important role in the field of network security. In order to improve the intrusion detection rate, intrusion detection algorithms based traditional machine learning are widely used in this field. These methods generally satisfy the following two assumptions: the training and the testing data must be under the condition of the independent and identical distribution; the training samples are sufficient. However, in practice, the above assumptions are difficult to satisfy, which will result in poor intrusion detection. This paper proposes an intrusion detection algorithm based on active transfer learning ACTrAdaBoost. ACTrAdaBoost takes advantage of transfer learning and need not to satisfy the two assumptions of the traditional machine learning. In addition, ACTrAdaBoost utilizes active learning and maximum mean discrepancy knowledge to obtain maximum knowledge with minimum training sample cost and solve the problem of negative transfer. The ACTrAdaBoost compared with the traditional machine learning method on the KDDCUP99, DARPA1998 and ISCX2012 datasets. The experimental results show that the intrusion detection rate of the ACTrAdaBoost algorithm is greater than benchmark algorithms, and the training time efficiency improves at the same time. The performance of ACTrAdaBoost is better than the traditional machine learning classification algorithm. The ACTrAdaBoost algorithm improves the accuracy of intrusion detection and provides a new research method for intrusion detection.																	1088-467X	1571-4128					2020	24	2					363	383		10.3233/IDA-194487													
J								Efficient heuristics for learning Bayesian network from labeled and unlabeled data	INTELLIGENT DATA ANALYSIS										Bayesian network classifiers; order-based greedy search; target learning; unlabeled data	NAIVE BAYES; CLASSIFIERS; ASSUMPTION	Bayesian network classifiers (BNCs) are powerful tools to mine statistical knowledge from data and infer under conditions of uncertainty. However, most of the traditional BNCs focus on mining the dependency relationships existed in labeled data while neglecting the information hidden in unlabeled data, which may result in the biased decision boundaries. To address this issue, we introduce a new order-based greedy search heuristic based on mutual information for building efficient structures in tree-augmented naive Bayes (TAN), which is a highly accurate learner while maintaining simplicity and efficiency. Target learning is used to dynamically describe the dependency relationships in each unlabeled test instance. Extensive experimental results on UCI (University of California at Irvine) machine learning repository demonstrate that our proposed algorithm is a competitive alternative to state-of-the-art classifiers like weighted averaged TAN and k-dependence Bayesian classifier, as well as Random forest.																	1088-467X	1571-4128					2020	24	2					385	408		10.3233/IDA-194509													
J								Biased transfer matching for less overlapping degree for unsupervised domain adaptation	INTELLIGENT DATA ANALYSIS										transfer learning; data mining; domain adaptation; instance reweighting		Domain adaptation is an important branch of transfer learning. Previous studies have always taken efforts to minimize the optimization goal, but they neglect the relative quality of features or instances. For example, a classic work treats different instances equally in a degree and chooses these instances which minimize the optimization function value. This method will discard these instances that make the data distribution in source and target data domain different and will neglect the instances' relative quality. To reduce interference between instances in the process of domain adaptation, we put forward a novel method of ODA that uses the overlapping degree to measure every feature or instance's relative quality and implement feature or instance reweighting. At the same time, we have noticed that there are many parameters with values that will influence the effect of the method. Previous studies do not have a reasonable method to determine the parameters' values. We can use the genetic algorithm to find the balance between marginal distribution adaptation and conditional distribution adaptation to find the best combination of multiple parameters. Experiments we have done verify that the ODA method outperforms by 3.26% compared with the best comparison method. We have found that our method of finding the optimal parameters can yield more accurate results than the original method.																	1088-467X	1571-4128					2020	24	2					409	425		10.3233/IDA-194516													
J								Learning labeling functions in distantly supervised relation extraction	INTELLIGENT DATA ANALYSIS										Relation extraction; distant supervision; labeling functions; markov logic networks		Distant supervision has become the leading method for training large-scale information extractors. It could be encoded in the form of labeling functions, which employ knowledge bases to provide labels for the data. However, most previous works use only simple labeling functions, resulting in too much noise in the training data, and the knowledge bases are far from well-explored. In this paper, in order to improve the labeling quality of the training data for distant supervision relation extraction, we propose to make use of existing knowledge bases to effectively learn labeling functions. Specifically, labeling functions are represented as Markov Logic, which can integrate various resources into a unified model naturally. Experimental results show that the training data produced by the learned labeling functions is significantly improved in quality. Different distantly supervised relation extraction models trained on the produced training data can also achieve better performances.																	1088-467X	1571-4128					2020	24	2					427	443		10.3233/IDA-194492													
J								Knowledge-embodied attention for distantly supervised relation extraction	INTELLIGENT DATA ANALYSIS										Relation extraction; distant supervision; neural networks; sentence-level attention; knowledge representation learning; de-noising		Knowledge bases (KBs) provide a large amount of structured information for entities and relations, which are successfully leveraged in many natural language processing tasks. However, distantly supervised relation extraction only utilizes KBs to automatically generate datasets, while ignoring the background information in KBs during the relation extraction process. We herein propose a knowledge-embodied attention that leverages knowledge information in KBs to reduce the impact of noisy data for distantly supervised relation extraction. Specifically, we pre-train distributed representations of KBs with the knowledge representation learning (KRL) model, and subsequently incorporate them into relation extraction to learn sentencelevel attention weights. The experimental results demonstrate that our approach outperforms all baselines, thus indicating that we can focus our attention on valid data by leveraging background information in KBs.																	1088-467X	1571-4128					2020	24	2					445	457		10.3233/IDA-194476													
J								MAPK-means: A clustering algorithm with quantitative preferences on attributes />	INTELLIGENT DATA ANALYSIS											FEATURE-SELECTION	This paper describes a new semi-supervised clustering algorithm as part of a more general framework of interactive exploratory clustering, that favors the exploration of possible clustering solutions so that an expert tailors the best clustering according to her domain knowledge and preferences. Contrary to most existing approaches, the novel algorithm considers the feature space as a first class citizen for the exploration of alternative solutions. Our proposal represents and integrates quantitative preferences on attributes that will guide the exploration of possible solutions by learning an appropriate space metric. It also achieves a compromise clustering based on expert confidence, between a data-driven and a user-driven solution and converges with a good complexity. We show experimentally that our method is also able to deal with irrelevant user preferences and correct those choices in order to achieve a better solution. Experiments show that the best results may be achieved only with the addition of preferences to traditional metric learning algorithms and that our approach performs better than state-of-the-art algorithms.																	1088-467X	1571-4128					2020	24	2					459	489		10.3233/IDA-184468													
J								Information Retrieval Based on Knowledge-Enhanced Word Embedding Through Dialog: A Case Study	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Information retrieval; Domain knowledge; Enhanced word embedding; Semantic understanding		The aim of this paper is to provide a systematic route of information retrieval from a knowledge-based database (or domain knowledge) through a dialog system of natural language interaction. The application is about a comprehensive building at a university, with classrooms, laboratory rooms, meeting rooms, research rooms and offices, and is to present related information the user asks for. First, the domain knowledge is expressed with predicate expressions based on the ontology structure; then the vocabulary is presented distributedly with word embedding enhanced with the domain knowledge; queries from the user arc then converted into the intent (general) and slot elements (specific) with the help of trained recurrent neural network (RNN). The system works smoothly. The key point is integrating the two methods of knowledge-based and data-driven natural language processing into one system, and the domain knowledge is in the central part which is incorporated into the word embedding to make it specifically fit the natural language in this application. (C) 2020 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2020	13	1					275	290		10.2991/ijcis.d.200310.002													
J								Multi-Objective Particle Swarm Optimization Algorithm for the Minimum Constraint Removal Problem	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Minimum constraint removal; Minimum constraint set; Path planning; Multi-objective optimization; Multi-objective particle swarm optimization algorithm		This paper proposes a multi-objective approach for the minimum constraint removal (MCR) A problem. First, a multi-objective model for MCR path planning is constructed. This model takes into account factors such as the minimum constraint set, the route length, and the cost. A multi-objective particle swarm optimization (MOPSO) algorithm is then designed based on the fitness function of the multi-objective MCR problem, and an iteration formula based on the personal best (pbest) and global best (gbest) of the algorithm is constructed to update the particle velocity and position. Finally, compared with ant colony optimization (ACO) A and the crow search algorithm (CSA) A, the experimental results show that the MOPSO-based path planning algorithm can find a shorter path that traverses fewer obstacle areas and can thus perform MCR path planning more effectively. (C) 2020 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2020	13	1					291	299		10.2991/ijcis.d.200310.005													
J								Research on National Pattern Reuse Design and Optimization Method Based on Improved Shape Grammar	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Shape grammar; Batik patterns; Optimization design; Pattern reuse; Pattern reuse design system		Considering the low degree of abstraction of traditional shape grammar in national pattern reuse design, this paper proposes a method based on the combination of improved shape grammar and an optimization algorithm to reuse national patterns design. As an application example, we carried out research on the batik pattern elements of the Miao nationality in the "Intangible Cultural Heritage of Guizhou Province" system. Firstly, an improved shape grammar based on synthetic predicate grammar coding is developed to represent the transformation process of design pattern reuse in batik patterns. Then, the number of patterns arranged in a specific position is taken as the optimization variable; the particle swarm optimization algorithm is used to optimize the variables, so as to obtain the optimized predicate shape grammar parameters; and the pattern is reused according to the parameters. A batik design pattern reuse system is also constructed to realize rapid design. The results show that the proposed method can enrich the reuse results of batik pattern elements and realize the rapid design of complex batik patterns. (C) 2020 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2020	13	1					300	309		10.2991/ijcis.d.200310.003													
J								Hierarchical Bayesian Choice of Laplacian ARMA Models Based on Reversible Jump MCMC Computation	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										ARMA time series; Hierarchical Bayesian; Laplacian noise; Reversible jump MCMC		An autoregressive moving average (ARMA) is a time series model that is applied in everyday life for pattern recognition and forecasting. The ARMA model contains a noise which is assumed to have a specific distribution. The noise is often considered to have a Gaussian distribution. However in applications, the noise is sometimes found that does not have a Gaussian distribution. The first objective is to develop the ARMA model in which noise has a Laplacian distribution. The second objective is to estimate the parameters of the ARMA model. The ARMA model parameters include ARMA model orders, ARMA model coefficients, and noise variance. The parameter estimation of the ARMA model is carried out in the Bayesian framework. In the Bayesian framework, the ARMA model parameters are treated as a variable that has a prior distribution. The prior distribution for the ARMA model parameters is combined with the likelihood function for the data to get the posterior distribution for the parameter. The posterior distribution for parameters has a complex form so that the Bayes estimator cannot be determined analytically. The reversible jump Markov chain Monte Carlo (MCMC) algorithm was adopted to determine the Bayes estimator. The first result, the ARMA model can be developed by assuming Laplacian distribution noise. The second result, the performance of the algorithm was tested using simulation studies. The simulation shows that the reversible jump MCMC algorithm can estimate the parameters of the ARMA model correctly. (C) 2020 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2020	13	1					310	317		10.2991/ijcis.d.200310.006													
J								Applying Heuristic Algorithms to Solve Inter-hospital Hierarchical Allocation and Scheduling Problems of Medical Staff	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Heuristic algorithms; Staff allocation; Staff scheduling; Inter-hospital; Particle swarm optimization	HUMAN-RESOURCE ALLOCATION; PROGRAMMING-MODEL; DECISION-SUPPORT; NURSE; INTELLIGENCE; SEARCH; ROSTER	To address the inter-hospital hierarchical allocation and scheduling problems, this research used the pooling resource concept to allocate medical staff among hospital branches as well as determine their monthly schedules. This study proposed a two-stage strategy. The first stage proposed three heuristic algorithms-HRA1 (human resource allocation based on the hospital's size), HRA2 (human resource allocation based on the average allocation), and HRA3 (human resource allocation based on the severity of penalties) for medical staff allocation. The second stage used the improved particle swarm optimization (PSO) algorithm to schedule the medical staff within a reasonable time. Based on the numerical results, HRA3 was superior to HRA1 and HRA2. Furthermore, the analysis of two scenarios-varying the sizes of hospital branches (Scenario 1) and varying the total number of medical staff (Scenario 2)-showed that, when the sizes of hospital branches varied (Scenario 1), HRA3 was superior to HRA1 and HRA2 whereas, when the sizes were given (Scenario 2), the lowest number of medical staff possible was approximately 60. The findings of this research will help hospital managers make decisions concerning the allocation and scheduling of medical staff. (C) 2020 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2020	13	1					318	331		10.2991/ijcis.d.200310.004													
J								An Extension of Social Network Group Decision-Making Based on TrustRank and Personas	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Social network; PageRank; TrustRank; Persona; Social network group decision making (SN-GDM)	SELF-CONFIDENCE; CONSENSUS MODEL; DECEPTION; PAGERANK	With the development of social networking big data, social network group decision-making (SN-GDM) has been widely applied in many fields. This paper focuses on three main components: (1) the determination of the decision makers' (DMs) weights based on different social influence; (2) the anti-deception mechanism; and (3) the persona method. We introduce the TrustRank algorithm and the persona method into SN-GDM. Based on the TrustRank algorithm, both trusted and deceptive DMs in a seed set are artificially identified and given initial static scores to derive the influence of each DM. Additionally, the persona method is introduced to cluster DMs and achieve personalized decision-making. Further, we present a numerical example and comparison to demonstrate the efficiency of the framework in coping with non-socially shared preferences in SN-GDM. As expected, our findings indicate that our framework reduces the influence of deceptive DMs on the decision results. (C) 2020 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2020	13	1					332	340		10.2991/ijcis.d.200310.001													
J								End-to-End Sequence Labeling via Convolutional Recurrent Neural Network with a Connectionist Temporal Classification Layer	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Sequence labeling; Convolutional recurrent neural network; Unified framework; End-to-end	RECOGNITION	Sequence labeling is a common machine-learning task which not only needs the most likely prediction of label for a local input but also seeks the most suitable annotation for the whole input sequence. So it requires the model that is able to handle both the local spatial features and temporal-dependence features effectively. Furthermore, it is common for the length of the label sequence to be much shorter than the input sequence in some tasks such as speech recognition and handwritten text recognition. In this paper, we propose a kind of novel deep neural network architecture which combines convolution, pooling and recurrent in a unified framework to construct the convolutional recurrent neural network (CRNN) for sequence labeling tasks with variable lengths of input and output. Specifically, we design a novel CRNN to achieve the joint extraction of local spatial features and long-distance temporal-dependence features in sequence, introduce pooling along time to achieve a transform of long input to short output which will also reduce he model's complexity, and adopt Connectionist Temporal Classification (CTC) layer to achieve an end-to-end pattern for sequence labeling. Experiments on phoneme sequence recognition and handwritten character sequence recognition have been conducted and the results show that our method achieves great performance while having a more simplified architecture with more efficient training and labeling procedure. (C) 2020 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2020	13	1					341	351		10.2991/ijcis.d.200316.001													
J								NNIR: N-Non-Intersecting-Routing Algorithm for Multi-Path Resilient Routing in Telecommunications Applications	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Resilient Routing; Multi-Path Resilient Routing; Genetic Algorithm Routing; Multi-Path Routing; Routing; Telecommunication Routing		In this paper, we will present a N-Non-Intersecting-Routing (NNIR) algorithm which is used to reduce the cost of resilient routing in telecommunications problems. Resilient Routing is the connections between two locations in a graph through the use of N completely independent routes. Resilient Routing is applicable in a wide variety of domains including telecommunications, logistics and embedded systems design. The proposed NNIR algorithm increase the cost of the primary route by taking a less optimal route, thus freeing a more optimal route for the resilient routes, in turn reducing the total cost of both routes. This is achieved through the use of a Genetic Algorithm, Dijkstra's Algorithm and the repair operator. The proposed NNIR shows an average improvement of 34.2% when compared to Dijkstra's Algorithm (one of the most widely used algorithm routing). Similarly, there is an average improvement of 34.2% when compared to A* (another popular shortest path algorithm). Additionally, there is an average improvement of 26.9% when compared to Simulated Annealing (a popular evolutionary technique used within routing problems). In this paper we show how NNIR performs within two different routing domains (telecommunications routing and road routing), and compares it against three other routing techniques to solve the resilient routing problem. (C) 2020 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2020	13	1					352	365		10.2991/ijcis.d.200313.001													
J								A Multi-Criteria Group Decision-Making Approach Based on Improved BWM and MULTIMOORA with Normal Wiggly Hesitant Fuzzy Information	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Multi-criteria group decision-making (MCGDM); Normal wiggly hesitant fuzzy set (NMHFS); Best-worst method (BWM); MULTIMOORA method; Train selection; Spring Festival travel rush	SETS; SELECTION; DISTANCE; FUSION	Multi-criteria group decision-making (MCGDM) problems are widespread in real life. However, most existing methods, such as hesitant fuzzy set (HFS), hesitant fuzzy linguistic term set (HFITS) and inter-valued hesitant fuzzy set (IVHFS) only consider the original evaluation data provided by experts but fail to dig the concealed valuable information. The normal wiggly hesitant fuzzy set (NWHFS) is a useful technique to depict experts' complex evaluation information toward MCGDM issues. In this paper, on the basis of the score function of NWHFS, we propose the linear best-worst method (BWM)-based weight-determining models with normal wiggly hesitant fuzzy (NWHF) information to compute the optimal weights of experts and criteria. In addition, we present some novel distance measures between NWHFSs and discuss their properties. After fusing the individual evaluation matrices, the NWHF-ranking position method is put forward to develop the group MULTIMOORA method, which can be determined by the final decision results. Moreover, we investigate the Spring Festival travel rush phenomenon deeply and apply our methodology to solve the train selection problem during the Spring Festival period. Finally, the applicability and superiority of the proposed approach is demonstrated by comparing with traditional methods based on two aggregation operators of NWHFSs. (C) 2020 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2020	13	1					366	381		10.2991/ijcis.d.200325.001													
J								Optimal Walking Gait Generator for Biped Robot Using Modified Jaya Optimization Technique	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Uncertain nonlinear biped robot system; Genetic Algorithm (GA); Particle Swarm Optimization (PSO); Jaya optimization algorithm; Central Force Optimization (CFO) algorithm; Modified Differential Evolution (MDE) algorithm; Gait optimization for biped robot; Zero moment point (ZMP) concept	ALGORITHM; STABILITY	This paper treats the optimization of the biped walking trajectory that can be used as a reference trajectory for control. The biped robot is modeled as a kinetic chain of 11 links connected by 10 joints. The inverse kinematics of the biped is derived for the specified positions of the hips and feet. The objective is to optimize the biped robot able to stably and naturally walking with preset foot-lift magnitude (or preset hip-shift, or preset step-length). The stability of the biped robot is quantified by the distance between the ZMP and the foot center in the step cycle, which represents the first objective function. Additionally, for the biped robot to follow the preset foot-lift value, the difference between the magnitude of foot-lift value and the foot-lift preset value represents the second objective function. Specifically, we minimize the value of the two objective functions by considering the gait parameters of biped robot as variables. The new Jaya optimization algorithm is innovatively applied to optimize the biped gait parameters as to ensure the biped robot walking robustly and steadily. The efficiency of the proposed Jaya-based identification method is compared with the Genetic Algorithm (GA), the Particle Swarm Optimization (PSO), the Central Force Optimization (CFO) and improved Differential Evolution (DE) [Modified Differential Evolution (MDE)] algorithms. The simulation results tested on the real small-sized biped robot system HUBOT-4 demonstrate that the novel proposed algorithm offers an efficient and stable gait for biped robot with precise foot-lift value. (C) 2020 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2020	13	1					382	399		10.2991/ijcis.d.200323.001													
J								Blind Channel and Data Estimation Using Fuzzy Logic Empowered Cognitive and Social Information-Based Particle Swarm Optimization (PSO)	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Multiple Input Multiple Output (MIMO); Orthogonal Frequency Division Multiple Access (OFDMA); Multi Carrier Code Division Multiple Access (MC-CDMA); Multi User Detection (MUD); Channel State Information (CSI)	GA; CONVERGENCE; ALGORITHM	Multiple Input Multiple Output (MIMO) is a technology used to improve the channel capacity of the wireless communication systems. Rapid increase in the number of users has led to data rate demand increased in growing modern wireless communication systems. To overcome this issue, MIMO is being used with several multicarrier techniques like Orthogonal Frequency Division Multiple Access (OFDMA), Multi-Carrier Code Division Multiple Access (MC-CDMA), etc. Multi-user detection (MUD) with artificial intelligence plays a vital role to enhance network capacity to meet the demands of future networks with an increased number of users and multimedia services. Computational intelligence techniques are used in a multicarrier system to boost the process of MUD. Some of the computational intelligence algorithms like Swarm and Evolutionary are stuck in local minima and due to this issue, the overall performance of the network decreases. For the convergence of Swann intelligence-based solutions, cognitive and social information (CSI) playa vital role. In this research article, the Fuzzy Logic empowered Cognitive and Social Information (FLeCSI) algorithm using a fuzzy logic and swarm intelligence algorithm is proposed. By using social and cognitive information FLeCSI updated each swarm position. After the simulation, it is observed that FLeCSI provides fast convergence and minimize MMSE and BER as compared to techniques used previously for MUD like Fuzzy Logic empowered Opposite Mutant Particle Swarm Optimization (FLOMPSO), Opposite Learning Mutant Particle Swarm Optimization (OLMPSO), Total Opposite Mutant Particle Swarm Optimization (TOMPSO), Partial Opposite Mutant Particle Swarm Optimization (POMPSO), etc. (C) 2020 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2020	13	1					400	408		10.2991/ijcis.d.200323.002													
J								m-Polar Picture Fuzzy Ideal of a BCK Algebra	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										m-polar picture fuzzy ideal; Homomorphism of m-polar picture fuzzy ideal; m-polar picture fuzzy implicative ideal; m-polar picture fuzzy commutative ideal	TRANSLATIONS	In this paper, the notions of m-polar picture fuzzy subalgebra (PFSA), m-polar picture fuzzy ideal (PFI) and m-polar picture fuzzy implicative ideal (PFII) of BCK algebra are introduced and some related basic results are presented. A relation between m-polar PFI and m-polar PFII is established. It is shown that an m-polar PFII of a BCK algebra is an m-polar PFI. But the converse of the proposition is not necessarily true. Converse is true only in implicative BCK algebra. The concept of m-polar picture fuzzy commutative ideal (PFCI) is also explored here and some related results are investigated. (C) 2020 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2020	13	1					409	420		10.2991/ijcis.d.20030.001													
J								Multi-Sine Cosine Algorithm for Solving Nonlinear Bilevel Programming Problems	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Nonlinear bilevel programming problems; Sine cosine algorithm; Optimization	NEURAL-NETWORK APPROACH; GENETIC ALGORITHM; OPTIMIZATION ALGORITHM	In this paper, multi-sine cosine algorithm (MSCA) is presented to solve nonlinear bilevel programming problems (NBLPPs); where three different populations (completely separate from one another) of sine cosine algorithm (SCA) are used. The first population is used to solve the upper level problem, while the second one is used to solve the lower level problem. In addition, the Kuhn-Tucker conditions are used to transform the bilevel programming problem to constrained optimization problem. This constrained optimization problem is solved by the third population of SCA and if the objective function value equal to zero, the obtained solution from solving the upper and lower levels is feasible. The heuristic algorithm didn't used only to get the feasible solution because this requires a lot of time and efforts, so we used Kuhn-Tucker conditions to get the feasible solution quickly. Finally, the computational experiments using 14 benchmark problems, taken from the literature demonstrate the effectiveness of the proposed algorithm to solve NBLPPs. (C) 2020 The Authors. Published by Atlantis Press SARI.																	1875-6891	1875-6883					2020	13	1					421	432		10.2991/ijcis.d.200411.001													
J								A Two-Stage Multi-objective Programming Model to Improve the Reliability of Solution	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Multi-objective programming; Randomness; Expectation value; Fuzzy events; Reliability	LEVEL DECISION-MAKING; SUPPLY CHAIN; PROBABILITY; MANAGEMENT; ALGORITHM; WATER	Randomness is a common uncertainty encountered in practical multi-objectives decision-making. But it is always a challenge for decision-makers to process randomness in multi-objective programming problems. This paper takes the decision-making objectives as fuzzy events and aims to solve numerical multi-objective programming problems under random environment. We first analyze the effects of randomness on multi-objective decision-making results. With the expectation value and the probability of fuzzy events as quantitative index of randomness, we then establish a two-stage random multi-objective programming model based on reliability (i.e., TS-MOPM). Specifically, we give several probability calculation methods of fuzzy events with common distributions, and further present the corresponding calculation procedures for solving TS-MOPM. Finally, a case study is implemented to test the proposed model TS-MOPM. Theoretical analysis and case study indicate that our model has better interpretability and operability. The research results enrich the existing random multi-objective programming methods to some extent. (C) 2020 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2020	13	1					433	443		10.2991/ijcis.d.200410.001													
J								A Novel Pythagorean Fuzzy LINMAP-Based Compromising Approach for Multiple Criteria Group Decision-Making with Preference Over Alternatives	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										compromising approach; Group decision making; Pythagorean fuzzy set; LINMAP; Dominance measure	MATHEMATICAL-PROGRAMMING METHOD; MULTIDIMENSIONAL-ANALYSIS; INCOMPLETE PREFERENCE; MEMBERSHIP GRADES; EXTENSION; DISTANCE; SETS	This paper presents a new compromising approach to multiple criteria group decision-making (MCGDM) for the treatment of uncertainty which is based on Pythagorean fuzzy (PF) sets. The present work intends to propose a novel linear programming technique for multidimensional analysis of preference (LINMAP) by way of some useful concepts related to PF dominance relations, individual consistency and inconsistency levels, and individual fit measurements. The concept of PF scalar function-based dominance measures is defined to conduct intracriterion comparisons concerning uncertain evaluation information based on Pythagorean fuzziness; moreover, several valuable properties are also investigated to demonstrate its effectiveness. For the assessment of overall dominance of alternatives, this paper provides a synthetic index, named a comprehensive dominance measure, which is the aggregation of the weighted dominance measures by combining unknown weight information and PF dominance measures of various criteria. For each decision-maker, this paper employs the proposed measures to evaluate the individual levels of rank consistency and rank inconsistency regarding the obtained overall dominance relations and the decision-maker's preference comparisons over paired alternatives. In the framework of individual fit measurements, this paper constructs bi-objective mathematical programming models and then provides their corresponding parametric linear programming models for generating the best compromise alternative. Realistic applications with some comparative analyses concerning railway project investment are implemented to demonstrate the appropriateness and usefulness of the proposed methodology in addressing actual MCGDM problems. (C) 2020 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2020	13	1					444	463		10.2991/ijcis.d.200408.001													
J								On Contradiction and Inclusion Using Functional Degrees	INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS										Fuzzy sets; Inclusion measure; Contradiction measure; Galois connections	FUZZY; CONSTRUCTION; DEFINITION; SIMILARITY	The notion of inclusion is a cornerstone in set theory and therefore, its generalization in fuzzy set theory is of great interest. The degree off-inclusion is one generalization of such a notion that differs from others existing in the literature because the degree of inclusion is considered as a mapping instead of a value in the unit interval. On the other hand, the degree of f-weak-contradiction was introduced to represent the contradiction between two fuzzy sets via a mapping and its definition has many similarities with thef-degree of inclusion. This suggests the existence of relations between bothf-degrees. Specifically, following this line, we analyze the relationship between thef-degree of inclusion and the f-degree of contradiction via the complement of fuzzy sets and Galois connections. (C) 2020 The Authors. Published by Atlantis Press SARL.																	1875-6891	1875-6883					2020	13	1					464	471		10.2991/ijcis.d.200409.001													
J								A Spectral Algorithm for 3-valued Function Equivalence Classification	JOURNAL OF MULTIPLE-VALUED LOGIC AND SOFT COMPUTING												Spectral techniques for Boolean and multiple-valued functions have been well studied and found to be useful in logic design and testing for conventional circuits. Spectral techniques also have potential application for reversible and quantum circuits. This paper addresses the classification of 3-valued functions into spectral translation equivalence classes. A transform algorithm is presented that determines the spectral translations to map a given function to the representative function for the equivalence class containing the given function. Using this algorithm we show, by exhaustive enumeration, that the 2-variable 3-valued functions partition into 11 equivalence classes. The number of 3-valued functions with 3 or more variables is very large, prohibiting exhaustive enumeration. We show that a search of 3-valued function 1-neighbourhoods yields 167,266 equivalence classes for 3 variables. The transform algorithm can be used for a higher number of variables to determine if two functions fall within the same equivalence class and, if they do, to find a sequence of spectral translations to map one to the other.																	1542-3980	1542-3999					2020	34	3-4					203	221															
J								Characterization of Multiple-Valued Threshold Functions in the Vilenkin-Chrestenson Basis	JOURNAL OF MULTIPLE-VALUED LOGIC AND SOFT COMPUTING										Threshold logic; harmonic analysis; Nomura parameters; Chow parameters	PARAMETERS	A multiple-valued threshold function is a discrete function which induces a partition of its domain set into (non-empty) subsets, where the subsets are separable with parallel hyperplanes. If the number of subsets is not greater than k, the function is called a k-level threshold. In this paper, we propose a characterization of threshold functions using the Vilenkin-Chrestenson transformation. The main result of the paper shows that a 2-level threshold function is uniquely characterized by only the partial spectrum of the function. We also provide a characterization of general k-level threshold functions using the additional zero-order spectral coefficients of a suitably chosen characters of the function. The initial results of this paper were presented at the conference ISMVL 2018, and published in [13].																	1542-3980	1542-3999					2020	34	3-4					223	238															
J								Bases for the Space of Fixed Points of the Reed-Muller-Fourier Transform	JOURNAL OF MULTIPLE-VALUED LOGIC AND SOFT COMPUTING										Reed-Muller-Fourier transform; eigenvector; eigenvalue; basis; fixed point; functions of several variables		We prove that the space of fixed points of the Reed-Muller-Fourier transform of n-variable functions on a p-element domain always has a basis. For odd p our proof is constructive and it proves the conjecture of C. Moraga, R. S. Stankovio, M. Stankovio and S. Stojkovie about the number of fixed points presented at ISMVL 2017. For even p we give a nonconstructive proof that relies on our earlier proof of the above mentioned conjecture.																	1542-3980	1542-3999					2020	34	3-4					239	259															
J								Different Types of Arcs in m-polar Fuzzy Graphs with Application	JOURNAL OF MULTIPLE-VALUED LOGIC AND SOFT COMPUTING										mPFGs; strongest and strong mPF path; mPF bridges; mPF cut nodes; mPF trees and mPF forests		Recently, m-polar fuzzy graph (mPFG) becomes a growing research topic as it is the generalization of fuzzy graph. In this paper, at first alpha-strong mPF arc, beta-strong mPF arc, delta-strong mPF arc and delta*-strong InPF arc in an mPFG are defined. An application of decision making using strong path is also given at the end.																	1542-3980	1542-3999					2020	34	3-4					263	282															
J								A New Type of Fuzzy Quasi-ideals of Ordered Semigroups	JOURNAL OF MULTIPLE-VALUED LOGIC AND SOFT COMPUTING										Ordered semigroup; fuzzy subsets; (is an element of, is an element of boolean OR(k*, q(k)))-fuzzy quasi-ideals	BI-IDEALS; ELEMENT; TERMS	In this paper, we generalize the concept of an (is an element of, is an element of boolean OR q(k))-fuzzy quasi-ideal of an ordered semigroup, using the notion of (k*, q)-quasicoincidence of an ordered fuzzy point with a fuzzy subset of the support ordered semigroup. First we define and characterize in different ways the (is an element of, is an element of boolean OR(k*, q(k)))-fuzzy quasi-ideals of an ordered semigroup. Secondly, we present some relationships between these generalized fuzzy quasi-ideals and similar generalizations of fuzzy left/right ideals or fuzzy bi-ideals (based on the (k*, q)-quasi-coincidence relation). Similarities with quasi-ideals in ordered semigroups are discussed at the end of the paper.																	1542-3980	1542-3999					2020	34	3-4					283	304															
J								Green Supplier Selection Using a Combined Fuzzy Decision-Making Approach	JOURNAL OF MULTIPLE-VALUED LOGIC AND SOFT COMPUTING										Green supplier selection; fuzzy ANP; fuzzy axiomatic design; multi-criteria decision-making (MCDM); green supply chain; performance measurement	AXIOMATIC DESIGN METHODOLOGY; ANALYTIC NETWORK PROCESS; CHAIN MANAGEMENT; INTEGRATED MODEL; PERFORMANCE EVALUATION; DEVELOPMENT PROGRAMS; PARTNER SELECTION; ORDER ALLOCATION; CRITERIA; SYSTEMS	This paper aims to evaluate and select green suppliers using a combined fuzzy decision-making approach considering the interactions of evaluation criteria and company's requirements. Firstly, fuzzy ANP is employed to deal with the interdependencies between criteria and to calculate the relative importance of each criterion. Secondly, a fuzzy axiomatic design (FAD) method is utilised that considers the functional requirements (FRs) of companies as well as the imprecise nature of the decision process for selecting green suppliers. A real case study is applied to verify the proposed methodology's practicality by comparison with other MCDM methods. Finally, Spearman's correlation test is employed to show that there is a strong association between the ranks of results. The proposed model will enable managers to improve their understanding of decision criteria from both economic and environmental perspectives. It will also assist them in selecting green suppliers that satisfy the needs of the company.																	1542-3980	1542-3999					2020	34	3-4					305	333															
J								Interval-valued Intuitionistic Fuzzy Competition Graph	JOURNAL OF MULTIPLE-VALUED LOGIC AND SOFT COMPUTING										Competition graph; interval-valued intuitionistic fuzzy competition graph; graph homomorphism; graph product		Fuzzy competition graph is the segment of competition graph. We study a special type of fuzzy competition graph. In this paper, interval-valued intuitionistic fuzzy competition graph of an interval-valued intuitionistic fuzzy digraph is introduced. We investigated their properties. Also, comparison of homomorphism properties between the several of interval-valued intuitionistic fuzzy competition graphs and their corresponding are established. An application of this graph is considered in food web competition																	1542-3980	1542-3999					2020	34	3-4					335	364															
J								Residue Product of Fuzzy Graph Structures	JOURNAL OF MULTIPLE-VALUED LOGIC AND SOFT COMPUTING										Fuzzy graph structure; residue; mu(i)-degree; total degree; mu(i)-total degree; decision-making; algorithm	ARCS	In this research paper, we introduce the concept of residue product of fuzzy graph structures and explain with examples. Moreover, we discuss degree, mu(i)-degree, total degree, mu(i)-total degree of vertex in residue product of fuzzy graph structures and investigate some of their properties. Further, we discuss identification of prominent relationships in decision making by our proposed algorithm.																	1542-3980	1542-3999					2020	34	3-4					365	399															
J								Lattice-reduction aided multiple-symbol differential detection in two-way relay transmission	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Multiple-symbol differential detection; generalized likelihood ratio test; lattice reduction; two-way relay transmission	RECEIVERS	Multiple-symbol differential detection (MSDD) algorithms are proposed in two-way relay transmission (TWRT). Firstly, generalized likelihood ratio test based MSDD (GLRT-MSDD) is proposed in TWRT. Unfortunately, as the number of observation windows increases, the computational complexity of GLRT-MSDD increases exponentially. Hence, this detection in TWRT constitutes a challenging problem. Moreover, we find a way to reformulate the GLRT-MSDD model and additionally propose a lattice-reduction aided MSDD (LR-MSDD) model. Performance analysis and simulations show that the proposed LR-MSDD provides bit-error rate performance close to that of GLRT-MSDD with lower complexity in TWRT.																	1300-0632	1303-6203					2020	28	3					1208	1216		10.3906/elk-1812-51													
J								A population based simulated annealing algorithm for capacitated vehicle routing problem	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Capacitated vehicle routing problem; best route; route enhancing; simulated annealing	OPTIMIZATION	The Vehicle Routing Problem (VRP) is one of the most discussed and researched topics nowadays. The VRP is briefly defined as the problem of identifying the best route to reduce distribution costs and improve the quality of service provided to customers. The Capacitated VRP (CVRP) is one of the most commonly researched among the VRP types. Therefore, the CVRP was studied in this paper and a new population based simulated annealing algorithm was proposed. In the algorithm, three different route development operators were used, which are exchange, insertion and reversion operators. It was tested on 63 well-known benchmark instances in the literature. The results showed that the optimum routes could be determined for the 23 instances.																	1300-0632	1303-6203					2020	28	3					1217	1235		10.3906/elk-1902-122													
J								Comparisons of extreme learning machine and backpropagation-based i-vector approach for speaker identification	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Speaker recognition; extreme learning machine; TIMIT database; i-vector	TRENDS	The extreme learning machine (ELM) is one of the machine learning applications used for regression and classification systems. In this paper, an extended comparison between an ELM and the backpropagation neural network (BPNN)-based i-vector is given in terms of a closed-set speaker identification task using 120 speakers from the TIMIT database. The system is composed of the mel frequency cepstal coefficient (MFCC) and power normalized cepstal coefficient (PNCC) approaches to form the feature extraction stage, while the cepstral mean variance normalization (CMVN) and feature warping are applied in order to mitigate the linear channel effect. The system is utilized with equal numbers of speakers of both genders with 120 speakers with eight dialects from the TIMIT database. The results demonstrate that the combination of the i-vector with the ELM for different features has the highest speaker identification accuracy (SIA) compared with the combination of the BPNN with the i-vector. The results also show that the i-vector with ELM approach is faster than the BPNN-based i-vector and it has the highest SIA.																	1300-0632	1303-6203					2020	28	3					1236	1245		10.3906/elk-1906-118													
J								Emulation of burst-based adaptive link rates in NetFPGA towards green networking	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Network prototype; green networks; hardware implementation; NetFPGA; policy-based forwarding; dynamic link rates; energy-efficient forwarding; emulation	ENERGY; TRANSMISSION	In recent times, energy consumption in communication media has been increasing drastically. In the literature, energy-saving techniques that enable network devices to enter sleep state or limit the data rate have been proposed to reduce energy costs. In our earlier work, we proposed an energy-saving technique called burst-based adaptive link rate (BBALR), the simulation of which assures increased energy savings. In this paper, we have emulated the hardware implementation of BBALR and compared its performance with the outputs of other prominent energy-saving policies based on dynamic link rate adaption. The energy savings are mapped from the measured sleep time and reference power values. We have used NetFPGA as the testbed, which is a research platform for building real-time network hardware prototypes.																	1300-0632	1303-6203					2020	28	3					1246	1263		10.3906/elk-1906-180													
J								Design of a spurious-free RF frequency synthesizer for fast-settling receivers	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Fast-frequency hopping spread spectrum; spurious-free; integer-boundary spur; synthesizer; fractional phase-lock loop; L-band; receiver; transmitter; RF phase-lock loop; spur mitigation		A tunable reference clock frequency topology is presented as a spur reduction application for frequency synthesizers of fast frequency hopping spread spectrum systems. The method was verified by measurements on a designed hardware operating at L-band frequencies. This spur reduction method is based on optimizing the reference clock frequency of synthesizers to mitigate spurs. By using the spur reduction method, the power of spurious signals was reduced up to 57 dB. The performance of the spur reduction method was also analyzed at different loop-filter configurations. Smaller lock time was obtained by enlarging the bandwidth of the loop filter up to 150 kHz. The required power response of the spurious signals specified in telecommunication standards was achieved even though the loop filter bandwidth was enlarged.																	1300-0632	1303-6203					2020	28	3					1264	1275		10.3906/elk-1907-105													
J								Optimization of real-world outdoor campaign allocations	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Out-of-home advertisement; billboard scheduling; campaign allocation; genetic algorithms; optimization		In this paper, we investigate the outdoor campaign allocation problem (OCAP), which asks for the distribution of campaign items to billboards considering a number of constraints. In particular, for a metropolitan city with a large number of billboards, the problem becomes challenging. We propose a genetic algorithm-based method to allocate campaign items effectively, and we compare our results with those of nonlinear integer programming and greedy approaches. Real-world data sets are collected with the given constraints of the price class ratios of billboards located in Istanbul and the budgets of the given campaigns. The methods are evaluated in terms of the efficiency of the constructed plans and the construction time of the planning. The results reveal that the genetic algorithm-based approach gives close to optimal results in the shortest scheduling time for the OCAP, and it scales linearly with the increasing data sizes.																	1300-0632	1303-6203					2020	28	3					1276	1292		10.3906/elk-1907-122													
J								Adjustable testing setup for a single-loop optoelectronic oscillator with an electrical bandpass filter	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Optical path selector; optoelectronic oscillator; phase noise; free spectral range; side-mode suppression ratio; electrical bandpass filter		In this paper we present a novel method to measure the free spectral range (FSR) and side-mode suppression ratio (SMSR) of an optoelectronic oscillator (OEO) by adjusting the optical fiber length using an optical path selector and signal source analyzer. We have designed a setup for a single-loop OEO operating around 5 GHz and 10 GHz that features electrical bandpass filters for side-mode suppression. The proposed approach makes it possible to evaluate the FSR and SMSR of OEOs with different optical fiber paths without requiring the changing of fiber spools or optical connectors. This approach could be useful for testbeds that investigate the implementation of an OEO in a 5G radio access network.																	1300-0632	1303-6203					2020	28	3					1293	1302		10.3906/elk-1907-186													
J								Analysis of acoustic sensor placement for PD location in power transformer	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Partial discharge; power transformer; wave propagation; time of arrival; AE sensor	PARTIAL DISCHARGE SOURCES; LOCALIZATION	Partial discharge (PD) is an abnormal activity that occurs in high-voltage components, such as power cables, switchgear, machines, and power transformers. Such activity needs to be diagnosed for the equipment to last longer as PD could harm the insulation and potentially lead to asset destruction from time to time. Moving one or more externally mounted acoustic sensors to different locations on the transformer tank is commonly used in order to detect and locate PD signal occurring in the power transformer. However, this procedure may lead to less accuracy in PD identification. Therefore, this research paper presents an analysis of acoustic sensor placement based on time of arrival (TOA) technique for PD location in a power transformer. The detection and location can be determined by permanently installing the acoustic sensor to provide valuable data in an early stage of occurrence for online condition PD monitoring. Several methods are available for the detection of PD signal, whereby one of the best choices is via acoustic emission (AE). PD creates an ultrasonic signal used for PD detection. This paper proposes the possible placement of AE sensors to be mounted on the power transformer wall based on ideal and static PD signals. The sensors were placed in order to capture the PD signal without any disturbance signal from inside or outside the tank. The time for the signal for the first approach for each sensor is recorded to estimate the PD location using the TOA technique. A comparison between the least square method (LSM) and Gauss-Jordan elimination (GJE) for the TOA technique was analyzed to differentiate the resulting performance. This research utilized three different PD sources to apply the performance analysis on PD locations, while five cases were proposed to represent the five different placements of four sensors for the analysis. This research ultimately suggests that sensors be placed and randomly mounted on the four sides of the transformer tank, with one sensor allocated to one side. Among all five cases, Case 1 and Case 5 yielded a displacement error (DE) less than others, while between these two cases, Case 5 gave the lowest DE. The findings were recorded based on LSM and GJE methods used to differentiate the resulting performance.																	1300-0632	1303-6203					2020	28	3					1303	1313		10.3906/elk-1907-187													
J								Combined analytic hierarchy process and binary particle swarm optimization for multiobjective plug-in electric vehicles charging coordination with time-of-use tariff	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Plug-in electric vehicle; charging coordination; analytic hierarchy process; cost minimization; optimization	DISTRIBUTION NETWORKS; SMART GRIDS; MANAGEMENT	Plug-in electric vehicles (PEVs) are gaining popularity as an alternative vehicle in the past few years. The charging activities of PEVs impose extra electrical load on residential distribution system as well as increasing operational cost. There are multiple conflicting requirements and constraints during the charging activities. Therefore, this paper presents multiobjective PEV charging coordination based on weighted sum technique to provide simultaneous benefits to the power utilities and PEV users. The optimization problem of the proposed coordination is solved using binary particle swam optimization. The objectives of the coordination are to (i) minimize daily power loss, (ii) maximize power delivery to PEV, and (iii) minimize charging cost of PEV considering time-of-use tariff. In order to determine balance weighting factor for each of these objectives, analytic hierarchy process is applied. By using this approach, the best result of charging coordination can be achieved compared to uncoordinated charging. A 23-kV residential distribution system with 449-nodes is used to test the proposed approach. From the attained results, it is shown that the proposed method is effective in minimizing power loss and cost of charging with safe operation of distribution system.																	1300-0632	1303-6203					2020	28	3					1314	1330		10.3906/elk-1907-189													
J								A fully batteryless multiinput single inductor single output energy harvesting architecture	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Energy combiner; energy harvesting; solar cell; microbial fuel cell; single inductor; boost converter; multiple energy sources	BIOFUEL-CELL; INTERFACE	Conventional energy architectures that utilize multiple ambient energy sources are initiated either by an external power supply or through the addition of an extra power source (e.g., battery) to the architecture. However, these interventions compromise the goal of a self-sustainable energy harvesting system. Moreover, conventional architectures are not effective in situations where space is limited (e.g., an artificial heart) or when access to this space is difficult (e.g., human implantable devices), due to their large battery size. Thus, conventional energy combiner circuits that use multiple energy sources are not well suited for supplying power to most applications. This paper presents a fully batteryless energy combiner architecture with a single inductor for the use of multiple ambient energy sources, including a solar cell and a microbial fuel cell. For each energy source, an auxiliary circuit (i.e. a charge pump) is implemented in order to provide a power supply to a digital control circuit, which consecutively connects each ambient energy source to a power converter. This novel architecture is completely self-starting and requires no additional extra power source or battery. This architecture has been designed and verified using a 0.13-mu m CMOS process and a peak end-to-end efficiency of 79.33% for two ambient sources is achieved. This proposed system is applicable to numerous loads utilized in energy harvesting systems.																	1300-0632	1303-6203					2020	28	3					1331	1343		10.3906/elk-1907-196													
J								An arbitrary waveform magnetic nanoparticle relaxometer with an asymmetrical three-section gradiometric receive coil	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Magnetic particle imaging; magnetic nanoparticles; magnetic particle relaxometer; biosensing; magnetic nanoparticle spectrometer	RELAXATION; RESOLUTION	Magnetic nanoparticles (MNPs) have a wide range of clinical applications for imaging, therapy, and biosensing. Superparamagnetic MNPs can be directly visualized with high spatiotemporal resolution using the magnetic particle imaging (MPI) modality. The image resolution of MPI depends on the relaxation properties of the MNPs. Therefore, characterization of MNP response under alternating magnetic field excitation is necessary to predict MPI imaging performance and develop optimized MNPs. Biosensing applications also make use of the change in the relaxation response of MNPs after binding to a target agent. As MNP relaxation properties change with temperature and viscosity, noninvasive probing of these microenvironmental properties is possible. In this work, we present an untuned relaxometer to measure the relaxation properties of the MNPs in a wide frequency and amplitude range. The developed relaxometer can produce above 80 mTpp magnetic field at up to 60 kHz frequency, and above 14 mTpp at up to 150 kHz frequency. An asymmetrical three-section gradiometer receive coil is used to cancel the direct coupled signal from the transmit coil. The position of one of the receive coil sections is manually tuned using a rotating knob for improved decoupling. The tuning coil section has a lower number of turns compared to the other sections to decrease the sensitivity to mechanical movement. By tuning the knob, the transmit-receive coupling can be decreased below -80 dB. We analyzed the x-space image resolution, harmonic levels, and effect of the number of used harmonics on the resolution for two different commercially available superparamagnetic iron oxide MNPs (Perimag and Synomag-D) in a multifrequency/multiamplitude measurement scheme. The magnetization properties of MNPs for arbitrary waveforms can be measured efficiently using the developed relaxometer.																	1300-0632	1303-6203					2020	28	3					1344	1354		10.3906/elk-1907-201													
J								Low harmonic 12-pulse rectifier with a circulating current shaping circuit	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										12-pulse diode rectifier; harmonic reduction; interphase reactor; circulating current	INTERPHASE REACTOR; SCHEME; QUALITY; FILTER	This paper proposes a four-star 12-pulse diode rectifier with a circulating current shaping circuit (CCSC) on the DC side to decrease the input current harmonics effectively. The type of circulating current that can eliminate the input current harmonics is analysed and its waveform parameters are derived. The effects of triangular circulating current on the harmonics of the input current are analysed, and the harmonic suppression mechanism of the triangular circulating current is revealed. This scheme has excellent harmonic suppression capability, and the capacity of CCSC is only 2.35% of output power of the 12-pulse rectifier. Thus, this scheme is cost effective for high power applications. The experimental results confirm the theoretical analysis and near sinusoidal input current is obtained.																	1300-0632	1303-6203					2020	28	3					1355	1370		10.3906/elk-1907-204													
J								Deep temporal motion descriptor (DTMD) for human action recognition	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Human activity recognition; deep convolutional neural network; motion history images; deep temporal motion descriptor; computer vision		Spatiotemporal features have significant importance in human action recognition, as they provide the actor's shape and motion characteristics specific to each action class. This paper presents a new deep spatiotemporal human action representation, the deep temporal motion descriptor (DTMD), which shares the attributes of holistic and deep learned features. To generate the DTMD descriptor, the actor's silhouettes are gathered into single motion templates by applying motion history images. These motion templates capture the spatiotemporal movements of the actor and compactly represent the human actions using a single 2D template. Then deep convolutional neural networks are used to compute discriminative deep features from motion history templates to produce the DTMD. Later, DTMD is used for learning a model to recognize human actions using a softmax classifier. The advantage of DTMD are that DTMD is automatically learned from videos and contains higher-dimensional discriminative spatiotemporal representations as compared to handcrafted features; DTMD reduces the computational complexity of human activity recognition as all the video frames are compactly represented as a single motion template; and DTMD works effectively for single and multiview action recognition. We conducted experiments on three challenging datasets: MuHAVI-Uncut, iXMAS, and IAVID-1. The experimental findings reveal that DTMD outperforms previous methods and achieves the highest action prediction rate on the MuHAVI-Uncut dataset.																	1300-0632	1303-6203					2020	28	3					1371	1385		10.3906/elk-1907-214													
J								Peak shaving and technical loss minimization in distribution grids: a time-of-use-based pricing approach for distribution service tariffs	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Time-of-use tariffs; demand response; distribution service tariff; peak-shaving; technical loss; elasticity	RESIDENTIAL CUSTOMER RESPONSE	Deployment of time-of-use (ToU)-based retail energy tariffs (i.e. tariff for energy consumption - not the tariff for distribution service) is a common practice to incentivize consumers to use more energy at off-peak times. Distribution service tariffs (DSTs) are usually time-independent, which results in insensitivity of load to the distribution service cost. However, DST can also be time-dependent, which is studied in this paper. This study presents a methodology to address the effect of ToU pricing (i.e. time-dependent) of DSTs on peak shaving and technical loss minimization in power distribution grids. Here, the main focus is to assess the level of consumers' responses to ToU-based DSTs. Addressing such a problem necessitates detailed modeling of the distribution grid (including low-voltage grid) on the one hand and accurate modeling of the elasticity of consumers to ToU-based DSTs on the other hand. The other significant factor is the share of DST-originated cost within the total bill of the consumers. Considering these factors, the proposed approach is implemented on the pilot networks in the region of service associated with different distribution companies in Turkey. Response of consumers to ToU-based DSTs are addressed quantitatively in terms of peak shaving and technical loss minimization in the pilot regions. In addition, financial aspects of ToU-based DSTs are outlined from distribution companies and consumers standpoints.																	1300-0632	1303-6203					2020	28	3					1386	1404		10.3906/elk-1907-30													
J								Deep neural network based m-learning model for predicting mobile learners' performance	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										M-learning model; artificial neural network; deep learning algorithm; features importance; machine learning; mobile learning	EDUCATION; DESIGN	The use of deep learning (DL) techniques for mobile learning is an emerging field aimed at developing methods for finding mobile learners' learning behavior and exploring important learning features. The learning features (learning time, learning location, repetition rate, content types, learning performance, learning time duration, and so on) act as fuel to DL algorithms based on which DL algorithms can classify mobile learners into different learning groups. In this study, a powerful and efficient m-learning model is proposed based on DL techniques to model the learning process of m-learners. The proposed m-learning model determines the impact of independent learning features on the dependent feature i.e. learners' performance. The m-learning model dynamically and intuitively explores the weights of optimum learning features on learning performance for different learners in their learning environment. Then it split learners into different groups based on features differences, weights, and interrelationships. Because of the high accuracy of the DL technique, it was used to classify learners into five different groups whereas random forest (RF) ensemble method was used in determining each feature importance in making adaptive m-learning model. Our experimental study also revealed that the m-learning model was successful in helping m-learners in increasing their performance and taking the right decision during the learning flow.																	1300-0632	1303-6203					2020	28	3					1422	1441		10.3906/elk-1907-8													
J								Comparative analysis of classification techniques for network fault management	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Classification algorithms; SVM; network fault management; machine learning; network management		Network troubleshooting is a significant process. Many studies were conducted about it. The first step in the troubleshooting procedures is represented in collecting information. It's collected in order to identify the problems. Syslog messages which are sent by almost all network devices include a massive amount of data that concern the network problems. Based on several studies, it was found that analyzing syslog data (which) can be a guideline for network problems and their causes. The detection of network problems can become more efficient if the detected problems have been classified based on the network layers. Classifying syslog data requires identifying the syslog messages that describe the network problems for each layer. It also requires taking into account the formats of syslog for vendors' devices. The present study aimed to propose a method for classifying the syslog messages which identify the network problem.This classification is conducted based on the network layers. This method uses data mining instrument to classify the syslog messages. The description part of the syslog message was used for carrying out the classification process.The relevant syslog messages were identified. The features were then selected to train the classifiers. Six classification algorithms were learned; LibSVM, SMO, KNN, Naive Bayes, J48, and Random Forest. A real data set was obtained from an educational network device. This dataset was used for the prediction stage. It was found that that LibSVM outperforms other classifiers in terms of the probability rate of the classified instances where it was in the range of 89.90%-32.80%. Furthermore, the validation results indicate that the probability rate of the correctly classified instances is >70%.																	1300-0632	1303-6203					2020	28	3					1442	1457		10.3906/elk-1907-84													
J								Feature points-based image registration between satellite imagery and aerial images of agricultural land	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Agriculture land; feature point detectors; descriptors; image registration; satellite imagery; UAV images	SAMPLE CONSENSUS; AUTOMATIC REGISTRATION; MATCHING ALGORITHM; SIFT; DESCRIPTOR	Rapid advancement in remote sensing sensors has resulted in an enormous increase in the use of satellite imagery (SI) and images taken from unmanned aerial vehicles (UAVs) in a wide range of remote sensing applications. These applications include urban planning, environment monitoring, map updating, change detection, and precision agriculture. This paper focuses on an agricultural application of SI and UAV images. SI-UAV images possess high temporal, textural, and intensity differences due to rapid changes in agricultural crops with the passage of time. Feature points such as scale invariant feature transform (SIFT), oriented FAST and rotated BRIEF (ORB), and speeded-up robust features (SURF) are not invariant to such differences and underperform in SI-UAV image registration. To deal with this problem, we propose a new method that combines the strength of nearest neighbor (NN) and brute force (BF) descriptor matching strategies to register SI-UAV images. The proposed method is named NN-BF. For NN-BF first corresponding feature point descriptor matches are identified between SI-UAV images of the training set with overlap error. Then the corresponding descriptors are matched with the descriptors of SI images of the test set with NN strategy. The resulting descriptor matches are then further matched with the descriptors of UAV images of the test set using BF strategy. Finally, the descriptor matches obtained are processed with RANSAC to remove outliers and estimate a homography for image registration. Experiments are performed on an agricultural land image dataset. The experimental results show that the NN-BF method improves SIFT, SURF, and ORB feature point performances and also outperforms recently proposed feature matching strategies for remote sensing images. SIFT on average obtains 6.1% and 18.9% better precision scores than SURF and ORB with NN-BF, respectively. SIFT also obtains lower root mean square error than SURF and ORB with NN-BF.																	1300-0632	1303-6203					2020	28	3					1458	1473		10.3906/elk-1907-92													
J								Implicit relation-based question answering to answer simple questions over DBpedia	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Semantic web; question answering; linked data; natural language; SPARQL; RDF		RDF-based question answering systems give users the capability of natural language querying over RDF data. In order to respond to natural language questions, it is necessary that the main concept of the question be interpreted correctly, and then it is mapped to RDF data. A natural language question includes entities, classes, and implicit and explicit relationships. In this article, by focusing on identification and mapping of implicit relationships in the question (in addition to the explicit relationships), the mapping step has been improved. In the proposed solution (IRQA), entities and implicit/explicit relationships are identified by means of the provided rules and then will be presented as a graph. In the next phase, according to the determined priority of graph nodes, unknown nodes and their adjacent relationships are mapped to their peers in the RDF dataset. The proposed solution is evaluated on the QALD-3 test set. The results prove that the proposed work has improved performance in mapping implicit relationships and reveals higher precision and F-measure values.																	1300-0632	1303-6203					2020	28	3					1474	1490		10.3906/elk-1908-102													
J								Fault identification of catenary dropper based on improved CapsNet	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Catenary dropper; fault identification; improved CapsNet		Traditional fault identification algorithms applied to catenary dropper suffer from various problems due to its small contact area. These problems include misidentification and lower recognition rate of the faulty dropper. Compared with the traditional convolutional neural network, the vector is utilized as the input of the capsule network (CapsNet) for the first time, which can well retain the feature information such as the direction and angle of the target, and is more suitable for identifying the dropper under complex background. Therefore, this paper proposes a dropper fault identification algorithm based on improved capsule network. The convolutional layer of traditional 9 x 9 capsule network is simplified through 1 x 1 reduction layer and 3 x 3 convolutional layer, and the optimization algorithm is adopted for parameter optimization to shorten the training weight time. At the same time, the output can retain more information such as direction and angle, which can accurately identify the breakage and falling of current carrying broken. Thus, in order to better improve the accuracy and real-time of detecting the fault dropper from a running train operation, a dropper fault identification algorithm based on an improved CapsNet is proposed in this paper. Experimental results show that the improved CapsNet is well-suited for fault identification of catenary dropper, as it can effectively remove the interference caused by the complex background on the dropper image, and identify the image containing the faulty dropper with a higher recognition rate.																	1300-0632	1303-6203					2020	28	3					1491	1502		10.3906/elk-1908-138													
J								Analysis of condition number and position estimation error for multiangulation position estimation system	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Angle of arrival; free-space path loss; singular-value decomposition; position estimation	LOCALIZATION; SIGNALS; ISSUES	A passive wireless positioning system could be used to detect the location of low-level airborne targets such as drones or unmanned aircraft systems from the electromagnetic emission detected at spatially deployed ground receiving stations (GRSs). The multiangulation system proposed in this paper makes use of the angle of arrival (AOA) of the transmitted signal from the target to estimate its position through a 2-stage process. The AOA is the position-dependent signal parameter (PDSP) obtained from the target emission in the first stage, and using the PDSP and GRSs, the target location is estimated in the second stage by the angulation algorithm. Noise in the received signal results in AOA estimation error and subsequently error in the position estimation (PE). This paper focuses on the angulation process, which is the second stage of the multiangulation target location estimation process. Analysis is conducted to determine the correlation between the PE error and the condition number of the coefficient matrix of a multiangulation position estimation system. The results based on Monte Carlo simulations show that both condition number and PE error distribution increase with the target range, where the higher condition number values correlate with higher PE error values appearing around 80 degrees to 110 degrees and 260 degrees to 280 degrees of both distributions.																	1300-0632	1303-6203					2020	28	3					1503	1518		10.3906/elk-1908-179													
J								Combining metadata and co-citations for recommending related papers	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Co-citation; cosine similarity; Jensen-Shannon Divergence; metadata relatedness; research paper recommender systems	SIMILARITY; PROXIMITY	Identification of relevant documents is performed to keep track of the state-of-the-art methods and relies on research paper recommender systems. The proposed approaches for these systems can be classified into categories like content-based, collaborative filtering-based, and bibliographic information-based approaches. The content-based approaches exploit the full text of articles and provide more promising results than other approaches. However, most content is not freely available because of subscription requirements. Therefore, the scope of content-based approaches is limited. In such scenarios, the best possible alternative could be the exploitation of other openly available resources. Therefore, this research explores the possible use of metadata and bibliographic information to find related articles. The approach incorporates metadata with co-citations to find and rank related articles against a query paper. The similarity score of metadata fields is calculated and combined with co-citations. The proposed approach is evaluated on a newly constructed dataset of 5116 articles. The benchmark ranking against each co-cited document set is established by applying Jensen-Shannon divergence (JSD) and results are evaluated with the state-of-the-art content-based approach in terms of normalized discounted cumulative gain (NDCG). The state-of-the-art content-based approach achieved an NDCG score of 0.86 while the traditional co-citation-based approach scored 0.72. The presented method achieved NDCG scores of 0.73, 0.77, and 0.78 by incorporating the title, co-citation and title, and abstract, respectively, whereas the highest NDCG score of 0.77 was achieved by combining co-citations with metadata. However, better results are achieved by incorporating the title and abstract with NDCG score of 0.81. Therefore, it can be concluded that the proposed approach could be a better alternative in cases where content is unavailable.																	1300-0632	1303-6203					2020	28	3					1519	1534		10.3906/elk-1908-19													
J								A fabrication-oriented remeshing method for auxetic pattern extraction	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Auxetic; texture synthesis; filigree synthesis; geometry processing; 3D printing; digital fabrication		We propose a method for extracting auxetic patterns from meshes for fabrication by modifying the existing mesh primitives directly and fully automatically. This direct approach is novel in the sense that most of the fabrication-oriented surface tiling methods introduce additional primitives, such as curve networks in an interactive semiautomatic framework. Our method is based on a remeshing procedure that converts a given quad mesh with arbitrary topology into our desired structure that is ready to be fabricated. The main advantages of establishing auxetic patterns on meshes are the achieved flexibility using cheap inflexible materials as well as less material usage and fabrication time, as demonstrated in our results.																	1300-0632	1303-6203					2020	28	3					1535	1548		10.3906/elk-1908-51													
J								A high-level and adaptive metaheuristic selection algorithm for solving high dimensional bound-constrained continuous optimization problems	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Metaheuristics; hybridization; adaptive selection; credit assignment; continuous optimization; multiarmed bandit	EVOLUTION; STRATEGY	Metaheuristic algorithms are used to find sufficiently good solutions for the optimization problems that are not solvable in a polynomial time. Although metaheuristics offer a general problem-solving framework and can be applied to various types of optimization problems, their performances depend heavily on the problem to be solved. Thus, hybrid metaheuristics are used to combine strong parts of different algorithms. In this study, a novel adaptive metaheuristic selection algorithm is proposed for solving bound-constrained continuous optimization problems. The developed method hybridizes artificial bee colony, differential evolution, and particle swarm optimization at a high level where each algorithm works independently from each other. As a main contribution to the literature, adaptive selection at metaheuristic level among these three algorithms is achieved by using a rank-based credit assignment and UCB1 multiarmed bandit selection. The effectiveness of the developed algorithm has been evaluated on CEC'17 standard benchmark functions. The obtained numerical results indicate that the proposed algorithm outperforms the individual metaheuristics on which it is built and is more effective especially in high dimensional problems. It is also shown that the proposed algorithm is highly comparable with the related algorithms in the literature. Lastly, a case study that achieves adaptive selection of two good-performing algorithms (namely, covariance matrix adaptation evolution strategy and JADE) for the benchmark used in this study supports the effectiveness of the proposed method.																	1300-0632	1303-6203					2020	28	3					1549	1566		10.3906/elk-1908-9													
J								User profiling for TV program recommendation based on hybrid television standards using controlled clustering with genetic algorithms and artificial neural networks	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Artificial neural networks; genetic algorithms; hybrid broadcast broadband television; program recommendation; user profiling	EVOLUTION	In this paper, an earlier method proposed by the authors to make smart recommendations utilizing artificial intelligence and the latest technologies developed for the television area is expanded further using controlled clustering with genetic algorithms (CCGA). For this purpose, genetic algorithms (GAs), artificial neural networks (ANNs), and hybrid broadcast broadband television (HbbTV) are combined to get the users' television viewing habits and to create profiles. Then television programs are recommended to the users based on that profiling. The data gathered by the developed HbbTV application for previous studies are reused in this study. These data are employed to cluster users. The number of clusters is found by CCGA, a method proposed in this paper. For each cluster formed by CCGA, a separate ANN is designed to learn the viewing habits of the users of the corresponding cluster. The weight matrices are initialized also by GA. The recommendations produced using the proposed model are then presented by the same HbbTV application developed by the authors. Clustering with GAs gives better results when compared to the well-known K-means clustering algorithm.																	1300-0632	1303-6203					2020	28	3					1567	1583		10.3906/elk-1909-139													
J								A spreadsheet-based decision support system for examination timetabling	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Examination timetabling; mixed integer programming; spreadsheet-based decision support system		Examination timetabling is an inevitable problem of educational institutions. Each institution has its own particular limitations; however, the main structure is the same: assigning exams to time slots and classrooms. Several institutions solve the problem manually, but it becomes more difficult every year with increasing numbers of students and limited resources. There are many studies in the literature addressing the examination timetabling problem (ETP) and providing high quality solutions within reasonable amounts of time. Nevertheless, almost none of them can be used in practice since they are not converted into a decision support system (DSS). Commercial DSSs, on the other hand, are generally transactionally based and do not have optimization capabilities, i.e. they prevent conflicts via functional user interfaces. In this study, we propose a mixed integer programming (MIP) model that addresses the ETP of the Industrial Engineering Department of Yildiz Technical University. The model, which is capable of solving a wide range of similar ETP instances, is embedded into a DSS in the form of a spreadsheet. Given the enrollment lists of the courses, it generates schedules with minimum conflicts and consecutive exams while addressing requests of the lecturers and students. It does not require any technical knowledge and can be used by an average spreadsheet user. Moreover, it is flexible in terms of use for scheduling problems of other educational institutions. Currently, the DSS is in use by the department and real-life instances can be solved within a few seconds, saving significant amount of man-hours.																	1300-0632	1303-6203					2020	28	3					1584	1598		10.3906/elk-1909-14													
J								Two novel radar detectors for spiky sea clutter with the presence of thermal noise and interfering targets	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Maritime radar; detection; compound Gaussian; thermal noise; interfering targets	CFAR DETECTION	In the context of noncoherent detection and high-resolution maritime radar system with low grazing angle, new Constant False Alarm Rate (CFAR) decision rules are suggested for two Compound Gaussian (CG) clutters namely: The K distribution and the Compound Inverse Gaussian (CIG) distribution, which are considered among the most appropriate models for sea clutter. The proposed decision rules are then modified to deal with the presence of thermal noise and interfering targets. The proposed detectors are investigated on the basis of synthetic data as well as real data of the IPIX radar database. The obtained results exhibit a high probability of detection as well as an excellent false alarm rate regulation especially for spiky clutter.																	1300-0632	1303-6203					2020	28	3					1599	1611		10.3906/elk-1909-20													
J								Nonlinear adaptive semiactive control of a half-vehicle model via hardware in the loop simulation	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Magnetorheological damper; hardware in the loop simulation; semiactive control; adaptive control; skyhook control	SKYHOOK CONTROL; SUSPENSION; DAMPER; SYSTEM	In this study, vehicle body vibrations are semiactively controlled using a nonlinear adaptive controller designed to improve passenger comfort by guaranteeing closed loop system stability under variable road disturbances with parametric uncertainty. Semiactive vibration control is implemented to the system through the magnetorheological damper. The MR damper test system is established in laboratory conditions, and the required values that are measured from the test system are used in computer simulations via the hardware in the loop simulation (HILS) method. By this way, it is possible to avoid the financial and other difficulties of the experimental study by establishing the test system completely, and also the hesitations that may arise in terms of producing realistic results of pure simulation studies of nonlinear dynamics. A 4-degree-of-freedom half-vehicle model is developed to examine the vehicle body bounce and pitch movements, and simulations are carried out under bump and random road irregularities, and the results are presented in comparison with the performance of the conventional skyhook controller. The performances of both controllers are interpreted from the aspect of acceleration and displacement responses of the vibrations and related criteria. As a result, the vibration reduction performances of both controllers are investigated experimentally using the HILS test system and the obtained results are evaluated with some comparative figures, performance criteria, and root mean square averages of vibrations.																	1300-0632	1303-6203					2020	28	3					1612	1630		10.3906/elk-1909-73													
J								Selective personalization and group profiles for improved web search personalization	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Personalized web search; topical user model; latent Dirichlet allocation		Personalization is a common technique used in Web search engines to improve the effectiveness of retrieval. While personalizing some queries yields significant improvements in user experience by providing a ranking in line with the user preferences, it fails to improve or even degrades the effectiveness for less ambiguous queries. A potential personalization metric could improve search engines by selectively applying personalization. One such measure, click entropy uses the query history and the clicked documents for the query, which might be sparse for some queries. In this article, the topic entropy measure is improved by integrating the user distribution into the metric, robust to the sparsity problem. Furthermore, a topic model-based ranking for the personalization method is proposed using grouped user profiles. Experiments reveal that the proposed potential prediction method correlates with human query ambiguity judgments and the group profile-based ranking method improves the mean reciprocal rank by 8%.																	1300-0632	1303-6203					2020	28	3					1631	1643		10.3906/elk-1909-9													
J								A new semiempirical model determining the dielectric characteristics of citrus leaves for the remote sensing at C band	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Dielectric constant; moisture content; citrus leaves; remote sensing; C band	CONSTANT MEASUREMENT; BANANA LEAVES; VEGETATION; PERMITTIVITY; RUBBER	Dielectric parameters (i.e. permittivity) are fundamental to the simulation, design, modeling, and developing of microwave applications. For targeted objects, the complex permittivity is an essential parameter that affects its characteristics of scattering and microwave radiation. Thus, in microwave remote sensing applications, the knowledge of the dielectric property of vegetable materials is used not only to detect planting areas for monitoring and to able to specify the growth stage of them in seasonal variations, but also to determine the water requirement of the plant for controlling (water stress). This paper focuses on determining the dielectric parameters of orange and lemon leaves, grown in the Mediterranean coasts of Turkey, depending on the moisture content (MC) and frequency by measuring the samples (leaves) with waveguide transmission line technique in the larger part of the C band frequency range (4.90-7.05 GHz) (compatible with WR159) in order to propose a novel model based on curve fitting method for estimating the real part of dielectric constant (epsilon') and the imaginary part of dielectric constant (epsilon ''). Using dielectric measurement results of orange leaves, our model based on frequency and MC is compared with the dielectric measurement results of the lemon leaves, which is in the same family with orange species, to specify the accuracy of the proposed model. The determination coefficient, R-2, and mean square root of errors values are also obtained as 0.966 and 0.824, respectively.																	1300-0632	1303-6203					2020	28	3					1644	1655		10.3906/elk-1909-92													
J								An optimized FPGA design of inverse quantization and transform for HEVC decoding blocks and validation in an SW/HW environment	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										High-efficiency video coding decoder; IDCT; inverse quantization; software/hardware environment; field-programmable gate array	IMPLEMENTATION; ARCHITECTURE	This paper presents an optimized hardware architecture of the inverse quantization and the inverse transform (IQ/IT) for a high-efficiency video coding (HEVC) decoder. Our highly parallel and pipelined architecture was designed to support all HEVC Transform Unit (TU) sizes: 4 x 4, 8 x 8, 16 x 16, and 32 x 32. The IQ/IT was described in the VHSIC hardware description language and synthesized to Xilinx XC7Z020 field-programmable gate array (FPGA) and to TSMC 180 nm standard-cell library. The throughput of the hardware architecture reached in the worst case a processing rate of up to 1080 p at 33 fps at 146 MHz and 1080 p at 25 fps at 110 MHz when mapped to FPGA and standard-cells, respectively. The validation of our architecture was conducted on the ZC702 platform using a Software/Hardware (SW/HW) environment in order to evaluate different implementation methods (SW and SW/HW) in terms of power consumption and run-time. The experimental results demonstrate that the SW/HW accelerations were enhanced by more than 70% in terms of the run-time speed relative to the SW solution. Besides, the power consumption of the SW/HW designs was reduced by nearly 60% compared with the SW case.																	1300-0632	1303-6203					2020	28	3					1656	1672		10.3906/elk-1910-122													
J								Wideband patch array antenna using superstrate configuration for future 5G applications	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Antenna arrays; microstrip patch; millimeter wave communication; superstrate; wideband; 5G applications	MICROSTRIP LINEAR-ARRAY; HIGH-GAIN; DIPOLE ANTENNA; ENHANCEMENT	In this work, four distinct antenna configurations for future-centric 5G applications are proposed. Initially, a single rectangular patch is designed to operate at the frequency of 28 GHz while maintaining a wide operational band. Performance of the antenna is improved by incorporating an array of identical rectangular elements resulting in a higher gain and wider bandwidth. The proposed arrangement consists of three rectangular elements realized using 0.508-mm-thick Rogers RT/Duroid 5880 laminate. The bandwidth is further enhanced by increasing the number of radiating elements in the array from three to five. Evolution of the proposed design is concluded by stacking the superstrate layer above the five-element array structure, thereby improving the gain associated with the proposed configuration. The antenna covers the frequency band from 25.1 GHz to 30.5 GHz under consideration for 5G communications. The proposed antenna occupies a physical footprint of 3.5 x 3.5 cm(2). Performance of the presented antenna configurations is analyzed using different electromagnetic descriptors including bandwidth, reflection coefficient, gain, radiation patterns, and radiation efficiency obtained using Computer Simulation Technology Microwave Studio (CST MWS) software. The measured results, obtained after fabrication and testing, demonstrate good overall agreement with the simulated outcome. The formulated design is a prime candidate for deployment in 5G applications of the future.																	1300-0632	1303-6203					2020	28	3					1673	1685		10.3906/elk-1910-160													
J								On efficient computation of equilibrium under social coalition structures	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Algorithmic game theory; laminar equilibrium; contiguous equilibrium; resource selection games	EXISTENCE; STRATEGY; PRICE	In game-theoretic settings the key notion of analysis is an equilibrium, which is a profile of agent strategies such that no viable coalition of agents can improve upon their coalitional welfare by jointly changing their strategies. A Nash equilibrium, where viable coalitions are only singletons, and a super strong equilibrium, where every coalition is deemed viable, are two extreme scenarios in regard to coalition formation. A recent trend in the literature is to consider equilibrium notions that allow for coalition formation in between these two extremes and which are suitable to model social coalition structures that arise in various real-life settings. The recent literature considered the question on the existence of equilibria under social coalition structures mainly in Resource Selection Games (RSGs), due to the simplicity of this game form and its wide range of application domains. We take the question on the existence of equilibria under social coalition structures from the perspective of computational complexity theory. We study the problem of deciding the existence of an equilibrium in RSGs with respect to a given social coalition structure. For an arbitrary coalition structure, we show that it is computationally intractable to decide whether an equilibrium exists even in very restricted settings of RSGs. In certain settings where an equilibrium is guaranteed to exist we give polynomial-time algorithms to find an equilibrium.																	1300-0632	1303-6203					2020	28	3					1686	1698		10.3906/elk-1910-164													
J								Investigating the efficiency of multithreading application programming interfaces for parallel packet classification in wireless sensor networks	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Packet Classification; Multi-threading; Thread; Open Multiprocessing; Threading Building Blocks; wireless sensor network; efficiency	PERFORMANCE; ALGORITHM; INTERNET	This paper investigates the most appropriate application programming interface (API) that best accelerates the flow-based applications on the wireless sensor networks (WSNs). Each WSN include many sensor nodes which have limited resources. These sensor nodes are connected together using base stations. The base stations are commonly network systems with conventional processors which are responsible for handling a large amount of communicated data in flows of network packets. For this purpose, classification of the communicated packets is considered the primary process in such systems. With the advent of high-performance multicore processors, developers in the network industry have considered these processors as a striking choice for implementing a wide range of flow-based wireless sensor networking applications. The main challenge in this field is choosing and exploiting an API which best allows multithreading; i.e. one which maximally hides the latency of performing complex operations by threads and increases the overall efficiency of the cores. This paper assesses the efficiency of Thread, Open Multiprocessing, and Threading Building Blocks (TBB) libraries in multithread implementation of set-pruning and grid-of-tries packet classification algorithms on dual-core and quad-core processors. In all cases, the speed and throughput of all parallel versions of the classification algorithms are much more than the corresponding serial versions. Moreover, for parallel classification of a sufficiently large number of packets by both classification algorithms, TBB library results in higher throughput and performance than the other libraries due to its automatic scheduling and internal task stealing mechanism.																	1300-0632	1303-6203					2020	28	3					1699	1715		10.3906/elk-1910-168													
J								Human activity recognition by using MHIs of frame sequences	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Motion history image; pretrained network; spatial stream; temporal stream	VIDEOS	A motion history image (MHI) is a temporal template that collapses temporal motion information into a single image in which intensity is a function of recency of motion. In recent years, the popularity of deep learning architectures for human activity recognition has encouraged us to explore the effectiveness of combining them and Mills. Based on this, two new methods are introduced in this paper. In the first method, which is called the basic method, each video splits into N groups of consecutive frames, and the MHI is calculated for each group. Transfer learning with the fine-tuning technique is used for classifying these temporal templates. The experimental results show that some misclassification errors are created because of the similarities between these temporal templates; these errors can be corrected by detecting specific objects in the scenes. Thus, spatial information consisting of a single frame is also added to the second method, called the proposed method. By converting video classification problems into image classification problems in the proposed method, less memory is needed and the time complexity is greatly reduced. They are implemented and compared with state-of-the-art approaches on two data sets. The results show that the proposed method significantly outperforms the others. It achieves recognition accuracies of 92% and 92.4% for the UCF Sport and UCF-11 action data sets, respectively.																	1300-0632	1303-6203					2020	28	3					1716	1730		10.3906/elk-1910-71													
J								A GA-based adaptive mechanism for sensorless vector control of induction motor drives for urban electric vehicles	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Electric vehicle; induction machine; sensorless control; vector control; genetic algorithm; Lyapunov theorem; Luenberger observer	SLIDING MODE OBSERVER; SPEED; MACHINE; ROTOR	Induction motors are more attractive to car manufacturers because they are more robust and more cost effective to maintain in comparison with other types of electric machines. The evolution of their control makes them more efficient and less expensive. However, a new control technique known as sensorless control is being used to simplify the implementation of electric machines in electric vehicles. This technique involves replacing the flux and speed sensors with an observer. The estimation of these elements is based on the measurement of currents and voltages. The main purpose of the present study is to design a novel robust structure of the sensorless vector control for an urban electric vehicle. The proposed structure aims to improve the accuracy of dynamics at low speeds, eliminate sensitivity to the machine's parameters, and maintain the stability of the system even if the variation reaches high values. The speed estimation is ensured by an enhanced PI adaptation mechanism based on the full order Luenberger observer. The proof of this stability is based on the Lyapunov theorem. Moreover, a GA-based adaptive control is used for self-tuning of the stator resistance. By combining these techniques, we can enhance the efficiency and stability of the whole system.																	1300-0632	1303-6203					2020	28	3					1731	1746		10.3906/elk-1910-39													
J								A template-based code generator for web applications	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Software engineering; template-based code generation; web architecture; web application	SYSTEM; ERP	The importance and usage of web applications grow every day. Today from small businesses to large-scale corporations, many institutions prefer web applications for both their internal and external services. Code size and complexity of these kinds of applications grow rapidly. This brings up the question of how to improve the development process of web applications. A solution can be to use code generators. This paper introduces a template-based code generator to improve the development process of web applications. The code generator was developed and integrated into a real-life web application. Today, the web application together with the code generator is actively used in industry. This proves that an effective integration of a template-based code generator into a real-life large-scale web application can be achieved. In addition, the effectiveness of automatic code generation to manual implementation was shown with experimentation. Throughout the experiments, bug-free code generation was observed. Also, 98.95% improvement in average development time, 93.97% improvement in average test run count, and 49.37% improvement in average code size was achieved.																	1300-0632	1303-6203					2020	28	3					1747	1762		10.3906/elk-1910-44													
J								Experimental and predicted XLPE cable insulation properties under UV radiation	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Crosslinked polyethylene; ultraviolet; cable; insulation; artificial neural network; Levenberg-Marquardt	ARTIFICIAL NEURAL-NETWORKS; DEGRADATION; VOLTAGE; CONDUCTIVITY; IRRADIATION	This paper deals with the behavior of the crosslinked polyethylene (XLPE) used as high-voltage power cable insulation under ultraviolet (UV) radiations. For this, XLPE samples have been irradiated for 240 h using low-pressure vapor fluorescent lamps. Electrical (surface and volume resistivities), mechanical (tensile strength, elongation at break and surface hardness) and physical (weight loss, water absorption, work of water adhesion and contact angle) tests have been first carried out. Experimental results show that the XLPE characteristics are affected by UV radiation. Indeed, a decline in surface resistivity, mechanical properties, and contact angle, and an increase in the water retention amount and weight loss have been recorded. In order to predict and extrapolate some XLPE properties, a supervised artificial neural network (ANN) trained by Levenberg-Marquardt algorithm has been designed. The collected database is used to train and test the ANN performance. The obtained results show that the proposed ANN algorithm presents good estimation and prediction since the predicted output values agree with the experimental data.																	1300-0632	1303-6203					2020	28	3					1763	1775		10.3906/elk-1910-58													
J								Plane wave diffraction by strip with an integral boundary condition	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Electromagnetic diffraction; fractional boundary condition; fractional strip; fractional calculus		In this article, a new solution method is proposed for plane wave diffraction by a strip. On the surface of the strip, an integral boundary condition is used. The impedance of the strip is investigated. The theoretical and numerical analyses show that there is a relation between the complex-valued fractional order of the integral boundary condition and properties of the material such as the impedance. As a further study, the total radar cross-section is investigated using the proposed method.																	1300-0632	1303-6203					2020	28	3					1776	1790		10.3906/elk-1906-170													
J								An alternative method of biomedical signal transmission through the GSM voice channel	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Electrocardiography; GSM; speech codecs; telemedicine; voice	CARE	In this work, a new solution for online and accurate biomedical data transmission is presented. For this purpose, a global system for mobile (GSM) communication voice channel is, for the first time, used as a communication link between the patient and healthcare provider. Biomedical signals are converted into speech-like signals before being transferred over a GSM voice channel. On the receiver side, speech-like symbols are stored in a symbols bank, and constructed using random stochastic signals. On the receiver end, the index of the symbol with the most similarity to the received signal is selected as the identified sample. This method enables us to communicate with an accuracy of 99.8% at a transfer rate of 110 samples per second and signal-to-noise ratio (SNR) of 10. By utilizing a GSM voice channel, any voice channel, such as a cell phone, can be used for data transmission. The transmitted signal is encoded; therefore, the connection is secured. GSM technology has benefits such as availability, reliability, and robustness. Additionally, GSM can be used as a backup or service for transmitting vital physiological signals in emergency situations (e.g. in an ambulance). This technology can also be used to transmit other physiological signals as well as nonphysiological generic data.																	1300-0632	1303-6203					2020	28	3					1791	1802		10.3906/elk-1902-25													
J								Efficient and Fast Real-World Noisy Image Denoising by Combining Pyramid Neural Network and Two-Pathway Unscented Kalman Filter	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image prior; real-world noisy image denoising; pyramid network; two-pathway unscented Kalman filter	SPARSE; ALGORITHM	Recently, image prior learning has emerged as an effective tool for image denoising, which exploits prior knowledge to obtain sparse coding models and utilize them to reconstruct the clean image from the noisy one. Albeit promising, these prior-learning based methods suffer from some limitations such as lack of adaptivity and failed attempts to improve performance and efficiency simultaneously. With the purpose of addressing these problems, in this paper, we propose a Pyramid Guided Filter Network (PGF-Net) integrated with pyramid-based neural network and Two-Pathway Unscented Kalman Filter (TP-UKF). The combination of pyramid network and TP-UKF is based on the consideration that the former enables our model to better exploit hierarchical and multi-scale features, while the latter can guide the network to produce an improved (a posteriori) estimation of the denoising results with fine-scale image details. Through synthesizing the respective advantages of pyramid network and TP-UKF, our proposed architecture, in stark contrast to prior learning methods, is able to decompose the image denoising task into a series of more manageable stages and adaptively eliminate the noise on real images in an efficient manner. We conduct extensive experiments and show that our PGF-Net achieves notable improvement on visual perceptual quality and higher computational efficiency compared to state-of-the-art methods.																	1057-7149	1941-0042					2020	29						3927	3940		10.1109/TIP.2020.2965294													
B								Machine Learning Modeling from Health Care Data	DATA DRIVEN APPROACHES FOR HEALTHCARE: MACHINE LEARNING FOR IDENTIFYING HIGH UTILIZERS	Chapman & Hall/CRC Big Data Series																														978-0-367-34290-6				2020							15	27															
B								Descriptive Analysis of High Utilizers	DATA DRIVEN APPROACHES FOR HEALTHCARE: MACHINE LEARNING FOR IDENTIFYING HIGH UTILIZERS	Chapman & Hall/CRC Big Data Series																														978-0-367-34290-6				2020							29	46															
B								Residuals Analysis for Identifying High Utilizers	DATA DRIVEN APPROACHES FOR HEALTHCARE: MACHINE LEARNING FOR IDENTIFYING HIGH UTILIZERS	Chapman & Hall/CRC Big Data Series																														978-0-367-34290-6				2020							47	64															
B								Machine Learning Results for High Utilizers	DATA DRIVEN APPROACHES FOR HEALTHCARE: MACHINE LEARNING FOR IDENTIFYING HIGH UTILIZERS	Chapman & Hall/CRC Big Data Series																														978-0-367-34290-6				2020							65	85															
J								Learned Fast HEVC Intra Coding	IEEE TRANSACTIONS ON IMAGE PROCESSING										High efficiency video coding (HEVC); fast intra coding; convolutional neural network (CNN)	CU SIZE DECISION; MODE DECISION; ALGORITHM; PREDICTION; SELECTION	In High Efficiency Video Coding (HEVC), excellent rate-distortion (RD) performance is achieved in part by having a flexible quadtree coding unit (CU) partition and a large number of intra-prediction modes. Such an excellent RD performance is achieved at the expense of much higher computational complexity. In this paper, we propose a learned fast HEVC intra coding (LFHI) framework taking into account the comprehensive factors of fast intra coding to reach an improved configurable tradeoff between coding performance and computational complexity. First, we design a low-complex shallow asymmetric-kernel CNN (AK-CNN) to efficiently extract the local directional texture features of each block for both fast CU partition and fast intra-mode decision. Second, we introduce the concept of the minimum number of RDO candidates (MNRC) into fast mode decision, which utilizes AK-CNN to predict the minimum number of best candidates for RDO calculation to further reduce the computation of intra-mode selection. Third, an evolution optimized threshold decision (EOTD) scheme is designed to achieve configurable complexity-efficiency tradeoffs. Finally, we propose an interpolation-based prediction scheme that allows for our framework to be generalized to all quantization parameters (QPs) without the need for training the network on each QP. The experimental results demonstrate that the LFHI framework has a high degree of parallelism and achieves a much better complexity-efficiency tradeoff, achieving up to 75.2% intra-mode encoding complexity reduction with negligible rate-distortion performance degradation, superior to the existing fast intra-coding schemes.																	1057-7149	1941-0042					2020	29						5431	5446		10.1109/TIP.2020.2982832													
J								Temporal Reasoning Graph for Activity Recognition	IEEE TRANSACTIONS ON IMAGE PROCESSING										Feature extraction; Activity recognition; Convolution; Semantics; Computer architecture; Video sequences; temporal reasoning; graph convolution network; temporal graph construction; activity recognition		Despite great success has been achieved in activity analysis, it still has many challenges. Most existing works in activity recognition pay more attention to designing efficient architecture or video sampling strategy. However, due to the property of fine-grained action and long term structure in video, activity recognition is expected to reason temporal relation between video sequences. In this paper, we propose an efficient temporal reasoning graph (TRG) to simultaneously capture the appearance features and temporal relation between video sequences at multiple time scales. Specifically, we construct learnable temporal relation graphs to explore temporal relation on the multi-scale range. Additionally, to facilitate multi-scale temporal relation extraction, we design a multi-head temporal adjacent matrix to represent multi-kinds of temporal relations. Eventually, a multi-head temporal relation aggregator is proposed to extract the semantic meaning of those features convolving through the graphs. Extensive experiments are performed on widely-used large-scale datasets, such as Something-Something, Charades and Jester, and the results show that our model can achieve state-of-the-art performance. Further analysis shows that temporal relation reasoning with our TRG can extract discriminative features for activity recognition.																	1057-7149	1941-0042					2020	29						5491	5506		10.1109/TIP.2020.2985219													
J								Variational Osmosis for Non-Linear Image Fusion	IEEE TRANSACTIONS ON IMAGE PROCESSING										Osmosis filtering; image fusion; non-convex optimization; primal-dual algorithm; cultural heritage imaging	ENHANCEMENT	We propose a new variational model for non-linear image fusion. Our approach is based on the use of an osmosis energy term related to the one studied in Vogel et al. and Weickert et al. The minimization of the proposed non-convex energy realizes visually plausible image data fusion, invariant to multiplicative brightness changes. On the practical side, it requires minimal supervision and parameter tuning and can encode prior information on the structure of the images to be fused. For the numerical solution of the proposed model, we develop a primal-dual algorithm and we apply the resulting minimization scheme to solve multi-modal face fusion, color transfer and cultural heritage conservation problems. Visual and quantitative comparisons to state-of-the-art approaches prove the out-performance and the flexibility of our method.																	1057-7149	1941-0042					2020	29						5507	5516		10.1109/TIP.2020.2983537													
J								Dynamic Spatial Predicted Background	IEEE TRANSACTIONS ON IMAGE PROCESSING										Background modeling; foreground-background separation; motion detection; spatial prediction; video analysis	SUBTRACTION; INITIALIZATION; IMAGE	We present a novel method for online background modeling for static video cameras - Dynamic Spatial Predicted Background (DSPB). Our unique method employs a small subset of image pixels to predict the whole scene by exploiting pixel correlations (distant and close). DSPB acts as a hybrid model combining successful elements taken from two major approaches: local-adaptive that propose to fit a distribution pixelwise, and global-linear that reconstruct the background by finding a low-rank version of the scene. To our knowledge, this is the first attempt to combine these approaches in a unified system. DSPB models the scene as a superposition of illumination effects and predicts each pixel's value by a linear estimator comprised of only 5 pixels of the scene and can initialize the background starting from the 5th frame. By doing so, we keep the computational load low, allowing our method to be used in many real-time applications using simple hardware. The suggested prediction model of scene appearance is novel, and the scheme is very accurate and efficient computationally. We show the method merits on an application for video FG-BG separation, and how some of the main existing approaches may be challenged and how their drawbacks are less dominant in our model. Experimental results validate our findings, by computation speed and mean F-measure values on several public datasets. We also examine how results may improve by analyzing each video individually according to its content. DSPB can be successfully incorporated in other image processing tasks like change detection, video compression and video inpainting.																	1057-7149	1941-0042					2020	29						5517	5530		10.1109/TIP.2020.2983598													
J								Long-Range Binocular Vision Target Geolocation Using Handheld Electronic Devices in Outdoor Environment	IEEE TRANSACTIONS ON IMAGE PROCESSING										Cameras; Geology; Three-dimensional displays; Global Positioning System; Machine vision; Visualization; Robot vision systems; Binocular vision; target geolocation; outdoor environment; handheld electronic device	STEREO; LOCALIZATION; VEGETATION; VEHICLE	Binocular vision is a passive method of simulating the human visual principle to perceive the distance to a target. Traditional binocular vision applied to target localization is usually suitable for short-range area and indoor environment. This paper presents a novel vision-based geolocation method for long-range targets in outdoor environment, using handheld electronic devices such as smart phones and tablets. This method solves the problems in long-range localization and determining geographic coordinates of the targets in outdoor environment. It is noted that these sensors necessary for binocular vision geolocation such as the camera, GPS, and inertial measurement unit (IMU), are intergrated in these handheld electronic devices. This method, employing binocular localization model and coordinate transformations, is provided for these handheld electronic devices to obtain the GPS coordinates of the targets. Finally, two types of handheld electronic devices are used to conduct the experiments for targets in long range up to 500m. The experimental results show that this method yields the target geolocation accuracy along horizontal direction with nearly 20m, achieving comparable or even better performance than monocular vision methods.																	1057-7149	1941-0042					2020	29						5531	5541		10.1109/TIP.2020.2984898													
J								Improving Description-Based Person Re-Identification by Multi-Granularity Image-Text Alignments	IEEE TRANSACTIONS ON IMAGE PROCESSING										Task analysis; Visualization; Training; Bidirectional control; Semantics; Footwear; Glass; Description-based person re-identification; multi-granularity image-text alignments; step training strategy	SURVEILLANCE	Description-based person re-identification (Re-id) is an important task in video surveillance that requires discriminative cross-modal representations to distinguish different people. It is difficult to directly measure the similarity between images and descriptions due to the modality heterogeneity (the cross-modal problem). And all samples belonging to a single category (the fine-grained problem) makes this task even harder than the conventional image-description matching task. In this paper, we propose a Multi-granularity Image-text Alignments (MIA) model to alleviate the cross-modal fine-grained problem for better similarity evaluation in description-based person Re-id. Specifically, three different granularities, i.e., global-global, global-local and local-local alignments are carried out hierarchically. Firstly, the global-global alignment in the Global Contrast (GC) module is for matching the global contexts of images and descriptions. Secondly, the global-local alignment employs the potential relations between local components and global contexts to highlight the distinguishable components while eliminating the uninvolved ones adaptively in the Relation-guided Global-local Alignment (RGA) module. Thirdly, as for the local-local alignment, we match visual human parts with noun phrases in the Bi-directional Fine-grained Matching (BFM) module. The whole network combining multiple granularities can be end-to-end trained without complex pre-processing. To address the difficulties in training the combination of multiple granularities, an effective step training strategy is proposed to train these granularities step-by-step. Extensive experiments and analysis have shown that our method obtains the state-of-the-art performance on the CUHK-PEDES dataset and outperforms the previous methods by a significant margin.																	1057-7149	1941-0042					2020	29						5542	5556		10.1109/TIP.2020.2984883													
J								Motion Segmentation of RGB-D Sequences: Combining Semantic and Motion Information Using Statistical Inference	IEEE TRANSACTIONS ON IMAGE PROCESSING										Motion segmentation; Computer vision; Tracking; Semantics; Three-dimensional displays; Data models; Dynamics; RGB-D motion segmentation; multibody structure and motion; dynamic SLAM; EVT; Kolmogorov-Smirnov test	VISUAL ODOMETRY; D SLAM; REMOVAL	This paper presents an innovative method for motion segmentation in RGB-D dynamic videos with multiple moving objects. The focus is on finding static, small or slow moving objects (often overlooked by other methods) that their inclusion can improve the motion segmentation results. In our approach, semantic object based segmentation and motion cues are combined to estimate the number of moving objects, their motion parameters and perform segmentation. Selective object-based sampling and correspondence matching are used to estimate object specific motion parameters. The main issue with such an approach is the over segmentation of moving parts due to the fact that different objects can have the same motion (e.g. background objects). To resolve this issue, we propose to identify objects with similar motions by characterizing each motion by a distribution of a simple metric and using a statistical inference theory to assess their similarities. To demonstrate the significance of the proposed statistical inference, we present an ablation study, with and without static objects inclusion, on SLAM accuracy using the TUM-RGBD dataset. To test the effectiveness of the proposed method for finding small or slow moving objects, we applied the method to RGB-D MultiBody and SBM-RGBD motion segmentation datasets. The results showed that we can improve the accuracy of motion segmentation for small objects while remaining competitive on overall measures.																	1057-7149	1941-0042					2020	29						5557	5570		10.1109/TIP.2020.2984893													
J								Quantifying and Detecting Collective Motion in Crowd Scenes	IEEE TRANSACTIONS ON IMAGE PROCESSING										Dynamics; Feature extraction; Motion detection; Manifolds; Optical imaging; Robustness; Trajectory; Crowd analysis; Collectiveness; Manifold learning; Group detection; Clustering	TRACKING; VIDEO; FLOW	People in crowd scenes always exhibit consistent behaviors and form collective motions. The analysis of collective motion has motivated a surge of interest in computer vision. Nevertheless, the effort is hampered by the complex nature of collective motions. Considering the fact that collective motions are formed by individuals, this paper proposes a new framework for both quantifying and detecting collective motion by investigating the spatio-temporal behavior of individuals. The main contributions of this work are threefold: 1) an intention-aware model is built to fully capture the intrinsic dynamics of individuals; 2) a structure-based collectiveness measurement is developed to accurately quantify the collective properties of crowds; 3) a multi-stage clustering strategy is formulated to detect both the local and global behavior consistency in crowd scenes. Experiments on real world data sets show that our method is able to handle crowds with various structures and time-varying dynamics. Especially, the proposed method shows nearly 10% improvement over the competitors in terms of NMI, Purity and RI. Its applicability is illustrated in the context of anomaly detection and semantic scene segmentation.																	1057-7149	1941-0042					2020	29						5571	5583		10.1109/TIP.2020.2985284													
J								Anisotropic Convolution for Image Classification	IEEE TRANSACTIONS ON IMAGE PROCESSING										Convolution; Shape; Kernel; Feature extraction; Task analysis; Training; Neural networks; Anisotropic convolution; image classification; object localization		Convolutional neural networks are built upon simple but useful convolution modules. The traditional convolution has a limitation on feature extraction and object localization due to its fixed scale and geometric structure. Besides, the loss of spatial information also restricts the networks' performance and depth. To overcome these limitations, this paper proposes a novel anisotropic convolution by adding a scale factor and a shape factor into the traditional convolution. The anisotropic convolution augments the receptive fields flexibly and dynamically depending on the valid sizes of objects. In addition, the anisotropic convolution is a generalized convolution. The traditional convolution, dilated convolution and deformable convolution can be viewed as its special cases. Furthermore, in order to improve the training efficiency and avoid falling into a local optimum, this paper introduces a simplified implementation of the anisotropic convolution. The anisotropic convolution can be applied to arbitrary convolutional networks and the enhanced networks are called ACNs (anisotropic convolutional networks). Experimental results show that ACNs achieve better performance than many state-of-the-art methods and the baseline networks in tasks of image classification and object localization, especially in classification task of tiny images.																	1057-7149	1941-0042					2020	29						5584	5595		10.1109/TIP.2020.2985875													
J								A New Polyphase Down-Sampling-Based Multiple Description Image Coding	IEEE TRANSACTIONS ON IMAGE PROCESSING										Multiple description coding (MDC); compression; error compression; quantization; deblocking	DESIGN	Multiple description coding (MDC) is an efficient source coding technique for error-prone transmission over multiple channels. In this paper, we focus on the design of a new polyphase down-sampling based MDC (NPDS-MDC) for image signals. The encoding of our proposed NPDS-MDC consists of three steps. First, we perform down-sampling on each ${N} \times {N}$ image block according to the quincunx down-sampling pattern. Second, we propose a new transform and apply it to the down-sampled pixels to produce the side descriptions. Third, we develop an error compensation algorithm to reduce the compression distortion occurring on the down-sampled pixels. In our scheme, the side decoding is performed posterior to image interpolation with reference to the down-sampled compressed pixels. Moreover, the central decoding is achieved by interlacing the side descriptions. We also propose a compression-constrained central deblocking algorithm to further improve the efficiency of the central decoding. The experimental results indicate that our proposed MDC scheme offers clearly superior performance, especially at high bit rates, as compared to the state-of-the-art methods for various types of images.																	1057-7149	1941-0042					2020	29						5596	5611		10.1109/TIP.2020.2984876													
J								No-Reference Video Quality Assessment Using Natural Spatiotemporal Scene Statistics	IEEE TRANSACTIONS ON IMAGE PROCESSING										Spatiotemporal phenomena; Quality assessment; Optical distortion; Feature extraction; Video recording; Distortion; Streaming media; Natural scene statistics of videos; spatiotemporal Gabor filters; human visual system (HVS); SVR and 3D-MSCN	SUPPRESSION; PERCEPTION; FILTERS; MODELS; SPARSE; NOISE	Robust spatiotemporal representations of natural videos have several applications including quality assessment, action recognition, object tracking etc. In this paper, we propose a video representation that is based on a parameterized statistical model for the spatiotemporal statistics of mean subtracted and contrast normalized (MSCN) coefficients of natural videos. Specifically, we propose an asymmetric generalized Gaussian distribution (AGGD) to model the statistics of MSCN coefficients of natural videos and their spatiotemporal Gabor bandpass filtered outputs. We then demonstrate that the AGGD model parameters serve as good representative features for distortion discrimination. Based on this observation, we propose a supervised learning approach using support vector regression (SVR) to address the no-reference video quality assessment (NRVQA) problem. The performance of the proposed algorithm is evaluated on publicly available video quality assessment (VQA) datasets with both traditional and in-capture/authentic distortions. We show that the proposed algorithm delivers competitive performance on traditional (synthetic) distortions and acceptable performance on authentic distortions. The code for our algorithm will be released at https://www.iith.ac.in/lfovia/downloads.html.																	1057-7149	1941-0042					2020	29						5612	5624		10.1109/TIP.2020.2984879													
J								All-Pass Parametric Image Registration	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image registration; parametric fitting; geometric transformation; intensity transformation; registration evaluation; local all-pass filters	MUTUAL-INFORMATION; ELASTIC REGISTRATION; INTERPOLATION; AFFINE; MAXIMIZATION; ALIGNMENT	Image registration is a required step in many practical applications that involve the acquisition of multiple related images. In this paper, we propose a methodology to deal with both the geometric and intensity transformations in the image registration problem. The main idea is to modify an accurate and fast elastic registration algorithm (Local All-Pass-LAP) so that it returns a parametric displacement field, and to estimate the intensity changes by fitting another parametric expression. Although we demonstrate the methodology using a low-order parametric model, our approach is highly flexible and easily allows substantially richer parametrisations, while requiring only limited extra computation cost. In addition, we propose two novel quantitative criteria to evaluate the accuracy of the alignment of two images ("salience correlation") and the number of degrees of freedom ("parsimony") of a displacement field, respectively. Experimental results on both synthetic and real images demonstrate the high accuracy and computational efficiency of our methodology. Furthermore, we demonstrate that the resulting displacement fields are more parsimonious than the ones obtained in other state-of-the-art image registration approaches.																	1057-7149	1941-0042					2020	29						5625	5640		10.1109/TIP.2020.2984897													
J								CGAN-TM: A Novel Domain-to-Domain Transferring Method for Person Re-Identification	IEEE TRANSACTIONS ON IMAGE PROCESSING										Task analysis; Training; Cameras; Object tracking; Image recognition; Target recognition; Surveillance; Person re-identification; CycleGAN; triplet net; maximum mean discrepancy	RANKING	Person re-identification (re-ID) is a technique aiming to recognize person cross different cameras. Although some supervised methods have achieved favorable performance, they are far from practical application owing to the lack of labeled data. Thus, unsupervised person re-ID methods are in urgent need. Generally, the commonly used approach in existing unsupervised methods is to first utilize the source image dataset for generating a model in supervised manner, and then transfer the source image domain to the target image domain. However, images may lose their identity information after translation, and the distributions between different domains are far away. To solve these problems, we propose an image domain-to-domain translation method by keeping pedestrian's identity information and pulling closer the domains' distributions for unsupervised person re-ID tasks. Our work exploits the CycleGAN to transfer the existing labeled image domain to the unlabeled image domain. Specially, a Self-labeled Triplet Net is proposed to maintain the pedestrian identity information, and maximum mean discrepancy is introduced to pull the domain distribution closer. Extensive experiments have been conducted and the results demonstrate that the proposed method performs superiorly than the state-of-the-art unsupervised methods on DukeMTMC-reID and Market-1501.																	1057-7149	1941-0042					2020	29						5641	5651		10.1109/TIP.2020.2985545													
J								Image Clustering via Deep Embedded Dimensionality Reduction and Probability-Based Triplet Loss	IEEE TRANSACTIONS ON IMAGE PROCESSING										Dimensionality reduction; Feature extraction; Loss measurement; Clustering algorithms; Unsupervised learning; Manifolds; Image reconstruction; Image clustering; unsupervised learning; dimensionality reduction		Image clustering is more challenging than image classification. Without supervised information, current deep learning methods are difficult to be directly applied to image clustering problems. Image clustering needs to deal with three main problems: 1) the curse of dimensionality caused by high-dimensional image data; 2) extracting the effective image features; 3) combining feature extraction, dimensionality reduction and clustering. In this paper, we propose a new clustering framework called Deep Embedded Dimensionality Reduction Clustering (DERC) via Probability-Based Triplet Loss, which effectively solves the above issues. To the best of our knowledge, the DERC is the first framework that effectively combines image embedding, dimensionality reduction, and clustering into the image clustering process. We also propose to incorporate a novel probability-based triplet loss measure to retrain the DERC network as a unified framework. By integrating the reconstruction loss and the probability-based triplet loss, we can improve the image clustering accuracy. Extensive experiments show that our proposed methods outperform state-of-the-art methods on many commonly used datasets.																	1057-7149	1941-0042					2020	29						5652	5661		10.1109/TIP.2020.2984360													
J								Super-Resolution of Optical Coherence Tomography Images by Scale Mixture Models	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image reconstruction; GSM; Spatial resolution; Signal resolution; Mixture models; Estimation; Optical coherent tomography (OCT); super-resolution; sparse representation; Bayesian framework; maximum a posterior; Gaussian; Laplacia scale mixture	SPECKLE NOISE-REDUCTION; SPARSE REPRESENTATION; TRANSFORM; RECONSTRUCTION; OCT; ACQUISITION; RECOVERY; FILTER	In this paper, a new statistical model is proposed for the single image super-resolution of retinal Optical Coherence Tomography (OCT) images. OCT imaging relies on interfero-metry, which explains why OCT images suffer from a high level of noise. Moreover, data subsampling is carried out during the acquisition of OCT A-scans and B-scans. So, it is necessary to utilize effective super-resolution algorithms to reconstruct high-resolution clean OCT images. In this paper, a nonlocal sparse model-based Bayesian framework is proposed for OCT restoration. For this reason, by characterizing nonlocal patches with similar structures, known as a group, the sparse coefficients of each group of OCT images are modeled by the scale mixture models. In this base, the coefficient vector is decomposed into the point-wise product of a random vector and a positive scaling variable. Estimation of the sparse coefficients depends on the proposed distribution for the random vector and scaling variable where the Laplacian random vector and Generalized Extreme-Value (GEV) scale parameter (Laplacian+GEV model) show the best goodness of fit for each group of OCT images. Finally, a new OCT super-resolution method based on this new scale mixture model is introduced, where the maximum a posterior estimation of both sparse coefficients and scaling variables are calculated efficiently by applying an alternating minimization method. Our experimental results prove that the proposed OCT super-resolution method based on the Laplacian+GEV model outperforms other competing methods in terms of both subjective and objective visual qualities.																	1057-7149	1941-0042					2020	29						5662	5676		10.1109/TIP.2020.2984896													
J								Extended Motion Diffusion-Based Change Detection for Airport Ground Surveillance	IEEE TRANSACTIONS ON IMAGE PROCESSING										Airports; Atmospheric modeling; Aircraft; Image color analysis; Parametric statistics; Computational modeling; Adaptation models; Airport ground; change detection; aircraft detection; moving object detection; motion diffusion	BACKGROUND SUBTRACTION; FOREGROUND DETECTION; DENSITY-ESTIMATION; COLOR MODEL; SEGMENTATION; MIXTURE; PIXEL	Change detection in airport ground is important for airport security. Due to the particularity of ground environment, e.g. haze and camouflage, airport ground change detection is generally incomplete. If an incomplete detection is used as reference for the detection in subsequent frames, it may result in noticeable detection defects across the frames. In this paper, extended motion diffusion (EMD) is proposed to address the problems. The core idea of the EMD is to design a novel model insensitive to incomplete detection. Firstly the one-to-many correspondence in traditional motion diffusion is extended in the prediction step of EMD to build up correspondence from incomplete detection to intact objects. Prior information, e.g. aircraft motion prior and ground structure prior, is employed in the development of the correspondence. Then based on the correspondence a number of new samples are synthesized and filtered in the identification step of the EMD to compensate possible detection defects. Finally, the reserved samples are collected to train a foreground model, which is used in conjunction with another background model for classification. The proposed method is verified based on the Airport Ground Video Surveillance (AGVS) benchmark. Experimental results show effectiveness of the proposed algorithm in dealing with haze and camouflage.																	1057-7149	1941-0042					2020	29						5677	5686		10.1109/TIP.2020.2984854													
J								Phase Recovery Guarantees From Designed Coded Diffraction Patterns in Optical Imaging	IEEE TRANSACTIONS ON IMAGE PROCESSING										Diffraction; Apertures; Optical diffraction; Optical imaging; X-ray diffraction; Modulation; Phase retrieval; diffraction zones; coded apertures	APERTURE DESIGN; RETRIEVAL; MICROSCOPY	Phase retrieval is an inverse problem that consists in estimating a scene from diffraction intensities. This problem appears in optical imaging, which has three main diffraction zones where the measurements can be acquired, i.e., near, middle and far. Recent works have theoretically solved this inverse problem for the far zone, creating redundancy in the measurement process by including a coded aperture, which allows to modulate the scene and acquire coded diffraction patterns (CDP). However, in the state-of-the-art, the PR problem has not been theoretically studied for CDP at the near and middle zones. Moreover, the structure of the coded aperture is selected at random, leading to suboptimal estimations. Indeed, some of the coding elements employed in the literature are unfeasible because they increase the power of the scene in the modulation process. This paper provides theoretical guarantees for the recovery of a scene from CDP acquired at the three diffraction zones using admissible modulations. Based on the theoretical results, it is established that the image reconstruction quality directly depends on the coded aperture structure; therefore, a design strategy is proposed. In fact, when the scene can be sparsely represented in some basis, its support can be better estimated for a suitable choice of the coding elements in the modulation process. Experimental results show that the scene is successfully recovered by using designed coded apertures with up to 40% less measurements compared to non-designed ensembles.																	1057-7149	1941-0042					2020	29						5687	5697		10.1109/TIP.2020.2985208													
J								Densely Distilled Flow-Based Knowledge Transfer in Teacher-Student Framework for Image Classification	IEEE TRANSACTIONS ON IMAGE PROCESSING										Knowledge transfer; Training; Computational modeling; Data mining; Optimization; Image classification; Computer architecture; Teacher-student framework; image classification; densely distilled knowledge; knowledge transfer; residual network	NEURAL-NETWORK	We propose a new teacher-student framework (TSF)-based knowledge transfer method, in which knowledge in the form of dense flow across layers is distilled from a pre-trained "teacher" deep neural network (DNN) and transferred to another "student" DNN. In the case of distilled knowledge, multiple overlapped flow-based items of information from the pre-trained teacher DNN are densely extracted across layers. Transference of the densely extracted teacher information is then achieved in the TSF using repetitive sequential training from bottom to top between the teacher and student DNN models. In other words, to efficiently transmit extracted useful teacher information to the student DNN, we perform bottom-up step-by-step transfer of densely distilled knowledge. The performance of the proposed method in terms of image classification accuracy and fast optimization is compared with those of existing TSF-based knowledge transfer methods for application to reliable image datasets, including CIFAR-10, CIFAR-100, MNIST, and SVHN. When the dense flow-based sequential knowledge transfer scheme is employed in the TSF, the trained student ResNet more accurately reflects the rich information of the pre-trained teacher ResNet and exhibits superior accuracy to the existing TSF-based knowledge transfer methods for all benchmark datasets considered in this study.																	1057-7149	1941-0042					2020	29						5698	5710		10.1109/TIP.2020.2984362													
J								Color Constancy by Reweighting Image Feature Maps	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image color analysis; Estimation; Feature extraction; Kernel; Computational modeling; Convolution; Task analysis; Color constancy; illuminant estimation; convolutional neural network; computer vision	AUTOMATIC WHITE BALANCE; ILLUMINANT ESTIMATION	In this study, a novel illuminant color estimation framework is proposed for computational color constancy, which incorporates the high representational capacity of deep-learning-based models and the great interpretability of assumption-based models. The well-designed building block, feature map reweight unit (ReWU), helps to achieve comparative accuracy on benchmark datasets with respect to prior state-of-the-art deep learning based models while requiring more compact model size and cheaper computational cost. In addition to local color estimation, a confidence estimation branch is also included such that the model is able to simultaneously produce point estimate and its uncertainty estimate, which provides useful clues for local estimates aggregation and multiple illumination estimation. The source code and the dataset have been made available (https://github.com/QiuJueqin/Reweight-CC).																	1057-7149	1941-0042					2020	29						5711	5721		10.1109/TIP.2020.2985296													
J								Learning to Explore Saliency for Stereoscopic Videos Via Component-Based Interaction	IEEE TRANSACTIONS ON IMAGE PROCESSING										Videos; Visualization; Stereo image processing; Three-dimensional displays; Computational modeling; Feature extraction; Predictive models; Visual saliency; stereoscopic video; deep learning	VISUAL-ATTENTION; MODEL	In this paper, we devise a saliency prediction model for stereoscopic videos that learns to explore saliency inspired by the component-based interactions including spatial, temporal, as well as depth cues. The model first takes advantage of specific structure of 3D residual network (3D-ResNet) to model the saliency driven by spatio-temporal coherence from consecutive frames. Subsequently, the saliency inferred by implicit-depth is automatically derived based on the displacement correlation between left and right views by leveraging a deep convolutional network (ConvNet). Finally, a component-wise refinement network is devised to produce final saliency maps over time by aggregating saliency distributions obtained from multiple components. In order to further facilitate research towards stereoscopic video saliency, we create a new dataset including 175 stereoscopic video sequences with diverse content, as well as their dense eye fixation annotations. Extensive experiments support that our proposed model can achieve superior performance compared to the state-of-the-art methods on all publicly available eye fixation datasets.																	1057-7149	1941-0042					2020	29						5722	5736		10.1109/TIP.2020.2985531													
J								Advancing Image Understanding in Poor Visibility Environments: A Collective Benchmark Study	IEEE TRANSACTIONS ON IMAGE PROCESSING										Poor visibility environment; object detection; face detection; haze; rain; low-light conditions	REMOVE RAIN; SUPERRESOLUTION; DECOMPOSITION; ENHANCEMENT; ALGORITHM; RETINEX; OBJECT	Existing enhancement methods are empirically expected to help the high-level end computer vision task: however, that is observed to not always be the case in practice. We focus on object or face detection in poor visibility enhancements caused by bad weathers (haze, rain) and low light conditions. To provide a more thorough examination and fair comparison, we introduce three benchmark sets collected in real-world hazy, rainy, and low-light conditions, respectively, with annotated objects/faces. We launched the UG(2+) challenge Track 2 competition in IEEE CVPR 2019, aiming to evoke a comprehensive discussion and exploration about whether and how low-level vision techniques can benefit the high-level automatic visual recognition in various scenarios. To our best knowledge, this is the first and currently largest effort of its kind. Baseline results by cascading existing enhancement and detection models are reported, indicating the highly challenging nature of our new data as well as the large room for further technical innovations. Thanks to a large participation from the research community, we are able to analyze representative team solutions, striving to better identify the strengths and limitations of existing mindsets as well as the future directions.																	1057-7149	1941-0042					2020	29						5737	5752		10.1109/TIP.2020.2981922													
J								Learning to Reconstruct and Understand Indoor Scenes From Sparse Views	IEEE TRANSACTIONS ON IMAGE PROCESSING										3D reconstruction; deep learning; semantic segmentation; indoor scenes; sparse views		This paper proposes a new method for simultaneous 3D reconstruction and semantic segmentation for indoor scenes. Unlike existing methods that require recording a video using a color camera and/or a depth camera, our method only needs a small number of (e.g., 35) color images from uncalibrated sparse views, which significantly simplifies data acquisition and broadens applicable scenarios. To achieve promising 3D reconstruction from sparse views with limited overlap, our method first recovers the depth map and semantic information for each view, and then fuses the depth maps into a 3D scene. To this end, we design an iterative deep architecture, named IterNet, to estimate the depth map and semantic segmentation alternately. To obtain accurate alignment between views with limited overlap, we further propose a joint global and local registration method to reconstruct a 3D scene with semantic information. We also make available a new indoor synthetic dataset, containing photorealistic high-resolution RGB images, accurate depth maps and pixel-level semantic labels for thousands of complex layouts. Experimental results on public datasets and our dataset demonstrate that our method achieves more accurate depth estimation, smaller semantic segmentation errors, and better 3D reconstruction results over state-of-the-art methods.																	1057-7149	1941-0042					2020	29						5753	5766		10.1109/TIP.2020.2986712													
J								Perceptual Temporal Incoherence-Guided Stereo Video Retargeting	IEEE TRANSACTIONS ON IMAGE PROCESSING										Shape; Coherence; Two dimensional displays; Three-dimensional displays; Visualization; Distortion; Microsoft Windows; Stereo video retargeting; shape preservation; depth preservation; nonuniform warping; temporal coherence	IMAGE QUALITY ASSESSMENT; PRESERVING APPROACH; COHERENCE	Stereo video retargeting aims at minimizing shape and depth distortions with temporal coherence in resizing a stereo video content to a desired size. Existing methods extend stereo image retargeting schemes to stereo video retargeting by adding additional temporal constraints that demand temporal coherence in all corresponding regions. However, such a straightforward extension incurs conflicts among multiple requirements (i.e., shape and depth preservation and their temporal coherence), thus failing to meet one or more of these requirements satisfactorily. To mitigate conflicts among depth, shape, and temporal constraints and avoid degrading temporal coherence perceptually, we relax temporal constraints for non-paired regions at frame boundaries, derive new temporal constraints to improve human viewing experience of a 3D scene, and propose an efficient grid-based implementation for stereo video retargeting. Experimental results demonstrate that our method achieves superior visual quality over existing methods.																	1057-7149	1941-0042					2020	29						5767	5782		10.1109/TIP.2020.2984899													
J								STA-CNN: Convolutional Spatial-Temporal Attention Learning for Action Recognition	IEEE TRANSACTIONS ON IMAGE PROCESSING										Videos; Feature extraction; Motion segmentation; Computational modeling; Image recognition; Solid modeling; Convolutional neural networks; Temporal attention; spatial attention; convolutional neural network; action recognition		Convolutional Neural Networks have achieved excellent successes for object recognition in still images. However, the improvement of Convolutional Neural Networks over the traditional methods for recognizing actions in videos is not so significant, because the raw videos usually have much more redundant or irrelevant information than still images. In this paper, we propose a Spatial-Temporal Attentive Convolutional Neural Network (STA-CNN) which selects the discriminative temporal segments and focuses on the informative spatial regions automatically. The STA-CNN model incorporates a Temporal Attention Mechanism and a Spatial Attention Mechanism into a unified convolutional network to recognize actions in videos. The novel Temporal Attention Mechanism automatically mines the discriminative temporal segments from long and noisy videos. The Spatial Attention Mechanism firstly exploits the instantaneous motion information in optical flow features to locate the motion salient regions and it is then trained by an auxiliary classification loss with a Global Average Pooling layer to focus on the discriminative non-motion regions in the video frame. The STA-CNN model achieves the state-of-the-art performance on two of the most challenging datasets, UCF-101 (95.8%) and HMDB-51 (71.5%).																	1057-7149	1941-0042					2020	29						5783	5793		10.1109/TIP.2020.2984904													
J								An adaptive correlation based video data mining using machine learning	INTERNATIONAL JOURNAL OF KNOWLEDGE-BASED AND INTELLIGENT ENGINEERING SYSTEMS										Video data mining; key frame reduction; adaptive similarity; video retrieval; multiple dataset performance		With the immense growth in the multimedia contents for education and other purposes, the availability of video contents has also increased. Nevertheless, the retrieval of content is always a challenge. The identification of two video contents based on internal content similarity highly depends on extraction of key frames and that makes the process highly time complex. Recently, many research attempts have tried to approach this problem with the intention to reduce the time complexity using various methods such as video to text conversion and further analysing both extracted text similarity analysis. Regardless to mention, this strategy is again language dependent and criticised for various reasons like local language dependencies and language paraphrase dependencies. Henceforth, this work approaches the problem with a different dimension with reduction possibilities of the video key frames using adaptive similarity. The proposed method analyses the key frames extracted from the library content and from the search video data based on various parameters and reduces the key frames using adaptive similarity. Also, this work uses machine learning and parallel programming algorithms to reduce the time complexity to a greater extend. The final outcome of this work is a reduced time complex algorithm for video data-based search to video content retrieval. The work demonstrates a nearly 50% reduction in the key frame without losing information with nearly 70% reduction in time complexity and 100% accuracy on search results.																	1327-2314	1875-8827					2020	24	1					1	9		10.3233/KES-200023													
J								Detection of shot boundaries and extraction of key frames for video retrieval	INTERNATIONAL JOURNAL OF KNOWLEDGE-BASED AND INTELLIGENT ENGINEERING SYSTEMS										Video dataset; frame separation; Scale Invariant Feature Extraction; similarity measures; shot transitions; representative frames	KEYFRAME EXTRACTION	The prerequisite steps for summarizing, retrieval of video are detection of shot transitions and extraction of key frames. We hypothesized an advanced, ultra-modern Scale Invariant Feature Transform (SIFT). This SIFT method captures statistical modifications of various shot transitions, next the key frames or representative frames are extracted from those segmented shot with the calculation of entropy for each frame in the shot. We can amplify the performance of over proposed system by removing the repeated representative frames using the technique called edge matching rate. This intensified algorithm is applied to variable classes of videos to perceive shot transitions and getting of the key frame. Thus, the proposed algorithm proves its efficacy and accuracy in exhibiting its experimental results.																	1327-2314	1875-8827					2020	24	1					11	17		10.3233/KES-200024													
J								Co-occurrence analysis of scientific documents in citation networks	INTERNATIONAL JOURNAL OF KNOWLEDGE-BASED AND INTELLIGENT ENGINEERING SYSTEMS										Citation; co-occurrence; co-citation analysis; network density; triadic closure	AUTHOR COCITATION; GENERATION; SCIENCE	Citation pattern analysis is an important component for measuring research performance of the author. Co-citation analysis has been used as an effective method to find citation pattern analysis, but it does not take co-occurrence analysis among authors. A method to find co-occurrence analysis among authors is proposed in this work. Web of science citation data set is used for experimentation and parsing of this data, extracted first author name and CR link. The next step in the proposed model is to find first author co-occurrence frequency and followed by all author co-occurrence frequency. In this work applied network density, Triadic closure on co-citation graph. The proposed method provides the author-author co-relation, predicting, who is probably next co-author and author collaborate on community of authors.																	1327-2314	1875-8827					2020	24	1					19	25		10.3233/KES-200025													
J								An integrated approach for mining closed and generator high utility itemsets	INTERNATIONAL JOURNAL OF KNOWLEDGE-BASED AND INTELLIGENT ENGINEERING SYSTEMS										HUI's; CHUIM; generator; hash; data mining	EFFICIENT ALGORITHMS	High Utility Itemset Mining (HUIM) is playing an important role in extracting meaningful knowledge that is profitable itemsets rather than their occurrence frequency. HUIM result thrives into a huge number of utility itemsets that makes it difficult in decision policy. The alternative procedure is condensed representation of HUI's that contains utility itemsets without any redundancy. Closed High Utility Itemsets (CHUI) and High Utility Generators (HUG) are lossless utility itemsets useful in recommendation systems. In this paper, we address how to extract Closed and Generator HUI's from the given dataset. For faster accessing, it is proposed hash based technique that store itemsets with the frequency, utility value, closed flag, and generator flag. Experimental results shows that it outperforms other approaches in time and space.																	1327-2314	1875-8827					2020	24	1					27	35		10.3233/KES-200026													
J								A novel energy efficient obstacle aware routing algorithm for MANET	INTERNATIONAL JOURNAL OF KNOWLEDGE-BASED AND INTELLIGENT ENGINEERING SYSTEMS										Energy efficiency; DeCasteljau algorithm; Bezier curve; obstacles		In Mobile Adhoc Network (MANET), all nodes are energy and mobility constrained. The presence of obstacles in MANET severely degrades the efficiency of routing protocols in MANETs and consumes more energy for searching new routes. The high energy consumption of wireless nodes makes the MANET routing unpredictable. Therefore, it is important to consider the presence of obstacles in the network while evaluating the performance of the MANET. In this work, we have developed an Energy Efficient and Obstacle Aware Routing Algorithm (EEOARA) for MANET. For obstacles avoidance, the concept of Bezier curve based on DeCasteljau algorithm is used. In the proposed algorithm, to increase the availability of routes and to reduce the number of non-participating nodes, minimum energy consuming path which has maximum residual energy is preferred and selected. The performance of MANET is evaluated using NS2 simulator. Simulation results show that the proposed EEOARA reduces energy consumption, network overhead, delay and increases data delivery in the network.																	1327-2314	1875-8827					2020	24	1					37	44		10.3233/KES-200027													
J								Development and performance assessment of adaptive nonlinear models for revenue prediction of a mobile network operator	INTERNATIONAL JOURNAL OF KNOWLEDGE-BASED AND INTELLIGENT ENGINEERING SYSTEMS										Value-added services; nonvoice services; adaptive linear combiner; non-linear adaptive prediction models	CUSTOMER CHURN PREDICTION; SUPPORT VECTOR MACHINE; NEURAL-NETWORKS; STOCK; EXCHANGE	The commoditization of voice, saturation of the urban market, fierce competition, and the increased cost of the spectrum have forced the mobile telecom network operators to promote and garner revenue from non-voice services (NVS) namely value-added services (VAS) and data services (DS). It is a fact that monthly revenue from different segments of a mobile telecom service provider is non-linearly related to its previous revenue. Hence, existing linear prediction models such as regression and linear combiner do not exhibit accurate prediction performance. This article proposes one linear adaptive linear combiner (ALC) and three nonlinear (trigonometric expansion based neural network (TENN), multi-layer perceptron (MLP) and radial basis function (RBF)) models for prediction of revenue from voice services (VS), DS and VAS segments. The real-life revenue data of a mobile telecom service provider in a licensing area of India is utilized for this study. The predictor performance has been obtained from the simulation study of the models and analyzed. It is observed that the prediction performance of linear ALC model is the worst. The TENN model outperforms the MLP and RBF models from amongst the proposed nonlinear models based on the available historical data. In essence, the study demonstrates that the overall ranking of the four models based on prediction performance are TENN, RBF, MLP, and ALC.																	1327-2314	1875-8827					2020	24	1					45	61		10.3233/KES-200028													
J								ACO based comprehensive model for software fault prediction	INTERNATIONAL JOURNAL OF KNOWLEDGE-BASED AND INTELLIGENT ENGINEERING SYSTEMS										Software metric; fault prediction; ACO	DEFECT PREDICTION; CLASSIFICATION	The comprehensive models can be used for software quality modelling which involves prediction of low-quality modules using interpretable rules. Such comprehensive model can guide the design and testing team to focus on the poor quality modules, thereby, limited resources allocated for software quality inspection can be targeted only towards modules that are likely to be defective. Ant Colony Optimization (ACO) based learner is one potential way to obtain rules that can classify the software modules faulty and not faulty. This paper investigates ACO based mining approach with ROC based rule quality updation to constructs a rule-based software fault prediction model with useful metrics. We have also investigated the effect of feature selection on ACO based and other benchmark algorithms. We tested the proposed method on several publicly available software fault data sets. We compared the performance of ACO based learning with the results of three benchmark classifiers on the basis of area under the receiver operating characteristic curve. The evaluation of performance measure proves that the ACO based learner outperforms other benchmark techniques.																	1327-2314	1875-8827					2020	24	1					63	71		10.3233/KES-200029													
J								Multi-Player Bandits: The Adversarial Case	JOURNAL OF MACHINE LEARNING RESEARCH										Multi-Armed Bandits; Multi-Player Problems; Online Learning; Sequential Decision Making; Cognitive Radio Networksd	MULTIARMED BANDIT; MEDIUM ACCESS	We consider a setting where multiple players sequentially choose among a common set of actions (arms). Motivated by an application to cognitive radio networks, we assume that players incur a loss upon colliding, and that communication between players is not possible. Existing approaches assume that the system is stationary. Yet this assumption is often violated in practice, e.g., due to signal strength fluctuations. In this work, we design the first multi-player Bandit algorithm that provably works in arbitrarily changing environments, where the losses of the arms may even be chosen by an adversary. This resolves an open problem posed by Rosenski et al. (2016).																	1532-4435						2020	21																						
J								Kymatio: Scattering Transforms in Python	JOURNAL OF MACHINE LEARNING RESEARCH										Scattering Transform; GPUs; Wavelets; Convolutional Networks; Invariance		The wavelet scattering transform is an invariant and stable signal representation suitable for many signal processing and machine learning applications. We present the Kymatio software package, an easy-to-use, high-performance Python implementation of the scattering transform in 1D, 2D, and 3D that is compatible with modern deep learning frameworks, including PyTorch and TensorFlow/Keras. The transforms are implemented on both CPUs and GPUs, the latter offering a significant speedup over the former. The package also has a small memory footprint. Source code, documentation, and examples are available under a BSD license at https://www.kymat.io.																	1532-4435						2020	21																						
J								Union of Low-Rank Tensor Spaces: Clustering and Completion	JOURNAL OF MACHINE LEARNING RESEARCH										Low-rank tensor completion; canonical polyadic (CP) decomposition; union of tensor spaces; clustering tensor spaces; finite completability; unique completability	ALGORITHM	We consider the problem of clustering and completing a set of tensors with missing data that are drawn from a union of low-rank tensor spaces. In the clustering problem, given a partially sampled tensor data that is composed of a number of subtensors, each chosen from one of a certain number of unknown tensor spaces, we need to group the subtensors that belong to the same tensor space. We provide a geometrical analysis on the sampling pattern and subsequently derive the sampling rate that guarantees the correct clustering under some assumptions with high probability. Moreover, we investigate the fundamental conditions for finite/unique completability for the union of tensor spaces completion problem. Both deterministic and probabilistic conditions on the sampling pattern to ensure finite/unique completability are obtained. For both the clustering and completion problems, our tensor analysis provides significantly better bound than the bound given by the matrix analysis applied to any unfolding of the tensor data.																	1532-4435						2020	21																						
J								The Weight Function in the Subtree Kernel is Decisive	JOURNAL OF MACHINE LEARNING RESEARCH										classification of tree data; kernel methods; subtree kernel; weight function; tree compression		Tree data are ubiquitous because they model a large variety of situations, e.g., the architecture of plants, the secondary structure of RNA, or the hierarchy of XML files. Nevertheless, the analysis of these non-Euclidean data is difficult per se. In this paper, we focus on the subtree kernel that is a convolution kernel for tree data introduced by Vishwanathan and Smola in the early 2000's. More precisely, we investigate the influence of the weight function from a theoretical perspective and in real data applications. We establish on a 2-classes stochastic model that the performance of the subtree kernel is improved when the weight of leaves vanishes, which motivates the definition of a new weight function, learned from the data and not fixed by the user as usually done. To this end, we define a unified framework for computing the subtree kernel from ordered or unordered trees, that is particularly suitable for tuning parameters. We show through eight real data classification problems the great efficiency of our approach, in particular for small data sets, which also states the high importance of the weight function. Finally, a visualization tool of the significant features is derived.																	1532-4435						2020	21																						
J								(1+epsilon)-class Classification: an Anomaly Detection Method for Highly Imbalanced or Incomplete Data Sets	JOURNAL OF MACHINE LEARNING RESEARCH										Anomaly Detection; Imbalanced Data Sets; Neural Networks; One-class Classification; Regularization	DEEP	Anomaly detection is not an easy problem since distribution of anomalous samples is unknown a priori. We explore a novel method that gives a trade-off possibility between one-class and two-class approaches, and leads to a better performance on anomaly detection problems with small or non-representative anomalous samples. The method is evaluated using several data sets and compared to a set of conventional one-class and two-class approaches.																	1532-4435						2020	21																						
J								On Stationary-Point Hitting Time and Ergodicity of Stochastic Gradient Langevin Dynamics	JOURNAL OF MACHINE LEARNING RESEARCH												Stochastic gradient Langevin dynamics (SGLD) is a fundamental algorithm in stochastic optimization. Recent work by Zhang et al. (2017) presents an analysis for the hitting time of SGLD for the first and second order stationary points. The proof in Zhang et al. (2017) is a two-stage procedure through bounding the Cheeger's constant, which is rather complicated and leads to loose bounds. In this paper, using intuitions from stochastic differential equations, we provide a direct analysis for the hitting times of SGLD to the first and second order stationary points. Our analysis is straightforward. It only relies on basic linear algebra and probability theory tools. Our direct analysis also leads to tighter bounds comparing to Zhang et al. (2017) and shows the explicit dependence of the hitting time on different factors, including dimensionality, smoothness, noise strength, and step size effects. Under suitable conditions, we show that the hitting time of SGLD to first-order stationary points can be dimension-independent. Moreover, we apply our analysis to study several important online estimation problems in machine learning, including linear regression, matrix factorization, and online PCA.																	1532-4435						2020	21																						
J								Expected Policy Gradients for Reinforcement Learning	JOURNAL OF MACHINE LEARNING RESEARCH										policy gradients; exploration; bounded actions; reinforcement learning; Markov decision process (MDP)		We propose expected policy gradients (EPG), which unify stochastic policy gradients (SPG) and deterministic policy gradients (DPG) for reinforcement learning. Inspired by expected sarsa, EPG integrates (or sums) across actions when estimating the gradient, instead of relying only on the action in the sampled trajectory. For continuous action spaces, we first derive a practical result for Gaussian policies and quadratic critics and then extend it to a universal analytical method, covering a broad class of actors and critics, including Gaussian, exponential families, and policies with bounded support. For Gaussian policies, we introduce an exploration method that uses covariance proportional to eH, where H is the scaled Hessian of the critic with respect to the actions. For discrete action spaces, we derive a variant of EPG based on softmax policies. We also establish a new general policy gradient theorem, of which the stochastic and deterministic policy gradient theorems are special cases. Furthermore, we prove that EPG reduces the variance of the gradient estimates without requiring deterministic policies and with little computational overhead. Finally, we provide an extensive experimental evaluation of EPG and show that it outperforms existing approaches on multiple challenging control domains.																	1532-4435						2020	21																						
J								WONDER: Weighted One-shot Distributed Ridge Regression in High Dimensions	JOURNAL OF MACHINE LEARNING RESEARCH										distributed learning; ridge regression; high-dimensional statistics; random matrix theory	DIVIDE-AND-CONQUER; QUANTILE REGRESSION	In many areas, practitioners need to analyze large data sets that challenge conventional single-machine computing. To scale up data analysis, distributed and parallel computing approaches are increasingly needed. Here we study a fundamental and highly important problem in this area: How to do ridge regression in a distributed computing environment? Ridge regression is an extremely popular method for supervised learning, and has several optimality properties, thus it is important to study. We study one-shot methods that construct weighted combinations of ridge regression estimators computed on each machine. By analyzing the mean squared error in a high-dimensional random-effects model where each predictor has a small effect, we discover several new phenomena. Infinite-worker limit: The distributed estimator works well for very large numbers of machines, a phenomenon we call "infinite-worker limit". Optimal weights: The optimal weights for combining local estimators sum to more than unity, due to the downward bias of ridge. Thus, all averaging methods are suboptimal. We also propose a new Weighted ONe-shot DistributEd Ridge regression algorithm (WONDER). We test WONDER in simulation studies and using the Million Song Dataset as an example. There it can save at least 100x in computation time, while nearly preserving test accuracy.																	1532-4435						2020	21																						
J								High-Dimensional Inference for Cluster-Based Graphical Models	JOURNAL OF MACHINE LEARNING RESEARCH										Berry-Esseen bound; Graphical model; Latent variables; High-dimensional inference; Clustering; False discovery rate	FALSE DISCOVERY RATE; CONFIDENCE-INTERVALS; COVARIANCE ESTIMATION; DEFAULT MODE; RATES; CONVERGENCE; SELECTION; REGIONS; BRAIN	Motivated by modern applications in which one constructs graphical models based on a very large number of features, this paper introduces a new class of cluster-based graphical models, in which variable clustering is applied as an initial step for reducing the dimension of the feature space. We employ model assisted clustering, in which the clusters contain features that are similar to the same unobserved latent variable. Two different cluster-based Gaussian graphical models are considered: the latent variable graph, corresponding to the graphical model associated with the unobserved latent variables, and the cluster-average graph, corresponding to the vector of features averaged over clusters. Our study reveals that likelihood based inference for the latent graph, not analyzed previously, is analytically intractable. Our main contribution is the development and analysis of alternative estimation and inference strategies, for the precision matrix of an unobservable latent vector Z. We replace the likelihood of the data by an appropriate class of empirical risk functions, that can be specialized to the latent graphical model and to the simpler, but under-analyzed, cluster-average graphical model. The estimators thus derived can be used for inference on the graph structure, for instance on edge strength or pattern recovery. Inference is based on the asymptotic limits of the entry-wise estimates of the precision matrices associated with the conditional independence graphs under consideration. While taking the uncertainty induced by the clustering step into account, we establish Berry-Esseen central limit theorems for the proposed estimators. It is noteworthy that, although the clusters are estimated adaptively from the data, the central limit theorems regarding the entries of the estimated graphs are proved under the same conditions one would use if the clusters were known in advance. As an illustration of the usage of these newly developed inferential tools, we show that they can be reliably used for recovery of the sparsity pattern of the graphs we study, under FDR control, which is verified via simulation studies and an fMRI data analysis. These experimental results confirm the theoretically established difference between the two graph structures. Furthermore, the data analysis suggests that the latent variable graph, corresponding to the unobserved cluster centers, can help provide more insight into the understanding of the brain connectivity networks relative to the simpler, average-based, graph.																	1532-4435						2020	21																						
J								Ensemble Learning for Relational Data	JOURNAL OF MACHINE LEARNING RESEARCH										Ensemble learning; relational ensemble; collective classification; collective inference; bias-variance decomposition; relational machine learning; theoretical framework	VARIANCE; BIAS	We present a theoretical analysis framework for relational ensemble models. We show that ensembles of collective classifiers can improve predictions for graph data by reducing errors due to variance in both learning and inference. In addition, we propose a relational ensemble framework that combines a relational ensemble learning approach with a relational ensemble inference approach for collective classification. The proposed ensemble techniques are applicable for both single and multiple graph settings. Experiments on both synthetic and real-world data demonstrate the effectiveness of the proposed framework. Finally, our experimental results support the theoretical analysis and confirm that ensemble algorithms that explicitly focus on both learning and inference processes and aim at reducing errors associated with both, are the best performers.																	1532-4435						2020	21																						
J								GADMM: Fast and Communication Efficient Framework for Distributed Machine Learning	JOURNAL OF MACHINE LEARNING RESEARCH											OPTIMIZATION; CONSENSUS; CONVERGENCE; ALGORITHM; ADMM	When the data is distributed across multiple servers, lowering the communication cost between the servers (or workers) while solving the distributed learning problem is an important problem and is the focus of this paper. In particular, we propose a fast, and communication-efficient decentralized framework to solve the distributed machine learning (DML) problem. The proposed algorithm, Group Alternating Direction Method of Multipliers (GADMM) is based on the Alternating Direction Method of Multipliers (ADMM) framework. The key novelty in GADMM is that it solves the problem in a decentralized topology where at most half of the workers are competing for the limited communication resources at any given time. Moreover, each worker exchanges the locally trained model only with two neighboring workers, thereby training a global model with a lower amount of communication overhead in each exchange. We prove that GADMM converges to the optimal solution for convex loss functions, and numerically show that it converges faster and more communication-efficient than the state-of-the-art communication-efficient algorithms such as the Lazily Aggregated Gradient (LAG) and dual averaging, in linear and logistic regression tasks on synthetic and real datasets. Furthermore, we propose Dynamic GADMM (D-GADMM), a variant of GADMM, and prove its convergence under the time-varying network topology of the workers.																	1532-4435						2020	21																						
J								Exact Guarantees on the Absence of Spurious Local Minima for Non-negative Rank-1 Robust Principal Component Analysis	JOURNAL OF MACHINE LEARNING RESEARCH											PROGRAMMING ALGORITHM; MATRIX FACTORIZATION; OPTIMIZATION; NONSMOOTH; NONCONVEX	This work is concerned with the non-negative rank-1 robust principal component analysis (RPCA), where the goal is to recover the dominant non-negative principal components of a data matrix precisely, where a number of measurements could be grossly corrupted with sparse and arbitrary large noise. Most of the known techniques for solving the RPCA rely on convex relaxation methods by lifting the problem to a higher dimension, which significantly increase the number of variables. As an alternative, the well-known Burer-Monteiro approach can be used to cast the RPCA as a non-convex and non-smooth '1 optimization problem with a significantly smaller number of variables. In this work, we show that the low-dimensional formulation of the symmetric and asymmetric positive rank-1 RPCA based on the Burer-Monteiro approach has benign landscape, i.e., 1) it does not have any spurious local solution, 2) has a unique global solution, and 3) its unique global solution coincides with the true components. An implication of this result is that simple local search algorithms are guaranteed to achieve a zero global optimality gap when directly applied to the low-dimensional formulation. Furthermore, we provide strong deterministic and probabilistic guarantees for the exact recovery of the true principal components. In particular, it is shown that a constant fraction of the measurements could be grossly corrupted and yet they would not create any spurious local solution.																	1532-4435						2020	21																						
J								Fast Rates for General Unbounded Loss Functions: From ERM to Generalized Bayes	JOURNAL OF MACHINE LEARNING RESEARCH										Statistical Learning Theory; Fast Rates; PAC-Bayes; Misspecification; Generalized Bayes	FAST LEARNING RATES; CONVERGENCE-RATES; MODEL SELECTION; POSTERIOR DISTRIBUTIONS; INFORMATION; BOUNDS; COMPLEXITY; ENTROPY; INEQUALITIES; AGGREGATION	We present new excess risk bounds for general unbounded loss functions including log loss and squared loss, where the distribution of the losses may be heavy-tailed. The bounds hold for general estimators, but they are optimized when applied to eta-generalized Bayesian, MDL, and empirical risk minimization estimators. In the case of log loss, the bounds imply convergence rates for generalized Bayesian inference under misspecification in terms of a generalization of the Hellinger metric as long as the learning rate eta is set correctly. For general loss functions, our bounds rely on two separate conditions: the v -GRIP(generalized reversed information projection) conditions, which control the lower tail of the excess loss; and the newly introduced witness condition, which controls the upper tail. The parameter v in the v-GRIP conditions determines the achievable rate and is akin to the exponent in the Tsybakov margin condition and the Bernstein condition for bounded losses, which the v-GRIP conditions generalize; favorable v in combination with small model complexity leads to (O) over tilde (1/n) rates. The witness condition allows us to connect the excess risk to an "annealed" version thereof, by which we generalize several previous results connecting Hellinger and Renyi divergence to KL divergence.																	1532-4435						2020	21																						
J								Scalable Approximate MCMC Algorithms for the Horseshoe Prior	JOURNAL OF MACHINE LEARNING RESEARCH										Bayesian; High dimensional; MCMC approximation; Perturbation theory; Shrinkage prior	GEOMETRIC ERGODICITY; CONVERGENCE; SELECTION; SHRINKAGE; ESTIMATOR; RATES	The horseshoe prior is frequently employed in Bayesian analysis of high-dimensional models, and has been shown to achieve minimax optimal risk properties when the truth is sparse. While optimization-based algorithms for the extremely popular Lasso and elastic net procedures can scale to dimension in the hundreds of thousands, algorithms for the horseshoe that use Markov chain Monte Carlo (MCMC) for computation are limited to problems an order of magnitude smaller. This is due to high computational cost per step and growth of the variance of time-averaging estimators as a function of dimension. We propose two new MCMC algorithms for computation in these models that have significantly improved performance compared to existing alternatives. One of the algorithms also approximates an expensive matrix product to give orders of magnitude speedup in high-dimensional applications. We prove guarantees for the accuracy of the approximate algorithm, and show that gradually decreasing the approximation error as the chain extends results in an exact algorithm. The scalability of the algorithm is illustrated in simulations with problem size as large as N = 5; 000 observations and p = 50; 000 predictors, and an application to a genome-wide association study with N = 2; 267 and p = 98; 385. The empirical results also show that the new algorithm yields estimates with lower mean squared error, intervals with better coverage, and elucidates features of the posterior that were often missed by previous algorithms in high dimensions, including bimodality of posterior marginals indicating uncertainty about which covariates belong in the model.																	1532-4435						2020	21																						
J								Generalized Optimal Matching Methods for Causal Inference	JOURNAL OF MACHINE LEARNING RESEARCH										Causal inference; optimal covariate balance; embeddings; matching; convex optimization	FINE BALANCE; REGRESSION; BIAS; RANDOMIZATION; ESTIMATORS	We develop an encompassing framework for matching, covariate balancing, and doubly-robust methods for causal inference from observational data called generalized optimal matching (GOM). The framework is given by generalizing a new functional-analytical formulation of optimal matching, giving rise to the class of GOM methods, for which we provide a single unified theory to analyze tractability and consistency. Many commonly used existing methods are included in GOM and, using their GOM interpretation, can be extended to optimally and automatically trade off balance for variance and outperform their standard counterparts. As a subclass, GOM gives rise to kernel optimal matching (KOM), which, as supported by new theoretical and empirical results, is notable for combining many of the positive properties of other methods in one. KOM, which is solved as a linearly-constrained convex-quadratic optimization problem, inherits both the interpretability and model-free consistency of matching but can also achieve the root n-consistency of well-specified regression and the bias reduction and robustness of doubly robust methods. In settings of limited overlap, KOM enables a very transparent method for interval estimation for partial identification and robust coverage. We demonstrate this in examples with both synthetic and real data.																	1532-4435						2020	21																						
J								Representation Learning for Dynamic Graphs: A Survey	JOURNAL OF MACHINE LEARNING RESEARCH										graph representation learning; dynamic graphs; knowledge graph embedding; heterogeneous information networks	LINK-PREDICTION-PROBLEM; NEURAL-NETWORKS; TIME-SERIES; MODELS	Graphs arise naturally in many real-world applications including social networks, recommender systems, ontologies, biology, and computational finance. Traditionally, machine learning models for graphs have been mostly designed for static graphs. However, many applications involve evolving graphs. This introduces important challenges for learning and inference since nodes, attributes, and edges change over time. In this survey, we review the recent advances in representation learning for dynamic graphs, including dynamic knowledge graphs. We describe existing models from an encoder-decoder perspective, categorize these encoders and decoders based on the techniques they employ, and analyze the approaches in each category. We also review several prominent applications and widely used datasets and highlight directions for future research.																	1532-4435						2020	21																						
J								Learning Causal Networks via Additive Faithfulness	JOURNAL OF MACHINE LEARNING RESEARCH										additive conditional independence; additive reproducing kernel Hilbert space; directed acyclic graph; global Markov property; normalized additive conditional covariance operator; PC-algorithm	TESTING CONDITIONAL-INDEPENDENCE; DIMENSION REDUCTION; KERNEL; CONSISTENCY; LIKELIHOOD; SHRINKAGE; SELECTION; MODEL; LASSO	In this paper we introduce a statistical model, called additively faithful directed acyclic graph (AFDAG), for causal learning from observational data. Our approach is based on additive conditional independence (ACI), a recently proposed three-way statistical relation that shares many similarities with conditional independence but without resorting to multi-dimensional kernels. This distinct feature strikes a balance between a parametric model and a fully nonparametric model, which makes the proposed model attractive for handling large networks. We develop an estimator for AFDAG based on a linear operator that characterizes ACI, and establish the consistency and convergence rates of this estimator, as well as the uniform consistency of the estimated DAG. Moreover, we introduce a modified PC-algorithm to implement the estimating procedure efficiently, so that its complexity is determined by the level of sparseness rather than the dimension of the network. Through simulation studies we show that our method outperforms existing methods when commonly assumed conditions such as Gaussian or Gaussian copula distributions do not hold. Finally, the usefulness of AFDAG formulation is demonstrated through an application to a proteomics data set.																	1532-4435						2020	21																						
J								High-dimensional Gaussian graphical models on network-linked data	JOURNAL OF MACHINE LEARNING RESEARCH										High-dimensional statistics; Gaussian graphical model; network analysis; network cohesion; statistical learning	INVERSE COVARIANCE ESTIMATION; GENERALIZED CROSS-VALIDATION; PRECISION MATRIX ESTIMATION; VARIABLE SELECTION; COMMUNITY DETECTION; SOCIAL NETWORK; REGRESSION; IDENTIFICATION; REDUCTION	Graphical models are commonly used to represent conditional dependence relationships between variables. There are multiple methods available for exploring them from high-dimensional data, but almost all of them rely on the assumption that the observations are independent and identically distributed. At the same time, observations connected by a network are becoming increasingly common, and tend to violate these assumptions. Here we develop a Gaussian graphical model for observations connected by a network with potentially different mean vectors, varying smoothly over the network. We propose an efficient estimation algorithm and demonstrate its effectiveness on both simulated and real data, obtaining meaningful and interpretable results on a statistics coauthorship network. We also prove that our method estimates both the inverse covariance matrix and the corresponding graph structure correctly under the assumption of network "cohesion", which refers to the empirically observed phenomenon of network neighbors sharing similar traits.																	1532-4435						2020	21																						
J								Smoothed Nonparametric Derivative Estimation using Weighted Difference Quotients	JOURNAL OF MACHINE LEARNING RESEARCH										derivative estimation; asymptotic properties; random design	CONFIDENCE BANDS; BANDWIDTH CHOICE; REGRESSION; SIZER	Derivatives play an important role in bandwidth selection methods (e.g., plug-ins), data analysis and bias-corrected confidence intervals. Therefore, obtaining accurate derivative information is crucial. Although many derivative estimation methods exist, the majority require a fixed design assumption. In this paper, we propose an effective and fully data-driven framework to estimate the first and second order derivative in random design. We establish the asymptotic properties of the proposed derivative estimator, and also propose a fast selection method for the tuning parameters. The performance and exibility of the method is illustrated via an extensive simulation study.																	1532-4435						2020	21																						
J								Self-paced Multi-view Co-training	JOURNAL OF MACHINE LEARNING RESEARCH										Co-training; self-paced learning; multi-view learning; semi-supervised learning; epsilon-expansion theory; probably approximately correct learnable		Co-training is a well-known semi-supervised learning approach which trains classifiers on two or more different views and exchanges pseudo labels of unlabeled instances in an iterative way. During the co-training process, pseudo labels of unlabeled instances are very likely to be false especially in the initial training, while the standard co-training algorithm adopts a "draw without replacement" strategy and does not remove these wrongly labeled instances from training stages. Besides, most of the traditional co-training approaches are implemented for two-view cases, and their extensions in multi-view scenarios are not intuitive. These issues not only degenerate their performance as well as available application range but also hamper their fundamental theory. Moreover, there is no optimization model to explain the objective a co-training process manages to optimize. To address these issues, in this study we design a unified self-paced multi-view co-training (SPamCo) framework which draws unlabeled instances with replacement. Two specified co-regularization terms are formulated to develop different strategies for selecting pseudo-labeled instances during training. Both forms share the same optimization strategy which is consistent with the iteration process in co-training and can be naturally extended to multi-view scenarios. A distributed optimization strategy is also introduced to train the classifier of each view in parallel to further improve the efficiency of the algorithm. Furthermore, the SPamCo algorithm is proved to be PAC learnable, supporting its theoretical soundness. Experiments conducted on synthetic, text categorization, person re-identification, image recognition and object detection data sets substantiate the superiority of the proposed method.																	1532-4435						2020	21																						
J								Identifiability of Additive Noise Models Using Conditional Variances	JOURNAL OF MACHINE LEARNING RESEARCH										Bayesian Network; Causal Inference; Directed Acyclic Graph; Identifiability; Structural Equation Modeling; Structure Learning	CAUSAL DISCOVERY; NETWORKS	This paper considers a new identifiability condition for additive noise models (ANMs) in which each variable is determined by an arbitrary Borel measurable function of its parents plus an independent error. It has been shown that ANMs are fully recoverable under some identifiability conditions, such as when all error variances are equal. However, this identifiable condition could be restrictive, and hence, this paper focuses on a relaxed identifiability condition that involves not only error variances, but also the influence of parents. This new class of identifiable ANMs does not put any constraints on the form of dependencies, or distributions of errors, and allows different error variances. It further provides a statistically consistent and computationally feasible structure learning algorithm for the identifiable ANMs based on the new identifiability condition. The proposed algorithm assumes that all relevant variables are observed, while it does not assume faithfulness or a sparse graph. Demonstrated through extensive simulated and real multivariate data is that the proposed algorithm successfully recovers directed acyclic graphs.																	1532-4435						2020	21																						
J								Community-Based Group Graphical Lasso	JOURNAL OF MACHINE LEARNING RESEARCH										community detection; graphical model; group penalty; joint graphical lasso	INVERSE COVARIANCE ESTIMATION; INSIGHTS; MODEL	A new strategy for probabilistic graphical modeling is developed that draws parallels to community detection analysis. The method jointly estimates an undirected graph and homogenous communities of nodes. The structure of the communities is taken into account when estimating the graph and at the same time, the structure of the graph is accounted for when estimating communities of nodes. The procedure uses a joint group graphical lasso approach with community detection-based grouping, such that some groups of edges cooccur in the estimated graph. The grouping structure is unknown and is estimated based on community detection algorithms. Theoretical derivations regarding graph convergence and sparsistency, as well as accuracy of community recovery are included, while the method's empirical performance is illustrated in an fMRI context, as well as with simulated examples.																	1532-4435						2020	21																						
J								Skill Rating for Multiplayer Games Introducing Hypernode Graphs and their Spectral Theory	JOURNAL OF MACHINE LEARNING RESEARCH										Hypergraphs; Graph Laplacians; Graph Kernels; Spectral Learning; Semi-supervised Learning; Multiplayer Games; Skill Rating Algorithms		We consider the skill rating problem for multiplayer games, that is how to infer player skills from game outcomes in multiplayer games. We formulate the problem as a minimization problem arg min(s) s(T) Delta s where Delta is a positive semidefinite matrix and s a real-valued function, of which some entries are the skill values to be inferred and other entries are constrained by the game outcomes. We leverage graph-based semi-supervised learning (SSL) algorithms for this problem. We apply our algorithms on several data sets of multiplayer games and obtain very promising results compared to ELO DUELLING (see Elo, 1978) and TrueSkill (see Herbrich et al., 2006). As we leverage graph-based SSL algorithms and because games can be seen as relations between sets of players, we then generalize the approach. For this aim, we introduce a new finite model, called hypernode graph, defined to be a set of weighted binary relations between sets of nodes. We define Laplacians of hypernode graphs. Then, we show that the skill rating problem for multiplayer games can be formulated as arg min(s) s(T) Delta s where Delta is the Laplacian of a hypernode graph constructed from a set of games. From a fundamental perspective, we show that hypernode graph Laplacians are symmetric positive semidefinite matrices with constant functions in their null space. We show that problems on hypernode graphs can not be solved with graph constructions and graph kernels. We relate hypernode graphs to signed graphs showing that positive relations between groups can lead to negative relations between individuals.																	1532-4435						2020	21																						
J								GraKeL: A Graph Kernel Library in Python	JOURNAL OF MACHINE LEARNING RESEARCH										graph similarity; graph kernels; scikit-learn; Python		The problem of accurately measuring the similarity between graphs is at the core of many applications in a variety of disciplines. Graph kernels have recently emerged as a promising approach to this problem. There are now many kernels, each focusing on different structural aspects of graphs. Here, we present GraKeL, a library that unifies several graph kernels into a common framework. The library is written in Python and adheres to the scikit-learn interface. It is simple to use and can be naturally combined with scikit-learn's modules to build a complete machine learning pipeline for tasks such as graph classification and clustering. The code is BSD licensed and is available at: https://github.com/ysig/GraKeL																	1532-4435						2020	21																						
J								Robust Asynchronous Stochastic Gradient-Push: Asymptotically Optimal and Network-Independent Performance for Strongly Convex Functions	JOURNAL OF MACHINE LEARNING RESEARCH										distributed optimization; stochastic gradient descent	DISTRIBUTED OPTIMIZATION; AVERAGE CONSENSUS; ALGORITHM; CONVERGENCE	We consider the standard model of distributed optimization of a sum of functions F(z) = Sigma(n)(i=1) f(i)(z), where node i in a network holds the function f(i)(z). We allow for a harsh network model characterized by asynchronous updates, message delays, unpredictable message losses, and directed communication among nodes. In this setting, we analyze a modification of the Gradient-Push method for distributed optimization, assuming that (i) node i is capable of generating gradients of its function f(i)(z) corrupted by zero-mean bounded{support additive noise at each step, (ii) F(z) is strongly convex, and (iii) each fi(z) has Lipschitz gradients. We show that our proposed method asymptotically performs as well as the best bounds on centralized gradient descent that takes steps in the direction of the sum of the noisy gradients of all the functions f(1)(z), ..., f(n)(z) at each step.																	1532-4435						2020	21																						
J								Multiparameter Persistence Landscapes	JOURNAL OF MACHINE LEARNING RESEARCH										Topological Data Analysis; Multiparameter Persistence; Persistence Landscapes; Machine Learning; Statistical Topology	MULTIDIMENSIONAL PERSISTENCE	An important problem in the field of Topological Data Analysis is defining topological summaries which can be combined with traditional data analytic tools. In recent work Bubenik introduced the persistence landscape, a stable representation of persistence diagrams amenable to statistical analysis and machine learning tools. In this paper we generalise the persistence landscape to multiparameter persistence modules providing a stable representation of the rank invariant. We show that multiparameter landscapes are stable with respect to the interleaving distance and persistence weighted Wasserstein distance, and that the collection of multiparameter landscapes faithfully represents the rank invariant. Finally we provide example calculations and statistical tests to demonstrate a range of potential applications and how one can interpret the landscapes associated to a multiparameter module.																	1532-4435						2020	21																						
J								Unique Sharp Local Minimum in l(1)-minimization Complete Dictionary Learning	JOURNAL OF MACHINE LEARNING RESEARCH										dictionary learning; l(1)-minimization; local and global identifiability; non-convex optimization; sharp local minimum	OVERCOMPLETE DICTIONARIES; MATRIX-FACTORIZATION; SPARSE; REPRESENTATIONS; IDENTIFICATION; DECOMPOSITION; ALGORITHMS	We study the problem of globally recovering a dictionary from a set of signals via l(1)-minimization. We assume that the signals are generated as i.i.d. random linear combinations of the K atoms from a complete reference dictionary D* is an element of R-KxK, where the linear combination coefficients are from either a Bernoulli type model or exact sparse model. First, we obtain a necessary and sufficient norm condition for the reference dictionary D to be a sharp local minimum of the expected l(1) objective function. Our result substantially extends that of Wu and Yu (2018) and allows the combination coefficient to be non-negative. Secondly, we obtain an explicit bound on the region within which the objective value of the reference dictionary is minimal. Thirdly, we show that the reference dictionary is the unique sharp local minimum, thus establishing the first known global property of l(1)-minimization dictionary learning. Motivated by the theoretical results, we introduce a perturbation based test to determine whether a dictionary is a sharp local minimum of the objective function. In addition, we also propose a new dictionary learning algorithm based on Block Coordinate Descent, called DL-BCD, which is guaranteed to decrease the obective function monotonically. Simulation studies show that DL-BCD has competitive performance in terms of recovery rate compared to other state-of-the-art dictionary learning algorithms when the reference dictionary is generated from random Gaussian matrices.																	1532-4435						2020	21																						
J								Estimation of a Low-rank Topic-Based Model for Information Cascades	JOURNAL OF MACHINE LEARNING RESEARCH										alternating gradient descent; low-rank models; information diffusion; influence-receptivity model; network science; nonconvex optimization	OPTIMIZATION; DIFFUSION; NETWORKS; GRAPHS	We consider the problem of estimating the latent structure of a social network based on the observed information diffusion events, or cascades, where the observations for a given cascade consist of only the timestamps of infection for infected nodes but not the source of the infection. Most of the existing work on this problem has focused on estimating a diffusion matrix without any structural assumptions on it. In this paper, we propose a novel model based on the intuition that an information is more likely to propagate among two nodes if they are interested in similar topics which are also prominent in the information content. In particular, our model endows each node with an influence vector (which measures how authoritative the node is on each topic) and a receptivity vector (which measures how susceptible the node is for each topic). We show how this node-topic structure can be estimated from the observed cascades, and prove the consistency of the estimator. Experiments on synthetic and real data demonstrate the improved performance and better interpretability of our model compared to existing state-of-the-art methods.																	1532-4435						2020	21																						
J								Rate-Distortion Driven Decomposition of Multiview Imagery to Diffuse and Specular Components	IEEE TRANSACTIONS ON IMAGE PROCESSING										Overcomplete decomposition; diffuse; specular; multiview; rate-distortion optimization	COLOR; COMPRESSION; EXTENSIONS	In this work, we propose an overcomplete representation of multiview imagery for the purpose of compression. We present a rate-distortion (R-D) driven approach to decompose multiview datasets into two additive parts which can be interpreted as diffuse and specular content. We choose distinct and different sparsifying transforms for the diffuse and specular components and employ an R-D inspired measure as our optimization cost function to drive the decomposition based solely on compressibility. We first describe a framework which performs data separation in a registered domain to avoid the complexity of warping between views. Then a more comprehensive approach is proposed to separate specular data progressively from coordinates of multiple reference views. Experimental results show a coding gain of up to 0.6 dB for synthetic datasets and up to 0.9 dB for real datasets.																	1057-7149	1941-0042					2020	29						5469	5480		10.1109/TIP.2020.2983849													
J								Unsupervised Person Re-identification via Cross-Camera Similarity Exploration	IEEE TRANSACTIONS ON IMAGE PROCESSING											NETWORK	Most person re-identification (re-ID) approaches are based on supervised learning, which requires manually annotated data. However, it is not only resource-intensive to acquire identity annotation but also impractical for large-scale data. To relieve this problem, we propose a cross-camera unsupervised approach that makes use of unsupervised style-transferred images to jointly optimize a convolutional neural network (CNN) and the relationship among the individual samples for person re-ID. Our algorithm considers two fundamental facts in the re-ID task, i.e., variance across diverse cameras and similarity within the same identity. In this paper, we propose an iterative framework which overcomes the camera variance and achieves across-camera similarity exploration. Specifically, we apply an unsupervised style transfer model to generate style-transferred training images with different camera styles. Then we iteratively exploit the similarity within the same identity from both the original and the style-transferred data. We start with considering each training image as a different class to initialize the Convolutional Neural Network (CNN) model. Then we measure the similarity and gradually group similar samples into one class, which increases similarity within each identity. We also introduce a diversity regularization term in the clustering to balance the cluster distribution. The experimental results demonstrate that our algorithm is not only superior to state-of-the-art unsupervised re-ID approaches, but also performs favorably compared with other competing unsupervised domain adaptation methods (UDA) and semi-supervised learning methods.																	1057-7149	1941-0042					2020	29						5481	5490		10.1109/TIP.2020.2982826													
J								FEATURE EXTRACTION ALGORITHM USING NEW CEPSTRAL TECHNIQUES FOR ROBUST SPEECH RECOGNITION	MALAYSIAN JOURNAL OF COMPUTER SCIENCE										Noise-robust feature; Automatic speech recognition; Differential power spectrum; MVA; Aurora2	SPECTRUM	In this work, we propose a novel feature extraction algorithm that improves the robustness of automatic speech recognition (ASR) systems in the presence of various types of noise. The proposed algorithm uses a new cepsfral technique based on the differential power spectrum (DPS) instead of the power spectrum (PS), the algorithm replaces the logarithmic non linearity by the power function. In order to reduce cepsfral coefficients mismatches between training and testing conditions, we used the mean and variance normalization, then we apply auto-regression moving-average filtering (MVA) in the cepstral domain. The ASR experiments were conducted using two databases, the first is LASA digit database designed for recognition the isolated Arabic digits in the presence of different types of noise. The second is Aurora 2 noisy speech database designed to recognize connected English digits in various operating environments. The experimental results show a substantial improvement from the proposed algorithm over the baseline Mel Frequency Cepstral Coefficients (MFCC), the relative improvement is the 28.92% for LASA database and is the 44.43% for aurora 2 database. The performance of our proposed algorithm was tested and verified by extensive comparisons with the state-of-the-art noise-robust features in aurora 2.																	0127-9084						2020	33	2					90	101		10.22452/mjcs.vol33no2.1													
J								TRENDS AND PATTERNS OF TEXT CLASSIFICATION TECHNIQUES: A SYSTEMATIC MAPPING STUDY	MALAYSIAN JOURNAL OF COMPUTER SCIENCE										Machine learning techniques; Text classification; Text categorization; Natural language processing (NLP); Systematic mapping (SM)	INFORMATION; SUPPORT	Due to the mass availability of textual data on Web, text classification (TC), classifying texts into predetermined sets becomes a spotlight for researchers. A number of TC applications have been proposed yet very few studies reported an overview of TC research area in a proper and systematic manner. This paper aims to provide an overview of TC research trends and gaps by structuring and analyzing research patterns, encountered problems and problem-solving methods in TC. In other words, this study highlights problem types, data sources, choice of language of text and types of applied techniques in TC. An intensive systematic study is conducted by applying guidelines proposed by Petersen and colleagues in 2007. In this paper, ninety-six literatures from five electronic databases from 2006 to 2017 were systematically reviewed and followed each and every step properly in accordance with systematic mapping study. Nine main problems in TC research area were identified and significant findings which highlighted the evolution of TC research within the past 12 years were investigated. Different from other review articles, this paper highlighted issues and technical gaps of TC area in a useful and effective manner.																	0127-9084						2020	33	2					102	117		10.22452/mjcs.vol33no2.2													
J								ADOPTION OF INTELLIGENT COMPUTATIONAL TECHNIQUES FOR STEAM BOILERS TUBE LEAK TRIP	MALAYSIAN JOURNAL OF COMPUTER SCIENCE										Coal-fired power plant; Boiler; Artificial neural networks (ANNs); Genetic algorithms (GAs); Tube leak	VISUALIZATION; PREDICTION; ALGORITHM	Frequent boiler tube trips in coal fired power plants can increase operating cost significantly. An early detection and diagnosis of boiler trips is essential for continuous safe operations in the plant. Several methodologies for the fault diagnosis in a plant have been developed. However these methodologies are difficult to be implemented. In this study, two artificial intelligent monitoring systems specialized in boiler trips have been proposed. The first intelligent monitoring system represents the use of pure artificial neural network system whereas the second intelligent monitoring system represents merging of genetic algorithms and artificial neural networks as a hybrid intelligent system. In the first system using pure artificial neural network, the trip was predicted 5 minutes before the actual trip occurrence. The hybrid intelligent system was able to optimize the selection of the most influencing variables successfully and predict the trip 2 minutes before the actual trip. The first intelligent system performed better than the second one based on the prediction time. The proposed artificial intelligent system could be adopted on-line as a reliable controller of the thermal power plant boiler.																	0127-9084						2020	33	2					133	151		10.22452/mjcs.vol33no2.4													
J								A PARTITION-BASED FEATURE SELECTION METHOD FOR MIXED DATA: A FILTER APPROACH	MALAYSIAN JOURNAL OF COMPUTER SCIENCE										Clustering; educational data mining; mixed data; unsupervised feature selection	GENERAL COEFFICIENT; SIMILARITY	Feature selection is fundamentally an optimization problem for selecting relevant features from several alternatives in clustering problems. Though several algorithms have been suggested, however till this day, there has not been any one of those that has been dubbed as the best for every problem scenario. Therefore, researchers continue to strive in developing superior algorithms. Even though clustering process is considered a pre-processing task but what it really does is just dividing the data into groups. In this paper we have attempted an improved distance function to cluster mixed data. A similarity measure for mixed data is Gower distance is adopted and modified to define the similarity between object pairs. A partitional algorithm for mixed data is employed to group similar objects in clusters. The performance of the proposed method has been evaluated on similar mixed and real educational dataset in terms of the silhouette coefficient. Results reveal the effectiveness of this algorithm in unsupervised discovery problems. The proposed algorithm performed better than other clustering algorithms for various datasets.																	0127-9084						2020	33	2					152	169		10.22452/mjcs.vol33no2.5													
B								Fundamentals of Machine Learning	ARTIFICIAL INTELLIGENCE IN MEDICAL IMAGING: FROM THEORY TO CLINICAL PRACTICE																															978-0-367-22917-7				2020							1	14			10.1201/9780367229184												
B								Applying AI in Medical Imaging	ARTIFICIAL INTELLIGENCE IN MEDICAL IMAGING: FROM THEORY TO CLINICAL PRACTICE																															978-0-367-22917-7				2020							47	83			10.1201/9780367229184												
B								Designing AI Systems for Clinical Practice	ARTIFICIAL INTELLIGENCE IN MEDICAL IMAGING: FROM THEORY TO CLINICAL PRACTICE																															978-0-367-22917-7				2020							85	107			10.1201/9780367229184												
J								Emotion mining from text for actionable recommendations detailed survey	INTERNATIONAL JOURNAL OF DATA MINING MODELLING AND MANAGEMENT										actionable pattern mining; data mining; text mining; sentiment analysis	ACTION RULES DISCOVERY; CIRCUMPLEX MODEL; RECOGNITION; ALGORITHMS; KNOWLEDGE; FEATURES; DATABASE	In the era of Web 2.0, people express their opinion, feelings and thoughts about topics including political and cultural events, natural disasters, products and services, through mediums such as blogs, forums, and micro-blogs, like Twitter. Also, large amount of text is generated through e-mail which contains the writer's feeling or opinion; for instance, customer care service e-mail. The texts generated through such platforms are a rich source of data which can be mined in order to gain useful information about user opinion or feeling which in turn can be utilised in specific applications such as: marketing, sale predictions, political surveys, health care, student-faculty culture, e-learning platforms, and social networks. This process of identifying and extracting information about the attitude of a speaker or writer about a topic, polarity, or emotion in a document is called sentiment analysis. There are variety of sources for extracting sentiment such as speech, music, facial expression. Due to the rich source of information available in the form of text data, this paper focuses on sentiment analysis and emotion mining from text, as well as discovering actionable patterns. The actionable patterns may suggest ways to alter the user's sentiment or emotion to a more positive or desirable state.																	1759-1163	1759-1171					2020	12	2					143	191															
J								Bees colonies for detecting communities evolution using data warehouse	INTERNATIONAL JOURNAL OF DATA MINING MODELLING AND MANAGEMENT										social network; community detection; bees colony	ALGORITHM	The analysis of social networks and their evolution has gained much interest in recent years. In fact, few methods revealed and tracked meaningful communities over time. These methods also dealt efficiently with structure and topic evolution of networks. In this paper, we propose a novel technique to track dynamic communities and their evolution behaviour. The main objective of our approach and using the artificial bee colony (ABC) is to trace the evolution of community and to optimise our objective function to keep proper partitioning. Moreover, we use a data warehouse as a mind of bees to store the information of different communities structure in every timestamp. The experimental results showed that the proposed method is efficient in discovering dynamics communities and tracking their evolution.																	1759-1163	1759-1171					2020	12	2					192	206															
J								A support architecture to MDA contribution for data mining	INTERNATIONAL JOURNAL OF DATA MINING MODELLING AND MANAGEMENT										data mining; model driven architecture; MDA; data warehouses; UML profiles; data multidimensional model; transformation	KNOWLEDGE DISCOVERY; DATA WAREHOUSES; UML PROFILE; MODEL	The data mining process is the sequence of tasks applied to data, in order to discover relations between them to have knowledge. However, the data mining process lacks a formal specification that allows it to be modelled independently of platforms. Model driven architecture (MDA) is an approach for the development of software systems, based on the use of models to improve their productivity. Several research works have been elaborated to align the MDA approach with data mining on data warehouses, to specify the data mining process in a very high level of abstraction. In our work, we propose a support architecture that allows positioning these researches in different abstraction levels, on the basis of several criteria; with the aim to identify strengths for each level, in term of modelling; and to have a clear visibility on the MDA contribution for data mining.																	1759-1163	1759-1171					2020	12	2					207	236															
J								A survey of term weighting schemes for text classification	INTERNATIONAL JOURNAL OF DATA MINING MODELLING AND MANAGEMENT										document frequency; supervised term weighting; text classification; unsupervised term weighting	SENTIMENT ANALYSIS; FEATURE-SELECTION; FREQUENCY; IDENTIFICATION; MODEL	Text document classification approaches are designed to categorise documents into predefined classes. These approaches have two main components: document representation models and term-weighting methods. The high dimensionality of feature space has always been a major problem in text classification methods. To resolve high dimensionality issues and to improve the accuracy of text classification, various feature selection approaches were presented in the literature. Besides which, several term-weighting schemes were introduced that can be utilised for feature selection methods. This work surveys and investigates various term (feature) weighting approaches that have been presented in the text classification context.																	1759-1163	1759-1171					2020	12	2					237	254															
J								Best-First Enumeration Based on Bounding Conflicts, and its Application to Large-scale Hybrid Estimation	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH											MULTIPLE-MODEL ESTIMATION; VARIABLE-STRUCTURE; KALMAN FILTER; SYSTEM	There is an increasing desire for autonomous systems to have high levels of robustness and safety, attained through continuously planning and self-repairing online. Underlying this is the need to accurately estimate the system state and diagnose subtle failures. Estimation methods based on hybrid discrete and continuous state models have emerged as a method of precisely computing these estimates. However, existing methods have difficulty scaling to systems with more than a handful of components. Discrete, consistency based state estimation capabilities can scale to this level by combining best-first enumeration and conflict-directed search. While best-first methods have been developed for hybrid estimation, conflict-directed methods have thus far been elusive as conflicts learn inconsistencies from constraint violation, but probabilistic hybrid estimation is relatively unconstrained. In this paper we present an approach to hybrid estimation that unifies best-first enumeration and conflict-directed search through the concept of "bounding" conflicts, an extension of conflicts that represent tighter bounds on the cost of regions of the search space. This paper presents a general best-first enumeration algorithm based on bounding conflicts (A*BC) and a hybrid estimation method using this enumeration algorithm. Experiments show that an A*BC powered state estimator produces estimates up to an order of magnitude faster than the current state of the art, particularly on large systems.																	1076-9757	1943-5037					2020	67						1	34															
J								The Impact of Treewidth on Grounding and Solving of Answer Set Programs	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH											LOGIC; COMPLEXITY; ALGORITHMS; POWER	In this paper, we aim to study how the performance of modern answer set programming (ASP) solvers is influenced by the treewidth of the input program and to investigate the consequences of this relationship. We first perform an experimental evaluation that shows that the solving performance is heavily influenced by treewidth, given ground input programs that are otherwise uniform, both in size and construction. This observation leads to an important question for ASP, namely, how to design encodings such that the treewidth of the resulting ground program remains small. To this end, we study two classes of disjunctive programs, namely guarded and connection-guarded programs. In order to investigate these classes, we formalize the grounding process using MSO transductions. Our main results show that both classes guarantee that the treewidth of the program after grounding only depends on the treewidth (and the maximum degree, in case of connection-guarded programs) of the input instance. In terms of parameterized complexity, our findings yield corresponding FPT results for answer-set existence for bounded treewidth (and also degree, for connection-guarded programs) of the input instance. We further show that bounding treewidth alone leads to NP-hardness in the data complexity for connection-guarded programs, which indicates that the two classes are fundamentally different. Finally, we show that for both classes, the data complexity remains as hard as in the general case of ASP.																	1076-9757	1943-5037					2020	67						35	80															
J								The 2(k) Neighborhoods for Grid Path Planning	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH												Grid path planning is an important problem in AI. Its understanding has been key for the development of autonomous navigation systems. An interesting and rather surprising fact about the vast literature on this problem is that only a few neighborhoods have been used when evaluating these algorithms. Indeed, only the 4- and 8-neighborhoods are usually considered, and rarely the 16-neighborhood. This paper describes three contributions that enable the construction of effective grid path planners for extended 2(k)-neighborhoods; that is, neighborhoods that admit 2(k) neighbors per state, where k is a parameter. First, we provide a simple recursive definition of the 2(k)-neighborhood in terms of the 2(k-1)-neighborhood. Second, we derive distance functions, for any k >= 2, which allow us to propose admissible heuristics that are perfect for obstacle-free grids, which generalize the well-known Manhattan and Octile distances. Third, we define the notion of canonical path for the 2(k)-neighborhood; this allows us to incorporate our neighborhoods into two versions of A*, namely Canonical A* and Jump Point Search (JPS), whose performance, we show, scales well when increasing k. Our empirical evaluation shows that, when increasing k, the cost of the solution found improves substantially. Used with the 2(k)-neighborhood, Canonical A* and JPS, in many configurations, are also superior to the any-angle path planner Theta* both in terms of solution quality and runtime. Our planner is competitive with one implementation of the any-angle path planner, ANYA in some configurations. Our main practical conclusion is that standard, well-understood grid path planning technology may provide an effective approach to any-angle grid path planning.																	1076-9757	1943-5037					2020	67						81	113															
J								Regret Bounds for Reinforcement Learning via Markov Chain Concentration	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH												We give a simple optimistic algorithm for which it is easy to derive regret bounds of (O) over tilde (root t(mix)SAT) after T steps in uniformly ergodic Markov decision processes with S states, A actions, and mixing time parameter t(mix). These bounds are the first regret bounds in the general, non-episodic setting with an optimal dependence on all given parameters. They could only be improved by using an alternative mixing time parameter.																	1076-9757	1943-5037					2020	67						115	128															
J								Saturated Cost Partitioning for Optimal Classical Planning	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH											ALGORITHMS	Cost partitioning is a method for admissibly combining a set of admissible heuristic estimators by distributing operator costs among the heuristics. Computing an optimal cost partitioning, i.e., the operator cost distribution that maximizes the heuristic value, is often prohibitively expensive to compute. Saturated cost partitioning is an alternative that is much faster to compute and has been shown to yield high-quality heuristics. However, its greedy nature makes it highly susceptible to the order in which the heuristics are considered. We propose a greedy algorithm to generate orders and show how to use hill-climbing search to optimize a given order. Combining both techniques leads to significantly better heuristic estimates than using the best random order that is generated in the same time. Since there is often no single order that gives good guidance on the whole state space, we use the maximum of multiple orders as a heuristic that is significantly better informed than any single-order heuristic, especially when we actively search for a set of diverse orders.																	1076-9757	1943-5037					2020	67						129	167															
J								The Force Awakens: Artificial Intelligence for Consumer Law	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH											INFORMATION; TERMS; BIG	Recent years have been tainted by market practices that continuously expose us, as consumers, to new risks and threats. We have become accustomed, and sometimes even resigned, to businesses monitoring our activities, examining our data, and even meddling with our choices. Artificial Intelligence (AI) is often depicted as a weapon in the hands of businesses and blamed for allowing this to happen. In this paper, we envision a paradigm shift, where AI technologies are brought to the side of consumers and their organizations, with the aim of building an efficient and effective counter-power. AI-powered tools can support a massive-scale automated analysis of textual and audiovisual data, as well as code, for the benefit of consumers and their organizations. This in turn can lead to a better oversight of business activities, help consumers exercise their rights, and enable the civil society to mitigate information overload. We discuss the societal, political, and technological challenges that stand before that vision.																	1076-9757	1943-5037					2020	67						169	190															
J								Blind Spot Detection for Safe Sim-to-Real Transfer	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH											NOVELTY DETECTION; UNCERTAINTY	Agents trained in simulation may make errors when performing actions in the real world due to mismatches between training and execution environments. These mistakes can be dangerous and difficult for the agent to discover because the agent is unable to predict them a priori. In this work, we propose the use of oracle feedback to learn a predictive model of these blind spots in order to reduce costly errors in real-world applications. We focus on blind spots in reinforcement learning (RL) that occur due to incomplete state representation: when the agent lacks necessary features to represent the true state of the world, and thus cannot distinguish between numerous states. We formalize the problem of discovering blind spots in RL as a noisy supervised learning problem with class imbalance. Our system learns models for predicting blind spots within unseen regions of the state space by combining techniques for label aggregation, calibration, and supervised learning. These models take into consideration noise emerging from different forms of oracle feedback, including demonstrations and corrections. We evaluate our approach across two domains and demonstrate that it achieves higher predictive performance than baseline methods, and also that the learned model can be used to selectively query an oracle at execution time to prevent errors. We also empirically analyze the biases of various feedback types and how these biases influence the discovery of blind spots. Further, we include analyses of our approach that incorporate relaxed initial optimality assumptions. (Interestingly, relaxing the assumptions of an optimal oracle and an optimal simulator policy helped our models to perform better.) We also propose extensions to our method that are intended to improve performance when using corrections and demonstrations data.																	1076-9757	1943-5037					2020	67						191	234															
J								Planning for Hybrid Systems via Satisfiability Modulo Theories	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH											HEURISTICS; DOMAINS	Planning for hybrid systems is important for dealing with real-world applications, and PDDL+ supports this representation of domains with mixed discrete and continuous dynamics. In this paper we present a new approach for planning for hybrid systems, based on encoding the planning problem as a Satisfiability Modulo Theories (SMT) formula. This is the first SMT encoding that can handle the whole set of PDDL+ features (including processes and events), and is implemented in the planner SMTPlan. SMTPlan not only covers the full semantics of PDDL+, but can also deal with non-linear polynomial continuous change without discretization. This allows it to generate plans with non-linear dynamics that are correct-by-construction. The encoding is based on the notion of happenings, and can be applied on domains with nonlinear continuous change. We describe the encoding in detail and provide in-depth examples. We apply this encoding in an iterative deepening planning algorithm. Experimental results show that the approach dramatically outperforms existing work in finding plans for PDDL+ problems. We also present experiments which explore the performance of the proposed approach on temporal planning problems, showing that the scalability of the approach is limited by the size of the discrete search space. We further extend the encoding to include planning with control parameters. The extended encoding allows the definition of actions to include infinite domain parameters, called control parameters. We present experiments on a set of problems with control parameters to demonstrate the positive effect they provide to the approach of planning via SMT.																	1076-9757	1943-5037					2020	67						235	283															
J								TensorLog: A Probabilistic Database Implemented Using Deep-Learning Infrastructure	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH											LOGIC	We present an implementation of a probabilistic first-order logic called TensorLog, in which classes of logical queries are compiled into differentiable functions in a neural-network infrastructure such as Tensorflow or Theano. This leads to a close integration of probabilistic logical reasoning with deep-learning infrastructure: in particular, it enables high-performance deep learning frameworks to be used for tuning the parameters of a probabilistic logic. The integration with these frameworks enables use of GPU-based parallel processors for inference and learning, making TensorLog the first highly parallellizable probabilistic logic. Experimental results show that TensorLog scales to problems involving hundreds of thousands of knowledge-base triples and tens of thousands of examples.																	1076-9757	1943-5037					2020	67						285	325															
J								Jointly Improving Parsing and Perception for Natural Language Commands through Human-Robot Dialog	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH												In this work, we present methods for using human-robot dialog to improve language understanding for a mobile robot agent. The agent parses natural language to underlying semantic meanings and uses robotic sensors to create multi-modal models of perceptual concepts like red and heavy. The agent can be used for showing navigation routes, delivering objects to people, and relocating objects from one location to another. We use dialog clarification questions both to understand commands and to generate additional parsing training data. The agent employs opportunistic active learning to select questions about how words relate to objects, improving its understanding of perceptual concepts. We evaluated this agent on Amazon Mechanical Turk. After training on data induced from conversations, the agent reduced the number of dialog questions it asked while receiving higher usability ratings. Additionally, we demonstrated the agent on a robotic platform, where it learned new perceptual concepts on the fly while completing a real-world task.																	1076-9757	1943-5037					2020	67						327	374															
J								Adversarial Attacks on Crowdsourcing Quality Control	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH											GIG ECONOMY; SURVEILLANCE; MODEL; LABOR; WORK	Crowdsourcing is a popular methodology to collect manual labels at scale. Such labels are often used to train AI models and, thus, quality control is a key aspect in the process. One of the most popular quality assurance mechanisms in paid micro-task crowdsourcing is based on gold questions: the use of a small set of tasks for which the requester knows the correct answer and, thus, is able to directly assess crowdwork quality. In this paper, we show that such a mechanism is prone to an attack carried out by a group of colluding crowdworkers that is easy to implement and deploy: the inherent size limit of the gold set can be exploited by building an inferential system to detect which parts of the job are more likely to be gold questions. The described attack is robust to various forms of randomisation and programmatic generation of gold questions. We present the architecture of the proposed system, composed of a browser plug-in and an external server used to share information, and briefly introduce its potential evolution to a decentralised implementation. We implement and experimentally validate the gold question detection system, using real-world data from a popular crowdsourcing platform. Our experimental results show that crowdworkers using the proposed system spend more time on signalled gold questions but do not neglect the others thus achieving an increased overall work quality. Finally, we discuss the economic and sociological implications of this kind of attack.																	1076-9757	1943-5037					2020	67						375	408															
J								Graph Width Measures for CNF-Encodings with Auxiliary Variables	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH											FORMULAS	We consider bounded width CNF-formulas where the width is measured by popular graph width measures on graphs associated to CNF-formulas. Such restricted graph classes, in particular those of bounded treewidth, have been extensively studied for their uses in the design of algorithms for various computational problems on CNF-formulas. Here we consider the expressivity of these formulas in the model of clausal encodings with auxiliary variables. We first show that bounding the width for many of the measures from the literature leads to a dramatic loss of expressivity, restricting the formulas to those of low communication complexity. We then show that the width of optimal encodings with respect to different measures is strongly linked: there are two classes of width measures, one containing primal treewidth and the other incidence cliquewidth, such that in each class the width of optimal encodings only differs by constant factors. Moreover, between the two classes the width differs at most by a factor logarithmic in the number of variables. Both these results are in stark contrast to the setting without auxiliary variables where all width measures we consider here differ by more than constant factors and in many cases even by linear factors.																	1076-9757	1943-5037					2020	67						409	436															
J								Catching Cheats: Detecting Strategic Manipulation in Distributed Optimisation of Electric Vehicle Aggregators	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH											DEMAND RESPONSE; ADMM; POWER	Given the rapid rise of electric vehicles (EVs) worldwide, and the ambitious targets set for the near future, the management of large EV fleets must be seen as a priority. Specifically, we study a scenario where EV charging is managed through self-interested EV aggregators who compete in the day-ahead market in order to purchase the electricity needed to meet their clients' requirements. With the aim of reducing electricity costs and lowering the impact on electricity markets, a centralised bidding coordination framework has been proposed in the literature employing a coordinator. In order to improve privacy and limit the need for the coordinator, we propose a reformulation of the coordination framework as a decentralised algorithm, employing the Alternating Direction Method of Multipliers (ADMM). However, given the self-interested nature of the aggregators, they can deviate from the algorithm in order to reduce their energy costs. Hence, we study the strategic manipulation of the ADMM algorithm and, in doing so, describe and analyse different possible attack vectors and propose a mathematical framework to quantify and detect manipulation. Importantly, this detection framework is not limited to the considered EV scenario and can be applied to general ADMM algorithms. Finally, we test the proposed decentralised coordination and manipulation detection algorithms in realistic scenarios using real market and driver data from Spain. Our empirical results show that the decentralised algorithm's convergence to the optimal solution can be effectively disrupted by manipulative attacks achieving convergence to a different non-optimal solution which benefits the attacker. With respect to the detection algorithm, results indicate that it achieves very high accuracies and significantly outperforms a naive benchmark.																	1076-9757	1943-5037					2020	67						437	470															
J								Fair Allocation with Diminishing Differences	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH											DIVISION; CHOICE; PREFERENCES; ASSIGNMENT; EXTENSION; CONSENSUS; RULES	Ranking alternatives is a natural way for humans to explain their preferences. It is used in many settings, such as school choice, course allocations and residency matches. Without having any information on the underlying cardinal utilities, arguing about the fairness of allocations requires extending the ordinal item ranking to ordinal bundle ranking. The most commonly used such extension is stochastic dominance (SD), where a bundle X is preferred over a bundle Y if its score is better according to all additive score functions. SD is a very conservative extension, by which few allocations are necessarily fair while many allocations are possibly fair. We propose to make a natural assumption on the underlying cardinal utilities of the players, namely that the difference between two items at the top is larger than the difference between two items at the bottom. This assumption implies a preference extension which we call diminishing differences (DD), where X is preferred over Y if its score is better according to all additive score functions satisfying the DD assumption. We give a full characterization of allocations that are necessarily-proportional or possibly-proportional according to this assumption. Based on this characterization, we present a polynomial-time algorithm for finding a necessarily-DD-proportional allocation whenever it exists. Using simulations, we compare the various fairness criteria in terms of their probability of existence, and their probability of being fair by the underlying cardinal valuations. We find that necessary-DD-proportionality fares well in both measures. We also consider envy-freeness and Pareto optimality under diminishing-differences, as well as chore allocation under the analogous condition - increasing-differences.																	1076-9757	1943-5037					2020	67						471	507															
J								A Global Constraint for the Exact Cover Problem: Application to Conceptual Clustering	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH												We introduce the exactCover global constraint dedicated to the exact cover problem, the goal of which is to select subsets such that each element of a given set belongs to exactly one selected subset. This NP-complete problem occurs in many applications, and we more particularly focus on a conceptual clustering application. We introduce three propagation algorithms for exactCover, called Basic, DL, and DL+: Basic ensures the same level of consistency as arc consistency on a classical decomposition of exactCover into binary constraints, without using any specific data structure; DL ensures the same level of consistency as Basic but uses Dancing Links to efficiently maintain the relation between elements and subsets; and DL+ is a stronger propagator which exploits an extra property to filter more values than DL. We also consider the case where the number of selected subsets is constrained to be equal to a given integer variable k, and we show that this may be achieved either by combining exactCover with existing constraints, or by designing a specific propagator that integrates algorithms designed for the NValues constraint. These different propagators are experimentally evaluated on conceptual clustering problems, and they are compared with state-of-the-art declarative approaches. In particular, we show that our global constraint is competitive with recent ILP and CP models for mono-criterion problems, and it has better scale-up properties for multi-criteria problems.																	1076-9757	1943-5037					2020	67						509	547															
J								Robust Multi-Agent Path Finding and Executing	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH											SEARCH	Multi-agent path-finding (MAPF) is the problem of finding a plan for moving a set of agents from their initial locations to their goals without collisions. Following this plan, however, may not be possible due to unexpected events that delay some of the agents. In this work, we propose a holistic solution for MAPF that is robust to such unexpected delays. First, we introduce the notion of a k-robust MAPF plan, which is a plan that can be executed even if a limited number (k) of delays occur. We propose sufficient and required conditions for finding a k-robust plan, and show how to convert several MAPF solvers to find such plans. Then, we propose several robust execution policies. An execution policy is a policy for agents executing a MAPF plan. An execution policy is robust if following it guarantees that the agents reach their goals even if they encounter unexpected delays. Several classes of such robust execution policies are proposed and evaluated experimentally. Finally, we present robust execution policies for cases where communication between the agents may also be delayed. We performed an extensive experimental evaluation in which we compared different algorithms for finding robust MAPF plans, compared different robust execution policies, and studied the interplay between having a robust plan and the performance when using a robust execution policy.																	1076-9757	1943-5037					2020	67						549	579															
J								Agreement on Target-Bidirectional Recurrent Neural Networks for Sequence-to-Sequence Learning	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH												Recurrent neural networks are extremely appealing for sequence-to-sequence learning tasks. Despite their great success, they typically suffer from a shortcoming: they are prone to generate unbalanced targets with good prefixes but bad suffixes, and thus performance suffers when dealing with long sequences. We propose a simple yet effective approach to overcome this shortcoming. Our approach relies on the agreement between a pair of target-directional RNNs, which generates more balanced targets. In addition, we develop two efficient approximate search methods for agreement that are empirically shown to be almost optimal in terms of either sequence level or non-sequence level metrics. Extensive experiments were performed on three standard sequence-to-sequence transduction tasks: machine transliteration, grapheme-to-phoneme transformation and machine translation. The results show that the proposed approach achieves consistent and substantial improvements, compared to many state-of-the-art systems.																	1076-9757	1943-5037					2020	67						581	606		10.1613/jair.1.12008													
J								Solving Delete Free Planning with Relaxed Decision Diagram Based Heuristics	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH											LINEAR PROGRAMMING-MODEL; OPTIMIZATION; STRIPS	We investigate the use of relaxed decision diagrams (DDs) for computing admissible heuristics for the cost-optimal delete-free planning (DFP) problem. Our main contributions are the introduction of two novel DD encodings for a DFP task: a multivalued decision diagram that includes the sequencing aspect of the problem and a binary decision diagram representation of its sequential relaxation. We present construction algorithms for each DD that leverage these different perspectives of the DFP task and provide theoretical and empirical analyses of the associated heuristics. We further show that relaxed DDs can be used beyond heuristic computation to extract delete-free plans, find action landmarks, and identify redundant actions. Our empirical analysis shows that while DD-based heuristics trail the state of the art, even small relaxed DDs are competitive with the linear programming heuristic for the DFP task, thus, revealing novel ways of designing admissible heuristics.																	1076-9757	1943-5037					2020	67						607	651		10.1613/jair.1.11659													
J								A Set of Recommendations for Assessing Human-Machine Parity in Language Translation	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH												The quality of machine translation has increased remarkably over the past years, to the degree that it was found to be indistinguishable from professional human translation in a number of empirical investigations. We reassess Hassan et al.'s 2018 investigation into Chinese to English news translation, showing that the finding of human-machine parity was owed to weaknesses in the evaluation design-which is currently considered best practice in the field. We show that the professional human translations contained significantly fewer errors, and that perceived quality in human evaluation depends on the choice of raters, the availability of linguistic context, and the creation of reference translations. Our results call for revisiting current best practices to assess strong machine translation systems in general and human-machine parity in particular, for which we offer a set of recommendations based on our empirical findings.																	1076-9757	1943-5037					2020	67						653	672															
J								Using Task Descriptions in Lifelong Machine Learning for Improved Performance and Zero-Shot Transfer	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH											ALGORITHMS	Knowledge transfer between tasks can improve the performance of learned models, but requires an accurate estimate of inter-task relationships to identify the relevant knowledge to transfer. These inter-task relationships are typically estimated based on training data for each task, which is inefficient in lifelong learning settings where the goal is to learn each consecutive task rapidly from as little data as possible. To reduce this burden, we develop a lifelong learning method based on coupled dictionary learning that utilizes high-level task descriptions to model inter-task relationships. We show that using task descriptors improves the performance of the learned task policies, providing both theoretical justification for the benefit and empirical demonstration of the improvement across a variety of learning problems. Given only the descriptor for a new task, the lifelong learner is also able to accurately predict a model for the new task through zero-shot learning using the coupled dictionary, eliminating the need to gather training data before addressing the task.																	1076-9757	1943-5037					2020	67						673	703															
J								Hedonic Games with Ordinal Preferences and Thresholds	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH											STABLE PARTITIONS; CORE STABILITY; AGGREGATION; COMPLEXITY; DIVISION; COMMON	We propose a new representation setting for hedonic games, where each agent partitions the set of other agents into friends, enemies, and neutral agents, with friends and enemies being ranked. Under the assumption that preferences are monotonic (respectively, antimonotonic) with respect to the addition of friends (respectively, enemies), we propose a bipolar extension of the responsive extension principle, and use this principle to derive the (partial) preferences of agents over coalitions. Then, for a number of solution concepts, we characterize partitions that necessarily or possibly satisfy them, and we study the related problems in terms of their complexity.																	1076-9757	1943-5037					2020	67						705	756															
J								Compositionality Decomposed: How do Neural Networks Generalise?	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH											SIMPLE RECURRENT NETWORKS; DISTRIBUTED REPRESENTATIONS; MODELS	Despite a multitude of empirical studies, little consensus exists on whether neural networks are able to generalise compositionally, a controversy that, in part, stems from a lack of agreement about what it means for a neural model to be compositional. As a response to this controversy, we present a set of tests that provide a bridge between, on the one hand, the vast amount of linguistic and philosophical theory about compositionality of language and, on the other, the successful neural models of language. We collect different interpretations of compositionality and translate them into five theoretically grounded tests for models that are formulated on a task-independent level. In particular, we provide tests to investigate (i) if models systematically recombine known parts and rules (ii) if models can extend their predictions beyond the length they have seen in the training data (iii) if models' composition operations are local or global (iv) if models' predictions are robust to synonym substitutions and (v) if models favour rules or exceptions during training. To demonstrate the usefulness of this evaluation paradigm, we instantiate these five tests on a highly compositional data set which we dub PCFG SET and apply the resulting tests to three popular sequence-to-sequence models: a recurrent, a convolution-based and a transformer model. We provide an in-depth analysis of the results, which uncover the strengths and weaknesses of these three architectures and point to potential areas of improvement.																	1076-9757	1943-5037					2020	67						757	795															
J								Incomplete Preferences in Single-Peaked Electorates	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH											COMPUTATIONAL ASPECTS; VOTING SCHEMES; TIME ALGORITHM; WINNER PROBLEM; COMPLEXITY; MANIPULATION; ELECTIONS; DECISION	Incomplete preferences are likely to arise in real-world preference aggregation scenarios. This paper deals with determining whether an incomplete preference profile is single-peaked. This is valuable information since many intractable voting problems become tractable given single-peaked preferences. We prove that the problem of recognizing single-peakedness is NP-complete for incomplete profiles consisting of partial orders. Despite this intractability result, we find several polynomial-time algorithms for reasonably restricted settings. In particular, we give polynomial-time recognition algorithms for weak orders, which can be viewed as preferences with indifference.																	1076-9757	1943-5037					2020	67						797	833															
J								HTN Planning as Heuristic Progression Search	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH											COMPLEXITY	The majority of search-based HTN planning systems can be divided into those searching a space of partial plans (a plan space) and those performing progression search, i.e., that build the solution in a forward manner. So far, all HTN planners that guide the search by using heuristic functions are based on plan space search. Those systems represent the set of search nodes more effectively by maintaining a partial ordering between tasks, but they have only limited information about the current state during search. In this article, we propose the use of progression search as basis for heuristic HTN planning systems. Such systems can calculate their heuristics incorporating the current state, because it is tracked during search. Our contribution is the following: We introduce two novel progression algorithms that avoid unnecessary branching when the problem at hand is partially ordered and show that both are sound and complete. We show that defining systematicity is problematic for search in HTN planning, propose a definition, and show that it is fulfilled by one of our algorithms. Then, we introduce a method to apply arbitrary classical planning heuristics to guide the search in HTN planning. It relaxes the HTN planning model to a classical model that is only used for calculating heuristics. It is updated during search and used to create heuristic values that are used to guide the HTN search. We show that it can be used to create HTN heuristics with interesting theoretical properties like safety, goal-awareness, and admissibility. Our empirical evaluation shows that the resulting system outperforms the state of the art in search-based HTN planning.																	1076-9757	1943-5037					2020	67						835	880		10.1613/jair.1.11282													
J								Learning the Language of Software Errors	JOURNAL OF ARTIFICIAL INTELLIGENCE RESEARCH											QUERIES	We propose to use algorithms for learning deterministic finite automata (DFA), such as Angluin's L* algorithm, for learning a DFA that describes the possible scenarios under which a given program error occurs. The alphabet of this automaton is given by the user (for instance, a subset of the function call sites or branches), and hence the automaton describes a user-defined abstraction of those scenarios. More generally, the same technique can be used for visualising the behavior of a program or parts thereof. It can also be used for visually comparing different versions of a program (by presenting an automaton for the behavior in the symmetric difference between them), and for assisting in merging several development branches. We present experiments that demonstrate the power of an abstract visual representation of errors and of program segments, accessible via the project's web page. In addition, our experiments in this paper demonstrate that such automata can be learned efficiently over real-world programs. We also present lazy learning, which is a method for reducing the number of membership queries while using L*, and demonstrate its effectiveness on standard benchmarks.																	1076-9757	1943-5037					2020	67						881	903															
J								Combining RDF and SPARQL with CP-theories to reason about preferences in a Linked Data setting	SEMANTIC WEB										Preferences; ceteris paribus; CP-theories; CP-nets; Linked Data; preference queries; SPARQL; DBpedia	CONDITIONAL PREFERENCE; SEMANTIC WEB	Preference representation and reasoning play a central role in supporting users with complex and multi-factorial decision processes. In fact, user tastes can be used to filter information and data in a personalized way, thus maximizing their expected utility. Over the years, many frameworks and languages have been proposed to deal with user preferences. Among them, one of the most prominent formalism to represent and reason with (qualitative) conditional preferences (CPs) are conditional preference theories (CP-theories). In this paper, we show how to combine them with Semantic Web technologies in order to encode in a standard SPARQL 1.1 query the semantics of a set of CP statements representing user preferences by means of RDF triples that refer to a "preference" OWL ontology. In particular, here we focus on context-uniform conditional (cuc) acyclic CP-theories [Artif. Intell. 175 2011, 1053-1091]. The framework that we propose allows a standard SPARQL client to query Linked Data datasets, and to order the results of such queries relative to a set of user preferences.																	1570-0844	2210-4968					2020	11	3					391	419		10.3233/SW-180339													
J								Towards a question answering system over the Semantic Web	SEMANTIC WEB										Question answering; Multilinguality; portability; QALD; SimpleQuestions		With the development of the Semantic Web, a lot of new structured data has become available on the Web in the form of knowledge bases (KBs). Making this valuable data accessible and usable for end-users is one of the main goals of question answering (QA) over KBs. Most current QA systems query one KB, in one language (namely English). The existing approaches are not designed to be easily adaptable to new KBs and languages. We first introduce a new approach for translating natural language questions to SPARQL queries. It is able to query several KBs simultaneously, in different languages, and can easily be ported to other KBs and languages. In our evaluation, the impact of our approach is proven using 5 different well-known and large KBs: Wikidata, DBpedia, MusicBrainz, DBLP and Freebase as well as 5 different languages namely English, German, French, Italian and Spanish. Second, we show how we integrated our approach, to make it easily accessible by the research community and by end-users. To summarize, we provide a conceptional solution for multilingual, KB-agnostic question answering over the Semantic Web. The provided first approximation validates this concept.																	1570-0844	2210-4968					2020	11	3					421	439		10.3233/SW-190343													
J								Completeness and soundness guarantees for conjunctive SPARQL queries over RDF data sources with completeness statements	SEMANTIC WEB										Data quality; data completeness; query completeness; query soundness; RDF; SPARQL		RDF generally follows the open-world assumption: information is incomplete by default. Consequently, SPARQL queries cannot retrieve with certainty complete answers, and even worse, when they involve negation, it is unclear whether they produce sound answers. Nevertheless, there is hope to lift this limitation. On many specific topics (e.g., children of Trump, Apollo 11 crew, EU founders), RDF data sources contain complete information, a fact that can be made explicit through completeness statements. In this work, we leverage completeness statements over RDF data sources to provide guarantees of completeness and soundness for conjunctive SPARQL queries. We develop a technique to check whether query completeness can be guaranteed by taking into account also the specifics of the queried graph, and analyze the complexity of such checking. For queries with negation, we approach the problem of query soundness checking, and distinguish between answer soundness (i.e., is an answer of a query sound?) and pattern soundness (i.e., is a query as a whole sound?). We provide a formalization and characterize the soundness problem via a reduction to the completeness problem. We further develop heuristic techniques for completeness checking, and conduct experimental evaluations based on Wikidata, a prominent, real-world knowledge base, to demonstrate the feasibility of our approach.																	1570-0844	2210-4968					2020	11	3					441	482		10.3233/SW-190344													
J								A comparison of object-triple mapping libraries	SEMANTIC WEB										Object-triple mapping; object-ontological mapping; object-oriented programming; RDF; benchmark	SEMANTIC WEB; SYSTEM; BENCHMARK; RDF; API	Domain-independent information systems like ontology editors provide only limited usability for non-experts when domain-specific linked data need to be created. On the contrary, domain-specific applications require adequate architecture for data authoring and validation, typically using the object-oriented paradigm. So far, several software libraries mapping the RDF model (representing linked data) to the object model have been introduced in the literature. In this paper, we develop a novel framework for comparison of object-triple mapping solutions in terms of features and performance. For feature comparison, we designed a set of qualitative criteria reflecting object-oriented application developer's needs. For the performance comparison, we introduce a benchmark based on a real-world information system that we implemented using one of the compared object-triple mapping (OTM) solutions - JOPA. We present a detailed evaluation of a selected set of OTM libraries, showing how they differ in terms of features. We further narrow down the set of selected OTM libraries to contain only RDF4J-compatible ones and use the benchmark to measure their time and memory efficiency.																	1570-0844	2210-4968					2020	11	3					483	524		10.3233/SW-190345													
J								Semantic modeling for engineering data solutions	SEMANTIC WEB										Semantic modeling; data analytics; software development; ontology	WEB; ONTOLOGY	Data Analytics Solution (DAS) engineering often involves multiple tasks from data exploration to result presentation which are applied in various contexts and on different datasets. Semantic modeling based on the open world assumption supports flexible modeling of linked knowledge. The objective of this paper is to review existing techniques that leverage semantic web technologies to tackle challenges such as heterogeneity and changing requirements in DAS engineering. We explore the application scope of those techniques, the different types of semantic concepts they use and the role these concepts play during DAS development process. To gather evidence for the study we performed a systematic mapping study by identifying and reviewing 82 papers that incorporate semantic models in engineering DASs. One of the paper's findings is that existing models can be classified within four types of knowledge spheres: domain knowledge, analytics knowledge, services and user intentions. Another finding is to show how this knowledge is used in literature to enhance different tasks within the analytics process. We conclude our study by discussing the limitations of the existing body of research, showcasing the potential of semantic modeling to enhance DASs and presenting the possibility of leveraging ontologies for effective end-to-end DAS engineering.																	1570-0844	2210-4968					2020	11	3					525	547		10.3233/SW-190352													
J								Learning expressive linkage rules from sparse data	SEMANTIC WEB										Entity resolution; sparse data; linkage rules; genetic programming; link discovery	ENTITY RESOLUTION	A central problem in the context of the Web of Data as well as in data integration in general is to identify entities in different data sources that describe the same real-world object. There exists a large body of research on entity resolution. Interestingly, most of the existing research focuses on entity resolution on dense data, meaning data that does not contain too many missing values. This paper sets a different focus and explores learning expressive linkage rules from as well as applying these rules to sparse data, i.e. data exhibiting a large amount of missing values. Sparse data is a common challenge in application domains such as e-commerce, online hotel booking, or online recruiting. We propose and compare three entity resolution methods that employ genetic programming to learn expressive linkage rules from sparse data. First, we introduce the GenLinkGL algorithm which learns groups of matching rules and applies specific rules out of these groups depending on which values are missing from a pair of records. Next, we propose GenLinkSA, which employs selective aggregation operators within rules. These operators exclude misleading similarity scores (which result from missing values) from the aggregations, but on the other hand also penalize the uncertainty that results from missing values. Finally, we introduce GenLinkComb, an algorithm which combines the central ideas of the previous two into one integrated method. We evaluate all methods using six benchmark datasets: three of them are e-commerce product datasets, the other datasets describe restaurants, movies, and drugs. We show improvements of up to 16% F-measure compared to handwritten rules, on average 12% F-measure improvement compared to the original GenLink algorithm, 15% compared to EAGLE, 8% compared to FEBRL, and 5% compared to CoSum-P.																	1570-0844	2210-4968					2020	11	3					549	567		10.3233/SW-190356													
J								A Novel Symmetry Driven Siamese Network for THz Concealed Object Verification	IEEE TRANSACTIONS ON IMAGE PROCESSING										Siamese network; symmetrical prior information; gaussian mixture model; adaptive identity normalization; concealed object verification	DEEP; SEGMENTATION	Security inspection aims to improve the high detection rate as well as reduce the false alarm rate. However, it still suffers from two challenges affecting its robustness. 1) Existing security inspection methods are mostly designed for natural images, which cannot reflect the uniqueness and imaging principle of THz images. 2) Existing methods is sensitive to noise interference and pose variations. This work revisits these challenges and presents a novel symmetry driven Siamese network (SDSN) for THz concealed object verification. Our idea is to employ a specially designed network architecture for THz concealed object verification. First, to reflect the uniqueness and the special property of THz images, Siamese network with Contrastive loss is used for feature extraction along with symmetrical prior information consideration, which can learn symmetrical metrics from the same person. Second, to alleviate the impact of noise interference and pose variations, the adaptive identity normalization (A-IDN) is proposed to normalize the symmetrical metrics each person. Finally, to enhance the generalization of network, an adaptive selective threshold based on Gaussian mixture model (AST-GMM) is designed, which serves as a classifier for the final classification results. Extensive experiments show that SDSN significantly improves the accuracy. Specially, SDSN outperforms the state-of-the-art methods without symmetrical prior information on THz security dataset.																	1057-7149	1941-0042					2020	29						5447	5456		10.1109/TIP.2020.2983554													
J								Web-Shaped Model for Head Pose Estimation: An Approach for Best Exemplar Selection	IEEE TRANSACTIONS ON IMAGE PROCESSING										Face; Pose estimation; Proposals; Training; Face recognition; Head pose estimation; smart cities applications; web-shaped model; head pose exemplar selection	RECOGNITION	Head pose estimation is a sensitive topic in video surveillance/smart ambient scenarios since head rotations can hide/distort discriminative features of the face. Face recognition would often tackle the problem of video frames where subjects appear in poses making it quite impossible. In this respect, the selection of the frames with the best face orientation can allow triggering recognition only on these, therefore decreasing the possibility of errors. This paper proposes a novel approach to head pose estimation for smart cities and video surveillance scenarios, aiming at this goal. The method relies on a cascade of two models: the first one predicts the positions of 68 well-known face landmarks; the second one applies a web-shaped model over the detected landmarks, to associate each of them to a specific face sector. The method can work on detected faces at a reasonable distance and with a resolution that is supported by several present devices. Results of experiments executed over some classical pose estimation benchmarks, namely Point '04, Biwi, and AFLW datasets show good performance in terms of both pose estimation and computing time. Further results refer to noisy images that are typical of the addressed settings. Finally, examples demonstrate the selection of the best frames from videos captured in video surveillance conditions.																	1057-7149	1941-0042					2020	29						5457	5468		10.1109/TIP.2020.2984373													
J								Quantum inspired monarch butterfly optimisation for UCAV path planning navigation problem	INTERNATIONAL JOURNAL OF BIO-INSPIRED COMPUTATION										unmanned combat air vehicle; monarch butterfly optimisation; MBO; path planning navigation; quantum computation; B-spline curve	ARTIFICIAL BEE COLONY; KRILL HERD ALGORITHM; CUCKOO SEARCH ALGORITHM; EVOLUTIONARY ALGORITHM; FIREFLY ALGORITHM; PARTICLE-SWARM; PERFORMANCE; DESIGN; MODEL	As a complicated high-dimensional optimisation problem, path planning navigation problem for uninhabited combat air vehicles (UCAV) is to obtain a shortest safe flight route with different types of constrains under complicated combating environments. Monarch butterfly optimisation (MBO) is a highly promising swarm intelligence algorithm. Since then, though it has successfully solved several challenging problems, MBO may be trapped into local optima sometimes. In order to improve the performance of MBO, quantum computation is firstly incorporated into the basic MBO algorithm and a new quantum inspired MBO algorithm is then proposed called QMBO. In QMBO, a certain number of the worst butterflies are updated by quantum operators. In this paper, the UCAV path planning navigation problem is modelled into an optimisation problem and then, its optimal path can be obtained by the proposed QMBO algorithm. In addition, B-spline curves are utilised to further smoothen the obtained path and make it more feasible for UCAV. The UCAV path obtained by QMBO is compared with the basic MBO and the experimental results show that QMBO can find much shorter path than MBO.																	1758-0366	1758-0374					2020	15	2					75	89		10.1504/IJBIC.2020.106428													
J								Parallel implementation of genetic algorithm on FPGA using Vivado high level synthesis	INTERNATIONAL JOURNAL OF BIO-INSPIRED COMPUTATION										genetic algorithm; GA; bio-inspired computation; Vivado HLS tool; parallel architecture; optimisation techniques		Genetic algorithm (GA) is one of most popular evolutionary search algorithms that simulates natural selection of genetic evolution for searching solution to arbitrary engineering problems. However, it is computationally intensive and will become a limiting factor for evolving solution to most of the real life problems as it involves large number of parameters that needs to be determined. Fortunately, there are some parallel platforms such as field programmable gate array (FPGA) that can be adopted to overcome this constrains by improving its latency. So, efficient parallel implementation of GA was proposed where each step of GA was exploited to improve its computational task. Moreover, many optimization and parallelisation techniques were adopted and applied to achieve high speed up. The results show that 43 speed up is achieved compared with the typical one. Moreover, higher speed up can be achieved with larger input size.																	1758-0366	1758-0374					2020	15	2					90	99		10.1504/IJBIC.2020.106439													
J								Fractional Levy flight bat algorithm for global optimisation	INTERNATIONAL JOURNAL OF BIO-INSPIRED COMPUTATION										fractional calculus; bat algorithm; Levy flight; non-parametric statistical tests		A well-known metaheuristic is the bat algorithm (BA), which consists of an iterative learning process inspired by bats echolocation behaviour in searching for prays. Basically, the BA uses a predefined number of bats that collectively move on the search space to find the global optimum. This article proposes the fractional Levy flight bat algorithm (FLFBA), which is an improved version of the classical BA. In the FLFBA the velocity is updated through fractional calculus and a local search procedure that uses a random walk based on Levy distribution. Such modifications enhance the ability of the algorithm to escape from local optimal values. The FLFBA has been tested using several well-known benchmark functions and its convergence is also compared with other evolutionary algorithms from the state-of-the-art. The results indicate that the FLFBA provided in several cases better performance in comparison to the selected evolutionary algorithms.																	1758-0366	1758-0374					2020	15	2					100	112		10.1504/IJBIC.2020.106441													
J								An enhanced genetic algorithm for the distributed assembly permutation flowshop scheduling problem	INTERNATIONAL JOURNAL OF BIO-INSPIRED COMPUTATION										distributed assembly scheduling; permutation flowshop; meta-heuristic; genetic algorithm; crossover; local search	SEARCH ALGORITHM; SETUP	The distributed assembly permutation flowshop scheduling problem (DAPFSP) is a new generalisation of the distributed permutation flowshop scheduling problem (DPFSP) and the assembly flowshop scheduling problem (AFSP), aiming to minimise makespan. This production mode is more complicated and competitive in the real production process and includes two phases: production and assembly. Firstly, the production is conducted in several identical factories, and the production in each factory can be considered to a permutation flowshop scheduling problem (PFSP) with multi-machines. Then, the jobs produced in the first stage are assembled into final products. An enhanced population-based meta-heuristic - genetic algorithm (GA) is proposed for this problem. A greedy mating pool is designed to select promising parents in the selection operation, and an effective crossover strategy is designed based on the local search for speeding up convergence. To enhance the exploitation capability, several different local search strategies are incorporated into the algorithm, which are based on two neighbourhood structures. The exhaustive experiment and statistical analysis show that the proposed algorithms outperform the existing algorithms.																	1758-0366	1758-0374					2020	15	2					113	124		10.1504/IJBIC.2020.106443													
J								A hybrid bio-inspired optimisation approach for wirelength minimisation of hardware tasks placement in field programmable gate array devices	INTERNATIONAL JOURNAL OF BIO-INSPIRED COMPUTATION										hardware tasks placement; NP-complete problem; wirelength minimisation; reconfigurable FPGAs; bio-inspired optimisation approach; genetic algorithm; particle swarm optimisation; PSO		In computer-aided design (CAD) flow of VLSI circuits, placement process is an NP-complete problem which requires an optimisation approach to obtain the system performance better. The main objective of placement is to reduce the wire length between the tasks with zero overlap. Fast response and better convergence algorithms are required to meet these desires. In this regard, bio-inspired optimisation algorithms such as genetic algorithm (GA) and particle swarm optimisation (PSO) algorithm have been considered. By using the salient features of these two algorithms, the optimised solution for placement problem has been obtained. The concept of GA has been applied followed by genetic algorithm to obtain optimised result. For experimentation, various directed data flow graphs (DDFGs) are randomly generated and the comparison is made between the GA, PSO and hybrid (GA-PSO) methods. The hybrid approach using GA-PSO produces better experimental results in wire length minimisation and, hence outperforms than the others.																	1758-0366	1758-0374					2020	15	2					125	134		10.1504/IJBIC.2020.106449													
J								Open-Ended Video Question Answering via Multi-Modal Conditional Adversarial Networks	IEEE TRANSACTIONS ON IMAGE PROCESSING										Open-ended video question answering; multi-modal neural network		As a challenging task in visual information retrieval, open-ended long-form video question answering automatically generates the natural language answer from the referenced video content according to the given question. However, the existing video question answering works mainly focus on the short-form video, which may be ineffectively applied for long-form video question answering directly, due to the insufficiency of modeling the semantic representation of long-form video content. In this paper, we study the problem of open-ended long-form video question answering from the viewpoint of hierarchical multi-modal conditional adversarial network learning. We propose the hierarchical attentional encoder network to learn the joint representation of long-form video content and given question with adaptive video segmentation. We then devise the reinforced decoder network to generate the natural language answer for open-ended video question answering with multi-modal conditional adversarial network learning. We construct three large-scale open-ended video question answering datasets. The extensive experiments validate the effectiveness of our method.																	1057-7149	1941-0042					2020	29						3859	3870		10.1109/TIP.2020.2963950													
J								Dynamic Random Walk for Superpixel Segmentation	IEEE TRANSACTIONS ON IMAGE PROCESSING										Random walk; image segmentation; superpixel segmentation; weighted random walk entropy		In this paper, we propose a novel random walk model, called Dynamic Random Walk (DRW), which adds a new type of dynamic node to the original RW model and reduces redundant calculation by limiting the walk range. To solve the seed-lacking problem of the proposed DRW, we redefine the energy function of the original RW and use the first arrival probability among each node pair to avoid the interference for each partition. Relaxation of our DRW is performed with the help of a greedy strategy and the Weighted Random Walk Entropy(WRWE) that uses the gradient feature to approximate the stationary distribution. The proposed DRW not only can enhance the boundary adherence but also can run with linear time complexity. To extend our DRW for superpixel segmentation, a seed initialization strategy is proposed. It can evenly distribute seeds in both 2D and 3D space and generate superpixels in only one iteration. The experimental results demonstrate that our DRW is faster than existing RW models and better than the state-of-the-art superpixel segmentation algorithms with respect to both efficiency and segmentation effects.																	1057-7149	1941-0042					2020	29						3871	3884		10.1109/TIP.2020.2967583													
J								OCTRexpert: A Feature-Based 3D Registration Method for Retinal OCT Images	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image registration; optical coherence tomography (OCT); retinal image	OPTICAL COHERENCE TOMOGRAPHY; LAYER SEGMENTATION; ARTIFACTS	Medical image registration can be used for studying longitudinal and cross-sectional data, quantitatively monitoring disease progression and guiding computer assisted diagnosis and treatments. However, deformable registration which enables more precise and quantitative comparison has not been well developed for retinal optical coherence tomography (OCT) images. This paper proposes a new 3D registration approach for retinal OCT data called OCTRexpert. To the best of our knowledge, the proposed algorithm is the first full 3D registration approach for retinal OCT images which can be applied to longitudinal OCT images for both normal and serious pathological subjects. In this approach, a pre-processing method is first performed to remove eye motion artifact and then a novel design-detection-deformation strategy is applied for the registration. In the design step, a couple of features are designed for each voxel in the image. In the detection step, active voxels are selected and the point-to-point correspondences between the subject and template images are established. In the deformation step, the image is hierarchically deformed according to the detected correspondences in multi-resolution. The proposed method is evaluated on a dataset with longitudinal OCT images from 20 healthy subjects and 4 subjects diagnosed with serious Choroidal Neovascularization (CNV). Experimental results show that the proposed registration algorithm consistently yields statistically significant improvements in both Dice similarity coefficient and the average unsigned surface error compared with the other registration methods.																	1057-7149	1941-0042					2020	29						3885	3897		10.1109/TIP.2020.2967589													
J								Personality-Assisted Multi-Task Learning for Generic and Personalized Image Aesthetics Assessment	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image aesthetics assessment; generic and personalized image aesthetics; personality traits; multi-task deep learning; Siamese network	PHOTO; PREFERENCES; SAMPLE; MODEL	Traditional image aesthetics assessment (IAA) approaches mainly predict the average aesthetic score of an image. However, people tend to have different tastes on image aesthetics, which is mainly determined by their subjective preferences. As an important subjective trait, personality is believed to be a key factor in modeling individual's subjective preference. In this paper, we present a personality-assisted multi-task deep learning framework for both generic and personalized image aesthetics assessment. The proposed framework comprises two stages. In the first stage, a multi-task learning network with shared weights is proposed to predict the aesthetics distribution of an image and Big-Five (BF) personality traits of people who like the image. The generic aesthetics score of the image can be generated based on the predicted aesthetics distribution. In order to capture the common representation of generic image aesthetics and people's personality traits, a Siamese network is trained using aesthetics data and personality data jointly. In the second stage, based on the predicted personality traits and generic aesthetics of an image, an inter-task fusion is introduced to generate individual's personalized aesthetic scores on the image. The performance of the proposed method is evaluated using two public image aesthetics databases. The experimental results demonstrate that the proposed method outperforms the state-of-the-arts in both generic and personalized IAA tasks.																	1057-7149	1941-0042					2020	29						3898	3910		10.1109/TIP.2020.2968285													
J								An Interpretable Deep Architecture for Similarity Learning Built Upon Hierarchical Concepts	IEEE TRANSACTIONS ON IMAGE PROCESSING										Similarity learning; neural networks; clustering; image retrieval; model interpretability	PERSON REIDENTIFICATION	In general, development of adequately complex mathematical models, such as deep neural networks, can be an effective way to improve the accuracy of learning models. However, this is achieved at the cost of reduced post-hoc model interpretability, because what is learned by the model can become less intelligible and tractable to humans as the model complexity increases. In this paper, we target a similarity learning task in the context of image retrieval, with a focus on the model interpretability issue. An effective similarity neural network (SNN) is proposed not only to seek robust retrieval performance but also to achieve satisfactory post-hoc interpretability. The network is designed by linking the neuron architecture with the organization of a concept tree and by formulating neuron operations to pass similarity information between concepts. Various ways of understanding and visualizing what is learned by the SNN neurons are proposed. We also exhaustively evaluate the proposed approach using a number of relevant datasets against a number of state-of-the-art approaches to demonstrate the effectiveness of the proposed network. Our results show that the proposed approach can offer superior performance when compared against state-of-the-art approaches. Neuron visualization results are demonstrated to support the understanding of the trained neurons.																	1057-7149	1941-0042					2020	29						3911	3926		10.1109/TIP.2020.2965275													
J								Learning Hybrid Representation by Robust Dictionary Learning in Factorized Compressed Space	IEEE TRANSACTIONS ON IMAGE PROCESSING										Hybrid salient representation; robust factorized compression; robust projective dictionary learning; classification	LOW-RANK; SPARSE REPRESENTATION; FACE RECOGNITION; DISCRIMINATIVE DICTIONARY; K-SVD; ILLUMINATION; REGRESSION; MODELS	In this paper, we investigate the robust dictionary learning (DL) to discover the hybrid salient low-rank and sparse representation in a factorized compressed space. A Joint Robust Factorization and Projective Dictionary Learning (J-RFDL) model is presented. The setting of J-RFDL aims at improving the data representations by enhancing the robustness to outliers and noise in data, encoding the reconstruction error more accurately and obtaining hybrid salient coefficients with accurate reconstruction ability. Specifically, J-RFDL performs the robust representation by DL in a factorized compressed space to eliminate the negative effects of noise and outliers on the results, which can also make the DL process efficient. To make the encoding process robust to noise in data, J-RFDL clearly uses sparse L-2,L- 1-norm that can potentially minimize the factorization and reconstruction errors jointly by forcing rows of the reconstruction errors to be zeros. To deliver salient coefficients with good structures to reconstruct given data well, J-RFDL imposes the joint low-rank and sparse constraints on the embedded coefficients with a synthesis dictionary. Based on the hybrid salient coefficients, we also extend J-RFDL for the joint classification and propose a discriminative J-RFDL model, which can improve the discriminating abilities of learnt coefficients by minimizing the classification error jointly. Extensive experiments on public datasets demonstrate that our formulations can deliver superior performance over other state-of-the-art methods.																	1057-7149	1941-0042					2020	29						3941	3956		10.1109/TIP.2020.2965289													
J								Modality Compensation Network: Cross-Modal Adaptation for Action Recognition	IEEE TRANSACTIONS ON IMAGE PROCESSING										Modality compensation; multi-modal; action recognition		With the prevalence of RGB-D cameras, multi-modal video data have become more available for human action recognition. One main challenge for this task lies in how to effectively leverage their complementary information. In this work, we propose a Modality Compensation Network (MCN) to explore the relationships of different modalities, and boost the representations for human action recognition. We regard RGB/optical flow videos as source modalities, skeletons as auxiliary modality. Our goal is to extract more discriminative features from source modalities, with the help of auxiliary modality. Built on deep Convolutional Neural Networks (CNN) and Long Short Term Memory (LSTM) networks, our model bridges data from source and auxiliary modalities by a modality adaptation block to achieve adaptive representation learning, that the network learns to compensate for the loss of skeletons at test time and even at training time. We explore multiple adaptation schemes to narrow the distance between source and auxiliary modal distributions from different levels, according to the alignment of source and auxiliary data in training. In addition, skeletons are only required in the training phase. Our model is able to improve the recognition performance with source data when testing. Experimental results reveal that MCN outperforms state-of-the-art approaches on four widely-used action recognition benchmarks.																	1057-7149	1941-0042					2020	29						3957	3969		10.1109/TIP.2020.2967577													
J								Graph Laplacian Regularization for Robust Optical Flow Estimation	IEEE TRANSACTIONS ON IMAGE PROCESSING										Laplace equations; Estimation; Optical imaging; Kernel; Eigenvalues and eigenfunctions; Optimization; Inverse problems; Optical flow; regularization; optimization; robust estimation	SMOOTHNESS CONSTRAINTS; COMPUTATION; ALGORITHM	This paper proposes graph Laplacian regularization for robust estimation of optical flow. First, we analyze the spectral properties of dense graph Laplacians and show that dense graphs achieve a better trade-off between preserving flow discontinuities and filtering noise, compared with the usual Laplacian. Using this analysis, we then propose a robust optical flow estimation method based on Gaussian graph Laplacians. We revisit the framework of iteratively reweighted least-squares from the perspective of graph edge reweighting, and employ the Welsch loss function to preserve flow discontinuities and handle occlusions. Our experiments using the Middlebury and MPI-Sintel optical flow datasets demonstrate the robustness and the efficiency of our proposed approach.																	1057-7149	1941-0042					2020	29						3970	3983		10.1109/TIP.2019.2945653													
J								Revisiting EmbodiedQA: A Simple Baseline and Beyond	IEEE TRANSACTIONS ON IMAGE PROCESSING										Embodied question answering; vision and language; visual question answering	OBSTACLE AVOIDANCE	In Embodied Question Answering (EmbodiedQA), an agent interacts with an environment to gather necessary information for answering user questions. Existing works have laid a solid foundation towards solving this interesting problem. But the current performance, especially in navigation, suggests that EmbodiedQA might be too challenging for the contemporary approaches. In this paper, we empirically study this problem and introduce 1) a simple yet effective baseline that achieves promising performance; 2) an easier and practical setting for EmbodiedQA where an agent has a chance to adapt the trained model to a new environment before it actually answers users questions. In this new setting, we randomly place a few objects in new environments, and upgrade the agent policy by a distillation network to retain the generalization ability from the trained model. On the EmbodiedQA v1 benchmark, under the standard setting, our simple baseline achieves very competitive results to the-state-of-the-art; in the new setting, we found the introduced small change in settings yields a notable gain in navigation.																	1057-7149	1941-0042					2020	29						3984	3992		10.1109/TIP.2020.2967584													
J								Unsupervised Multi-Target Domain Adaptation: An Information Theoretic Approach	IEEE TRANSACTIONS ON IMAGE PROCESSING										Domain adaptation; mutual information; variational inference; adversarial learning		Unsupervised domain adaptation (uDA) models focus on pairwise adaptation settings where there is a single, labeled, source and a single target domain. However, in many real-world settings one seeks to adapt to multiple, but somewhat similar, target domains. Applying pairwise adaptation approaches to this setting may be suboptimal, as they fail to leverage shared information among multiple domains. In this work, we propose an information theoretic approach for domain adaptation in the novel context of multiple target domains with unlabeled instances and one source domain with labeled instances. Our model aims to find a shared latent space common to all domains, while simultaneously accounting for the remaining private, domain-specific factors. Disentanglement of shared and private information is accomplished using a unified information-theoretic approach, which also serves to establish a stronger link between the latent representations and the observed data. The resulting model, accompanied by an efficient optimization algorithm, allows simultaneous adaptation from a single source to multiple target domains. We test our approach on three challenging publicly-available datasets, showing that it outperforms several popular domain adaptation methods.																	1057-7149	1941-0042					2020	29						3993	4002		10.1109/TIP.2019.2963389													
J								A Context-Aware Locality Measure for Inlier Pool Enrichment in Stepwise Image Registration	IEEE TRANSACTIONS ON IMAGE PROCESSING										Feature matching; registration; dissimilarity measure; non-rigid	UNCERTAINTY; MODEL	We present a feature-based image registration method, the stepwise image registration (SIR), with a closed-form solution. Our SIR creates an inlier pool and a candidate pool as the initialization, and then gradually enriches the inlier pool and refines the transformation. In each step, the enriched correspondence exclusively tunes the transformation coefficient within the confirmed inlier pairs, instead of updating the mapping using the complete putative set. In turn, the refined transformation prunes inconsistent mismatches to alleviate the incoming matching ambiguity. The context-aware locality measure (CALM) is designed for dissimilarity measure. The capability of the CALM can be enhanced by the progressive inlier pool enrichment. Finally, a retrieval process is performed based on the finest CALM and alignment, by which the inlier pool is maximized. Extensive experiments of enrichment evaluation, feature matching, image registration, and image retrieval demonstrate the favorable performance of our SIR against state-of-the-art methods. The code and datasets are available at https://github.com/sucv/SIR.																	1057-7149	1941-0042					2020	29						4281	4295		10.1109/TIP.2019.2961480													
J								Improved Saliency Detection in RGB-D Images Using Two-Phase Depth Estimation and Selective Deep Fusion	IEEE TRANSACTIONS ON IMAGE PROCESSING										RGB-D saliency detection; inter-image correspondences; low-level saliency; selective deep fusion	OBJECT DETECTION; VIDEO	To solve the saliency detection problem in RGB-D images, the depth information plays a critical role in distinguishing salient objects or foregrounds from cluttered backgrounds. As the complementary component to color information, the depth quality directly dictates the subsequent saliency detection performance. However, due to artifacts and the limitation of depth acquisition devices, the quality of the obtained depth varies tremendously across different scenarios. Consequently, conventional selective fusion-based RGB-D saliency detection methods may result in a degraded detection performance in cases containing salient objects with low color contrast coupled with a low depth quality. To solve this problem, we make our initial attempt to estimate additional high-quality depth information, which is denoted by Depth(+). Serving as a complement to the original depth, Depth(+) will be fed into our newly designed selective fusion network to boost the detection performance. To achieve this aim, we first retrieve a small group of images that are similar to the given input, and then the inter-image, nonlocal correspondences are built accordingly. Thus, by using these inter-image correspondences, the overall depth can be coarsely estimated by utilizing our newly designed depth-transferring strategy. Next, we build fine-grained, object-level correspondences coupled with a saliency prior to further improve the depth quality of the previous estimation. Compared to the original depth, our newly estimated Depth(+) is potentially more informative for detection improvement. Finally, we feed both the original depth and the newly estimated Depth(+) into our selective deep fusion network, whose key novelty is to achieve an optimal complementary balance to make better decisions toward improving saliency boundaries.																	1057-7149	1941-0042					2020	29						4296	4307		10.1109/TIP.2020.2968250													
J								Deep HDR Imaging via A Non-Local Network	IEEE TRANSACTIONS ON IMAGE PROCESSING										High dynamic range image; ghosting artifacts; non-local module; hybrid network	DYNAMIC-RANGE; IMAGES	One of the most challenging problems in reconstructing a high dynamic range (HDR) image from multiple low dynamic range (LDR) inputs is the ghosting artifacts caused by the object motion across different inputs. When the object motion is slight, most existing methods can well suppress the ghosting artifacts through aligning LDR inputs based on optical flow or detecting anomalies among them. However, they often fail to produce satisfactory results in practice, since the real object motion can be very large. In this study, we present a novel deep framework, termed NHDRRnet, which adopts an alternative direction and attempts to remove ghosting artifacts by exploiting the non-local correlation in inputs. In NHDRRnet, we first adopt an Unet architecture to fuse all inputs and map the fusion results into a low-dimensional deep feature space. Then, we feed the resultant features into a novel global non-local module which reconstructs each pixel by weighted averaging all the other pixels using the weights determined by their correspondences. By doing this, the proposed NHDRRnet is able to adaptively select the useful information (e.g., which are not corrupted by large motions or adverse lighting conditions) in the whole deep feature space to accurately reconstruct each pixel. In addition, we also incorporate a triple-pass residual module to capture more powerful local features, which proves to be effective in further boosting the performance. Extensive experiments on three benchmark datasets demonstrate the superiority of the proposed NDHRnet in terms of suppressing the ghosting artifacts in HDR reconstruction, especially when the objects have large motions.																	1057-7149	1941-0042					2020	29						4308	4322		10.1109/TIP.2020.2971346													
J								Deep Video Super-Resolution Using HR Optical Flow Estimation	IEEE TRANSACTIONS ON IMAGE PROCESSING										Video super-resolution; optical flow estimation; temporal consistency; scale-recurrent architecture	IMAGE SUPERRESOLUTION; RECONSTRUCTION	Video super-resolution (SR) aims at generating a sequence of high-resolution (HR) frames with plausible and temporally consistent details from their low-resolution (LR) counterparts. The key challenge for video SR lies in the effective exploitation of temporal dependency between consecutive frames. Existing deep learning based methods commonly estimate optical flows between LR frames to provide temporal dependency. However, the resolution conflict between LR optical flows and HR outputs hinders the recovery of fine details. In this paper, we propose an end-to-end video SR network to super-resolve both optical flows and images. Optical flow SR from LR frames provides accurate temporal dependency and ultimately improves video SR performance. Specifically, we first propose an optical flow reconstruction network (OFRnet) to infer HR optical flows in a coarse-to-fine manner. Then, motion compensation is performed using HR optical flows to encode temporal dependency. Finally, compensated LR inputs are fed to a super-resolution network (SRnet) to generate SR results. Extensive experiments have been conducted to demonstrate the effectiveness of HR optical flows for SR performance improvement. Comparative results on the Vid4 and DAVIS-10 datasets show that our network achieves the state-of-the-art performance.																	1057-7149	1941-0042					2020	29						4323	4336		10.1109/TIP.2020.2967596													
J								Improving Dataset Volumes and Model Accuracy With Semi-Supervised Iterative Self-Learning	IEEE TRANSACTIONS ON IMAGE PROCESSING										Training; Data models; Semisupervised learning; Task analysis; Noise measurement; Deep learning; Solid modeling; Semi-supervised; image classification; deep learning; machine learning		Within this paper, a novel semi-supervised learning technique is introduced based on a simple iterative learning cycle together with learned thresholding techniques and an ensemble decision support system. The state-of-the-art model performance and increased training data volume are demonstrated through the use of unlabeled data when training deeply learned classification models. The methods presented work independently from the model architectures or loss functions, making this approach applicable to a wide range of machine learning and classification tasks. Evaluation of the proposed approach is performed on commonly used datasets when evaluating semi-supervised learning techniques and a number of more challenging image classification datasets (CIFAR-100 and a 200 class subset of ImageNet).																	1057-7149	1941-0042					2020	29						4337	4348		10.1109/TIP.2019.2913986													
J								Autonomous Selective Parts-Based Tracking	IEEE TRANSACTIONS ON IMAGE PROCESSING										Correlation filters; parts-based; tracking; video; genetic algorithm	OBJECT TRACKING	Object tracking from videos is still a challenging task due to various changes throughout a video sequence including occlusions, motion blur, scale and other deformation changes. In this paper, we propose a selective parts-based approach, using correlation filters, that makes choices based on a consensus of the parts and global tracking. Moreover, we further enhance our parts-based approach by introducing a segmentation-assisted parts initialization. In addition, we present a genetic algorithm-based method to autonomously select various parameters of the tracking algorithm, as opposed to the common practice of manually tuning those parameters. In contrast to existing part-based methods, the proposed method does not dilute accurate tracking by averaging results over multiple parts at every frame. Instead, we take a selective approach based on the relative weight of the responses across parts. Moreover, we only make location corrections when a part diverges, and rely on these location corrections to maintain an accurate appearance model. In the case of occlusions, which are among the main reasons for using a parts-based approach, our proposed approach consistently achieves the best performance. It is due to the ability to handle occlusion and not dilute decisions with incorrect parts, that our proposed approach enables state-of-the-art performance. The proposed approach was evaluated on videos from three different challenging benchmark datasets. Our approach has resulted in better overall precision and success rates for three different base tracking approaches.																	1057-7149	1941-0042					2020	29						4349	4361		10.1109/TIP.2020.2967580													
J								A Spatio-Temporal Multi-Scale Binary Descriptor	IEEE TRANSACTIONS ON IMAGE PROCESSING										Binary descriptor; spatio-temporal feature; multi-scale	SLAM	Binary descriptors are widely used for multi-view matching and robotic navigation. However, their matching performance decreases considerably under severe scale and viewpoint changes in non-planar scenes. To overcome this problem, we propose to encode the varying appearance of selected 3D scene points tracked by a moving camera with compact spatio-temporal descriptors. To this end, we first track interest points and capture their temporal variations at multiple scales. Then, we validate feature tracks through 3D reconstruction and compress the temporal sequence of descriptors by encoding the most frequent and stable binary values. Finally, we determine multi-scale correspondences across views with a matching strategy that handles severe scale differences. The proposed spatio-temporal multi-scale approach is generic and can be used with a variety of binary descriptors. We show the effectiveness of the joint multi-scale extraction and temporal reduction through comparisons of different temporal reduction strategies and the application to several binary descriptors.																	1057-7149	1941-0042					2020	29						4362	4375		10.1109/TIP.2020.2965277													
J								An Underwater Image Enhancement Benchmark Dataset and Beyond	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image enhancement; Image color analysis; Benchmark testing; Image restoration; Electronic mail; Gallium nitride; Training; Underwater image enhancement; real-world underwater images; comprehensive evaluation; deep learning	COLOR; RESTORATION; VISIBILITY; WATER	Underwater image enhancement has been attracting much attention due to its significance in marine engineering and aquatic robotics. Numerous underwater image enhancement algorithms have been proposed in the last few years. However, these algorithms are mainly evaluated using either synthetic datasets or few selected real-world images. It is thus unclear how these algorithms would perform on images acquired in the wild and how we could gauge the progress in the field. To bridge this gap, we present the first comprehensive perceptual study and analysis of underwater image enhancement using large-scale real-world images. In this paper, we construct an Underwater Image Enhancement Benchmark (UIEB) including 950 real-world underwater images, 890 of which have the corresponding reference images. We treat the rest 60 underwater images which cannot obtain satisfactory reference images as challenging data. Using this dataset, we conduct a comprehensive study of the state-of-the-art underwater image enhancement algorithms qualitatively and quantitatively. In addition, we propose an underwater image enhancement network (called Water-Net) trained on this benchmark as a baseline, which indicates the generalization of the proposed UIEB for training Convolutional Neural Networks (CNNs). The benchmark evaluations and the proposed Water-Net demonstrate the performance and limitations of state-of-the-art algorithms, which shed light on future research in underwater image enhancement. The dataset and code are available at https://li-chongyi.github.io/proj_benchmark.html.																	1057-7149	1941-0042					2020	29						4376	4389		10.1109/TIP.2019.2955241													
J								Robust Structural Low-Rank Tracking	IEEE TRANSACTIONS ON IMAGE PROCESSING										Visual object tracking (VOT); structural constraints; low-rank modeling	VISUAL TRACKING; OBJECT TRACKING; CONTOURS; IMAGE	Visual object tracking is an essential task for many computer vision applications. It becomes very challenging when the target appearance changes especially in the presence of occlusion, background clutter, and sudden illumination variations. Methods, that incorporate sparse representation and low-rank assumptions on the target particles have achieved promising results. However, because of the lack of structural constraints, these methods show performance degradation when facing the aforementioned challenges. To alleviate these limitations, we propose a new structural low-rank modeling algorithm for robust object tracking in complex scenarios. In the proposed algorithm, we consider spatial and temporal appearance consistency constraints, among the particles in the low-rank subspace, embedded in four different graphs. The resulting objective function encoding these constraints is novel and it is solved using linearized alternating direction method with adaptive penalty both in batch fashion as well as in online fashion. Our proposed objective function jointly learns the spatial and temporal structure of the target particles in consecutive frames and makes the proposed tracker consistent against many complex tracking scenarios. Results on four challenging datasets demonstrate excellent performance of the proposed algorithm as compared to current state-of-the-art methods.																	1057-7149	1941-0042					2020	29						4390	4405		10.1109/TIP.2020.2972102													
J								Arc Adjacency Matrix-Based Fast Ellipse Detection	IEEE TRANSACTIONS ON IMAGE PROCESSING										Ellipse detection; arc adjacency matrix; ellipse validation	RANDOMIZED HOUGH TRANSFORM; CIRCLE	Fast and accurate ellipse detection is critical in certain computer vision tasks. In this paper, we propose an arc adjacency matrix-based ellipse detection (AAMED) method to fulfill this requirement. At first, after segmenting the edges into elliptic arcs, the digraph-based arc adjacency matrix (AAM) is constructed to describe their triple sequential adjacency states. Curvature and region constraints are employed to make the AAM sparse. Secondly, through bidirectionally searching the AAM, we can get all arc combinations which are probably true ellipse candidates. The cumulative-factor (CF) based cumulative matrices (CM) are worked out simultaneously. CF is irrelative to the image context and can be pre-calculated. CM is related to the arcs or arc combinations and can be calculated by the addition or subtraction of CF. Then the ellipses are efficiently fitted from these candidates through twice eigendecomposition of CM using Jacobi method. Finally, a comprehensive validation score is proposed to eliminate false ellipses effectively. The score is mainly influenced by the constraints about adaptive shape, tangent similarity, distribution compensation. Experiments show that our method outperforms the 12 state-of-the-art methods on 9 datasets as a whole, with reference to recall, precision, F-measure, and time-consumption.																	1057-7149	1941-0042					2020	29						4406	4420		10.1109/TIP.2020.2967601													
J								Light Field Saliency Detection With Deep Convolutional Networks	IEEE TRANSACTIONS ON IMAGE PROCESSING										Saliency detection; light field; micro-lens images; angular changes; deep neural network	OBJECT DETECTION	Light field imaging presents an attractive alternative to RGB imaging because of the recording of the direction of the incoming light. The detection of salient regions in a light field image benefits from the additional modeling of angular patterns. For RGB imaging, methods using CNNs have achieved excellent results on a range of tasks, including saliency detection. However, it is not trivial to use CNN-based methods for saliency detection on light field images because these methods are not specifically designed for processing light field inputs. In addition, current light field datasets are not sufficiently large to train CNNs. To overcome these issues, we present a new Lytro Illum dataset, which contains 640 light fields and their corresponding ground-truth saliency maps. Compared to current publicly available light field saliency datasets [1], [2], our new dataset is larger, of higher quality, contains more variation and more types of light field inputs. This makes our dataset suitable for training deeper networks and benchmarking. Furthermore, we propose a novel end-to-end CNN-based framework for light field saliency detection. Specifically, we propose three novel MAC (Model Angular Changes) blocks to process light field micro-lens images. We systematically study the impact of different architecture variants and compare light field saliency with regular 2D saliency. Our extensive comparisons indicate that our novel network significantly outperforms state-of-the-art methods on the proposed dataset and has desired generalization abilities on other existing datasets.																	1057-7149	1941-0042					2020	29						4421	4434		10.1109/TIP.2020.2970529													
J								Color Matching Images With Unknown Non-Linear Encodings	IEEE TRANSACTIONS ON IMAGE PROCESSING										Color stabilization; color matching; logarithmic encoded images; gamma-corrected images; HDR encoding; PQ; HLG	SCENE	We present a color matching method that deals with different non-linear encodings. In particular, given two different views of the same scene taken by two cameras with unknown settings and internal parameters, and encoded with unknown non-linear curves, our method is able to correct the colors of one of the images making it look as if it was captured under the other camera's settings. Our method is based on treating the in-camera color processing pipeline as a concatenation of a matrix multiplication on the linear image followed by a non-linearity. This allows us to model a color stabilization transformation among the two shots by estimating a single matrix -that will contain information from both of the original images- and an extra parameter that complies with the non-linearity. The method is fast and the results have no spurious colors. It outperforms the state-of-the-art both visually and according to several metrics, and can handle HDR encodings and very challenging real-life examples.																	1057-7149	1941-0042					2020	29						4435	4444		10.1109/TIP.2020.2968766													
J								Geometry Guided Pose-Invariant Facial Expression Recognition	IEEE TRANSACTIONS ON IMAGE PROCESSING										Facial expression recognition; facial image synthesis; generative adversarial network; facial landmarks	GAUSSIAN-PROCESSES; MULTIVIEW	Driven by recent advances in human-centered computing, Facial Expression Recognition (FER) has attracted significant attention in many applications. However, most conventional approaches either perform face frontalization on a non-frontal facial image or learn separate classifier for each pose. Different from existing methods, this paper proposes an end-to-end deep learning model that allows to simultaneous facial image synthesis and pose-invariant facial expression recognition by exploiting shape geometry of the face image. The proposed model is based on generative adversarial network (GAN) and enjoys several merits. First, given an input face and a target pose and expression designated by a set of facial landmarks, an identity-preserving face can be generated through guiding by the target pose and expression. Second, the identity representation is explicitly disentangled from both expression and pose variations through the shape geometry delivered by facial landmarks. Third, our model can automatically generate face images with different expressions and poses in a continuous way to enlarge and enrich the training set for the FER task. Our approach is demonstrated to perform well when compared with state-of-the-art algorithms on both controlled and in-the-wild benchmark datasets including Multi-PIE, BU-3DFE, and SFEW. The code is included in the supplementary material.																	1057-7149	1941-0042					2020	29						4445	4460		10.1109/TIP.2020.2972114													
J								Multi-Scale Temporal Cues Learning for Video Person Re-Identification	IEEE TRANSACTIONS ON IMAGE PROCESSING										Video Person ReID; Convolutional Neural Networks; Spatial Temporal Feature Learning	ATTENTION	Temporal cues embedded in videos provide important clues for person Re-Identification (ReID). To efficiently exploit temporal cues with a compact neural network, this work proposes a novel 3D convolution layer called Multi-scale 3D (M3D) convolution layer. The M3D layer is easy to implement and could be inserted into traditional 2D convolution networks to learn multi-scale temporal cues by end-to-end training. According to its inserted location, the M3D layer has two variants, i.e., local M3D layer and global M3D layer, respectively. The local M3D layer is inserted between 2D convolution layers to learn spatial-temporal cues among adjacent 2D feature maps. The global M3D layer is computed on adjacent frame feature vectors to learn their global temporal relations. The local and global M3D layers hence learn complementary temporal cues. Their combination introduces a fraction of parameters to traditional 2D CNN, but leads to the strong multi-scale temporal feature learning capability. The learned temporal feature is fused with a spatial feature to compose the final spatial-temporal representation for video person ReID. Evaluations on four widely used video person ReID datasets, i.e., MARS, DukeMTMC-VideoReID, PRID2011, and iLIDS-VID demonstrate the substantial advantages of our method over the state-of-the art. For example, it achieves rank1 accuracy of 88.63% on MARS without re-ranking. Our method also achieves a reasonable trade-off between ReID accuracy and model size, e.g., it saves about 40% parameters of I3D CNN.																	1057-7149	1941-0042					2020	29						4461	4473		10.1109/TIP.2020.2972108													
J								Learning a Deep Dual Attention Network for Video Super-Resolution	IEEE TRANSACTIONS ON IMAGE PROCESSING										Video super-resolution; motion compensation; detail components; attention mechanisms; high-frequency details	IMAGE SUPERRESOLUTION	Recently, deep learning based video super-resolution (SR) methods combine the convolutional neural networks (CNN) with motion compensation to estimate a high-resolution (HR) video from its low-resolution (LR) counterpart. However, most previous methods conduct downscaling motion estimation to handle large motions, which can lead to detrimental effects on the accuracy of motion estimation due to the reduction of spatial resolution. Besides, these methods usually treat different types of intermediate features equally, which lack flexibility to emphasize meaningful information for revealing the high-frequency details. In this paper, to solve above issues, we propose a deep dual attention network (DDAN), including a motion compensation network (MCNet) and a SR reconstruction network (ReconNet), to fully exploit the spatio-temporal informative features for accurate video SR. The MCNet progressively learns the optical flow representations to synthesize the motion information across adjacent frames in a pyramid fashion. To decrease the mis-registration errors caused by the optical flow based motion compensation, we extract the detail components of original LR neighboring frames as complementary information for accurate feature extraction. In the ReconNet, we implement dual attention mechanisms on a residual unit and form a residual attention unit to focus on the intermediate informative features for high-frequency details recovery. Extensive experimental results on numerous datasets demonstrate the proposed method can effectively achieve superior performance in terms of quantitative and qualitative assessments compared with state-of-the-art methods.																	1057-7149	1941-0042					2020	29						4474	4488		10.1109/TIP.2020.2972118													
J								A Novel Saliency Detection Algorithm Based on Adversarial Learning Model	IEEE TRANSACTIONS ON IMAGE PROCESSING										Adversarial learning model; discriminative models generative model; saliency detection		The traditional salient object detection models can be divided into several classes based on the low-level features of images and contrast between the pixels. This paper proposes an adversarial learning model (ALM) that includes the generative model and discriminative model. The ALM uses the original image as an input of the generative model to extract the high-level features and forms an initial salient map. Then, the discriminative model is utilized to compare differences in the features between the initial salient map and the ground truth, and the obtained differences are sent to the convolutional layers of the generative model to adjust the parameters for the generative model updating. Due to the serial-iterative adjustment, the salient map of the generative model becomes more similar to the ground truth. Lastly, the ALM forms the salient map fused with the super-pixels by enhancing the color and texture features, so the final salient map is obtained. The ALM is not limited to the color and texture features; on the contrary, it fuses multiple features and achieves good results in the salient target extraction. The experimental results show that ALM performs better than the other ten state-of-the-art models on three different datasets. Thus, the proposed ALM is widely applicable to the salient target extraction.																	1057-7149	1941-0042					2020	29						4489	4504		10.1109/TIP.2020.2972692													
J								Realizing a Low-Power Head-Mounted Phase-Only Holographic Display by Light-Weight Compression	IEEE TRANSACTIONS ON IMAGE PROCESSING										Holography; augmented reality; displays; wearable computers; data compression		Head-mounted holographic displays (HMHD) are projected to be the first commercial realization of holographic video display systems. HMHDs use liquid crystal on silicon (LCoS) spatial light modulators (SLM), which are best suited to display phase-only holograms (POH). The performance/watt requirement of a monochrome, 60 fps Full HD, 2-eye, POH HMHD system is about 10 TFLOPS/W, which is orders of magnitude higher than that is achievable by commercially available mobile processors. To mitigate this compute power constraint, display-ready POHs shall be generated on a nearby server and sent to the HMHD in compressed form over a wireless link. This paper discusses design of a feasible HMHD-based augmented reality system, focusing on compression requirements and per-pixel rate-distortion trade-off for transmission of display-ready POH from the server to HMHD. Since the decoder in the HMHD needs to operate on low power, only coding methods that have low-power decoder implementation are considered. Effects of 2D phase unwrapping and flat quantization on compression performance are also reported. We next propose a versatile PCM-POH codec with progressive quantization that can adapt to SLM-dynamic-range and available bitrate, and features per-pixel rate-distortion control to achieve acceptable POH quality at target rates of 60-200 Mbit/s that can be reliably achieved by current wireless technologies. Our results demonstrate feasibility of realizing a low-power, quality-ensured, multi-user, interactive HMHD augmented reality system with commercially available components using the proposed adaptive compression of display-ready POH with light-weight decoding.																	1057-7149	1941-0042					2020	29						4505	4515		10.1109/TIP.2020.2972112													
J								One-Pass Multi-Task Networks With Cross-Task Guided Attention for Brain Tumor Segmentation	IEEE TRANSACTIONS ON IMAGE PROCESSING										Brain tumor segmentation; magnetic resonance imaging; class imbalance; convolutional neural networks; multi-task learning; channel attention	NEURAL-NETWORKS; DEEP	Class imbalance has emerged as one of the major challenges for medical image segmentation. The model cascade (MC) strategy, a popular scheme, significantly alleviates the class imbalance issue via running a set of individual deep models for coarse-to-fine segmentation. Despite its outstanding performance, however, this method leads to undesired system complexity and also ignores the correlation among the models. To handle these flaws in the MC approach, we propose in this paper a light-weight deep model, i.e., the One-pass Multi-task Network (OM-Net) to solve class imbalance better than MC does, while requiring only one-pass computation for brain tumor segmentation. First, OM-Net integrates the separate segmentation tasks into one deep model, which consists of shared parameters to learn joint features, as well as task-specific parameters to learn discriminative features. Second, to more effectively optimize OM-Net, we take advantage of the correlation among tasks to design both an online training data transfer strategy and a curriculum learning-based training strategy. Third, we further propose sharing prediction results between tasks, which enables us to design a cross-task guided attention (CGA) module. By following the guidance of the prediction results provided by the previous task, CGA can adaptively recalibrate channel-wise feature responses based on the category-specific statistics. Finally, a simple yet effective post-processing method is introduced to refine the segmentation results of the proposed attention network. Extensive experiments are conducted to demonstrate the effectiveness of the proposed techniques. Most impressively, we achieve state-of-the-art performance on the BraTS 2015 testing set and BraTS 2017 online validation set. Using these proposed approaches, we also won joint third place in the BraTS 2018 challenge among 64 participating teams. The code is publicly available at https://github.com/chenhong-zhou/OM-Net.																	1057-7149	1941-0042					2020	29						4516	4529		10.1109/TIP.2020.2973510													
J								Second-Order Spectral Transform Block for 3D Shape Classification and Retrieval	IEEE TRANSACTIONS ON IMAGE PROCESSING										3D shape analysis; second-order pooling; spectral transform; shape representation	RECOGNITION; FRAMEWORK	In this paper, we propose a novel network block, dubbed as second-order spectral transform block, for 3D shape retrieval and classification. This network block generalizes the second-order pooling to 3D surface by designing a learnable non-linear transform on the spectrum of the pooled descriptor. The proposed block consists of following two components. First, the second-order average (SO-Avr) and max-pooling (SO-Max) operations are designed on 3D surface to aggregate local descriptors, which are shown to be more discriminative than the popular average-pooling or max-pooling. Second, a learnable spectral transform parameterized by mixture of power function is proposed to perform non-linear feature mapping in the space of pooled descriptors, i.e., manifold of symmetric positive definite matrix for SO-Avr, and space of symmetric matrix for SO-Max. The proposed block can be plugged into existing network architectures to aggregate local shape descriptors for boosting their performance. We apply it to a shallow network for non-rigid 3D shape analysis and to existing networks for rigid shape analysis, where it improves the first-tier retrieval accuracy by 7.2% on SHREC'14 Real dataset and achieves state-of-the-art classification accuracy on ModelNet40. As an extension, we apply our block to 2D image classification, showing its superiority compared with traditional second-order pooling methods. We also provide theoretical and experimental analysis on stability of the proposed second-order spectral transform block.																	1057-7149	1941-0042					2020	29						4530	4543		10.1109/TIP.2020.2967579													
J								Confidence Measure Guided Single Image De-Raining	IEEE TRANSACTIONS ON IMAGE PROCESSING										Deraining; convolutional neural networks; image restoration	STREAKS REMOVAL	Single image de-raining is an extremely challenging problem since the rainy images contain rain streaks which often vary in size, direction and density. This varying characteristic of rain streaks affect different parts of the image differently. Previous approaches have attempted to address this problem by leveraging some prior information to remove rain streaks from a single image. One of the major limitations of these approaches is that they do not consider the location information of rain drops in the image. We extend our previous work UMRL network, and propose Image Quality-based single image Deraining using Confidence measure (QuDeC), network addresses this issue by learning the quality or distortion level of each patch in the rainy image, and further processes this information to learn the rain content at different scales. In addition, we introduce a technique which guides the network to learn the network weights based on the confidence measure about the estimate of both quality at each location and residual rain streak information (residual map). Extensive experiments on synthetic and real datasets demonstrate that the proposed method achieves significant improvements over the recent state-of-the-art methods.																	1057-7149	1941-0042					2020	29						4544	4555		10.1109/TIP.2020.2973802													
J								Deep Multiphase Level Set for Scene Parsing	IEEE TRANSACTIONS ON IMAGE PROCESSING										Semantic scene parsing; multiphase level set; recurrent convolutional network; object boundary estimation	IMAGE SEGMENTATION; NETWORKS; MUMFORD; MODEL	Recently, Fully Convolutional Network (FCN) seems to be the go-to architecture for image segmentation, including semantic scene parsing. However, it is difficult for a generic FCN to predict semantic labels around the object boundaries, thus FCN-based methods usually produce parsing results with inaccurate boundaries. Meanwhile, many works have demonstrate that level set based active contours are superior to the boundary estimation in sub-pixel accuracy. However, they are quite sensitive to initial settings. To address these limitations, in this paper we propose a novel Deep Multiphase Level Set (DMLS) method for semantic scene parsing, which efficiently incorporates multiphase level sets into deep neural networks. The proposed method consists of three modules, i.e., recurrent FCNs, adaptive multiphase level set, and deeply supervised learning. More specifically, recurrent FCNs learn multi-level representations of input images with different contexts. Adaptive multiphase level set drives the discriminative contour for each semantic class, which makes use of the advantages of both global and local information. In each time-step of the recurrent FCNs, deeply supervised learning is incorporated for model training. Extensive experiments on three public benchmarks have shown that our proposed method achieves new state-of-the-art performances. The source codes will be released at https://github.com/Pchank/DMLS-for-SSP.																	1057-7149	1941-0042					2020	29						4556	4567		10.1109/TIP.2019.2957915													
J								Person Search by Separated Modeling and A Mask-Guided Two-Stream CNN Model	IEEE TRANSACTIONS ON IMAGE PROCESSING										Feature extraction; Streaming media; Task analysis; Image segmentation; Training; Detectors; Search problems; Person search; pedestrian detection; person re-identification		In this work, we tackle the problem of person search, which is a challenging task consisted of pedestrian detection and person re-identification (re-ID). Instead of sharing representations in a single joint model, we find that separating detector and re-ID feature extraction yields better performance. In order to extract more representative features for each identity, we segment out the foreground person from the original image patch. We propose a simple yet effective re-ID method, which models foreground person and original image patches individually, and obtains enriched representations from two separate CNN streams. We also propose a Confidence Weighted Stream Attention method which further re-adjusts the relative importance of the two streams by incorporating the detection confidence. Furthermore, we simplify the whole pipeline by incorporating semantic segmentation into the re-ID network, which is trained by bounding boxes as weakly-annotated masks and identification labels simultaneously. From the experiments on two standard person search benchmarks i.e. CUHK-SYSU and PRW, we achieve mAP of 83.3% and 32.8% respectively, surpassing the state of the art by a large margin. The extensive ablation study and model inspection further justifies our motivation.																	1057-7149	1941-0042					2020	29						4669	4682		10.1109/TIP.2020.2973513													
J								The Devil is in the Channels: Mutual-Channel Loss for Fine-Grained Image Classification	IEEE TRANSACTIONS ON IMAGE PROCESSING										Feature extraction; Training; Visualization; Automobiles; Task analysis; Data mining; Manuals; Fine-grained image classification; deep learning; loss function; mutual channel		The key to solving fine-grained image categorization is finding discriminate and local regions that correspond to subtle visual traits. Great strides have been made, with complex networks designed specifically to learn part-level discriminate feature representations. In this paper, we show that it is possible to cultivate subtle details without the need for overly complicated network designs or training mechanisms - a single loss is all it takes. The main trick lies with how we delve into individual feature channels early on, as opposed to the convention of starting from a consolidated feature map. The proposed loss function, termed as mutual-channel loss (MC-Loss), consists of two channel-specific components: a discriminality component and a diversity component. The discriminality component forces all feature channels belonging to the same class to be discriminative, through a novel channel-wise attention mechanism. The diversity component additionally constraints channels so that they become mutually exclusive across the spatial dimension. The end result is therefore a set of feature channels, each of which reflects different locally discriminative regions for a specific class. The MC-Loss can be trained end-to-end, without the need for any bounding-box/part annotations, and yields highly discriminative regions during inference. Experimental results show our MC-Loss when implemented on top of common base networks can achieve state-of-the-art performance on all four fine-grained categorization datasets (CUB-Birds, FGVC-Aircraft, Flowers-102, and Stanford Cars). Ablative studies further demonstrate the superiority of the MC-Loss when compared with other recently proposed general-purpose losses for visual classification, on two different base networks. Codes are available at: https://github.com/dongliangchang/Mutual-Channel-Loss.																	1057-7149	1941-0042					2020	29						4683	4695		10.1109/TIP.2020.2973812													
J								Residual Learning for Salient Object Detection	IEEE TRANSACTIONS ON IMAGE PROCESSING										Object detection; Feature extraction; Image reconstruction; Visualization; Task analysis; Image segmentation; Spatial resolution; Salient object detection; residual refinement; deep learning		Recent deep learning based salient object detection methods improve the performance by introducing multi-scale strategies into fully convolutional neural networks (FCNs). The final result is obtained by integrating all the predictions at each scale. However, the existing multi-scale based methods suffer from several problems: 1) it is difficult to directly learn discriminative features and filters to regress high-resolution saliency masks for each scale; 2) rescaling the multi-scale features could pull in many redundant and inaccurate values, and this weakens the representational ability of the network. In this paper, we propose a residual learning strategy and introduce to gradually refine the coarse prediction scale-by-scale. Concretely, instead of directly predicting the finest-resolution result at each scale, we learn to predict residuals to remedy the errors between coarse saliency map and scale-matching ground truth masks. We employ a Dilated Convolutional Pyramid Pooling (DCPP) module to generate the coarse prediction and guide the the residual learning process through several novel Attentional Residual Modules (ARMs). We name our network as Residual Refinement Network (R(2)Net). We demonstrate the effectiveness of the proposed method against other state-of-the-art algorithms on five released benchmark datasets. Our R(2)Net is a fully convolutional network which does not need any post-processing and achieves a real-time speed of 33 FPS when it is run on one GPU.																	1057-7149	1941-0042					2020	29						4696	4708		10.1109/TIP.2020.2975919													
J								Hyperspectral and Multispectral Image Fusion Using Optimized Twin Dictionaries	IEEE TRANSACTIONS ON IMAGE PROCESSING										Dictionaries; Hyperspectral imaging; Image fusion; Spatial resolution; Optimization; Bayes methods; Mathematical model; Hyperspectral image fusion; optimized twin dictionaries (OTD); spectral dictionary; spatial dictionary	SPARSE; ALGORITHM; SUPERRESOLUTION	Spectral or spatial dictionary has been widely used in fusing low-spatial-resolution hyperspectral (LH) images and high-spatial-resolution multispectral (HM) images. However, only using spectral dictionary is insufficient for preserving spatial information, and vice versa. To address this problem, a new LH and HM image fusion method termed OTD using optimized twin dictionaries is proposed in this paper. The fusion problem of OTD is formulated analytically in the framework of sparse representation, as an optimization of twin spectral-spatial dictionaries and their corresponding sparse coefficients. More specifically, the spectral dictionary representing the generalized spectrums and its spectral sparse coefficients are optimized by utilizing the observed LH and HM images in the spectral domain; and the spatial dictionary representing the spatial information and its spatial sparse coefficients are optimized by modeling the rest of high-frequency information in the spatial domain. In addition, without non-negative constraints, the alternating direction methods of multipliers (ADMM) are employed to implement the above optimization process. Comparison results with the related state-of-the-art fusion methods on various datasets demonstrate that our proposed OTD method achieves a better fusion performance in both spatial and spectral domains.																	1057-7149	1941-0042					2020	29						4709	4720		10.1109/TIP.2020.2968773													
J								Fusion of Heterogeneous Adversarial Networks for Single Image Dehazing	IEEE TRANSACTIONS ON IMAGE PROCESSING										Atmospheric modeling; Image color analysis; Training; Scattering; Estimation; Gallium nitride; Generative adversarial networks; Image dehazing; generative adversarial networks; fusion method		In this paper, we propose a novel image dehazing method. Typical deep learning models for dehazing are trained on paired synthetic indoor dataset. Therefore, these models may be effective for indoor image dehazing but less so for outdoor images. We propose a heterogeneous Generative Adversarial Networks (GAN) based method composed of a cycle-consistent Generative Adversarial Networks (CycleGAN) for producing haze-clear images and a conditional Generative Adversarial Networks (cGAN) for preserving textural details. We introduce a novel loss function in the training of the fused network to minimize GAN generated artifacts, to recover fine details, and to preserve color components. These networks are fused via a convolutional neural network (CNN) to generate dehazed image. Extensive experiments demonstrate that the proposed method significantly outperforms the state-of-the-art methods on both synthetic and real-world hazy images.																	1057-7149	1941-0042					2020	29						4721	4732		10.1109/TIP.2020.2975986													
J								MDLatLRR: A Novel Decomposition Method for Infrared and Visible Image Fusion	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image fusion; Task analysis; Transforms; Matrix decomposition; Sparse matrices; Feature extraction; Image decomposition; Image fusion; latent low-rank representation; multi-level decomposition; infrared image; visible image	SHEARLET TRANSFORM; FACE RECOGNITION; PERFORMANCE; INFORMATION	Image decomposition is crucial for many image processing tasks, as it allows to extract salient features from source images. A good image decomposition method could lead to a better performance, especially in image fusion tasks. We propose a multi-level image decomposition method based on latent low-rank representation(LatLRR), which is called MDLatLRR. This decomposition method is applicable to many image processing fields. In this paper, we focus on the image fusion task. We build a novel image fusion framework based on MDLatLRR which is used to decompose source images into detail parts(salient features) and base parts. A nuclear-norm based fusion strategy is used to fuse the detail parts and the base parts are fused by an averaging strategy. Compared with other state-of-the-art fusion methods, the proposed algorithm exhibits better fusion performance in both subjective and objective evaluation.																	1057-7149	1941-0042					2020	29						4733	4746		10.1109/TIP.2020.2975984													
J								A Joint Relationship Aware Neural Network for Single-Image 3D Human Pose Estimation	IEEE TRANSACTIONS ON IMAGE PROCESSING										3D human pose estimation; joint relationship; dual attention module		This paper studies the task of 3D human pose estimation from a single RGB image, which is challenging without depth information. Recently many deep learning methods are proposed and achieve great improvements due to their strong representation learning. However, most existing methods ignore the relationship between joint features. In this paper, a joint relationship aware neural network is proposed to take both global and local joint relationship into consideration. First, a whole feature block representing all human body joints is extracted by a convolutional neural network. A Dual Attention Module (DAM) is applied on the whole feature block to generate attention weights. By exploiting the attention module, the global relationship between the whole joints is encoded. Second, the weighted whole feature block is divided into some individual joint features. To capture salient joint feature, the individual joint features are refined by individual DAMs. Finally, a joint angle prediction constraint is proposed to consider local joint relationship. Quantitative and qualitative experiments on 3D human pose estimation benchmarks demonstrate the effectiveness of the proposed method.																	1057-7149	1941-0042					2020	29						4747	4758		10.1109/TIP.2020.2972104													
J								Improving the Harmony of the Composite Image by Spatial-Separated Attention Module	IEEE TRANSACTIONS ON IMAGE PROCESSING										Task analysis; Neural networks; Image color analysis; Convolution; Semantics; Gallium nitride; Image harmonization; image synthesis; attention mechanism		Image composition is one of the most important applications in image processing. However, the inharmonious appearance between the spliced region and background degrade the quality of the image. Thus, we address the problem of Image Harmonization: Given a spliced image and the mask of the spliced region, we try to harmonize the "style" of the pasted region with the background (non-spliced region). Previous approaches have been focusing on learning directly by the neural network. In this work, we start from an empirical observation: the differences can only be found in the spliced region between the spliced image and the harmonized result while they share the same semantic information and the appearance in the non-spliced region. Thus, in order to learn the feature map in the masked region and the others individually, we propose a novel attention module named Spatial-Separated Attention Module (S(2)AM). Furthermore, we design a novel image harmonization framework by inserting the S(2)AM in the coarser low-level features of the Unet structure by two different ways. Besides image harmonization, we make a big step for harmonizing the composite image without the specific mask under previous observation. The experiments show that the proposed S(2)AM performs better than other state-of-the-art attention modules in our task. Moreover, we demonstrate the advantages of our model against other state-of-the-art image harmonization methods via criteria from multiple points of view.																	1057-7149	1941-0042					2020	29						4759	4771		10.1109/TIP.2020.2975979													
J								Semi-Supervised Robust Mixture Models in RKHS for Abnormality Detection in Medical Images	IEEE TRANSACTIONS ON IMAGE PROCESSING										Robustness; Training; Semisupervised learning; Kernel; Data models; Image segmentation; Biomedical imaging; Abnormality detection; one-class classification; reproducing kernel Hilbert space; robust model; generalized multivariate Gaussian; mixture model; expectation maximization; semi-supervised learning	UNSUPERVISED TEXTURE SEGMENTATION; PRINCIPAL COMPONENT ANALYSIS; SUPPORT VECTOR MACHINES; CLASSIFICATION; ALGORITHM	Abnormality detection in medical images is a one-class classification problem for which existing methods typically involve variants of kernel principal component analysis or one-class support vector machines. However, existing methods rely on highly-curated training sets with full supervision, often using heuristics for model fitting or ignore the variances of the data within principal subspaces. In contrast, we propose novel methods that can work with imperfectly curated datasets using robust statistical learning, by extending the multivariate generalized-Gaussian distribution to a reproducing kernel Hilbert space (RKHS) and employing it within a mixture model. We propose a novel semi-supervised extension of our learning scheme, showing that a small amount of expert feedback through high-quality labeled data of the outlier class can boost performance. We propose expectation maximization for our semi-supervised robust mixture-model learning in RKHS, using solely the Gram matrix and without the explicit lifting map. Our methods incorporate optimal component means, principal directions, and variances for abnormality detection. Results on four large public datasets on retinopathy and cancer, compared against a variety of contemporary methods, show that our method gives benefits over the state of the art in one-class classification for abnormality detection.																	1057-7149	1941-0042					2020	29						4772	4787		10.1109/TIP.2020.2975958													
J								A Deep Multi-Modal Explanation Model for Zero-Shot Learning	IEEE TRANSACTIONS ON IMAGE PROCESSING										Visualization; Semantics; Task analysis; Training; Image color analysis; Head; Extraterrestrial phenomena; Zero-shot learning; multi-modal explanation; visual-attribute embedding; class activation map; LSTM		Zero-shot learning (ZSL) has attracted significant attention due to its capabilities of classifying new images from unseen classes. To perform the classification task for ZSL, learning visual and semantic embeddings has been the main research approach in existing literature. At the same time, generating complementary explanations to justify the classification decision has remained largely unexplored. In this paper, we propose to address a new and challenging task, namely explainable zero-shot learning (XZSL), which aims to generate visual and textual explanations to support the classification decision. To accomplish this task, we build a novel Deep Multi-modal Explanation (DME) model that incorporates a joint visual-attribute embedding module and a multi-channel explanation module in an end-to-end fashion. In contrast to existing ZSL approaches, our visual-attribute embedding is associated not only with the decision, but also with new visual and textual explanations. For visual explanations, we first capture several attribute activation maps (AAM) and then merge them into a class activation map (CAM) that visually infers which region of an image is relevant to the class. Textual explanations are generated from the multi-channel explanation module, jointly integrating three long short-term memory models (LSTMs) each of which is conditioned on a different feature representation. Additionally, we suggest that the DME model can retain explanatory consistency for similar instances and explanatory diversity for diverse instances. We conduct qualitative and quantitative experiments to assess the model for ZSL classification and explanation. Specifically, the ablation studies verify the effectiveness of the components in our model. Our results on three well-known datasets are competitive with prior approaches. More importantly, the joint training of our embedding and explanation modules demonstrates mutual performance improvements between ZSL classification and explanation. We shed more light on DME to analyze and diagnose its advantages and limitations.																	1057-7149	1941-0042					2020	29						4788	4803		10.1109/TIP.2020.2975980													
J								Principal Component Adversarial Example	IEEE TRANSACTIONS ON IMAGE PROCESSING										Manifolds; Neural networks; Perturbation methods; Distortion; Task analysis; Robustness; Principal component analysis; Deep learning; adversarial examples; classification; manifold learning	NEURAL-NETWORKS; DEEP; REPRESENTATION; ROBUSTNESS	Despite having achieved excellent performance on various tasks, deep neural networks have been shown to be susceptible to adversarial examples, i.e., visual inputs crafted with structural imperceptible noise. To explain this phenomenon, previous works implicate the weak capability of the classification models and the difficulty of the classification tasks. These explanations appear to account for some of the empirical observations but lack deep insight into the intrinsic nature of adversarial examples, such as the generation method and transferability. Furthermore, previous works generate adversarial examples completely rely on a specific classifier (model). Consequently, the attack ability of adversarial examples is strongly dependent on the specific classifier. More importantly, adversarial examples cannot be generated without a trained classifier. In this paper, we raise a question: what is the real cause of the generation of adversarial examples? To answer this question, we propose a new concept, called the adversarial region, which explains the existence of adversarial examples as perturbations perpendicular to the tangent plane of the data manifold. This view yields a clear explanation of the transfer property across different models of adversarial examples. Moreover, with the notion of the adversarial region, we propose a novel target-free method to generate adversarial examples via principal component analysis. We verify our adversarial region hypothesis on a synthetic dataset and demonstrate through extensive experiments on real datasets that the adversarial examples generated by our method have competitive or even strong transferability compared with model-dependent adversarial example generating methods. Moreover, our experiment shows that the proposed method is more robust to defensive methods than previous methods.																	1057-7149	1941-0042					2020	29						4804	4815		10.1109/TIP.2020.2975918													
J								DRPL: Deep Regression Pair Learning for Multi-Focus Image Fusion	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image fusion; Image edge detection; Transforms; Machine learning; Electronic mail; Convolution; Convolutional neural networks; Multi-focus; image fusion; deep learning; pair learning; regression	PERFORMANCE; ENHANCEMENT; NETWORK; CNN	In this paper, a novel deep network is proposed for multi-focus image fusion, named Deep Regression Pair Learning (DRPL). In contrast to existing deep fusion methods which divide the input image into small patches and apply a classifier to judge whether the patch is in focus or not, DRPL directly converts the whole image into a binary mask without any patch operation, subsequently tackling the difficulty of the blur level estimation around the focused/defocused boundary. Simultaneously, a pair learning strategy, which takes a pair of complementary source images as inputs and generates two corresponding binary masks, is introduced into the model, greatly imposing the complementary constraint on each pair and making a large contribution to the performance improvement. Furthermore, as the edge or gradient does exist in the focus part while there is no similar property for the defocus part, we also embed a gradient loss to ensure the generated image to be all-in-focus. Then the structural similarity index (SSIM) is utilized to make a trade-off between the reference and fused images. Experimental results conducted on the synthetic and real-world datasets substantiate the effectiveness and superiority of DRPL compared with other state-of-the-art approaches. The source code can be found in https://github.com/sasky1/DPRL.																	1057-7149	1941-0042					2020	29						4816	4831		10.1109/TIP.2020.2976190													
J								Lower Bound on Transmission Using Non-Linear Bounding Function in Single Image Dehazing	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image color analysis; Meteorology; Atmospheric modeling; Estimation; Image edge detection; Scattering; Visualization; Atmospheric scattering; dark channel prior (DCP); defogging; dehazing; transmission; visibility; noise removal	FRAMEWORK	The visibility of an image captured in poor weather (such as haze, fog, mist, smog) degrades due to scattering of light by atmospheric particles. Single image dehazing (SID) methods are used to restore visibility from a single hazy image. The SID is a challenging problem due to its ill-posed nature. Typically, the atmospheric scattering model (ATSM) is used to solve SID problem. The transmission and atmospheric light are two prime parameters of ATSM. The accuracy and effectiveness of SID depends on accurate value of transmission and atmospheric light. The proposed method translates transmission estimation problem into estimation of the difference between minimum color channel of hazy and haze-free image. The translated problem presents a lower bound on transmission and is used to minimize reconstruction error in dehazing. The lower bound depends upon the bounding function (BF) and a quality control parameter. A non-linear model is then proposed to estimate BF for accurate estimation of transmission. The proposed quality control parameter can be utilized to tune the effect of dehazing. The accuracy obtained by the proposed method for transmission is compared with state of the art dehazing methods. Visual comparison of dehazed images and objective evaluation further validates the effectiveness of the proposed method.																	1057-7149	1941-0042					2020	29						4832	4847		10.1109/TIP.2020.2975909													
J								Temporal Incoherence-Free Video Retargeting Using Foreground Aware Extrapolation	IEEE TRANSACTIONS ON IMAGE PROCESSING										Streaming media; Coherence; Extrapolation; Distortion; Strain; Two dimensional displays; Computational complexity; Video retargeting; MAP-based block matching; fallback; extrapolation		Video retargeting is a method of adjusting the aspect ratio of a given video to the target aspect ratio. However, temporal incoherence of video contents, which can occur frequently by video retargeting, is the most dominant factor that degrades the quality of retargeted videos. Current methods to maintain temporal coherence use the entire frames of the input videos; however, these methods cannot be implemented as on-time systems because of their tremendous computational complexity. As far as we know, there is no existing on-time video retargeting method that can avoid spatial distortion while perfectly maintaining temporal coherence. In this paper, we propose a novel on-time video retargeting method that can perfectly maintain temporal coherence and prevent the spatial distortion by using only two consecutive input frames. In our method, the maximum a posteriori-based foreground aware-block matching is used for the extrapolation that extends the side area of a given video to adjust its aspect ratio to the target. To maintain the temporal coherence of the extended area, the result of block matching for backward warping-based extrapolation of the start frame after the scene change occurs, is reused for the other frames until the next scene change occurs. In addition, we propose a scene scenario-adaptive fallback scheme to prevent severe distortions that can occur with reusing block matching results or extrapolation-based side extension. The simulation results showed that the proposed method greatly improved the bidirectional similarity value, which can measure the quality of video retargeting, by up to 10.26 compared with the existing on-time video retargeting methods.																	1057-7149	1941-0042					2020	29						4848	4861		10.1109/TIP.2020.2977171													
J								PhaseNet 2.0: Phase Unwrapping of Noisy Data Based on Deep Learning Approach	IEEE TRANSACTIONS ON IMAGE PROCESSING										Phase unwrapping; semantic segmentation; deep learning; deep convolutional neural network (DCNN); fringe projection	FRINGE PROJECTION PROFILOMETRY; FOURIER-TRANSFORM; ALGORITHM; NETWORK	Phase unwrapping is an ill-posed classical problem in many practical applications of significance such as 3D profiling through fringe projection, synthetic aperture radar and magnetic resonance imaging. Conventional phase unwrapping techniques estimate the phase either by integrating through the confined path (referred to as path-following methods) or by minimizing the energy function between the wrapped phase and the approximated true phase (referred to as minimum-norm approaches). However, these conventional methods have some critical challenges like error accumulation and high computational time and often fail under low SNR conditions. To address these problems, this paper proposes a novel deep learning framework for unwrapping the phase and is referred to as "PhaseNet 2.0". The phase unwrapping problem is formulated as a dense classification problem and a fully convolutional DenseNet based neural network is trained to predict the wrap-count at each pixel from the wrapped phase maps. To train this network, we simulate arbitrary shapes and propose new loss function that integrates the residues by minimizing the difference of gradients and also uses loss to overcome class imbalance problem. The proposed method, unlike our previous approach PhaseNet, does not require post-processing, highly robust to noise, accurately unwraps the phase even at the severe noise level of -5 dB, and can unwrap the phase maps even at relatively high dynamic ranges. Simulation results from the proposed framework are compared with different classes of existing phase unwrapping methods for varying SNR values and discontinuity, and these evaluations demonstrate the advantages of the proposed framework. We also demonstrate the generality of the proposed method on 3D reconstruction of synthetic CAD models that have diverse structures and finer geometric variations. Finally, the proposed method is applied to real-data for 3D profiling of objects using fringe projection technique and digital holographic interferometry. The proposed framework achieves significant improvements over existing methods while being highly efficient with interactive frame-rates on modern GPUs.																	1057-7149	1941-0042					2020	29						4862	4872		10.1109/TIP.2020.2977213													
J								ICNet: Information Conversion Network for RGB-D Based Salient Object Detection	IEEE TRANSACTIONS ON IMAGE PROCESSING										Feature extraction; Correlation; Fuses; Decoding; Object detection; Visualization; Convolution; RGB-D based salient object detection; information conversion; cross-modal depth-weighted combination; siamese structure	FUSION	RGB-D based salient object detection (SOD) methods leverage the depth map as a valuable complementary information for better SOD performance. Previous methods mainly resort to exploit the correlation between RGB image and depth map in three fusion domains: input images, extracted features, and output results. However, these fusion strategies cannot fully capture the complex correlation between the RGB image and depth map. Besides, these methods do not fully explore the cross-modal complementarity and the cross-level continuity of information, and treat information from different sources without discrimination. In this paper, to address these problems, we propose a novel Information Conversion Network (ICNet) for RGB-D based SOD by employing the siamese structure with encoder-decoder architecture. To fuse high-level RGB and depth features in an interactive and adaptive way, we propose a novel Information Conversion Module (ICM), which contains concatenation operations and correlation layers. Furthermore, we design a Cross-modal Depth-weighted Combination (CDC) block to discriminate the cross-modal features from different sources and to enhance RGB features with depth features at each level. Extensive experiments on five commonly tested datasets demonstrate the superiority of our ICNet over 15 state-of-the-art RGB-D based SOD methods, and validate the effectiveness of the proposed ICM and CDC block.																	1057-7149	1941-0042					2020	29						4873	4884		10.1109/TIP.2020.2976689													
J								Blind Universal Bayesian Image Denoising With Gaussian Noise Level Learning	IEEE TRANSACTIONS ON IMAGE PROCESSING										Additive Gaussian noise removal; Bayesian estimation theory; deep learning; CNN image denoiser optimality	SPARSE; REPRESENTATION; TRANSFORM	Blind and universal image denoising consists of using a unique model that denoises images with any level of noise. It is especially practical as noise levels do not need to be known when the model is developed or at test time. We propose a theoretically-grounded blind and universal deep learning image denoiser for additive Gaussian noise removal. Our network is based on an optimal denoising solution, which we call fusion denoising. It is derived theoretically with a Gaussian image prior assumption. Synthetic experiments show our network's generalization strength to unseen additive noise levels. We also adapt the fusion denoising network architecture for image denoising on real images. Our approach improves real-world grayscale additive image denoising PSNR results for training noise levels and further on noise levels not seen during training. It also improves state-of-the-art color image denoising performance on every single noise level, by an average of $0.1dB$ , whether trained on or not.																	1057-7149	1941-0042					2020	29						4885	4897		10.1109/TIP.2020.2976814													
J								A Spatially Constrained Probabilistic Model for Robust Image Segmentation	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image segmentation; Hidden Markov models; Brain modeling; Probabilistic logic; Estimation; Labeling; Robustness; Segmentation; expectation-maximization; clique potential; hidden Markov random field; class label distribution	GAUSSIAN MIXTURE MODEL; BIAS FIELD ESTIMATION; C-MEANS ALGORITHM; MR-IMAGES	In general, the hidden Markov random field (HMRF) represents the class label distribution of an image in probabilistic model based segmentation. The class label distributions provided by existing HMRF models consider either the number of neighboring pixels with similar class labels or the spatial distance of neighboring pixels with dissimilar class labels. Also, this spatial information is only considered for estimation of class labels of the image pixels, while its contribution in parameter estimation is completely ignored. This, in turn, deteriorates the parameter estimation, resulting in sub-optimal segmentation performance. Moreover, the existing models assign equal weightage to the spatial information for class label estimation of all pixels throughout the image, which, create significant misclassification for the pixels in boundary region of image classes. In this regard, the paper develops a new clique potential function and a new class label distribution, incorporating the information of image class parameters. Unlike existing HMRF model based segmentation techniques, the proposed framework introduces a new scaling parameter that adaptively measures the contribution of spatial information for class label estimation of image pixels. The importance of the proposed framework is depicted by modifying the HMRF based segmentation methods. The advantage of proposed class label distribution is also demonstrated irrespective of the underlying intensity distributions. The comparative performance of the proposed and existing class label distributions in HMRF model is demonstrated both qualitatively and quantitatively for brain MR image segmentation, HEp-2 cell delineation, natural image and object segmentation.																	1057-7149	1941-0042					2020	29						4898	4910		10.1109/TIP.2020.2975717													
J								A Multiple-Instance Densely-Connected ConvNet for Aerial Scene Classification	IEEE TRANSACTIONS ON IMAGE PROCESSING										Feature extraction; Semantics; Machine learning; Task analysis; Training; Neural networks; Visualization; Scene classification; convolutional neural network; multiple instance learning; dense connection; aerial image	CONVOLUTIONAL NEURAL-NETWORKS; IMAGE; SCALE; EFFICIENT; FEATURES; FUSION; MODEL; COLOR	In contrast with nature scenes, aerial scenes are often composed of many objects crowdedly distributed on the surface in bird's view, the description of which usually demands more discriminative features as well as local semantics. However, when applied to scene classification, most of the existing convolution neural networks (ConvNets) tend to depict global semantics of images, and the loss of low- and mid-level features can hardly be avoided, especially when the model goes deeper. To tackle these challenges, in this paper, we propose a multiple-instance densely-connected ConvNet (MIDC-Net) for aerial scene classification. It regards aerial scene classification as a multiple-instance learning problem so that local semantics can be further investigated. Our classification model consists of an instance-level classifier, a multiple instance pooling and followed by a bag-level classification layer. In the instance-level classifier, we propose a simplified dense connection structure to effectively preserve features from different levels. The extracted convolution features are further converted into instance feature vectors. Then, we propose a trainable attention-based multiple instance pooling. It highlights the local semantics relevant to the scene label and outputs the bag-level probability directly. Finally, with our bag-level classification layer, this multiple instance learning framework is under the direct supervision of bag labels. Experiments on three widely-utilized aerial scene benchmarks demonstrate that our proposed method outperforms many state-of-the-art methods by a large margin with much fewer parameters.																	1057-7149	1941-0042					2020	29						4911	4926		10.1109/TIP.2020.2975718													
J								Understanding and Predicting the Memorability of Outdoor Natural Scenes	IEEE TRANSACTIONS ON IMAGE PROCESSING										Databases; Predictive models; Visualization; Analytical models; Face; Feature extraction; Correlation; Memorability; outdoor natural scenes; computer vision	IMAGE	Memorability measures how easily an image is to be memorized after glancing, which may contribute to designing magazine covers, tourism publicity materials, and so forth. Recent works have shed light on the visual features that make generic images, object images or face photographs memorable. However, these methods are not able to effectively predict the memorability of outdoor natural scene images. To overcome this shortcoming of previous works, in this paper, we provide an attempt to answer: "what exactly makes outdoor natural scenes memorable". To this end, we first establish a large-scale outdoor natural scene image memorability (LNSIM) database, containing 2,632 outdoor natural scene images with their ground truth memorability scores and the multi-label scene category annotations. Then, similar to previous works, we mine our database to investigate how low-, middle- and high-level handcrafted features affect the memorability of outdoor natural scenes. In particular, we find that the high-level feature of scene category is rather correlated with outdoor natural scene memorability, and the deep features learnt by deep neural network (DNN) are also effective in predicting the memorability scores. Moreover, combining the deep features with the category feature can further boost the performance of memorability prediction. Therefore, we propose an end-to-end DNN based outdoor natural scene memorability (DeepNSM) predictor, which takes advantage of the learned category-related features. Then, the experimental results validate the effectiveness of our DeepNSM model, exceeding the state-of-the-art methods. Finally, we try to understand the reason of the good performance for our DeepNSM model, and also study the cases that our DeepNSM model succeeds or fails to accurately predict the memorability of outdoor natural scenes.																	1057-7149	1941-0042					2020	29						4927	4941		10.1109/TIP.2020.2975957													
J								Gaze Estimation by Exploring Two-Eye Asymmetry	IEEE TRANSACTIONS ON IMAGE PROCESSING										Gaze estimation; asymmetric regression; evaluation network; eye appearance	TRACKING TECHNIQUES; APPEARANCE; PREDICTION	Eye gaze estimation is increasingly demanded by recent intelligent systems to facilitate a range of interactive applications. Unfortunately, learning the highly complicated regression from a single eye image to the gaze direction is not trivial. Thus, the problem is yet to be solved efficiently. Inspired by the two-eye asymmetry as two eyes of the same person may appear uneven, we propose the face-based asymmetric regression-evaluation network (FARE-Net) to optimize the gaze estimation results by considering the difference between left and right eyes. The proposed method includes one face-based asymmetric regression network (FAR-Net) and one evaluation network (E-Net). The FAR-Net predicts 3D gaze directions for both eyes and is trained with the asymmetric mechanism, which asymmetrically weights and sums the loss generated by two-eye gaze directions. With the asymmetric mechanism, the FAR-Net utilizes the eyes that can achieve high performance to optimize network. The E-Net learns the reliabilities of two eyes to balance the learning of the asymmetric mechanism and symmetric mechanism. Our FARE-Net achieves leading performances on MPIIGaze, EyeDiap and RT-Gene datasets. Additionally, we investigate the effectiveness of FARE-Net by analyzing the distribution of errors and ablation study.																	1057-7149	1941-0042					2020	29						5259	5272		10.1109/TIP.2020.2982828													
J								Dynamic Scene Deblurring by Depth Guided Model	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image deblurring; deep learning; scene depth	CAMERA SHAKE; IMAGES	Dynamic scene blur is usually caused by object motion, depth variation as well as camera shake. Most existing methods usually solve this problem using image segmentation or fully end-to-end trainable deep convolutional neural networks by considering different object motions or camera shakes. However, these algorithms are less effective when there exist depth variations. In this work, we propose a deep neural convolutional network that exploits the depth map for dynamic scene deblurring. Given a blurred image, we first extract the depth map and adopt a depth refinement network to restore the edges and structure in the depth map. To effectively exploit the depth map, we adopt the spatial feature transform layer to extract depth features and fuse with the image features through scaling and shifting. Our image deblurring network thus learns to restore a clear image under the guidance of the depth map. With substantial experiments and analysis, we show that the depth information is crucial to the performance of the proposed model. Finally, extensive quantitative and qualitative evaluations demonstrate that the proposed model performs favorably against the state-of-the-art dynamic scene deblurring approaches as well as conventional depth-based deblurring algorithms.																	1057-7149	1941-0042					2020	29						5273	5288		10.1109/TIP.2020.2980173													
J								Similarity-Preserving Linkage Hashing for Online Image Retrieval	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image retrieval; online hashing; binary codes; similarity preservation	QUANTIZATION; FACE	Online image hashing aims to update hash functions on-the-fly along with newly arriving data streams, which has found broad applications in computer vision and beyond. To this end, most existing methods update hash functions simply using discrete labels or pairwise similarity to explore intra-class relationships, which, however, often deteriorates search performance when facing a domain gap or semantic shift. One reason is that they ignore the particular semantic relationships among different classes, which should be taken into account in updating hash functions. Besides, the common characteristics between the label vectors (can be regarded as a sort of binary codes) and to-be-learned binary hash codes have left unexploited. In this paper, we present a novel online hashing method, termed Similarity Preserving Linkage Hashing (SPLH), which not only utilizes pairwise similarity to learn the intra-class relationships, but also fully exploits a latent linkage space to capture the inter-class relationships and the common characteristics between label vectors and to-be-learned hash codes. Specifically, SPLH first maps the independent discrete label vectors and binary hash codes into a linkage space, through which the relative semantic distance between data points can be assessed precisely. As a result, the pairwise similarities within the newly arriving data stream are exploited to learn the latent semantic space to benefit binary code learning. To learn the model parameters effectively, we further propose an alternating optimization algorithm. Extensive experiments conducted on three widely-used datasets demonstrate the superior performance of SPLH over several state-of-the-art online hashing methods.																	1057-7149	1941-0042					2020	29						5289	5300		10.1109/TIP.2020.2981879													
J								Outdoor RGBD Instance Segmentation With Residual Regretting Learning	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image segmentation; Feature extraction; Proposals; Semantics; Robot sensing systems; Robustness; Electronic mail; RGBD instance segmentation; residual regretting		Indoor semantic segmentation with RGBD input has received decent progress recently, but studies on instance-level objects in outdoor scenarios meet challenges due to the ambiguity in the acquired outdoor depth map. To tackle this problem, we proposed a residual regretting mechanism, incorporated into current flexible, general and solid instance segmentation framework Mask R-CNN in an end-to-end manner. Specifically, regretting cascade is designed to gradually refine and fully unearth useful information in depth maps, acting in a filtering and backup way. Additionally, embedded by a novel residual connection structure, the regretting module combines RGB and depth branches with pixel-level mask robustly. Extensive experiments on the challenging Cityscapes and KITTI dataset manifest the effectiveness of our residual regretting scheme for handling outdoor depth map. Our approach achieves state-of-the-art performance on RGBD instance segmentation, with 13.4% relative improvement over Mask R-CNN on Cityscapes by depth cue.																	1057-7149	1941-0042					2020	29						5301	5309		10.1109/TIP.2020.2975711													
J								Image Recovery via Transform Learning and Low-Rank Modeling: The Power of Complementary Regularizers	IEEE TRANSACTIONS ON IMAGE PROCESSING										Sparse representation; image denoising; image inpainting; image reconstruction; block matching; collaborative filtering; machine learning	SPARSE REPRESENTATION; SPARSIFYING TRANSFORMS; CONVERGENCE GUARANTEES; NONLOCAL IMAGE; RECONSTRUCTION; APPROXIMATION; RESTORATION; ALGORITHMS; FRAMEWORK; DOMAIN	Recent works on adaptive sparse and on low-rank signal modeling have demonstrated their usefulness in various image/video processing applications. Patch-based methods exploit local patch sparsity, whereas other works apply low-rankness of grouped patches to exploit image non-local structures. However, using either approach alone usually limits performance in image reconstruction or recovery applications. In this work, we propose a simultaneous sparsity and low-rank model, dubbed STROLLR, to better represent natural images. In order to fully utilize both the local and non-local image properties, we develop an image restoration framework using a transform learning scheme with joint low-rank regularization. The approach owes some of its computational efficiency and good performance to the use of transform learning for adaptive sparse representation rather than the popular synthesis dictionary learning algorithms, which involve approximation of NP-hard sparse coding and expensive learning steps. We demonstrate the proposed framework in various applications to image denoising, inpainting, and compressed sensing based magnetic resonance imaging. Results show promising performance compared to state-of-the-art competing methods.																	1057-7149	1941-0042					2020	29						5310	5323		10.1109/TIP.2020.2980753													
J								Attribute-Guided Attention for Referring Expression Generation and Comprehension	IEEE TRANSACTIONS ON IMAGE PROCESSING										Referring expression; generation; comprehension; attributes; attribute-guided attention		Referring expression is a special kind of verbal expression. The goal of referring expression is to refer to a particular object in some scenarios. Referring expression generation and comprehension are two inverse tasks within the field. Considering the critical role that visual attributes play in distinguishing the referred object from other objects, we propose an attribute-guided attention model to address the two tasks. In our proposed framework, attributes collected from referring expressions are used as explicit supervision signals on the generation and comprehension modules. The online predicted attributes of the visual object can benefit both tasks in two aspects: First, attributes can be directly embedded into the generation and comprehension modules, distinguishing the referred object as additional visual representations. Second, since attributes have their correspondence in both visual and textual space, an attribute-guided attention module is proposed as a bridging part to link the counterparts in visual representation and textual expression. Attention weights learned on both visual feature and word embeddings validate our motivation. We experiment on three standard datasets of RefCOCO, RefCOCO & x002B; and RefCOCOg commonly used in this field. Both quantitative and qualitative results demonstrate the effectiveness of our proposed framework. The experimental results show significant improvements over baseline methods, and are favorably comparable to the state-of-the-art results. Further ablation study and analysis clearly demonstrate the contribution of each module, which could provide useful inspirations to the community.																	1057-7149	1941-0042					2020	29						5244	5258		10.1109/TIP.2020.2979010													
J								Cross-Weather Image Alignment via Latent Generative Model With Intensity Consistency	IEEE TRANSACTIONS ON IMAGE PROCESSING										Roads; Gallium nitride; Meteorology; Generative adversarial networks; Manifolds; Task analysis; Computer vision; Cross-weather road scene alignment; latent image manifold; intensity constancy; image registration		Image alignment/registration/correspondence is a critical prerequisite for many vision-based tasks, and it has been widely studied in computer vision. However, aligning images from different domains, such as cross-weather/season road scenes, remains a challenging problem. Inspired by the success of classic intensity-constancy-based image alignment methods and the modern generative adversarial network (GAN) technology, we propose a cross-weather road scene alignment method called latent generative model with intensity constancy. From a novel perspective, the alignment problem is formulated as a constrained 2D flow optimization problem with latent encoding, which can be decoded into an intensity-constancy image on the latent image manifold. The manifold is parameterized by a pre-trained GAN, which is able to capture statistic characteristics from large datasets. Moreover, we employ the learned manifold to constrain the warped latent image identical to the target image, thereby producing a realistic warping effect. Experimental results on several cross-weather/season road scene datasets demonstrate that our approach can significantly outperform the state-of-the-art methods.																	1057-7149	1941-0042					2020	29						5216	5228		10.1109/TIP.2020.2980210													
J								Adaptive blind equalization for a MIMO chaotic communication system	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Adaptive equalizers; blind equalizers; chaotic communication; MIMO equalization	ENCRYPTION SYSTEMS; ALGORITHMS	There exist few blind solutions for chaotic MIMO channel equalization. In this work, a chaotic MIMO channel equalization framework is proposed. The objective function to be minimized in the proposed solution is obtained by adopting the objective function developed for chaotic SISO channel equalization. Furthermore, an optimum filter that minimizes the proposed cost function is designed to recover chaotic input signals assuming that the channel is known. The stationary point of the adaptive solution is equal to the optimal filter if the adaptive filter coefficients change sufficiently slowly. The adaptive solution is contrasted with the optimum filter in terms of mean-square error and bit error rate performances. In addition, the proposed solution reconstructs chaotic input signals at the same time. Consequently, it can be applied to multiple signal separation problems as well.																	1300-0632	1303-6203					2020	28	2					591	605		10.3906/elk-1810-46													
J								A novel chaos-based modulation scheme: adaptive threshold level chaotic on-off keying for increased BER performance	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Direct chaotic communication system; field programmable analogue arrays; bit error rate	CELLULAR NEURAL-NETWORKS; SYNCHRONIZATION	A novel modulation scheme called adaptive threshold level-chaotic on-off keying (ATL-COOK) is proposed. This scheme is applied to direct chaotic communication (DCC) systems where the chaotic signals are used as carrier signals. The objective of the proposed adaptive method is to increase the low BER versus SNR performance caused by the constant threshold voltage level. In the proposed method, the communication signal received by the receiver circuits was defined as a Dirac delta function and a comparison signal was obtained from this signal. Then the BER versus SNR performance was analyzed and compared with that of various chaotic generator structures by using an instantaneous adaptive signal instead of a constant threshold voltage in the receiver circuit decision block. Both the simulation results and the experiments show the efficiency of the proposed method.																	1300-0632	1303-6203					2020	28	2					606	620		10.3906/elk-1904-186													
J								Cooperative communications with optimal harvesting duration for Nakagami fading channels	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Energy harvesting; relaying techniques; AF and DF relaying	OUTAGE PERFORMANCE ANALYSIS; RELAYING PROTOCOLS; ENERGY; NETWORK; SELECTION; SYSTEM	In this paper, we analyze the throughput of cooperative communications with wireless energy harvesting. Relay nodes harvest energy from Radio Frequency (RF) signal transmitted by the source. We derive the packet error probability as well as the throughput for Nakagami fading channels. We also suggest to enhance the throughput by choosing the value of harvesting duration. Our results are valid for both Amplify and Forward (AF) and Decode and Forward (DF) relaying.																	1300-0632	1303-6203					2020	28	2					621	+		10.3906/elk-1904-30													
J								Rule extraction and performance estimation by using variable neighborhood search for solar power plant in Konya	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Variable neighborhood search; renewable sources; artificial neural network	NEURAL-NETWORK	The use of renewable energy sources in the production of electricity has become inevitable in order to reduce the greenhouse gases left in the atmosphere that cause the Earth to warm up. Although countries on a national basis have implemented a number of policies to support electricity generated from renewable energy sources, investments to produce electricity without a license on a local basis are not desirable. Those who want to invest medium and small scale for the most reason expect that this work will be supported by real data. Although the electricity generated by renewable investments is generated by simulation data, these data are not realistic for such investors. In this study, the climatic conditions of the power plant of 1 MW installed in Konya and power plant production data are monitored. The artificial neural network (ANN) can achieve a high value for accuracy, but these values are sometimes complex and unclear. In the literature, a number of studies have been conducted using different methods to overcome such problems. Real-time solar power plant (SPP) data were used to determine the feasibility and success of the proposed method. The variable neighborhood search (VNS) metaheuristic method was used to acquire the optimal values belonging to input vectors, G(h), which were maximized to the value of the fitness function F-s belonging to output class node s. The results obtained by the VNS method showed that the proposed method has the potential to produce the correct rules. Generally, energy investors are curious about the return on their investment. It is very important for energy providers to estimate how much electricity will be generated from existing solar power plants and accordingly determine the measures they will take to meet the electricity demand in the future. In this study, the performance estimation value obtained from the solar power plant depending on the weather conditions was obtained with 95.55% accuracy.																	1300-0632	1303-6203					2020	28	2					635	645		10.3906/elk-1901-232													
J								Towards human activity recognition for ubiquitous health care using data from a waist-mounted smartphone	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Health care; activity recognition; feature selection; multiclass support vector machine; radial basis function; loose grid search; linear kernel	FEATURE-SELECTION; PHYSICAL-ACTIVITY; TRIAXIAL ACCELEROMETER; CLASSIFICATION; SENSOR; SYSTEM	Understanding human activities is a newly emerging paradigm that is greatly involved in developing ubiquitous health care (u-Health) systems. The aim of these systems is to seamlessly gather knowledge about the patient's health and, after collecting knowledge, make suggestions to the patient according to his/her health profile. For this purpose, one of the most important ubiquitous communication trends is the smartphone, which has drawn the attention of both professionals and caregivers for monitoring the aging population, childcare, fall detection, and cognitive impairment. Recognizing human actions in a ubiquitous environment is very challenging and researchers have extensively investigated different methods to recognize human activities in the past decade. However, this field of research still needs further exploration in order to improve the accuracy and reduce the computational cost of these health care systems. Therefore, for expediting the existing system, this research work investigated a novel approach based on feature selection and classification. In the proposed work, sparse Bayesian multinomial logistic regression (SBMLR) is used for feature subset selection and a multiclass support vector machine (SVM) is adapted for the classification of six human daily activities (laying down, walking up stairs, walking down stairs, sitting, standing, and walking). For identifying the best features among the features returned by the SBMLR, a tuned threshold value is used for the selection of the features. Further, other classification algorithms including K-nearest neighbor, decision tree, and naive Bayes and different feature selection methods such as principal component analysis and random subset feature selection are also used for evaluation and comparison. The dataset used for testing is obtained from the UCI Machine Learning Repository. It is collected by using a smartphone embedded with an accelerometer and gyroscope. The experimental results show that the highest accuracy of 99.40% can be achieved by using the proposed method. Moreover, the paired sample two-tailed t-test over the significance level of 0.05 reveals that the performance difference between the proposed technique and a competing technique is statistically significant.																	1300-0632	1303-6203					2020	28	2					646	663		10.3906/elk-1901-31													
J								Geographic variation and ethnicity in diabetic retinopathy detection via deep learning	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Deep learning; diabetic retinopathy; ethnicity; fundus images; geographic variation	RETINAL IMAGES; VALIDATION	The prevalence of diabetes is on the rise steadily around the globe. Diabetic retinopathy (DR) is a result of damage to the blood vessels in the retina due to diabetes and its fast treatment is crucial for preventing possible blindness. The diagnosis of DR is done mostly using a comprehensive eye exam, where the eye is dilated for better inspection. Analysis by an ophthalmologist is prone to human error and thus automatic and highly accurate detection of DR is preferred for an earlier and better diagnosis. It is important, however, that automatic detection be accurate for all data collected from patients of different geographic and ethnic backgrounds. In this paper, the automatic detection of DR with a deep learning algorithm is analyzed when geographic and ethnic information of the patients is also integrated into the architecture. It is shown that robust and generalizable DR detection performance is linearly related to the correlation of geographic and ethnic patient information between the training and the testing datasets. The deep learning model created eliminates geographic variation in the detection and works for patients of all ethnicities.																	1300-0632	1303-6203					2020	28	2					664	678		10.3906/elk-1902-131													
J								A modified relay-race algorithm for floorplanning in PCB and IC design	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Placement; floorplanning; CAD; VLSI; printed circuit board	GENETIC ALGORITHM; PACKING	Floorplanning is a fundamental design step in the physical design of printed circuit boards (PCBs) and integrated circuits (ICs), as it handles the complexity of layout design. From a computational point of view, the floorplanning problem is an NP hard problem, and the size of the search space grows exponentially with increasing numbers of modules. Thus, the algorithm used is an essential factor for speed and quality of the floorplanning process. Although polynomial-time floorplanning algorithms can be implemented when solution space is limited to slicing floorplans, optimal solutions often exist only in the nonslicing floorplan search space. Various stochastic algorithms such as simulated annealing (SA), the genetic algorithm (GA), and the relay race algorithm (RRA) can be used with nonslicing floorplans. In this paper, a modified relay race algorithm (MRRA) is proposed. Based on the experimental results utilizing MCNC benchmarks, MRRA improved both solution quality and run time for area optimization when compared with SA, GA, and RRA.																	1300-0632	1303-6203					2020	28	2					679	692		10.3906/elk-1903-122													
J								Optimal design of a flux reversal permanent magnet machine as a wind turbine generator	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Flux reversal generator; permanent magnet; wind turbine; optimization; inductance; demagnetization	PERFORMANCE; FIELD	Flux reversal permanent magnet generators are well suited for use as wind turbine generators owing to their high torque generation ability and magnetic gear. However, they suffer from poor voltage regulation due to their high winding inductance. In this paper, a design optimization method is proposed for flux reversal generators in wind turbine applications. The proposed method includes a new multiobjective function. Cost, volume of the generator, and mass of the permanent magnet are considered in it independently and simultaneously. Besides the new objective function, the main superiority of this paper compared with published papers is considering winding inductance in optimization procedures as a constraint and analyzing the optimization results for different values of it. Also, for the first time, the equations for permanent magnet sizing are considered based on a demagnetization curve for designing a flux reversal generator. For this purpose, a step-by-step design procedure is proposed and sensitivity analysis is performed to determine the sensitivity of output parameters to specific electrical loading, the height of the permanent magnets, and the machine length-to-diameter ratio. Then a multiobjective optimization based on a genetic algorithm is carried out and the best combination of pole number and number of slots/pole/phase is obtained. Then, for this combination, the optimum value of the constraint is obtained, too. Then specifications and dimensions of the optimum flux reversal machine as a wind turbine generator is presented. Finally, a time-stepping finite element method is used to validate the design and optimization results.																	1300-0632	1303-6203					2020	28	2					693	707		10.3906/elk-1903-157													
J								Novel random models of entity mobility models and performance analysis of random entity mobility models	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Random entity mobility model; modeling and simulation; analysis of mobile networks; mobile crowd sensing; sensor; connectivity; mobile sensor networks		It has become possible to collect data from geographically large areas with smart devices that are prevalently used today. Sensors that are integrated into smart devices make it possible for these devices to receive and transmit data wirelessly. The most important problem of this model that is known as mobile crowd sensing and that allows inferences on the data obtained from its users is lack of data. The main reason for this problem is the lack of sufficient usage of the sensors on devices by the user. To increase the amount of data collected, while users may be incentivized in various ways, the amount and accuracy of the collected data may be increased by developing random entity mobility models (REMMs). In this study, two new models (random point and random journey) were proposed as alternatives to existing REMMs. In the experiment environment that was created to measure the performances of the proposed models, their performances were compared to those that are currently used prevalently (random waypoint (RWP), random walk (RW), and random direction (RD)). In the experiment environment, the performances were compared in terms of three different metrics (visiting rates of nodes, rates of reaching the basis, and the number of messages they carried to the basis). The greatest increase in differently sized areas and at different numbers of nodes in the RP model in terms of rates of reaching the basis was 2.6% compared to RWP, 7% compared to RW, and 46.34% compared to RD, while these values for the number of nodes that were visited were 3% compared to RWP, 1.5% compared to RW, and 17.67% compared to RD. In the same conditions in terms of the metric on the number of messages, the model collected 1465.4, 2933.46, and 7260.12 more messages than those in respectively RWP, RW, and RD. The greatest increase in differently sized areas and at different numbers of nodes in the RJ model in terms of reaching the basis was 1% compared to RWP, 3.5% compared to RW, and 25% compared to RD, while these values for the number of nodes that were visited were 0.75% compared to RWP, 2% compared to RW, and 21.4% compared to RD. In the same conditions in terms of the metric on the number of messages, the model collected 1109.56, 1534.26, and 4488.5 more messages than those in RWP, RW, and RD, respectively.																	1300-0632	1303-6203					2020	28	2					708	726		10.3906/elk-1904-102													
J								A priority-based queuing model approach using destination parameters for real-time applications on IPv6 networks	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Quality of service; end-to-end delay; flow label; priority queue; IPv6; voice over IP; video over IP	PERFORMANCE ANALYSIS; QOS; ALGORITHM; SCHEME; FLOW	In the early days of the Internet architecture, the most important aim is to transmit data over packet switched networks. The traditional Internet architecture used in these networks lacks quality of service. However, today, as real-time applications increase, it is needed. There are approaches to improving the quality of service using the flow label field in the Internet Protocol version 6 header. In this study, a novel algorithm that uses destination network parameters to reduce queuing and end-to-end delay is created. A round-robin-based time-aware priority queue new model is used within this algorithm. Data packets using this proposed queue are prioritized with metric values of the destination network. In order to provide end-to-end service quality, the prioritization value is used by placing it in the flow label field. For this purpose, a new approach to the use of this field is proposed. Delay, one of the most important factors affecting quality of service, is reduced with the proposed algorithm and flow label usage approach. As a result, the reduction in delay times between 22 and 39 ms resulted in various improvement rates between 16.79% and 35.13%.																	1300-0632	1303-6203					2020	28	2					727	742		10.3906/elk-1904-123													
J								HUBBLE: an optical link management system for dense wavelength division multiplexing networks	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Dense wavelength division multiplexing; service provider; fault management; link quality		Timely detection of Dense Wavelength Division Multiplexing (DWDM) link quality and service performance problems of fiber deployment are important and critical for telecommunication operators. In this paper, we propose a new methodology for network fault detection inside optical transmission systems deployed in a real-operator environment and present the working principles of the system. Our new calculation methodology is used for joint fiber and DWDM link quality evaluation inside the proposed High-level Unified BackBone Link Examiner (HUBBLE) platform. At the end of the paper, we also detail some of the benefits, challenges, and opportunities of automation in DWDM networks using the proposed HUBBLE platform.																	1300-0632	1303-6203					2020	28	2					743	756		10.3906/elk-1904-207													
J								An Inter-Domain Attack Mitigating Solution	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Denial of service; Cyber; Attack; Software defined network; Openflow; Flowspec	DDOS ATTACKS; DEFENSE-MECHANISMS; ANOMALY DETECTION; SDN	Online services on the Internet are increasing day by day, and in parallel, the number of cyber-attacks is rapidly increasing. These attacks are not always about data theft, but they can cause severe damage by denial of service attacks. Intrusion Prevention System products that many organizations use at the border of their enterprise networks are not strong enough to protect against DoS attacks. The typical way to mitigate such attacks is to get support from a service provider. However, a service provider only provides solutions for the traffic originating from itself. If the source of attack is in another ISP domain, it is possible to inform that ISP via phone or e-mail. As a result, the source of the attack is blocked by the manual intervention of the service provider whose domain hosts it. Border Gateway Protocol (BGP) based solutions are also available for automating a blocking system, but not all enterprise networks support BGP. In this research, we have developed a centralized automation solution for software defined network (SDN) environments that is capable of preventing cyber-attacks at the source of attack. This solution does not require any BGP support. Non-SDN environments can also use this attack mitigation and notification system. In the long run, we may use this system to create a national protection shield in order to mitigate Cybersecurity attacks.																	1300-0632	1303-6203					2020	28	2					757	772		10.3906/elk-1904-179													
J								Coordinated charging of electric vehicles including customer options for slow or fast charging	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Plug in electric vehicles; vehicle charging costs; coordinated charging; linear programming	MANAGEMENT; SYSTEM	Transportation system electrification in the world decreases the gasoline consumption that leads to increase in usage of number of plug in electric vehicles (PEVs). PEV is a bidirectional resource which, while playing the role of a resource, poses challenges in its management. These vehicles are to be charged at a residential standard outlet or in a corporate car charging station. This paper mainly aims to maximize the benefits of a customer who comes to a charging station for charging their vehicle. An incentive-based cost mechanism is introduced to optimally schedule the vehicles; this mechanism minimizes the overall charging cost, considers their random arrival and departure times and maximizes battery energy before they leave the station. As far as we know, there are no studies on minimizing the cost of coordinated optimal charging of electric vehicles at an isolated charging station with different charging modes. This paper presents and solves a linear optimization problem by LINGO, where the vehicles are connected for charging either in slow charging mode or fast charging mode at hourly basis for 6 h. The results are analyzed and validated. A 30-vehicle model is worked out. A 24-h schedule can also be worked on the same lines as given in this paper when the number of incoming vehicles is large.																	1300-0632	1303-6203					2020	28	2					773	783		10.3906/elk-1806-196													
J								A multibeam subarrayed time-modulated linear array	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Time-modulated arrays; subarrayed antennas; sideband radiation; harmonic beamforming; beam steering; cognitive radio	SIDE-BAND RADIATIONS; DESIGN; SUPPRESSION; LEVEL	In conventional time-modulated arrays (TMAs), because of the usage of the RF-switch, harmonics are generated at multiples of the modulation frequency. In this study, the synthesis of time-modulated arrays has been analyzed for cognitive radio (CR) systems, in which these harmonics are suitably exploited for more efficient utilization of the spectrum. In order to accomplish the desired pattern at requested harmonic frequencies, a new excitation strategy with sinusoidal signals is proposed. The use of sinusoidal waveforms creates independent beams, allowing independent steering capability. Moreover, by utilizing the subarray structure, it is possible to have a smaller number of excitation functions in the hardware. It is shown by means of explanatory examples that the effectiveness of the proposed approach is due to free-adjustable beamforming, beam steering, and low complexity.																	1300-0632	1303-6203					2020	28	2					784	798		10.3906/elk-1904-201													
J								Robust optimal operation of smart distribution grids with renewable based generators	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Fuzzy clustering method; particle swarm optimization; point estimation method; probabilistic assessment; robust operating; smart distribution grid	BATTERY ENERGY-STORAGE; OPTIMAL ESS ALLOCATION; DISTRIBUTION-SYSTEMS; MANAGEMENT; INTEGRATION; MICROGRIDS; PLACEMENT; FLOW	Modern distribution systems are equipped with various distributed energy resources (DERs) because of the importance of local generation. These distribution systems encounter more and more uncertainties because of the ever-increasing use of renewable energies. Other sources of uncertainty, such as load variation and system components' failure, will intensify the unpredictable nature of modern distribution systems. Integrating energy storage systems into distribution grids can play a role as a flexible bidirectional source to accommodate issues from constantly varying loads and renewable resources. The overall functionality of these modern distribution systems is enhanced using communication and computational abilities in smart grid frameworks. Robust operation of these systems is effectively taken into consideration to manage the uncertainty, which offers an explicit way to control the desired conservativeness. This paper presents an optimal operating program for smart grids equipped with wind generators, controllable distributed generators, energy storage systems, and reactive power compensators. In order to make the studies more practical, uncertainty about wind generators and grid loads is taken into account. Furthermore, the presented operating program is robust in various conditions, i.e. there is no need to change the operating program in a wide range of probable states. The point estimation method and fuzzy clustering method are used for probabilistic assessment of the distribution system in the presence of uncertainties. The IEEE 37-node standard test system, which is a highly unbalanced system, is selected for the case study and the results are discussed comprehensively.																	1300-0632	1303-6203					2020	28	2					799	820		10.3906/elk-1901-182													
J								Hyperheuristics for explicit resource partitioning in simultaneous multithreaded processors	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Hyperheuristics; simultaneous multithreading; resource partitioning	HYPER-HEURISTICS	In simultaneous multithreaded (SMT) processors, various data path resources are concurrently shared by many threads. A few heuristic approaches that explicitly distribute those resources among threads with the goal of improved overall performance have already been proposed. A selection hyperheuristic is a high-level search methodology that mixes a predetermined set of heuristics in an iterative framework to utilize their strengths for solving a given problem instance. In this study, we propose a set of selection hyperheuristics for selecting and executing the heuristic with the best performance at a given stage. To the best of our knowledge, this is one of the first studies implementing a hyperheuristic algorithm on hardware. The results of our experimental study show that hyperheuristics are indeed capable of improving the performance of the studied workloads. Our best performing hyperheuristic achieves better throughput than both baseline heuristics in 5 out of 12 workloads and gives about 15% peak performance gain. The average performance gains over the well-known hill-climbing and adaptive resource partitioning heuristics are about 5% and 2%, respectively.																	1300-0632	1303-6203					2020	28	2					821	835		10.3906/elk-1904-49													
J								Measurement of sound velocity in oil wells based on fast adaptive median filtering	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Sound velocity measurement; fast adaptive median filtering; curve fitting; oil wells	NOISE	Measurement of sound velocity in oil wells has long been a challenging industrial issue due to the difficulty in obtaining clear oil pipe coupling waves in strong noise. In this paper, a novel sound velocity measurement method is developed for the dynamic liquid level of oil wells based on fast adaptive median filtering. First, to solve the noise interference problem in the reflected oil pipe coupling wave, a fast adaptive median filtering algorithm is proposed to obtain an accurate oil pipe coupling wave. Then a curve fitting method based on range discrete coefficient is developed to estimate the sound velocity in the tubing-casing annular space of oil wells. The fitting sound velocity can reflect the propagation rule of the real sound velocity. In particular, the sound velocity at any position within the tubing-casing annular space can be accurately calculated by the fitting function. Finally, the proposed method is applied to the dynamic liquid level of an oil well. Experimental results show the effectiveness and favorable accuracy in estimating the sound velocity distribution.																	1300-0632	1303-6203					2020	28	2					836	849		10.3906/elk-1904-94													
J								Event-based summarization of news articles	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Event-based summarization; event extraction; feature selection; single-text summarization; extractive summarization	DOCUMENT; SINGLE	In recent years, with the increase of available digital information on the Web, the time needed to find relevant information is also increased. Therefore, to reduce the time spent on searching, research on automatic text summarization has gained importance. The proposed summarization process is based on event extraction methods and is called an event-based extractive single-document summarization. In this method, the important features of event extraction and summarization methods are analyzed and combined together to extract the summaries from single-source news documents. Among the tested features, six features are found to be the most effective in constructing good summaries. The constructed summaries are tested on benchmark Document Understanding Conferences 2001 and 2002 datasets, and the results outperformed most of the other well-known summarization methods.																	1300-0632	1303-6203					2020	28	2					850	864		10.3906/elk-1904-98													
J								Design of a low pass filter using rhombus-shaped resonators with an analytical LC equivalent circuit	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Compact size; LC equivalent circuit; low pass filter; microstrip; rhombus-shaped resonator; wideband	MICROSTRIP LOWPASS FILTER; PERFORMANCE	In this paper, a new method is presented for the design of a low pass filter (LPF) based on an LC equivalent circuit. Firstly, new formulas are proposed to calculate an LC equivalent circuit of a rhombus-shaped resonator. Secondly, the transfer function and transmission zero of the rhombus-shaped resonator are extracted, based on the presented formulas. Then other rhombus-shaped resonators are designed, based on the extracted formulas. The proposed filter has a cut-off frequency with an attenuation level of 3 dB at 1.54 GHz. The obtained return loss and insertion loss in the pass band are 16 dB and 0.1 dB, respectively. The designed filter is fabricated on RT-5880 with 31 mil thickness.																	1300-0632	1303-6203					2020	28	2					865	874		10.3906/elk-1905-153													
J								Short unsegmented PCG classification based on ensemble classifier	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Phonocardiogram; mel frequency cepstral coefficient; homomorphic filtering; ensemble classifier; feature extraction; machine learning	SOUND CLASSIFICATION; NEURAL-NETWORKS; HEART SOUNDS; SEGMENTATION; RECOGNITION; HSMM	Diseases associated with the heart are one of the main reasons of death worldwide. Hence, early examination of the heart is important. For analysis of cardiac disorders, a study of heart sounds is a crucial and beneficial approach. Still, automated classification of heart sounds is a challenging task that mainly depends on segmentation of heart sounds and derivation of features using segmented samples. In the literature available for PCG classification provided by PhysioNet/CinC Challenge 2016, most of the research has focused on enhancing the accuracy of the classification model based on complicated segmentation processes and has failed to improve the sensitivity. In this paper, we present an automated heart sound classification by eliminating the segmentation steps using multidomain features, which results in enhanced sensitivity. The study is based on homomorphic envelogram, mel frequency cepstral coefficient (MFCC), power spectral density (PSD), and multidomain feature extraction. The extracted features are trained using the 5-fold cross-validation method based on an ensemble boosting algorithm over 100 independent iterations. Our proposed design is evaluated using public datasets published in PhysioNet/Computers in Cardiology Challenge 2016. Accuracy of 92.47% with improved sensitivity of 94.08% and specificity of 91.95% is achieved using our model. The output performance proves that our proposed model offers superior performance results.																	1300-0632	1303-6203					2020	28	2					875	889		10.3906/elk-1905-165													
J								Dynamic software rejuvenation in web services: a whale optimization algorithm-based approach	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Software aging; software rejuvenation; web service; whale optimization algorithm	PERFORMANCE; POLICY	In this paper, we suggest a method for determining the restarting time for web services to increase availability, known as rejuvenation. We consider different parameters such as number of users, maximum service request number, response time, and throughput of a web service to determine its restarting time. Software rejuvenation is an effective technique to counteract software aging in continuously running applications such as web service-based systems. In these systems, web services are allocated based on the needs of the receivers and facilities of servers. One of the challenges while assigning web services is selecting the appropriate server to reduce faults. Since the selection of a server among candidates while maintaining the optimal quality of service is an NP-hard problem, metaheuristics seem to be suitable. In this paper, we propose dynamic software rejuvenation as a proactive fault-tolerance technique based on the whale optimization algorithm. The threshold for the rejuvenation of each of the web services is considered and training is done based on the features of the service providers as well as the needs of the receivers. The whale optimization algorithm with the criterion of movement radius is utilized for flexibility of web service provider selection. Here, we detect and rejuvenate systems that required rejuvenation before the occurrence of a fault. The simulation results reveal that our strategy can decrease the failure rate by an average of 30 percent in comparison with state-of-the-art strategies and improve the system availability in web services.																	1300-0632	1303-6203					2020	28	2					890	903		10.3906/elk-1905-177													
J								Ternary logical naming convention and application in ternary optical computers	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Ternary logical; ternary optical computer; naming convention	GROUP DECISION; DESIGN; PARALLEL	This article introduces the ternary logical naming convention, which was newly discovered in the study of logical units of ternary optical computers (TOCs). First, the design principle and design specification of the ternary logical naming convention are elaborated in detail and several examples are given to illustrate the use of the naming convention. Second, taking the modified signed-digit (MSD) adder of the TOC as an example, the naming convention is applied to build four ternary logical units of the MSD adder, and the implementation method of pipelined addition is introduced. Finally, the correctness of the ternary logical naming convention proposed in this paper and the usability of the adder built according to this convention are illustrated by experiments. The ternary logical naming convention can easily obtain the relationship between the ternary logic transforms, thus judging the characteristics of the logic units of TOCs, which is helpful to promote the in-depth study of TOCs in the field of numerical calculation.																	1300-0632	1303-6203					2020	28	2					904	916		10.3906/elk-1905-35													
J								An automated eye disease recognition system from visual content of facial images using machine learning techniques	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Deep convolution neural network; support vector machine; principal component analysis; t-distributed stochastic neighbor embedding; automated eye disease recognition		Many eye diseases like cataracts, trachoma, or corneal ulcer can cause vision problems. Progression of these eye diseases can only be prevented if they are recognized accurately at the early stage. Visually observable symptoms differ a lot among these eye diseases. However, a wide variety of symptoms is necessary to be analyzed for the accurate detection of eye diseases. In this paper, we propose a novel approach to provide an automated eye disease recognition system using visually observable symptoms applying digital image processing techniques and machine learning techniques such as deep convolution neural network (DCNN) and support vector machine (SVM). We apply the principal component analysis and t-distributed stochastic neighbor embedding methods for better feature selection. The proposed system automatically divides the facial components from the frontal facial image and extracts the eye part. The proposed method analyzes and classifies seven eye diseases including cataracts, trachoma, conjunctivitis, corneal ulcer, ectropion, periorbital cellulitis, and Bitot's spot of vitamin A deficiency. From the experimental results, we see that the DCNN model outperforms SVM models. We also compare our method with some other existing methods. Our method shows improved accuracy compared to other methods. The average accuracy rate of our DCNN model is 98.79% with sensitivity of 97% and specificity of 99%.																	1300-0632	1303-6203					2020	28	2					917	932		10.3906/elk-1905-42													
J								Multitask-based association rule mining	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Association rule mining; multitask learning; data mining; the frequent pattern (FP)-Growth algorithm	PARALLEL; ALGORITHMS; CLASSIFICATION	Recently, there has been a growing interest in association rule mining (ARM) in various fields. However, standard ARM algorithms fail to discover rules for multitask problems as they do not consider task-oriented investigation and, therefore, they ignore the correlation among the tasks. Considering this situation, this paper proposes a novel algorithm, named multitask association rule miner (MTARM), that tends to jointly discover rules by considering multiple tasks. This paper also introduces two novel concepts: single-task rule and multiple-task rule. In the first phase of the proposed approach, highly frequent local rules (single-task rules) are explored for each task separately and then these local rules are combined to produce the global result (multitask rules) using a majority voting mechanism. Experiments were conducted on four different real-world multitask learning datasets. The experimental results indicated that the proposed MTARM approach discovers more information than that of traditional ARM algorithms by jointly considering the relationships among multiple tasks.																	1300-0632	1303-6203					2020	28	2					933	955		10.3906/elk-1905-88													
J								Adaptive prescribed performance servo control of an automotive electronic throttle system with actuator constraint	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Automotive electronic throttle; position tracking; prescribed performance; adaptive technique	SLIDING-MODE CONTROL	To further improve the transient and steady-state performance of automotive electronic throttle position tracking, in this paper an adaptive prescribed performance servo control strategy is designed and applied to a real electronic throttle control system. In view of the possible high gain of the prescribed performance controller in practice, the actuator constraint is also considered in the controller design. The designed servo controller can ensure the transient and steady-state responses of tracking error are limited in the range prescribed by the performance function, and converge with the prescribed convergence rate and have no overshoot. The incorporated adaptive updating law can enhance the robustness of the transient and steady-state performance against uncertainty from the product tolerance, the operating conditions, and the aging of components. Both Matlab/Simulink simulation and dSPACE-based hardware-in-the-loop experimental verification show the effectiveness and applicability of the proposed control strategy.																	1300-0632	1303-6203					2020	28	2					956	968		10.3906/elk-1906-143													
J								Correlation coefficients of Pythagorean hesitant fuzzy sets and their application to radar LPI performance evaluation	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Correlation coefficients; Pythagorean hesitant fuzzy sets; low probability of interception; multidomain evaluation	DECISION-MAKING; PROBABILITY; OPERATORS	Evaluating low probability of intercept (LPI) performance is the first step to design parameters and arrange radar resources. In the evaluation process it is hard to rely on the intercept receiver's working scenarios and operating parameters. On the other hand, indicators that affect the LPI performance of radiating side are difficult to consider comprehensively. Thus, building an effective evaluation system is crucial. This research considers the natural parameters of radar extracted from a radiating scenario. Subsequently, a number of criteria are selected, including spatial, time, frequency domain, polarization status, energy status, and waveform features. A multidomain radar LPI performance evaluation method is established, which is based on Pythagorean hesitant fuzzy sets (PHFSs). The paper is motivated by other scholars' research on fuzzy set theories and derives correlation coefficients as well as their properties for PHFSs. Concretely speaking, this study takes account of membership degree, nonmembership degree, and the hesitation of decision makers, so it integrates the benefits of correlation coefficients of hesitant fuzzy sets with Pythagorean fuzzy sets. Meanwhile, weighted correlation coefficients of PHFSs and their properties are proposed in detail. This provides a feasible approach for evaluation problems. For the sake of application, this article gives the specific LPI performance evaluation process. Finally, a novel method is presented to evaluate four fire control radars' LPI performances and is proved to be viable.																	1300-0632	1303-6203					2020	28	2					969	983		10.3906/elk-1906-177													
J								Piezoresistive disposable weight sensor with increased sensitivity	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Weight sensor; piezoresistive sensor; paper substrate; perforated-structure; disposable sensor; read-out circuitry		This study presents the design, simulation, implementation, and experimental characterization of a paper-based perforated disposable weight sensor system with a double piezoresistive layer. The demonstrated system is designed to achieve highly sensitive weight sensing operations with low-cost materials. For that purpose, the main fabrication material of the proposed disposable sensor is selected as a 289-mu m-thick Strathmore 400 series Bristol paper. Approximately 48-mu m-thick piezoresistive graphite paste is coated onto both sides of the paper-based cantilever beam with the aim of acquiring more sensitive weight-sensing capability. Additionally, the proposed paper-based structure has rows of closely spaced perforations at prespecified locations to facilitate the bending of the cantilever beam and to further increase the sensitivity of the system. A peripheral electronic read-out circuitry is developed and integrated into the system. It is experimentally demonstrated that the proposed weight-sensing system can measure miniature weights ranging to 2 g with a resolution of 20 mg. The implemented sensor has a sensitivity of 17.13 mV/mN or 168.01 mV/g.																	1300-0632	1303-6203					2020	28	2					984	998		10.3906/elk-1906-181													
J								A fast text similarity measure for large document collections using multireference cosine and genetic algorithm	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Text similarity; near-duplicate; reference text; genetic algorithm		One of the critical factors that make a search engine fast and accurate is a concise and duplicate free index. In order to remove duplicate and near-duplicate (DND) documents from the index, a search engine needs a swift and reliable DND text document detection system. Traditional approaches to this problem, such as brute force comparisons or simple hash-based algorithms, are not suitable as they are not scalable and are not capable of detecting near-duplicate documents effectively. In this paper, a new signature-based approach to text similarity detection is introduced, which is fast, scalable, and reliable and needs less storage space. The proposed method is examined on standard text document datasets such as CiteseerX, Enron, Gold Set of Near-duplicate News Articles, and other similar datasets. The results are promising and comparable with the best cutting-edge algorithms considering accuracy and performance. The proposed method is based on the idea of using reference texts to generate signatures for text documents. The novelty of this paper is the use of genetic algorithms to generate better reference texts.																	1300-0632	1303-6203					2020	28	2					999	1013		10.3906/elk-1906-30													
J								Accurate indoor positioning with ultra-wide band sensors	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Indoor positioning; ultra-wide band sensors; optimization; big bang-big crunch algorithm; genetic algorithms; the Kalman filter	COMMUNICATION; SYSTEMS; ALGORITHM	Ultra-wide band is one of the emerging indoor positioning technologies. In the application phase, accuracy and interference are important criteria of indoor positioning systems. Not only the method used in positioning, but also the algorithms used in improving the accuracy is a key factor. In this paper, we tried to eliminate the effects of off-set and noise in the data of the ultra-wide band sensor-based indoor positioning system. For this purpose, optimization algorithms and filters have been applied to the raw data, and the accuracy has been improved. A test bed with the dimensions of 7.35 m x 5.41 m and 50 cm x 50 cm grids has been selected, and a total of 27,000 measurements have been collected from 180 test points. The average positioning error of this test bed is calculated as 16.34 cm. Then, several combinations of algorithms are applied to raw data. The combination of Big Bang-Big Crunch algorithm for optimization, and then the Kalman Filter have yielded the most accurate results. Briefly, the average positioning error has been reduced from 16.34 cm to 7.43 cm.																	1300-0632	1303-6203					2020	28	2					1014	1029		10.3906/elk-1911-79													
J								Development of a supervised classification method to construct 2D mineral maps on backscattered electron images	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Random forest; Mineral Liberation Analyzer; backscattered electron images; mineral map; confusion matrix	IRON-ORE; LIBERATION; INTERPOLATION; FLOTATION	The Mineral Liberation Analyzer (MLA) can be used to obtain mineral maps from backscattered electron (BSE) images of particles. This paper proposes an alternative methodology that includes random forest classification, a prospective machine learning algorithm, to develop mineral maps from BSE images. The results show that the overall accuracy and kappa statistic of the proposed method are 97% and 0.94, respectively, proving that random forest classification is accurate. The accuracy indicators also suggest that the proposed method may be applied to classify minerals with similar appearances under BSE imaging. Meanwhile, random forest predicts fewer middling particles with binary and ternary composition, but the MLA predicts more middling particles only with ternary composition. These discrepancies may arise because the MLA, unlike random forest, may also measure the elemental compositions of mineral surfaces below the polished section.																	1300-0632	1303-6203					2020	28	2					1030	1043		10.3906/elk-1906-60													
J								Prediction of railway switch point failures by artificial intelligence methods	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Railway switch point; artificial neural networks; support vector machines; failure prediction; maintenance management		In recent years, railway transport has been preferred intensively in local and intercity freight and passenger transport. For this reason, it is of utmost importance that railway lines are operated in an uninterrupted and safe manner. In order to carry out continuous operation, all systems must continue to operate with maximum availability. In this study, data were collected from switch motors, which are the important equipment of railways, and the related equipment and these data were evaluated with sector experience and the results related to the failure status of the switch points were revealed. The obtained results were processed with support vector machines and artificial neural networks, which are artificial intelligence methods, and machine learning was performed. In the light of this learning, a decision support model, which predicts possible failures and gives information about the root cause of the failures that have occurred, was developed. This model aims to ensure that the data obtained in each movement of the railway switch point are processed and the necessary corrective and preventive actions are communicated to the maintenance personnel; thus, failures are eliminated before they affect the railway operation and the solution process of the failures that have occurred is shortened. Considering the six switch points from which the data were collected, the experimental results were predicted with 24% RMSE error rates in the SVM method, while they were successfully predicted with RMSE error rates ranging from 2.4% to 6.6% in the ANN method. Therefore, it is observed that the ANN method is more appropriate in the implementation of the established model.																	1300-0632	1303-6203					2020	28	2					1044	1058		10.3906/elk-1906-66													
J								A viable snore detection system: hardware and software implementations	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Biomedical equipment; biomedical signal processing; medical signal detection; sleep apnea; snore	AUTOMATIC DETECTION; SLEEP; CLASSIFICATION; SEGMENTATION; ASSOCIATION; VALIDATION; EXTRACTION; DEVICE	A stand-alone, custom-made biomedical system was introduced for long-term monitoring of sleep and detection of snoring events. Commercially available electronic components were assembled for recording audio, pulse, and respiration signals. Its software was implemented for off-line processing of the acquired signals in C++ and MATLAB environments. The linear and nonlinear features of the signals were extracted and characterized using spectral energy distribution, entropy, and largest Lyapunov exponent (LLE). The performance of the system was evaluated with real physiological data gathered from 14 chronic snorers. Analysis of the cases indicated that the system identified the snoring events with an accuracy of 88.22%, sensitivity of 94.91%, and positive predictive value of 90.95%. This high level of validation confirmed the reliability and utility of the system in detecting snoring.																	1300-0632	1303-6203					2020	28	2					1059	1069		10.3906/elk-1906-90													
J								Estimating spatiotemporal focus of documents using entropy with PMI	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Document analysis; spatiotemporal focus estimation; temporal entropy; spatial entropy; pointwise mutual information	TEMPORAL LANGUAGE MODELS	Many text documents are spatiotemporal in nature, i.e. contents of a document can be mapped to a specific time period or location. For example, a news article about the French Revolution can be mapped to year 1789 as time and France as place. Identifying this time period and location associated with the document can be useful for various downstream applications such as document reasoning or spatiotemporal information retrieval. In this paper, temporal entropy with pointwise mutual information (PMI) is proposed to estimate the temporal focus of a document. PMI is used to measure the association of words with time expressions. Moreover, a word's temporal entropy is considered as a weight to its association with a time point and a single time point with the highest overall score is chosen as the focus time of a document. The proposed method is generic in the sense that it can also be applied for spatial focus estimation of documents. In the case of spatial entropy with PMI, PMI is used to calculate the association between words and place entities. The effectiveness of our proposed methods for spatiotemporal focus estimation is evaluated on diverse datasets of text documents. The experimental evaluation confirms the superiority of our proposed temporal and spatial focus estimation methods.																	1300-0632	1303-6203					2020	28	2					1070	1085		10.3906/elk-1907-10													
J								Satire identification in Turkish news articles based on ensemble of classifiers	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Satire detection; figurative language; machine learning; classifier ensembles		Social media and microblogging platforms generally contain elements of figurative and nonliteral language, including satire. The identification of figurative language is a fundamental task for sentiment analysis. It will not be possible to obtain sentiment analysis methods with high classification accuracy if elements of figurative language have not been properly identified. Satirical text is a kind of figurative language, in which irony and humor have been utilized to ridicule or criticize an event or entity. Satirical news is a pervasive issue on social media platforms, which can be deceptive and harmful. This paper presents an ensemble scheme for satirical news identification in Turkish news articles. In the presented scheme, linguistic and psychological feature sets have been utilized to extract the feature sets (i.e. linguistic, psychological, personal, spoken categories, and punctuation). In the classification phase, accuracy rates of five supervised learning algorithms (i.e. naive Bayes algorithm, logistic regression, support vector machines, random forest, and k-nearest neighbor algorithm) with three widely utilized ensemble methods (i.e. AdaBoost, bagging, and random subspace) have been considered. Based on the results, we concluded that the random forest algorithm yielded the highest performance, with a classification accuracy of 96.92% for satire detection in Turkish. For deep learning-based architectures, we have achieved classification accuracy of 97.72% with the recurrent neural network architecture with attention mechanism.																	1300-0632	1303-6203					2020	28	2					1086	1106		10.3906/elk-1907-11													
J								Crash course learning: an automated approach to simulation-driven LiDAR-based training of neural networks for obstacle avoidance in mobile robotics	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Autonomous mobile robots; obstacle avoidance; neural networks; simulation-based learning		This paper proposes and implements a self-supervised simulation-driven approach to data collection used for training of perception-based shallow neural networks for mobile robot obstacle avoidance. In the approach, a 2D LiDAR sensor was used as an information source for training neural networks. The paper analyzes neural network performance in terms of numbers of layers and neurons, as well as the amount of data needed for reliable robot operation. Once the best architecture is identified, it is trained using only data obtained in simulation and then implemented and tested on a real robot (Turtlebot 2) in several simulations and real-world scenarios. Based on obtained results it is shown that this fast and simple approach is very powerful with good results in a variety of challenging environments, with both static and dynamic obstacles.																	1300-0632	1303-6203					2020	28	2					1107	1120		10.3906/elk-1907-112													
J								Design and characterization of a compact single-layer multibeam array antenna using an 8 x 8 Butler matrix for 5G base station applications	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Base station; Butler matrix; fifth generation; 5G; multibeam array antenna; single-layer	REQUIREMENTS; TECHNOLOGIES	A multibeam array antenna employing a Butler matrix is a promising solution for fifth generation (5G) base stations. Due to inaccurate phase differences between output ports in the Butler matrix, the radiation characteristics could show incorrect main beam directions. In addition, the literature has also reported the issue of high amplitude imbalance in the Butler matrix. This paper presents a single-layer multibeam array antenna fed by an 8 x 8 Butler matrix operating at 28 GHz for 5G base station applications-a more cost-effective solution for large-scale production. The Butler matrix consists of twelve quadrature hybrids, sixteen crossovers, and eight phase shifters. This circuit was integrated with eight antenna elements at the output ports of the Butler matrix. The proposed multibeam array antenna was fabricated using a low dielectric constant and a low loss tangent substrate. The dimensions of the multibeam array antenna were 88 x 106 x 0.254 mm(3). The Butler matrix achieved low insertion losses and low phase error with average values of 2.5 dB and less than +/- 10 degrees at 28 GHz, respectively. The measured return losses were less than -10 dB at 28 GHz. The measured radiation patterns were obtained and eight main beams were pointed at +/- 6 degrees , +/- 18 degrees , +/- 30 degrees , and +/- 44 degrees with measured gains between 9 dBi and 14 dBi.																	1300-0632	1303-6203					2020	28	2					1121	1134		10.3906/elk-1907-119													
J								Convolutional auto encoders for sentence representation generation	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Convolutional networks; bi-gram; n-gram; question answering problem; deep learning; variational autoencoder; sentence modeling		In this study, we have proposed an alternative approach for sentence modeling problem. The difficulty of the choice of answer, the semantically related questions and the lack of syntactic closeness of the answers give rise to the difficulty of selecting the answer. The deep learning field has recently achieved a pivotal success in semantic analysis, machine translation, and text summaries. The essence of this work, inspired by the human orthographic processing mechanism and using multiple convolution filters with pre-rendered 2-Dimension (2D) representations of sentences, input or output size is to learn the basic features of the language without concerns. For this reason, the semantic relations in the sentence structure are learned by the convolutional variational auto-encoders first, and then the question and answer spaces learned by the auto-encoders are linked with proposed intermediate models. We have benchmarked five variations of our proposed model, which is based on Variational Auto-Encoder with multiple latent spaces and able to achieve lower error rates than the baseline model, which is the base Convolutional LSTM.																	1300-0632	1303-6203					2020	28	2					1135	+		10.3906/elk-1907-13													
J								Measurement based threat aware drone base station deployment	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Drone base stations; tactical networks; metaheuristic; urban warfare	LOCALIZATION	Unmanned aerial vehicles are gaining importance with many civilian and military applications. Especially the surveillance, search/rescue, and military operations may have to be carried out in extremely constrained environments. In such scenarios, drone base stations (DBSs) have to provide communication services to the people at the ground. The ground users may have no access to the global positioning system (GPS); therefore, their locations have to be estimated using alternative techniques. Besides there may be threats in the environment, such as shooters. In this work, we address the problem of optimal DBS deployment under the aforementioned constraints. We propose a novel DBS deployment algorithm that uses estimated positions of ground users and threats. The proposed algorithm is based on receiver signal strength-based maximum likelihood estimate of user locations and K-means clustering supported heuristic that takes into account the positions of threats. Numerical results show that proposed algorithm performs close to the computation intensive near-optimal algorithm and strikes a good trade-off between the number of unserved users and the probability of DBSs not being hit.																	1300-0632	1303-6203					2020	28	2					1149	1163		10.3906/elk-1907-183													
J								Fast adaptive reclosing in double-circuit transmission lines for improving power system stability based on harmonic analysis scheme	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Transient and permanent faults; double-circuit transmission lines; single phase auto-reclosure; EMTP-RV	SINGLE-PHASE RECLOSURE; TRANSIENT STABILITY; DIGITAL ALGORITHM; TIME; IMPLEMENTATION	An appropriate method is proposed for identifying permanent faults from transient faults in double-circuit transmission lines. This method could be used in adaptive single phase auto-reclosures in order to diagnose between permanent and transient faults, determine extinguishing time of the secondary arc, and calculate issuing time of reclosing commands during the occurrence of transient single phase to ground faults. The proposed method is based on harmonic analysis of the adjacent healthy circuit and could be an effective solution for blocking permanent disconnection of the power flow, improving stability, and maintaining the power network synchronism. In this paper, transient and permanent faults are simulated on a typical power system, and then current harmonics of the healthy circuit terminal are extracted and finally the proposed index for identifying the fault type is applied. In the case of transient faults, this method determines the minimum time needed for a fast successful adaptive reclosing in the network and thus prevents instability and nonsynchronism in both sides of the transmission line. Results of the various simulations run in EMTP-RV verifies accurate performance of the proposed approach.																	1300-0632	1303-6203					2020	28	2					1164	1178		10.3906/elk-1907-161													
J								Chemical disease relation extraction task using genetic algorithm with two novel voting methods for classifier subset selection	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										Multiple classifier systems; genetic algorithm; chemical disease relation; relation extraction; text mining; classifier ensemble		Biomedical relation extraction is an important preliminary step for knowledge discovery in the biomedical domain. This paper proposes a multiple classifier system (MCS) for the extraction of chemical-induced disease relations. A genetic algorithm (GA) is employed to select classifier ensembles from a pool of base classifiers. Moreover, the voting method used for combining the members of each of the ensembles is also selected during evolution in the GA framework. The performances of the MCSs are determined by the algorithms used for selecting the classifiers, the diversity among the selected classifiers, and the voting method used in the classifier combination. The base classifiers are represented in the form of chromosomes, where each chromosome contains all information on the ensemble it represents: the subset of classifiers voting and the voting method. The chromosomes are evolved using a variety of genetic selection, mating, and mutation techniques in order to find an optimal solution. The aim of the proposed system is to select the subset of classifiers with diverse abilities while maximizing the strengths of the best classifiers in the classifier ensemble for a given voting method. Two main contributions of this work are the evolution of the voting bit as part of the GA and the novel approach of using two different decision-making under uncertainty techniques as voting methods. Furthermore, two different selection algorithms and crossover operators are employed as ways of increasing variations during evolution. We validated our proposed method on nine different experimental settings and they produced good results comparable to the state-of-the-art systems, thereby justifying our approach.																	1300-0632	1303-6203					2020	28	2					1179	1196		10.3906/elk-1906-46													
J								Investigation on leakage current, erosion, and hydrophobic performance of high-voltage insulator coatings of different thicknesses	TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES										RTV coating; hydrophobicity; eroded masses; inclined-plane tests; dynamic drop test	SILICONE-RUBBER; RTV SILICONE; ELECTRICAL PERFORMANCE	Room-temperature vulcanizing (RTV) silicone rubber is used as a high-voltage insulator coating to enhance the pollution performance of conventional insulators, and the electrical performance of RTV coating depends on the several parameters. The present study shows the effects of the thickness on the leakage current, erosion, and hydrophobic performance of high-voltage insulator coating. Several different thicknesses of RTV silicone rubber were taken into consideration on porcelain substrates. Erosion and leakage current performances of the samples were evaluated by using inclined-plane tests (IPTs) based on the IEC 60587 standard. Eroded masses and fundamental and harmonic components of the leakage currents were investigated with respect to coating thicknesses. On the other hand, hydrophobic features of the samples were evaluated by using dynamic drop tests (DDTs) with regard to CIGRE TB442 documentation. Time to loss of hydrophobicity and the level of the leakage currents were investigated in DDTs considering the thicknesses.																	1300-0632	1303-6203					2020	28	2					1197	1207		10.3906/elk-1907-234													
J								A New Representation of Intuitionistic Fuzzy Systems and Their Applications in Critical Decision Making	IEEE INTELLIGENT SYSTEMS										Fuzzy sets; Fuzzy systems; Decision making; Human computer interaction; complex numbers; decision making; intuitionistic fuzzy set; logical operations; evolutionary fuzzy systems	SET PAIR ANALYSIS; TOPSIS METHOD	In decision-making problem, fuzzy system is considered as an effective tool with access to uncertain information by fuzzy representations. Evolutionary fuzzy systems have been developed with the appearance of intuitionistic fuzzy, hesitant fuzzy, neutrosophic representations, etc. Moreover, by capturing compound features and convey multifaceted information, complex numbers are utilized to generalize fuzzy and intuitionistic fuzzy sets. However, the order relations established in these existing systems have certain limitations, such as they are not total order relations or they are defined based on intermediate functions, hence it is difficult to use in building and ensuring important properties of logical operators and distance measures in the systems. In this article, a representation of the intuitionistic fuzzy systems based on complex numbers (IFS-C) in the polar form by a new way is proposed to overcome the above restrictions. Specifically, an intuitionistic fuzzy set is characterized by the two functions of modulus and argument. A new order relation, set-theoretic operations, and a new distance measure by the polar form of IFS-C are defined and investigated. The applicability of the proposal is illustrated by a new decision-making model called P-distance measure. It is tested on the benchmark medical datasets in comparison with the existing methods. The experiments confirm the advantages of the proposal.																	1541-1672	1941-1294				JAN-FEB	2020	35	1					6	16		10.1109/MIS.2019.2938441													
J								Research on Road Traffic Situation Awareness System Based on Image Big Data	IEEE INTELLIGENT SYSTEMS										Human computer interaction; Convolutional neural networks; Computational modeling; Task analysis; Big Data; Biological neural networks; convolutional neural network; database; road traffic; situational awarenesss		Road traffic is an important component of the national economy and social life. Promoting intelligent and Informa ionization construction in the field of road traffic is conducive to the construction of smart cities and the formulation of macro strategies and construction plans for urban traffic development. Aiming at the shortcomings of the current road traffic system, this article, on the basis of combining convolution neural network, situational awareness technology, database and other technologies, takes the road traffic situational awareness system as the research object, and analyzes the information collection, processing, and analysis process of road traffic situational awareness system. Convolutional neural networks (CNN), region-CNN (R-CNN), fast R-CNN, and faster R-CNN are used for vehicle class classification and location identification in road image big data. The deep convolutional neural network model based on road traffic image big data was further established, and the system requirements analysis and system framework design and implementation were carried out. Through the analysis and trial of actual cases, the results show the application effect of the realized road traffic situational awareness system, which provides a scientific reference and basis for the establishment of modern intelligent transportation system.																	1541-1672	1941-1294				JAN-FEB	2020	35	1					18	25		10.1109/MIS.2019.2942836													
J								Decision Making in IoT Environment through Unsupervised Learning	IEEE INTELLIGENT SYSTEMS										Decision making; Cultural differences; Unsupervised learning; Principal component analysis; Intelligent systems; Internet of Things; Human computer interaction; Machine Learning; Clustering; Context-aware systems; Users behavioural monitoring; Decision Making		Nowadays, unsupervised learning can provide new perspectives to identify hidden patterns and classes inside the huge amount of data coming from the Internet of Things (IoT) world. Analyzing IoT data through machine learning techniques requires the use of mathematical algorithms, computational techniques, and an accurate tuning of the input parameters. In this article, we present a study of unsupervised learning techniques applied on IoT data to support decision-making processes inside intelligent environments. To assess the proposed approach, we discuss two case studies in which behavioral IoT data have been collected, also in a noninvasive way, in order to achieve an unsupervised classification that can be adopted during a decision-making process. The use of unsupervised learning techniques is acquiring a key role to complement the more traditional services with new decision-making ones supporting the needs of companies, stakeholders, and consumers.																	1541-1672	1941-1294				JAN-FEB	2020	35	1					27	34		10.1109/MIS.2019.2944783													
J								Battlefield Image Situational Awareness Application Based on Deep Learning	IEEE INTELLIGENT SYSTEMS										Convolution; Feature extraction; Training data; Deep learning; Object detection; Neural networks; Human computer interaction; Deep learning; Convolution neural network; YOLO; Situational awareness		With the rapid development of information technology, it has become an important topic to construct a situational awareness system that can independently mine data and information as well as perceive environmental situations by using deep learning. First, this article introduced the structure of convolutional neural networks (CNN) and You Only Look Once (YOLO) model. Then, it analyzed the structure and function of battlefield situational awareness system, and concluded that: in the whole situational awareness system, the discovery, category, and location analysis of situational elements, namely object target, is the foundation and key to realize the function. On this basis, this article establishes a battlefield situational awareness model based on the YOLO model. Finally, five common objects on the battlefield (helicopter gunship, missile, tank, soldier and gun) are classified and located, respectively. The YOLO model based on CNN is used to process the input image, and then the position, category, and corresponding confidence probability of all objects in the image are obtained directly, which realizes end-to-end learning, greatly improves the speed of target detection, and lays a foundation for assessing the battlefield situation.																	1541-1672	1941-1294				JAN-FEB	2020	35	1					36	42		10.1109/MIS.2019.2953685													
J								A Novel Siamese-Based Approach for Scene Change Detection With Applications to Obstructed Routes in Hazardous Environments	IEEE INTELLIGENT SYSTEMS										Decoding; Image segmentation; Semantics; Training data; Neural networks; Intelligent systems; Task analysis; Human computer interaction; Scene Change Detection; Siamese Convolutional Neural Networks; U-Nets; Route Obstruction Detection		The demand for automatic scene change detection has massively increased in the last decades due to its importance regarding safety and security issues. Although deep learning techniques have provided significant enhancements in the field, such methods must learn which object belongs to the foreground or background beforehand. In this article, we propose an approach that employs siamese U-Nets to address the task of change detection, such that the model learns to perform semantic segmentation using background reference frames only. Therefore, any object that comes up into the scene defines a change. The experimental results show the robustness of the proposed model over the well-known public dataset CDNet2014. Additionally, we also consider a private dataset called "PetrobrasROUTES," which comprises obstruction or abandoned objects in escape routes in hazardous environments. Moreover, the experiments show that the proposed approach is more robust to noise and illumination changes.																	1541-1672	1941-1294				JAN-FEB	2020	35	1					44	53		10.1109/MIS.2019.2949984													
J								Study on the Situational Awareness System of Mine Fire Rescue Using Faster Ross Girshick-Convolutional Neural Network	IEEE INTELLIGENT SYSTEMS										Human computer interaction; Convolutional neural networks; Big Data; Biological neural networks; Security; Analytical models; Task analysis; Big data; Mine fire rescue; CNN network; Situational awareness		With the continuous development of society, with the advent of the era of big data, situational awareness systems are gradually becoming well known and play an important role. Situational awareness systems are based on safe big data, and they are environmentally, dynamically, and holistically aware of security. A comprehensive system of risk capabilities. Therefore, this article uses the situational awareness system to study the rescue problem of mine fires, in order to reduce the casualties and economic losses caused by mine fires. On this basis, the convolutional neural network algorithm is used for situational awareness. By optimizing the algorithm, from region-based convolutional neural network (R-CNN) model to fast R-CNN model, the optimal model of faster R-CNN is finally proposed and implemented. The mine fire rescue problem.																	1541-1672	1941-1294				JAN-FEB	2020	35	1					54	61		10.1109/MIS.2019.2943850													
J								Adaptive Learning in Electricity Market Negotiations Based on Determinism Theory	IEEE INTELLIGENT SYSTEMS										Adaptation models; Computational modeling; Intelligent systems; Electricity supply industry; Prediction algorithms; Optimization; Analytical models	BEHAVIOR; MODEL	This research proposes a novel methodology for adaptive learning in electricity markets negotiations, based on the principles of the determinism theory. The determinism theory states that all events are predetermined due to the cause-effect rule. At the same time, it is unmanageable to consider all causes to a certain effect, making it impossible to predict future events. However, in a controlled simulation environment, it is possible to access and analyze all involved variables; thus, making the application of this theory promising in such environments. This research applies the principles of the determinism theory to a new learning methodology, which optimizes players' actions, considering the predicted behavior of all involved players, with the objective of maximizing market gains. A case-based reasoning approach is used, providing adaptive context-aware decision support. Results show that the proposed approach is able to achieve better market results than all reference market strategies.																	1541-1672	1941-1294				JAN-FEB	2020	35	1					62	72		10.1109/MIS.2019.2950903													
J								Recognizing Nested Named Entity Based on the Neural Network Boundary Assembling Model	IEEE INTELLIGENT SYSTEMS										Task analysis; Semantics; Feature extraction; Artificial neural networks; Immune system; Labeling; Human computer interaction		The task to recognize named entities is often modeled as a sequence labeling process, which selects a label path whose probability is maximum for an input sentence. Because it makes the assumption that the input sentence has a flattened structure, it often fails to recognize nested named entities. In our previous work, a boundary assembling (BA) model was proposed. It is a cascading framework, which identifies named entity boundaries first, and then assembles them into entity candidates for further assessment. This model is effective to recognize nested named entities, but still suffers from poor performance caused by the sparse feature problem. In this article, the BA model is remodeled with the advancement of neural networks, which enables the model to capture semantic information of a sentence by using word embeddings pretrained in external resources. In our experiments, it shows an impressive improvement on the final performance, outperforming the state of the art more than 17% in F-score.																	1541-1672	1941-1294				JAN-FEB	2020	35	1					74	81		10.1109/MIS.2019.2952334													
J								Intent Classification for Dialogue Utterances	IEEE INTELLIGENT SYSTEMS										Support vector machines; Training data; Intelligent systems; Machine learning; Affective computing; Sentiment analysis; Human computer interaction		In this work, we investigate several machine learning methods to tackle the problem of intent classification for dialogue utterances. We start with bag-of-words in combination with Naive Bayes. After that, we employ continuous bag-of-words coupled with support vector machines (SVM). Then, we follow long short-term memory (LSTM) networks, which are made bidirectional. The best performing model is hierarchical, such that it can take advantage of the natural taxonomy within classes. The main experiments are a comparison between these methods on an open sourced academic dataset. In the first experiment, we consider the full dataset. We also consider the given subsets of data separately, in order to compare our results with state-of-the-art vendor solutions. In general we find that the SVM models outperform the LSTM models. The former models achieve the highest macro-F1 for the full dataset, and in most of the individual datasets. We also found out that the incorporation of the hierarchical structure in the intents improves the performance.																	1541-1672	1941-1294				JAN-FEB	2020	35	1					82	88		10.1109/MIS.2019.2954966													
J								Social Signal Detection by Probabilistic Sampling DNN Training	IEEE TRANSACTIONS ON AFFECTIVE COMPUTING										Training; Probabilistic logic; Hidden Markov models; Task analysis; Training data; Neural networks; Speech recognition; Deep neural networks; instance sampling; social signals; laughter detection	ANTISOCIAL-BEHAVIOR; NETWORK; HARASSMENT; MIDDLE; RISK	When our task is to detect social signals such as laughter and filler events in an audio recording, the most straightforward way is to apply a Hidden Markov Model-or a Hidden Markov Model/Deep Neural Network (HMM/DNN) hybrid, which is considered state-of-the-art nowadays. In this hybrid model, the DNN component is trained on frame-level samples of the classes we are looking for. In such event detection tasks, however, the training labels are seriously imbalanced, as typically only a small fraction of the training data corresponds to these social signals, while the bulk of the utterances consists of speech segments or silence. A strong imbalance of the training classes is known to cause difficulties during DNN training. To alleviate these problems, here we apply the technique called probabilistic sampling, which seeks to balance the class distribution. Probabilistic sampling is a mathematically well-founded combination of upsampling and downsampling, which was found to outperform both of these simple resampling approaches. With this strategy, we managed to achieve a 7-8 percent relative error reduction both at the segment level and frame level, and we efficiently reduced the DNN training times as well.																	1949-3045					JAN-MAR	2020	11	1					3	24		10.1109/TAFFC.2018.2871450													
J								Feature Learning from Spectrograms for Assessment of Personality Traits	IEEE TRANSACTIONS ON AFFECTIVE COMPUTING										Automatic personality perception; spectrograms analysis; sparse coding; dictionary learning; feature learning; speaker trait classification; acoustic and prosodic modeling	SPARSE; CLASSIFICATION; RECOGNITION; PERCEPTION; TEXTURE	Several methods have recently been proposed to analyze speech and automatically infer the personality of the speaker. These methods often rely on prosodic and other hand crafted speech processing features extracted with off-the-shelf toolboxes. To achieve high accuracy, numerous features are typically extracted using complex and highly parameterized algorithms. In this paper, a new method based on feature learning and spectrogram analysis is proposed to simplify the feature extraction process while maintaining a high level of accuracy. The proposed method learns a dictionary of discriminant features from patches extracted in the spectrogram representations of training speech segments. Each speech segment is then encoded using the dictionary, and the resulting feature set is used to perform classification of personality traits. Experiments indicate that the proposed method achieves state-of-the-art results with an important reduction in complexity when compared to the most recent reference methods. The number of features, and difficulties linked to the feature extraction process are greatly reduced as only one type of descriptors is used, for which the 7 parameters can be tuned automatically. In contrast, the simplest reference method uses 4 types of descriptors to which 6 functionals are applied, resulting in over 20 parameters to be tuned.																	1949-3045					JAN-MAR	2020	11	1					25	31		10.1109/TAFFC.2017.2763132													
J								Dimensional Affect Recognition from HRV: An Approach Based on Supervised SOM and ELM	IEEE TRANSACTIONS ON AFFECTIVE COMPUTING										Heart rate variability; Physiology; Estimation; Sensors; Electrocardiography; Computational modeling; Physiological measures; affect sensing and analysis; supervised self-organization; extreme learning machines; dimensional affect estimation	EXTREME LEARNING MACHINES; HEART-RATE-VARIABILITY; EMOTION RECOGNITION; CLASSIFICATION; VALENCE; TRENDS; EEG	Dimensional affect recognition is a challenging topic and current techniques do not yet provide the accuracy necessary for HCI applications. In this work we propose two new methods. The first is a novel self-organizing model that learns from similarity between features and affects. This method produces a graphical representation of the multidimensional data which may assist the expert analysis. The second method uses extreme learning machines, an emerging artificial neural network model. Aiming for minimum intrusiveness, we use only the heart rate variability, which can be recorded using a small set of sensors. The methods were validated with two datasets. The first is composed of 16 sessions with different participants and was used to evaluate the models in a classification task. The second one was the publicly available Remote Collaborative and Affective Interaction (RECOLA) dataset, which was used for dimensional affect estimation. The performance evaluation used the kappa score, unweighted average recall and the concordance correlation coefficient. The concordance coefficient on the RECOLA test partition was 0.421 in arousal and 0.321 in valence. Results show that our models outperform state-of-the-art models on the same data and provides new ways to analyze affective states.																	1949-3045					JAN-MAR	2020	11	1					32	44		10.1109/TAFFC.2017.2763943													
J								Affective Recognition in Dynamic and Interactive Virtual Environments	IEEE TRANSACTIONS ON AFFECTIVE COMPUTING										Games; Databases; Feature extraction; Human computer interaction; Electroencephalography; Physiology; Virtual reality; Virtual reality; interactive environments; affective computing; affective VR; emotion-based affective physiological database	REALITY EXPOSURE THERAPY; PAIN-CONTROL; EMOTION; STRESS; HEART; AGE	The past decade has witnessed a significant increase in interest in human emotional behaviours in the future of interactive multimodal computing. Although much consideration has been given to non-interactive affective stimuli (e.g., images and videos), the recognition of emotions within interactive virtual environments has not received an equal level of attention. In the present study, a psychophysiological database, cataloguing the EEG, GSR and heart rate of 30 participants, exposed to an affective virtual environment, has been constructed. 743 features were extracted from the physiological signals. Then, by employing a feature selection technique, the dimensionality of the feature space was reduced to a smaller subset, containing only 30 features. Four classification techniques (KNN, SVM, Discriminant Analysis (DA) and Classification Tree) were employed to classify the affective psychophysiological database into four Affective Clusters (derived from a Valence-Arousal space) and eight Emotion Labels. By employing cross-validation techniques, the performances of more than a quarter of a million different classification settings (various window lengths, classifier settings, etc.) were investigated. The results suggested that the physiological signals could be employed to classify emotional experiences, with high precision. The KNN and SVM outperformed both Classification Tree and DA classifiers; with 97.01 percent and 92.84 percent mean accuracies, respectively.																	1949-3045					JAN-MAR	2020	11	1					45	62		10.1109/TAFFC.2017.2764896													
J								A Novel Technique to Develop Cognitive Models for Ambiguous Image Identification Using Eye Tracker	IEEE TRANSACTIONS ON AFFECTIVE COMPUTING										Feature extraction; Gaze tracking; Computational modeling; Data models; Cognition; Object recognition; Electronic mail; Eye tracking; object identification; scan-path; ambiguous image; eye fixation; ROI; classifier; precision; recall	MENTAL IMAGES; CLASSIFICATION; PERCEPTION; MOVEMENTS; ALGORITHM; FIXATION; TASK	Human behavior can be analyzed using Eye tracker. Thus, it is used for revealing the cognitive processes for object identification. Cognitive process is the mental ability for identification of what our eyes see. Vision with 20/20 sometimes may not reveal the purpose. In this study, ambiguous images are taken to observe the cognitive process in participants. During the perception of an object, a participant uses goal-directed search for identifying various objects. Dense gaze coordinates provide the region of interests and are considered as the target regions for object identification in ambiguous images. These data are used to develop cognitive models for identification of ambiguous images. Features such as, eye fixation, pupil diameter, fixation durations, moments of inertia, and polar moments are used for developing the cognitive model. Three different feature selection methods along with six different classifiers are used for the task of classification. The selection of a subset of features using hypothesis testing performed well, compared to principal component analysis based dimensionality reduction method. This study could be used in detecting whether a participants is lying or not while perceiving an ambiguous image.																	1949-3045					JAN-MAR	2020	11	1					63	77		10.1109/TAFFC.2017.2768026													
J								Continuous, Real-Time Emotion Annotation: A Novel Joystick-Based Analysis Framework	IEEE TRANSACTIONS ON AFFECTIVE COMPUTING										Videos; Usability; Tools; Standards; Two dimensional displays; Real-time systems; Support vector machines; Affective computing; emotion in human-computer interaction; tools and methods of annotation; time-series analysis; change-point analysis; pattern recognition		Emotion labels are usually obtained via either manual annotation, which is tedious and time-consuming, or questionnaires, which neglect the time-varying nature of emotions and depend on human's unreliable introspection. To overcome these limitations, we developed a continuous, real-time, joystick-based emotion annotation framework. To assess the same, 30 subjects each watched 8 emotion-inducing videos. They were asked to indicate their instantaneous emotional state in a valence-arousal (V-A) space, using a joystick. Subsequently, five analyses were undertaken: (i) a System Usability Scale (SUS) questionnaire unveiled the framework's excellent usability; (ii) MANOVA analysis of the mean V-A ratings and (iii) trajectory similarity analyses of the annotations confirmed the successful elicitation of emotions; (iv) Change point analysis of the annotations, revealed a direct mapping between emotional events and annotations, thereby enabling automatic detection of emotionally salient points in the videos; and (v) Support Vector Machines (SVM) were trained on classification of 5 second chunks of annotations as well as their change-points. The classification results confirmed that ratings patterns were cohesive across the participants. These analyses confirm the value, validity, and usability of our annotation framework. They also showcase novel tools for gaining greater insights into the emotional experience of the participants.																	1949-3045					JAN-MAR	2020	11	1					78	84		10.1109/TAFFC.2017.2772882													
J								Toward Constructing a Real-time Social Anxiety Evaluation System: Exploring Effective Heart Rate Features	IEEE TRANSACTIONS ON AFFECTIVE COMPUTING										Electrocardiography; Heart beat; Public speaking; Biomedical monitoring; Speech; Complexity theory; Heart rate; local Hurst exponents; psychophysiological reactivity; social anxiety	AROUSAL; MULTIFRACTALITY; RESPONSES; DYNAMICS; STRESS; MODEL	Social anxiety is a negative emotion which may impair the health of the heart and social functioning of an individual. This work analyzes the influence of social anxiety on the autonomic nerve control of the heart in two social exposure events: public speaking and thesis defending. In an experiment of public speaking, 59 human subjects were tested, and 11 conventional heartbeat measures and a heartbeat measure named the range of local Hurst exponents (RLHE) were evaluated for their capabilities to reveal the onset of social anxiety. Two-sample t-test between the baseline data and high anxiety data shows that social anxiety significantly reduces the complexity of the heartbeats. In an experiment of thesis defense, heartbeats data were acquired from nine graduate students. With the combination of three conventional features and the RLHE feature, a support vector machine classifier obtained true positive rate and true negative rate of 84.88 and 97.29 percent in the five-fold cross validation process of binary classification between high anxiety status and low anxiety status; the classifier also realized a generalization accuracy of 81.82 percent in detecting the high anxiety status in the thesis defense. A real-time anxiety monitoring system was established based on the above anxiety detecting method.																	1949-3045					JAN-MAR	2020	11	1					100	110		10.1109/TAFFC.2018.2792000													
J								Personalised, Multi-Modal, Affective State Detection for Hybrid Brain-Computer Music Interfacing	IEEE TRANSACTIONS ON AFFECTIVE COMPUTING										EEG; GSR; affective state detection; BCMI; personalised affective state detection	EMOTIONAL RESPONSES; RECOGNITION; CLASSIFICATION	Brain-computer music interfaces (BCMIs) may be used to modulate affective states, with applications in music therapy, composition, and entertainment. However, for such systems to work they need to be able to reliably detect their user's current affective state. We present a method for personalised affective state detection for use in BCMI. We compare it to a population-based detection method trained on 17 users and demonstrate that personalised affective state detection is significantly (p<0.01p<0.01) more accurate, with average improvements in accuracy of 10.2 percent for valence and 9.3 percent for arousal. We also compare a hybrid BCMI (a BCMI that combines physiological signals with neurological signals) to a conventional BCMI design (one based upon the use of only EEG features) and demonstrate that the hybrid design results in a significant (p<0.01p<0.01) 6.2 percent improvement in performance for arousal classification and a significant (p<0.01p<0.01) 5.9 percent improvement for valence classification.																	1949-3045					JAN-MAR	2020	11	1					111	124		10.1109/TAFFC.2018.2801811													
J								Facial Expression Recognition with Neighborhood-Aware Edge Directional Pattern (NEDP)	IEEE TRANSACTIONS ON AFFECTIVE COMPUTING										Image edge detection; Face recognition; Shape; Noise measurement; Encoding; Distortion; NEDP; neighborhood-aware edge directional pattern; template-orientation; shape representation; facial expression recognition	FACE; EIGENFACES	Currently available local feature descriptors used in facial expression recognition at times suffer from unstable feature descriptions, especially in the presence of weak and distorted edges due to noise, limiting their performances. We propose a novel local descriptor named Neighborhood-aware Edge Directional Pattern (NEDP) to overcome such limitations. Instead of relying solely on the local neighborhood to describe the feature around a pixel, as done by the existing local descriptors, NEDP examines the gradients at the target (center) pixel as well as its neighboring pixels to explore a wider neighborhood for the consistency of the feature in spite of the presence of subtle distortion and noise in local region. We introduce template-orientations for the neighboring pixels, which give importance to the gradients in consistent edge directions, prioritizing the specific neighbors falling in the direction of the local edge to represent the shape of the local textures, unambiguously. Moreover, due to the effective management of the featureless regions, no such region is erroneously encoded as a feature by NEDP. Experiments of the performances for person-independent recognition on benchmark expression datasets also show that NEDP performs better than other existing descriptors, and thereby, improves the overall performance of facial expression recognition.																	1949-3045					JAN-MAR	2020	11	1					125	137		10.1109/TAFFC.2018.2829707													
J								Haptic Expression and Perception of Spontaneous Stress	IEEE TRANSACTIONS ON AFFECTIVE COMPUTING										Haptic expression; haptic perception; spontaneous; stress	TOUCH; EMOTIONS; RESPONSES; TOOL	Previous studies about kinesthetic expressions of emotions are mainly based on acted expressions of emotions, which can be quite different from spontaneous expressions. This paper describes a study involving a stress induction procedure and stress perception through haptic expressions with total = 41 young men (aged 19 - 37 19-37), all right-handed. We designed a game application to collect spontaneous expressions of stress. This application involved haptic interactions and a stressful event. We observed changes in the haptic behaviors of the participants over different phases: before, during and after the stress induction. In the next step, we investigated how the collected haptic behaviors (both kinematic and force components) were haptically perceived by the other participants. The results suggest the ability of kinesthetic expressions to communicate a spontaneous stress from one person to another.																	1949-3045					JAN-MAR	2020	11	1					138	150		10.1109/TAFFC.2018.2830371													
J								Automatic Estimation of Taste Liking Through Facial Expression Dynamics	IEEE TRANSACTIONS ON AFFECTIVE COMPUTING										Videos; Acceleration; Gold; Databases; Face; Shape; Estimation; Taste liking; taste appreciation; facial expression dynamics; spontaneous expression; taste-induced expression	RECOGNITION; RESPONSES	The level of taste liking is an important measure for a number of applications such as the prediction of long-term consumer acceptance for different food and beverage products. Based on the fact that facial expressions are spontaneous, instant and heterogeneous sources of information, this paper aims to automatically estimate the level of taste liking through facial expression videos. Instead of using handcrafted features, the proposed approach deep learns the regional expression dynamics, and encodes them to a Fisher vector for video representation. Regional Fisher vectors are then concatenated, and classified by linear SVM classifiers. The aim is to reveal the hidden patterns of taste-elicited responses by exploiting expression dynamics such as the speed and acceleration of facial movements. To this end, we have collected the first large-scale beverage tasting database in the literature. The database has 2,970 videos of taste-induced facial expressions collected from 495 subjects. Our large-scale experiments on this database show that the proposed approach achieves an accuracy of 70.37 percent for distinguishing between three levels of taste-liking. Furthermore, we assess the human performance recruiting 45 participants, and show that humans are significantly less reliable for estimating taste appreciation from facial expressions in comparison to the proposed method.																	1949-3045					JAN-MAR	2020	11	1					151	162		10.1109/TAFFC.2018.2832044													
J								Using Temporal Features of Observers' Physiological Measures to Distinguish Between Genuine and Fake Smiles	IEEE TRANSACTIONS ON AFFECTIVE COMPUTING										Observers; Videos; Physiology; Databases; Electrocardiography; Feature extraction; Face; Affective computing; genuine smile; fake smile; physiological signals; temporal features		Future affective computing research could be enhanced by enabling the computer to recognise a displayer's mental state from an observer's reaction (measured by physiological signals), using this information to improve recognition algorithms, and eventually to computer systems which are more responsive to human emotions. In this paper, an observer's physiological signals are analysed to distinguish displayers' genuine from fake smiles. Overall, thirty smile videos were collected from four benchmark database and classified as showing genuine or fake smiles. Overall, forty observers viewed videos. We generally recorded four physiological signals: pupillary response (PR), electrocardiogram (ECG), galvanic skin response (GSR), and blood volume pulse (BVP). A number of temporal features were extracted after a few processing steps, and minimally correlated features between genuine and fake smiles were selected using the NCCA (canonical correlation analysis with neural network) system. Finally, classification accuracy was found to be as high as 98.8 percent from PR features using a leave-one-observer-out process. In comparison, the best current image processing technique [1] on the same video data was 95 percent correct. Observers were 59 percent (on average) to 90 percent (by voting) correct by their conscious choices. Our results demonstrate that humans can non-consciously (or emotionally) recognise the quality of smiles 4 percent better than current image processing techniques and 9 percent better than the conscious choices of groups.																	1949-3045					JAN-MAR	2020	11	1					164	177		10.1109/TAFFC.2018.2878029													
J								SIF: Self-Inspirited Feature Learning for Person Re-Identification	IEEE TRANSACTIONS ON IMAGE PROCESSING										Feature extraction; Training; Optimization; Task analysis; Computational modeling; Data mining; Semantics; Person re-identification; representation learning; convolutional neural network; image retrieval	NETWORK	The re-identification (ReID) task has received increasing studies in recent years and its performance has gained significant improvement. The progress mainly comes from searching for new network structures to learn person representations. However, limited efforts have been made to explore the potential performance of existing ReID networks directly by better training scheme, which leaves a large space for ReID research. In this paper, we propose a Self-Inspirited Feature Learning (SIF) method to enhance the performance of given ReID networks from the viewpoint of optimization. We design a simple adversarial learning scheme to encourage a network to learn more discriminative person representation. In our method, an auxiliary branch is added into the network only in the training stage, while the structure of the original network stays unchanged during the testing stage. In summary, SIF has three aspects of advantages: 1) it is designed under general setting; 2) it is compatible with many existing feature learning networks on the ReID task; 3) it is easy to implement and has steady performance. We evaluate the performance of SIF on three public ReID datasets: Market1501, DuckMTMC-reID, and CUHK03(both labeled and detected). The results demonstrate significant improvement in performance brought by SIF. We also apply SIF to obtain state-of-the-art results on all the three datasets. Specifically, mAP / Rank-1 accuracy are: 87.6%/95.2% (without re-rank) on Market1501, 79.4%/89.8% on DuckMTMC-reID, 77.0%/79.5% on CUHK03 (labeled) and 73.9%/76.6% on CUHK03 (detected), respectively.																	1057-7149	1941-0042					2020	29						4942	4951		10.1109/TIP.2020.2975712													
J								Color Image Demosaicing Using Progressive Collaborative Representation	IEEE TRANSACTIONS ON IMAGE PROCESSING										Training; Image color analysis; Collaboration; Rough surfaces; Color; Feature extraction; Image quality; Image demosaicing; color filter array (CFA); residual interpolation; progressive collaborative representation	INTERPOLATION	In this paper, a progressive collaborative representation (PCR) framework is proposed that is able to incorporate any existing color image demosaicing method for further boosting its demosaicing performance. Our PCR consists of two phases: (i) offline training and (ii) online refinement. In phase (i), multiple training-and-refining stages will be performed. In each stage, a new dictionary will be established through the learning of a large number of feature-patch pairs, extracted from the demosaicked images of the current stage and their corresponding original full-color images. After training, a projection matrix will be generated and exploited to refine the current demosaicked image. The updated image with improved image quality will be used as the input for the next training-and-refining stage and performed the same processing likewise. At the end of phase (i), all the projection matrices generated as above-mentioned will be exploited in phase (ii) to conduct online demosaicked image refinement of the test image. Extensive simulations conducted on two commonly-used test datasets (i.e., IMAX and Kodak) for evaluating the demosaicing algorithms have clearly demonstrated that our proposed PCR framework is able to constantly boost the performance of any image demosaicing method we experimented, in terms of objective and subjective performance evaluations.																	1057-7149	1941-0042					2020	29						4952	4964		10.1109/TIP.2020.2975978													
J								A Blind Multiscale Spatial Regularization Framework for Kernel-Based Spectral Unmixing	IEEE TRANSACTIONS ON IMAGE PROCESSING										Hyperspectral data; multiscale; spatial regularization; nonlinear unmixing; kernel methods	ENDMEMBER VARIABILITY; IMAGE-RESTORATION; HYPERSPECTRAL IMAGES; MIXTURE ANALYSIS; PARAMETER; INFORMATION; ALGORITHM; NOISE	Introducing spatial prior information in hyperspectral imaging (HSI) analysis has led to an overall improvement of the performance of many HSI methods applied for denoising, classification, and unmixing. Extending such methodologies to nonlinear settings is not always straightforward, specially for unmixing problems where the consideration of spatial relationships between neighboring pixels might comprise intricate interactions between their fractional abundances and nonlinear contributions. In this paper, we consider a multiscale regularization strategy for nonlinear spectral unmixing with kernels. The proposed methodology splits the unmixing problem into two sub-problems at two different spatial scales: a coarse scale containing low-dimensional structures, and the original fine scale. The coarse spatial domain is defined using superpixels that result from a multiscale transformation. Spectral unmixing is then formulated as the solution of quadratically constrained optimization problems, which are solved efficiently by exploring their strong duality and a reformulation of their dual cost functions in the form of root-finding problems. Furthermore, we employ a theory-based statistical framework to devise a consistent strategy to estimate all required parameters, including both the regularization parameters of the algorithm and the number of superpixels of the transformation, resulting in a truly blind (from the parameters setting perspective) unmixing method. Experimental results attest the superior performance of the proposed method when comparing with other, state-of-the-art, related strategies.																	1057-7149	1941-0042					2020	29						4965	4979		10.1109/TIP.2020.2978342													
J								DDcGAN: A Dual-Discriminator Conditional Generative Adversarial Network for Multi-Resolution Image Fusion	IEEE TRANSACTIONS ON IMAGE PROCESSING										Image fusion; generative adversarial network; infrared image; medical image; different resolutions	MULTISCALE DECOMPOSITION; VISIBLE IMAGES; TRANSFORM; ALGORITHM; WAVELET	In this paper, we proposed a new end-to-end model, termed as dual-discriminator conditional generative adversarial network (DDcGAN), for fusing infrared and visible images of different resolutions. Our method establishes an adversarial game between a generator and two discriminators. The generator aims to generate a real-like fused image based on a specifically designed content loss to fool the two discriminators, while the two discriminators aim to distinguish the structure differences between the fused image and two source images, respectively, in addition to the content loss. Consequently, the fused image is forced to simultaneously keep the thermal radiation in the infrared image and the texture details in the visible image. Moreover, to fuse source images of different resolutions, e.g., a low-resolution infrared image and a high-resolution visible image, our DDcGAN constrains the downsampled fused image to have similar property with the infrared image. This can avoid causing thermal radiation information blurring or visible texture detail loss, which typically happens in traditional methods. In addition, we also apply our DDcGAN to fusing multi-modality medical images of different resolutions, e.g., a low-resolution positron emission tomography image and a high-resolution magnetic resonance image. The qualitative and quantitative experiments on publicly available datasets demonstrate the superiority of our DDcGAN over the state-of-the-art, in terms of both visual effect and quantitative metrics. Our code is publicly available at https://github.com/jiayi-ma/DDcGAN.																	1057-7149	1941-0042					2020	29						4980	4995		10.1109/TIP.2020.2977573													
J								Multi-Objective Matrix Normalization for Fine-Grained Visual Recognition	IEEE TRANSACTIONS ON IMAGE PROCESSING										Visualization; Graphics processing units; Feature extraction; Convergence; Optimization; Covariance matrices; Training; Fine-grained visual recognition; bilinear pooling; matrix normalization; multi-objective optimization		Bilinear pooling achieves great success in fine-grained visual recognition (FGVC). Recent methods have shown that the matrix power normalization can stabilize the second-order information in bilinear features, but some problems, e.g., redundant information and over-fitting, remain to be resolved. In this paper, we propose an efficient Multi-Objective Matrix Normalization (MOMN) method that can simultaneously normalize a bilinear representation in terms of square-root, low-rank, and sparsity. These three regularizers can not only stabilize the second-order information, but also compact the bilinear features and promote model generalization. In MOMN, a core challenge is how to jointly optimize three non-smooth regularizers of different convex properties. To this end, MOMN first formulates them into an augmented Lagrange formula with approximated regularizer constraints. Then, auxiliary variables are introduced to relax different constraints, which allow each regularizer to be solved alternately. Finally, several updating strategies based on gradient descent are designed to obtain consistent convergence and efficient implementation. Consequently, MOMN is implemented with only matrix multiplication, which is well-compatible with GPU acceleration, and the normalized bilinear features are stabilized and discriminative. Experiments on five public benchmarks for FGVC demonstrate that the proposed MOMN is superior to existing normalization-based methods in terms of both accuracy and efficiency. The code is available: https://github.com/mboboGO/MOMN.																	1057-7149	1941-0042					2020	29						4996	5009		10.1109/TIP.2020.2977457													
J								Bayesian High Resolution Range Profile Reconstruction of High-Speed Moving Target From Under-Sampled Data	IEEE TRANSACTIONS ON IMAGE PROCESSING										High resolution range profile (HRRP); inverse synthetic aperture radar (ISAR) imaging; under-sampled data; velocity estimation; variational Bayesian inference; entropy minimization; Newton method	SPARSE SIGNAL RECOVERY; ROTATING TARGETS; ISAR; MOTION	Obtained by wide band radar system, high resolution range profile (HRRP) is the projection of scatterers of target to the radar line-of-sight (LOS). HRRP reconstruction is unavoidable for inverse synthetic aperture radar (ISAR) imaging, and of particular usage for target recognition, especially in cases that the ISAR image of target is not able to be achieved. For the high-speed moving target, however, its HRRP is stretched by the high order phase error. To obtain well-focused HRRP, the phase error induced by target velocity should be compensated, utilizing either measured or estimated target velocity. Noting in case of under-sampled data, the traditional velocity estimation and HRRP reconstruction algorithms become invalid, a novel HRRP reconstruction of high-speed target for under-sampled data is proposed. The Laplacian scale mixture (LSM) is used as the sparse prior of HRRP, and the variational Bayesian inference is utilized to derive its posterior, so as to reconstruct it with high resolution from the under-sampled data. Additionally, during the reconstruction of HRRP, the target velocity is estimated via joint constraint of entropy minimization and sparseness of HRRP to compensate the high order phase error brought by the target velocity to concentrate HRRP. Experimental results based on both simulated and measured data validate the effectiveness of the proposed Bayesian HRRP reconstruction algorithm.																	1057-7149	1941-0042					2020	29						5110	5120		10.1109/TIP.2020.2980149													
J								NLH: A Blind Pixel-Level Non-Local Method for Real-World Image Denoising	IEEE TRANSACTIONS ON IMAGE PROCESSING										Non-local self similarity; pixel-level similarity; image denoising	SPARSE REPRESENTATION; NOISE	Non-local self similarity (NSS) is a powerful prior of natural images for image denoising. Most of existing denoising methods employ similar patches, which is a patch-level NSS prior. In this paper, we take one step forward by introducing a pixel-level NSS prior, i.e., searching similar pixels across a non-local region. This is motivated by the fact that finding closely similar pixels is more feasible than similar patches in natural images, which can be used to enhance image denoising performance. With the introduced pixel-level NSS prior, we propose an accurate noise level estimation method, and then develop a blind image denoising method based on the lifting Haar transform and Wiener filtering techniques. Experiments on benchmark datasets demonstrate that, the proposed method achieves much better performance than previous non-deep methods, and is still competitive with existing state-of-the-art deep learning based methods on real-world image denoising. The code is publicly available at https://github.com/njusthyk1972/NLH.																	1057-7149	1941-0042					2020	29						5121	5135		10.1109/TIP.2020.2980116													
J								Orthogonal Directional Transforms Using Discrete Directional Laplacian Eigen Solutions for Beyond HEVC Intra Coding	IEEE TRANSACTIONS ON IMAGE PROCESSING										Laplace equations; Discrete cosine transforms; Standards; Image coding; Two dimensional displays; Transform coding; Video coding; directional bases; laplace equations; discrete cosine transforms		We introduce new transforms for efficient compression of image blocks with directional preferences. Each transform, which is an orthogonal basis for a specific direction, is constructed from an eigen-decomposition of a discrete directional Laplacian system matrix. The method is a natural extension of the DCT, expressing the Laplacian in Cartesian coordinates rotated to some predetermined angles. Symmetry properties of the transforms over square domains lead to efficient computation and compact storage of the directional transforms. A version of the directional transforms was implemented within the beyond HEVC software and demonstrated significant improvement for intra block coding.																	1057-7149	1941-0042					2020	29						5136	5146		10.1109/TIP.2020.2977863													
J								Boosting Structure Consistency for Multispectral and Multimodal Image Registration	IEEE TRANSACTIONS ON IMAGE PROCESSING										Multispectral image; multimodal image; image registration; structural consistency boosting; similarity enhancement; common measures; gradient descent; image pyramid; optimization	DESCRIPTOR; FLASH	Multispectral imaging plays a vital role in the area of computer vision and computational photography. As spectral band images can be misaligned due to imaging device movement or alternation, image registration is necessary to avoid spectral information distortion. The current registration measures specialized for multispectral data are typically robust yet complex, requiring excessive computation. The common measures such as sum of squared differences (SSD) and sum of absolute differences (SAD) are computationally efficient whereas they perform poorly on multispectral data. To cope with this challenge, we propose a structure consistency boosting (SCB) transform that aims at boosting the structural similarity of multispectral images. With SCB, the common measures can be employed for multispectral image registration. The SCB transform exploits the fact that inherent edge structures maintain relative saliency locally despite the nonlinear variation between band images. A statistical prior of the natural image, which is based on the gradient-intensity correlation, is explored to build a parametric form of SCB. Experimental results validate that the SCB transform outperforms current similarity enhancement algorithms, and performs better than the state-of-the-art multispectral registration measures. Thanks to the generality of the statistical prior, the SCB transform is also applicable to various multimodal data such as flash/no-flash images and medical images.																	1057-7149	1941-0042					2020	29						5147	5162		10.1109/TIP.2020.2980972													
J								Pose-Based View Synthesis for Vehicles: A Perspective Aware Method	IEEE TRANSACTIONS ON IMAGE PROCESSING										Novel view synthesis; generative adversarial nets; perspective transformation; generative model; vehicle pose		In this paper, we focus on the problem of novel view synthesis for vehicles. Some previous works solve the problem of novel view synthesis in a controlled 3D environment by exploiting additional 3D details (i.e., camera viewpoints and underlying 3D models). However, in real scenarios, the 3D details are difficult to obtain. In this case, we find that introducing vehicle pose to represent the views of vehicles is an alternative paradigm to solve the lack of 3D details. In novel view synthesis, preserving local details is one of the most challenging problems. To address this problem, we propose a perspective-aware generative model (PAGM). We are motivated by the prior that vehicles are made of quadrilateral planes. Preserving these rigid planes during image generation ensures that image details are kept. To this end, a classic image transformation method is leveraged, i.e., perspective transformation. In our GAN-based system, the perspective transformation is applied to the encoder feature maps, and the resulting maps are regarded as new conditions for the decoder. This strategy preserves the quadrilateral planes all the way through the network, thus shuttling the texture details from the input image to the generated image. In the experiments, we show that PAGM can generate high-quality vehicle images with fine details. Quantitatively, our method is superior to several competing approaches employing either GAN or the perspective transformation. Code is available at: https://github.com/ilvkai/view-synthesis-for-vehicles																	1057-7149	1941-0042					2020	29						5163	5174		10.1109/TIP.2020.2980130													
J								Small Object Augmentation of Urban Scenes for Real-Time Semantic Segmentation	IEEE TRANSACTIONS ON IMAGE PROCESSING										Semantic segmentation; scene understanding; autonomous driving; synthetic dataset	NETWORK	Semantic segmentation is a key step in scene understanding for autonomous driving. Although deep learning has significantly improved the segmentation accuracy, current high-quality models such as PSPNet and DeepLabV3 are inefficient given their complex architectures and reliance on multi-scale inputs. Thus, it is difficult to apply them to real-time or practical applications. On the other hand, existing real-time methods cannot yet produce satisfactory results on small objects such as traffic lights, which are imperative to safe autonomous driving. In this paper, we improve the performance of real-time semantic segmentation from two perspectives, methodology and data. Specifically, we propose a real-time segmentation model coined Narrow Deep Network (NDNet) and build a synthetic dataset by inserting additional small objects into the training images. The proposed method achieves 65.7% mean intersection over union (mIoU) on the Cityscapes test set with only 8.4G floating-point operations (FLOPs) on $1024\times 2048$ inputs. Furthermore, by re-training the existing PSPNet and DeepLabV3 models on our synthetic dataset, we obtained an average 2% mIoU improvement on small objects.																	1057-7149	1941-0042					2020	29						5175	5190		10.1109/TIP.2020.2976856													
J								Multi-Target Multi-Camera Tracking by Tracklet-to-Target Assignment	IEEE TRANSACTIONS ON IMAGE PROCESSING										Multi-camera racking; multi-target tracking; non-negative matrix factorization; tracklet association	OBJECT TRACKING; VISUAL TRACKING; SIGMA SET; PEOPLE	This paper focuses on the Multi-Target Multi-Camera Tracking task (MTMCT), which aims at tracking multiple targets within a multi-camera network. As the trajectory of each target is inherently split into multiple sub-trajectories (namely local tracklets) in a multi-camera network, a major challenge of MTMCT is how to accurately match the local tracklets generated within each camera across different cameras and generate a complete global trajectory for each target, i.e., the cross-camera tracklet matching problem. We solve the cross-camera tracklet matching problem by TRACklet-to-Target Assignment (TRACTA), and propose the Restricted Non-negative Matrix Factorization (RNMF) algorithm to compute the optimal assignment solution that meets a set of constraints, which should be in force in practice. TRACTA can correct the tracking errors caused by occlusions and missed detections in local tracklets, and produce a complete global trajectory for each target across all the cameras. Moreover, we also develop an analytical way of estimating the total number of targets in the camera network, which plays an important role to compute the tracklet-to-target assignment. Experimental evaluations and ablation studies on four MTMCT benchmark datasets show the superiority of the proposed TRACTA method.																	1057-7149	1941-0042					2020	29						5191	5205		10.1109/TIP.2020.2980070													
J								A Novel Flash P2P Network Traffic Prediction Algorithm based on ELMD and Garch	INTERNATIONAL JOURNAL OF INFORMATION TECHNOLOGY & DECISION MAKING										Self-similarity; ELMD; LMD; ARMA; GARCH; traffic prediction	EMPIRICAL MODE DECOMPOSITION; ENSEMBLE; IMPACT	To improve the quality of service and network performance for the Flash P2P video-on-demand, the prediction Flash P2P network traffic flow is beneficial for the control of the network video traffic. In this paper, a novel prediction algorithm to forecast the traffic rate of Flash P2P video is proposed. This algorithm is based on the combination of the ensemble local mean decomposition (ELMD) and the generalized autoregressive conditional heteroscedasticity (GARCH). The ELMD is used to decompose the original long-related flow into the summation of the short-related flow. Then, the GRACH is utilized to predict the short-related flow. The developed algorithm is tested in a university's campus network. The predicted results show that our proposed method can further achieve higher accuracy than those obtained by existing algorithms, such as GARCH and Local Mean Decomposition and Generalized AutoRegressive Conditional Heteroskedasticity (LMD-GARCH) while keeping lower computational complexity.																	0219-6220	1793-6845				JAN	2020	19	1					127	141		10.1142/S0219622019500469													
J								An Extended VIKOR Method for Multiple Attribute Decision Making with Linguistic D Numbers Based on Fuzzy Entropy	INTERNATIONAL JOURNAL OF INFORMATION TECHNOLOGY & DECISION MAKING										Multiple attribute decision making (MADM); linguistic D numbers (LDNs); VIKOR method; fuzzy entropy	MCDM	The linguistic D numbers (LDNs) can express the fuzzy evaluation information more easily and precisely by combining the advantages of linguistic terms (LTs) and D numbers (DNs). Existing researches on the fuzzy entropy of LDNs are rare, and most of the definitions of fuzzy entropy for LDNs are unreasonable. In view of this research gap, this paper improves the definition of fuzzy entropy of LDNs, which simultaneously considers the effects of confidence degrees and LTs on the value of fuzzy entropy in LDNs. Then, the weights of attributes can be calculated by the improved fuzzy entropy. Further, a new combination rule for LDNs is also presented in this paper. Based on these studies, we extend the traditional Vlsekriterijumska Optimizacija I Kompromisno Resenje (VIKOR) method to the LDNs and develop the LD-VIKOR method. The proposed LD-VIKOR method is convenient to handle MADM problems with unknown attributes weights under the environment of LDNs. Finally, we verify the validity and reliability of the proposed method by an illustrative example, and analyze the advantages of the proposed method by comparing it with other existing MADM methods.																	0219-6220	1793-6845				JAN	2020	19	1					143	167		10.1142/S0219622019500433													
J								Dependence Structure Analysis and VaR Estimation Based on China's and International Gold Price: A Copula Approach	INTERNATIONAL JOURNAL OF INFORMATION TECHNOLOGY & DECISION MAKING										Gold price; dependence break; GARCH; copula; value-at-risk	DYNAMIC CONDITIONAL CORRELATION; TIME LAG STRUCTURE; NONPARAMETRIC DETERMINATION; CROSS-CORRELATIONS; COMMODITY FUTURES; STOCK MARKETS; SAFE HAVEN; WORLD; MODELS; SERIES	Since 2013, China has become the world's largest gold producer and consumer. To gain the corresponding global pricing power in gold, many actions have been taken by China in recent years, including the International Board at Shanghai Gold Exchange, Shanghai-Hong Kong Gold Connect and Shanghai Gold Fix. Our work studies the dependence structure between China's and international gold price and examines whether these moves are changing the dependence structure. We use GARCH-copula models to detect the dynamic dependence and tail dependence. The research period is set to contain the Financial Crisis in 2008, the dramatical plunge of gold price in 2013 and a series of black swan events in 2016. The empirical study shows that some event driven dependence structure breaks are statistically insignificant. And the time-varying Symmetrized Joe-Clayton copula is the best copula to model the dependence structure based on AIC value. Finally, an example of applications of this dependence structure is given by estimating the VaR of an equally weighted portfolio with a simulation-based method.																	0219-6220	1793-6845				JAN	2020	19	1					169	193		10.1142/S0219622019500445													
J								Subset Selection Using Frequency Decomposition with Applications	INTERNATIONAL JOURNAL OF INFORMATION TECHNOLOGY & DECISION MAKING										Subset selection; frequency analysis; big data analytics; multi-factor model	ALGORITHMS; WRAPPER; FILTER	In time series modeling, one problem is to identify a small number of influential factors to explain variations in the variable of interest. With a vast number of possible factors available, suitable features need to be identified to yield multi-factor models with good explanatory power. In this paper, we propose a novel subset selection method which makes use of the properties in the frequency domain environment. The proposed system ensures key patterns in the target variable be sought and suitable factors be selected based on frequency peaks in common. It can perform well even when the number of factors is significantly greater than the sample size. Moreover, a very important feature of the proposed system is the capability of handling factors with different timeframes, which is lacking in existing methods. We demonstrate the system via several examples with dataset from finance, economic, road traffic and air pollution.																	0219-6220	1793-6845				JAN	2020	19	1					195	220		10.1142/S0219622019500500													
J								Modeling Uncertainty in the Wings Method Using Interval Arithmetic	INTERNATIONAL JOURNAL OF INFORMATION TECHNOLOGY & DECISION MAKING										Decision analysis; group decision-making; interval arithmetic; MCDM; uncertainty; weighted influence nonlinear gauge system (WINGS)	DECISION-MAKING METHODS; SUPPLIER SELECTION	This paper introduces a novel approach to support decision-making by combining the Weighted Influence Nonlinear Gauge System (WINGS) method with interval arithmetic. This approach allows to include uncertain judgments and/or different opinions in a decision process. Our research aims at increasing the ability of WINGS to model decisions in situations of uncertainty and at extending the reach of its practical applications. The new, relatively simple and transparent method can become a useful and practical tool for the decision makers. Mathematical correctness of the proposed methodology is proven. Based on the new method, a procedure for solving a complex decision problem is created. Its applicability is illustrated by two case studies. Choosing the best option for the organization's competitive position in a health-care organization shows how the proposed method works with uncertain judgments. Its usefulness for group decision-making is illustrated by applying it to a decision concerning allocation of public funds for sport development in a small commune.																	0219-6220	1793-6845				JAN	2020	19	1					221	240		10.1142/S0219622019500494													
J								Adapted Visual Analytics Process for Intelligent Decision-Making: Application in a Medical Context	INTERNATIONAL JOURNAL OF INFORMATION TECHNOLOGY & DECISION MAKING										Decision support system; data mining; visual analytics; knowledge; pattern	KDD-BASED DSS; KNOWLEDGE GENERATION; SUPPORT; DESIGN; VISUALIZATION; MODEL; IMPLEMENTATION; SCIENCE; SYSTEM; RULE	The theoretical and practical researches on Visual Analytics for intelligent decision-making tasks have remarkably advanced in the past few years. Intelligent Decision Support Systems (IDSS) introduce effective and efficient paths from raw data to decision by involving visualization and data mining technologies. Data mining-based DSS produces potentially interesting patterns from data. The transition from extracted patterns to knowledge is a delicate task. In this context, we propose to adapt a common visual analytics process for creating a path that enables the user (decision-maker) to automatically explore and visually extract insights by interacting with the patterns. This proposal is inspired from integrating traditional visual analytics concepts with the mental model of knowledge visualization. The idea is to combine an automatic and visual analysis of patterns to generate knowledge for the purpose of decision-making. To validate our proposal, we have applied it to a medical case study for the fight against Nosocomial Infections in Intensive Care Units. The developed platform was evaluated according to the utility and usability dimensions.																	0219-6220	1793-6845				JAN	2020	19	1					241	282		10.1142/S0219622019500470													
J								LAMDA-HAD, an Extension to the LAMDA Classier in the Context of Supervised Learning	INTERNATIONAL JOURNAL OF INFORMATION TECHNOLOGY & DECISION MAKING										LAMDA; fuzzy classication; supervised learning; adequacy degree	FUZZY CLASSIFICATION; ALGORITHM; METHODOLOGY; HYBRID; MODELS	This paper proposes a new approach to improve the performance of Learning Algorithm for Multivariable Data Analysis (LAMDA). This algorithm can be used for supervised and unsupervised learning, based on the calculation of the Global Adequacy Degree (GAD) of one individual to a class, through the contributions of all its descriptors. LAMDA has the capability of creating new classes after the training stage. If an individual does not have enough similarity to the preexisting classes, it is evaluated with respect to a threshold called the Non-Informative Class (NIC), this being the novelty of the algorithm. However, LAMDA has problems making good classications, either because the NIC is constant for all classes, or because the GAD calculation is unreliable. In this work, its efficiency is improved by two strategies, the first one, by the calculation of adaptable NICs for each class, which prevents that correctly classified individuals create new classes; and the second one, by computing the Higher Adequacy Degree (HAD), which grants more robustness to the algorithm. LAMDA-HAD is validated by applying it in different benchmarks and comparing it with LAMDA and other classifiers, through a statistical analysis to determinate the cases in which our algorithm presents a better performance.																	0219-6220	1793-6845				JAN	2020	19	1					283	316		10.1142/S0219622019500457													
J								An Approach using Multicriteria Decision Methods to Barges Conguration for Pushed Convoys in the Amazon	INTERNATIONAL JOURNAL OF INFORMATION TECHNOLOGY & DECISION MAKING										Pushed convoys design; Amazon; logistics; multiple criteria decision making; AHP-DEMATEL; ELECTRE IV	PERFORMANCE EVALUATION; SUCCESS FACTORS; DEMATEL METHOD; AHP; ENVIRONMENT; MANAGEMENT; SYSTEM; VIKOR; MCDM	The Amazon region has the largest hydrographic basin in the world and a favorable geographical location as a strategic export point but does not satisfactorily exploit its potential for transport and cargo flow , mainly as a diffierential factor of logistic competitiveness. For the waterway mode, cargo transportation is predominantly carried out by pushed convoys. The objective of this research is to evaluate and select, the best river train for the outflow of iron ore on the Maraba - Vila do Conde stretch, through a multiple criteria decision-making approach. To meet the objectives, a methodological framework is proposed using a hybrid AHP-DEMATEL method, based on a statistical analysis and to guarantee the reliability of the results for the proposed alternatives and not only considering the preference function, the ELECTRE method was used and also a parametric comparison of the AHP results. The main goals of this work were achieved and it was possible to infer that the combination of these methods has, in addition to the evident efficacy, a high reliability in the water transport area, being able to be used with accuracy of diverse forms in the academic area, as well as in industrial applications. Therefore, it can be affirmed that the best-pushed convoy configuration for the iron ore runoff in the Maraba - Vila do Conde stretch is composed by six barges in the arrangement of two columns and three rows with the measurements, by barge, with length of 60.96 m; beam of 13.75 m; and a 5m molded depth.																	0219-6220	1793-6845				JAN	2020	19	1					317	341		10.1142/S0219622019500482													
J								A Correlation-Based TOPSIS Method for Multiple Attribute Decision Making with Single-Valued Neutrosophic Information	INTERNATIONAL JOURNAL OF INFORMATION TECHNOLOGY & DECISION MAKING										Single-valued neutrosophic set; TOPSIS; correlation coefficient; MADM	HESITANT FUZZY-SETS; CORRELATION-COEFFICIENTS; AGGREGATION OPERATORS; DISTANCE MEASURES	The single-valued neutrosophic set (SVNS) is considered as an attractive tool for handling highly uncertain and vague information. With this regard, diffierent from the most current distance-based technique for order preference by similarity to ideal solution (TOPSIS) methods, this study proposes a correlation-based TOPSIS model for addressing the single-valued neutrosophic (SVN) multiple attribute decision making (MADM) problems. To achieve this aim, we first develop a novel conception of SVN correlation coefficient, whose significant feature is that it lies in the interval [-1,1], which is in accordance with the classical correlation coefficient in statistics, whereas all the existing SVN correlation coefficients in the literature are within unit interval [0,1]. Afterwards, a weighted SVN correlation coefficient is also introduced to infuse the importance of attributes. Moreover, a correlation-based comprehensive index is further proposed to establish the central structure of TOPSIS model, called the SVN correlation-based TOPSIS approach. Finally, a numerical example and relevant comparative analysis are implemented to explain the applicability and effectiveness of the mentioned methodology.																	0219-6220	1793-6845				JAN	2020	19	1					343	358		10.1142/S0219622019500512													
J								A Study of Periodic Motions in Homogeneous Nonlinear Multivariable Systems Written in the Polynomial Vector-Matrix Representation	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL												In this paper, we suggest a method for estimating the parameters of periodic motions and also a frequency-domain criterion of their stability for nonlinear time-invariant homogeneous (identical) multivariable automatic control systems with structurally identical subsystems. This method can be used to simply determine the parameters of periodic motions and their stability in the class of systems mentioned above based on the structural decomposition and the method of harmonic linearization. The results are confirmed by mathematical modeling.																	1064-2307	1555-6530				JAN	2020	59	1					1	7		10.1134/S1064230719060078													
J								Sufficient Conditions of Interval Stability of a Class of Linear Impulsive Systems with a Delay	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL											DIFFERENTIAL-EQUATIONS; ROBUST STABILITY; INSTABILITY; CRITERION	A class of linear periodic impulsive systems with a delay is considered. Sufficient conditions of asymptotic stability are established for systems of this class, reducing the study of the original system's stability to a similar problem for a system of linear impulsive differential equations (without a delay). This auxiliary result is used to study the interval stability of a linear impulsive system with a delay under general assumptions about the dynamic properties of continuous and discrete components. Examples are given to illustrate the effectiveness of the obtained results.																	1064-2307	1555-6530				JAN	2020	59	1					8	18		10.1134/S1064230719060145													
J								Controlling the Spectrum of Linear Completely Regular Differential-Algebraic Systems with Delays	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL											MODAL CONTROLLABILITY; NEUTRAL TYPE; TIME-DELAY; ASSIGNMENT PROBLEM; STABILIZATION; CRITERIA; STABILITY	For linear autonomous completely regular differential-algebraic systems with commensurable delays in the state and control, we study the control problem for the spectrum, treating it as follows: each characteristic quasi-polynomial given in advance is assigned to the system by closing it by a differential-difference controller. We formulate modal controllability problems and weak controllability problems: they characterize various control possibilities for the spectrum of the original system. Reducing the investigated system to a special form, we obtain the necessary and sufficient solvability conditions for the specified problems. The proof of the main assertions is constructive: for each particular system with the given number matrices, we can construct the corresponding controller. We provide illustrative examples.																	1064-2307	1555-6530				JAN	2020	59	1					19	38		10.1134/S1064230720010086													
J								Dynamic Models for Coordinating Private and Public Interests in Economic Corruption	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL											SYSTEMS	Dynamic game-theoretic models of fighting against the corrupt behavior of subjects in the models for coordinating private and public interests are considered. The case of the economic impact of an upper-level subject on a lower-level subjects is investigated. An algorithm for finding equilibria in the case of impulsion is described. Examples of calculations for various sets of input data are presented. A meaningful interpretation of the results is discussed.																	1064-2307	1555-6530				JAN	2020	59	1					39	48		10.1134/S1064230720010128													
J								Optimal Recurrent Nonlinear Filter of a Large Order for Jump Diffusion Markov Signals	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL												In this paper, we consider the problem of the root-mean-square optimal estimation of the current state of a continuous stochastic object of observation exposed to continuous and pulsed random impacts based on the results of discrete measurements of its output at certain clock time points. To obtain real-time estimates using a low-performance computer, a new discrete finite-dimensional filter that provides estimates only at certain clock and possibly inter-cycle time points is proposed. The vector of its state is composed of the last few clock estimates, while the next estimate is sought in the form of its explicit dependence on the last measurement and the previous state of the filter. The number of previous clock estimates to be taken into account can be selected from the condition of a compromise between the required estimation accuracy and the available measurement processing speed. The prediction between measurements is based on the optimal clock and inter-cycle estimates heuristically. The filter synthesis algorithm and methods for constructing its covariance approximations are presented. An example is considered.																	1064-2307	1555-6530				JAN	2020	59	1					49	62		10.1134/S1064230720010104													
J								CONTROLLABILITY CRITERION FOR DISCRETE BILINEAR SYSTEMS WITH TRANSITION MATRICES	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL												New results on controllability properties for bilinear systems with discrete time and scalar control without restrictions are presented. The controllability properties of bilinear systems with permutation matrices are discussed. For the first time, it was possible to establish and prove the necessary and sufficient conditions for the controllability of the systems under study.																	1064-2307	1555-6530				JAN	2020	59	1					63	82		10.1134/S1064230720010116													
J								Advanced "Confident Judgments" Method when Choosing Multicriteria Solutions in a Multipurpose Approach	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL											OPTIMIZATION; SET	The article considers the problem of making multicriteria decisions in which the decision maker (DM) has the opportunity to indicate the range of selection policies to determine the most effective scope of the considered alternatives. The concept of the Dirichlet area of an alternative is introduced in the blurred area of the target settings of the DM, and four groups of quantitative characteristics are proposed for a holistic description of these areas. A six-step decision-making algorithm is developed. Two mathematical models of multipurpose optimization and systematization of selection policies in the Dirichlet domain are proposed. The proposed method is illustrated by solving the problem of choosing the complex of the most rational concepts of a high-altitude unmanned aerial vehicle.																	1064-2307	1555-6530				JAN	2020	59	1					83	94		10.1134/S1064230720010049													
J								Detection of Thermal Anomalies in the Images of Volcanoes Taken at Night	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL												The problem of revealing the appearance and development of thermal anomalies in the images of volcanoes taken at night in the visible and near-infrared ranges is discussed. An algorithm for detecting and classifying such anomalies is proposed and is tested on the data from the archive of the video monitoring of volcanoes on Kamchatka. The results obtained suggest the possible use of the developed solution in the tasks of real-time monitoring of volcanic activity in the Russian Far East.																	1064-2307	1555-6530				JAN	2020	59	1					95	104		10.1134/S106423071906008X													
J								Method for Selecting the Parameter Values of the Takeoff Phase of Civil and Military Transport AC, as well as Unmanned Aerial Vehicles	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL												The model of (AC's) movement in the takeoff phase is based on a predetermined scenario for this phase containing many parameters that uniquely determine the trajectory. The rational values of these parameters are selected according to the criterion for the specific safety conditions. The equations of motion of the center of mass in the vertical plane are used as the AC's motion model with the approximation of the motion around the center of mass by an oscillating link. For a typical medium-haul AC, a list of relevant parameters is defined, and a region is constructed in the space of these parameters in which all the specific safety conditions are satisfied at each of its points.																	1064-2307	1555-6530				JAN	2020	59	1					105	121		10.1134/S1064230720010074													
J								Control of a System with Two Degrees of Freedom by Means of Potential Forces	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL												This work determines the restoring forces of the elastic elements of a system with two degrees of freedom by its predetermined movement. Two bodies of a certain mass are considered. The first body is connected by the first elastic element with a fixed base; and the second body is connected with the first body by the second elastic element. The movement occurs in a straight line in a horizontal plane. Gravity and friction forces are not taken into account. Then, the trigonometric functions of the law of motion of each body and the restoring forces of both elastic elements are analytically determined. These restoring forces are the control functions by which the desired movement is realized. Technically, such controls can be implemented in various ways, in particular, through passive elastic elements, the design and calculation method of which is proposed in the article. A variant of creating elastic elements with the restoring forces based on springs moving between two guides of the calculated form perpendicular to their axis of symmetry is given.																	1064-2307	1555-6530				JAN	2020	59	1					122	128		10.1134/S1064230720010062													
J								On the Existence, Uniqueness, and Stability of Periodic Modes of Motion of a Locomotion System with a Mobile Internal Mass	JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL											RECTILINEAR MOTION	In this paper, we consider the rectilinear motion of a locomotion system consisting of a hull and an internal mass in a resisting medium during periodic movement of the internal mass relative to the hull. The periodic modes of the system's movement, in which the hull's speed is also a time periodic function, are studied. The questions of the existence and uniqueness of periodic modes of system motion, their stability with respect to the initial conditions, and the rate of convergence of arbitrary movements in relation to them are studied. The periodic mode of motion of the locomotion system is shown to exist, be unique, and be exponentially stable if the medium resistance is monotonic and unlimitedly increases with increasing speed and if the speed of the internal mass relative to the hull is continuous. A two-sided assessment of the hull's speed in a periodic mode of motion is obtained. In particular cases of linear and piecewise linear medium resistance, a periodic mode of the system's motion is constructed, and the rate of exponential convergence is calculated.																	1064-2307	1555-6530				JAN	2020	59	1					129	137		10.1134/S1064230719060108													
J								Parameter Reduction of Neutrosophic Soft Sets and Their Applications	NEUTROSOPHIC SETS AND SYSTEMS										Neutrosophic set; neutrosophic soft set; parameter reduction; decision making	SIMILARITY; ENTROPY	Parameter reduction can be treated as an effective tool in many fields, including pattern recognition. Many reduction techniques have been reported so far for soft sets, fuzzy soft sets and bipolar fuzzy soft sets to solve decision-making problems. However, there is almost no attention to the parameter reduction of neutrosophic soft sets. In this present paper we focus our discussion on the parameter reduction of neutrosophic soft sets as an extension of parameter reduction of soft sets and fuzzy soft sets. To do that, using the concept of indiscernibility relation, we first define the terms 'dispensable set' and 'indispensible set'. We utilize these definitions to define the terms 'decision partition', 'parameter reduction' and 'degree of importance of a parameter' with a suitable example. Next we present an algorithm based on the concept of degree of importance and parameter reduction of a neutrosophic soft set. An illustrative example is employed to show the feasibility and validity of our proposed algorithm based on parameter reduction of neutrosophic soft sets in real life decision making problem.																	2331-6055	2331-608X					2020	32						1	14															
J								Neutrosophic Geometric Programming (NGP) Problems Subject to (V, . ) Operator; the Minimum Solution	NEUTROSOPHIC SETS AND SYSTEMS										Relational Neutrosophic Geometric Programming (RNGP); (V ,.) Operator; Neutrosophic Relation Equations; Distinguishing Matrix; Facilitation Matrix; Minimum Solution; Incompatible Problem		This paper comes as a second step serves the purpose of constructing a neutrosophic optimization model for the relation geometric programming problems subject to (max, product) operator in its constraints. This essay comes simultaneously with my previous paper entitled (Neutrosophic Geometric Programming (NGP) with (max-product) Operator, An Innovative Model) which contains the structure of the maximum solution. The purpose of this article is to set up the minimum solution for the (RNGP) problems, the author faced many difficulties, where the feasible region for this type of problems is already nonconvex; furthermore, the negative signs of the exponents with neutrosophic variables x(j) is an element of [0,1] u I . A new technique to avoid the divided by the indeterminacy component (I) was introduced; Separate the neutrosophic geometric programming into two optimization models, introducing two new matrices named as the distinguishing matrix and the facilitation matrix. All these notions were important for finding the minimum solution of the program. Finally, two numerical examples were presented to enable the reader to understand this work.																	2331-6055	2331-608X					2020	32						15	24															
J								Ngpr Homeomorphism in Neutrosophic Topological Spaces	NEUTROSOPHIC SETS AND SYSTEMS										Neutrosophic generalized pre regular closed set; Ngpr open mappings; Ngpr closed mappings; Ngpr homeomorphism and Nigpr homeomorphism	SUPPLIER SELECTION	As a generalization of Fuzzy sets introduced by Zadeh [21] in 1965 and Intuitionistic Fuzzy sets introduced by Atanassav [8] in 1983, the Neutrosophic set had been introduced and developed by Smarandache. A Neutrosophic set is characterized by a truth value (membership), an indeterminacy value and a falsity value (non-membership). Salama and Alblowi [17] introduced the new concept of neutrosophic topological space (NTS) in 2012, which had been investigated recently. In 2018, Parimala M et al. introduced and studied the concept of Neutrosophic homeomorphism and Neutrosophic a homeomorphism in Neutrosophic topological spaces. The impact of this article is to introduce and study the concepts of Ngpr homeomorphism and Nigpr homeomorphism in Neutrosophic topological space. Further, the work is extended to Ngpr open mappings, Ngpr closed mappings, Nigpr closed mappings and some of their properties are explored in Neutrosophic topological space.																	2331-6055	2331-608X					2020	32						25	37															
J								Generalized Neutrosophic Separation Axioms in Neutrosophic Soft Topological Spaces	NEUTROSOPHIC SETS AND SYSTEMS										neutrosophic soft set; neutrosophic soft point; neutrosophic soft p-open set; neutrosophic soft p-neighborhood; neutrosophic soft p-separation axioms		The idea of neutrosophic set was floated by Smarandache by considering a truth membership, an indeterminacy membership and a falsehood or falsity membership functions. The engagement between neutrosophic set and soft set was done by Maji. More over it was used effectively to model uncertainty in different areas of application, such as control, reasoning, pattern recognition and computer vision. The first aim of this paper leaks out the notion of neutrosophic soft p-open set,neutrosophic soft p-closed sets and their important characteristics. Also the notion of neutrosophic soft p-neighborhood and neutrosophic soft p-separation axioms in neutrosophic soft topological spaces are developed. Important results are examed marrying to these newly defined notion relative to soft points. The notion of neutrosophic soft p-separation axioms of neutrosophic soft topological spaces is diffused in different results concerning soft points. Furthermore, properties of neutrosophic soft -P-i-space (i = 0, 1, 2, 3, 4) and linkage between them is built up.																	2331-6055	2331-608X					2020	32						38	51															
J								Interval Valued Neutrosophic Topological Spaces	NEUTROSOPHIC SETS AND SYSTEMS										Interval valued neutrosophic topology; Interval valued neutrosophic subspace topology	FUZZY; SETS	Within this paper, we present and research the definition of interval valued neutrosophic topological space along with interval valued neutrosophic finer and interval valued neutrosophic coarser topologies. We also describe interval valued neutrosophic interior and closer of an interval valued neutrosophic set. Interval valued neutrosophic subspace topology also studied. Some examples and theorems are presented concerning this concept.																	2331-6055	2331-608X					2020	32						52	60															
J								Arithmetic and Geometric Operators of Pentagonal Neutrosophic Number and its Application in Mobile Communication Service Based MCGDM Problem	NEUTROSOPHIC SETS AND SYSTEMS										Pentagonal neutrosophic number; Weighted arithmetic and geometric averaging operator; Score functions; MCGDM	DECISION-MAKING; SUPPLIER SELECTION; SOFT SETS	In this paper, the theory of pentagonal neutrosophic number has been studied in a disjunctive frame of reference. Moreover, the dependency and independency of the membership functions for the pentagonal neutrosophic number are also classified here. Additionally, the development of a new score function and its computation have been formulated in distinct rational perspectives. Further, weighted arithmetic averaging operator and weighted geometric averaging operator in the pentagonal neutrosophic environment are introduced here using an influx of different logical & innovative thought. Also, a multi-criteria group decision-making problem (MCGDM) in a mobile communication system is formulated in this paper as an application in the pentagonal neutrosophic arena. Lastly, the sensitivity analysis portion reflects the variation of this noble work.																	2331-6055	2331-608X					2020	32						61	79															
J								On Q-Neutrosophic Soft Fields	NEUTROSOPHIC SETS AND SYSTEMS										Neutrosophic soft field; Neutrosophic soft set; Q-neutrosophic soft field; Q-neutrosophic soft set	FUZZY H-IDEALS; ALGEBRAS; SETS	As an extension of neutrosophic soft sets, Q-neutrosophic soft sets were established to deal with two-dimensional indeterminate data. Different hybrid models of fuzzy sets were utilized to different algebraic structures, for example groups, rings, fields and lie-algebras. A field is an essential algebraic structure, which is widely used in algebra and several domains of mathematics. The motivation of the current work is to extend the thought of Q-neutrosophic soft sets to fields. In this paper, we define the notion of Q-neutrosophic soft fields. Structural characteristics of it are investigated. Moreover, the concepts of homomorphic image and pre-image of Q-neutrosophic soft fields are discussed. Finally, the Cartesian product of Q-neutrosophic soft fields is defined and some related properties are discussed.																	2331-6055	2331-608X					2020	32						80	93															
J								Neutrosophic projective G-submodules	NEUTROSOPHIC SETS AND SYSTEMS										Neutrosophic set; Neutrosophic G-module; Direct sum; Projective G-module; Neutrosophic projective G-module		A significant area of module theory is the concept of free modules, projective modules and injective modules. The goal of this study is to characterize the projective G-modules under a single-valued neutrosophic set. So we define neutrosophic G-submodule as a generic version of projective G-submodule. It also describes and derives fundamental algebraic properties including quotient space and direct sum of neutrosophic projective G-submodules																	2331-6055	2331-608X					2020	32						94	106															
J								Quadripartitioned Single valued Neutrosophic Dombi Weighted Aggregation Operators for Multiple Attribute Decision Making	NEUTROSOPHIC SETS AND SYSTEMS										Quadripartitioned single valued neutrosophic sets; Score and Accuracy functions; Dombi Weighted Aggregation Operators	SIMILARITY MEASURES	In this paper we have introduced the concept of score and accuracy function of the Quadripartitioned Single valued Neutrosophic Numbers (QSVNN) and also defined ranking methods between two QSVNNs which is based on its score function. Dombi operators are used in solving many Multicriteria Attribute Group decision making (MAGDM) problems because of its very good flexibility with a general parameter.Here Dombi T-norm and T-conorm operations of two QSVNNs are defined. Based on this Dombi operations, we introduced two Dombi weighted aggregation operators QSVNDWAA and QSVNDWGA under Quadripartitioned Single valued Neutrosophic environment and also studied its properties. Finally, we discussed about Multicriteria Attribute Decision making method (MADM) using QSVNDWAA or QSVNDWGA operator and also an illustrative example is given for the proposed method which gives a detailed results to select the best alternative based upon the ranking orders.																	2331-6055	2331-608X					2020	32						107	122															
J								Polarity of generalized neutrosophic subalgebras in BCK/BCI-algebras	NEUTROSOPHIC SETS AND SYSTEMS										k-polar generalized neutrosophic subalgebra; k-polar generalized (is an element of, is an element of boolean OR q)-neutrosophic subalgebra; k-polar generalized (q is an element of boolean OR q)-neutrosophic subalgebra	BIPOLAR FUZZY SUBALGEBRAS; IDEALS	k-polar generalized neutrosophic set is introduced, and it is applied to BCK/BCI-algebras. The notions of 1-polar generalized subalgebra, 1-polar generalized (is an element of, is an element of boolean OR q)-neutrosophic subalgebra and 1-polar generalized (q, is an element of boolean OR q)-neutrosophic subalgebra are defined, and several properties are investigated. Characterizations of 1-polar generalized neutrosophic subalgebra and 1-polar generalized (is an element of, is an element of boolean OR q)-neutrosophic subalgebra are discussed, and the necessity and possibility operator of k-polar generalized neutrosophic subalgebra are are considered. We show that the generaliged neutrosophic q-sets and the generaliged neutrosophic is an element of boolean OR q-sets subalgebras by using the k-polar generalized (is an element of, is an element of boolean OR q)-neutrosophic subalgebra and the 1-polar generalized (q, is an element of boolean OR q)-neutrosophic subalgebra. A 1-polar generalized (is an element of, is an element of boolean OR q)-neutrosophic subalgebra is established by using the generaliged neutrosophic is an element of boolean OR q-sets, conditions for a k-polar generalized neutrosophic set to be a k-polar generalized neutrosophic subalgebra and a 1-polar generalized (q, is an element of boolean OR q)-neutrosophic subalgebra are provided.																	2331-6055	2331-608X					2020	32						123	145															
J								Neutrosophic N-Soft Sets with TOPSIS method for Multiple Attribute Decision Making	NEUTROSOPHIC SETS AND SYSTEMS										Neutosophic N-soft set; operations on neutosophic N-soft sets; MADM; TOPSIS; medical diagnosis	FUZZY; TOPOLOGY; MODEL	The objective of this article is to introduce a new hybrid model of neutrosophic N-soft set which is combination of neutrosophic set and N-soft set. We introduce some basic operations on neutrosophic N-soft sets along with their fundamental properties. For multi-attribute decision-making (MADM) problems with neutrosophic N-soft sets, we propose an extended TOPSIS (technique based on order preference by similarity to ideal solution) method. In this method, we first propose a weighted decision matrix based comparison method to identify the positive and the negative ideal solutions. Afterwards, we define a separation measurement of these solutions. Finally, we calculate relative closeness to identify the optimal alternative. At length, a numerical example is rendered to illustrate the developed scheme in medical diagnosis via hypothetical case study.																	2331-6055	2331-608X					2020	32						146	170															
J								A Novel of neutrosophic tau-Structure Ring ExtB and ExtV Spaces	NEUTROSOPHIC SETS AND SYSTEMS										neutrosophic tau-structure ring space; neutrosophic tau-structure ring G(delta)T(1)(/2) space; neutrosophic tau-structure ring ExtB space and neutrosophic tau-structure ring ExtV space		In this paper, the concepts of a neutrosophic tau-structure ring spaces, neutrosophic tau-structure ring G(delta)T(1)(/2) spaces and neutrosophic tau-structure ring exterior B spaces and neutrosophic tau-structure ring exterior V spaces are introduced. Some interesting functions that preserve neutrosophic tau-structure ring exterior B spaces and neutrosophic tau-structure ring exterior V spaces in the context of image and preimage are derived with the necessary examples.																	2331-6055	2331-608X					2020	32						171	186															
J								An MCDM Method under Neutrosophic Cubic Fuzzy Sets with Geometric Bonferroni Mean Operator	NEUTROSOPHIC SETS AND SYSTEMS										Neutrosophic Sets; Cubic Fuzzy Sets; Bonferroni Geometric Mean; Aggregation Operators; MCDM		Neutrosophic cubic fuzzy sets (NCFSs) involve interval valued and single valued neutrosophic sets, and are used to describe uncertainty or fuzziness in a more efficient way. Aggregation of neutrosopic cubic fuzzy information is crucial and necessary in a decision making theory. In order to get a better solution to decision making problems under neutrosophic cubic fuzzy environment, this paper introduces an aggregating operator to neutrosophic cubic fuzzy sets with the help of Bonferroni mean and geometric mean, and proposes neutrosophic cubic fuzzy geometric Bonferroni mean operator (NCFGBM(u, v)) with its properties. Then, an efficient decision making technique is introduced based on weighted operator WNCFGBM(w)(u, v). An application of the established method is also examined for a real life problem.																	2331-6055	2331-608X					2020	32						187	202															
J								Single valued neutrosophic mappings defined by single valued neutrosophic relations with applications	NEUTROSOPHIC SETS AND SYSTEMS										Single valued neutrosophic set; Binary relation; Mapping; Topology; Continuous mapping	FUZZY	In this paper, we introduce the notion of single valued neutrosophic mapping defined by single valued neutrosophic relation which is considered as a generalization of fuzzy mapping defined by fuzzy relation and several properties related to this notion are studied. Moreover, we generalize the notion of fuzzy topology on fuzzy sets introduced by Kandil et al. to the setting of single valued neutrosophic sets. As applications, we establish the property of continuity in single valued neutrosophic topological space and investigate relationships among various types of single valued neutrosophic continuous mapping.																	2331-6055	2331-608X					2020	32						203	220															
J								A Study on Bipolar Single-Valued Neutrosophic Graphs With Novel Application	NEUTROSOPHIC SETS AND SYSTEMS										symmetric difference; residue product; maximal product; rejection of BSVNG; Application; algorithm		Unipolar is less fundamental than bipolar cognition based on truth, and composure is a restraint for truth-based worlds. Bipolarity is the most powerful phenomenon that survives when truth disappeared in a black hole due to Hawking radiation or particular / anti-particular emission. The purpose of this research study is to define few four operations, including residue product, rejection, maximal product and symmetric difference of bipolar single-valued neutrosophic graph (BSVNG) and to explore some of their related properties with examples. Bipolar single-valued neutrosophic graph (BSVNG) is the generalization of the single-valued neutrosophic graph (SVNG), intuitionistic fuzzy graph, bipolar intuitionistic fuzzy graph, bipolar fuzzy graph and fuzzy graph. BSVNG plays a significant role in the study of neural networks, daily energy issues, energy systems, and coding. Moreover, we will determine related properties like the degree of a vertex in a BSVNG or total degree of a vertex in a BSVNG. We provide examples of the vertex degree in BSVNG and the total vertex degree in BSVNG. In order to make this useful, we develop an algorithm for our useful method in steps.																	2331-6055	2331-608X					2020	32						221	268															
J								Neutrosophic Geometric Programming (NGP) with (Max, Product) Operator; An Innovative Model	NEUTROSOPHIC SETS AND SYSTEMS										Neutrosophic Geometric Programming (NGP); (max-product) Operator; Neutrosophic Relation Constraints; Maximum Solution; Incompatible Problem; Pre-Maximum Solution; Relational Neutrosophic Geometric Programming (RNGP)		In this paper, a neutrosophic optimization model has been first constructed for the neutrosophic geometric programming subject to (max-product) neutrosophic relation constraints. For finding the maximum solution, two new operations (i.e. (sic), Theta) between a(ij) and b(i) have been defined, which have a key role in the structure of the maximum solution. Also, two new theorems and some propositions are introduced that discussed the cases of the incompatibility in the relational equations Aox = b, with some properties of the operation Theta. Numerical examples have been solved to illustrate new concepts.																	2331-6055	2331-608X					2020	32						269	281															
J								Neutrosophic Soft Sets Applied on Incomplete Data	NEUTROSOPHIC SETS AND SYSTEMS										Soft set; neutrosophic set; neutrosophic soft set; data filling		A neutrosophic set is a part of neutrosophy that studies the origin, nature and scope of neutralities as well as their interactions with different ideational spectra. In this present paper first we have introduced the concept of a neutrosophic soft set having incomplete data with suitable examples. Then we have tried to explain the consistent and inconsistent association between the parameters. We have introduced few new definitions, namely- consistent association number between the parameters, consistent association degree, inconsistent association number between the parameters and inconsistent association degree to measure these associations. Lastly we have presented a data filling algorithm. An illustrative example is employed to show the feasibility and validity of our algorithm in practical situation.																	2331-6055	2331-608X					2020	32						282	293															
J								Aggregate Operators of Neutrosophic Hypersoft Set	NEUTROSOPHIC SETS AND SYSTEMS										MCDM; Uncertainty; Soft set; Neutrosophic soft set; Hyper soft	SOFT SET; SUPPLIER SELECTION; DECISION-MAKING; TOPSIS	Multi-criteria decision making (MCDM) is concerned about organizing and taking care of choice and planning issues including multi-criteria. When attributes are more than one, and further bifurcated, neutrosophic softset environment cannot be used to tackle such type of issues. Therefore, there was a dire need to define a new approach to solve such type of problems, So, for this purpose a new environment namely, Neutrosophic Hypersoft set (NHSS) is defined. This paper includes basics operator's like union, intersection, complement, subset, null set, equal set etc., of Neutrosophic Hypersoft set (NHSS). The validity and the implementation are presented along with suitable examples. For more precision and accuracy, in future, proposed operations will play a vital role is decision-makings like personal selection, management problems and many others.																	2331-6055	2331-608X					2020	32						294	306															
J								A New Approach of Neutrosophic Soft Set with Generalized Fuzzy TOPSIS in Application of Smart Phone Selection	NEUTROSOPHIC SETS AND SYSTEMS										Accuracy Function; MCDM; TOPSIS; Mobile Phone; Soft set; Neutrosophic Numbers NNs; Neutrosophic Soft set; Linguistic Variable	GROUP DECISION-MAKING; TOPOLOGY	With the invention of new technologies, the competition elevates in market. Therefore, it creates more difficulties for consumer to select the right smart phone. In this paper, a new approach is proposed to select smart phone, in which environment of decision-making is MCDM. Firstly, an algorithm is proposed in which problem is formulated in the form of neutrosophic soft set and then solved with generalized fuzzy TOPSIS (GFT). Secondly, rankings are compared with [10]. Finally, it is concluded that proposed approach is applicable in decision-making where uncertainty and imprecise information-based environment is confronted. In future, this evolutionary algorithm can be used along with other methodologies to solve MCDM problems.																	2331-6055	2331-608X					2020	32						307	316															
J								Single and Multi-valued Neutrosophic Hypersoft set and Tangent Similarity Measure of Single valued Neutrosophic Hypersoft Sets	NEUTROSOPHIC SETS AND SYSTEMS										Neutrosophic hypersoft set (NHSS); single-valued neutrosophic hypersoft set (SVNHSS); multi-valued Neutrosophic Hypersoft set (MVNHSS); tangent similarity measure (TSM); multiple attribute decision making; cricket player	DECISION-MAKING METHOD; FUZZY-SETS; TOPSIS	In this paper, we present a single-valued Neutrosophic Hypersoft set, multi-valued Neutrosophic Hypersoft set and tangent similarity measure for single-valued neutrosophic hypersoft sets and its properties. Then we use this technique in an application namely selection of cricket players for different types of matches (ODI, T20, and test) based on Neutrosophic Hypersoft set in decision making of single-valued neutrosophic hypersoft sets. This technique will help us to decide the best option for the players.																	2331-6055	2331-608X					2020	32						317	329															
J								On Optimizing Neutrosophic Complex Programming Using Lexicographic Order	NEUTROSOPHIC SETS AND SYSTEMS										Complex programming; Neutrosophic numbers; Score function; Lexicographic order; Lingo software; Kuhn- Tucker conditions; Neutrosophic optimal solution	DECISION-MAKING; DUALITY; NUMBER; TOPSIS	Neutrosophic sets are a generalization of the crisp set, fuzzy set, and intuitionistic fuzzy set for representing the uncertainty, inconsistency, and incomplete knowledge about the real world problems. This paper aims to characterize the solution of complex programming (CP) problem with imprecise data instead of its prices information. The neutrosophic complex programming (NCP) problem is considered by incorporating single valued trapezoidal neutrosophic numbers in all the parameters of objective function and constraints. The score function corresponding to the neutrosophic number is used to transform the problem into the corresponding crisp CP. Here, Lexicographic order is applied for the comparison between any two complex numbers. The comparison is developed between the real and imaginary parts separately. Through this manner, the CP problem is divided into two real sub-problems. In the last, a numerical example is solved for the illustration that shows the applicability of the proposed approach. The advantage of this approach is more flexible and makes a real-world situation more realistic.																	2331-6055	2331-608X					2020	32						330	343															
J								A New Model for the Selection of Information Technology Project in a Neutrosophic Environment	NEUTROSOPHIC SETS AND SYSTEMS										Information Technology Project; Balanced Scorecard Model; Neutrosophic Analytic Hierarchy Process; zero-one linear programming		Usually, companies confront the difficulty to make the best decision about the way to invest their recourses in different project alternatives. The company acquires competitive advantages when their software development projects are well evaluated and correctly selected. Selecting projects in the Information Technology field presents challenges in many senses; e.g., the difficulty that entails assessing intangible benefits, projects are interdependent and companies impose self-constraints. In addition, the framework to make the decision is generally uncertain with many unknown factors. This paper aims to propose a model that integrates methods, techniques and tools such as the Balanced Scorecard Model, neutrosophic Analytic Hierarchy Process and zero-one linear programming. The proposed model is designed to select the best portfolio of Information Technology projects, it overcomes the obstacles mentioned above and can be coherently incorporated in the strategic plan process of any company. In addition, it eases the course of experts' decision making, because it is based on Neutrosophy and hence incorporates the indeterminacy term.																	2331-6055	2331-608X					2020	32						344	360															
J								Analyzing Age Group and Time of the Day Using Interval Valued Neutrosophic Sets	NEUTROSOPHIC SETS AND SYSTEMS										Neutrosophic Logic; Human Psychological Behavior; Age Group; Day; Interval Valued Neutrosophic Set	SPANNING TREE; SINGLE; IMAGE	Human psychological behavior is always uncertain in nature with the truth, indeterminacy and falsity of the information and hence neutrosophic logic is able to deal with this kind of real world problems as it resembles human's attitude very closely. In this paper, age group analysis and time (day or night) analysis have been carried out using interval valued neutrosophic sets. Further, the impact of the present work is presented.																	2331-6055	2331-608X					2020	32						361	371															
J								Air Pollution Model using Neutrosophic Cubic Einstein Averaging Operators	NEUTROSOPHIC SETS AND SYSTEMS										Air pollution; neutrosophic cubic weighted averaging; neutrosophic cubic hybrid averaging; neutrosophic cubic Einstein weighted averaging; neutrosophic cubic Einstein hybrid averaging	GROUP DECISION-MAKING; AGGREGATION OPERATORS	The neutrosophic cubic averaging and Einstein averaging aggregation operators are presented and applied to the air pollution model of the city of Peshawar, Pakistan. Neutrosophic cubic set (NCS) is a more generalized version of the neutrosophic set (NS) and an interval neutrosophic set (INS). It is in a better position to express consistent, indeterminant and incomplete information, thus it is able to be applied to aggregate the air pollution model. Aggregation operators have a key role in science and engineering problems. Firstly, the neutrosophic cubic weighted averaging (NCWA) operator, neutrosophic cubic ordered weighted averaging (NCOWA) operator, neutrosophic cubic hybrid aggregation (NCHA) operator, neutrosophic cubic Einstine weighted averaging (NCEWA) operator, neutrosophic cubic Einstine ordered weighted averaging (NCEOWA) operator and neutrosophic cubic Einstine hybrid aggregation (NCEHA) operator are defined. Secondly, these operators are applied to the air pollution model of particulate matter with the size of less than 10 micron (PM10) in Peshawar. Subsequently, the results are compared with the World Health Organization (WHO) standards using score/accuracy function. The pollution of PM10 is found to be very much higher than WHO standards. Hence, strong measures are required to control air pollution.																	2331-6055	2331-608X					2020	32						372	389															
J								Neutrosophic alpha-Irresolute Multifunction in Neutrosophic Topological Spaces	NEUTROSOPHIC SETS AND SYSTEMS										Neutrosophic alpha-irresolute lower; Neutrosophic alpha irresolute upper; Neutrosophic alpha - closed sets; Neutrosophic topological spaces		Aim of this present paper is, we define some new type of irresolute multifunction between the two spaces. We obtain some characterization and some properties between such as Lower & Upper alpha-irresolute multifunction																	2331-6055	2331-608X					2020	32						390	400															
J								Neutrosophic Fuzzy Matrices and Some Algebraic Operations	NEUTROSOPHIC SETS AND SYSTEMS										Neutrosophic fuzzy matrix; Neutrosophic set. Commutativity; Distributive; Subtraction of neutrosophic matrices		In this article, we study neutrosophic fuzzy set and define the subtraction and multiplication of two rectangular and square neutrosophic fuzzy matrices. Some properties of subtraction, addition and multiplication of these matrices and commutative property, distributive property have been examined.																	2331-6055	2331-608X					2020	32						401	409															
J								Neutrosophic Bipolar Vague Soft Set and Its Application to Decision Making Problems	NEUTROSOPHIC SETS AND SYSTEMS										Neutrosophic set; Neutrosophic bipolar vague set; Soft set; vague set; Neutrosophic bipolar vague soft set		In this paper we study the concept of neutrosophic bipolar vague soft sets and some of its operations. It is the combination of neutrosophic bipolar vague sets and soft sets. Further we develop a decision making method based on neutrosophic bipolar vague soft set. A numerical example has been shown. Some new operations on neutrosophic bipolar vague soft set have also been designed.																	2331-6055	2331-608X					2020	32						410	424															
J								On Some Types of Neutrosophic Topological Groups with Respect to Neutrosophic Alpha Open Sets	NEUTROSOPHIC SETS AND SYSTEMS										Neutrosophic alpha -open sets; neutrosophic alpha -continuous functions; neutrosophic topological groups; neutrosophic topological groups of type (R); R = 1,2,3, ...,8	SUPPLIER SELECTION	In this article, we presented eight different types of neutrosophic topological groups, each of which depends on the conceptions of neutrosophic alpha-open sets and neutrosophic alpha-continuous functions. Also, we found the relation between these types, and we gave some properties on the other side.																	2331-6055	2331-608X					2020	32						425	434															
J								A Contemporary Approach on Neutrosophic Nano Topological Spaces	NEUTROSOPHIC SETS AND SYSTEMS										Neutrosophic nano j-closed set; neutrosophic nano generalized closed set; neutrosophic nano generalized j-closed set; neutrosophic nano generalized j*-closed set	SUPPLIER SELECTION; DECISION-MAKING	In this article, we implement a new notion of sets namely neutrosophic nano j-closed set, neutrosophic nano generalized closed set, neutrosophic nano generalized j-closed set and neutrosophic nano generalized j*-closed set in neutrosophic nano topological spaces. We also provide some appropriate examples to study the properties of these sets. The existing relations between some of these sets in neutrosophic nano topological space have been investigated.																	2331-6055	2331-608X					2020	32						435	443															
J								RAPNet: Residual Atrous Pyramid Network for Importance-Aware Street Scene Parsing	IEEE TRANSACTIONS ON IMAGE PROCESSING										Semantics; Feature extraction; Machine learning; Labeling; Coherence; Convolution; Autonomous vehicles; Street scene parsing; importance-aware feature selection; residual atrous spatial pyramid; fully convolutional network	SEGMENTATION	Street Scene Parsing (SSP) is a fundamental and important step for autonomous driving and traffic scene understanding. Recently, Fully Convolutional Network (FCN) based methods have delivered expressive performances with the help of large-scale dense-labeling datasets. However, in urban traffic environments, not all the labels contribute equally for making the control decision. Certain labels such as pedestrian, car, bicyclist, road lane or sidewalk would be more important in comparison with labels for vegetation, sky or building. Based on this fact, in this paper we propose a novel deep learning framework, named Residual Atrous Pyramid Network (RAPNet), for importance-aware SSP. More specifically, to incorporate the importance of various object classes, we propose an Importance-Aware Feature Selection (IAFS) mechanism which automatically selects the important features for label predictions. The IAFS can operate in each convolutional block, and the semantic features with different importance are captured in different channels so that they are automatically assigned with corresponding weights. To enhance the labeling coherence, we also propose a Residual Atrous Spatial Pyramid (RASP) module to sequentially aggregate global-to-local context information in a residual refinement manner. Extensive experiments on two public benchmarks have shown that our approach achieves new state-of-the-art performances, and can consistently obtain more accurate results on the semantic classes with high importance levels.																	1057-7149	1941-0042					2020	29						5010	5021		10.1109/TIP.2020.2978339													
