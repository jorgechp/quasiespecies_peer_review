PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	RP	EM	RI	OI	FU	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	D2	EA	PG	WC	SC	GA	UT	PM	OA	HC	HP	DA
J								Neuromodulated attention and goal-driven perception in uncertain domains	NEURAL NETWORKS										Neuromodulation; Goal-driven perception; Uncertainty; Top-down attention; Contrastive excitation backprop	BASAL FOREBRAIN; TOP-DOWN; MODULATION; MECHANISMS; RECEPTORS; BRAIN	In uncertain domains, the goals are often unknown and need to be predicted by the organism or system. In this paper, contrastive Excitation Backprop (c-EB) was used in two goal-driven perception tasks - one with pairs of noisy MNIST digits and the other with a robot in an action-based attention scenario. The first task included attending to even, odd, low, and high digits, whereas the second task included action goals, such as "eat", "work-on-computer", "read", and "say-hi" that led to attention to objects associated with those actions. The system needed to increase attention to target items and decrease attention to distractor items and background noise. Because the valid goal was unknown, an online learning model based on the cholinergic and noradrenergic neuromodulatory systems was used to predict a noisy goal (expected uncertainty) and re-adapt when the goal changed (unexpected uncertainty). This neurobiologically plausible model demonstrates how neuromodulatory systems can predict goals in uncertain domains and how attentional mechanisms can enhance the perception for that goal. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAY	2020	125						56	69		10.1016/j.neunet.2020.01.031													
J								Training high-performance and large-scale deep neural networks with full 8-bit integers	NEURAL NETWORKS										Neural network quantization; 8-bit training; Full quantization; Online learning device	PRECISION	Deep neural network (DNN) quantization converting floating-point (FP) data in the network to integers (INT) is an effective way to shrink the model size for memory saving and simplify the operations for compute acceleration. Recently, researches on DNN quantization develop from inference to training, laying a foundation for the online training on accelerators. However, existing schemes leaving batch normalization (BN) untouched during training are mostly incomplete quantization that still adopts high precision FP in some parts of the data paths. Currently, there is no solution that can use only low bit-width INT data during the whole training process of large-scale DNNs with acceptable accuracy. In this work, through decomposing all the computation steps in DNNs and fusing three special quantization functions to satisfy the different precision requirements, we propose a unified complete quantization framework termed as "WAGEUBN" to quantize DNNs involving all data paths including W (Weights), A (Activation), G (Gradient), E (Error), U (Update), and BN. Moreover, the Momentum optimizer is also quantized to realize a completely quantized framework. Experiments on ResNet18/34/50 models demonstrate that WAGEUBN can achieve competitive accuracy on the ImageNet dataset. For the first time, the study of quantization in large-scale DNNs is advanced to the full 8-bit INT level. In this way, all the operations in the training and inference can be bit-wise operations, pushing towards faster processing speed, decreased memory cost, and higher energy efficiency. Our throughout quantization framework has great potential for future efficient portable devices with online learning ability. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAY	2020	125						70	82		10.1016/j.neunet.2019.12.027													
J								PET image super-resolution using generative adversarial networks	NEURAL NETWORKS										Super-resolution; Self-supervised; CNN; GAN; PET; Multimodality imaging	BRAIN PET	The intrinsically low spatial resolution of positron emission tomography (PET) leads to image quality degradation and inaccurate image-based quantitation. Recently developed supervised super-resolution (SR) approaches are of great relevance to PET but require paired low- and high-resolution images for training, which are usually unavailable for clinical datasets. In this paper, we present a self-supervised SR (SSSR) technique for PET based on dual generative adversarial networks (GANs), which precludes the need for paired training data, ensuring wider applicability and adoptability. The SSSR network receives as inputs a low-resolution PET image, a high-resolution anatomical magnetic resonance (MR) image, spatial information (axial and radial coordinates), and a high-dimensional feature set extracted from an auxiliary CNN which is separately-trained in a supervised manner using paired simulation datasets. The network is trained using a loss function which includes two adversarial loss terms, a cycle consistency term, and a total variation penalty on the SR image. We validate the SSSR technique using a clinical neuroimaging dataset. We demonstrate that SSSR is promising in terms of image quality, peak signal-to-noise ratio, structural similarity index, contrast-to-noise ratio, and an additional no-reference metric developed specifically for SR image quality assessment. Comparisons with other SSSR variants suggest that its high performance is largely attributable to simulation guidance. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAY	2020	125						83	91		10.1016/j.neunet.2020.01.029													
J								Mu-net: Multi-scale U-net for two-photon microscopy image denoising and restoration	NEURAL NETWORKS										Image denoising; Two-photon microscopy image; Deep learning; U-net; GAN		Advances in two two-photon microscopy (2PM) have made three-dimensional (3D) neural imaging of deep cortical regions possible. However, 2PM often suffers from poor image quality because of various noise factors, including blur, white noise, and photo bleaching. In addition, the effectiveness of the existing image processing methods is limited because of the special features of 2PM images such as deeper tissue penetration but higher image noises owing to rapid laser scanning. To address the denoising problems in 2PM 3D images, we present a new algorithm based on deep convolutional neural networks (CNNs). The proposed model consists of multiple U-nets in which an individual U-net removes noises at different scales and then yields a performance improvement based on a coarse-to-fine strategy. Moreover, the constituent CNNs employ fully 3D convolution operations. Such an architecture enables the proposed model to facilitate end-to-end learning without any pre/post processing. Based on the experiments on 2PM image denoising, we observed that our new algorithm demonstrates substantial performance improvements over other baseline methods. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAY	2020	125						92	103		10.1016/j.neunet.2020.01.026													
J								Weighted discriminative collaborative competitive representation for robust image classification	NEURAL NETWORKS										Collaborative representation-based classification; Collaborative representation; Representation-based classification; Image classification; Pattern recognition	FACE RECOGNITION; SPARSE REPRESENTATION; DICTIONARY; LOCALITY; ALGORITHMS	Collaborative representation-based classification (CRC) is a famous representation-based classification method in pattern recognition. Recently, many variants of CRC have been designed for many classification tasks with the good classification performance. However, most of them ignore the inter-class pattern discrimination among the class-specific representations, which is very critical for strengthening the pattern discrimination of collaborative representation (CR). In this article, we propose a novel CR approach for image classification, called weighted discriminative collaborative competitive representation (WDCCR). The proposed WDCCR designs the discriminative and competitive collaborative representation among all the classes by fully considering the class information. On the one hand, we incorporate two discriminative constraints into the unified WDCCR model. Both constraints are the competitive class-specific representation residuals and the pairs of class-specific representations for each query sample. On the other hand, the constraint of the weighted categorical representation coefficients is introduced into the proposed model for further enhancing the power of discriminative and competitive representation. In the weighted constraint, we assume that the different classes of each query sample should have less contribution to the representation with the small representation coefficients, and then two types of weight factors are designed to constrain the representation coefficients. Furthermore, the robust WDCCR (R-WDCCR) is proposed with l(1)-norm representation fidelity for recognizing noisy images. Extensive experiments on six image data sets demonstrate the effective and robust superiorities of the proposed WDCCR and R-WDCCR over the related state-of-the-art representation-based classification methods. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAY	2020	125						104	120		10.1016/j.neunet.2020.01.020													
J								On the localness modeling for the self-attention based end-to-end speech synthesis	NEURAL NETWORKS										Speech synthesis; Self attention; Localness modeling; Relative-position-aware; Gaussian bias		Attention based end-to-end speech synthesis achieves better performance in both prosody and quality compared to the conventional "front-end"-"back-end" structure. But training such end-to-end framework is usually time-consuming because of the use of recurrent neural networks. To enable parallel calculation and long-range dependency modeling, a solely self-attention based framework named Transformer is proposed recently in the end-to-end family. However, it lacks position information in sequential modeling, so that the extra position representation is crucial to achieve good performance. Besides, the weighted sum form of self-attention is conducted over the whole input sequence when computing latent representation, which may disperse the attention to the whole input sequence other than focusing on the more important neighboring input states, resulting in generation errors. In this paper, we introduce two localness modeling methods to enhance the self-attention based representation for speech synthesis, which maintain the abilities of parallel computation and global-range dependency modeling in self-attention while improving the generation stability. We systematically analyze the solely self-attention based end-to-end speech synthesis framework, and unveil the importance of local context. Then we add the proposed relative-position-aware method to enhance local edges and experiment with different architectures to examine the effectiveness of localness modeling. In order to achieve query-specific window and discard the hyper-parameter of the relative-position-aware approach, we further conduct Gaussian-based bias to enhance localness. Experimental results indicate that the two proposed localness enhanced methods can both improve the performance of the self-attention model, especially when applied to the encoder part. And the query-specific window of Gaussian bias approach is more robust compared with the fixed relative edges. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAY	2020	125						121	130		10.1016/j.neunet.2020.01.034													
J								Preserving differential privacy in deep neural networks with relevance-based adaptive noise imposition	NEURAL NETWORKS										Deep neural networks; Differential privacy; Relevance analysis	REGRESSION-ANALYSIS; RECOGNITION	In recent years, deep learning achieves remarkable results in the field of artificial intelligence. However, the training process of deep neural networks may cause the leakage of individual privacy. Given the model and some background information of the target individual, the adversary can maliciously infer the sensitive feature of the target individual. Therefore, it is imperative to preserve the sensitive information in the training data. Differential privacy is a state-of-the-art paradigm for providing the privacy guarantee of datasets, which protects the private and sensitive information from the attack of adversaries significantly. However, the existing privacy-preserving models based on differential privacy are less than satisfactory since traditional approaches always inject the same amount of noise into parameters to preserve the sensitive information, which may impact the trade-off between the model utility and the privacy guarantee of training data. In this paper, we present a general differentially private deep neural networks learning framework based on relevance analysis, which aims to bridge the gap between private and non-private models while providing an effective privacy guarantee of sensitive information. The proposed model perturbs gradients according to the relevance between neurons in different layers and the model output. Specifically, during the process of backward propagation, more noise is added to gradients of neurons that have less relevance to the model output, and vice-versa. Experiments on five real datasets demonstrate that our mechanism not only bridges the gap between private and non-private models, but also prevents the disclosure of sensitive information effectively. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAY	2020	125						131	141		10.1016/j.neunet.2020.02.001													
J								Fast discrete cross-modal hashing with semantic consistency	NEURAL NETWORKS										Cross-modal retrieval; Semantic consistency; Discrete optimization; Hashing	CODES	Supervised cross-modal hashing has attracted widespread concentrations for large-scale retrieval task due to its promising retrieval performance. However, most existing works suffer from some of following issues. Firstly, most of them only leverage the pair-wise similarity matrix to learn hash codes, which may result in class information loss. Secondly, the pair-wise similarity matrix generally lead to high computing complexity and memory cost. Thirdly, most of them relax the discrete constraints during optimization, which generally results in large cumulative quantization error and consequent inferior hash codes. To address above problems, we present a Fast Discrete Cross-modal Hashing method in this paper, FDCH for short. Specifically, it firstly leverages both class labels and the pair-wise similarity matrix to learn a sharing Hamming space where the semantic consistency can be better preserved. Then we propose an asymmetric hash codes learning model to avoid the challenging issue of symmetric matrix factorization. Finally, an effective and efficient discrete optimal scheme is designed to generate discrete hash codes directly, and the computing complexity and memory cost caused by the pair-wise similarity matrix are reduced from O(n(2)) to O(n), where n denotes the size of training set. Extensive experiments conducted on three real world datasets highlight the superiority of FDCH compared with several cross-modal hashing methods and demonstrate its effectiveness and efficiency. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAY	2020	125						142	152		10.1016/j.neunet.2020.01.035													
J								Robust min-max optimal control design for systems with uncertain models: A neural dynamic programming approach	NEURAL NETWORKS										Approximate dynamic-programming; Artificial neural networks; Hamilton-Jacobi-Bellman equation; Bellman function; Sub-optimal controller	JACOBI-BELLMAN EQUATIONS; NONLINEAR-SYSTEMS; TRACKING CONTROL; NETWORK; APPROXIMATION	The design of an artificial neural network (ANN) based sub-optimal controller to solve the finite-horizon optimization problem for a class of systems with uncertainties is the main outcome of this study. The optimization problem considers a convex performance index in the Bolza form. The dynamic uncertain restriction is considered as a linear system affected by modeling uncertainties, as well as by external bounded perturbations. The proposed controller implements a min-max approach based on the dynamic neural programming approximate solution. An ANN approximates the Value function to get the estimate of the Hamilton-Jacobi-Bellman (HJB) equation solution. The explicit adaptive law for the weights in the ANN is obtained from the approximation of the HJB solution. The stability analysis based on the Lyapunov theory yields to confirm that the approximate Value function serves as a Lyapunov function candidate and to conclude the practical stability of the equilibrium point. A simulation example illustrates the characteristics of the sub-optimal controller. The comparison of the performance indexes obtained with the application of different controllers evaluates the effect of perturbations and the sub-optimal solution. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAY	2020	125						153	164		10.1016/j.neunet.2020.01.016													
J								Reachable set bounding for neural networks with mixed delays: Reciprocally convex approach	NEURAL NETWORKS										Reachable set; Maximal Lyapunov functional; Mixed delays systems; Polytopic uncertainties; Reciprocally convex	FINITE-TIME CONSENSUS; LINEAR-SYSTEMS; MULTIAGENT SYSTEMS; STABILITY; DISCRETE	This paper discusses the reachable set estimation problem of neural networks with mixed delays. Firstly, by means of the maximal Lyapunov-Krasovskii functional, we obtain a non-ellipsoid form of the reachable set. Further more, when calculating the derivative of the maximum Lyapunov functional, the lower bound lemma and reciprocally convex approach method are used to solve the reciprocally convex combination term, which reduce the related decision variables. Secondly, we extend the results to polytopic uncertainties neural networks and consider the case of uncertain differentiable parameters. Finally, two numerical examples and one application example are listed to show the validity of our methods. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAY	2020	125						165	173		10.1016/j.neunet.2020.02.005													
J								Chaos in fractional-order discrete neural networks with application to image encryption	NEURAL NETWORKS										Fractional-order discrete systems; Neural networks; Synchronization; Image encryption	MITTAG-LEFFLER STABILITY; PROJECTIVE SYNCHRONIZATION; ALGORITHM; TIME	In this paper, a three-dimensional fractional-order (FO) discrete Hopfield neural network (FODHNN) in the left Caputo discrete delta's sense is proposed, the dynamic behavior and synchronization of FODHNN are studied, and the system is applied to image encryption. First, FODHNN is shown to exhibit rich nonlinear dynamics behaviors. Phase portraits, bifurcation diagrams and Lyapunov exponents are carried out to verify chaotic dynamics in this system. Moreover, by using stability theorem of FO discrete linear systems, a suitable control scheme is designed to achieve synchronization of the FODHNN. Finally, image encryption system based on the chaotic FODHNN is presented. Some security analysis and tests are given to show the effective of the encryption system. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAY	2020	125						174	184		10.1016/j.neunet.2020.02.008													
J								Causal importance of low-level feature selectivity for generalization in image recognition	NEURAL NETWORKS										Neural network; Orientation selectivity; Causality	DEEP NEURAL-NETWORKS; ORIENTATION SELECTIVITY; VISUAL-CORTEX; NEURONS; REPRESENTATIONS; SENSITIVITY; CELLS	Although our brain and deep neural networks (DNNs) can perform high-level sensory-perception tasks, such as image or speech recognition, the inner mechanism of these hierarchical information-processing systems is poorly understood in both neuroscience and machine learning. Recently, Morcos et al. (2018) examined the effect of class-selective units in DNNs, i.e., units with high-level selectivity, on network generalization, concluding that hidden units that are selectively activated by specific input patterns may harm the network's performance. In this study, we revisited their hypothesis, considering units with selectivity for lower-level features, and argue that selective units are not always harmful to the network performance. Specifically, by using DNNs trained for image classification, we analyzed the orientation selectivity of individual units, a low-level selectivity widely studied in visual neuroscience. We found that orientation-selective units exist in both lower and higher layers of these DNNs, as in our brain. In particular, units in lower layers became more orientation-selective as the generalization performance improved during the course of training. Consistently, networks that generalized better were more orientation-selective in the lower layers. We finally revealed that ablating these selective units in the lower layers substantially degraded the generalization performance of the networks, at least by disrupting the shift-invariance of the higher layers. These results suggest that orientation selectivity can play a causally important role in object recognition, and that, contrary to the triviality of units with high-level selectivity, lower-layer units with selectivity for low-level features may be indispensable for generalization, at least for the several network architectures. (c) 2020 The Author(s). Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by- nc-nd/4.0/).																	0893-6080	1879-2782				MAY	2020	125						185	193		10.1016/j.neunet.2020.02.009													
J								Resilient fault-tolerant anti-synchronization for stochastic delayed reaction-diffusion neural networks with semi-Markov jump parameters	NEURAL NETWORKS										Anti-synchronization; Semi-Markov process; Fault-tolerant control; Resilient control; Neural networks	H-INFINITY CONTROL; EXPONENTIAL STABILITY; SYSTEMS; STABILIZATION; CRITERIA	This paper deals with the anti-synchronization issue for stochastic delayed reaction-diffusion neural networks subject to semi-Markov jump parameters. A resilient fault-tolerant controller is utilized to ensure the anti-synchronization in the presence of actuator failures as well as gain perturbations, simultaneously. Firstly, by means of the Lyapunov functional and stochastic analysis methods, a mean-square exponential stability criterion is derived for the resulting error system. It is shown the obtained criterion improves a previously reported result. Then, based on the present analysis result and using several decoupling techniques, a strategy for designing the desired resilient fault-tolerant controller is proposed. At last, two numerical examples are given to illustrate the superiority of the present stability analysis method and the applicability of the proposed resilient fault-tolerant anti-synchronization control strategy, respectively. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAY	2020	125						194	204		10.1016/j.neunet.2020.02.015													
J								Collaborative learning with corrupted labels	NEURAL NETWORKS										Deep neural networks; Corrupted labels; Robustness	NEURAL-NETWORKS	Deep neural networks (DNNs) have been very successful for supervised learning. However, their high generalization performance often comes with the high cost of annotating data manually. Collecting low-quality labeled dataset is relatively cheap, e.g., using web search engines, while DNNs tend to overfit to corrupted labels easily. In this paper, we propose a collaborative learning (co-learning) approach to improve the robustness and generalization performance of DNNs on datasets with corrupted labels. This is achieved by designing a deep network with two separate branches, coupled with a relabeling mechanism. Co-learning could safely recover the true labels of most mislabeled samples, not only preventing the model from overfitting the noise, but also exploiting useful information from all the samples. Although being very simple, the proposed algorithm is able to achieve high generalization performance even a large portion of the labels are corrupted. Experiments show that co-learning consistently outperforms existing state-of-the-art methods on three widely used benchmark datasets. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAY	2020	125						205	213		10.1016/j.neunet.2020.02.010													
J								Hyper-Laplacian regularized multi-view subspace clustering with low-rank tensor constraint	NEURAL NETWORKS										Multi-view features; Subspace clustering; Manifold regularization; Low-rank tensor representation	DIMENSIONALITY REDUCTION	In this paper, we propose a novel hyper-Laplacian regularized multiview subspace clustering with low-rank tensor constraint method, which is referred as HLR-MSCLRT. In the HLR-MSCLRT model, the subspace representation matrices of different views are stacked as a tensor, and then the high order correlations among data can be captured. To reduce the redundancy information of the learned subspace representations, a low-rank constraint is adopted to the constructed tensor. Since data in the real world often reside in multiple nonlinear subspaces, the HLR-MSCLRT model utilizes the hyper-Laplacian graph regularization to preserve the local geometry structure embedded in a high-dimensional ambient space. An efficient algorithm is also presented to solve the optimization problem of the HLR-MSCLRT model. The experimental results on some data sets show that the proposed HLR-MSCLRT model outperforms many state-of-the-art multi-view clustering approaches. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAY	2020	125						214	223		10.1016/j.neunet.2020.02.014													
J								Synchronization of complex networks with time-varying delay of unknown bound via delayed impulsive control	NEURAL NETWORKS										Time-varying delay; Delayed impulsive control; Complex networks; Synchronization; Impulsive differential inequality	VALUED NEURAL-NETWORKS; DYNAMICAL NETWORKS; MU-STABILITY; EXPONENTIAL STABILITY; ROBUST STABILITY; DIFFERENTIAL-SYSTEMS; STABILIZATION; CRITERION; LEAKAGE	The synchronization problem for complex networks with time-varying delays of unknown bound is investigated in this paper. From the impulsive control point of view, a novel delayed impulsive differential inequality is proposed, where the bounds of time-varying delays in continuous dynamic and discrete dynamic are both unknown. Based on the inequality, a class of delayed impulsive controllers is designed to achieve the synchronization of complex networks, where the restriction between impulses interval and time-varying delays is dropped. A numerical example is presented to illustrate the effectiveness of the obtained results. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAY	2020	125						224	232		10.1016/j.neunet.2020.02.003													
J								End-to-end semantic segmentation of personalized deep brain structures for non-invasive brain stimulation	NEURAL NETWORKS										End-to-end semantic segmentation; Convolutional neural network; Brain stimulation; MRI; tDCS	ELECTRIC-FIELDS; MOTOR; VARIABILITY; MODULATION; DOSIMETRY; NETWORKS; TDCS; MRI	Electro-stimulation or modulation of deep brain regions is commonly used in clinical procedures for the treatment of several nervous system disorders. In particular, transcranial direct current stimulation (tDCS) is widely used as an affordable clinical application that is applied through electrodes attached to the scalp. However, it is difficult to determine the amount and distribution of the electric field (EF) in the different brain regions due to anatomical complexity and high inter-subject variability. Personalized tDCS is an emerging clinical procedure that is used to tolerate electrode montage for accurate targeting. This procedure is guided by computational head models generated from anatomical images such as MRI. Distribution of the EF in segmented head models can be calculated through simulation studies. Therefore, fast, accurate, and feasible segmentation of different brain structures would lead to a better adjustment for customized tDCS studies. In this study, a single-encoder multi-decoders convolutional neural network is proposed for deep brain segmentation. The proposed architecture is trained to segment seven deep brain structures using T1-weighted MRI. Network generated models are compared with a reference model constructed using a semi-automatic method, and it presents a high matching especially in Thalamus (Dice Coefficient (DC) = 94.70%), Caudate (DC = 91.98%) and Putamen (DC = 90.31%) structures. Electric field distribution during tDCS in generated and reference models matched well each other, suggesting its potential usefulness in clinical practice. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAY	2020	125						233	244		10.1016/j.neunet.2020.02.006													
J								Low-rank discriminative regression learning for image classification	NEURAL NETWORKS										Regression; Low-rank; Discriminative; Robust; Image representation	LEAST-SQUARES REGRESSION; FACE-RECOGNITION; SIGNAL RECOVERY; ROBUST; REGULARIZATION; SYSTEMS; RIDGE; NORM	As a famous multivariable analysis technique, regression methods, such as ridge regression, are widely used for image representation and dimensionality reduction. However, the metric of ridge regression and its variants is always the Frobenius norm (F-norm), which is sensitive to outliers and noise in data. At the same time, the performance of the ridge regression and its extensions is limited by the class number of the data. To address these problems, we propose a novel regression learning method which named low-rank discriminative regression learning (LDRL) for image representation. LDRL assumes that the input data is corrupted and thus the L-1 norm can be used as a sparse constraint on the noised matrix to recover the clean data for regression, which can improve the robustness of the algorithm. Due to learn a novel project matrix that is not limited by the number of classes, LDRL is suitable for classifying the data set no matter whether there is a small or large number of classes. The performance of the proposed LDRL is evaluated on six public image databases. The experimental results prove that LDRL obtains better performance than existing regression methods. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAY	2020	125						245	257		10.1016/j.neunet.2020.02.007													
J								Parametric Deformable Exponential Linear Units for deep neural networks	NEURAL NETWORKS										Rectified activation; Deformable exponential; Image classification; Deep learning		Rectified activation units make an important contribution to the success of deep neural networks in many computer vision tasks. In this paper, we propose a Parametric Deformable Exponential Linear Unit (PDELU) and theoretically verify its effectiveness for improving the convergence speed of learning procedure. By means of flexible map shape, the proposed PDELU could push the mean value of activation responses closer to zero, which ensures the steepest descent in training a deep neural network. We verify the effectiveness of the proposed method in the image classification task. Extensive experiments on three classical databases (i.e., CIFAR-10, CIFAR-100, and ImageNet-2015) indicate that the proposed method leads to higher convergence speed and better accuracy when it is embedded into different CNN architectures (i.e., NIN, ResNet, WRN, and DenseNet). Meanwhile, the proposed PDELU outperforms many existing shape-specific activation functions (i.e., Maxout, ReLU, LeakyReLU, ELU, SELU, SoftPlus, Swish) and the shape-adaptive activation functions (i.e., APL, PReLU, MPELU, FReLU). (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAY	2020	125						281	289		10.1016/j.neunet.2020.02.012													
J								Multiple Discrimination and Pairwise CNN for view-based 3D object retrieval	NEURAL NETWORKS										MDPCNN; Pairwise CNN; 3D object retrieval; Multi-view Discrimination	MODEL RETRIEVAL	With the rapid development and wide application of computer, camera device, network and hardware technology, 3D object (or model) retrieval has attracted widespread attention and it has become a hot research topic in the computer vision domain. Deep learning features already available in 3D object retrieval have been proven to be better than the retrieval performance of hand-crafted features. However, most existing networks do not take into account the impact of multi-view image selection on network training, and the use of contrastive loss alone only forcing the same-class samples to be as close as possible. In this work, a novel solution named Multi-view Discrimination and Pairwise CNN (MDPCNN) for 3D object retrieval is proposed to tackle these issues. It can simultaneously input multiple batches and multiple views by adding the Slice layer and the Concat layer. Furthermore, a highly discriminative network is obtained by training samples that are not easy to be classified by clustering. Lastly, we deploy the contrastive-center loss and contrastive loss as the optimization objective that has better intra-class compactness and inter-class separability. Large-scale experiments show that the proposed MDPCNN can achieve a significant performance over the state-of-the-art algorithms in 3D object retrieval. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAY	2020	125						290	302		10.1016/j.neunet.2020.02.017													
J								CNN-MHSA: A Convolutional Neural Network and multi-head self-attention combined approach for detecting phishing websites	NEURAL NETWORKS										Phishing; URL; Deep learning; Convolutional layer; Multi-head self-attention		Increasing phishing sites today have posed great threats due to their terribly imperceptible hazard. They expect users to mistake them as legitimate ones so as to steal user information and properties without notice. The conventional way to mitigate such threats is to set up blacklists. However, it cannot detect one-time Uniform Resource Locators (URL) that have not appeared in the list. As an improvement, deep learning methods are applied to increase detection accuracy and reduce the misjudgment ratio. However, some of them only focus on the characters in URLs but ignore the relationships between characters, which results in that the detection accuracy still needs to be improved. Considering the multi-head self-attention (MHSA) can learn the inner structures of URLs, in this paper, we propose CNN-MHSA, a Convolutional Neural Network (CNN) and the MHSA combined approach for highly-precise. To achieve this goal, CNN-MHSA first takes a URL string as the input data and feeds it into a mature CNN model so as to extract its features. In the meanwhile, MHSA is applied to exploit characters' relationships in the URL so as to calculate the corresponding weights for the CNN learned features. Finally, CNN-MHSA can produce highly-precise detection result for a URL object by integrating its features and their weights. The thorough experiments on a dataset collected in real environment demonstrate that our method achieves 99.84% accuracy, which outperforms the classical method CNN-LSTM and at least 6.25% higher than other similar methods on average. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAY	2020	125						303	312		10.1016/j.neunet.2020.02.013													
J								Improved multi-view GEPSVM via Inter-View Difference Maximization and Intra-view Agreement Minimization	NEURAL NETWORKS										Multi-view learning; GEPSVM; L1-norm; IMvGEPSVM; Robustness	SUPPORT VECTOR MACHINE; DISCRIMINANT-ANALYSIS; CLASSIFICATION; EFFICIENT	Multiview Generalized Eigenvalue Proximal Support Vector Machine (MvGEPSVM) is an effective method for multiview data classification proposed recently. However, it ignores discriminations between different views and the agreement of the same view. Moreover, there is no robustness guarantee. In this paper, we propose an improved multiview GEPSVM (IMvGEPSVM) method, which adds a multi-view regularization that can connect different views of the same class and simultaneously considers the maximization of the samples from different classes in heterogeneous views for promoting discriminations. This makes the classification more effective. In addition, L1-norm rather than squared L2-norm is employed to calculate the distances from each of the sample points to the hyperplane so as to reduce the effect of outliers in the proposed model. To solve the resulting objective, an efficient iterative algorithm is presented. Theoretically, we conduct the proof of the algorithm's convergence. Experimental results show the effectiveness of the proposed method. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAY	2020	125						313	329		10.1016/j.neunet.2020.02.002													
J								New criteria for global stability of neutral-type Cohen-Grossberg neural networks with multiple delays	NEURAL NETWORKS										Neutral systems; Delayed neural networks; Stability analysis; Lyapunov stability theorems	TIME-VARYING DELAY; EXPONENTIAL STABILITY; DEPENDENT STABILITY; DISTRIBUTED DELAYS; SYSTEMS; DISCRETE; STORAGE; DESIGN	The significant contribution of this paper is the addressing the stability issue of neutral-type Cohen-Grossberg neural networks possessing multiple time delays in the states of the neurons and multiple neutral delays in time derivative of states of the neurons. By making the use of a novel and enhanced Lyapunov functional, some new sufficient stability criteria are presented for this model of neutral-type neural systems. The obtained stability conditions are completely dependent of the parameters of the neural system and independent of time delays and neutral delays. A constructive numerical example is presented for the sake of proving the key advantages of the proposed stability results over the previously reported corresponding stability criteria for Cohen-Grossberg neural networks of neutral type. Since, stability analysis of Cohen-Grossberg neural networks involving multiple time delays and multiple neutral delays is a difficult problem to overcome, the investigations of the stability conditions of the neutral-type the stability analysis of this class of neural network models have not been given much attention. Therefore, the stability criteria derived in this work can be evaluated as a valuable contribution to the stability analysis of neutral-type Cohen-Grossberg neural systems involving multiple delays. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAY	2020	125						330	337		10.1016/j.neunet.2020.02.020													
J								Constructing large-scale cortical brain networks from scalp EEG with Bayesian nonnegative matrix factorization	NEURAL NETWORKS										Bayesian NMF; Large-scale network; Functional network connectivity; EEG; Decision-making	INDEPENDENT COMPONENT ANALYSIS; DECISION-MAKING; ULTIMATUM GAME; FUNCTIONAL CONNECTIVITY; NEURAL RESPONSES; UNFAIR OFFERS; ATTENTION; FAIRNESS; FMRI; ALGORITHMS	A large-scale network provides a high hierarchical level for understanding the adaptive adjustment of the human brain during cognition processes. Since high spatial resolution is required, most of the related works are based on functional magnetic resonance imaging (fMRI); however, fMRI lacks the temporal information that is important in investigating the high cognition processes. Although combining electroencephalography (EEG) inverse solution and independent component analysis (ICA), researchers detected large-scale functional subnetworks recently, few researchers focus on the unreasonable negative activation, which is biased from the nonnegative electrical source activations in the brain. In this study, considering the favorable nonnegative property of Bayesian nonnegative matrix factorization (Bayesian NMF) and combining EEG source imaging, we developed a robust approach for EEG large-scale network construction and applied it to two independent real EEG datasets (i.e., decision-making and P300). Eight and nine best-fit networks, including such important subnetworks as the somatosensory-motor network (SMN), the default mode network (DMN), etc., were successfully identified for decision-making and P300, respectively. Compared to the networks acquired with ICA, these networks not only lacked confusing negative activations but also showed clear spatial distributions that are compatible with specific brain function. Based on the constructed large-scale network, we further probed that the self-referential network (SRN), the primary visual network (PVN), and the visual network (VN) demonstrated different interaction patterns with other networks between different responses in decision-making. Our results confirm the possibility of probing the neural mechanisms of high cognition processes at a very high temporal and spatial resolution level. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAY	2020	125						338	348		10.1016/j.neunet.2020.02.021													
J								SOMprocessor: A high throughput FPGA-based architecture for implementing Self-Organizing Maps and its application to video processing	NEURAL NETWORKS										Self-organizing Maps; Neuromorphic chip; Unsupervised learning; FPGA; Video surveillance	VECTOR-QUANTIZATION; IP CORE; PARALLEL; ACCELERATOR; PERFORMANCE; PREDICTION; ALGORITHM; CIRCUIT; NETWORK; SIGNALS	The design of neuromorphic chips aims to develop electronic circuits dedicated to executing artificial neural networks, mainly by exploring parallel processing. Unsupervised learning models, such as Self-organizing Maps (SOM), may benefit from massively concurrent hardware-based implementations to meet the requirements of real-time and embedded applications. This work first presents a theoretical analysis of the algorithms implemented in hardware to compute SOM learning and recall phases. This is important because, albeit similar, the processing steps executed in hardware are not necessarily identical to those executed in software. Then, the proposed FPGA architecture entitled SOMprocessor is shown in detail. The circuit of the processor explores two different computational strategies for increasing the performance of current state-of-the-art works. These computational strategies aim to improve the data flow through the processor and its flexibility to implement different network topologies. Finally, this work presents the application of the SOMprocessor to a video categorization task. The results show that topographic and quantization errors are similar between hardware and software implementations, as well as the overall accuracy. Moreover, the proposed FPGA architecture achieves acceleration of 3 to 4 orders of magnitude as compared to CPU executions. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAY	2020	125						349	362		10.1016/j.neunet.2020.02.019													
J								Towards Real-Time Path Planning through Deep Reinforcement Learning for a UAV in Dynamic Environments	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Unmanned aerial vehicle (UAV); Path planning; Reinforcement learning; Deep Q-network; STAGE scenario		Path planning remains a challenge for Unmanned Aerial Vehicles (UAVs) in dynamic environments with potential threats. In this paper, we have proposed a Deep Reinforcement Learning (DRL) approach for UAV path planning based on the global situation information. We have chosen the STAGE Scenario software to provide the simulation environment where a situation assessment model is developed with consideration of the UAV survival probability under enemy radar detection and missile attack. We have employed the dueling double deep Q-networks (D3QN) algorithm that takes a set of situation maps as input to approximate the Q-values corresponding to all candidate actions. In addition, the epsilon-greedy strategy is combined with heuristic search rules to select an action. We have demonstrated the performance of the proposed method under both static and dynamic task settings.																	0921-0296	1573-0409				MAY	2020	98	2					297	309		10.1007/s10846-019-01073-3													
J								Lyapunov Theory Based Adaptive Neural Observers Design for Aircraft Sensors Fault Detection and Isolation	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Aircraft; Sensors; Artificial intelligence; Neurl network; Fault detection	TOLERANT CONTROL; NONLINEAR-SYSTEMS; PROJECT INDUSTRIAL; ACTUATOR FAULTS; QUADROTOR; DIAGNOSIS; VEHICLE	In this research, two novel online fault detection algorithms are proposed for sensor faults in aircraft. Radial basis function neural network (RBFNN) is used as a fault detection technique, for weight updating parameters adaptive learning rates are used instead of fixed learning rates, two different adaptive learning rate strategies are proposed based on Lyapunov functions which are compared to Extended Kalman Filter (EKF) algorithm. Boeing 747-100/200 aircraft is used for testing and validation of these algorithms. All algorithms have the ability to detect various types of faults such as simultaneous, intermittent, abrupt and incipient with high preciseness and accuracy occur in aircraft sensors. The capability of sensors fault detection of all algorithms are compared, it is proved that all algorithms have the ability to detect faults but Lyapunov function theory II based algorithm is more efficient and having a fast response in faults detection as compared to Lyapunov function theory I and EKF based algorithms. It is also proved that the Lyapunov function theory II based algorithm is more effective in reducing the computational time and computation load.																	0921-0296	1573-0409				MAY	2020	98	2					311	323		10.1007/s10846-019-01098-8													
J								Adaptive Visual Regulation of Wheeled Mobile Robots: a Switching Approach	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Visual regulation; Wheeled mobile robots; Adaptive control; Invariant manifold	STABILIZATION; NAVIGATION	This paper deals with the visual regulation problem of wheeled mobile robots (WMRs) in the presence of uncalibrated camera-to-robot parameters and unknown image depth. A two-stage controller is designed by using a switching approach. Specifically, in the first stage, an invariant-manifold-based adaptive controller is presented to bring the lateral error and angular error within an arbitrarily small neighborhood of zero; in the second stage, the longitudinal error is regulated by employing a proportional controller. Utilizing the Lyapunov stability analysis, the exponentially bounded stability of the closed-loop system is proved. Both simulation and experimental results are presented to validate the effectiveness of the proposed approach.																	0921-0296	1573-0409				MAY	2020	98	2					345	358		10.1007/s10846-019-01065-3													
J								Dynamic Leader Allocation in Multi-robot Systems Based on Nonlinear Model Predictive Control	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Nonlinear model predictive control; Behavior-based robotics; Formation control; Multi-robot systems	SELECTION	This paper presents an approach to the dynamic leader selection problem in autonomous non-holonomic mobile robot formations when the current leader enters a failure state. Our method is based on a tree structure coupled with a modified version of the Nonlinear Model Predictive Control (NMPC) that allows for behavior change at the controller level. An explanation of the control algorithm, behavior selection, and leader selection structure is given, after which the results of both simulations and experiments using a three robot formation are shown and discussed.																	0921-0296	1573-0409				MAY	2020	98	2					359	376		10.1007/s10846-019-01064-4													
J								Online Object Detection and Localization on Stereo Visual SLAM System	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Semantic SLAM; Deep learning; Object detection; Object localization		In order to navigate an unknown environment, an autonomous robot must be able to build a map of its surroundings while estimating its position at the same time. This problem is known as SLAM. We propose a SLAM system for stereo cameras which builds a map of objects in a scene. The system is based on the SLAM method S-PTAM and an object detection module. The object detection module uses Deep Learning to perform online detection and provide the 3d pose estimations of objects present in an input image, while S-PTAM estimates the camera pose in real time. The system was tested on a real world environment, achieving good object localization results.																	0921-0296	1573-0409				MAY	2020	98	2					377	386		10.1007/s10846-019-01074-2													
J								Controlling a Quadrotor Carrying a Cable-Suspended Load to Pass Through a Window	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Micro aerial vehicles; Model identification; Optimal control; Trajectory generation	UNMANNED AERIAL VEHICLES; TRAJECTORY GENERATION; SYSTEM-IDENTIFICATION; FLIGHT-TEST; LOW-COST	In this paper, we design an optimal control system for a quadrotor to carry a cable-suspended load flying through a window. As the window is narrower than the length of the cable, it is very challenging to design a practical control system to pass through it. Our solution includes a system identification component, a trajectory generation component, and a trajectory tracking control component. The exact dynamic model that usually derived from the first principles is assumed to be unavailable. Instead, a model identification approach is adopted, which relies on a simple but effective low order equivalent system (LOES) to describe the core dynamical characteristics of the system. After being excited by some specifically designed manoeuvres, the unknown parameters in the LOES are obtained by using a frequency based least square estimation algorithm. Based on the estimated LOES, a numerical optimization algorithm is then utilized for aggressive trajectory generation when relevant constraints are given. The generated trajectory can lead to the quadrotor and load system passing through a narrow window with a cascade PD trajectory tracking controller. Finally, a practical flight test based on an Astec Hummingbird quadrotor is demonstrated and the result validates the proposed approach.																	0921-0296	1573-0409				MAY	2020	98	2					387	401		10.1007/s10846-019-01038-6													
J								Bayesian Mapping-Based Autonomous Exploration and Patrol of 3D Structured Indoor Environments with Multiple Flying Robots	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Cooperative patrol; Multi-vehicle autonomous exploration; 3D occupancy grid mapping		Mobile robots are frequently faced with mapping and exploring uncertain environments in surveillance, military, and convenience tasks. Often times, human teleoperation is either inconvenient or infeasible for these kinds missions. Furthermore, these tasks can be improved by cooperative multi-agent systems, where coordinating robotic efforts can be complicated and computationally-expensive. This paper presents a stochastic framework for autonomous exploration and patrol with multiple cooperating robots. The first contribution extends the authors' prior work in single-robot exact occupancy grid mapping and autonomous exploration in a 2D environment to mapping and exploring in a 3D environment. The proposed 3D occupancy grid map is computed efficiently using an inverse sensor model that accounts for the sensor uncertainty, where we propose how several measurement sources may be fused together by considering depth readings individually. This approach is scalable to larger and more complex scenarios for real-time mapping. Furthermore, this paper shows how important aspects of a 3D map representing a structured environment are projected onto a 2D occupancy grid map, where an autonomous exploration algorithm is designed to select robotic motions that maximize map information gain. The mapping and exploration algorithms are demonstrated with an experiment where a quadrotor autonomously maps and explores an initially-uncertain environment. The second contribution is a novel approach to multi-vehicle cooperative patrol of environments based on map uncertainty. We propose a cooperative autonomous exploration algorithm, which applies a bidding-based framework to coordinate robotic efforts for improving occupancy grid map information gain. Since these exploration approaches are based on probabilistic knowledge about the map, the 3D occupancy grid map is systematically degraded over time to encourage the robots to revisit regions as time passes, thereby patrolling the environment. Furthermore, using a Bayesian framework and receding horizons, the algorithm is robust to dynamic obstacles within the mapping space. The efficacy of the proposed multi-vehicle cooperative patrol is illustrated with a simulation involving three robots patrolling a large floor plan with a non-cooperative person walking around the space.																	0921-0296	1573-0409				MAY	2020	98	2					403	419		10.1007/s10846-019-01066-2													
J								PIE: a Tool for Data-Driven Autonomous UAV Flight Testing	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										UAV; Decision tree; Test & evaluation; Data-driven autonomous flight and behavior analysis; AR drone; 3DR IRIS; SOLO; Intel RTF drone; ROS; Gazebo; VICON	CLASSIFICATION; MODEL	In this paper, a novel technique is presented to test the flight of an unmanned aerial vehicle autonomously in a real-world scenario using a data-driven technique without intervening with its onboard software. With the growing applications of such vehicles, testing of autonomous flight is a very important task for rapid deployment. There are different tools for modeling and simulating unmanned vehicles in virtual worlds such as Gazebo, MATLAB, Simulink, and Webots to name a few. None of these simulation tools are able to model all possible physical parameters of a real-world environment. Hence, the flight controller or mission planning software has to be tested in the physical world in the presence of an expert before deployment for a specific task. A Perception Inference Engine evaluation tool is presented that can infer internal states of the autonomous system from external observations only. The Gazebo simulation platform is used to collect data to develop the perception model. For real-time data collection, a VICON motion capture system is used to observe the autonomous flight of a small unmanned aerial vehicle. A state-of-the-art decision tree algorithm is used to implement the data-driven approach. The technique was tested using simulation data and verified with real-time data from Intel Aero Ready to Fly and Parrot AR. 2.0 drones. Moreover, we analyzed the robustness of the proposed system by introducing noise in sensor measurement and ambiguity in the testing scenario. We compared the performance of the decision tree classifier with Naive bayes and support vector machine classifiers. It is shown that the developed system can be used for the performance evaluation of a UAV operating in the physical world by significantly reducing uncertainty in mission failure due to environmental parameters.																	0921-0296	1573-0409				MAY	2020	98	2					421	438		10.1007/s10846-019-01078-y													
J								A Robust Model Predictive Control Strategy for Trajectory Tracking of Omni-directional Mobile Robots	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Trajectory tracking control; Model predictive control; Quadratic programming; Delayed neural network; Omni-directional mobile robot	NEURAL-NETWORK; OPTIMIZATION	This paper proposes a robust model predictive control (MPC) strategy for the trajectory tracking control of a four-mecanum-wheeled omni-directional mobile robot (FM-OMR) under various constraints. The method proposed in this paper can solve various constraints while implementing trajectory tracking of the FM-OMR. Firstly, a kinematics model with constraint relationship of the FM-OMR is established. On the basis of the kinematics model, the kinematics trajectory tracking error model of the FM-OMR is further formulated. Then, it is transformed into a constrained quadratic programming(QP) problem by the method of MPC. In addition, aiming at the speed deficiencies of conventional neural networks in QP solving, a delayed neural network (DNN) is applied to solve the optimal solution of the QP problem, and compared with the Lagrange programming neural network (LPNN) to show the rapidity of the DNN. Finally, two simulation cases considering bounded random disturbance are provided to verify the robustness and effectiveness of the proposed method. Theoretical analysis and simulation results show that the control strategy is effective and feasible.																	0921-0296	1573-0409				MAY	2020	98	2					439	453		10.1007/s10846-019-01083-1													
J								Socially Acceptable Navigation of People with Multi-robot Teams	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Multi-robot systems; Human-robot interaction	AWARE NAVIGATION; BEHAVIOR; ROBOTS	Socially acceptable navigation is a subject that involves developments regarding Human-Robot Interaction (HRI) and autonomous mobile robot navigation. In this context, there is little research considering people interacting with a robot team, and even fewer considering multi-robot teams with people. In this paper, a study on socially acceptable navigation involving a human and a robot team is presented. Four navigation strategies that consider social aspects are presented and compared in simulated environment by terms as the average number of robots invading the personal space and the number of robots to the person's side, with two of them using Asymmetric Gaussian Functions (AGFs) as the person's social zone model; a navigation perception comparison is made involving people to investigate their view on navigating with three robots in contrast to a single robot. Simulated and real-world experiments were performed showing that the proposed methods have advantages over each other on different aspects. The follow-up techniques for interaction between humans and a team of robots suggest that people's perception may not be affected significantly when interacting with more than one robot.																	0921-0296	1573-0409				MAY	2020	98	2					481	510		10.1007/s10846-019-01080-4													
J								A Simple Approach to Regulate a PVTOL System Using Matching Conditions	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										PVTOL aircraft; Super-twisting based observer; Passivity based control; Lyapunov method	PASSIVITY-BASED CONTROL; MECHANICAL SYSTEMS; GLOBAL STABILIZATION; CONTROL STRATEGY; INTERCONNECTION; AIRCRAFT; DESIGN	In this work, based on shaping an energy function, it is introduced an output-feedback regulation control law for a Planar Vertical Takeoff and Landing (PVTOL) aircraft. To this end, a version of the matching control energy method is used to construct the Lyapunov candidate function and the stabilizer controller to regulate the PVTOL aircraft. The non-measurable system velocities are recovered using a suitable exact differentiator observer, achieved through super-twisting based observer. The theorem of LaSalle, together with some tools from the cascade system theory, was used to carry out the closed-loop stability analysis. Using a suitable finite time-varying identificator, the obtained controllers were improved to compensate bounded, smooth, and matching perturbations. Numerical simulations were included to test the effectiveness of the obtained controller.																	0921-0296	1573-0409				MAY	2020	98	2					511	524		10.1007/s10846-019-01087-x													
J								Influence of a Compatible Design on Physical Human-Robot Interaction Force: a Case Study of a Self-Adapting Lower-Limb Exoskeleton Mechanism	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Lower-limb exoskeleton mechanism; Compatible design; Axis misalignment; Self-adapting; Physical human-robot interaction force	LEG EXOSKELETON; KNEE-JOINT; GAIT; RECOVERY; MOVEMENT; THERAPY; STROKE; MODEL; ARM	In the kinematic design of wearable exoskeletons, the issue of axis misalignments between the human and the exoskeleton joints should be well dealt with. Otherwise, large physical human-robot interaction (p-HRI) forces may occur at the human-robot interfaces, which makes the p-HRI uncomfortable or even unsafe. To cope with this issue, a kinematically compatible design approach of wearable exoskeletons has been investigated by researchers, and great development has been made in recent years. Moreover, the influence of such a design on the exoskeleton's p-HRI performance should be evaluated to determine if the design is feasible. In this paper, a self-adapting lower-limb exoskeleton mechanism for three degrees of freedom gait training is proposed, and the mechanical structure of the exoskeleton mechanism is designed in detail. Then, based on the presented exoskeleton mechanism and the use of suitable force/torque sensors, a p-HRI force measurement system is developed. Subsequently, the p-HRI forces of the human-robot closed chain under the static and motion modes are detected, and the influence of the self-adapting design on the lower-limb exoskeleton mechanism's p-HRI force feature is evaluated. The results indicate that additional human-robot connective joints could reduce the p-HRI force significantly, the compatible design of the exoskeleton mechanism is effective, and is thus applied to human lower-limb gait training.																	0921-0296	1573-0409				MAY	2020	98	2					525	538		10.1007/s10846-019-01063-5													
J								A decision-making algorithm for online shopping using deep-learning-based opinion pairs mining and q-rung orthopair fuzzy interaction Heronian mean operators	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										deep learning; online shopping; opinion pairs mining; q-rung orthopair fuzzy interaction Heronian mean operators	SENTIMENT ANALYSIS; AGGREGATION OPERATORS; REVIEWS; SELECTION; CLASSIFICATION; MODELS	In the process of online shopping, consumers usually compare the review information of the same product in different e-commerce platforms. The sentiment orientation of online reviews from different platforms interactively influences on consumers' purchase decision. However, due to the limitation of the ability to process information manually, it is difficult for a consumer to accurately identify the sentiment orientation of all reviews one by one and describe the process of their interactive influence. To this end, we proposed an online shopping support model using deep-learning-based opinion mining and q-rung orthopair fuzzy interaction weighted Heronian mean (q-ROFIWHM) operators. First, in the proposed method, the deep-learning model is used to automatically extract different product attribute words and opinion words from online reviews, and match the corresponding attribute-opinion pairs; meanwhile, the sentiment dictionary is used to calculate sentiment orientation, including positive, negative, and neutral sentiments. Second, the proportions of the three kinds of sentiments about each attribute of the same product are calculated. According to the proportion value of attribute sentiment from different platforms, the sentiment information is converted into multiple cross-decision matrices, which are represented by the q-rung orthopair fuzzy set. Third, considering the interactive characteristics of decision matrix, the q-ROFIWHM operators are proposed to aggregate this cross-decision information, and then the ranking result was determined by score function to support consumers' purchase decisions. Finally, an actual example of mobile phone purchase is given to verify the rationality of the proposed method, and the sensitivity and the comparison analysis are used to show its effectiveness and superiority.																	0884-8173	1098-111X				MAY	2020	35	5					783	825		10.1002/int.22225													
J								Robust adaptive multivariate Hotelling's T-2 control chart based on kernel density estimation for intrusion detection system	EXPERT SYSTEMS WITH APPLICATIONS										Fast-MCD; Kernel density estimation; Hotelling's T-2 chart; Intrusion detection; Statistical process control	ANOMALY DETECTION; STATISTICAL-ANALYSIS; ECONOMIC DESIGN; NETWORK; ALGORITHM; MACHINE; RISK; SET; PCA	The utilization of conventional multivariate control chart in network intrusion detection will deal with two main problems. First, the high false alarm occurs due to the distribution of network traffic data that is not following the theory. Second, the inability of the control chart to detect outliers caused by the masking effect. To overcome these problems, the multivariate control chart based on the fast minimum covariance determinant (MCD) algorithm and kernel density estimation (KDE) is proposed in this paper. The employment of KDE technique is expected to adaptively follow the network traffic data pattern, thereby reducing the occurrence of false alarms. Meanwhile, the usage of Fast-MCD will improve the capabilities of the proposed control chart to quickly and accurately detect the outliers. For the simulated data, the proposed chart shows a better level of accuracy when it is compared to conventional T-2 and other robust 72 based on successive difference covariate matrix (SDSM) charts. For the data generated from some distributions, the proposed chart shows its adaptability by producing low false alarm with high detection rate. The proposed chart shows excellent performance to monitor the KDD99 dataset with 98.61% accuracy, NSL-KDD dataset with 91.71% accuracy, and UNSW-NB 15 dataset with 91.02% accuracy. The proposed method has consistent performance when monitoring the small subset of the datasets, which can minimize the computational time by more than 90% without decreasing its level of accuracy and precision. Also, the performance from the proposed chart surpasses the other benchmarks. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 1	2020	145								113105	10.1016/j.eswa.2019.113105													
J								A hybrid approach for portfolio selection with higher-order moments: Empirical evidence from Shanghai Stock Exchange	EXPERT SYSTEMS WITH APPLICATIONS										Portfolio optimization; Higher-order moments; Genetic algorithm; Machine learning algorithm	MEAN-VARIANCE MODELS; SKEWNESS; OPTIMIZATION	Skewness and kurtosis, the third and fourth order moments, are statistics to summarize the shape of a distribution function. Recent studies show that investors would take these higher-order moments into consideration to make a profitable investment decision. Unfortunately, due to the difficulties in solving the multi-objective problem with higher-order moments, the literature on portfolio selection problem with higher-order moments is few. This paper proposes a new hybrid approach to solve the portfolio selection problem with skewness and kurtosis, which includes not only the multi-objective optimization but also the data-driven asset selection and return prediction, where the techniques of two-stage clustering, radial basis function neural network and genetic algorithm are employed. With the historical data from Shanghai stock exchange, we find that the out-of-sample performance of our model with higher-order moments is significantly better than that of traditional mean-variance model and verify the robustness of our hybrid algorithm. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 1	2020	145								113104	10.1016/j.eswa.2019.113104													
J								Use of support vector machines with a parallel local search algorithm for data classification and feature selection	EXPERT SYSTEMS WITH APPLICATIONS										Support vector machines; Feature selection; Classification; Heuristic; Machine learning	EXTRACTION	Over the last decade, the number of studies on machine learning has significantly increased. One of the most widely researched areas of machine learning is data classification. Most big data systems require a large amount of information storage for analytic purposes; however, this involves some disadvantages, such as the costs of processing and collecting data. Thus, many researchers and practitioners are working on effectively reducing the number of features used in classification. This paper proposes a method which jointly optimizes both feature selection and classification. A survey of the relevant literature shows that the vast majority of studies focus on either feature selection or classification. In this study, the proposed parallel local search algorithm both selects features and finds a classifier with high rates of accuracy. Moreover, the proposed method is capable of finding solutions for problems that have extremely high numbers of features within a reasonable computation time. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 1	2020	145								113133	10.1016/j.eswa.2019.113133													
J								Iterated greedy with variable neighborhood search for a multiobjective waste collection problem	EXPERT SYSTEMS WITH APPLICATIONS										Metaheuristics; Multiobjective optimization; Vehicle routing	VEHICLE-ROUTING PROBLEM; GENETIC ALGORITHM; SCATTER SEARCH; TABU SEARCH; OPTIMIZATION; MODEL; SYSTEM; GIS; TRANSPORTATION; ROUTES	In the last few years, the application of decision making to logistic problems has become crucial for public and private organizations. Efficient decisions clearly contribute to improve operational aspects such as cost reduction or service improvement. The particular case of waste collection service considered in this paper involves a set of economic, labor and environmental issues that translate into difficult operational problems. They pose a challenge to nowadays optimization technologies since they have multiple constraints and multiple objectives that may be in conflict. We therefore need to resort to multiobjective approaches to model and solve this problem, providing efficient solutions in short computational times. In particular, we consider four different objectives to model the waste collection problem: travel cost, route length balance, route time balance, and number of routes. We propose an iterated greedy algorithm coupled with a variable neighborhood search to minimize an achievement function to determine a good approximation to the Pareto front. The performance of our method is empirically analyzed on a set of instances (both generated and real), and compared with the well-known NSGA-II and SPEA2 methods. The comparison favors our proposal. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 1	2020	145								113101	10.1016/j.eswa.2019.113101													
J								A hierarchical structure based on Stacking approach for skin lesion classification	EXPERT SYSTEMS WITH APPLICATIONS										Skin cancer; Melanoma; Ensemble classifiers; Meta learning; Stacking	FEATURE-SELECTION; METHODOLOGICAL APPROACH; MELANOMA; DIAGNOSIS; FEATURES; SYSTEM; SHAPE	Malignant melanoma is the most dangerous type of skin cancer. The diagnosis of melanoma in the early stages can greatly increase the possibility of its successful treatment. In the recent years, automated systems have played an pivotal role in increasing the skin cancer diagnosis rate. The main objective of this paper was to improve the performance of a skin cancer automated diagnostic system by introducing a new approach to combining classifiers in the classification stage. Therefore, the Stacking Ensemble Method based on the Meta Learning algorithm was proposed for the skin lesion classification. To classify skin lesions as melanoma, dysplastic and benign, two new hybrid approaches of Structure Based on Stacking (SBS) and Hierarchical Structure Based on Stacking (HSBS) were introduced to combine the heterogeneous classifiers. The proposed methods for skin lesions classification were implemented and evaluated based on the dermoscopic images of two PH2 and Ganster datasets using the Five Fold Cross Validation procedure and different numbers of the selected features. The results showed that the SBS approach had a good performance in diagnosing melanoma lesions from non-melanoma lesions for both datasets. Moreover, the results indicated that the HSBS method compared to the SBS approach and other works on the same dataset offers a far better performance in classifying skin lesions as benign, dysplastic and melanoma. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 1	2020	145								113127	10.1016/j.eswa.2019.113127													
J								A simulation-optimisation genetic algorithm approach to product allocation in vending machine systems	EXPERT SYSTEMS WITH APPLICATIONS										Vending machine systems; Portfolio optimisation; Genetic algorithm; Simulation	DEMAND; MODEL	In recent years, vending machines have seen increasing levels of popularity. In a fast-paced world where convenience and accessibility of products is highly sought after the vending industry has provided a suitable solution. Although the economic impact of the vending industry is indisputable, it is not without challenges, especially when it comes to the efficiency of the vending logistics operations. The optimisation of logistic vending machine systems is decidedly complex. Product allocation to columns in a vending machine, replenishment points of products, product thresholds at vending machines, and vehicle routes for inventory replenishments are all essential challenges in vending machine system management and operation. If all facets of the problem were to be addressed, it would require techniques such as forecasting, machine learning, data mining, combinatorial optimization and vehicle routing, among others. In the past, these approaches have been explored individually despite their intrinsic interdependence within the problem. This paper aims to help to fill in this gap and proposes a model for the optimisation of product allocation within a vending machine under the constraint of fixed restocking instances. The optimal product allocation is based on the definition of product profitability which accounts for the net revenue earned after the cost of restock, as opposed to the revenue earned until first stock-out to prevent arbitrary extension of the stock-out period. The whole approach is encompassed in the simulation optimisation framework that utilises a Genetic Algorithm, with fitness evaluated as simulated revenue, to determine the optimal product allocation. The acceptable threshold of missed sales for a machine is also determined as a means to make intelligent restocking decisions. Overall, the proposed approach allows the strengths of mathematically robust optimization algorithms and the implementation of analytic solutions to be combined and applied to realistic scenarios where uncertainty may rule out some high quality analytic solutions. It respects problem intricacies proper to vending and addresses the interdependence between routing and portfolio optimisation. The proposal is application-driven and stems from a collaboration with an industry partner. The model is validated against an authentic data set supplied by the partner. The case study results revealed a network-wide improvement in net revenue of approximately 3.4%, with varied efficacy based on machine popularity. The method of optimisation was found to be significantly more effective for higher performing machines, with median improvements as high as 6%. Our framework based on the optimization-simulation model yields clear benefits to vending logistics operations management. The simulation component provides the decision maker with a more comprehensive view on the actual implementation of the solution. Effectively, the joint use of simulation and optimization methods provides managers with enhanced information to help decide on both: (i) the most beneficial product portfolio, and (ii) the quality of the proposed restocking schedules. Simulation-optimisation based approach is a powerful technique used to address stochastic problems. However, it was yet to be applied specifically to logistic vending machine systems. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 1	2020	145								113110	10.1016/j.eswa.2019.113110													
J								An Efficient JAYA Algorithm with Levy Flight for Non-linear Channel Equalization	EXPERT SYSTEMS WITH APPLICATIONS										Non-linear channel equalization; Functional link artificial neural network; Back-propagation algorithm; JAYA algorithm	PARTICLE SWARM OPTIMIZATION; ARTIFICIAL NEURAL-NETWORK; GREY WOLF OPTIMIZER; DIFFERENTIAL EVOLUTION; DESIGN OPTIMIZATION; SEARCH PATTERNS; CUCKOO SEARCH; IDENTIFICATION; RECONSTRUCTION; COMBINATION	Neural network (NN) based equalizers are known to outperform the linear equalizers based on finite impulse response (FIR) adaptive filter for highly dispersive and non-linear channels. To overcome the limitations of the back-propagation (BP) algorithm, metaheuristic algorithms are emerging as promising alternatives for training the NN based equalizers. JAYA is a simple and efficient metaheuristic algorithm. Hence, its application to channel equalization problem is worth investigating. Despite its simplicity and efficiency, the JAYA algorithm has problems such as being trapped in local minima due to insufficient diversity of population and weak exploration capability. To alleviate these issues, in this paper the concept of Levy flight (LF) and greedy selection scheme has been incorporated into the basic JAYA algorithm. The LF concept enhances the population diversity and thus avoids the state of stagnation. The greedy selection scheme is employed to improve the exploitation ability without loss of population diversity. Furthermore, in order to maintain the balance between the exploration and exploitation capabilities of the algorithm, an adaptive Levy index is proposed based on a linear control parameter strategy. An extensive simulation-based sensitivity analysis of proposed method called JAYA algorithm with Levy flight (JAYALF) with respect to key parameters is carried out to select the optimized values for these parameters. In order to validate the local optima avoidance ability, exploitation and convergence rate of the proposed JAYALF algorithm, it is tested on seventeen well-known unimodal and multimodal benchmark functions and to verify the effectiveness of the JAYALF for non-linear channel equalization problem, three wireless communication channels with two different nonlinearities have been considered for simulation. In addition, the non-parametric pairwise Wilcoxon rank-sum test has been employed to test the statistical validity of the results obtained from JAYALF. The results of experiments and statistical test demonstrate that the proposed algorithm significantly outperforms JAYA, variants of JAYA, state-of-the-art algorithms and BP algorithm in terms of solution quality, convergence speed, and robustness. Furthermore, the results of experimental analyses conducted indicate that proposed JAYALF algorithm has a better exploration ability and rapidly converges without getting stuck in local optima. (C) 2019 Published by Elsevier Ltd.																	0957-4174	1873-6793				MAY 1	2020	145								112970	10.1016/j.eswa.2019.112970													
J								Synthetic accessibility assessment using auxiliary responses	EXPERT SYSTEMS WITH APPLICATIONS										Synthetic accessibility; Semi-expert; Auxiliary responses	PREDICTION; CHEMISTS	Despite the recent advances in computational approaches to discovering new chemical compounds, accessibility assessment of designed compounds has still been a difficult task to automate because it is a heavily knowledge intensive task. A promising solution to such "Al-hard" tasks is collective intelligence approaches that aggregate opinions of a group of human non-experts or semi-experts. However, the existing aggregation methods rely only on synthetic accessibility evaluation scores given by humans, and they do not exploit auxiliary information obtained as byproducts of human evaluations such as that related to chemical structures. In this paper, we propose to exploit such auxiliary responses to obtain better aggregations. We introduce a new two-stage aggregation method of semi-expert judgments consisting of synthetic accessibility evaluation scores along with auxiliary responses that select substructures of targets obstructive to their synthesis. The first stage divides both semi-experts and substructures into clusters using stochastic block models to identify similar skills or properties. The second stage aggregates judgments while considering groups of semi-experts and substructures, and predicts synthetic accessibility. Our experiments show that the use of auxiliary responses improves the prediction. performance and gives insight into evaluators and the structure of evaluated compounds. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 1	2020	145								113106	10.1016/j.eswa.2019.113106													
J								Trendlets: A novel probabilistic representational structures for clustering the time series data	EXPERT SYSTEMS WITH APPLICATIONS										Time series; Clustering; Pattern matching; Trend; Seasonality	SELECTION	Time series data is a sequence of values recorded systematically over a period which are mostly used for prediction, clustering, and analysis. The two essential features of a time series data are trend and seasonality. Preprocessing of the time series data is necessary for performing prediction tasks. In most of the cases, the trend and the seasonality are removed before applying the regression algorithms. The accuracy of such algorithms depends upon the functions used for the removal of trend and seasonality. Clustering of an unlabeled time series data with the presence of trend and seasonality is challenging. In this paper, we propose a probabilistic representational learning method for grouping the time series data. We introduce five terminologies in our method of clustering namely the trendlets, uplets, downlets, equalets and trendlet string. These elements are the representational building blocks of our proposed method. Experiments on the proposed algorithm are performed with the renewable energy data on the electricity supply system of continental Europe which includes the demand and inflow of renewable energy for the term 2012 to 2014 and UCR-2018 time series archive containing 128 datasets. We compared our proposed representational method with various clustering algorithms using the silhouette score. Mini-batch k-means and agglomerative hierarchical clustering algorithms show better performance in terms of quality, logical accordance with data and time taken for clustering. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 1	2020	145								113119	10.1016/j.eswa.2019.113119													
J								A new technique with improved control quality of nonlinear systems using an optimized fuzzy logic controller	EXPERT SYSTEMS WITH APPLICATIONS										Improved control quality; Multi-criteria decision making; Non-dominated sorting genetic algorithm; Nonlinear systems; Fuzzy expert systems; Takagi-Sugeno fuzzy logic controller	NSGA-II	Fuzzy logic controllers are increasingly applied to control complex systems because they have several advantages. The objective of this work is to propose a new technique to optimize a Takagi-Sugeno fuzzy logic controller with quality using the Non-dominated Sorting Genetic Algorithm-II by optimizing three objectives functions which are a cost function, the number of fuzzy inference rules, and the maximum instantaneous quadratic error. In this technique, the Multi-Criteria Decision-Making approach is used to choose one of the best controllers from the Pareto set of the last generation of the genetic algorithm. The proposed technique ensure: (i) that the output of the controlled system correctly follows the desired reference. (ii) the acceleration of the control process and (iii) avoid the existence of large overshoots: which are usually observed when applying commands to complex processes with variable behavior. At the end of the control process, a robustness test is performed to verify the efficiency of the proposed technique. It is shown here that the optimization of the third objective function, allows the improvement of the control quality. This new technique can be used to improve expert and intelligent systems based on fuzzy rules to control high complex systems with variable behavior which they have disturbing overshoots during their control. This technique; allows to accelerate the calculation of the control law of expert systems based on fuzzy rules; while ensuring that the quality of the control and the output signal are good. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 1	2020	145								113148	10.1016/j.eswa.2019.113148													
J								Bottleneck feature supervised U-Net for pixel-wise liver and tumor segmentation	EXPERT SYSTEMS WITH APPLICATIONS										CNN; Liver tumor; Segmentation; U-Net; Encoding; Bottleneck	MODEL	Liver cancer is one of the most common cancer types with high death rate. Doctors diagnose cancer by examining the CT images, which can be time-consuming and prone to error. Therefore, an automatic segmentation method is desired for clinical practice. In the literature, many U-Net-based models were proposed. But few of them focus on the bottleneck feature vectors, which are low dimensional representations of the input. In this paper, we propose a bottleneck feature supervised (BS) U-Net model and apply it to liver and tumor segmentation. Our main contributions are: (1) we propose a variation of the original U-Net that has better performance with a smaller number of parameters; (2) we propose a bottleneck feature supervised (BS) U-Net that contains an encoding U-Net and a segmentation U-Net. The encoding U-Net is first trained as an auto-encoder to get encodings of the label maps, which are subsequently used as additional supervision to train the segmentation U-Net. Compared with most U-Net-based models in the literature that only use the pair information between images and label maps, BS U-Net additionally uses the information extracted from the label maps as supervision. The model is evaluated on the liver and tumor segmentation (LiTS) competition. 2D BS U-Net achieves dice per case (DPC) 96.1% for liver segmentation and 56.9% for tumor segmentation. This result is better than most state-of-the-art 2D UNet-based networks in both tasks. Furthermore, the idea of bottleneck feature supervision can also be generalized to other U-Net-based models, making it have good potential for future development. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 1	2020	145								113131	10.1016/j.eswa.2019.113131													
J								A multiple search strategies based grey wolf optimizer for solving multi-objective optimization problems	EXPERT SYSTEMS WITH APPLICATIONS										Multi-objective grey wolf optimizer; Multiple search strategies; Multi-objective optimal scheduling problem; Cascade hydropower stations; Constraints handling methods	BEE COLONY ALGORITHM; OPTIMAL POWER-FLOW; EVOLUTIONARY ALGORITHMS; DIFFERENTIAL EVOLUTION; PERFORMANCE ASSESSMENT; GENETIC ALGORITHM; SCHEDULING PROBLEM; OPTIMAL OPERATION; RESERVOIR SYSTEM; OBJECTIVES	In this paper, a novel multi-objective grey wolf optimizer (MOGWO) based on multiple search strategies (i.e., adaptive chaotic mutation strategy, boundary mutation strategy, and elitism strategy) which we shall call MMOGWO is proposed to solve multi-objective optimization problems (MOPs). The algorithm uses a fixed-sized external archive that is adaptively maintained according to a grid-based approach to preserve the non-dominated solutions found during the search process. Then, the archive is used to define the social hierarchy and simulate the hunting behaviors of grey wolves. In the proposed algorithm, an adaptive chaotic mutation strategy based on a chaotic cubic map and modified generational distance (GD) is applied to the archive to dynamically adjust the convergence speed and balance the exploration and exploitation. To prevent the population diversity loss, a boundary mutation strategy based on the concept of multi-level parallel is employed to manage boundary constraint violations. Moreover, a non dominated sorting and crowding distance-based elitism strategy is also incorporated into the algorithm for exploiting more potential Pareto optimal solutions and preserve the diversity of solutions in the approximated set. The proposed algorithm is evaluated on a wide range of multi-objective optimization problems (MOPs), and compared with other state-of-the-art multi-objective optimization algorithms in terms of often-used performance metrics with the help of statistical analysis, average ranks test and Wilcoxon Signed-Rank Test (WSRT). It is revealed by the experimental results that the algorithm is highly competitive and significantly outperforms other well-known algorithms on most of the test problems. On obtaining satisfactory performance for test problems, to investigate the performance of the MMOGWO for solving real-world optimization problems with various constraints, MMOGWO is further applied to handle the multi-objective optimal scheduling problem (MOOSP) of cascade hydropower stations (CHSs) based on a novel constraints handling method designed in this paper. Simulation results indicate that, compared with other algorithms, MMOGWO can produce better quality solutions and it can be considered as a promising alternative tool to deal with multi-objective real-life engineering problems with complex constraints by equipping with constraints handling methods. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 1	2020	145								113134	10.1016/j.eswa.2019.113134													
J								Automated domain-specific healthcare knowledge graph curation framework: Subarachnoid hemorrhage as phenotype	EXPERT SYSTEMS WITH APPLICATIONS										Knowledge Graph; Ontology; Electronic Health Records; Intracranial Aneurysm; Association Rules; Ensemble Learning; Subarachnoid Hemorrhage Stroke	BASE	To derive meaningful insights from voluminous healthcare data, it is essential to convert it into machine understandable knowledge. Currently, machine understandable domain specific healthcare knowledge curation framework does not exist for complex neurological diseases such as subarachnoid hemorrhage stroke. We envisage futuristic clinical decision support systems and tools backed with such knowledge will aide in complex neurological disease prognosis, diagnosis, and treatment. Existing knowledge graphs (KGs) only contain concepts and relationships between them and offer this knowledge to information extraction and knowledge management applications. However, the proposed domain-specific automated KG curation framework enables extraction of concepts, relationships, individual and cohort graphs, and predictive knowledge. By employing ontology-based information extraction, ensemble learning and word embedding based on skip-gram techniques on structured and unstructured data from electronic health records of 1025 patients with an intracranial aneurysm, this paper proposes a novel fully automated framework to curate knowledge graph, consisting of concepts, different hierarchical and non-hierarchical relationships, and predictive rules for prediction of subarachnoid hemorrhage. The evaluation shows that proposed framework achieves 78% precision and 71% recall respectively, for concept extraction from clinical text. Taxonomic relationships evaluation had precision and recall of 68%, and 95%, respectively. Evaluation of knowledge to predict unruptured status using validation dataset shows accuracy, precision, recall, of 73%, 76%, and 90% respectively. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 1	2020	145								113120	10.1016/j.eswa.2019.113120													
J								Leveraging feature selection to detect potential tax fraudsters	EXPERT SYSTEMS WITH APPLICATIONS										Feature selection; Tax fraud detection; Association rules; Graph centrality measure	HEALTH-CARE FRAUD; CENTRALITY; NETWORKS	Tax evasion is any act that knowingly or unknowingly, legally or unlawfully, leads to non-payment or underpayment of tax due. Enforcing the correct payment of taxes by taxpayers is fundamental in maintaining investments that are necessary and benefits a society as a whole. Indeed, without taxes it is not possible to guarantee basic services such as health-care, education, sanitation, transportation, infrastructure, among other services essential to the population. This issue is especially relevant in developing countries such as Brazil. In this work we consider a real-world case study involving the Treasury Office of the State of Ceara (SEFAZ-CE, Brazil), the agency in charge of supervising more than 300,000 active taxpayers companies. SEFAZ-CE maintains a very large database containing vast amounts of information concerning such companies. Its enforcement team struggles to perform thorough inspections on taxpayers accounts as the underlying traditional human-based inspection processes involve the evaluation of countless fraud indicators (i.e., binary features), thus requiring burdensome amounts of time and being potentially prone to human errors. On the other hand, the vast amount of taxpayer information collected by fiscal agencies opens up the possibility of devising novel techniques able to tackle fiscal evasion much more effectively than traditional approaches. In this work we address the problem of using feature selection to select the most relevant binary features to improve the classification of potential tax fraudsters. Finding out possible fraudsters from taxpayer data with binary features presents several challenges. First, taxpayer data typically have features with low linear correlation between themselves. Also, tax frauds may originate from intricate illicit tactics, which in turn requires to uncover non-linear relationships between multiple features. Finally, few features may be correlated with the targeted class. In this work we propose ALICIA, a new feature selection method based on association rules and propositional logic with a carefully crafted graph centrality measure that attempts to tackle the above challenges while, at the same time, being agnostic to specific classification techniques. ALICIA is structured in three phases: first, it generates a set of relevant association rules from a set of fraud indicators (features). Subsequently, from such association rules ALICIA builds a graph, which structure is then used to determine the most relevant features. To achieve this ALICIA applies a novel centrality measure we call the Feature Topological Importance. We perform an extensive experimental evaluation to assess the validity of our proposal on four different real-world datasets, where we compare our solution with eight other feature selection methods. The results show that ALICIA achieves F-measure scores up to 76.88%, and consistently outperforms its competitors. (C) 2019 Published by Elsevier Ltd.																	0957-4174	1873-6793				MAY 1	2020	145								113128	10.1016/j.eswa.2019.113128													
J								Boosting salp swarm algorithm by sine cosine algorithm and disrupt operator for feature selection	EXPERT SYSTEMS WITH APPLICATIONS										Feature Selection (FS); Disruption Operator (Dop); Salp Swarm Algorithm (SSA); Sine Cosine Algorithm (SCA); Metaheuristics (MH)	OPTIMIZATION ALGORITHM; INFORMATION; STRATEGY; COLONY; CHAINS	Features Selection (FS) plays an important role in enhancing the performance of machine learning techniques in terms of accuracy and response time. As FS is known to be an NP-hard problem, the aim of this paper is to introduce basically a new variant of Salp Swarm Optimizer (SSA) for FS (called ISSAFD (Improved Followers of Salp swarm Algorithm using Sine Cosine algorithm and Disrupt Operator), that updates the position of followers (F) in SSA using sinusoidal mathematical functions that were inspired from the Sine Cosine Algorithm (SCA). This enhancement helps to improve the exploration phase and to avoid stagnation in a local area. Moreover, the Disruption Operator (D-op) is applied for all solutions, in order to enhance the population diversity and to maintain the balance between exploration and exploitation processes. Two other variants of SSA are developed based on SCA called ISSALD (Improved Leaders of Salp swarm Algorithm using Sine Cosine algorithm and Disrupt Operator) and ISSAF (Improved Followers of Salp swarm Algorithm using Sine Cosine algorithm). The updating process in consists to update the leaders (L) position by SCA and applying (D-op), whereas in ISSAF, the D-op is omitted and the position of followers is updated by SCA. Experimental results are evaluated on twenty datasets where four of them represent high dimensionality with a small number of instances. The obtained results show a good performance of ISSAFD in terms of accuracy, sensitivity, specificity, and the number of selected features in comparison with other metaheuristics (MH). (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 1	2020	145								113103	10.1016/j.eswa.2019.113103													
J								Splitting the fitness and penalty factor for temporal diversity increase in practical problem solving	EXPERT SYSTEMS WITH APPLICATIONS										Penalty factor separation; Diversity preservation; Evolutionary computation; Dynamic subpopulation number control; Messy-coding; Genetic algorithm	PARTICLE SWARM OPTIMIZATION; PRIMARY ROUTES ASSIGNMENT; EVOLUTIONARY ALGORITHM; HYBRID EVOLUTIONARY; FLOW OPTIMIZATION; COMPUTER-NETWORKS; CONSTRAINT; ALLOCATION; LINKAGE; MODELS	In this paper, we propose an optimization system based on an evolutionary algorithm developed to solve a real-life optimization problem related to the optimization of the computer networks. In particular, we focus on an NP-hard flow allocation problem in computer networks related to network survivability and addressing various constraints specific to computer networks. The constrained problems are usually handled by evolutionary methods by the introduction of the so-called penalty factor. However, such techniques raise difficulties regardless of their simple working principle. The more oppressive the penalty factor is, the more likely the evolutionary method is to stuck in the local optima with feasible solutions and propose final results of low-quality. On the other hand, a low penalty factor may lead the method to regions of high-rated solutions that violate the constraints and are useless due to their infeasibility. Therefore, we address the issue of penalty handling by proposing Ranking-based Fitness and Penalty Weighting (RFPW). RFPW is inspired by a theory-driven penalty parameter estimation using a bi-objective and weighted sum approach. It separates the objective and penalty functions and removes the necessity of hand-made weighting. RFPW was introduced to the recent proposition of an evolutionary method dedicated to solving hard NP-complete practical problem. The performed experiments related to the flow allocation problem confirm that employing RFPW may lead to a significant improvement in results quality. In our opinion, the proposed RFPW framework of the penalty function handling has the potential to be adapted to a wide range of optimization systems that utilize the idea of penalty function. RFPW allows the optimization method to reduce or increase the strength of a convergence automatically. Therefore, RFPW has the potential of results quality improvement and may replace expensive commonly-used diversity preservation techniques (e.g., island models). (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 1	2020	145								113126	10.1016/j.eswa.2019.113126													
J								Sensitivity analysis of Bayesian networks to parameters of the conditional probability model using a Beta regression approach	EXPERT SYSTEMS WITH APPLICATIONS										Bayesian network; Sensitivity; Distributional regression; Beta distribution; Gradient boosting; Stability selection	VARIABLE SELECTION; EXPERT-SYSTEMS; NUMBERS	Ensuring the validity and credibility of Bayesian Belief Network (BBN) as a modelling tool for expert systems requires appropriate methods for sensitivity analysis (SA), in order to test the robustness of the BBN diagnostic and prognostic with respect to the parameterisation of the conditional probability model (CPM). Yet, the most widely used techniques (based on sensitivity functions for discrete BBNs) only provide a local insight on the CPM influence, i.e. by varying only one CPM parameter at a time (or a few of them) while keeping the other ones unchanged. To overcome this limitation, the present study proposes an approach for global SA relying on Beta Regression using gradient boosting (potentially combined with stability selection analysis): it presents the benefit of keeping the presentation intuitive through a graph based approach, while being applicable to a large number of CPM parameters. The implementation of this approach is investigated for three cases, which cover a large spectrum of situations: (1) a small discrete BBN, used to capture medical knowledge, demonstrates the proposed approach; (2) a linear Gaussian BBN, used to assess the damage of reinforced concrete structures, exemplifies a case where the number of parameters is too large to be easily processed and interpreted (>40 parameters); (3) a discrete BBN, used for reliability analysis of nuclear power plant, exemplifies a case where analytical solutions for sensitivity can hardly be derived. Finally, provided that the validity of the BBR model is carefully checked, we show that the proposed approach can provide richer information than traditional SA methods at different levels: (i) it selects the most influential parameters; (ii) it provides the functional relation between the CPM parameter and the result of the probabilistic query; and (iii) it identifies how the CPM parameters can lead to situations of high probability, while quantifying the confidence in the occurrence of these situations. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 1	2020	145								113130	10.1016/j.eswa.2019.113130													
J								Rough set based lattice structure for knowledge representation in medical expert systems: Low back pain management case study	EXPERT SYSTEMS WITH APPLICATIONS										Medical expert system; Lattice structure; Rough set theory; Knowledge-base; Low back pain	RULES; REDUCTION; INFERENCE; JOINT	The aim of medical knowledge representation is to capture detailed domain knowledge in a clinically efficient manner, and to offer reliable resolution with the acquired knowledge. The knowledge base should allow incremental growth with inclusion of updated knowledge over the time. Accommodating the knowledge acquired from a variety of knowledge sources by different knowledge engineers may lead to a redundant and inconsistent design of the knowledge base, increasing the storage size and the time for knowledge retrieval. In this paper, we have proposed a rough set based lattice framework for representation of knowledge in medical expert systems which overcomes the problem of redundancy and inconsistency in knowledge and offers computational efficiency with respect to both time and space. The proposed knowledge representation offers a flexible scheme for expressing diverse possibilities of interrelatedness among the symptoms and diseases in a systematic manner within the lattice structures. Through our design, we generate an optimal set of decision rules for use during inference. Reliability of each rule is measured using a new metric called credibility factor, and the certainty and coverage factors of a decision rule have been re-defined. During inference, the medical expert system considers the highly reliable and certain rules first, and the possible and uncertain rules at the later stages, if recommended by physicians. The proposed scheme ensures completeness, consistency, integrity, non-redundancy, and ease of access. These qualities are automatically preserved in the designed knowledge base while being updated with the new knowledge of medical advancements. As a result, the overall maintenance cost of a medical expert system is significantly reduced. The proposed knowledge representation technique has been illustrated using an example from the domain of low back pain. Though the proposed technique has been demonstrated for low back pain, it offers a generalized scheme for knowledge representation and may be adopted by medical expert systems (e.g. pathology, psychology, and many other specialties) having complexity similar to this one; no application-specific knowledge representation technique needs to be designed. The medical expert system developed using this scheme may act as a standardized example for development of different unexplored or semi-explored health care automated systems. This scheme may also be applied for pattern recognition, rule mining, and conflict analysis in complex medical data domains. Our design will have a wide ranging impact towards finding low-cost, reliable and available healthcare solutions for different diseases, especially in primary care units where expert physicians are scarce. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 1	2020	145								113084	10.1016/j.eswa.2019.113084													
J								A harmonic estimator design with evolutionary operators equipped grey wolf optimizer	EXPERT SYSTEMS WITH APPLICATIONS										Grey Wolf Optimizer (GWO); Mutation; Crossover; Harmonics; Power quality; Optimization; Algorithm	POWER-SYSTEM HARMONICS; ALGORITHM; TRACKING	Harmonic estimation is a challenging design problem in power networks. Accurate estimation of the inter, power and sub harmonics in networks can be a helpful aspect for designing potential solutions for elimination of these harmonics. Harmonic estimation design problem has been considered as an optimization problem with the amalgamation of least square algorithm in past. In this paper, we first propose an Evolutionary Operators Equipped Grey Wolf Optimizer (E-GWO). In this proposal a sinusoidal function enabled bridging is proposed and along with this tournament selection operator and crossover and mutation operation are incorporated at position updation phase. The variant is first benchmarked on latest CEC-2017 functions and then this design problem is addressed. After a meaningful comparison with the previously published approaches, we arrive at the conclusion that proposed modifications have positive implications on the performance of GWO. Proposed harmonic designs are robust when tested with different operating conditions. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 1	2020	145								113125	10.1016/j.eswa.2019.113125													
J								Hybrid enhanced discrete fruit fly optimization algorithm for scheduling blocking flow-shop in distributed environment	EXPERT SYSTEMS WITH APPLICATIONS										Distributed production environment; Blocking flow-shop scheduling; Makespan; Constructive heuristic; Metaheuristic	ITERATED GREEDY ALGORITHM; COMPETITIVE MEMETIC ALGORITHM; 2-STAGE ASSEMBLY FLOWSHOP; MINIMIZING MAKESPAN; TOTAL FLOWTIME; SCATTER SEARCH; HEURISTICS; MACHINE; METAHEURISTICS; TIMES	Scheduling in distributed production environments is becoming widespread in recent years due to the increasing advantages of multi-factory manufacture. This paper investigates the distributed blocking flow-shop scheduling problem (DBFSP) with the objective of minimizing the makespan. To solve this problem, a hybrid enhanced discrete fruit fly optimization algorithm (HEDFOA) is proposed. In the proposed algorithm, an effective constructive heuristic is developed based on a new assignment rule of jobs and an insertion-based improvement procedure to initialize the common central location of all fruit fly swarms. In the smell-based foraging, an effective insertion-based neighborhood operator is designed for exploration in global scope. In the vision-based foraging, a local search is embedded to intensify the exploitation ability of algorithm in local region. Meanwhile, a simulated annealing-like acceptance criterion is employed to help algorithm escape from the local optimum. Finally, an extensive computational experiment is conducted. Experimental results show that the proposed HEDFOA is more effective than the existing state-of-the-art methods. Furthermore, 516 best known solutions out of 720 benchmark instances are also updated. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 1	2020	145								113147	10.1016/j.eswa.2019.113147													
J								Multi-depot vehicle routing problem with risk mitigation: Model and solution algorithm	EXPERT SYSTEMS WITH APPLICATIONS										Multi-depot vehicle routing problem; Heuristic algorithm; Supply chain management; Transportation risk mitigation; Transportation plan adaptation	HAZARDOUS MATERIALS; GENETIC ALGORITHM; SEARCH; TRANSPORTATION; DISCRETE; SYSTEM	In practice, the execution of plans with vehicle routing components is often subjected to external events since the transporting vehicles can be exposed to various risk factors. This may lead to delivery failure, vehicle breakdown, commodity loses, etc. In this setting, the stakeholders can benefit from logistic planning techniques whereby potential vehicle breakdown and cargo delivery failure can be mitigated by limiting vehicle risk exposure and prioritizing deliveries of larger payloads. In this paper, we propose a cost effective learning-based heuristic technique to minimize the routing cost along with the potential cost due to the risk of vehicle breakdown and cargo delivery failure. The approach is elaborated by means of an illustrative case study, and it is accompanied by benchmark results along with a comparative study. The heuristic solution generation approach can be used to mitigate vehicle routing risk at the planning stage as well as during various proactive and reactive plan adaptation activities in response to the occurrence of exogenous events. Crown Copyright (C) 2019 Published by Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 1	2020	145								113099	10.1016/j.eswa.2019.113099													
J								Executive summaries of uncertain values close to the gain/loss threshold - linguistic modelling perspective	EXPERT SYSTEMS WITH APPLICATIONS										Executive summary; Framing; Linguistic approximation; Gains; Losses; Linguistic scale	SIMILARITY MEASURES; PROSPECT-THEORY; FUZZY APPROACH; LOSS AVERSION; APPROXIMATION; DISTANCE; DECISION; AHP	In this paper we propose a novel method for the assessment of linguistic approximation of fuzzy outputs of decision-support and evaluation models in the presence of thresholds. The method provides graphical and numerical summaries of performance of different distance/similarity measures in combination with various linguistic scales in the process of assigning linguistic labels to the outputs of expert systems and decision-support models. We assume the existence of a specific threshold on the output scale that splits the outputs in two categories, i.e. gains/losses, acceptable/unacceptable values, better/worse than average values etc. This way a framing of the outputs can be obtained by labelling them linguistically. We consider numerical outputs in monetary units and assume zero to be the threshold value, splitting the universe into gains and losses. Based on a numerical analysis and yet without the knowledge of the most fitting linguistic label, the proposed analytical method is able to identify the cases where a clearly incorrect label is assigned (a loss label for a gain an/or vice versa) and hence the combinations of linguistic scales and distance/similarity measures of fuzzy numbers not to be used for the given purpose. We can also analyze specific features of some similarity/distance and linguistic scale combinations. The proposed method and its outputs is intended for the design of such expert systems and decision-support models, where a linguistic level of communicating the results to the users of these models is of importance, e.g., for the creation of executive summaries of outputs of mathematical models and results of financial data analyses. The method brings together the mathematical analysis of the linguistic approximation tools and the behavioral aspect of framing of the outputs e.g., as gains or losses prior to the final decision-making step. This way it provides much need guidance for the selection of reasonable distance/similarity measures of fuzzy numbers and reasonable linguistic scale for linguistic approximation. As such it is a useful tool for the design of system-user interfaces including a linguistic level of description. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 1	2020	145								113108	10.1016/j.eswa.2019.113108													
J								Improved Salp Swarm Algorithm based on opposition based learning and novel local search algorithm for feature selection	EXPERT SYSTEMS WITH APPLICATIONS										Salp Swarm Algorithm; Classification; Feature selection; Optimization; Machine Learning; Algorithm; Opposition Based Learning	INTRUSION DETECTION SYSTEM; INTEGRATING FEATURE-SELECTION; OPTIMIZATION ALGORITHM; GENETIC-ALGORITHM; CLASSIFICATION; PARAMETERS; DIAGNOSIS; NETWORK	Many fields such as data science, data mining suffered from the rapid growth of data volume and high data dimensionality. The main problems which are faced by these fields include the high computational cost, memory cost, and low accuracy performance. These problems will occur because these fields are mainly used machine learning classifiers. However, machine learning accuracy is affected by the noisy and irrelevant features. In addition, the computational and memory cost of the machine learning is mainly affected by the size of the used datasets. Thus, to solve these problems, feature selection can be used to select optimal subset of features and reduce the data dimensionality. Feature selection represents an important preprocessing step in many intelligent and expert systems such as intrusion detection, disease prediction, and sentiment analysis. An improved version of Salp Swarm Algorithm (ISSA) is proposed in this study to solve feature selection problems and select the optimal subset of features in wrapper-mode. Two main improvements were included into the original SSA algorithm to alleviate its drawbacks and adapt it for feature selection problems. The first improvement includes the use of Opposition Based Learning (OBL) at initialization phase of SSA to improve its population diversity in the search space. The second improvement includes the development and use of new Local Search Algorithm with SSA to improve its exploitation. To confirm and validate the performance of the proposed improved SSA (ISSA), ISSA was applied on 18 datasets from UCI repository. In addition, ISSA was compared with four well-known optimization algorithms such as Genetic Algorithm, Particle Swarm Optimization, Grasshopper Optimization Algorithm, and Ant Lion Optimizer. In these experiments four different assessment criteria were used. The rdemonstrate that ISSA outperforms all baseline algorithms in terms of fitness values, accuracy, convergence curves, and feature reduction in most of the used datasets. The wrapper feature selection mode can be used in different application areas of expert and intelligent systems and this is confirmed from the obtained results over different types of datasets. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 1	2020	145								113122	10.1016/j.eswa.2019.113122													
J								A long-term fleet renewal problem under uncertainty: A simulation-based optimization approach	EXPERT SYSTEMS WITH APPLICATIONS										Fleet renewal; Simulation-optimization; System dynamics; Genetic algorithm; Scenario discovery	VEHICLE-ROUTING PROBLEM; SYSTEM DYNAMICS MODEL; GENETIC ALGORITHM; HETEROGENEOUS FLEET; HYBRID APPROACH; MANAGEMENT; POLICIES; SEARCH; SIZE; MIX	In this paper, we model and solve a strategic problem of fleet renewal to meet future operational needs under uncertain conditions. The fleet renewal problem focuses on mainly strategic decisions involving from fleet size, fleet mix and timing of replacement, yet it is essential to consider a significant amount of detail regarding short-term decisions to prevent inferior or infeasible strategies. In this direction, we develop a hybrid simulation model by combining system dynamics (SD) and discrete event simulation (DES) approaches. The standalone use of this model enables the decision maker to analyze the effects of both short- and long-term decisions on availability by simulating the processes that the fleet undertakes through its life-cycle from asset acquisition to retirement. Nevertheless, the simulation neither suggests nor seeks the best renewal strategy(ies). To alleviate this difficulty, we propose a simulation-based optimization that uses a genetic algorithm (GA) to effectively search a very large set of feasible fleet renewal strategies and uses the developed hybrid simulation model to evaluate candidate strategies found by GA. To provide a decision context where the approach has been developed and applied, we use a naval fleet renewal application. The extensive numerical experiments show that the proposed approach not only finds good and robust renewal strategies but also identify critical resources that influence the fleet's availability. Finally, the robustness of optimized strategies under uncertainty is tested by sensitivity analysis, and mappings between implemented strategies and the fleet performance are constructed by scenario discovery analysis to provide insights for decision makers. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 1	2020	145								113158	10.1016/j.eswa.2019.113158													
J								A real-time map merging strategy for robust collaborative reconstruction of unknown environments	EXPERT SYSTEMS WITH APPLICATIONS										Map merging; MRSLAM; Feature extraction; Decision-making; Collaborative reconstruction	VISUAL ODOMETRY; MULTIROBOT SLAM; LOCALIZATION; INFORMATION	The development of collaborative techniques for exploring and mapping environments has been rising in the last decade. These techniques, known as multi-robot SLAM (MRSLAM), aim to extend the use of autonomous mobile robots to autonomous multi-agent systems. The MRSLAM technique presented here consists mainly of a robust map merging algorithm and a decision-making algorithm that controls agents in the field. On the one hand, the proposed merging algorithm performs a consistent and robust map fusion in real time. It consists of an own corner detector, a cylindrical descriptor, a matching technique and the RANSAC algorithm. On the other hand, once the fusion of maps is performed, the decision-making algorithm is responsible for controlling the robot operation in the field, based on the general current state of the multi-robot system. The main contribution of this MRSLAM technique is the robust map merging algorithm, since it was implemented and validated in simulated and real scenarios, resulting in collaborative maps that are consistent with the environment and obtained in less than 280 ms. This technique also achieves a significant decrease in reconstruction time when two or three robots are used: up to 35% in a simulated scenario and up to 49% in a real one. The proposed MRSLAM technique shows important similarities to expert multi-agent systems, as it is able to control and organize a team of robots in order to collaboratively explore and map an unknown environment. This approach was developed under the ROS framework to be used and tested by the scientific and academic community. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 1	2020	145								113109	10.1016/j.eswa.2019.113109													
J								TDR: Two-stage deep recommendation model based on mSDA and DNN	EXPERT SYSTEMS WITH APPLICATIONS										Recommender system; Deep learning; Input optimization; Marginalized denoising auto-encoder; Deep neural network		Recently, deep learning techniques have been widely used in recommendation tasks and have attained record performance. However, the input quality of the deep learning model has a great influence on the recommendation performance. In this work, an efficient and effective input optimization method is proposed. Specifically, we propose an integrated recommendation framework based on two-stage deep learning. In the first stage, with user and item features as the original input, a low-cost marginalized stacked denoising auto-encoder (mSDA) model is used to learn the latent factors of users and items. In the second stage, the resulting latent factors are combined and used as input vector to the DNN model for fast and accurate prediction. Using the latent factor vector as the input to the deep learning-based recommendation model not only captures the high-order feature interaction, but also reduces the burden of the hidden layer, and also avoids the model training falling into local optimum. Extensive experiments with real-world datasets show that the proposed model shows much better performance than the state-of-the-art recommendation methods in terms of prediction accuracy, parameter space and training speed. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 1	2020	145								113116	10.1016/j.eswa.2019.113116													
J								A new approach to solve opinion dynamics on complex networks	EXPERT SYSTEMS WITH APPLICATIONS										Complex networks; Opinion dynamics; Asymptotic method; Spectral analysis; Weighted-directed social networks	CONSENSUS; MODEL	In this paper, we use the differential equations to establish the continuous opinion models on the complex networks, so as to observe how the nodes holding different opinions influence each other and how the opinion dynamics evolve. One of nodes in the complex network is selected randomly as a social outcast who opposes the opinions of other nodes and is reacted by the opponents equally. The analytical framework of dynamics on weighted-directed complex networks is proposed. We investigate how the strength and position of social outcast impact the opinion evolution. Then, we analyze how the network structures impact the result of dynamical process. Meanwhile, we propose a periodic description and an analytical method to solve the equation-based models. Finally, a series of experiments are conducted on five kinds of typical weighted-directed complex networks to demonstrate the feasibility and effectiveness of our proposed approach. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 1	2020	145								113132	10.1016/j.eswa.2019.113132													
J								A new clustering method based on morphological operations	EXPERT SYSTEMS WITH APPLICATIONS										Clustering; Morphological operations; Dilation; Label	ALGORITHM	With the booming development of data science, many clustering methods have been proposed. All clustering methods have inherent merits and deficiencies. Therefore, they are only capable of clustering some specific types of data robustly. In addition, the accuracies of the clustering methods rely heavily on the characteristics of the data. In this paper, we propose a new clustering method based on the morphological operations. The morphological dilation is used to connect the data points based on their adjacency and form different connected domains. The iteration of the morphological dilation process stops when the number of connected domains equals the number of the clusters or when the maximum number of iteration is reached. The morphological labeling is then used to label the connected domains. The Euclidean distance between each data point and the points in each labeled connected domain is calculated. For each data point, there is a labeled connected domain that contains a point that yields the smallest Euclidean distance. The data point is assigned with the same labeling number as the labeled connected domain. We evaluate and compare the proposed method with state of the art clustering methods with different types of data. Experimental results are favorable. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 1	2020	145								113102	10.1016/j.eswa.2019.113102													
J								Automatic detonator code recognition via deep neural network	EXPERT SYSTEMS WITH APPLICATIONS										Automatic detonator code recognition; Machine vision system; Convolutional neural network; Deep learning	SCENE TEXT DETECTION	Detonators are hazardous and must be strictly controlled to strengthen public security since they contain explosive. To deal with the stressful management of detonators by manually recording detonator codes, an expert and intelligent system for detonator code recognition is an alternative management scheme for automatically recording detonator codes. How to achieve accurate and reliable recognition performance is also a challenging problem in such an intelligent system. In this study, we develop an intelligent vision based system based on deep learning for identifying detonator codes. Specifically, we design a detonator image acquisition subsystem to construct the detonator code image dataset, and an intelligent image processing subsystem named as automatic detonator code recognition network (ADCR-Net). The ADCR-Net is a pipeline cascading a deep localization network and a deep recognition network. The multi-scale concatenation block and the features integration module are proposed for the localization network of the ADCR-Net to improve the features expressions. The multi-level progressive self-attention block is put forward for the recognition network of the ADCR-Net to help with focusing on multi-level activation maps. Also, a multi-label loss function for the recognition network is designed to balance the significance of the classifiers of the intelligent system during training. Experiments on our released dataset demonstrate the effectiveness and efficiency of the proposed expert and intelligent system, which achieves 99.18% accuracy in end-to-end recognition. Our work provides an effective resolution to the expert and intelligent system for automatic detonator code recognition. Also, we provide an alternative framework to deal with the tradeoff between inference time and recognition accuracy in deep learning which is becoming a popular method in many expert and intelligent systems. Furthermore, the released dataset is available for distribution to the related researchers to develop their expert systems in the field of industrial optical character recognition. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 1	2020	145								113121	10.1016/j.eswa.2019.113121													
J								Fuzzy optimization model for electric vehicle routing problem with time windows and recharging stations	EXPERT SYSTEMS WITH APPLICATIONS										Vehicle routing problem; Credibility theory; Electric vehicle; Fuzzy simulation; Adaptive large neighborhood search	LARGE NEIGHBORHOOD SEARCH; DELIVERY PROBLEM; TRANSPORTATION; PICKUP	As fuel prices increase and emission regulations become increasingly strict, electric vehicles have been used in various logistics distribution activities. Most studies have focused on the electric vehicle routing problem under a deterministic environment, neglecting the effects of uncertain factors in practical logistics distribution. Thus, a novel fuzzy electric vehicle routing problem with time windows and recharging stations (FEVRPTW) is investigated in this study, and a fuzzy optimization model is established based on credibility theory for this problem. In the presented model, fuzzy numbers are used to denote the uncertainties of service time, battery energy consumption, and travel time. Moreover, the partial recharge is allowed under the uncertain environment. To solve the model, an adaptive large neighborhood search (ALNS) algorithm enhanced with the fuzzy simulation method is proposed. In the proposed ALNS algorithm, four new removal algorithms are designed and integrated for addressing the FEVRPTW. To further improve the algorithmic performance, the variable neighborhood descent algorithm is embedded into the proposed ALNS algorithm and five local search operators are applied. The experiments were conducted to verify the effectiveness of the proposed ALNS algorithm for solving the presented model. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 1	2020	145								113123	10.1016/j.eswa.2019.113123													
J								Hybrid multiobjective evolutionary algorithm with fast sampling strategy-based global search and route sequence difference-based local search for VRPTW	EXPERT SYSTEMS WITH APPLICATIONS										Multiobjective evolutionary algorithm; Vehicle routing problem; Global search; Local search; Transportation	PARTICLE SWARM OPTIMIZATION; TIME WINDOWS	The vehicle routing problem with time windows (VRPTW) is an important and widely studied combinatorial optimization problems. This paper aims at VRPTW with the objectives of reducing the number of vehicles and minimizing the time-wasting during the delivery process caused by early arrival. To solve this NP-hard problem, a hybrid multiobjective evolutionary algorithm with fast sampling strategy-based global search and route sequence difference-based local search (HMOEA-GL) is proposed. Firstly, fast sampling strategy-based global search (FSS-GS) of HMOEA-GL extensively explores the entire solution space to quickly guide the search direction towards the center and edge areas of Pareto frontier. Secondly, route sequence difference-based local search (RSD-LS) is executed on the individuals with poor performance in the population obtained by FSS-GS to enhance the search ability of HMOEA-GL. In addition, the suitable coding method and proper genetic operators are designed, especially, a simple insertion search is used to reduce the number of vehicles in VRPTW. Comparing with NSGA-II, SPEA2, and MOEA/D, experimental results on 12 Solomon benchmark test problems indicate that the proposed HMOEA-GL is effective, and more excellent in convergence, while maintaining a satisfying distribution performance. HMOEA-GL could be an effective intelligent algorithm for expert and intelligent decision support system to help logistics companies to make decisions. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 1	2020	145								113151	10.1016/j.eswa.2019.113151													
J								Market segmentation using high-dimensional sparse consumers data	EXPERT SYSTEMS WITH APPLICATIONS										Precision marketing; RFM theory; Sparse K-means algorithm; BCBimax algorithm; Mobile telecommunications industry	DECISION-MAKING; CUSTOMER; FRAMEWORK; BEHAVIOR; NETWORK; SUPPORT; IMPACT; CHURN	Good segmentation contributes towards a better understanding of the market and customer demands. This study aims to develop a new methodological approach, integrating "Recency, Frequency and Monetary" with the sparse K-means clustering algorithm of Witten and Tibshirani (2010). The proposed approach is suitable for handling large, high-dimensional and sparse consumer data. Drawing on the proposed methodology, alongside data collection from the Chinese mobile telecommunications market, and considering specific services, our treatment is further assessed empirically and appears to provide robust results when compared to the Dolnicar, Kaiser, Lazarevski and Leisch (2012) biclustering of customers method. Following the attainment of a clear and robust market segmentation structure, our theoretical treatment and its empirical analysis provide a useful tool and valid methodology for marketers, and decision makers in general, to accurately determine the most profitable market segments. (C) 2019 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				MAY 1	2020	145								113136	10.1016/j.eswa.2019.113136													
J								Trajectory-based recognition of dynamic Persian sign language using hidden Markov model	COMPUTER SPEECH AND LANGUAGE										Sign language recognition; Persian sign language; Trajectory; Hidden Markov model; Classification	GESTURE RECOGNITION	Sign Language Recognition (SLR) is an important step in facilitating the communication among deaf people and the rest of society. Existing Persian sign language recognition systems are mainly restricted to static signs which are not very useful in everyday communications. In this study, a dynamic Persian sign language recognition system is presented. A collection of 1200 videos were captured from 12 individuals performing 20 dynamic signs with a simple white glove. The trajectory of the hands, along with hand shape information were extracted from each video using a simple region-growing technique. These time-varying trajectories were then modeled using Hidden Markov Model (HMM) with Gaussian probability density functions as observations. The performance of the system was evaluated in different experimental strategies. Signer-independent and signer-dependent experiments were performed on the proposed system and the average accuracy of 97.48% was obtained. The experimental results demonstrated that the performance of the system is independent of the subject and it can also perform excellently even with a limited number of training data. (C) 2019 Elsevier Ltd. All rights reserved.																	0885-2308	1095-8363				MAY	2020	61								UNSP 101053	10.1016/j.csl.2019.101053													
J								Assessing Parkinson's disease severity using speech analysis in non-native speakers	COMPUTER SPEECH AND LANGUAGE										Cepstrum; Dysphonia; Dysprosody; Parkinson's disease; Speech processing; Tele-monitoring	INTELLIGIBILITY; IDENTIFICATION; CLASSIFICATION; SUITABILITY; IMPAIRMENT; PERCEPTION; ACCENT; SPOKEN	Background: Speech disorder is a common manifestation of Parkinson's disease with two main symptoms, dysprosody and dysphonia. Previous research studying objective measures of speech symptoms involved patients and examiners who were native language speakers. Measures such as cepstral separation difference (CSD) features to quantify dysphonia and dysprosody accurately distinguish the severity of speech impairment. Importantly CSD, together with other speech features, including Mel-frequency coefficients, fundamental-frequency variation, and spectral dynamics, characterize speech intelligibility in PD. However, non-native language speakers transfer phonological rules of their mother language that tamper speech assessment. Objectives: This paper explores CSD's capability: first, to quantify dysprosody and dysphonia of non-native language speakers, Parkinson patients and controls, and secondly, to characterize the severity of speech impairment when Parkinson's dysprosody accompanies non-native linguistic dysprosody. Methods: CSD features were extracted from 168 speech samples recorded from 19 healthy controls, 15 rehabilitated and 23 not-rehabilitated Parkinson patients in three different clinical speech tests based on Unified Parkinson's disease rating scale motor-speech examination. Statistical analyses were performed to compare groups using analysis of variance, intraclass correlation, and Guttman correlation coefficient mu(2). Random forests were trained to classify the severity of speech impairment using CSD and the other speech features. Feature importance in classification was determined using permutation importance score. Results: Results showed that the CSD feature describing dysphonia was uninfluenced by non-native accents, strongly correlated with the clinical examination (mu(2)>0.5), and significantly discriminated between the healthy, rehabilitated, and not-rehabilitated patient groups based on the severity of speech symptoms. However, the feature describing dysprosody did not correlate with the clinical examination but significantly distinguished the groups. The classification model based on random forests and selected features characterized the severity of speech impairment of non-native language speakers with high accuracy. Importantly, the permutation importance score of the CSD feature representing dysphonia was the highest compared to other features. Results showed a strong negative correlation (mu(2)<-0.5) between L-dopa administration and the CSD features. Conclusions: Although non-native accents reduce speech intelligibility, the CSD features can accurately characterize speech impairment, which is not always possible in the clinical examination. Findings support using CSD for monitoring Parkinson's disease. (C) 2019 Elsevier Ltd. All rights reserved.																	0885-2308	1095-8363				MAY	2020	61								101047	10.1016/j.csl.2019.101047													
J								NEC-TT System for Mixed-Bandwidth and Multi-Domain Speaker Recognition	COMPUTER SPEECH AND LANGUAGE										Speaker recognition; benchmark evaluation; domain adaptation		This paper describes the NEC-TT speaker recognition system designed for the 2018 Speaker Recognition Evaluation (SRE'18) benchmarking. The NEC-TT submission was among the best-performing systems in this latest edition of SRE organized by the National Institute of Standards and Technology (NIST). It comprises multiple sub-systems based on a deep speaker embedding front-end followed by a probabilistic linear discriminant analysis (PLDA) back-end. Speaker embeddings are continuous-valued vector representations that allow easy comparison between speaker voices with simple geometric operations. The effectiveness of deep speaker embeddings relies on the quantity and diversity of the training data. To this end, we hinge on data augmentation and mixed-bandwidth training strategies to increase the number of training examples and speakers. By doing so, we not only increase the quantity of the training data but also expand the output softmax layer with a larger number of speaker classes. From a system design perspective, we adopted a two-stage pipeline consisting of a general multi-domain speaker embedding front-end followed by a domain-specific PLDA back-end. This has a significant benefit in commercial deployment since the same speaker embedding front-end could be used with multiple domain-adapted PLDA back-ends to cater to every specific deployment. This paper provides a detailed description and analysis of the design methodology, data augmentation, bandwidth extension, multi-head attention, PLDA adaptation, and other components that have contributed to good performance in NEC-TT's SRE'18 results. (C) 2019 Elsevier Ltd. All rights reserved.																	0885-2308	1095-8363				MAY	2020	61								UNSP 101033	10.1016/j.csl.2019.101033													
J								State gradients for analyzing memory in LSTM language models	COMPUTER SPEECH AND LANGUAGE										Language modeling; Neural networks; Memory; Gradients; Interpreting	NEURAL-NETWORKS; INFORMATION	Gradients can be used to train neural networks, but they can also be used to interpret them. We investigate how well the inputs of RNNs are remembered by their state by calculating 'state gradients', and applying SVD on the gradient matrix to reveal which directions in embedding space are remembered and to what extent. Our method can be applied to any RNN and reveals which properties in the embedding space influence the state space, without the need to know and label the properties beforehand. In this paper, we propose a normalization method that alleviates the influence of variance in embedding space on the state gradients and show the effectiveness of our method on a synthetic dataset. Additionally, the influence of several training settings on the RNN memory is investigated, and a more fine-grained analysis based on POS and word types shows that LSTM language models learn to model linguistic intuitions. Our code and datasets are publicly available. (C) 2019 Elsevier Ltd. All rights reserved.																	0885-2308	1095-8363				MAY	2020	61								101034	10.1016/j.csl.2019.101034													
J								Indic handwritten script identification using offline-online multi-modal deep network	INFORMATION FUSION										Handwritten script identification; Deep neural network; Multi-modal learning; Offline and online handwriting; Character level training	WORD RECOGNITION; INFORMATION; FRAMEWORK; RECOVERY; IMAGES; SYSTEM	In this paper, we propose a novel approach of word-level Indic script identification using only character-level data in training stage. Our method uses a mull-modal deep network which takes both offline and online modality of the data as input in order to explore the information from both the modalities jointly for script identification task. We take handwritten data in either modality as input and the opposite modality is generated through intermodality conversion. Thereafter, we feed this offline-online modality pair to our network. Hence, along with the advantage of utilizing information from both the modalities, the proposed framework can work for both offline and online script identification which alleviates the need for designing two separate script identification modules for individual modality. We also propose a novel conditional mull-modal fusion scheme to combine the information from offline and online modally which takes into account the original modally of the data being fed to our network and thus it combines adaptively. An exhaustive experimental study has been done on a data set including English(Roman) and 6 other official Indic scripts. Our proposed scheme outperforms traditional classifiers along with handcrafted features and deep learning based methods. Experiment results show that using only character level training data can achieve competitive performance against traditional training using word level data.																	1566-2535	1872-6305				MAY	2020	57						1	14		10.1016/j.inffus.2019.10.010													
J								Multi-sensor fusion for body sensor network in medical human-robot interaction scenario	INFORMATION FUSION										Body sensor network; Multi-sensor fusion; Medical human-robot interaction; Neural network; Fusion decision	PERCEPTION; DESIGN; SYSTEM	With the development of sensor and communication technologies, body sensor networks(BSNs) have become an indispensable part of smart medical services by monitoring the real-time state of users. Due to introducing of smart medical robots, BSNs are not related to users, but also responsible for data acquisition and mull-sensor fusion in medical human-robot interaction scenarios. In this paper, a hybrid body sensor network architecture based on mull-sensor fusion(HBMF) is designed to support the most advanced smart medical services, which combines various sensor, communication, robot, and data processing technologies. The infrastructure and system functions are described in detail and compared with other architectures. Especially, A mull-sensor fusion method based on interpretable neural network(MFIN) for BSNs in medical human-robot interaction scenario is designed and analyzed to improve the performance of fusion decision-making. Compared with the current mull-sensor fusion methods, our design guarantees both the flexibility and reliability of the service in the medical human-robot interaction scenario.																	1566-2535	1872-6305				MAY	2020	57						15	26		10.1016/j.inffus.2019.11.001													
J								The state-of-art of the generalizations of the Choquet integral: From aggregation and pre-aggregation to ordered directionally monotone functions	INFORMATION FUSION										Aggregation functions; Pre-aggregation functions; Pseudo pre-aggregation function pair; Ordered directionally monotonicity; Choquet integral; C-T-integral; C-F-integral; CC-integral; C-F1F2-Integral; gC(F1F2)-Integral	OVERLAP FUNCTIONS; PROSPECT-THEORY; FUZZY MEASURES; ADDITIVE GENERATORS; WEAK MONOTONICITY; CLASSIFICATION; DECISION; OPERATORS; SET; REPRESENTATION	In 2013, Barrenechea et al. used the Choquet integral as an aggregation function in the fuzzy reasoning method (FRM) of fuzzy rule-based classification systems. After that, starting from 2016, new aggregation-like functions generalizing the Choquet integral have appeared in the literature, in particular in the works by Lucca et al. Those generalizations of the Choquet integral, namely C-T-integrals (by t-norm T), C-F-integrals(by a fusion function F satisfying some specific properties), CC-integrals (by a copula C), C-F1F2-integrals (by a pair of fusion functions (F-1, F-2) under some specific constraints) and their generalization gC(F)(1F2)-integrals, achieved excellent results in classification problems. The works by Lucca et al. showed that the aggregation task in a FRM may be performed by either aggregation, pre-aggregation or just ordered directional monotonic functions satisfying some boundary conditions, that is, it is not necessary to have an aggregation function to obtain competitive results in classification. The aim of this paper is to present and discuss such generalizations of the Choquet integral, offering a general panorama of the state of the art, showing the relations and intersections among such five classes of generalizations. First, we present them from a theoretical point of view. Then, we also summarize some applications found in the literature.																	1566-2535	1872-6305				MAY	2020	57						27	43		10.1016/j.inffus.2019.10.005													
J								Heterogeneous sensor data fusion for multiple object association using belief functions	INFORMATION FUSION										Multiple object association; Evidence theory; Multi-attribute modeling/fusion; Autonomous vehicles; Hierarchical fusion	TRACKING; ASSIGNMENT; FRAMEWORK	In this study, the object association issue is tackled in order to ensure a correct affiliation of perceived objects with known ones. The proposed approach is based on the evidence theory and includes multiple object features in order to manage pairing issues in a complex environment. Two heterogeneous information sources are built based on kinematic features related to the objects: their position and size on one hand and their direction of motion on the other hand. A study on the estimation of the belief expressed by these independent sources is performed. The multiple features are managed through a hierarchical fusion which includes two levels of combination. The first level is a pairwise combination, fusing position and orientation data of each pair of objects and the second one processes sequentially the previously combined information over all possible associations. This paper also investigates the effectiveness of the association according to different combination operators at both levels. The performance of the proposed approach is demonstrated in the Intelligent Transportation Systems context for which environmental perception is crucial. The validation exploits a large amount of real data issued from a camera and a 3D LiDAR from the KITTI database.																	1566-2535	1872-6305				MAY	2020	57						44	58		10.1016/j.inffus.2019.11.002													
J								Granger causality-based information fusion applied to electrical measurements from power transformers	INFORMATION FUSION										Granger causality; Power transformers; Functional connectivity; SCADA measurements; Time series analysis	EFFECTIVE CONNECTIVITY; LINEAR-DEPENDENCE; FEEDBACK; TOOLBOX	In the immediate future, with the increasing presence of electrical vehicles and the large increase in the use of renewable energies, it will be crucial that distribution power networks are managed, supervised and exploited in a similar way as the transmission power systems were in previous decades. To achieve this, the underlying infrastructure requires automated monitoring and digitization, including smart-meters, wide-band communication systems, electronic device based-local controllers, and the Internet of Things. All of these technologies demand a huge amount of data to be curated, processed, interpreted and fused with the aim of real-time predictive control and supervision of medium/low voltage transformer substations. Wiener-Granger causally, a statistical notion of causal inference based on Information Fusion could help in the prediction of electrical behaviour arising from common causal dependencies. Originally developed in econometrics, it has successfully been applied to several fields of research such as the neurosciences and is applicable to time series data whereby cause precedes effect. In this paper, we demonstrate the potential of this methodology in the context of power measures for providing theoretical models of low/medium power transformers. Up to our knowledge, the proposed method in this context is the first attempt to build a data-driven power system model based on G-causality. In particular, we analysed directed functional connectivity of electrical measures providing a statistical description of observed responses, and identified the causal structure within data in an exploratory analysis. Pair-wise conditional G-causality of power transformers, their independent evolution in time, and the joint evolution in time and frequency are discussed and analysed in the experimental section.																	1566-2535	1872-6305				MAY	2020	57						59	70		10.1016/j.inffus.2019.12.005													
J								Multi-dimensional belief fusion of multi-Gaussian structures	INFORMATION FUSION										Belief fusion; Sensor fusion; Probabilistic methods; Non-Gaussian belief; Estimation; Bayesian inference		This paper describes the mathematical formulation of belief fusion of multi-Gaussian probability distribution functions (PDFs) in N-D, as well as the construction of some useful non-Gaussian structures. An emphasis of this work is the development of concise algorithms for constructing and efficiently fusing these non-Gaussian structures in N-space. In order to address decision-making using multi-Gaussian PDFs, an efficient probabilistic decision-making scheme is introduced and validated here. We investigate the trade-off between precision and efficiency in comparison with spatially discretizing fusion methods, concluding that the proposed techniques offer an improvement in both accuracy and efficiency for many contexts requiring non-Gaussian representation of belief. The proposed framework can be implemented in a diverse range of scenarios, potentially with real-time capability, and often without substantial sacrifice in accuracy.																	1566-2535	1872-6305				MAY	2020	57						71	88		10.1016/j.inffus.2019.12.006													
J								Neural architecture search for image saliency fusion	INFORMATION FUSION										Saliency fusion; Evolutionary algorithms; Neural architecture search	OBJECT DETECTION; NETWORK; DRIVEN; MODEL	Saliency detection methods proposed in the literature exploit different rationales, visual clues, and assumptions, but there is no single best saliency detection algorithm that is able to achieve good results on all the different benchmark datasets. In this paper we show that fusing different saliency detection algorithms together by exploiting neural network architectures makes it possible to obtain better results. Designing the best architecture for a given task is still an open problem since the existing techniques have some limits with respect to the problem formulation, to the search space, and require very high computational resources. To overcome these problems, in this paper we propose a three-step fusion approach. In the first step, genetic programming techniques are exploited to combine the outputs of existing saliency algorithms using a set of provided operations. Having a discrete search space allows us a fast generation of the candidate solutions. In the second step, the obtained solutions are converted into backbone Convolutional Neural Networks (CNNs) where operations are all implemented with differentiable functions, allowing an efficient optimization of the corresponding parameters (in a continuous space) by backpropagation. In the last step, to enrich the expressiveness of the initial architectures, the networks are further extended with additional operations on intermediate levels of the processing that are once again efficiently optimized through backpropagation. Extensive experimental evaluations show that the proposed saliency fusion approach outperforms the state-of-the-art on the MSRAB dataset and it is able to generalize to unseen data of different benchmark datasets.																	1566-2535	1872-6305				MAY	2020	57						89	101		10.1016/j.inffus.2019.12.007													
J								Towards perceptual image fusion: A novel two-layer framework	INFORMATION FUSION										Image fusion; Information; Perceptual importance; Regular layer; Irregular layer	QUALITY ASSESSMENT; CONTOURLET TRANSFORM; WAVELET TRANSFORM; PERFORMANCE; CURVELET; BRAIN; MODEL	Recent studies in neuroscience indicate that the human visual system perceives regular and irregular contents separately: the former mainly illustrate primary visual information such as image structures; while the latter are generally messy and independent, and seem to be less important in perception. However, without any reference to such perceptual theory, the existing image fusion algorithms treat these two types of contents equally, and may not preserve the most perceptually important information in source images. In this work, we propose a new two-layer image fusion framework towards consistency with human perception. The main contributions are as follows: (1) We firstly explore the recently revealed perceptual theory and characterize perceptual significance from regular and irregular image contents in image fusion. This creates the possibilities to develop fusion algorithms towards consistency with human perception and preserve more desirable information in the fused result (2) Following the concept of active inference mechanisms in perception, we present a perceptual image decomposition model with a local regression method to separate images into regular and irregular layers. In this way, we could treat these two kinds of image contents discriminatively, with elaborately selected fusion strategies based on sparse representation and local energy. We conduct extensive experiments including subjective evaluation, objective evaluation and perceptual assessment, and the experimental results demonstrate the superiorly of the proposed model.																	1566-2535	1872-6305				MAY	2020	57						102	114		10.1016/j.inffus.2019.12.002													
J								A survey on machine learning for data fusion	INFORMATION FUSION										Data fusion; Machine learning; Fusion methods; Fusion criteria	MULTISENSOR DATA FUSION; SUPPORT VECTOR MACHINE; INFORMATION FUSION; INTRUSION DETECTION; MINING OPINIONS; INTERNET; PRIVACY	Data fusion is a prevalent way to deal with imperfect raw data for capturing reliable, valuable and accurate information. Comparing with a range of classical probabilistic data fusion techniques, machine learning method that automatically learns from past experiences without explicitly programming, remarkably renovates fusion techniques by offering the strong ability of computing and predicting. Nevertheless, the literature still lacks a thorough review of the recent advances of machine learning for data fusion. Therefore, it is beneficial to review and summarize the state of the art in order to gain a deep insight on how machine learning can benefit and optimize data fusion. In this paper, we provide a comprehensive survey on data fusion methods based on machine learning. We first offer a detailed introduction to the background of data fusion and machine learning in terms of definitions, applications, architectures, processes, and typical techniques. Then, we propose a number of requirements and employ them as criteria to review and evaluate the performance of existing fusion methods based on machine learning. Through the literature review, analysis and comparison, we finally come up with a number of open issues and propose future research directions in this field.																	1566-2535	1872-6305				MAY	2020	57						115	129		10.1016/j.inffus.2019.12.001													
J								Hyperspectral image visualization with edge-preserving filtering and principal component analysis	INFORMATION FUSION										Hyperspectral image visualization; Principal component analysis; Edge-preserving filtering; Dimension reduction	BAND SELECTION; DIMENSION REDUCTION; COLOR DISPLAY; FUSION; CLASSIFICATION; LEVEL	In this paper, an edge-preserving filtering and principal component analysis (PCA)-based visualization method is proposed for hyperspectral images, in which both global and local image information of hyperspectral images (HSIs) are taken into account in the proposed visualization framework that consists of the following major steps. First, the band number of the original image is reduced with averaging-based image fusion (AIF). Then, the edge-preserving filtering is performed on the dimension reduced image so as to decompose it into two components, i.e., the base layers which contain the large-scale boundary information, and the detail layers which contain the mid-and small-scale edges and textures. Next, the base layers are fused with the principal component analysis method, and the detail layers are fused with the weighted sum method, in which the fusion weights are determined by the transform matrix of the base layers. Finally, the fused detail layers are enhanced with histogram equalization and combined with the fused base layers to visualize the hyperspectral image. Experimental results on four real hyperspectral data sets demonstrate that the proposed approach performs the best in improving image contrast and preserving the details. In addition, compared with seven state-of-the-art visualization methods, the quantitative metrics obtained by the proposed method on four data sets have been increased by 80.45%, 69.29%, 138.61%, and 23.21% on average in terms of separability of features (SF), standard deviation (SD), average gradient (AG), and entropy (EN).																	1566-2535	1872-6305				MAY	2020	57						130	143		10.1016/j.inffus.2019.12.003													
J								Development of computational models of emotions: A software engineering perspective	COGNITIVE SYSTEMS RESEARCH										Computational model of emotion; Software engineering; Formal development process; Software methodology	OF-THE-ART; AUTONOMOUS AGENTS; APPRAISAL; COGNITION; COMMUNICATION	Computational Models of Emotions (CMEs) are software systems designed to explain the phenomenon of emotions. The mechanisms implemented in this type of computational models are based on human emotion theories reported in the literature and designed to provide intelligent agents with affective capabilities and improve human-computer interaction. However, despite the growing interest in this type of models, the development process of CMEs does not seem to follow formal software methodologies. In this paper, we present an analysis of CMEs from a software engineering perspective. We aim to identify what elements of software engineering are used in the development process of CMEs and to demonstrate how some software engineering techniques may support and improve their development process. We discuss a series of challenges to be addressed in order to take advantage of software engineering techniques: (1) definition of guidelines to help decide which emotion theories should be implemented computationally, (2) homogenization of terms about human emotions, their components, phases, and cycles implemented in CMEs, (3) design of CMEs whose components can be reusable, (4) definition of standard criteria for comparative analysis between CMEs, (5) identification of software engineering principles, concepts, and design practices useful in the construction of CMEs, and (6) definition of standard frameworks to validate CMEs. (C) 2019 Elsevier B.V. All rights reserved.																	1389-0417					MAY	2020	60						1	19		10.1016/j.cogsys.2019.11.001													
J								High capacity adaptive image steganography with cover region selection using dual-tree complex wavelet transform	COGNITIVE SYSTEMS RESEARCH										Image steganography; Adaptive embedding; DT-CWT; Edge detection; K-NN method	APPROXIMATION; LEVEL	The importance of image steganography is unquestionable in the field of secure multimedia communication. Imperceptibility and high payload capacity are some of the crucial parts of any mode of steganography. The proposed work is an attempt to modify the edge-based image steganography which provides higher payload capacity and imperceptibility by making use of machine learning techniques. The approach uses an adaptive embedding process over Dual-Tree Complex Wavelet Transform (DT-CWT) subband coefficients. Machine learning based optimization techniques are employed here to embed the secret data over optimal cover-image-blocks with minimal retrieval error. The embedding process will create a unique secret key which is imperative for the retrieval of data and need to be transmitted to the receiver side via a secure channel. This enhances the security concerns and avoids data hacking by intruders. The algorithm performance is evaluated with standard benchmark parameters like PSNR, SSIM, CF, Retrieval error, BPP and Histogram. The results of the proposed method show the stego-image with PSNR above 50 dB even with a dense embedding of up to 7.87 BPP. This clearly indicates that the proposed work surpasses the state-of-the-art image steganographic systems significantly. (C) 2019 Elsevier B.V. All rights reserved.																	1389-0417					MAY	2020	60						20	32		10.1016/j.cogsys.2019.11.002													
J								Differences in numeric, verbal, and spatial reasoning between engineering and literature students through a neurocognitive lens	COGNITIVE SYSTEMS RESEARCH										Brain electrical activation; Engineering students; Literature students; Numeric reasoning; Spatial reasoning; Verbal reasoning	FUTURE; MEMORY	This study used electroencephalography to investigate the brain activations of college students of various disciplines when they responded to questions in numeric, verbal, and spatial reasoning tasks. In total, 15 engineering students and 15 literature students were recruited in this experiment and were asked to respond to 12 intelligence test questions. The results were as follows: (i) the participants' brain activations increased in the frontoparietal network during the numeric reasoning task, and the spectral power in the right anterior temporal cortex was generally higher in the literature students than in the engineering students. (ii) Activations of the language network were observed during the verbal reasoning task, and the spectral power in the right-biased posterior frontal cortex was generally higher in the literature students than in the engineering students; by contrast, the spectral power in the left lateral frontal cortex was generally higher in the engineering students than in the literature students. (iii) The participants' brain activations increased in the spatial processing network during the spatial reasoning task, and the spectral power in the right posterior temporal cortex was generally higher in the literature students than in the engineering students. (C) 2019 Elsevier B.V. All rights reserved.																	1389-0417					MAY	2020	60						33	43		10.1016/j.cogsys.2019.11.003													
J								Empirical and modeling study of emotional state dynamics in social videogame paradigms	COGNITIVE SYSTEMS RESEARCH										Human-computer interface; Affective computing; Artificial emotional intelligence; Virtual characters; Intelligent agents; Semantic correlates; Cognitive architectures; Electromyography; EMG	ERRORS	The objective of this work was to study the dynamics of human emotional states in the process of social interaction in a virtual environment. The previously developed for this purpose prototypes of the virtual actor (NPC) and its virtual environment simulator "Teleport" underwent significant re-design and modification. The experimental platform was re-implemented and used in experiments with college student participants, combining electromyography, emotion recognition in facial recordings and model-based game log analysis in a social videogame paradigm. Participants interacted with two virtual actors implemented based on the eBICA cognitive architecture (Samsonovich, 2013, 2018). Positive correlations were found between eBICA model predictions and participant affects extracted from their facial expressions and facial muscle activity. Affective dynamics of social phenomena, such as the establishment of partnership or an act of betrayal, were characterized and found consistent with the model predictions. Other findings include a gradually developing emotional reaction, possibly due to the integration of appraisals of game events. Overall, obtained results confirm the eBICA model, suggesting its further extension and refinement. (C) 2019 Elsevier B.V. All rights reserved.																	1389-0417					MAY	2020	60						44	56		10.1016/j.cogsys.2019.12.001													
J								Socially emotional brain-inspired cognitive architecture framework for artificial intelligence	COGNITIVE SYSTEMS RESEARCH										Cognitive architecture; Affective computing; Emotional intelligence; Moral schema; Semantic mapping; Strong AI; Active learning; BICA Challenge	MODEL; NEUROSCIENCE; SELF	Artificial Intelligence of the next generation needs to interact with users socially, convincing them in its ability to understand human minds, including emotions. For this to happen, an artificial emotional intelligence is needed, capable of adequate, believable behavior in social emotional interactions. Building on previous developments, the present work extends the general framework of emotional Biologically Inspired Cognitive Architecture (eBICA: Samsonovich, 2013, 2018), endowing it with fluents describing, in addition to appraisals, somatic markers, feelings, emotions, moods, emotional reactions and biases. Key building blocks that integrate them are moral schemas and semantic maps. The model describes interaction of three factors: plans and commitments, moral and ethical values, and somatic comfort. Learning in this framework includes self-organization of semantic maps that in turn may provide guidance for active humanlike learning. Implications for empirical studies and practical applications are discussed together with the expected impact. (C) 2019 Elsevier B.V. All rights reserved.																	1389-0417					MAY	2020	60						57	76		10.1016/j.cogsys.2019.12.002													
J								Pedestrian detection using a moving camera: A novel framework for foreground detection	COGNITIVE SYSTEMS RESEARCH										Moving pedestrian; Moving camera; Ego-motion compensation; Trajectory classification; Pedestrian detection challenge	FASTER R-CNN; OBJECT DETECTION; TRACKING; ROBUST; FEATURES; CLASSIFICATION; ALGORITHMS; NIGHTTIME; OCCLUSION; VISION	While background subtraction techniques have been widely applied to detect moving objects in a video stream captured by a static camera, detecting moving objects using a moving camera still represents a challenging task. In this context, pedestrian detection using a camera placed on the top of a vehicle's windshield has been rarely investigated. This is mainly due to the background ego-motion. Since the scene captured by the camera seems in motion, it is very difficult to distinguish the moving pedestrians from the others that belong to the static part of the scene. For this reason, a compensation step is needed to suppress the ego-motion. This paper presents a study on the main challenges facing pedestrian detection systems as well as methods proposed to handle these challenges. A novel trajectory classification framework for detecting pedestrians even in challenging real-world environments is proposed. The proposed method models the background motion between two consecutive frames in order to compensate the camera motion. Then, it defines a classification process that differentiates between the background and the foreground in the frame. Using the defined foreground, we consequently identify the presence of pedestrians in the scene. The proposed method was validated on a public benchmark dataset: CVC-14 containing both visible and far infrared video sequences in day and night times. Experimental results confirm the effectiveness of the proposed approach in capturing the dynamic aspect between frames and therefore detecting the presence of pedestrians in the scene. (C) 2019 Elsevier B.V. All rights reserved.																	2214-4366	1389-0417				MAY	2020	60						77	96		10.1016/j.cogsys.2019.12.003													
J								An ontology constructing technology oriented on massive social security policy documents	COGNITIVE SYSTEMS RESEARCH										Ontology expansion; Text fragment extraction; Semi-automatic construction		To solve the problem brought from enormous policy documents and complex management in the social security domain, the article uses ontology as the way of representing and storing knowledge. The article first constructs the framework of ontology through manual work so that it can ensure the relative accuracy of the ontology structure. Then it achieves the automatic ontology expansion based on the inclusion relationship of property sets or operational object sets. The article uses a semi-automatic method that extracts hierarchical concepts and non-hierarchical concepts from domain thesaurus by using the method combining statistics with rules to construct the ontology. Besides constructing the ontology, the article proposes the concepts of concept phrase vector model and high frequency characteristics phrase vector model. The experiment result indicates that ontology semi-automatic construction process can help experts to construct the social security ontology effectively oriented on massive policy documents and is a considerable reference for the construction of ontology in other domains. (C) 2019 Elsevier B.V. All rights reserved.																	1389-0417					MAY	2020	60						97	105		10.1016/j.cogsys.2019.09.005													
J								"Blessed by the algorithm": Theistic conceptions of artificial intelligence in online discourse	AI & SOCIETY										Artificial intelligence; Religion; Algorithm; Theism; Social media		"My first long haul flight that didn't fill up and an empty row for me. I have been blessed by the algorithm ". The phrase 'blessed by the algorithm' expresses the feeling of having been fortunate in what appears on your feed on various social media platforms, or in the success or virality of your content as a creator, or in what gig economy jobs you are offered. However, we can also place it within wider public discourse employing theistic conceptions of AI. Building on anthropological fieldwork into the 'entanglements of AI and Religion' (Singler 2017a), this article will explore how 'blessed by the algorithm' tweets are indicative of the impact of theistic AI narratives: modes of thinking about AI in an implicitly religious way. This thinking also represents continuities that push back against the secularisation thesis and other grand narratives of disenchantment that claim secularity occurs because of technological and intellectual progress. This article will also explore new religious movements, where theistic conceptions of AI entangle technological aspirations with religious ones.																	0951-5666	1435-5655															10.1007/s00146-020-00968-2		APR 2020											
J								Particle swarm optimization based parameter selection technique for unsupervised discriminant analysis in transfer learning framework	APPLIED INTELLIGENCE										Unsupervised discriminant analysis; Transfer learning; Particle swarm optimization; Domain adaptation; Classification; Parameter selection	DOMAIN ADAPTATION; KERNEL	The purpose of transfer learning is to utilize the knowledge gained from the existing (source) domain to enhance the performance on a distinct but related (target) domain. Existing works on transfer learning are not capable of optimizing different quality measures (components) such as minimizing the marginal distribution, minimizing the conditional distribution, maximizing the target domain variance, modeling the manifold by utilizing the common geometric properties in the source as well as the target domain at the same time. Moreover, existing transfer learning methods use conventional approaches to determine the appropriate values of their parameters, which is very hectic and time-consuming. Therefore, in order to overcome the drawbacks of existing approaches, we propose a Particle Swarm Optimization based Parameter Selection Approach for Unsupervised Discriminant Analysis (UDATL-PSO) in transfer learning framework. In UDATL-PSO, all the quality measures are considered at the same time, as well as the PSO approach has been used to select the best values of their parameters. Extensive experiments on various transfer learning tasks show that the proposed method has a significant influence on state-of-the-art methods.																	0924-669X	1573-7497				OCT	2020	50	10					3071	3089		10.1007/s10489-020-01710-7		APR 2020											
J								Evolutionary generation of orthomorphisms in the Galois field F-2(8)	EVOLUTIONARY INTELLIGENCE										Global optimization and rapid convergence; Evolutionary computation; Orthomorphisms enumeration; The information security; The cryptographic components	PARTICLE SWARM; OPTIMIZATION; ALGORITHM	Irreplaceable orthomorphisms over the Galois field F-2(8) plays a key part in the information security domain about the automation design to cryptographic components. By searching, the genetic algorithm and the particle swarm algorithm are used to gain the orthomorphisms over the Galois field F-2(8) in this paper. Their difference uniformity and nonlinearity are also calculated which is beneficial to carry on the automation design of cryptographic components. In a useful way, the orthomorphisms over the Galois field F-2(8) is converted into an orthomorphism polynomial with the positive degree, which helps the authors to study how the degree of polynomials over the Galois field is distributed.																	1864-5909	1864-5917															10.1007/s12065-020-00411-x		APR 2020											
J								Hybrid encryption framework for securing big data storage in multi-cloud environment	EVOLUTIONARY INTELLIGENCE										Big data; Multi cloud; Security; Data storage; Attackers	ACCESS	In the present scenario, big data is facing many challenges regarding the data storage, data theft and unauthorized access. Many researchers are concentrated on developing the security mechanism for big data storage. To overcome the above issue, this paper concentrated on developing the encryption algorithm for storing big data in the multi cloud storage. The multi cloud storage environment permits the user to store the data in to different cloud storage services. This paper aims to develop the secure framework which restricts the insider attacks. The proposed framework contains data uploading, slicing, indexing, encryption, distribution, decryption, retrieval and merging process. The hybrid encryption algorithm was developed to provide the security to the big data before storing it in to the multi cloud. The Simulation analysis is carried with real time cloud storage environments. The proposed algorithm recorded around 2630 KB/S for the encryption process. The results prove the superiority of the proposed algorithm compared to the bench mark algorithms.																	1864-5909	1864-5917															10.1007/s12065-020-00404-w		APR 2020											
J								Construction of cascaded depth model based on boosting feature selection and classification	EVOLUTIONARY INTELLIGENCE										Boosting; Cascade model; Feature extraction		Artificial intelligence is an important research direction in the field of computer science. Its vision is to better understand the world around us. In this paper, the specific feature transformation, feature selection and classifier algorithm used in the framework are studied and analyzed, and a cascade depth model is constructed. Through detailed analysis of the feature transformation, feature selection and classification methods used in the framework, an effective cascade depth model based on feature extraction and feature selection is successfully implemented, and the effectiveness of the proposed feature combination method is verified.																	1864-5909	1864-5917															10.1007/s12065-020-00413-9		APR 2020											
J								Research and application of AHP-fuzzy comprehensive evaluation model	EVOLUTIONARY INTELLIGENCE										AHP; Fuzzy comprehensive evaluation method; Quantitative evaluation; Weight calculation		AHP refers to the decision making method that decomposes the elements that are always related to decision-making into levels of objectives, criteria, schemes, etc., and conducts qualitative and quantitative analysis on this basis. Fuzzy comprehensive evaluation method is a comprehensive evaluation method based on fuzzy mathematics, which converts qualitative evaluation into quantitative evaluation according to the membership theory of fuzzy mathematics. In this paper, Yaahp software is used for weight calculation and consistency test of ahp, and sensitivity analysis is also carried out. The fuzzy comprehensive evaluation method is used to determine the evaluation index set and the fuzzy comment set, and the evaluation model is constructed by fuzzy synthesis. On this basis, the empirical analysis is carried out.																	1864-5909	1864-5917															10.1007/s12065-020-00415-7		APR 2020											
J								Stock selection heuristics for performing frequent intraday trading with genetic programming	GENETIC PROGRAMMING AND EVOLVABLE MACHINES										Stock; Trading; Intraday; Portfolio	PORTFOLIO OPTIMIZATION	Intraday trading attempts to obtain a profit from the microstructure implicit in price data. Intraday trading implies many more transactions per stock compared to long term buy-and-hold strategies. As a consequence, transaction costs will have a more significant impact on the profitability. Furthermore, the application of existing long term portfolio selection algorithms for intraday trading cannot guarantee optimal stock selection. This implies that intraday trading strategies may require a different approach to stock selection for daily portfolios. In this work, we assume a symbiotic genetic programming framework that simultaneously coevolves the decision trees and technical indicators to generate trading signals. We generalize this approach to identify specific stocks for intraday trading using stock ranking heuristics: Moving Sharpe ratio and a Moving Average of Daily Returns. Specifically, the trading scenario adopted by this work assumes that a bag of available stocks exist. Our agent then has to both identify which subset of stocks to trade in the next trading day, and the specific buy-hold-sell decisions for each selected stock during real-time trading for the duration of the intraday period. A benchmarking comparison of the proposed ranking heuristics with stock selection performed using the well known Kelly Criterion is conducted and a strong preference for the proposed Moving Sharpe ratio demonstrated. Moreover, portfolios ranked by both the Moving Sharpe ratio and a Moving Average of Daily Returns perform significantly better than any of the comparator methods (buy-and-hold strategy, investment in the full set of 86 stocks, portfolios built from random stock selection and Kelly Criterion).																	1389-2576	1573-7632															10.1007/s10710-020-09390-5		APR 2020											
J								Unsupervised and supervised methods for the detection of hurriedly created profiles in recommender systems	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Recommender systems; Rofile injection attacks; Synthetic coordinates; k-means clustering; Random forest	INJECTION ATTACKS; CLASSIFICATION; SEGMENTATION	Recommender systems try to provide users with accurate personalized suggestions for items based on an analysis of previous user decisions and the decisions made by other users. These systems suffer from profile injection attacks, where malicious profiles are generated in order to promote or demote a particular item introducing abnormal ratings. The problem of automatic detection of such malicious profiles has been recently addressed by a great number of authors in the literature using supervised and unsupervised approaches. In this paper, we propose a framework to identify anomalous rating profiles, where each attacker (outlier) hurriedly creates profiles that inject into the system an unspecified combination of random ratings and specific ratings, without any prior knowledge of the existing ratings. This attack is a superset of the two different attacks (Uniform and Delta) proposed in Harper et al. (ACM Trans Interact Intell Syst 5(4):19, 2016) making the attack model more realistic and its detection more challenging. The proposed detection method is based on several attributes related to the unpredictable behavior of the outliers in a validation set, on the user-item rating matrix, on the similarity between users and on the filler items. In this work, we propose a new attribute (RIS) to capture the randomness in item selection of the abnormal profiles. In this work, three different systems are proposed: (1) a probabilistic framework that estimates the probability of a user to be an outlier by combining several features in a completely unsupervised way. (2) An unsupervised clustering system based on the k-means algorithm that automatically spots the spurious profiles. (3) A supervised framework that uses a random forest classifier for cases where labeling sample data is available. Experimental results on the MovieLens and the Small Netflix datasets demonstrate the high performance of the proposed methods as well as the discrimination accuracy of the proposed features.																	1868-8071	1868-808X				SEP	2020	11	9					2165	2179		10.1007/s13042-020-01108-4		APR 2020											
J								A novel handover detection model via frequent trajectory patterns mining	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Mobile computing; Cybernetic system; Handover detection; Frequent trajectory patterns mining; Machine learning	ROUTING METHOD; PREDICTION; ALGORITHM; INTERNET; STRATEGY	As the cellular wireless communication techniques grow rapidly, the cells become smaller than the traditional communication system, then the handover events are very frequent and need to support a large number of users, and handover detection has become a very active research direction in a mobile computing environment. In order to copy with the problem of frequent handover operations between base stations in current cellular communication networks as cybernetic systems, we propose a novel handover detection approach by integrating frequent trajectory patterns mining and location prediction techniques. The proposed model contains the following essential steps: (1) mining frequent trajectory patterns from large-scale historical trajectory databases by applying an improved Apriori-like rule-based machine learning algorithm, which finds the intersection of candidate items by applying the trajectory connection operation instead of calculating the support count of each trajectory patterns and the candidate items are considerably reduced; (2) discovering movement rules based on the frequent trajectory pattern set by finding the postfix items of given prefix items satisfying the minimum support threshold; (3) inferring the future locations of moving objects by applying the movement rules matching strategy; (4) determining whether or not to perform handover detection across base stations in a cellular network beyond the discovered prediction results, according to the coverage area of cellular networks. Extensive experiments were conducted on the mobile datasets and the experimental results demonstrate the advantages of the proposed algorithm from the following four aspects: (1) the accuracy of handover detection is above 95% at average which is a very satisfactory result in a mobile computing environment; (2) the time cost is less than 20 s when the number of movement rules and handover detection is 1000, which further shows the merit of the runtime performance of the proposed method; (3) the frequent-trajectory-patterns based handover detection algorithm can successfully avoid the ping-pong effect due to unnecessary handover operations; (4) and lastly significantly reduce the error rate of frequent handover decisions and the average unnecessary handover rate is lower than 0.05 when compared with the state-of-the-art methods.																	1868-8071	1868-808X				DEC	2020	11	12					2587	2606		10.1007/s13042-020-01126-2		APR 2020											
J								Time series real time naive bayes electrocardiogram signal classification for efficient disease prediction using fuzzy rules	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										ECG; Signal processing; Disease prediction; Fuzzy rule; Naive bayes classifier; Time series data	ECG; FEATURES	Towards the problem of ECG classification and disease prediction, various approaches are analyzed and discussed. However, the methods suffer to achieve higher performance in classification or disease prediction. To improve the performance, an efficient time series real time Naive Bayes ECG classification and disease prediction approach using fuzzy rule is presented in this paper. The method reads the ECG signals available and performs noise removal initially. From the graphs available, the features mentioned above are extracted and if there exist any incomplete or missing signal then the ECG sample has been removed from the data set. Once the preprocessing and feature extraction are done, then the features extracted. With the learned features, the method generates fuzzy rule for different disease class. The proposed algorithm computes posterior probability according to the mapping of different features of fuzzy rule. The classification or disease prediction is performed by measuring multi-feature signal similarity (MFSS). Estimated MFFS value has been used to measure the cardiac disease prone weight (CDPW) towards various classes available. According to the value of CDPW has been used to perform classification or disease prediction.																	1868-5137	1868-5145															10.1007/s12652-020-02003-0		APR 2020											
J								Trust aware localized routing and class based dynamic block chain encryption scheme for improved security in WSN	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										WSN; Data security; Secure routing; Block chain; TDFS; Trust aware routing		The modern wireless sensor network has great impact in the development of various domains of applications. The presence of malicious nodes introduces various threats and challenges to the network services. Different algorithms have been proposed towards the data security but not achieved the expected performance. Towards performance hike, a novel trust aware localized routing and class based dynamic encryption scheme has been presented. The method first discovers the route to reach the destination and transmit the data packet. But the localized nature of each hop in the route estimates the trust measure for each neighbor according to their prior involvement in data transmission and number of retransmission of the same packets with other neighbor, number of successful transmission. By identifying the values of those parameters, the value of trusted data forwarding support (TDFS) is measured. According to the TDFS value of several routes, a route with the specific neighbor only selected for route selection. On the other side, the method maintains and classifies the data being transmitting into number of classes. Further, the method uses different signature and encryption schemes for various classes. The data has been encrypted with class specific scheme and key before transmission. The method generates a block chain where each block contains the part of encrypted data and represented by a hash and pointer to the next block. The same has been reversed to produce original data from the encrypted key. The method introduces higher performance data security and improves the overall network performance.																	1868-5137	1868-5145															10.1007/s12652-020-02007-w		APR 2020											
J								Enhanced fuzzy finite state machine for human activity modelling and recognition	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Human activity modelling and recognition; Fuzzy finite state machine; Long short-term memory neural network; Convolutional neural network	SHORT-TERM-MEMORY; BINARY SENSORS; NETWORKS; BEHAVIOR	A challenging key aspect of modelling and recognising human activity is to design a model that can deal with the uncertainty in human behaviour. Several machine learning and deep learning techniques are employed to model the Activity of Daily Living (ADL) representing the human activity. This paper proposes an enhanced Fuzzy Finite State Machine (FFSM) model by combining the classical FFSM with Long Short-Term Memory (LSTM) neural network and Convolutional Neural Network (CNN). The learning capability in the LSTM and CNN allows the system to learn the relationship in the temporal human activity data and to identify the parameters of the rule-based system as building blocks of the FFSM through time steps in the learning mode. The learned parameters are then used for generating the fuzzy rules that govern the transitions between the system's states representing activities. The proposed enhanced FFSMs were tested and evaluated using two different datasets; a real dataset collected by our research group and a public dataset collected from CASAS smart home project. Using LSTM-FFSM, the experimental results achieved 95.7%\ for the first dataset and the second dataset, respectively. Once CNN-FFSM was applied to both datasets, the obtained results were 94.2% and 99.3%, respectively.																	1868-5137	1868-5145															10.1007/s12652-020-01917-z		APR 2020											
J								Security protocol using elliptic curve cryptography algorithm for wireless sensor networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										WSN; Security; Elliptic curve digital signature; Key management; Cryptography	AUTHENTICATION; QUALITY	Information security broadly refers to the state of protection against unsanctioned access to information or data, principally electronic or digital data. In today's world of modern technology, there is a need to design and develop security measures to protect information from various security risks and threats. Wireless Sensor Networks (WSNs) hold significant importance in this era of the technological world; as its wide range of applications are being used around the globe in almost every domain. WSNs are being deployed with several constraints and limitations, due to which deploying security mechanisms on such networks becomes a difficult task for the developers. This research work specifically targeted security issues in WSNs and hence subjected to provide authentication and data encryption in a novel manner for node-to-node communication. The proposed scheme not only provides security for the node to node communication network but also hoards memory space on nodes with the help of Elliptic Curve Digital Signature (ECDSA) cryptographic scheme to provide an appropriate mechanism for measuring key generation time, count of hello message and packet size. Furthermore, the Algorithm for Wireless Secure Communication (ASCW) also provides key management with acceptable key length. In addition to this, ASCW helps in securing the communication on node level which helps in securing the whole network in a better and efficient manner. ASCW also reduces the cost of risk and security threats on the network with the help of authentication mechanism. A physical testbed has designed based on devices and sensor motes according to the required specifications. The proposed solutions have evaluated in terms of key generation time, several hello message and size of data packets. Experimental results have indicated that ASCW is one of the suitable and a novel approach for securing data on nodes during communication in WSNs.																	1868-5137	1868-5145															10.1007/s12652-020-02020-z		APR 2020											
J								Deep learning based big medical data analytic model for diabetes complication prediction	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Big data; Deep belief network; Support vector machine; Random forest; K nearest neighbour; Long short term memory; Gated recruitment unit; Convolutional neural network	BELIEF NETWORKS; CLASSIFICATION	The revolution in digitization makes the health care sector as a prime source of big data. The analysis of these data could be a great supporting source for deriving new insights, which increases the care and awareness about health. Diabetes together with its complications has been recognized worldwide as a chief public health threat. Predicting diabetic complications is considered as a highly effectual technique for augmenting the survival rate of diabetic patients. While many studies currently use medical images and structured medical records, very limited efforts have been dedicated for applying Data Mining (DM) techniques for unstructured textual medical records, for instance, admission and discharge records. Many DM techniques have been generated for envisaging diabetic complications. But in existing methods, the classification as well as prediction accuracy is not so high. So this paper proposes a model centered on Deep Learning (DL) for predicting complications of Type 2 Diabetes Mellitus. The proposed model follows data collection, pre-training, feature extraction, Deep Belief Network (DBN), validation process, and classification steps for predicting diabetic complications. Finally, the performances proffered by the proposed DL based Big Medical Data Analytics model using DBN as well as the prevailing techniques are contrasted with respect to Precision, accuracy, and Recall. The Training, as well as the Testing process, delineates the pervasiveness of risk with an accuracy of 81.20%. This realistic prediction model will be very much useful for effectively managing diabetes.																	1868-5137	1868-5145															10.1007/s12652-020-01930-2		APR 2020											
J								Formalization of the Poincare Disc Model of Hyperbolic Geometry	JOURNAL OF AUTOMATED REASONING										Poincare model; Isabelle; HOL; Formalization of geometry		We describe formalization of the Poincare disc model of hyperbolic geometry within the Isabelle/HOL proof assistant. The model is defined within the complex projective line CP1and is shown to satisfy Tarski's axioms except for Euclid's axiom-it is shown to satisfy it's negation, and, moreover, to satisfy the existence of limiting parallels axiom.																	0168-7433	1573-0670															10.1007/s10817-020-09551-2		APR 2020											
J								Detection and discrimination of Carica papaya fungi through the analysis of volatile metabolites by gas chromatography and analysis of variance-principal component analysis	JOURNAL OF CHEMOMETRICS										chemometrics; fruit; GC-MS; microbial metabolomics; PCA; SPME	SOLID-PHASE MICROEXTRACTION; SPECTROMETRY-BASED METABOLOMICS; ORGANIC-COMPOUNDS; ANOVA-PCA; FRUITS	Conventional detection and identification of the fungi causing postharvest diseases in fruits are time-consuming, laborious, and can only be performed after the manifestation of symptoms. In this work, an alternative method based on headspace analysis, and which allows the early detection of fungi species frequently found papaya fruit, is presented. Volatile compounds of four in vitro fungi cultures (Alternaria alternata, Colletotrichum gloeosporioides, Fusarium solani, and Lasiodiplodia theobromae) were extracted using solid-phase microextraction (SPME) and analyzed by gas chromatography coupled with mass spectrometry. The resulting chromatographic fingerprints were explored by conventional principal component analysis (PCA) and analysis of variance (ANOVA)-PCA. Decomposition of the original matrix, according to the factors proposed in the experimental design, by ANOVA before applying the PCA improved the distinction of the control and fungi samples. The main chromatographic peaks referring to the metabolites produced by each species were successfully identified using the proposed analysis. A primary alcohol with five carbons and phenylethyl alcohol were observed in all fungi species and so could be used as an indicative of postharvest disease. Although unique metabolites were detected for all fungi species, only those from C. gloeosporioides and another from F. solani could be surely identified, such as thymolmethyl and 3,6-dimethylhept-6-en-4-yn-3-ol, respectively. The in vitro results obtained are promising, and it is expected that the biomarkers detected in this work will be useful in future development of methods for the early detection and classification of papaya fungi species.																	0886-9383	1099-128X														e3244	10.1002/cem.3244		APR 2020											
J								Automated facial video-based recognition of depression and anxiety symptom severity: cross-corpus validation	MACHINE VISION AND APPLICATIONS										Affective computing; Depression assessment; Facial image analysis; Image processing; Machine learning	APPEARANCE; DIAGNOSIS; EMOTION	There is a growing interest in computational approaches permitting accurate detection of nonverbal signs of depression and related symptoms (i.e., anxiety and distress) that may serve as minimally intrusive means of monitoring illness progression. The aim of the present work was to develop a methodology for detecting such signs and to evaluate its generalizability and clinical specificity for detecting signs of depression and anxiety. Our approach focused on dynamic descriptors of facial expressions, employing motion history image, combined with appearance-based feature extraction algorithms (local binary patterns, histogram of oriented gradients), and visual geometry group features derived using deep learning networks through transfer learning. The relative performance of various alternative feature description and extraction techniques was first evaluated on a novel dataset comprising patients with a clinical diagnosis of depression (n=20\) and healthy volunteers (n=45). Among various schemes involving depression measures as outcomes, best performance was obtained for continuous assessment of depression severity (as opposed to binary classification of patients and healthy volunteers). Comparable performance was achieved on a benchmark dataset, the audio/visual emotion challenge (AVEC'14). Regarding clinical specificity, results indicated that the proposed methodology was more accurate in detecting visual signs associated with self-reported anxiety symptoms. Findings are discussed in relation to clinical and technical limitations and future improvements.																	0932-8092	1432-1769				APR 30	2020	31	4							30	10.1007/s00138-020-01080-7													
J								Design and implementation of bank CRM system based on decision tree algorithm	NEURAL COMPUTING & APPLICATIONS										Decision tree algorithm; Data mining; Customer relationship management; Information system	CUSTOMER RELATIONSHIP MANAGEMENT; INFORMATION-SYSTEM; PERFORMANCE; ADVANTAGE	With the rapid development of the national economy, the level of informationization and office automation of banks has gradually increased. At the same time, with the development of bank management concepts, the traditional bank customer relationship management (CRM) method has been unable to meet the basic needs of bank development. The purpose of this paper is to design and implement a bank CRM system based on decision tree algorithm. This paper uses decision tree technology, data preprocessing, and other technologies for data mining. Based on the mining results, a customer relationship management system is designed based on the scalability and maintainability of the system. Finally, the system is designed, implemented, and performed functional tests. The experimental results show that the system can fully exploit the consumer demand and consumption habits of existing customers and improve customer satisfaction. In addition, it can adapt to the complex banking information system environment, with sufficient computing power and high accuracy can provide valuable information for bank decision makers. The data mining performance of the system was tested, and the time for running 1 million data volumes was 650 s. It can be seen that the system has excellent running performance.																	0941-0643	1433-3058															10.1007/s00521-020-04959-8		APR 2020											
J								Time series prediction using deep echo state networks	NEURAL COMPUTING & APPLICATIONS										Time series forecasting; Reservoir computing; Echo state network; Additive decomposition; Recurrent neural network	ENSEMBLE	Artificial neural networks have been used for time series modeling and forecasting in many domains. However, they are often limited in their handling of nonlinear and chaotic data. More recently, reservoir-based recurrent neural net systems, most notably echo state networks (ESN), have made substantial improvements for time series modeling. Their shallow nature lends themselves to an efficient training method, but has limitations on nonstationary, nonlinear chaotic time series, particularly large, multidimensional time series. In this paper, we propose a novel approach for forecasting time series data based on an additive decomposition (AD) applied to the time series as a preprocessor to a deep echo state network. We compare the performance of our method, AD-DeepESN, on popular neural net architectures used for time series prediction. Stationary and nonstationary data sets are used to evaluate the performance of the methods. Our results are compelling, demonstrating that AD-DeepESN has superior performance, particularly on the most challenging time series that exhibit non-stationarity and chaotic behavior compared to existing methods.																	0941-0643	1433-3058															10.1007/s00521-020-04948-x		APR 2020											
J								Design of traffic object recognition system based on machine learning	NEURAL COMPUTING & APPLICATIONS										Machine learning; Image recognition; Traffic; Vehicle identification system	IDENTIFICATION	In recent years, researchers have proposed many methods to solve the problem of obstacle detection. However, computer vision-based vehicle detection and recognition technology is still not mature enough. This research combines machine learning technology to construct a traffic object recognition system and applies innovative technology to the computer vision recognition system to construct an automatic identification system suitable for current traffic demand and improve the stability of the traffic system. Moreover, this study uses a combination of a monocular camera and a binocular camera to sense the traffic environment and obtain vehicle position and velocity information. In addition, this study is based on the binocular stereo camera to find the obstacle space and obtain the obstacle relative to the position and speed of the vehicle and combine the obstacle space information to optimize the obstacle frame of the target vehicle. Through experimental research and analysis, it can be seen that the algorithm proposed in this study has certain recognition effect and can be applied to traffic object recognition.																	0941-0643	1433-3058															10.1007/s00521-020-04912-9		APR 2020											
J								A deep wavelet sparse autoencoder method for online and automatic electrooculographical artifact removal	NEURAL COMPUTING & APPLICATIONS										EEG; EOG artifact; Deep wavelet sparse autoencoder	OCULAR ARTIFACTS; EOG ARTIFACTS; EEG; SEPARATION	Electrooculographical (EOG) artifacts are problematic to electroencephalographical (EEG) signal analysis and degrade performance of brain-computer interfaces. A novel, robust deep wavelet sparse autoencoder (DWSAE) method is presented and validated for fully automated EOG artifact removal. DWSAE takes advantage of wavelet transform and sparse autoencoder to become a universal EOG artifact corrector. After being trained without supervision, the sparse autoencoder performs EOG correction on time-frequency coefficients collected after brain wave signal wavelet decomposition. Corrected coefficients are then used for wavelet reconstruction of uncontaminated EEG signals. DWSAE is compared with five other methods: second-order blind identification, information maximization, joint approximation diagonalization of eigen-matrices, wavelet neural network (WNN) and wavelet thresholding (WT). Experimental results on a visual attention task dataset, a mental state recognition dataset and a semi-simulated contaminated EEG dataset show that DWSAE is capable of suppressing EOG artifacts effectively, while preserving the nature of background EEG signals. The mean square error of signals before and after correction by DWSAE on a semi-simulated contaminated EEG segment of 30 s is the lowest (65.62) when compared to the results produced by WNN and WT. DWSAE addresses limitations posed by these methods in three ways. First, DWSAE can be performed automatically and online in a single channel of EEG data; this has advantages over independent component analysis-based methods. Second, its results are robust and stable in comparison with those of other wavelet-based methods. Third, as an unsupervised learning scheme, DWSAE does not require the off-line training that is necessary for WNN and other supervised learning machine learning-based methods.																	0941-0643	1433-3058															10.1007/s00521-020-04953-0		APR 2020											
J								A semi-self-taught network intrusion detection system	NEURAL COMPUTING & APPLICATIONS										Network intrusion detection system; Self-taught learning; Self-supervision; Autoencoder; Discriminant analysis; Malware; BotNets		The ever increasing threat and complexity of modern cyber-attacks requires search for integrated and flexible intelligent defense mechanisms. Such approaches can provide optimal countermeasures, reliable credentials extraction and self-adjusting potential. Given the widespread scale of modern networks and the complexity of cyber-attacks, the problem of self-adaptation goes far beyond the capabilities of network Intrusion Detection Systems (IDS). The main weakness of IDS is the fact that they cannot adapt to new network conditions ("zero day" attacks). This research tries to overcome the above limitation, by introducing a Semi-supervised Discriminant Autoencoder (AUE) which combines Denoising AUEs with a heuristic method of class separation. In essence, the proposed algorithm learns to remodel the displaced specimens instead of the original ones in the super-sphere defined by their closest neighbors. The purpose is to understand the nature of an attack, based on generalized transformed features derived directly from unknown web environments and data.																	0941-0643	1433-3058															10.1007/s00521-020-04914-7		APR 2020											
J								A Context Based Deep Temporal Embedding Network in Action Recognition	NEURAL PROCESSING LETTERS										Deep temporal embedding; Self-supervision; Residual technique; Two-step deep method; Long-term temporal representation	ATTENTION	Long term temporal representation methods demand high computational cost, restricting their practical use in real world applications. We propose a two-step deep residual method for efficiently learning long-term discriminative temporal representation, whilst significantly reducing computational cost. In the first step, a novel self-supervision deep temporal embedding method is presented to embed repetitive short-term motions at a cluster-friendly feature space. In the second step, an efficient temporal representation is made by leveraging the differences between the original data and its associated repetitive motion clusters as a novel deep residual method. Experimental results demonstrate that, the proposed method achieves competitive results on some challenging human action recognition datasets like UCF101, HMDB51, THUMOS14, and Kinetics-400.																	1370-4621	1573-773X				AUG	2020	52	1			SI		187	220		10.1007/s11063-020-10248-1		APR 2020											
J								Intelligent Crime Prevention and Control Big Data Analysis System Based on Imaging and Capsule Network Model	NEURAL PROCESSING LETTERS										Robotic Internet of Things; Industrial IoT system; Intelligent crime prevention; Big data analysis; Machine vision	IDENTIFICATION; RECOGNITION; VISION	With the rapid development of China's national economy, the effects of traditional public security management methods have been greatly weakened, and various new types of criminal activities have continued to occur. Social development has gradually separated from the previous model, and new social contradictions have become prominent. These social contradictions not only impact the lifestyle of ordinary individuals, but also affect the development of society. The data contains the laws of social development and crime governance. How criminal governance adapts to the big data wave has become the key to carrying out the number management in the three-dimensional security prevention and control. The data culture has promoted the transformation of the social governance model and has also led to the transformation of the social science research paradigm. At present, most of the research on big data is only from the perspective of informatics, and there is not much discussion about big data from the legal system level. With the advent of the era of big data, a series of problems have come along. In addition to strengthening the security of the big data operation process at the technical level, it is necessary to strengthen research from a technical level. In this paper, we propose a smart crime prevention and control big data analysis system based on machine Internet of Things and industrial object system. The experimental results show that the proposed method has higher data collection rate and crime prevention and control efficiency.																	1370-4621	1573-773X															10.1007/s11063-020-10256-1		APR 2020											
J								Multimedia Imaging Model of Information System Based on Self-Organizing Capsule Neural Network and Game Theory	NEURAL PROCESSING LETTERS										Self-organizing neural network; Neuron structure; Game theory; Imaging model	FUNCTIONALS	With the advent of multimedia technology, the application fields of computers are further broadened, such as computers widely used in video conferencing, video telephony, shopping mall shopping guides, tour guides, computer-aided integrated manufacturing systems, geographic information systems, computer-aided teaching, etc., with new The emergence of application fields and the increasing demands of people for complex data management are increasing, and the requirements for database management systems are increasing. In general, numbers and characters are formatted data, and existing database technologies, especially relational database technology, can be used for good access management. However, text, sound, and images are unformatted data, so traditional relational models cannot be described and processed. To this end, people began to research multimedia database technology in order to manage multimedia data. In this paper, we proposed a multimedia model of management system based on self-organizing capsule neural network and game theory. Experimental results show that the proposed method has higher efficiency and robustness.																	1370-4621	1573-773X															10.1007/s11063-020-10258-z		APR 2020											
J								MV-algebras as sheaves of l-groups on fuzzy topological spaces	SOFT COMPUTING										MV-algebra; Sheaf representation; Fuzzy topology; Lattice-ordered group	SET; REPRESENTATION	We introduce the concept of fuzzy sheaf as a natural generalization of a sheaf over a topological space in the context of fuzzy topologies. Then, we prove a representation for a class of MV-algebras that we called "locally retractive," in which the representing object is an MV-sheaf of lattice-ordered Abelian groups, namely a fuzzy sheaf in which the base (fuzzy) topological space is an MV-topological space and the stalks are Abelian l-groups. Last, we show that any MV-algebra is embeddable in a locally retractive algebra and, therefore, in the algebra of global sections of one of such sheaves.																	1432-7643	1433-7479				JUN	2020	24	12			SI		8793	8804		10.1007/s00500-020-04944-2		APR 2020											
J								Uncertain maximum likelihood estimation with application to uncertain regression analysis	SOFT COMPUTING										Uncertainty theory; Regression analysis; Maximum likelihood estimation; Imprecise observation		Regression analysis is a mathematical tool to estimate the relationship between explanatory variables and response variable. This paper defines a likelihood function in the sense of uncertain measure to represent the likelihood of unknown parameters. Furthermore, the method of maximum likelihood estimation is used for the parameter estimation of uncertain regression models, and the uncertainty distribution of the disturbance term is simultaneously calculated. Finally, some numerical examples are documented to illustrate the proposed method.																	1432-7643	1433-7479				JUL	2020	24	13					9351	9360		10.1007/s00500-020-04951-3		APR 2020											
J								Industrial time series forecasting based on improved Gaussian process regression	SOFT COMPUTING										Predictive modeling; Industrial time series; Gaussian process regression; Gaussian mixture model; Particle swarm optimization	PARTICLE SWARM OPTIMIZATION; ADAPTIVE SOFT SENSOR; QUALITY PREDICTION; EM ALGORITHM; ONLINE; MIXTURE; MODEL; DYNAMICS	Industrial processes often include shifting operating phases and dynamics, and system uncertainty. Industrial time series data may obey different distributions because of the time-varying characteristic. Therefore, a single global model cannot describe the local characteristics of multiple distributions. In this work, a hybrid GMM-IGPR model is proposed to solve this kind of time series prediction problem by using an improved Gaussian process regression (GPR) based on Gaussian mixture model (GMM) and a variant of the basic particles swarm optimization (PSO). In a first treatment to the time series, different distributions of the original dataset are characterized by adopting the GMM as a cluster method. Then, multiple localized GPR models are built to characterize the different properties between inputs and output within various clusters. In order to optimize the proposed algorithms, this paper utilizes the DEPSO which introduces differential evolution (DE) operator into the basic PSO algorithm to estimate hyperparameters of the GPR model, instead of using the traditional conjugate gradient (CG) method. Lastly, the Bayesian inference strategy is used to estimate the posterior probabilities of the test data with respect to different clusters. The various localized GPR models are integrated through these posterior probabilities as the weightings so that a global predictive model is developed for the final prediction. The effectiveness of the proposed algorithm is verified by means of a numerical example and a real industrial winding process. Statistical tests of experimental results compared with other popular prediction models demonstrate the good performance of the proposed model.																	1432-7643	1433-7479				OCT	2020	24	20					15853	15869		10.1007/s00500-020-04916-6		APR 2020											
J								Analysis of social networks and Wi-Fi networks by using the concept of picture fuzzy graphs	SOFT COMPUTING										Intuitionistic fuzzy graphs; Picture fuzzy graphs; Social networks; Wi-Fi networks	SIMILARITY MEASURES; AGGREGATION OPERATORS; DISTANCE; SETS	Atanassov's intuitionistic fuzzy set described the uncertainty of real-life events with the help of a membership and a non-membership degree. However, human opinion cannot be restricted to yes or no, but there is some abstinence and refusal degree as well. In such cases, picture fuzzy set is a suitable solution which described the abstinence and refusal grade of human opinion along with membership and non-membership grades. The aim of this paper is to analyse a social network and a wife network using the concept of picture fuzzy graph (PFG). For this purpose, the concept of PFG is proposed and some basic terms are demonstrated including complement, degree and bridges. The main advantage of the proposed PFG is that it describes the uncertainty in any real-life event with the help of four membership degrees where the traditional FG and IFG fail to be applied. The viability of PFG is shown by utilizing the concept in demonstrating two real-life problems including a social network and a Wi-Fi-network. A comparison of PFG with existing notions is established showing its superiority over the existing frameworks.																	1432-7643	1433-7479				NOV	2020	24	21					16551	16563		10.1007/s00500-020-04959-9		APR 2020											
J								CASH: correlation-aware scheduling to mitigate soft error impact on heterogeneous multicores	CONNECTION SCIENCE										Reliability; soft error; correlation; heterogeneous multicore; scheduling	RELIABILITY; ENERGY; SYSTEMS; DESIGN; CORES; POWER	With the exponential increase in the number of transistors under fast-paced technology progress, the soft error induced reliability issue is becoming even more challenging in heterogeneous multicore processor design. As there are significant opportunities to mitigate the soft error impacts through heterogeneous multicore scheduling, we show in this paper that the correlation among multiple applications exhibits important reliability characteristics, by defining a new metric to measure the system-level vulnerability factor of multiple applications and an approximate estimator to evaluate the metric fast and accurately for effective scheduling decisions. To approach these issues, we propose CASH, a Correlation-Aware Scheduling strategy to optimise heterogeneous multicore system reliability. Comprehensive simulation results demonstrate that the proposed approach is promising, achieving up to 21.4% reliability improvement with only 3.6% performance degradation when compared with performance-oriented scheduling policy.																	0954-0091	1360-0494															10.1080/09540091.2020.1758924		APR 2020											
J								Securing top-k query processing in two-tiered sensor networks	CONNECTION SCIENCE										Two-tiered wireless sensor networks; securing query processing; top-k query		Integrity and privacy are two important secure matrices in cyber security. Due to the limited resources and computing capability of the sensor nodes, it is challenging to simultaneously satisfy these two matrices for top-k querying in two-tiered sensor networks. To solve this problem, this paper proposes a weight-bind-based secure top-k query processing scheme (WBB-TQ), which utilises both the order-preserving symmetric encryption scheme (OPES) and the pairwise-key encryption technique to ensure data privacy in top-k querying. Since OPES can keep the size orders of the sensed data items unchanged before and after they are encrypted, the upper-layer storage nodes in the network can process top-k queries without knowing the exact values of the sensed data items. To guarantee the completeness of query results, we propose a novel method to establish chaining relationship among all the data items generated by each sensor node. By checking whether the relationship holds on not, Sink can find out whether adversaries drop and/or tamper with part or all of the qualified top-k data items in the query results. Theoretical analyses show that WBB-TQ can preserve data integrity and privacy of the top-k query results. Extensive simulation results further demonstrate that, WBB-TQ incurs very low computational and communication cost in securing top-k querying.																	0954-0091	1360-0494															10.1080/09540091.2020.1753173		APR 2020											
J								Projection multi-birth support vector machinea for multi-classification	APPLIED INTELLIGENCE										Multi-classification; Multi-birth support vector machine; Projection multi-birth support vector machine	IMPROVEMENTS	As an important multi-classification learning tool, multi-birth support vector machine (MBSVM) has been widely studied and applied due to its low computational complexity and good generalization. In this paper, a new multi-birth support vector machine is proposed to handle multi-class classification problem, called projection multi-birth support vector machine (PMBSVM). Specifically, we intend to seek a projection direction w(k) for k-th class, so that the covariance of remaining samples (except the k-th class) is as small as possible, and the samples of k-th class are as far as possible from the mean of the remaining samples. The proposed PMBSVM not only inherits the advantages of MBSVM, but also can find a suitable projection direction for each class so that the sample is separable in the projection space. Additionally, a regularization term is introduced to maximize the margin of different classes in the projected space. Moreover, a recursive PMBSVM algorithm is proposed for generating multiple orthogonal projection directions for each class. Then we extend the proposed approaches to nonlinear situations through kernel technology. Simulation results on benchmark datasets show that the proposed algorithms improve the generalization in most cases.																	0924-669X	1573-7497				OCT	2020	50	10					3040	3056		10.1007/s10489-020-01699-z		APR 2020											
J								Deep spatial-temporal networks for crowd flows prediction by dilated convolutions and region-shifting attention mechanism	APPLIED INTELLIGENCE										Dilated convolution; Region-level attention mechanism; Crowd flows prediction; Spatial-temporal network	NEURAL-NETWORK	Flow prediction at a citywide level is of great significance to traffic management and public safety. Since deep learning has achieved success to deal with complex nonlinear problems, it has drawn increasing attention on making crowd flows prediction through neural networks. Generally, convolutional neural network (CNN) and recurrent neural network (RNN) have been applied to model the spatial-temporal dependency of the city. However, there are still two major challenges in predicting flows. First, it is difficult to train the model with the ability to capture both the nearby and distant spatial dependency by deep local convolutions. Second, daily and weekly patterns in temporal dependency are not strictly periodic for their dynamic temporal shifting in each region. To address these issues, we propose a novel deep learning model which called Local-Dilated Region-Shifting Network (LDRSN). LDRSN combines local convolutions with dilated convolutions to learn the nearby and distant spatial dependency. Furthermore, a new region-level attention mechanism is proposed to model the temporal shifting which varies by region. In the experiments, we compare the proposed method with other state-of-the-art methods in two real-world crowd flows datasets. The Mean Absolute Error (MAE), Mean Absolute Percentage Error (MAPE), Root Mean Square Error (RMSE) were used as the evaluation indexes. The experiment results show the effectiveness of the proposed model.																	0924-669X	1573-7497				OCT	2020	50	10					3057	3070		10.1007/s10489-020-01698-0		APR 2020											
J								Sentiment classification with adversarial learning and attention mechanism	COMPUTATIONAL INTELLIGENCE										adversarial learning; attention mechanism; LSTM network; PU learning problem; sentiment classification		Sentiment classification is a key task in sentiment analysis, reviews mining, and other text mining applications. Various models have been proposed to build sentiment classifiers, but the classification performances of some existing methods are not good enough. Meanwhile, as a subproblem of sentiment classification, positive and unlabeled learning (PU learning) problem widely exists in real-world cases, but it has not been given enough attention. In this article, we aim to solve the two problems in one framework. We first build a model for traditional sentiment classification based on adversarial learning, attention mechanism, and long short-term memory (LSTM) network. We further propose an enhanced adversarial learning method to tackle PU learning problem. We conducted extensive experiments in three real-world datasets. The experimental results demonstrate that our models outperform the compared methods in both traditional sentiment classification problem and PU learning problem. Furthermore, we study the effect of our models on word embedding. Finally, we report and discuss the sensitivity of our models to parameters.																	0824-7935	1467-8640															10.1111/coin.12329		APR 2020											
J								Ontology-based prediction of cochlear implantation outcome using cross-modal plasticity analysis	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cochlear implant; EEG signal; VEP signal; Fuzzy interface system; Ontology; Fuzzy ontology	VISUAL-EVOKED POTENTIALS; AUDITORY-CORTEX; DEAF; REORGANIZATION; DEPRIVATION; INTEGRATION; CORTICES	Cochlear implantation is a surgical procedure by which an electronic medical device, namely a cochlear implant, is fitted to the individual who has the challenge of hearing. It takes the function of the damaged inner ear, the cochlea. The hearing impaired may not benefit from this procedure because of cross-modal plasticity. This work aims to study the implantee's visual modal adaptation by analyzing the visual evoked potential. The proposed methodology analyses the existence of cross-modal reorganization in the auditory cortex of bilateral prelingually deaf children after cochlear implantation using visual evoked potential. Fifty healthy, 50 prelingually, deaf children 50 cochlear implanted were considered as a cohort of the Visual evoked potential. The evoked potential recorded using pattern reversal stimulation. The amplitude and latency of N75, P100, and N145 components show a significant difference in normal, cochlear, and deaf. The early diagnosis of hearing impairment demands the patients and doctors to make a series of decisions for the betterment of the implantee in an accelerated manner through traditional database methodology implemented for making decisions, analyzing, interpreting, processing of data is so difficult in the conventional system. Medical knowledge represented for the computers to analyze the inferred data and to make the decisions. Ontology is the most potent tool to encode medical data semantically. A fuzzy ontology-based decision support system built to predict the cochlear implantation outcome. The ontology created using Protege software tool and the decision taken using Jena and pellet reasoner. A fuzzy-based prediction model designed using a fuzzy interface system to estimate the categories of auditory performance (CAP) test reliable indicators of cochlear implantation success.																	1868-5137	1868-5145															10.1007/s12652-020-02011-0		APR 2020											
J								Statistical-model based voice activity identification for human-elephant conflict mitigation	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Human-elephant; Feature extraction; Classification; SVM; PCA; Performance metrics		Human-Elephant conflict has become very common in the forest borders causing phenomenal increase in death of elephants as well as loss of human life. Elephants face a shortage of resources such as food and water, resulting in Human-Elephant Conflict. The presence of elephants can be predicted using elephant voice detection. We propose a machine learning based system for detecting elephant voice and predicting the presence of elephants in the forest border areas. Existing methods for elephant voice detection in literature require large feature data dimensions. In the proposed system, feature extraction methods combined with Principal Component Analysis (PCA) that greatly reduces feature data dimension is proposed to improve the performance metrics of the recorded elephant voice samples. A Support vector machine (SVM) classifier is used for the predictive model in this work. The proposed system is validated using the cross validation method and the performance metrics such as Accuracy, Sensitivity, Specificity, Precision, F1 Score and Computation time are evaluated. It is observed that with the proposed approach the average accuracy is 93.32% and feature data dimension is 1422 compared to an average accuracy of 83.5% obtained and feature data dimension of 18,882 with methods using Mel-Frequency Cepstral Co-efficient (MFCC)																	1868-5137	1868-5145															10.1007/s12652-020-02005-y		APR 2020											
J								Fuzzy fractional coloring of fuzzy graph with its application	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Fuzzy graph; Strongly adjacent vertices; Product of fuzzy graph; Fuzzy fractional coloring; Fuzzy fractional clique	COMPLEMENT	In this article, a new idea of fuzzy fractional coloring of fuzzy graph is presented and fuzzy fractional chromatic number is defined. A relationship between fuzzy fractional chromatic number and fuzzy fractional clique number is established. Some properties of fuzzy chromatic number of fuzzy graphs and fuzzy fractional chromatic number of fuzzy graphs are proved and the concept of k-strong adjacent vertices is introduced. Fuzzy chromatic number and fuzzy fractional chromatic number have been calculated on lexicographic product of two fuzzy graphs. Also, fuzzy chromatic number, independence number and fuzzy fractional chromatic number have been investigated on disjoint union of two fuzzy graphs. Lastly, a real life application of fuzzy fractional coloring on fuzzy graph is discussed.																	1868-5137	1868-5145															10.1007/s12652-020-01953-9		APR 2020											
J								Secured segmentation for ICD datasets	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Privacy; Accuracy; Dataset; Algorithm		Data publishing can infringe individual privacy. When there is a necessity to extract knowledge from data mining results, there should be a discipline followed by not disclosing individual information. There is an opportunity for the user to interpret specific individual information from data mining results and further misuse the interpreted results, resulting in infringement of privacy. Privacy preserving data publishing research attempts to guarantee accurate results yet preserves privacy of individual information. The proposed Secured Segmentation technique guarantees better accuracy (41.5) and privacy when dealing with record linkage and attribute linkage attacks. Secured Segmentation technique demonstrates K-anonymity to retain privacy when dealing with record linkage attacks. It also demonstrates L-diversity and t-closeness to retain privacy when dealing with attribute linkage attacks.																	1868-5137	1868-5145															10.1007/s12652-020-02009-8		APR 2020											
J								A cooperative coevolutionary method for optimizing random weight networks and its application for medical classification problems	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										RWN; Random weight network; Particle swarm optimization; PSO; Optimisation; Feature selection; Medical classification	EXTREME LEARNING-MACHINE; PARTICLE SWARM OPTIMIZATION; FEATURE-SELECTION; NEURAL-NETWORKS; ENSEMBLE; MODELS; SCHEME; PSO; ELM	Learning algorithms are mainly used to optimize a performance criterion. Random weight network (RWN) is one of learning algorithms with strong performance that used in wide range of applications. However, the performance of RWN is highly affected by the number of data inputs that is why finding the best input features becomes a necessity when dealing with high dimensional data. In literature, many methods have attempted to determine the optimal subset of features and structure of the RWN separately. In this paper, we propose a cooperative coevolution method based on Particle Swarm Optimisation. The goal of the proposed method is to optimise the structure of the RWN network and simultaneously to find the best subset of features. Three experiments are conducted on thirty medical classification datasets to assess the accuracy of the proposed method. The experimental results showed that reducing the number of features and minimising the complexity of RWN networks causes the high performance of the proposed method which exceeded all other methods in terms of accuracy on most datasets.																	1868-5137	1868-5145															10.1007/s12652-020-01975-3		APR 2020											
J								On evolutionary computation techniques for multi-view triangulation	MACHINE VISION AND APPLICATIONS										Triangulation; Parameter tuning; Evolutionary computation; 3D reconstruction	OPTIMIZATION	Multi-view triangulation is an essential step in recovering three-dimensional structure from a set of images. It is a well-studied problem in computer vision with many suboptimal and optimal methods based on different optimality criteria. In this paper, we assess the ability of evolutionary computation (EC) methods in finding highly accurate solutions to this problem. We use an overlaying Luus-Jaakola optimizer to find good parameter configurations and determine appropriate computational budget for the EC methods. Empirical results on synthetic and real data demonstrate the superior performance of EC methods over existing triangulation methods.																	0932-8092	1432-1769				APR 29	2020	31	4							29	10.1007/s00138-020-01077-2													
J								GAN-Poser: an improvised bidirectional GAN model for human motion prediction	NEURAL COMPUTING & APPLICATIONS										Human motion; GAN; Probability theory; Pose estimation; Sequence model; 3D model	NEURAL-NETWORKS; 3D	A novel method called GAN-Poser has been explored to predict human motion in less time given an input 3D human skeleton sequence based on a generator-discriminator framework. Specifically, rather than using the conventional Euclidean loss, a frame-wise geodesic loss is used for geometrically meaningful and more precise distance measurement. In this paper, we have used a bidirectional GAN framework along with a recursive prediction strategy to avoid mode-collapse and to further regularize the training. To be able to generate multiple probable human-pose sequences conditioned on a given starting sequence, a random extrinsic factor Theta has also been introduced. The discriminator is trained in order to regress the extrinsic factor Theta, which is used alongside with the intrinsic factor (encoded starting pose sequence) to generate a particular pose sequence. In spite of being in a probabilistic framework, the modified discriminator architecture allows predictions of an intermediate part of pose sequence to be used as conditioning for prediction of the latter part of the sequence. This adversarial learning-based model takes into consideration of the stochasticity, and the bidirectional setup provides a new direction to evaluate the prediction quality against a given test sequence. Our resulting novel method, GAN-Poser, achieves superior performance over the state-of-the-art deep learning approaches when evaluated on the standard NTU-RGB-D and Human3.6 M dataset.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14579	14591		10.1007/s00521-020-04941-4		APR 2020											
J								Short-term traffic flow prediction based on improved wavelet neural network	NEURAL COMPUTING & APPLICATIONS										Short-term traffic flow forecasting; Wavelet neural network; Particle swarm algorithm; Improved particle swarm optimization		Due to the characteristics of time-varying traffic and nonlinearity, the short-term traffic flow data are difficult to predict accurately. The purpose of this paper is to improve the short-term traffic flow prediction accuracy through the proposed improved wavelet neural network prediction model and provide basic data and decision support for the intelligent traffic management system. In view of the extremely strong nonlinear processing power, self-organization, self-adaptation and learning ability of wavelet neural network (WNN), this paper uses it as the basic prediction model and uses the particle swarm optimization algorithm for the slow convergence rate and local optimal problem of WNN prediction algorithm. With the advantages of fast convergence, high robustness and strong global search ability, an improved particle swarm optimization algorithm is proposed to optimize the wavelet neural network prediction model. The improved wavelet neural network is used to predict short-term traffic flow. The experimental results show that the proposed algorithm is more efficient than the WNN and PSO-WNN algorithms alone. The prediction results are more stable and more accurate. Compared with the traditional wavelet neural network, the error is reduced by 14.994%.																	0941-0643	1433-3058															10.1007/s00521-020-04932-5		APR 2020											
J								Diagnosis method of ultrasonic elasticity image of peripheral lung cancer based on genetic algorithm	NEURAL COMPUTING & APPLICATIONS										Genetic algorithm; Lung cancer; Ultrasound image; Clinical diagnosis	ENDOBRONCHIAL ULTRASOUND; ELASTOGRAPHY; CONCRETE	The current clinical diagnosis of peripheral lung cancer is affected by many factors, which leads to certain uncertainty in the diagnosis results. In order to improve the clinical diagnosis of peripheral lung cancer, based on genetic algorithm, this study constructs a proprietary model for lung cancer detection and diagnosis, designs corresponding training methods and image processing methods in the model, and outputs clinically identifiable diagnostic images for clinical analysis. In order to study the role of genetic algorithm in clinical diagnosis, a comparative trial was designed to compare the clinical diagnosis results with the HRCT method. The experimental results show that the genetic algorithm based on cross-validation optimization has good clinical results in the diagnosis and analysis of peripheral lung cancer and can provide theoretical reference for subsequent related research.																	0941-0643	1433-3058															10.1007/s00521-020-04957-w		APR 2020											
J								Camera calibration by using weighted differential evolution algorithm: a comparative study with ABC, PSO, COBIDE, DE, CS, GWO, TLBO, MVMO, FOA, LSHADE, ZHANG and BOUGUET	NEURAL COMPUTING & APPLICATIONS										Camera calibration; Weighted differential evolution; 3D point cloud; SfM photogrammetry	SELF-CALIBRATION; OPTIMIZATION ALGORITHM; RECONSTRUCTION	Camera calibration is an avoidable process for computationalvision applications, such as 3D reverse engineering, industrial robot calibration, optic-pattern recognition, simultaneous localization and mapping, autonomous visual-driving and photogrammetric vision. The camera calibration problem is too complex, nonlinear and multimodal. Traditional camera calibration methods using gradient-based optimization often trap to one of the many local solutions available. Accurate computation ability of traditional camera calibration methods is limited since they use gradient-based optimization methods. Since evolutionary computing algorithms can avoid local solutions of numerical problems, they have the potential to accurately compute the required camera calibration parameters for high-precision computationalvision applications. In this paper, the camera calibration parameters are computed by using 11 evolutionary computing algorithms, i.e., WDE, ABC, PSO, COBIDE, DE, CS, GWO, TLBO, MVMO, FOA and LSHADE. In order to make unbiased evaluation of the camera calibration results provided by the related evolutionary computing algorithms, two gradient-based traditional camera calibration methods, i.e., Zhang and Bouguet, have been used in the conducted experiments in this paper. The camera calibration results of the related methods were used to model a 3D physical test scene by using Structure from Motion photogrammetry method. The reference data set of the related 3D physical scene has been captured by using a 3D terrestrial laser scanner. Statistical comparison of the camera calibration results exposed that WDE supplies statistically better results than other comparison algorithms.																	0941-0643	1433-3058															10.1007/s00521-020-04944-1		APR 2020											
J								Autonomic cloud resource provisioning and scheduling using meta-heuristic algorithm	NEURAL COMPUTING & APPLICATIONS										Energy consumption; Resource provisioning; Resource scheduling; Meta-heuristic; Binary PSO	LOAD BALANCING ALGORITHM; GENETIC ALGORITHM; OPTIMIZATION; ENVIRONMENT; STRATEGY; TASKS	We investigate that resource provisioning and scheduling is a prominent problem due to heterogeneity as well as dispersion of cloud resources. Cloud service providers are building more and more datacenters due to demand of high computational power which is a serious threat to environment in terms of energy requirement. To overcome these issues, we need an efficient meta-heuristic technique that allocates applications among the virtual machines fairly and optimizes the quality of services (QoS) parameters to meet the end user objectives. Binary particle swarm optimization (BPSO) is used to solve real-world discrete optimization problems but simple BPSO does not provide optimal solution due to improper behavior of transfer function. To overcome this problem, we have modified transfer function of binary PSO that provides exploration and exploitation capability in better way and optimize various QoS parameters such as makespan time, energy consumption, and execution cost. The computational results demonstrate that modified transfer function-based BPSO algorithm is more efficient and outperform in comparison with other baseline algorithm over various synthetic datasets.																	0941-0643	1433-3058															10.1007/s00521-020-04955-y		APR 2020											
J								Prediction and analysis of PM2.5 in Fuling District of Chongqing by artificial neural network	NEURAL COMPUTING & APPLICATIONS										PM2; 5; Prediction; BP neural network		The meteorological data, measurements of aerosol optical depth (AOD) and PM2.5 concentration from 2016 to 2017 in Fuling District of Chongqing were selected to study their correlation. The back propagation (BP) artificial neural network (ANN) was used to build a PM2.5 prediction model with the meteorological factors as input, and the predicted PM2.5 values were compared with the measured ones. The results show that PM2.5 concentration has a piecewise linear relationship with temperature attributed to diffusion rate and premise conversion rate, a positive correlation with relative humidity, and a significant inverse correlation with wind speed, but no apparent linear relationship with rainfall, although rainfall has a significant purification effect on PM2.5. The similarity in the influence mechanism of AOD and PM2.5 concentration leads to a certain positive correlation between them. The predicted PM2.5 by the BP ANN model shows a similar trend with the measured one, but has some significant differences in numerical values. Therefore, it is feasible to establish BP artificial neural network to predict PM2.5 by using meteorological data.																	0941-0643	1433-3058															10.1007/s00521-020-04962-z		APR 2020											
J								CT image classification based on convolutional neural network	NEURAL COMPUTING & APPLICATIONS										Convolution neural network; Convolution layer; CT image classification; CDBN model	SEGMENTATION	With the rapid development of the Internet, image information is explosively growing. Traditional image classification methods are difficult to deal with huge image data and cannot meet people's requirements on the accuracy and speed of image classification. In recent years, the convolutional neural network (CNN) has been developing rapidly, and it has performed extremely well. The image classification method based on CNN breaks through the bottleneck of traditional image classification methods and becomes the mainstream image classification algorithm at present. CT image classification algorithm is one of the research hot spots in the field of medical image. The purpose of this paper is to apply convolutional neural network to CT image classification, so as to speed up CT image classification and improve the accuracy of CT image classification and so as to reduce the workload of doctors and improve work efficiency. In this paper, CT images are classified by CDBN model. Vector machine SVM is used as the feature classifier of CDBN model to enhance feature transfer and reuse so as to enrich the features. It also suppresses features that are not very useful for current tasks and improves the performance of the model. Using CDBN to classify CT images, several commonly used gray images are compared. Comparing the results of the ordinary gradient algorithm with Adam algorithm, we can get the CDBN model using Adam optimization algorithm. In CT image classification, both accuracy and speed have a good effect. The experimental results show that the training speed of CDBN model of Adam optimization algorithm in CT image classification is 3% faster than that of general gradient algorithm.																	0941-0643	1433-3058															10.1007/s00521-020-04933-4		APR 2020											
J								Hybrid Optimization Algorithms for Resource Allocation in Heterogeneous Cognitive Radio Networks	NEURAL PROCESSING LETTERS										Remote radio head; Enhanced coverage area; Heterogeneous cognitive radio network	POWER ALLOCATION; OFDMA FEMTOCELLS	In recent days, accessing the spectrum of Cognitive Radio spread has gained significance in the applications of sensor based wireless devices. In this promising environment the bottle neck existed in accessing the spectrum which makes complications in the setup on wireless networks. In each cognitive space the spread spectrum optimization to the entire user is much more needed for enjoying the fruits of cognitive Radio. To achieve this, an effective mechanism is needed for the effective utilization of spectrum in cognitive radio system. Even though there are many substantial innovations in the area of cognitive radio none of the prevailing articles have focused on providing solutions to the problems which are related to efficient allocation of spectrum for each subordinate user with less power utilization and interference. In this article, the framework has been analyzed in both presence and absence of prime users with maximizing the capacity and data rate of the network. Further, the proposed model is solved by integrating a hybrid optimization algorithm with effective decision-making mechanism. Moreover, the simulation results validate that best solution is achieved considering capacity, spectrum sharing, data rate and interference for each subordinate users in entire network.																	1370-4621	1573-773X															10.1007/s11063-020-10255-2		APR 2020											
J								Automatic method for classification of groundnut diseases using deep convolutional neural network	SOFT COMPUTING										Agriculture; Groundnut leaf disease; Convolutional neural network; Deep learning	FRUIT CLASSIFICATION; COMPUTER VISION; ENTROPY; ALGORITHM	Groundnut is one of the most important and popular oilseed foods in the agricultural field, and its botanical name is Arachis hypogaea L. Approximately, the pod of mature groundnut contains 1-5 seeds with 57% of oil and 25% of protein content. The oil obtained from the groundnut is widely used for cooking and losing body weight, and its fats are widely used for making soaps. The groundnut cultivation is affected by different kinds of diseases such as fungi, viruses, and bacteria. Hence, these diseases affect the leaf, root and stem of the groundnut plant and it leads to heavy loss in yield. Moreover, the enlarger number of diseases affects the leaf and root-like Alternaria, Pestalotiopsis, Bud necrosis, tikka, Phyllosticta, Rust, Pepper spot, Choanephora, early and late leaf spot. To overcome these issues, we introduce an efficient method of deep convolutional neural network (DCNN) because it automatically detects the important features without any human supervision. The DCNN procedure can deeply detect plant disease by using a deep learning process. Moreover, the DCNN training and testing process demonstrate an accurate groundnut disease determination and classification result. The number of groundnut leaf disease images is chosen from the plant village dataset, and it is used for the training and testing process. The stochastic gradient decent momentum method is used for dataset training, and it has shown the better performance of proposed DCNN. From the comparison analysis, the 6th combined layer of proposed DCNN delivers a 95.28% accuracy value. Ultimately, the groundnut disease classification with its overall performance of proposed DCNN provides 99.88% accuracy.																	1432-7643	1433-7479				NOV	2020	24	21					16347	16360		10.1007/s00500-020-04946-0		APR 2020											
J								Design of Muscle Reflex Control for Upright Standing Push-Recovery Based on a Series Elastic Robot Ankle Joint	FRONTIERS IN NEUROROBOTICS										muscle reflex; ankle joint; upright standing; push-recovery; series elastic actuator	BALANCE; STIFFNESS; SWAY; FEEDBACK; MOMENTS; FORCES; ENERGY	In physical human-robot interaction environment, ankle joint muscle reflex control remains significant and promising in human bipedal stance. The reflex control mechanism contains rich information of human joint dynamic behavior, which is valuable in the application of real-time decoding motion intention. Thus, investigating the human muscle reflex mechanism is not only meaningful in human physiology study but also useful for the robotic system design in the field of human-robot physical interaction. In this paper, a specialized ankle joint muscle reflex control algorithm for human upright standing push-recovery is proposed. The proposed control algorithm is composed of a proportional-derivative (PD)-like controller and a positive force controller, which are employed to mimic the human muscle stretch reflex and muscle tendon force reflex, respectively. Reflex gains are regulated by muscle activation levels of contralateral ankle muscles. The proposed method was implemented on a self-designed series elastic robot ankle joint (SERAJ), where the series elastic actuator (SEA) has the potential to mimic human muscle-tendon unit (MTU). During the push-recovery experimental study, the surface electromyography (sEMG), ankle torque, body sway angle, and velocity of each subject were recorded in the case where the SERAJ was unilaterally kneed on each subject. The experimental results indicate that the proposed muscle reflex control method can easily realize upright standing push-recovery behavior, which is analogous to the original human behavior.																	1662-5218					APR 28	2020	14								20	10.3389/fnbot.2020.00020													
J								Artificial intelligence vs COVID-19: limitations, constraints and pitfalls	AI & SOCIETY										COVID-19; Data science; AI; Surveillance; Public health	NEURAL-NETWORKS	This paper provides an early evaluation of Artificial Intelligence (AI) against COVID-19. The main areas where AI can contribute to the fight against COVID-19 are discussed. It is concluded that AI has not yet been impactful against COVID-19. Its use is hampered by a lack of data, and by too much data. Overcoming these constraints will require a careful balance between data privacy and public health, and rigorous human-AI interaction. It is unlikely that these will be addressed in time to be of much help during the present pandemic. In the meantime, extensive gathering of diagnostic data on who is infectious will be essential to save lives, train AI, and limit economic damages.																	0951-5666	1435-5655				SEP	2020	35	3					761	765		10.1007/s00146-020-00978-0		APR 2020											
J								Local keypoint-based Faster R-CNN	APPLIED INTELLIGENCE										Keypoint; SIFT; Convolutional neural network; Faster R-CNN		Region-based Convolutional Neural Network (R-CNN) detectors have achieved state-of-the-art results on various challenging benchmarks. Although R-CNN has achieved high detection performance, the research of local information in producing candidates is insufficient. In this paper, we design a Keypoint-based Faster R-CNN (K-Faster) method for object detection. K-Faster incorporates local keypoints in Faster R-CNN to improve the detection performance. In detail, a sparse descriptor, which first detects the points of interest in a given image and then samples a local patch and describes its invariant features, is first employed to produce keypoints. All 2-combinations of the produced keypoints are second selected to generate keypoint anchors, which are helpful for object detection. The heterogeneously distributed anchors are then encoded in feature maps based on their areas and center coordinates. Finally, the keypoint anchors are coupled with the anchors produced by Faster R-CNN, and the coupled anchors are used for Region Proposal Network (RPN) training. Comparison experiments are implemented on PASCAL VOC 07/12 and MS COCO. The experimental results show that our K-Faster approach not only increases the mean Average Precision (mAP) performance but also improves the positioning precision of the detected boxes.																	0924-669X	1573-7497				OCT	2020	50	10					3007	3022		10.1007/s10489-020-01665-9		APR 2020											
J								Elective future: The influence factor mining of students' graduation development based on hierarchical attention neural network model with graph	APPLIED INTELLIGENCE										Graduation development; Hierarchical attention; GCN; Elective course; Individualized interest mining	PERFORMANCE	The graduation development such as employment and graduate school admission of college students are important tasks. However, mining the factors that can affect the development of graduation remains challenging, because the most important factor "course" is not independent and inequality, which are always ignored by previous researchers. Furthermore, traditional structured methods cannot handle the complex relationships between courses, and attention networks cannot distinguish the weights of compulsory and elective courses with different distributions. Therefore, we present a Graph-based Hierarchical Attention Neural Network Model with Elective Course (GHANN-EC) for the prediction of graduation development in this study. Specifically, we use graph embedding that captures the unstructured relationships between courses and hierarchical attention that assigns the importance of the courses to excavating course information that represent students' independent interests, and can more accurately understand the relationship between graduation development and academic performance. Experimental results on the real-world datasets show that GHANN-EC outperforms the existing popular approach.																	0924-669X	1573-7497				OCT	2020	50	10					3023	3039		10.1007/s10489-020-01692-6		APR 2020											
J								Logic-based specification and verification of homogeneous dynamic multi-agent systems	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Dynamic multi-agent systems; Logics for multi-agent systems; Logics for strategic reasoning; Model-checking		We develop a logic-based framework for formal specification and algorithmic verification of homogeneous and dynamic concurrent multi-agent transition systems. Homogeneity means that all agents have the same available actions at any given state and the actions have the same effects regardless of which agents perform them. The state transitions are therefore determined only by the vector of numbers of agents performing each action and are specified symbolically, by means of conditions on these numbers definable in Presburger arithmetic. The agents are divided into controllable (by the system supervisor/controller) and uncontrollable, representing the environment or adversary. Dynamicity means that the numbers of controllable and uncontrollable agents may vary throughout the system evolution, possibly at every transition. As a language for formal specification we use a suitably extended version of Alternating-time Temporal Logic, where one can specify properties of the type "a coalition of (at least) n controllable agents can ensure against (at most) m uncontrollable agents that any possible evolution of the system satisfies a given objective ., where is specified again as a formula of that language and each of n and m is either a fixed number or a variable that can be quantified over. We provide formal semantics to our logic L HDMAS and define normal form of its formulae. We then prove that every formula in L HDMAS is equivalent in the finite to one in a normal form and develop an algorithm for global model checking of formulae in normal form in finite HDMAS models, which invokes model checking truth of Presburger formulae. We establish worst case complexity estimates for the model checking algorithm and illustrate it on a running example.																	1387-2532	1573-7454				APR 28	2020	34	2							34	10.1007/s10458-020-09457-8													
J								Intrusion detection system for cloud forensics using bayesian fuzzy clustering and optimization based SVNN	EVOLUTIONARY INTELLIGENCE										Intrusion detection; Cloud forensics; Two-level classifiers; GSO; GSA; Support vector neural network		Intrusion detection has emerged as one of the major challenges involved in the cloud forensics. This work introduces an intrusion detection framework for the cloud environment with clustering and two-level classifiers. In the first step of the process, a Bayesian fuzzy clustering is used for clustering the nodes in the cloud. And in the next step, two-level gravitational group search-based support vector neural network (GG-SVNN) classifier identifies intrusion in clusters. GG-SVNN is a novel optimization scheme proposed in this work, by combining the group search optimizer, and gravitational search algorithm. The intrusion information provided by level 1 classifier is arranged to form compact data, and provided to the level 2 classifier. The level 2 classifier finally identifies total nodes affected by the intruders. The simulation of the proposed intrusion detection is done with the help of KDD cup dataset. From the simulation results, it is evident that the proposed GG-SVNN classifier has achieved overall best performance by achieving high accuracy value of 92.41% and low false alarm rate of 4.75% respectively.																	1864-5909	1864-5917															10.1007/s12065-020-00410-y		APR 2020											
J								Towards Image-to-Video Translation: A Structure-Aware Approach via Multi-stage Generative Adversarial Networks	INTERNATIONAL JOURNAL OF COMPUTER VISION										Image-to-video translation; Video generation; Multi-stage GANs; Motion prediction; Residual learning		In this paper, we consider the problem of image-to-video translation, where one or a set of input images are translated into an output video which contains motions of a single object. Especially, we focus on predicting motions conditioned by high-level structures, such as facial expression and human pose. Recent approaches are either driven by structural conditions or temporal-based. Condition-driven approaches typically train transformation networks to generate future frames conditioned on the predicted structural sequence. Temporal-based approaches, on the other hand, have shown that short high-quality motions can be generated using 3D convolutional networks with temporal knowledge learned from massive training data. In this work, we combine the benefits of both approaches and propose a two-stage generative framework where videos are forecast from the structural sequence and then refined by temporal signals. To model motions more efficiently in the forecasting stage, we train networks with dense connections to learn residual motions between the current and future frames, which avoids learning motion-irrelevant details. To ensure temporal consistency in the refining stage, we adopt the ranking loss for adversarial training. We conduct extensive experiments on two image-to-video translation tasks: facial expression retargeting and human pose forecasting. Superior results over the state of the art on both tasks demonstrate the effectiveness of our approach.																	0920-5691	1573-1405				NOV	2020	128	10-11			SI		2514	2533		10.1007/s11263-020-01328-9		APR 2020											
J								Mapping Crisp Structural Semantic Similarity Measures to Fuzzy Context: A Generic Approach	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Similarity measure; Fuzzy ontology; Semantic reasoning; Generic model	FORMAL CONCEPT ANALYSIS; INFORMATION-CONTENT; ONTOLOGY; KNOWLEDGE	Ontology-based similarity measures have received much importance in recent years. In many real-world cases, the domain considered in the ontological similarity assessment consists of uncertainty or incomplete information. Such vagueness has led to the successful implementation of fuzzy ontology (FO)-based similarity measures. Despite various applications of FO-based similarity measures, limited methods have so far been proposed for this purpose. Accordingly, this paper presents a generic model for semantic similarity assessment based on a fuzzy ontology. The proposed approach relies on the broad literature of Crisp Ontology-based Structural Semantic Similarity Measures (CO-SSSM). It provides an approach for mapping CO-SSSMs to fuzzy context. Consequently, the proposed generic model can be applied to various CO-SSSMs to develop their corresponding FO-SSSMs. In this regard, as an empirical investigation, four of the common CO-SSSMs were selected, their equivalent FO-SSSMs were developed by means of the proposed approach, and the accuracy of their similarity assessment was compared with each other. The results show the power of FO-SSSMs in describing the relations between concepts and their superiority over CO-SSSMs.																	1562-2479	2199-3211				JUN	2020	22	4					1224	1242		10.1007/s40815-020-00833-w		APR 2020											
J								A secure IoT sensors communication in industry 4.0 using blockchain technology	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Industrial IoT; Blockchain; Industry 4; 0; Sensors security; IoT devices; Security; Wireless sensors	INTERNET; THINGS	Industrial IoT in the advancement of organizations consigns to the next level in order to trace and manage every single activity of their entities. However, the interdependence, implementation and communication among such wireless devices also known as IoT devices that lead to various secrecy and personnel concerns. Even though the use of smart sensors in industries assists and reduces human efforts with the increased quality besides of enhanced production cost. Several attacks may further encountered by various attackers by hacking several sensors/objects/devices activities. In this paper, in order to preserve transparency and secure each and every activity of smart sensors, we have proposed a secure wireless mechanism using Blockchain technology that stores extorted proceedings of each record into number of blocks. Further, the simulation results of proposed blockchain mechanism are executed against various security transmission processes. In addition, the simulated results are scrutinized besides traditional mechanism and verified over certain metrics such as Probability of attack success, ease of attack detection by the system, falsification attack, authentication delay and probabilistic scenarios to appraise the authenticity of IoT devices.																	1868-5137	1868-5145															10.1007/s12652-020-02017-8		APR 2020											
J								A hybrid swarm intelligent framework to support efficient military communication in MANET	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Optimal clustering; Cluster head selection; Military communication; Scattered nodes; Micro clustering; Macro clustering; Multiple objectives	CLUSTERING-ALGORITHM; AD	Military communication involves sharing of secret information which needs to be done with more concern to assure secured transmission of data. In the war field communication would be more difficult task where the soldiers are scattered in different location. In this case route path establishment and routing would be more difficult task due to presence of scattered soldiers in multiple locations. There is various research works has been focused to provide the efficient and better route path establishment in MANET. In the previous work, load and energy aware micro-macro density clustering approach is introduced which focus on performing clustering in two levels micro clustering and macro clustering. However this research work might degrades its clustering performance with respect to energy and time in case of more cluster count generation and also optimal route path construction is also not performed correctly. This is resolved in proposed research work by introducing new framework called multi-objective aware micro-macro clustering using hybridized additive weight based genetic algorithm (MO-MMC-HAWGA). This proposed research work leads to optimal clustering of mobile node under multiple objectives consideration like distance, energy, bandwidth, and stability. Initially it first chooses the optimal cluster head based on which optimal clustering would be done. This work improves the performance by avoiding the more number of clusters which are generated at run time by performing macro clustering. The research work/s experimental evaluation is conducted in NS2 simulation environment and it is proved that proposed research work namely MO-MMC-HAWGA provides optimal result than existing research methods.																	1868-5137	1868-5145															10.1007/s12652-020-01999-9		APR 2020											
J								ETM: Enrichment by topic modeling for automated clinical sentence classification to detect patients' disease history	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Sentence classification; Clinical sentence classification; Short text classification; Latent Dirichlet allocation; Enriched text representation	TEXT CLASSIFICATION; IDENTIFICATION; RECORDS	Given the rapid rate at which text data are being digitally gathered in the medical domain, there is growing need for automated tools that can analyze clinical notes and classify their sentences in electronic health records (EHRs). This study uses EHR texts to detect patients' disease history from clinical sentences. However, in EHRs, sentences are less topic-focused and shorter than that in general domain, which leads to the sparsity of co-occurrence patterns and the lack of semantic features. To tackle this challenge, current approaches for clinical sentence classification are dependent on external information to improve classification performance. However, this is implausible owing to a lack of universal medical dictionaries. This study proposes the ETM (enrichment by topic modeling) algorithm, based on latent Dirichlet allocation, to smoothen the semantic representations of short sentences. The ETM enriches text representation by incorporating probability distributions generated by an unsupervised algorithm into it. It considers the length of the original texts to enhance representation by using an internal knowledge acquisition procedure. When it comes to clinical predictive modeling, interpretability improves the acceptance of the model. Thus, for clinical sentence classification, the ETM approach employs an initial TFiDF (term frequency inverse document frequency) representation, where we use the support vector machine and neural network algorithms for the classification task. We conducted three sets of experiments on a data set consisting of clinical cardiovascular notes from the Netherlands to test the sentence classification performance of the proposed method in comparison with prevalent approaches. The results show that the proposed ETM approach outperformed state-of-the-art baselines.																	0925-9902	1573-7675				OCT	2020	55	2					329	349		10.1007/s10844-020-00605-w		APR 2020											
J								Unsupervised deep representation learning for motor fault diagnosis by mutual information maximization	JOURNAL OF INTELLIGENT MANUFACTURING										Motor fault diagnosis; Unsupervised deep learning; Deep representations; Mutual information	NEURAL-NETWORK; TRANSMISSION; COEFFICIENTS; DIVERGENCE	Data-driven deep learning technology has gained many achievements in the field of motor fault diagnosis and prognostics. However, the application objects of those previous studies are commonly limited to the faulty data sharing the similar distribution under unvarying stable working condition. Unfortunately, this limitation is nearly invalid in the real-world scenario, where the working condition is complicated and changes invariably, resulting in the unfavourable situation that the deep representation learning methods of the previous studies always fail in extracting the effective representations for fault diagnosis in real applications. To tackle this issue, inspired by f-divergence estimation, this work takes a different route and proposes an unsupervised deep representation learning approach, named Deep Mutual Information Maximization (DMIM), using variational divergence estimation approach to maximize mutual information (MI) between the input and output of a deep neural network. Meanwhile the representation distribution is automatically tuned by matching to a prior distribution with the same philosophy of Variational Autoencoder. Opposite to previous works which learn representations basically with supervised feedback regulation or unsupervised reconstruction, the proposed unsupervised MI maximization framework aims to make representational characteristics like independence play a bigger role to capture the most unique representations. To verify the effectiveness of our proposal, faulty motor data from the motor tests under European driving cycle for simulating the real working scenario, are collected for validation. It turns out that DMIM outperforms many popular unsupervised and fully-supervised learning methods. It opens new avenues for unsupervised learning of representations for motor fault diagnosis.																	0956-5515	1572-8145															10.1007/s10845-020-01577-y		APR 2020											
J								Intelligent manufacturing Lie Group Machine Learning: real-time and efficient inspection system based on fog computing	JOURNAL OF INTELLIGENT MANUFACTURING										Lie Group Machine Learning; Lie group intrinsic mean; Fog computing; Inspection system	DEFECT DETECTION; MODEL; IDENTIFICATION; INTERNET; DESIGN; THINGS; IOT	Due to the improvement of network infrastructure and the application of Internet of Things equipment, a large number of sensors are deployed in the industrial pipeline production, and the large size of data is generated. The most typical case in the production line is product inspection, that is, defect inspection. To implement an efficient and robust detection system, in this study, we propose a classification computing model based on Lie Group Machine Learning, which can find the possible defective products in production. Usually, a workshop has a lot of assembly lines. How to process large data on so many production lines in real-time and accurately is a difficult problem. To solve this problem, we use the concept of fog computing to design the system. By offloading the computation burden from the cloud server center to the fog nodes, the system obtains the ability to deal with extremely data. Our system has two obvious advantages. The first one is to apply Lie Group Machine Learning to fog computing environment to improve the computational efficiency and robustness of the system. The other is that without increasing any production costs, it can quickly detect products, reduce network latency, and reduce the load on bandwidth. The simulations prove that, compared with the existing methods, the proposed method has an average running efficiency increase of 52.57%, an average delay reduction of 42.13%, and an average accuracy increase of 27.86%.																	0956-5515	1572-8145															10.1007/s10845-020-01570-5		APR 2020											
J								LoPoFly: Location and Positioning Optimization for Flying Networks	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Deterministic annealing; Unmanned aerial vehicles; Facility location; Positioning; Optimization	VEHICLE BASE STATION; TARGET DETECTION; 3-D PLACEMENT; UAV; COVERAGE; PERFORMANCE	In areas that demand short-term Internet connectivity, like events and mobile offices, it is not feasible to have a permanent wireless Internet infrastructure. To provide broadband Internet access to temporary clients, we propose the use of flying networks. These networks need to be carefully managed, mainly due to the limitation on their nodes' battery capacity. Considering these issues, we introduce a new Location and Positioning Optimization Technique for Flying Networks (LoPoFly). LoPoFly includes two modules: (i) location that uses the Deterministic Annealing (DA) metaheuristic to find a location where a flying node is required based on client distribution and (ii) positioning that manages relocation and exchange of flying nodes. To the best of our knowledge, this is the first approach that employs the DA metaheuristic to manage the flying networks covering restrictions related to energy, replacement, communication, and mobility together. Through simulations, we analyze the performance of LoPoFly in two different mobility scenarios. The results show that in both scenarios, LoPoFly mitigates the number of required flying nodes, even serving a higher number of clients.																	0921-0296	1573-0409				NOV	2020	100	2					711	728		10.1007/s10846-020-01194-0		APR 2020											
J								A General Formulation for Managing Trajectory Tracking in Non-holonomic Moving Manipulators with Rotary-Sliding Joints	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Moving manipulators; Rotary-sliding joints; Gibbs-Appell methodology; Trajectory tracking; Predictive control	MODEL-PREDICTIVE CONTROL; MOBILE MANIPULATORS; ROBOTIC MANIPULATORS; VEHICLE BRAKING; DYNAMICS; SYSTEMS; ALGORITHM; EQUATIONS; MOTION; CHAIN	This article presents a general innovative technique for dynamically modeling and control of trajectory tracking in an industrial manipulator with multi-rigid links (connected by rotary-sliding (R-S) joints), which is installed on a non-holonomic moving platform on wheels. To accomplish this purpose, first the Gibbs-Appell (G-A) technique is used for getting the dynamic equations of the mentioned manipulator. Indeed, by employing the G-A methodology, one gets rid of the difficulties of Lagrange Multipliers that originate from non-holonomic constraints. To show the generality of the proposed technique, a recursive predictive control-based formulation is subsequently developed for the studied mechanism to systematically find the kinematic control rules. This multivariable kinematic controller specifies the desired angular and linear velocities of the moving base and manipulator links by finding the minimum tracking error between the system's current position and reference trajectory from a point-wise quadratic objective function. Again, according to the predictive control approach, the system's dynamic model in state space form and the desired velocities achieved from the kinematic controller are used to obtain the appropriate input torque and force controls while taking the existing uncertainties into consideration. Lastly, computer simulations are performed to emphasize that the suggested method is able to mathematically model the moving platform and end-effector of such complex robotic systems and also control their trajectory tracking.																	0921-0296	1573-0409				SEP	2020	99	3-4			SI		729	746		10.1007/s10846-019-01143-6		APR 2020											
J								A morphological approach to piecewise constant active contour model incorporated with the geodesic edge term	MACHINE VISION AND APPLICATIONS										Image segmentation; Level set; Mathematical morphology; Piecewise constant; Geodesic edge term; Curvature morphological operator	MINIMIZATION; DIFFUSION; REGIONS; ENERGY	Traditional level set-based image segmentation method has to solve the level set evolution equation which is the Euler-Lagrange equation of the energy functional defined on the image domain. Solving level set evolution equation is very time-consuming, and reinitialization is usually needed. The level set evolution equation can also be solved by mathematical morphology. The morphological implementation is very simple, fast and stable. The piecewise constant active contour model incorporated with the geodesic edge term is a hybrid active contour model which combines two active contour models which are active contour model without edges and geodesic active contour model. In this paper, the mathematical morphology-based level set evolution method is applied to the piecewise constant active contour model incorporated with the geodesic edge term. The curvature morphological operator is also improved. Experimental results show that, compared with the original piecewise constant active contour model incorporated the geodesic edge term, the new mathematical morphology-based model can segment images more accurately and there are significant gains in simplicity, speed and stability. The new mathematical morphology-based model is also compared to morphological piecewise constant active contour model, morphological geodesic active contour model, traditional piecewise constant active contour model and two other active contour models. Results show that the proposed method gets the segmentation result with faster speed and higher accuracy.																	0932-8092	1432-1769				APR 28	2020	31	4							28	10.1007/s00138-020-01083-4													
J								Graph-Theoretic Approach to Finite-Time Synchronization for Fuzzy Cohen-Grossberg Neural Networks with Mixed Delays and Discontinuous Activations	NEURAL PROCESSING LETTERS										Fuzzy Cohen-Grossberg neural networks; Distributed delays; Finite-time synchronization; Switching state-feedback control	STATIONARY DISTRIBUTION; EXPONENTIAL STABILITY; ROBUST STABILITY; INTERMITTENT CONTROL; FEEDBACK-CONTROL; COUPLED SYSTEMS; LEVY NOISE; DISCRETE; STABILIZATION; CONVERGENCE	This paper investigates finite-time synchronization for fuzzy Cohen-Grossberg neural networks (FCGNNs) with mixed delays and discontinuous activations via state-feedback control. The features of FCGNNs, discrete time delays, distributed delays and discontinuous activations are taken into account which makes our networks more general and practical in comparison with the most existing Cohen-Grossberg neural networks. Two switching state-feedback controllers designed for the implement of finite-time synchronization can be used to effectively overcome the limitations of the traditional continuous linear feedback controllers. Different from previous work, graph theory and Lyapunov method are used to study finite-time synchronization of FCGNNs for the first time in this paper, then some sufficient criteria are obtained to guarantee the finite-time synchronization of FCGNNs. In particular, it is worth noting that the settling time for finite-time synchronization is closely related to the topological structure of FCNNs. Finally, two numerical examples are given to verify the feasibility and effectiveness of the theoretical results.																	1370-4621	1573-773X				AUG	2020	52	1			SI		905	933		10.1007/s11063-020-10237-4		APR 2020											
J								LegalCap: a model for complex case discrimination based on capsule neural network	SOFT COMPUTING										LegalCap; Charge prediction; Capsule neural network; Gated recurrent unit; Intelligent court	CLASSIFICATION	In recent years, artificial intelligence, especially deep learning techniques, has provided an excellent theoretical and technical basis for the development of intelligent court. Given that case description usually contains multiple charges, charge prediction can be regarded as a task of multi-label text classification. We utilize the gated recurrent unit (GRU), a variant of recurrent neural network (RNN), to extract the chronological features of sequence. Due to the lack of retaining the positional relationship of entities when using RNN to extract features from text, a fraction of information will be lost in the context of multi-label classification. To address the drawbacks aforementioned, capsule neural network is utilized to extract the positional relation. Considering the basic and positional feature of instance, a composite model LegalCap, which combines capsule neural network with GRU, is proposed to model the complicated cases. Extensive experiments prove that the proposed model outperforms the selected baselines in the task of charge prediction. To overcome the unsatisfactory performance of directly conducting deep learning in charge prediction on the minority cases, we relieve the impact of training case imbalance by means of re-sampling. Experimental results demonstrate that after modifying the data distribution, the LegalCap model has a significant improvement on models' bias on labels, i.e., better predictions on minority cases.																	1432-7643	1433-7479				NOV	2020	24	21					16043	16055		10.1007/s00500-020-04922-8		APR 2020											
J								A forefront to machine translation technology: deployment on the cloud as a service to enhance QoS parameters	SOFT COMPUTING										Machine translation system; Amazon web server; Elastic computing unit; Quality of service	FUZZY INFERENCE SYSTEM; POS TAGGER; ANFIS	Machine translation system (MTS) constitutes of functionally heterogeneous modules for processing source language to a given target language. Deploying such an application on a stand-alone system requires much time, knowledge and complications. It even becomes more challenging for a common user to utilize such a complex application. This paper presents a MTS that has been developed using a combination of linguistic rich, rule-based and prominent neural-based approach. The proposed MTS is deployed on the cloud to offer translation as a cloud service and improve the quality of service (QoS) from a stand-alone system. It is developed on TensorFlow and deployed under the cluster of virtual machines in the Amazon web server (EC2). The significance of this paper is to demonstrate management of recurrent changes in term of corpus, domain, algorithm and rules. Further, the paper also compares the MTS as deployed on stand-alone machine and on cloud for different QoS parameters like response time, server load, CPU utilization and throughput. The experimental results assert that in the translation task, with the availability of elastic computing resources in the cloud environment, the job completion time irrespective of its size can be assured to be within a fixed time limit with high accuracy.																	1432-7643	1433-7479				NOV	2020	24	21					16057	16079		10.1007/s00500-020-04923-7		APR 2020											
J								Granular matrix method of attribute reduction in formal contexts	SOFT COMPUTING										Formal contexts; Granular reduction; Irreducible elements; Matrix method	CONCEPT LATTICE REDUCTION; KNOWLEDGE REDUCTION; DISCOVERY; ALGORITHM	Granular reduction has been of great interests for formal context analysis. From the perspective of granular computing, granular matrices are proposed to represent the extensions and intensions of formal concepts. Within this framework, irreducible elements are studied. Furthermore, similarity degree, information granular and information entropy are developed to specify the significance of attribute. In this case, heuristic approaches for granular reduct are proposed. Finally, several data sets are experimented to demonstrate the feasibility and efficiency of our method. Our methods present a new framework for granular reduction in formal concept analysis.																	1432-7643	1433-7479				NOV	2020	24	21					16303	16314		10.1007/s00500-020-04941-5		APR 2020											
J								Interaction design research based on large data rule mining and blockchain communication technology	SOFT COMPUTING										Data sharing; Rule mining; Blockchain; Communication technology; Interaction design; Data transmission	MANAGEMENT	As the amount of information in the internet of things increases, data storage management tends to be distributed, which leads to problems such as difficult data cooperation and interaction between sites, low communication efficiency, and poor reliability. Blockchain is one of the new information technologies supporting the development of management information system. It provides a solution for the storage, verification, transmission, and exchange of distributed data. To optimize the communication performance from two aspects of communication topology and communication mechanism, a multi-link and concurrent communication tree model was constructed. To improve blockchain communication technology, an interactive design method based on big data rule mining and blockchain communication technology was proposed, which mainly solved the optimization of transmission performance of blockchain data. On the basis of ensuring the stability and reliability of data transmission, the efficiency of data transmission in blockchain was further optimized, and the integrated factor communication tree algorithm (IFT) was proposed. To solve the influence of transmission delay between nodes on communication performance, a multi-link multifactor weighted communication tree algorithm (MMWT) considering weight was proposed. Moreover, to improve the efficiency of data communication, ensure the reliability of transmission, and improve the fairness of service in the blockchain, different strategies for optimizing the performance of data communication in the blockchain were proposed under the constraints of factors such as node communication ability, node trust, weight, and service request priority. Finally, the simulation results showed that MMWT algorithm had good communication performance in concurrent communication time, communication tree reliability, communication tree depth, and other aspects.																	1432-7643	1433-7479				NOV	2020	24	21					16593	16604		10.1007/s00500-020-04962-0		APR 2020											
J								On viability of detecting malwares online using ensemble classification method with performance metrics	COMPUTATIONAL INTELLIGENCE										function call; gain ratio; malware; meta classifier; support vector machine	HEALTH-CARE; ARCHITECTURES; IOT	Nowadays, most of the services from cloud are protuberant within the all commercial, public, and private areas. A primary difficulty of cloud computing system is making a virtualized environment safe from all intruders. The existing system uses signature-based methods, which cannot provide accurate detection of malware. This paper put forward an approach to detect the malware by using the approach based on feature extraction and various classification techniques. Initially the clean files and malware files are extracted. The feature selection includes gain ratio to provide subset features. The classification is used to predict any malware that has been entered in the mobile device. In this paper, it is proposed to use the ensemble classifier which contains different kinds of classifiers such as Support Vector Machine, K-Nearest Neighbor, and Naive Bayes classification. These together are known as a meta classifier. These three classification methods had been used for proposed work and get the results with higher accuracy. This measures the correctness of the prediction happened using ensemble method with high precision and recall values which is specifically identifies the quality of the techniques used.																	0824-7935	1467-8640				AUG	2020	36	3					1097	1112		10.1111/coin.12314		APR 2020											
J								Ultrasound-elastic-image-assisted diagnosis of pulmonary nodules based on genetic algorithm	NEURAL COMPUTING & APPLICATIONS										Genetic algorithm; Pulmonary nodules; Ultrasound elastic image; Auxiliary diagnosis; Improved algorithm		In the process of ultrasound elastic image detection of pulmonary nodules, due to various factors, the detection process of nodules will produce less sensitive and higher false positives, which will affect the detection accuracy of nodules. In order to improve the ultrasound-assisted diagnosis of pulmonary nodules, this study based on genetic algorithm and combined with fish-following algorithm image recognition technology to construct an improved algorithm based on genetic algorithm. In addition, this study combs and improves the algorithm through process design and sets up the simulation environment for algorithm simulation research. Finally, in order to verify the performance of the algorithm, the ultrasound image of the pulmonary nodule is analyzed by an example to obtain the processed recognition image. From the point of view of identification, the algorithm proposed in this study has certain clinical effects and can provide theoretical reference for subsequent related research.																	0941-0643	1433-3058															10.1007/s00521-020-04956-x		APR 2020											
J								Optimisation of data reliability in UASN using adaptive Buffalo algorithm	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Underwater acoustic network; Lookup table; Buffalo optimisation algorithm; Vector-based routing protocol; Depth-based routing protocol; Channel aware routing protocol; Congestion; Link failure	ROUTING PROTOCOL; UNDERWATER; AWARE	Underwater acoustic network is introduced in recent times to explore the resources available in an underwater environment. The scientific data are collected, and the data packets are transmitted to the onshore base station through the acoustic network. The underwater acoustic network faces several challenges like higher propagation delay, increased energy consumption and limited bandwidth availability due to harsh environmental condition. The most notable limitation is to forward the data packets efficiently from the underwater sensor node to the onshore sink node. Identifying energy-efficient path is not enough to achieve maximum efficiency. The Routing protocol developed for the underwater environment must overcome link failure due to congestion, harsh environment or due to energy drop out. A new optimisation protocol is developed based on biological inspiration over the characteristics of African Buffalo, and it is named as Buffalo optimisation algorithm (BOA). In underwater acoustic network, most of the energy is wasted due to the occurrence of collision in the intermediate routing nodes. And uneven traffic generation is also increasing the possibilities of occurrence of early dead nodes. To overcome all these limitations, BOA utilises lookup table based multipath communication. The algorithm is implemented in NS2 simulator, and the performance was compared with the conventional algorithms like vector based flooding protocol, depth-based forwarding protocol and channel aware routing protocol. The simulation results show throughput, packet delivery ratio increased by 10% and 12% respectively. The end-to-end delay and overhead reduced by 8% and 4% respectively. Furthermore, the test bed is developed to identify the performance of the buffalo optimisation protocol in a hardware environment. Comparative analysis of BOA with fore mentioned algorithms shows, energy consumption reduced by13% and response time improved by 3%.																	1868-5137	1868-5145															10.1007/s12652-020-02018-7		APR 2020											
J								Change detection based on tensor RPCA for longitudinal retinal fundus images	NEUROCOMPUTING										Change detection; Longitudinal images; Tensor RPCA; PG-TenRPCA; H-TenRPCA	DIABETIC-RETINOPATHY; ALGORITHMS; SUBTRACTION; ADMM	Change detection of longitudinal fundus images is an important problem in computer aided diagnosis system (CAD). Detecting regions of change in multiple fundus images from the same eye is seldom developed in the literature due to the complication and interpretability. This paper presents a longitudinal change detection framework based on tensor robust principal component analysis (RPCA) for a long retinal fundus image serial. The proposed method chooses an image of the best condition in serial as the background, then models each image as a slice of tensor, utilizes total variation to constraint the temporal continuity of change regions, finally obtains the change regions by Tucker decomposition and alternating direction method of multipliers (ADMM). Comparing with the method based on matrix RPCA, tensor RPCA preserves the original spatial structure of each image, imposes the temporal continuity on change regions and models the background by patches to avoid the little disturbance of blood vessels. Results on a real fundus image serial are presented and show the effectiveness of the proposed algorithm. (C) 2020 Published by Elsevier B.V.																	0925-2312	1872-8286				APR 28	2020	387						1	12		10.1016/j.neucom.2019.12.104													
J								Discriminative globality-locality preserving extreme learning machine for image classification	NEUROCOMPUTING										Extreme learning machine; Manifold structure; Discriminating information; Local discriminative geometry	DIMENSIONALITY REDUCTION; REGULARIZATION; INFORMATION; NETWORKS	Extreme learning machines (ELM) have been widely used in classification due to their simple theory and good generalization ability. However, there remains a major challenge: it is difficult for ELM algorithms to maintain the manifold structure and the discriminant information contained in the data. To address this issue, we propose a discriminant globality-locality preserving extreme learning machine (DGLELM) in this paper. In contrast to ELM, DGLELM not only considers the global discriminative structure of the dataset but also makes the best use of the local discriminative geometry information. DGLELM optimizes the projection direction of the ELM output weights by maximizing the inter-class dispersion and minimizing the intra-class dispersion for global and local data. Experiments on several widely used image databases validate the performance of DGLELM. The experimental results show that our approach achieves significant improvements over state-of-the-art ELM algorithms. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				APR 28	2020	387						13	21		10.1016/j.neucom.2019.09.013													
J								PAC-GAN: An effective pose augmentation scheme for unsupervised cross-view person re-identification	NEUROCOMPUTING										Cross-view person Re-Id; Pose augmentation; Generative adversarial networks; Unsupervised learning	NEURAL-NETWORKS; SIMRANK SEARCH	Person re-identification (person Re-Id) aims to retrieve the pedestrian images of the same person that captured by disjoint and non-overlapping cameras. Lots of researchers recently focused on this hot issue and proposed deep learning based methods to enhance the recognition rate in a supervised or unsupervised manner. However,there are two limitations that cannot be ignored: firstly, compared with other image retrieval benchmarks, the size of existing person Re-Id datasets is far from meeting the requirement, which cannot provide sufficient pedestrian samples for the training of deep model; secondly, the samples in existing datasets do not have sufficient human motions or postures coverage to provide more priori knowledges for learning. In this paper, we introduce a novel unsupervised pose augmentation cross-view person Re-Id scheme called PAC-GAN to overcome these limitations. We firstly present the formal definition of cross-view pose augmentation and then propose the framework of PAC-GAN that is a novel conditional generative adversarial network (CGAN) based approach to improve the performance of unsupervised corss-view person Re-Id. Specifically, the pose generation model in PAC-GAN called CPGNet is to generate enough quantity of pose-rich samples from original image and skeleton samples. The pose augmentation dataset is produced by combining the synthesized pose-rich samples with the original samples, which is fed into the corss-view person Re-Id model named Cross-GAN. Besides, we use weight-sharing strategy in the CPG-Net to improve the quality of new generated samples. To the best of our knowledge, we are the first to enhance the unsupervised cross-view person Re-Id by pose augmentation, and the results of extensive experiments show that the proposed scheme can combat the state-of-the-arts with recognition rate. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 28	2020	387						22	39		10.1016/j.neucom.2019.12.094													
J								Deterministic policy gradient adaptive dynamic programming for model-free optimal control	NEUROCOMPUTING										Adaptive dynamic programming; Deterministic policy gradient; Optimal control; Neural networks; Model-free	NONLINEAR-SYSTEMS; TRACKING CONTROL; FEEDBACK	In this paper, a deterministic policy gradient adaptive dynamic programming (DPGADP) algorithm is proposed for solving model-free optimal control problems of discrete-time nonlinear systems. By using the measured data, the developed algorithm improves the control performance with the policy gradient method. The convergence of DPGADP algorithm is demonstrated by showing that the constructed Q-function sequence is monotonically non-increasing and converges to the optimal Q-function. An actorcritic neural network (NN) structure is established to implement the DPGADP algorithm. Experience replay and target network techniques from deep Q-learning are employed during the training process. The stability of the NN weight error dynamics is also analyzed. Finally, two simulation examples are presented to verify the effectiveness of the proposed method. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 28	2020	387						40	50		10.1016/j.neucom.2019.11.032													
J								A novel statistical analysis and autoencoder driven intelligent intrusion detection approach	NEUROCOMPUTING										Anomaly detection; Deep learning; Autoencoder; Optimized features extraction; NSL-KDD database	NEURAL-NETWORK; DEEP	In the current digital era, one of the most critical and challenging issues is ensuring cybersecurity in information technology (IT) infrastructures. With significant improvements in technology, hackers have been developing ever more complex and dangerous malware attacks that make intrusion recognition a very difficult task. In this context, traditional analytical tools are facing severe challenges to detect and mitigate these threats. In this work, we introduce a novel statistical analysis and autoencoder (AE) driven intelligent intrusion detection system (IDS). Specifically, the proposed IDS combines data analytics and statistical techniques with recent advances in machine learning theory to extract more optimized, strongly correlated features. The proposed IDS is evaluated using the benchmark NSL-KDD database. Comparative experimental results show that the designed statistical analysis and AE based IDS achieves better classification performance compared to conventional deep and shallow machine learning and other recently proposed state-of-the-art techniques. Crown Copyright (C) 2019 Published by Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 28	2020	387						51	62		10.1016/j.neucom.2019.11.016													
J								Bidirectional LSTM with self-attention mechanism and multi-channel features for sentiment classification	NEUROCOMPUTING										Self-attention mechanism; Multi-channel features; Bidirectional long short-term memory; Sentiment classification	NETWORKS	There are a lot of linguistic knowledge and sentiment resources nowadays, but in the current research with deep learning framework, these kinds of unique sentiment information are not fully used in sentiment analysis tasks. Moreover, the sentiment analysis task can be seen as a sequence model, and the sequence model has a problem: the model will decode the input file sequences into a specific length vector. If the length of the vectors is set too short, the input text information will be lost, and finally the text will be misjudged. To solve these problems, we propose a bidirectional LSTM model with self-attention mechanism and multi-channel features (SAMF-BiLSTM). The method models the existing linguistic knowledge and sentiment resources in sentiment analysis tasks to form different feature channels, and uses self-attention mechanism to enhance the sentiment information. SAMF-BiLSTM model can fully exploit the relationship between target words and sentiment polarity words in a sentence, and does not rely on manually organized sentiment lexicon. In addition, we propose the SAMF-BiLSTM-D model based on SAMF-BiLSTM model for document-level text classification tasks. The method obtains the representation of all sentences in the document through SAMF-BiLSTM training, then integrates BiLSTM to learn the representation of all sentences, and further obtains the sentiment feature information of the entire document. Finally, we evaluate experiment results under five datasets. The results show that SAMF-BiLSTM and SAMF-BiLSTM-D are superior to other advanced methods in classification accuracy in most cases. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 28	2020	387						63	77		10.1016/j.neucom.2020.01.006													
J								A framework for least squares nonnegative matrix factorizations with Tikhonov regularization	NEUROCOMPUTING										Generic updating rule; Nonnegative matrix factorization; Surrogate; Tikhonov regularization	ALGORITHMS	Nonnegative matrix factorization (NMF) is widely used for dimensionality reduction, clustering and signal unmixing. This paper presents a generic model for least squares NMFs with Tikhonov regularization, which covers many well-known NMF models as well as new models. We also develop a generic updating rule with a simple structure to iteratively solve the optimization problem by constructing a surrogate function, which possesses properties similar to that of the standard NMF. The simulation results demonstrate the power of the framework in which some new algorithms can be derived to provide a performance superior to that of other commonly used methods. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 28	2020	387						78	90		10.1016/j.neucom.2019.12.103													
J								Multi-Attention Generative Adversarial Network for image captioning	NEUROCOMPUTING										Image captioning; Multi-Attention; GANs; Reinforcement learning		Recently, it has been shown that generative-adversarial-nets (GANs) can be directly utilized as an extension of traditional reinforcement-learning in image captioning tasks. However, the GANs-based methods generate captions as a function of only local points in the feature map without capturing non-local information. In this paper, a Multi-Attention mechanism is first proposed by utilizing both of the local and non-local evidence for more effective feature representation and reasoning in image captioning. Based on the mechanism, a Multi-Attention Generative Adversarial Image Captioning Network (MAGAN) is also proposed which contains a Multi-Attention generator and a Multi-Attention discriminator. The proposed generator is designed to generate more accurate sentences, while the proposed discriminator is employed to determine whether generated sentences are human described or machine generated. Extensive experiments are conducted to validate the proposed framework on MSCOCO benchmark dataset, and it achieves very competitive results evaluated by the evaluation server of MS COCO captioning challenge. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				APR 28	2020	387						91	99		10.1016/j.neucom.2019.12.073													
J								Face recognition with dense supervision	NEUROCOMPUTING										Face recognition; Dense supervision; Feature consistency; Importance sampling		Recent advances in face recognition mostly concentrate on designing more discriminative loss functions or adding normalization on features/weights to make a single feature more accurate. In this work, inspired by the frequently used multi-patch ensemble method for face recognition and part-based models for person re-identification, we propose a novel training strategy to enhance the discriminability of deeply learned feature from another perspective, namely learning with dense supervision. The main idea is to apply multiple classification losses on top of multiple component features extracted from a single network. Ideally, each component feature is expected to be accurate and have low correlation with the others. To this end, we first design a metric called feature consistency to evaluate the correlation between one component feature and the others, which is defined as the sum of distances between one component feature and the others, where the distance here is measured with KL divergence between corresponding softmax probabilities. Then we use feature consistency to select which component features to sample for one learning pass by importance sampling. The dense supervision significantly outperforms the single supervision baseline and even performs on par with its multi-patch ensemble counterpart which has much more parameters (9x). Our experimental results match state-of-the-art performance on LFW, YTF, MegaFace and surpass the others on LFW BLUFR and VGGFace2 pose protocol, thereby achieving state-of-the-art. Specially, results on VGGFace2 also show the superiority of the dense supervision on cross-pose face matching. Communicated by Dr Zhen Lei Keywords: Face recognition Dense supervision Feature consistency Importance sampling (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 28	2020	387						100	109		10.1016/j.neucom.2019.12.052													
J								Multi-view feature selection via Nonnegative Structured Graph Learning	NEUROCOMPUTING										Unsupervised multi-view feature selection; Structured graph; Pseudo label learning; Alternate optimization	RECOGNITION; MOMENTS	Graph-based solutions have achieved state-of-the-art performance on unsupervised multi-view feature selection. However, existing methods generally characterize the sample similarities first by constructing multiple fixed graphs with manually determined parameters, and then perform the feature selection on a composite one. They will suffer from two severe problems: (1) The fixed graphs may be unreliable as the raw multi-view features usually contain adverse noises and cannot accurately capture the intrinsic sample relations. (2) The graph construction and feature selection are separate and independent, the two-step learning may lead to sub-optimal performance. To tackle these problems, in this paper, we propose an effective unsupervised multi-view feature selection method, dubbed as Nonnegative Structured Graph Learning (NSGL). Specifically, we develop a unified learning framework, which directly learns the structured graph from the raw features by imposing a rank constraint, and simultaneously performs adaptive feature selection with exploiting the complementarity of multi-view features. Besides, we introduce the pseudo label learning to extract the discriminative semantic information in unsupervised scenarios and steer the graph learning process. The informative features are finally selected by forcing the feature selection matrix to be sparse in rows with sparse regression. To solve the challenging optimization problem, we first transform the formulated problem into an equivalent one that can be tackled more easily, and then develop an efficient alternate optimization algorithm guaranteed with convergence to calculate the solution iteratively. Extensive experiments on several widely tested benchmarks demonstrate the superiority of NSGL compared with several state-of-the-art approaches. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 28	2020	387						110	122		10.1016/j.neucom.2020.01.044													
J								Hopfield neural networks using Klein four-group	NEUROCOMPUTING										Hopfield neural networks; Hypercomplex number; Group theory; Multistate neuron		Complex-valued Hopfield neural networks have been extended to several high-dimensional Hopfield neural networks (HDHNNs) using hypercomplex numbers. However, the extensions by hypercomplex numbers are limited. For example, the dimensions of Clifford and Cayley-Dickson algebras are power of two. Group theory provides more various algebras since the orders of groups are any. In this work, Klein fourgroup is introduced to HDHNNs. It is an abelian group whose order is 4. A Klein Hopfield neural network employs the twin-multistate activation function. We compare the noise tolerance with that of quaternionvalued and commutative quaternion-valued twin-multistate Hopfield neural networks by computer simulations. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 28	2020	387						123	128		10.1016/j.neucom.2019.12.127													
J								Finite time anti-synchronization of complex-valued neural networks with bounded asynchronous time-varying delays	NEUROCOMPUTING										Anti-synchronization; Asynchronous; Complex-valued neural network; Finite time; Time-varying delay	GLOBAL EXPONENTIAL STABILITY; STABILIZATION	The finite time anti-synchronization of master-slave coupled complex-valued neural networks (CVNNs) with bounded asynchronous time-varying delays is studied. With the decomposing technique and generalized {xi, infinity}-norm, several criteria for ensuring the finite time anti-synchronization are obtained. The whole anti-synchronized process is divided into two phases (it is called Two-Phases-Method, 2PM): first, the norm of each error state component will flow from initial value to 1 in finite time, then flow from 1 to 0 also in finite time, thus the whole processing time is finite. Finally, a numerical example is presented to demonstrate the effectiveness of our obtained results. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 28	2020	387						129	138		10.1016/j.neucom.2020.01.035													
J								Robust stochastic configuration network multi-output modeling of molten iron quality in blast furnace ironmaking	NEUROCOMPUTING										Robust modeling; Robust stochastic configuration networks; Kernel density estimation; Multi-input multi-output modeling; Blast furnace ironmaking; Molten iron quality	FUNCTION APPROXIMATION; NEURAL-NETWORKS; REGRESSION; SYSTEM	Blast furnace ironmaking (BFI) is currently the most widely used method of pig iron smelting. In order to achieve efficient and reasonable control, how to quickly and accurately obtain the molten iron quality (MIQ) model is a key issue. Aiming at this problem, this paper applies robust stochastic configuration networks (RSCNs) based on kernel density estimation (KDE) into the BFI modeling to obtain the MIQ model with good modeling accuracy and strong robustness quickly and effectively. Firstly, the network model is incrementally constructed by adding neurons one by one using the conventional SCNs algorithm. Secondly, in order to solve the problem of insufficient robustness of conventional SCNs, kernel density estimation algorithm is introduced to obtain the corresponding probability density estimates of each training set, and it's used as the penalty weight introduced into constructing process of conventional SCNs. At the same time, the network output weight is obtained by an improved method to solve the problem that the output weight of the conventional RSCNs is abnormal in the multi-output modeling application. Finally, modeling experiments based on actual industrial data of BFI production verified that RSCNs can achieve good modeling accuracy and strong robust performance. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 28	2020	387						139	149		10.1016/j.neucom.2020.01.030													
J								Spectral-spatial classification for hyperspectral image based on a single GRU	NEUROCOMPUTING										Hyperspectral image pixel-level; classification; Deep learning; RNN; GRU	FRAMEWORK	Deep learning methods have been successfully used to extract deep features of many hyperspectral tasks. Multiple neural networks have been introduced in the classification of hyperspectral images, such as convolutional neural network (CNN) and recurrent neural network (RNN). In this study, we offer a different perspective on addressing the hyperspectral pixel-level classification task. Most existing methods utilize complex models for this task, but the efficiency of these methods is often ignored. Based on this observation, we propose an effective tiny model for spectral-spatial classification on hyperspectral images based on a single gate recurrent unit (GRU). In our approach, the core GRU can learn spectral correlation within a whole spectrum input, and the spatial information can be fused as the initial hidden state of the GRU. By this way, spectral and spatial features are calculated and expanded together in a single GRU. By comparing the different utilization patterns of RNN with a variety of spatial information fusion methods, our approach demonstrates a competitive advantage in both accuracy and efficiency. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 28	2020	387						150	160		10.1016/j.neucom.2020.01.029													
J								Recurrent convolutions of binary-constraint Cellular Neural Network for texture recognition	NEUROCOMPUTING										Cellular Neural Network; Recurrent convolution; Local binary pattern; Texture recognition; Fully-connected classifier	ROTATION-INVARIANT; CLASSIFICATION; PATTERNS; FEATURES; SCALE; CNN	Texture recognition is one of the most important branches in image research. This paper mainly aims to develop a new solution to address texture recognition using a Cellular Neural Network (CellNN). Firstly, it proposes an improved model of CellNN by the binary constraints of local receptive fields, and then designs a recurrent convolution framework of such a model to generate two types of texture feature maps, including state feature maps and output feature maps. In order to obtain low-dimensional features, state feature maps are further compressed by the mapping of rotation-invariant patterns and the merging of low-frequency occurrence patterns. By the statistics of joint-distribution patterns, state feature maps and output feature maps are fused together to generate the features of single resolution. Moreover, a multi-resolution feature combination scheme is also designed by the optimization of softmax & variance and concatenation of multiple features. Finally, a fully-connected neural network is trained to work as a texture recognizer. The experimental comparisons of totally 15 algorithms on five benchmark datasets show that, on the dataset whose texture-class quantity is not beyond 30, such as Brodatz, our method could always acquire the highest recognition accuracy, outperforming any other compared ones. On the big dataset with huge texture-class quantity, such as ALOT, our method could also surpass any other non-deep-learning one, such as the state-of-the-art gLBP, only slightly falling behind the best two deep-learning ones, FV-Alex and FV-VGGVD. However, in terms of time cost, our method could always outperform any deep-learning one in feature extraction stage, and also surpass any compared one except original LBP in feature matching. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 28	2020	387						161	171		10.1016/j.neucom.2019.12.119													
J								Locating splicing forgery by adaptive-SVD noise estimation and vicinity noise descriptor	NEUROCOMPUTING										Noise estimation; Adaptive SVD; Splicing forgery; Image forensics	TRANSFORM-DOMAIN; IMAGE; AUTHENTICATION; LOCALIZATION; SCHEME; DCT	Splicing forgery in digital images is a common form of photographic manipulation which composites two or more images into a single picture. Detection of splicing forgery remains a challenging task in image forensics. Base on the fact that images from different origins should have different amount of noise produced by image sensors or post processing by software, in this paper we proposed a novel detection method by analyzing noise discrepancy to expose and locate splicing forgery in digital images. To improve the accuracy of noise estimation, we proposed the Adaptive Singular Value Decomposition (Adaptive-SVD) to estimate the local noise. By combining local and global noise clues, we proposed the Vicinity Noise Descriptor to locate splicing forgery. Regional forensic information is inferred via machine learning method - Support Vector Machine (SVM). To evaluate the proposed method, we constructed splicing forgery databases which include various scenarios and different spliced objects with artificially added or camera-generated noise. Experimental results show that our proposed method is able to locate multi-objects spliced from different origins and comparing to state-of-the-art noise difference based methods, our method improves detection accuracy especially the value of precision, which significantly affects observers' judgement. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 28	2020	387						172	187		10.1016/j.neucom.2019.12.105													
J								Bi-Decoder Augmented Network for Neural Machine Translation	NEUROCOMPUTING										Neural Machine Translation; Bi-decoder; Denoising; Reinforcement learning		Neural Machine Translation (NMT) has become a popular technology in recent years, and the encoder-decoder framework is the mainstream among all the methods. It is obvious that the quality of the semantic representations from encoding is very crucial and can significantly affect the performance of the model. However, existing unidirectional source-to-target architectures may hardly produce a language-independent representation of the text because they rely heavily on the specific relations of the given language pairs. To alleviate this problem, in this paper, we propose a novel Bi-Decoder Augmented Network (BiDAN) for the neural machine translation task. Besides the original decoder which generates the target language sequence, we add an auxiliary decoder to generate back the source language sequence at the training time. Since each decoder transforms the representations of the input text into its corresponding language, jointly training with two target ends can make the shared encoder has the potential to produce a language-independent semantic space. We conduct extensive experiments on several NMT benchmark datasets and the results demonstrate the effectiveness of our proposed approach. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 28	2020	387						188	194		10.1016/j.neucom.2020.01.003													
J								Multi-label symbolic value partitioning through random walks	NEUROCOMPUTING										Clustering; Random walk; Symbolic value partition; Weighted graph	FEATURE-SELECTION; FEATURE-EXTRACTION; CLASSIFICATION; TRANSFORMATION	Feature selection and symbolic value partitioning are effective knowledge reduction techniques in the field of data mining. A large body of feature selection methods has been proposed for multi-label data. By contrast, symbolic value partitioning for such data has not been studied. In this paper, we propose the multi-label symbolic value partitioning through random walks algorithm with two stages. In the first stage, an undirected weighted graph is constructed for each attribute. Each node corresponds to an attribute value and the weight of each edge corresponds to the similarity between two nodes. Similarity is defined based on the attribute value distribution for each label. In the second stage, a random walk algorithm is used to cluster attribute values. The average weight serves as the separation operator to sharpen the inter-cluster edges. We tested the new algorithm and seven popular feature selection algorithms on 13 datasets. The experimental results demonstrate the effectiveness of the proposed algorithm in reducing the data size and improving classification accuracy. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 28	2020	387						195	209		10.1016/j.neucom.2020.01.046													
J								A cross-modal adaptive gated fusion generative adversarial network for RGB-D salient object detection	NEUROCOMPUTING										RGB-D salient object detection; Generative adversarial network; Cross-modal guidance; Adaptive gated fusion		Salient object detection in RGB-D images aims to identify the most attractive objects in a pair of color and depth images for the observer. As an important branch of salient object detection, it focuses on solving the following two major challenges: how to achieve cross-modal fusion that is efficient and beneficial for salient object detection; how to effectively extract the information of depth image with relatively poor quality. This paper proposes a cross-modal adaptive gated fusion generative adversarial network for RGB-D salient object detection by using color and depth images. Specifically, the generator network adopts double-stream encoder-decoder network and receives RGB and depth images at the same time. The proposed depthwise separable residual convolution module is used to deal with deep semantic information, and the processed feature is combined with side-output features of the encoder network progressively. In order to compensate for the shortcoming of poor quality of the depth image, the proposed method adds the cross-modal guidance from the side-output features of the RGB stream to the decoder network of depth stream. The discriminator network adaptively fuses the features of double streams using a gated fusion module, then sends the gated fusion saliency map to the discriminator to distinguish the similarity from ground-truth map. Adversarial learning forms the better generator network and discriminator network, and the gated fusion saliency map generated by the best generator network is served as final result. Experiments on five publicly RGB-D datasets demonstrate the effect of cross-modal fusion, depthwise separable residual convolution and adaptive gated fusion. Compared with the state-of-the-art methods, our method achieves the better performance. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 28	2020	387						210	220		10.1016/j.neucom.2020.01.045													
J								Synchronization with general decay rate for memristor-based BAM neural networks with distributed delays and discontinuous activation functions	NEUROCOMPUTING										Memristor; Mixed time delays; BAM neural networks; Discontinuous activations; General decay synchronization	FIXED-TIME SYNCHRONIZATION; FINITE-TIME; EXPONENTIAL SYNCHRONIZATION; GLOBAL CONVERGENCE; STABILITY ANALYSIS; DISCRETE; SYNAPSE; DISSIPATIVITY	In this paper, we investigate the general decay synchronization (GDS) of memristor-based BAM neural networks (MBAMNNs) with mixed time delays as well as discontinuous activations. First, the concept of the Filippov solution is extended to functional differential equations with discontinuous right-hand sides via functional differential inclusions. Then, by constructing suitable Lyapunov-krasovskii functionals and employing some useful inequality techniques, several novel conditions ensuring the GDS of considered BAM neural networks are obtained through designing two different types of nonlinear controllers. The results we proposed in this paper are general since GDS have generalized the traditional asymptotical synchronization, exponential synchronization, polynomial synchronization and contained them as its special cases. Finally, two examples and their numerical simulations are provided to demonstrate the effectiveness of our theoretical results. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 28	2020	387						221	240		10.1016/j.neucom.2019.12.124													
J								Hybrid-driven finite-time H-infinity sampling synchronization control for coupling memory complex networks with stochastic cyber attacks	NEUROCOMPUTING										Coupling memory complex networks; Finite-time synchronization; Memory sampled-data control; Stochastic cyber attacks; Hybrid-driven mechanism	SWITCHED NONLINEAR-SYSTEMS; CHAOTIC NEURAL-NETWORKS; DYNAMICAL NETWORKS; EXPONENTIAL SYNCHRONIZATION; STATE ESTIMATION; STABILIZATION; MECHANISM; SUBJECT; ROBUST	This paper proposes a new hybrid-driven (HD) sampling control strategy to investigate the finite-time (FT) H-infinity synchronization issue for complex networks with stochastic cyber attacks (SCAB) and random memory information exchanges (RMIEs). The HD control scheme with random switching memory storage which is described by independent Bernoulli variables, is employed to alleviate the burden of the network. Different from the existing work, a novel memory interconnection Lyapunov-Krasovskii functional (LKF) is structured by taking full advantage of more information of sampling interval and state, and developing some new terms. Moreover, by utilizing the integral inequality treatment techniques together with stochastic analysis methods, several optimized control algorithms (OCAS) with a larger sampling period (LSP) are established for analyzing the FT synchronization issue of homologous error system with the optimal performance index. Then, the desired controllers can be synthesized by resorting to some matrix inequalities. Finally, the chua's circuit model is presented to demonstrate the feasibility and validity of the proposed strategies. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 28	2020	387						241	254		10.1016/j.neucom.2020.01.022													
J								Tensor p-shrinkage nuclear norm for low-rank tensor completion	NEUROCOMPUTING										Low-rank tensor completion; Tensor p-shrinkage nuclear norm; t-SVD; Recovery error	DECOMPOSITION; FACTORIZATION; FRAMEWORK; RECOVERY; IMAGE	In recent times, low-rank tensor completion (LRTC), which involves completing missing entries in partially observed tensors, is attracting significant attention from researchers. Several previously proposed LRTC methods have been successfully applied to practical applications. However, while the tensor nuclear norm has been successfully applied to solve the LRTC problem, penalizing all singular values equally restricts its recovery accuracy. In this study, we propose an efficient tensor p-shrinkage nuclear norm (p-TNN) based on tensor singular value decomposition (t-SVD). Under the condition -infinity < p < 1, we theoretically prove that p-TNN outperforms traditional tensor nuclear norm when they are used to approximate the tensor average rank. Based on the proposed p-TNN, we design an LRTC model to obtain an underlying tensor from its partial observations, which provides an upper bound for recovery error. In addition, we introduce an algorithm to solve the nonconvex optimization problem that originates in our developed model and accelerate this algorithm using an adaptive momentum scheme. Theoretical analyses indicate that our algorithm enjoys a globally geometric convergence rate under the smoothness assumption. Numerical experiments conducted on both synthetic and real-world data sets verify our method and demonstrate the superiority of our proposed p-TNN in solving LRTC problems over several state-of-the-art methods (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 28	2020	387						255	267		10.1016/j.neucom.2020.01.009													
J								Feature selection for multi-label learning with streaming label	NEUROCOMPUTING										Feature selection; Multi-label learning; Streaming label; Mutual information	CLASSIFICATION; FRAMEWORK	Multi-label feature selection has drawn wide attention in recent years. The existing multi-label feature selection algorithms mainly assume that the labels of the training data are obtained before feature selection takes place. However, this assumption does not always founded because the acquisition of labeling data is costly. In real-world applications, the available labels usually arrive one by one over time. To address this problem, we develop a novel multi-label feature selection method under the circumstance of streaming label to select a set of the most relevant and discriminative features. Specifically, we firstly select label-specific features for each newly-arrived label by designing inter-class discrimination and intra-class neighbor recognition. Then, a feature conversion is created to fuse the generated label-specific feature sets. Comprehensive experiments on a series of benchmark data sets clearly demonstrate the superiority of the proposed method against other state-of-the-art multi-label feature selection methods. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 28	2020	387						268	278		10.1016/j.neucom.2020.01.005													
J								Output based transfer learning with least squares support vector machine and its application in bladder cancer prognosis	NEUROCOMPUTING										Transfer learning; Machine learning; Least squares support vector machine; Cancer prediction	RADICAL CYSTECTOMY; MORTALITY; PREDICTION; SURVIVAL	Two dilemmas frequently occur in many real-world clinical prognoses. First, the on-hand data cannot be put entirely into the existing prediction model, since the features from new data do not perfectly match those of the model. As a result, some unique features collected from the patients in the current domain of interest might be wasted. Second, the on-hand data is not sufficient enough to learn a new prediction model. To overcome these challenges, we propose an output-based transfer learning approach with least squares support vector machine (LS-SVM) to make the maximum use of the small dataset and guarantee an enhanced generalization capability. The proposed approach can learn a current domain of interest with limited samples effectively by leveraging the knowledge from the predicted outputs of the existing model in the source domain. Also, the extent of output knowledge transfer from the source domain to the current one can be automatically and rapidly determined using a proposed fast leaveone-out cross validation strategy. The proposed approach is applied to a real-world clinical dataset to predict 5-year overall and cancer-specific mortality of bladder cancer patients after radical cystectomy. The experimental results indicate that the proposed approach achieves better classification performances than the other comparative methods and has the potential to be implemented into the real-world context to deal with small data problems in cancer prediction and prognosis. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 28	2020	387						279	292		10.1016/j.neucom.2019.11.010													
J								Dynamical mechanism for conduction failure behavior of action potentials related to pain information transmission	NEUROCOMPUTING										Conduction failure of action potential; Afterpotential; Bifurcation; Potassium ion channel; Diabetic neuropathy; Current threshold	HIGH-FREQUENCY STIMULATION; NOCICEPTIVE C-FIBERS; AXONAL CONDUCTION; SPIKE PROPAGATION; I-H; NERVE-FIBERS; BLOCK; EXCITABILITY; MODULATION; HIPPOCAMPUS	Conduction of action potentials along nerve fiber is an important nonlinear phenomenon related to neural information transition. Recently, the conduction failure behavior that some action potentials with high frequency fail to conduct along C-fiber was identified to be associated with diabetic neuropathy and induced by potassium (K+) channel. In the present paper, the dynamical mechanism for downregulated expression of K+ channel induced-reduction of conduction failure, i.e. enhancement of painful information, is acquired with a chain network model composed of Hodgkin-Huxley (HH) neurons. The conduction failure behavior appears for the action potentials induced by stimulation with high frequency and small coupling strength corresponding to C-fiber, and the conduction failure degree reduces with decreasing K+ conductance, which closely matches with those observed in the biological experiment. Moreover, the dynamics of the conduction failure behavior are explained with the Hopf bifurcation of HH model. The afterpotential near the Hopf bifurcation exhibits the damping oscillation, which leads to that the current intensity threshold of a pulse stimulation to evoke an action potential from the afterpotential also manifests damping oscillations. Such a current threshold is the intrinsic cause that conduction failure behavior appears for action potential with high frequency. With decreasing K+ conductance, the current threshold becomes lower, which is the intrinsic dynamical mechanism for the reduction of conduction failure degree. By using current threshold determined by Hopf bifurcation, the results present the dynamical mechanism of potassium channel induced-conduction failure behavior, which shows that potassium channel is a potential modulation candidate for the pathological pain. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 28	2020	387						293	308		10.1016/j.neucom.2019.12.114													
J								Global-local fusion network for face super-resolution	NEUROCOMPUTING										Face hallucination; Residual enhancement; Residual fusion; Deep convolutional neural network	IMAGE SUPERRESOLUTION; HALLUCINATION; RECOGNITION; REGRESSION; FEATURES; LIMITS	Face hallucination is a domain-specific super-resolution (SR) algorithm, that generates high-resolution (HR) images from the observed low-resolution (LR) inputs. Recently, deep convolutional neural network (CNN) based SR offers an end-to-end solution for learning the complex relationship between LR and HR images, and achieves superior performance. However, most of them ignore the role of high-frequency (HF) information in image recovering. We design a novel global-local fused network (GLFSR) to refine HF information for recovering fine details of facial images. In contrast to existing methods that often increase the depth of network, we enhance the residual HF information from local to global levels through the networks. The proposed global-local fused network involves four sub-modules: first, reconstruction network, which is used to super-resolve the synthetic HR image from pixel level by reconstruction network, local and global residual enhancement networks, which generate residual information for learning; and fusion module, which is used to generate the final HR image. Experimental results on CAS-PEAL-R1 and CASIA-Webface databases demonstrate that GLFSR is superior to other state-of-the-art deep learning approaches. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 28	2020	387						309	320		10.1016/j.neucom.2020.01.015													
J								Design of affinity-aware encoding by embedding graph centrality for graph classification	NEUROCOMPUTING										Graph centrality; Affinity-aware encoding; Separate encoding function; Graph classification	NEURAL-NETWORKS; REPRESENTATION	Deep learning methods for graph classification are critical for graph data mining. Recently, graph convolutional networks (GCNs) have been able to achieve state-of-the-art node classification. A typical process for GCNs includes two iterative steps: node feature encoding and message passing. While the former encodes each graph node independently via the uniform encoding function, the latter updates the features of each node by weighted aggregation of the features of neighboring nodes, where the weights are generated by predefined or learned graph Laplacian. However, their accuracy deteriorates for graph classification tasks because the uniform encoding function encodes all the node features involved. In this study, we propose a novel affinity-aware encoding for graph classification. In our model, we implement a separate encoding function for the neighboring nodes of each node for updating the node features, where the nodes are arranged in the order of affinity values, such as graph centrality, in order to determine the correspondence between an encoding function and a specific neighboring node. Our separate encoding function performs non-Euclidean neighboring encoding for each node by weight sharing, which enables message passing. We also develop two variants based on our separate encoding function: the graph centrality-convolutional neural network (C-CNN) and the graph centrality-graph convolutional network (C-GCN). The former performs the separate encoding function on graph data directly by the function of message passing. The latter combines the separate encoding function with the normalized graph Laplacian implemented on the graph data. Experiments demonstrate that the results obtained by our models are consistent with those obtained by classical convolutional neural networks (CNNs) on the MNIST dataset, and they outperform existing GCNs on the 20NEWS, Reuters8, and Reuters52 datasets. We also apply our two variants to online car-hailing service data for traffic congestion recognition. Our methods show state-of-the-art results compared with GCNs. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 28	2020	387						321	333		10.1016/j.neucom.2020.01.010													
J								Blind image deblurring via hybrid deep priors modeling	NEUROCOMPUTING										Image processing; Blind image deblurring; Kernel estimation; Hybrid deep priors; Residual network	DECONVOLUTION	Blind image deblurring is a challenging low-level vision problem which aims to restore a sharp image only from the blurry observation. Few known information makes this problem fundamentally ill-posed. Most recent works focus on designing various priors on both latent image and blur kernel based on the maximum a posteriori (MAP) model to restrict the solution space. However, their performance is highly related to these hand-crafted explicit priors. In fact, the pre-designed explicit priors may have less flexibility to fit different image structures in real-world scenarios. To overcome these difficulties, we propose a novel framework, named Hybrid Deep Priors Model (HDPM), to simulate the propagation of sharp latent image used in kernel estimation and final deconvolution. Specifically, we introduce the learnable implicit deep prior and hand-crafted explicit prior as regularizations into the MAP inference process to extract the detailed texture and sharp structures of latent image, respectively. In HDPM, we can successfully take the advantages of explicit cues based on task information and implicit deep priors from training data to facilitate the propagation of sharp latent image which is beneficial for the kernel estimation. Extensive experiments demonstrate that the proposed method performs favorably against the state-of-the-art deblurring methods on benchmarks, challenging scenarios and non-uniform images. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 28	2020	387						334	345		10.1016/j.neucom.2020.01.004													
J								Two-layer distributed formation-containment control of multiple Euler-Lagrange systems with unknown control directions	NEUROCOMPUTING										Distributed formation containment control; Nussbaum gain technique; Neutral networks	NONLINEAR MULTIAGENT SYSTEMS; LEADER-FOLLOWING CONSENSUS; TRACKING CONTROL; NETWORK; COORDINATION; DELAYS	This paper addresses the distributed formation containment control problem of Euler-Lagrangian systems(ELSs) with unknown control directions by two layers, namely, time-varying formation for the leaders and containment formation control for the followers. We first estimate the desired states and velocities for the leaders and the followers in finite time. Second, novel adaptive neutral networks are used to compensate the model uncertainties. Third, using the Nussbaum gain technique, the proposed control laws can guarantee the ultimate convergence of error signals to zero and that all of the signals in the closed-loop system are bounded. Finally, simulation examples are given to demonstrate the effectiveness of the proposed method. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 28	2020	387						359	368		10.1016/j.neucom.2020.01.033													
J								Deep gaze pooling: Inferring and visually decoding search intents from human gaze fixations	NEUROCOMPUTING										Gaze pooling; Visual search; Deep learning; Mental image; Visual search target prediction; Visual search target reconstruction	CATEGORY	Predicting the target of visual search from human eye fixations (gaze) is a difficult problem with many applications, e.g. in human-computer interaction. While previous work has focused on predicting specific search target instances, we propose the first approach to predict categories and attributes of search intents from gaze data and to visually reconstruct plausible targets. However, state-of-the-art models for categorical recognition, in general, require large amounts of training data, which is prohibitive for gaze data. To address this challenge, we further propose a novel Gaze Pooling Layer that combines gaze information with visual representations from Deep Learning approaches. Our scheme incorporates both spatial and temporal aspects of human gaze behavior as well as the appearance of the fixated locations. We propose an experimental setup and novel dataset and demonstrate the effectiveness of our method for gaze-based search target prediction and reconstruction. We highlight several practical advantages of our approach, such as compatibility with existing architectures, no need for gaze training data, and robustness to noise from common gaze sources. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 28	2020	387						369	382		10.1016/j.neucom.2020.01.028													
J								Aerial Load Transportation with Multiple Quadrotors Based on a Kinematic Controller and a Neural SMC Dynamic Compensation	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Cooperative aerial transportation; Aerial formation control; Dynamic compensation; Neural adaptive SMC	TRAJECTORY TRACKING; CIVIL APPLICATIONS; VEHICLES; UAVS	A novel formation control to transport a cable-suspended payload with two quadrotors is presented. The control structure is based on a layered scheme combining a kinematic null-space based controller and a neural sliding mode controller. The null-space controller is designed to generate velocity references to the quadrotors in the formation, whereas the neural sliding mode controller receives such reference velocities and performs a dynamic compensation for possible parametric uncertainties as well as for the dynamic perturbations caused by the load attached to the quadrotors. The stability of the closed-loop control system thus implemented is also proven with basis on the theory of Lyapunov. Very detailed dynamic models for the quadrotors, the flexible cables, and the payload are included in a highly realistic scenario. To close the work, numerical simulations are presented, whose results demonstrate a good performance of the proposed controller.																	0921-0296	1573-0409				NOV	2020	100	2					519	530		10.1007/s10846-020-01195-z		APR 2020											
J								Salient object detection based on distribution-edge guidance and iterative Bayesian optimization	APPLIED INTELLIGENCE										Background scatter; Contour completeness; Iterative Bayesian; Salient object detection	DIFFUSION PROCESS; REGION DETECTION	Salient object detection has witnessed rapid progress, despite most existing methods still struggling in complex scenes, unfortunately. In this paper, we propose an efficient framework for salient object detection based on distribution-edge guidance and iterative Bayesian optimization. By considering color, spatial, and edge information, a discriminative metric is first constructed to measure the similarity between different regions. Next, boundary prior embedded with background scatter distribution is utilized to yield the boundary contrast map, and then a contour completeness map is derived through a wholly closed shape of the object. Finally, the above both maps are jointly integrated into an iterative Bayesian optimization framework to obtain the final saliency map. Results from an extensive number of experimentations demonstrate that the promising performance of the proposed algorithm against the state-of-the-art saliency detection methods in terms of different evaluation metrics on several benchmark datasets.																	0924-669X	1573-7497				OCT	2020	50	10					2977	2990		10.1007/s10489-020-01691-7		APR 2020											
J								Research on image classification method based on convolutional neural network	NEURAL COMPUTING & APPLICATIONS										Convolutional neural network; Image classification; Activation function; Improved algorithm		Image classification method is currently the more popular image technology, but it still has certain problems in practice. In order to improve the image classification effect, this study proposes a new convolution kernel, which can effectively detect the corresponding features with different transformations by actively transforming the relative positions of the connections in the convolution kernel. Moreover, in a network, replacing a traditional convolution kernel with a complex convolution kernel can significantly improve network performance. In order to verify the performance of the image classification method proposed in this study, the performance comparison of the algorithm was performed by setting a control experiment. The research results show that the proposed method has certain effects and can provide theoretical reference for subsequent related research.																	0941-0643	1433-3058															10.1007/s00521-020-04930-7		APR 2020											
J								An online self-organizing algorithm for feedforward neural network	NEURAL COMPUTING & APPLICATIONS										Feedforward neural network; Self-organizing algorithm; Online gradient method; Local sensitivity analysis	SEQUENTIAL LEARNING ALGORITHM; CONSTRUCTIVE ALGORITHM; SENSITIVITY-ANALYSIS; GRADIENT-METHOD; SINGLE; CONVERGENCE	Feedforward neural network (FNN) is the most popular network model, and the appropriate structure and learning algorithms are the key of its performance. This paper proposes an online self-organizing algorithm for feedforward neural network (OSNN) with a single hidden layer. The proposed OSNN optimizes the structure of FNN for time-varying system including structure design and parameter learning. In structure design, this paper measures the contribution ratios of hidden nodes by local sensitivity analysis based on differentiation method. OSNN merges hidden nodes with the others that have the highest correlation when their contribution ratios are almost zero and adds new hidden nodes by error reparation. For parameter learning, an improved online gradient method (OGM), called online gradient method with fixed memory (FMOGM), is proposed to improve the convergence speed and accuracy of OGM. In addition, this paper calculates the contribution ratios and the network error and estimates the local minima by using the fixed-sized training set of FMOGM instead of one sample at the current time, which can obtain more effective local information and a compact network structure. Finally, the proposed OSNN is verified using a number of benchmark problems and a practical problem for biochemical oxygen demand prediction in wastewater treatment. The experimental results show that OSNN has better convergence speed and accuracy than other algorithms.																	0941-0643	1433-3058															10.1007/s00521-020-04907-6		APR 2020											
J								Research on vehicle intelligent wireless location algorithm based on convolutional neural network	NEURAL COMPUTING & APPLICATIONS										Convolutional neural network; Vehicle positioning; Intelligence; Wireless positioning; Algorithm improvement		Vehicle positioning and vehicle identification of natural scene images are an important part of intelligent transportation systems and unmanned driving research. In current situation, there are still some problems in vehicle intelligent wireless positioning. In order to improve the intelligent wireless positioning efficiency of vehicles, based on the convolutional neural network, this research combines the concept of deep learning to carry out algorithm innovation in the research. Moreover, this paper combines the actual vehicle positioning problem points to collect data, simulates the vehicle positioning situation in a variety of complex situations, and designs a controlled test to verify. The results show that the algorithm of this study has certain effects, which can provide reference for subsequent related research and has certain practical significance.																	0941-0643	1433-3058															10.1007/s00521-020-04911-w		APR 2020											
J								Multi objective programming problem in the hesitant fuzzy environment	APPLIED INTELLIGENCE										Multi-objective programming; Hesitant fuzzy sets; Hesitant fuzzy Pareto optimal solution	DECISION-MAKING; OPTIMIZATION; SETS; INFORMATION; OPERATORS	In this paper, we introduce a hesitant fuzzy multi-objective programming problem, in which the evaluation information provided by the decision makers is expressed in a hesitant fuzzy environment. For this purpose a new solution concept, namely hesitant fuzzy Pareto optimal solution to the problem is introduced, and two methods are proposed to obtain it. Then it is shown that the optimal solutions of these methods are the hesitant fuzzy Pareto optimal solutions. Finally, these methods are implemented on some illustrative examples and comparative analysis of our methodology is taken with other extensions of fuzzy sets.																	0924-669X	1573-7497				OCT	2020	50	10					2991	3006		10.1007/s10489-020-01682-8		APR 2020											
J								Optimal action sequence generation for assistive agents in fixed horizon tasks	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Assistive agents; Sequential decision-making; Human-robot interaction; Autism spectrum disorder	HINT GENERATION	Agents providing assistance to humans are faced with the challenge of automatically adjusting the level of assistance to ensure optimal performance. In this work, we argue that identifying the right level of assistance consists in balancing positive assistance outcomes and some (domain-dependent) measure of cost associated with assistive actions. Towards this goal, we contribute a general mathematical framework for structured tasks where an agent playing the role of a 'provider'-e.g., therapist, teacher-assists a human 'receiver'-e.g., patient, student. We specifically consider tasks where the provider agent needs to plan a sequence of actions over a fixed time horizon, where actions are organized along a hierarchy with increasing success probabilities, and some associated costs. The goal of the provider is to achieve a success with the lowest expected cost possible. We present OAssistMe, an algorithm that generates cost-optimal action sequences given the action parameters, and investigate several extensions of it, motivated by different potential application domains. We provide an analysis of the algorithms, including proofs for a number of properties of optimal solutions that, we show, align with typical human provider strategies. Finally, we instantiate our theoretical framework in the context of robot-assisted therapy tasks for children with Autism Spectrum Disorder (ASD). In this context, we present methods for determining action parameters based on a survey of domain experts and real child-robot interaction data. Our contributions unlock increased levels of flexibility for agents introduced in a variety of assistive contexts.																	1387-2532	1573-7454				APR 27	2020	34	2							33	10.1007/s10458-020-09458-7													
J								Fast and improved backpropagation learning of multi-layer artificial neural network using adaptive activation function	EXPERT SYSTEMS										adaptive learning activation function based classification and nonlinear system identification; artificial neural network; classification; improved fast BP algorithm; mean absolute deviation; sigmoid functions; root mean square error	ALGORITHM; CONVERGENCE; PERCEPTRON	In the conventional backpropagation (BP) learning algorithm used for the training of the connecting weights of the artificial neural network (ANN), a fixed slope-based sigmoidal activation function is used. This limitation leads to slower training of the network because only the weights of different layers are adjusted using the conventional BP algorithm. To accelerate the rate of convergence during the training phase of the ANN, in addition to updates of weights, the slope of the sigmoid function associated with artificial neuron can also be adjusted by using a newly developed learning rule. To achieve this objective, in this paper, new BP learning rules for slope adjustment of the activation function associated with the neurons have been derived. The combined rules both for connecting weights and slopes of sigmoid functions are then applied to the ANN structure to achieve faster training. In addition, two benchmark problems: classification and nonlinear system identification are solved using the trained ANN. The results of simulation-based experiments demonstrate that, in general, the proposed new BP learning rules for slope and weight adjustments of ANN provide superior convergence performance during the training phase as well as improved performance in terms of root mean square error and mean absolute deviation for classification and nonlinear system identification problems.																	0266-4720	1468-0394				OCT	2020	37	5			SI				e12555	10.1111/exsy.12555		APR 2020											
J								Solving predictive control problem of fast-varying multivariable systems by incorporating unknown active dynamics generated by real-time adaptive learning machine	EXPERT SYSTEMS										fuzzy wavelet neural networks; intelligent feedbacks; sliding predictive control; real-time model generation; suboptimal predictive control; switched systems	SWITCHED NONLINEAR-SYSTEMS; LINEAR-SYSTEMS; IDENTIFICATION; MPC	Not only adaptive predictive control of switched systems is a computationally intensive procedure, it also involves various challenges in addressing the problems of robust stabilization and precise tracking. This study proposes new strategies to deal with the aforementioned issues (namely safe and precise control alongside with reduction of computational burden). The first contribution of this work is reduction of conservatism for described class of systems. Control of switched systems with undetectable switching signals is often conducted in worst case switching configuration to ensure robustness, which potentially results in conservative design. The issue of conservativeness is intensified in multi input-multi output (MIMO) dynamical systems due to increased dimensions. However, attaining a robust control scheme for all switching configurations while ensuring precise response is inherently paradoxical. To overcome this issue, this study proposes a new dual-mode algorithm where control modes corresponding to safety and precision are activated at appropriate stages of system response. This is conducted based on incorporation of an adaptive fuzzy-wavelet neural network identification scheme in predictive control of MIMO switched systems. However, as convergence of the adaptive algorithm to actual system is attained after a finite period of time, a safe-mode control algorithm is proposed to maintain quality of transient response in convergence period. In other words, the proposed algorithm operates in safe and precise control modes to ensure robust stability in the convergence period and non-conservative design in steady-state. Second major contribution of the work is reduction of calculation burden based on incorporation of a suboptimal control algorithm. To this end, we propose a predictive control scheme based on a suboptimal gradient-descent based controller, calculating feasible stabilizing inputs instead of optimal inputs. Effects of dynamical variations are incorporated in the model predictive control framework for increased compatibility with high-speed switching dynamics. Then, based on incorporation of dual-mode algorithm, precise steady-state performance is attained while preventing notable perturbations in dynamical discontinuities at switching.																	0266-4720	1468-0394				OCT	2020	37	5			SI				e12567	10.1111/exsy.12567		APR 2020											
J								Bayesian Method for the Generalized Exponential Model Using Fuzzy Data	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Generalized exponential distribution; Imprecise information; Analysis of fuzzy data; Bayesian procedure	STATISTICAL-INFERENCE; WEIBULL DISTRIBUTION; LINDLEY DISTRIBUTION; RECORD VALUES; PARAMETERS	This paper focuses on Bayesian inference for the parameters of the generalized exponential model under asymmetric and symmetric loss functions when the observations are described in terms of fuzzy numbers. First, a generalized likelihood function based on fuzzy data is derived. Then, considering general entropy, linear exponential and squared error loss functions, the Bayes estimates of the parameters are obtained. Since Bayes estimates could not be expressed in closed forms, Metropolis-Hasting samplers are used to compute the approximate Bayes estimates. For comparison purposes, the maximum likelihood estimates of the parameters are also computed. The proposed inferences are illustrated using three real-world examples. The numerical simulation results demonstrate the superiority of the Bayesian method over the maximum likelihood procedure.																	1562-2479	2199-3211				JUN	2020	22	4					1243	1260		10.1007/s40815-020-00843-8		APR 2020											
J								A novel intelligent construction method of individual portraits for WeChat users for future academic networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Future academic network; WeChat user portraits; Intelligent construction methods; Accurate knowledge services	SEMANTIC SEGMENTATION; CLASSIFICATION; ALGORITHM	The services provided by the academic WeChat public account in the future academic network need to transform from traditional knowledge services to innovative precise knowledge services. The construction of user portraits is a prerequisite for understanding user knowledge needs and developing accurate knowledge services. Academic WeChat user portraits can analyze and express changes in user needs and preference habits, and solve the problem of asymmetry between users' precise knowledge needs and academic WeChat extensive knowledge services. In view of this, this article conducts research on the issue of academic WeChat user portraits. Data collection was first carried out using questionnaires. Second, perform statistical analysis on the characteristics of the samples and extract classification labels. Finally, the K-means clustering algorithm was used to divide WeChat users into three types: initial introduction participation users, resource acquisition users, and mature users. The experimental results show that the main groups of academic WeChat users are young students, teachers and researchers. The frequency of access to WeChat public accounts by resource acquisition users and mature users is significantly higher than that of initial participation users. Participating users and mature users were introduced in the early stage to pay more attention to the dynamics and freshness of the frontiers of the subject, while resource-acquiring users paid more attention to the subject's basic knowledge and thesis writing methods.																	1868-5137	1868-5145															10.1007/s12652-020-02004-z		APR 2020											
J								MT-IVSN: a novel model for vehicle re-identification	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Vehicle re-identification; Vehicle retrieval; Viewpoint normalization of vehicle; Construction of color feature vector		Image re-identification is usually used to find specific images from image libraries or video sequences. In recent years, convolutional neural networks have gradually become the dominant method in this field. In this paper, a Multitask Identification-Verification Siamese Network for vehicles is proposed, which combines color feature vectors with the inherent structure, category and other attributes of the vehicle. A vehicle 2D structural model and a method of image viewpoint normalization are also presented to ensure the high consistency of features in the expression of images from different angles. In addition, based on the vehicle 2D structural model, a dynamic annular non-uniform partition color super-pixel sampling strategy for vehicle face is investigated to construct a color feature vector. In the experiments, the proposed model and method are evaluated on the public vehicles and VeRi datasets. The experimental results show that the proposed method and model have made great progress.																	1868-5137	1868-5145															10.1007/s12652-020-01988-y		APR 2020											
J								Guangdong-Hong Kong-Macao Greater Bay Area public goods supply governance research based on data mining algorithms	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Data mining; Guangdong-Hong Kong-Macao Greater Bay Area; Public goods supply; Governance		Building the Guangdong-Hong Kong-Macao Greater Bay Area is conducive to promoting the high economic integration of Guangdong, Hong Kong and Macao, building a new open economic system, and carrying out dual exploration of politics and economy, which is of great strategic significance. Public governance is not only a substitute for the traditional public administration paradigm, but also a product of interdisciplinary research and multidisciplinary development of social science. Its greatest feature is that it is a transcendence of traditional government domination theory. Close integration of economic decentralization and vertical political management system is the core connotation of Chinese-style decentralization. In terms of fiscal system, China is among the countries with the highest degree of decentralization in the world. Starting from the concept of public goods and based on data mining algorithm, this paper expounds the basic theory of public goods supply mode and the factors affecting the supply of public goods. On the basis of clear theory, this paper probes into the characteristics and existing problems of the traditional supply mode of public goods in China, which has positive significance for improving the quantity and quality of the supply of public goods in Guangdong-Hong Kong-Macao Greater Bay Area.																	1868-5137	1868-5145															10.1007/s12652-020-02019-6		APR 2020											
J								TACMA: total annual cost minimization algorithm for optimal sizing of hybrid energy systems	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Unit sizing; Stand-alone system; Renewable energy sources; Energy storage system; Meta-heuristic algorithms; Optimization	STORAGE-SYSTEM; OPTIMAL ALLOCATION; BATTERY SYSTEMS; OPTIMAL-DESIGN; OPTIMIZATION; WIND; PV; FEASIBILITY; HOME; ELECTRIFICATION	In a stand-alone environment, a system comprising of non-renewable source, renewable energy sources (RESs), and energy storage systems like fuel cells (FCs) provide an effective and reliable solution to fulfill the user's load. In this paper, a diesel generator (DG), photovoltaics (PVs), wind turbines (WTs) and FCs are modeled, optimally sized, and compared in three scenarios: PV-WT-FC-DG, PV-FC-DG, and WT-FC-DG in terms of environmental emission and total annual cost (TAC) for a home, located in Hawksbay, Pakistan. The optimal size of hybrid RESs and their components is achieved using a novel TAC minimization algorithm (TACMA). The TACMA achieves superior results in terms of TAC when it is compared to two algorithm-specific parameter-less schemes: Jaya and teaching learning-based optimization. Further, the PV-WT-FC-DG and PV-FC-DG hybrid systems are found as the most economical and nature-friendly scenarios, respectively.																	1868-5137	1868-5145															10.1007/s12652-020-01964-6		APR 2020											
J								Application of human body gesture recognition algorithm based on deep learning in non-contact human body measurement	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Attitude estimation; Deep learning; Human parameters; Remote measurement	REPRESENTATIONS	For image-based non-contact anthropometrics, a multi-task learning network for compliance check of images to be measured in human body parameters is proposed. The basis of PyraNet pyramid stacking hourglass network for human key prediction tasks is completed. Join the goal detection idea of CornerNet to realize the construction of multi-task learning network that obtains the above two kinds of information at the same time. The network can simultaneously predict the position information of the human body and various key points of the human body; the network is designed and tested in detail. Based on the results of the above network, the design and implementation of the image compliance judgment algorithm was carried out.																	1868-5137	1868-5145															10.1007/s12652-020-01993-1		APR 2020											
J								Permeability prediction of petroleum reservoirs using stochastic gradient boosting regression	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Permeability prediction; Petroleum reservoirs; Machine learning techniques; Stochastic gradient boosting (SGB); Random forest (RF)	BCI-COMPETITION-III; TIME-SERIES; ENSEMBLE; CLASSIFICATION; PERFORMANCE; MODELS	Reservoir permeability is a crucial parameter for reservoir characterization and the estimation of current and future production from hydrocarbon reservoirs. Permeability can be conventionally estimated from traditional approaches such as core analysis and well-test data, which are time-consuming and expensive. Many scientists tried to estimate permeability from nuclear magnetic resonance (NMR) logs utilizing complex mathematical equations that may achieve imprecise approximation of the permeability values. Gradient boosting generates additive regression models by successively fitting a straightforward base learner to present pseudo-residuals using least squares at every iteration. The execution speed and accuracy of gradient boosting can be considerably enhanced by applying randomization process. Besides, the randomization process improves robustness against over fitting of the base learner. So, the novelty of the current study is the development of a Stochastic Gradient Boosting (SGB) regression model to predict the permeability of petroleum reservoirs based on well logs. Besides benchmark machine learning techniques are used to predict reservoir permeability from well logs. The correlation coefficient (R), relative absolute error (RAE), mean-absolute error (MAE), root mean-squared error (RMSE), and root relative squared error (RRSE) are utilized to check the overall fitting between measured permeability versus predicted ones. It is found that Stochastic Gradient Boosting model achieved overall better performance than the other models.																	1868-5137	1868-5145															10.1007/s12652-020-01986-0		APR 2020											
J								Model selection challenges with application to multivariate calibration updating methods	JOURNAL OF CHEMOMETRICS										model updating; model quality measures; bias-variance trade-off; U-curve; spectral similarity measure	PARAMETER SELECTION; RANKING DIFFERENCES; REGRESSION; VALIDATION; SUM	An important issue in multivariate calibration including model updating methods with multiple tuning parameters is selection of final models. Model updating is an adaption process where models are updated from predicting in primary sample and measurement conditions to predict the analyte in new secondary conditions. A single process to select models (tuning parameter values) with satisfactory bias-variance trade-offs across multiple data sets and modeling methods is challenging. This paper reports on evaluating the consistency of a collection of model quality measures to select models across five near-infrared (NIR) data sets for three calibration updating approaches. The goal is to formulate a reliable model selection process that is nearly data and model updating method independent. Two of the three model updating approaches require primary and secondary analyte reference values, and the third only needs primary reference values (unlabeled relative to secondary). However, all model selection methods considered do require secondary samples with reference values. It is found that which evaluated model quality measure to use depends on the degree of spectral similarity between primary and secondary spectra as characterized by the indicator of spectral uniqueness measure developed in this paper. From the results presented, more work is needed to better characterize model selection dependency on model quality measures, number of samples and respective inherent compositions (data set-dependent matrix effects), and tuning parameter ranges.																	0886-9383	1099-128X				JUL	2020	34	7			SI				e3245	10.1002/cem.3245		APR 2020											
J								Multiple player tracking in basketball court videos	JOURNAL OF REAL-TIME IMAGE PROCESSING										Smart basketball court system; Monitoring scene; Real time; Multiple object tracking		To build a smart basketball court, one basic task is to track players with the aid of the basketball court monitoring. This task can be regarded as a special case of the multiple object tracking (MOT) problem. But different from it, the task puts request to the existing MOT methods with both good accuracy and operational efficiency (toward real time) in the new basketball scenario. To deal with this task, we make the following attempts: (1) Considering the differences between pedestrians and basketball players and the lack of corresponding dataset for basketball players tracking, we construct a new MOT dataset under the basketball court monitoring scene, to better evaluate MOT methods in our task and to help promote future research in the related task, (2) evaluating the performance of the several candidate MOT methods on the new dataset, and (3) proposing the issues to be further addressed in this specific scenario.																	1861-8200	1861-8219															10.1007/s11554-020-00968-x		APR 2020											
J								Optimized highway deep learning network for fast single image super-resolution reconstruction	JOURNAL OF REAL-TIME IMAGE PROCESSING										Single image super-resolution; Highway connection; Residual learning; Gating mechanism	SALIENCY DETECTION; RESOLUTION	With the success of the deep residual network for image recognition tasks, the residual connection or skip connection has been widely used in deep learning models for various vision tasks, including single image super-resolution (SISR). Most existing SISR approaches pay particular attention to residual learning, while few studies investigate highway connection for SISR. Although skip connection can help to alleviate the vanishing gradient problem and enable fast training of the deep network, it still provides the coarse level of approximation in both forward and backward propagation paths and thus challenging to recover high-frequency details. To address this issue, we propose a novel model for SISR by using highway connection (HNSR), which composes of a nonlinear gating mechanism to further regulate the information. By using the global residual learning and replacing all local residual learning with designed gate unit in highway connection, HNSR has the capability of efficiently learning different hierarchical features and recovering much more details in image reconstruction. The experimental results have validated that HNSR can provide not only improved quality but also less prone to a few common problems during training. Besides, the more robust and efficient model is suitable for implementation in real-time and mobile systems.																	1861-8200	1861-8219															10.1007/s11554-020-00973-0		APR 2020											
J								Coverage-based query subtopic diversification leveraging semantic relevance	KNOWLEDGE AND INFORMATION SYSTEMS										Subtopic mining; Relatedness; Sentence embedding; Coverage-based diversification		Generally, users are reserved in describing their search intention when submitting queries into the search engine. Therefore, a large number of search queries are usually short, ambiguous and tend to have multiple interpretations. With the gigantic size of the web, ignoring the information needs underlying such queries can misguide the search engine. To mitigate these issues, an effective approach is to diversify the search results considering the query subtopics with diverse intents. The task of identifying possible subtopics with diverse intents underlying a query is known as subtopic mining. This paper is aimed at mining and diversifying subtopics underlying a query. Our method first exacts noun phrases containing the query terms from the top-retrieved web documents. We also extract query suggestions and completions from commercial search engines. The extracted candidates highly related to the query are then selected as subtopics. We introduce a new relatedness score function to estimate the degree of relatedness between the query and the candidate. To estimate the relevancy between the query and the subtopic, this paper introduces a semantic relevance measure using a locally trained sentence embedding model. Finally, we propose a novel coverage-based diversification technique to rank the subtopics combining their relevancy and the coverage estimated by the web documents. The experimental results on two NTCIR English subtopic mining datasets demonstrate that our proposed method achieves new state-of-the-art performance and significantly outperforms some known related methods in terms of relevance (D-nDCG) and diversity (D#-nDCG) metric at cut of 10.																	0219-1377	0219-3116				JUL	2020	62	7					2873	2891		10.1007/s10115-020-01470-3		APR 2020											
J								Robust classification via MOM minimization	MACHINE LEARNING										Robust machine learning; Empirical process; Vapnik; Gradient descent	EMPIRICAL RISK MINIMIZATION; HALFSPACES	We present an extension of Chervonenkis and Vapnik's classical empirical risk minimization (ERM) where the empirical risk is replaced by a median-of-means (MOM) estimator of the risk. The resulting new estimators are called MOM minimizers. While ERM is sensitive to corruption of the dataset for many classical loss functions used in classification, we show that MOM minimizers behave well in theory, in the sense that it achieves Vapnik's (slow) rates of convergence under weak assumptions: the functions in the hypothesis class are only required to have a finite second moment and some outliers may also have corrupted the dataset. We propose algorithms, inspired by MOM minimizers, which may be interpreted as MOM version of block stochastic gradient descent (BSGD). The key point of these algorithms is that the block of data onto which a descent step is performed is chosen according to its " centrality" among the other blocks. This choice of " descent block" makes these algorithms robust to outliers; also, this is the only extra step added to classical BSGD algorithms. As a consequence, classical BSGD algorithms can be easily turn into robust MOM versions. Moreover, MOM algorithms perform a smart subsampling which may help to reduce substantially time computations and memory resources when applied to non linear algorithms. These empirical performances are illustrated on both simulated and real datasets.																	0885-6125	1573-0565				AUG	2020	109	8					1635	1665		10.1007/s10994-019-05863-6		APR 2020											
J								Parameter selection framework for stereo correspondence	MACHINE VISION AND APPLICATIONS										Parameter selection; Stereo matching; Evolutionary algorithm	OPTIMIZATION	In this paper, we propose a method to select parameter values for stereo matching methods. The proposed method was trained in a supervised manner, and an evolutionary algorithm is used to select optimized parameter values for a given domain and a cost function constructed to measure the goodness level of candidate parameter values. Performance of the proposed method is compared to that of five current stereo matching methods, including the efficient large-scale stereo matching, belief propagation, semi-global block matching, stereo matching by training a convolutional neural network to compare image patches, and the efficient deep learning for stereo matching, for KITTI 2012, KITTI 2015, Middlebury, and EISAT datasets. The optimized parameters improve accuracy for all stereo matching methods considered, with some cases of improvement reaching up to 24%. Source code and experimental results are available online.																	0932-8092	1432-1769				APR 27	2020	31	4							27	10.1007/s00138-020-01076-3													
J								Search space reduction of asynchrony immune cellular automata	NATURAL COMPUTING										Cellular automata; Cryptography; Asynchrony immunity; Correlation immunity; Nonlinearity; Side-channel attacks; Permutivity		We continue the study of asynchrony immunity in cellular automata (CA), which can be considered as a generalization of correlation immunity in the case of vectorial Boolean functions. The property could have applications as a countermeasure for side-channel attacks in CA-based cryptographic primitives, such as S-boxes and pseudorandom number generators. We first give some theoretical results on the properties that a CA rule must satisfy in order to meet asynchrony immunity, like central permutivity. Next, we perform an exhaustive search of all asynchrony immune CA rules of neighborhood size up to 5, leveraging on the discovered theoretical properties to greatly reduce the size of the search space.																	1567-7818	1572-9796				JUN	2020	19	2			SI		287	293		10.1007/s11047-020-09788-1		APR 2020											
J								ETIP: a lengthy nested NER problem for Chinese insurance policy analysis	PATTERN ANALYSIS AND APPLICATIONS										Information extraction; Convolutional neural network; Heuristic method; F1-score optimization		Contract analysis can significantly ease the work for humans using AI techniques. This paper shows a lengthy nested NER problem of element tagging on insurance policy (ETIP). Compared to NER, ETIP deals with not only different types of entities which vary from a short phrase to a long sentence, but also phrase or clause entities that could be nested. We present a novel hybrid framework of deep learning and heuristic filtering method to recognize the lengthy nested elements. First, a convolutional neural network is constructed to obtain good initial candidates of sliding windows with high softmax probability. Then, the concatenation operator on adjacent candidate segments is introduced to create phrase, clause, or sentence candidates. We design an effective voting strategy to resolve the classification conflict of the concatenated candidates and present a theoretical proof of F1-score optimization. In experiments, we have collected a large Chinese insurance contract dataset to test the performance of the proposed method. An extensive set of experiments is performed to investigate how sliding window candidates can work effectively in our filtering and voting strategy. The optimal parameters are determined by statistical analysis of the experimental data. The results show the promising performance of our method in the ETIP problem.																	1433-7541	1433-755X				NOV	2020	23	4					1755	1765		10.1007/s10044-020-00885-6		APR 2020											
J								Analysis on the construction of ideological and political education system for college students based on mobile artificial intelligence terminal	SOFT COMPUTING										BP algorithm; AI; Ideological and political education; System construction		With the development of modern mobile communication, artificial intelligence (AI) has begun to enter people's life, and it is also constantly changing the modern education mode. First, the structure of BPN (back-propagation network) is designed in this paper. On this basis, genetic algorithm is applied to optimize, which can accelerate the convergence speed and achieve the effect of global optimization. The results obtained from the research also meet the expected value, realizing the purpose of optimizing the BP algorithm. This paper analyzes the AI teaching expert system, summarizes their functions and characteristics, and points out that the college students' ideological and political teaching system based on the mobile AI terminal can be used as the teaching manager, teaching assistant, and even as the teaching object to guide students' learning. At the same time, the article also points out that the application and development direction of artificial intelligence in the teaching field can be divided into three stages: primary application, intermediate application, and advanced application, so as to provide theoretical guidance for the construction and analysis of ideological and political teaching system for college students using mobile artificial intelligence terminals.																	1432-7643	1433-7479				JUN	2020	24	11			SI		8365	8375		10.1007/s00500-020-04932-6		APR 2020											
J								Laser-induced breakdown spectroscopy applied to the rapid identification of different types of polyethylene used for toy manufacturing	JOURNAL OF CHEMOMETRICS										LIBS; PCA; plastics; polyethylene; recycling; toy	RESTABILIZATION TECHNIQUE; LIBS; CLASSIFICATION; POLYMERS; WASTE; DISCRIMINATION; PLASTICS	The identification of plastics is of great importance to recycling companies, as the littering of plastic wastes has rapidly increased because of the extensive use of plastics. To maintain the economics of recycling extremely large volumes of waste materials, rapid and accurate identification of these plastics is crucial. In this study, we demonstrate the efficacy of laser-induced breakdown spectroscopy (LIBS) in the rapid identification of high-density polyethylene (HDPE) and low-density polyethylene (LDPE) used for toy manufacturing. For data analysis of the LIBS spectra, multivariate data analysis using principal component analysis (PCA) was utilized. The analyses of the data clearly showed that the elemental and molecular information obtained from LIBS is effective in the identification of three types of polyethylene. The proposed method was successfully applied and can be used as an alternative to the traditional methods used for analyses of plastics.																	0886-9383	1099-128X														e3248	10.1002/cem.3248		APR 2020											
J								Enhanced constrained application protocol for secured medical data transmission model for internet of things	COMPUTATIONAL INTELLIGENCE										CoAP; DTLS; ECC; internet of things; TLS; transmission	AUTHENTICATION; SYSTEM	The medical data produced from various sensors of sensitive information so there is a need to secure them through security solutions. At present, many security solutions are available in the market to secure the medical data. Transport layer security (TLS) is designed to transfer the client from the client to the server in a more secure manner. The major advantage of using TLS is it will not lose any data while transferring the message from client to server. Datagram transport layer security (DTLS) is designed specifically to use in constrained networks. The DTLS protocol consists of major components like base protocol, record layer, and handshake protocol. The main disadvantage of DTLS is the attacker can send multiple Hello messages to the server. This can cause an attack against the server and this attack encourages a new connection between the client and server and also it increases required bandwidth and resources for every message. The main objective of the proposed work is to use the constrained application protocol (CoAP) with DTLS in place of user datagram protocol (UDP) for the secure transmission of data. To evaluate the efficiency of the system, the packet loss ratio, data transmission time, and handshake time are calculated.																	0824-7935	1467-8640															10.1111/coin.12321		APR 2020											
J								A synergy Thompson sampling hyper-heuristic for the feature selection problem	COMPUTATIONAL INTELLIGENCE										classification; combinatorial optimization; feature selection; hyper-heuristics; machine learning; Thompson sampling	SUPPORT VECTOR MACHINE; ALGORITHMS; ENSEMBLE	To classify high-dimensional data, feature selection plays a key role to eliminate irrelevant attributes and enhance the classification accuracy and efficiency. Since feature selection is an NP-Hard problem, many heuristics and metaheuristics have been used to tackle in practice this problem. In this article, we propose a novel approach that consists in a probabilistic selection hyper-heuristic called the synergy Thompson sampling hyper-heuristic. The Thompson sampling selection strategy is a probabilistic reinforcement learning mechanism to assess the behavior of the low-level heuristics, and to predict which one will be more efficient at each point during the search process. The proposed hyper-heuristic is combined with a 1 nearest neighbor classifier from the Weka framework. It aims to find the best subset of features that maximizes the classification accuracy rate. Experimental results show a good performance in favor of the proposed method when comparing with other existing approaches.																	0824-7935	1467-8640															10.1111/coin.12325		APR 2020											
J								The importance of public support in the implementation of green transportation in the smart cities	COMPUTATIONAL INTELLIGENCE										green transportation; public support; smart city; smart mobility	MODEL	Smart cities are increasingly evolving, initializing new strategies, and programs that have a significant influence on policy making and scheduling while coexisting with urban facilities. To recognize urban planning offers to a smarter city context, it is now necessary to understand the contribution of the smart city in overall urban planning and vice versa. Currently, transportation has been seen as a connection to all aspects of life across the world. This article presents the results of a survey testing into whether the public supports the concept of green transportation in the smart cities. The green urban mobility model has been proposed to investigate urban traffic information to characterize important features of smart mobility in the smart cities. The development of intelligent transportation systems using the proposed model that makes traffic easier in the city to transport safe and more comfortable. The experimental results suggest the required factors of green transportation and realistic behavior of smart mobility.																	0824-7935	1467-8640															10.1111/coin.12326		APR 2020											
J								A survey on intention analysis: successful approaches and open challenges	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Intention analysis; Social media; Machine learning; Natural language processing; Text Mining		Intention Analysis is a computational task that analyzes people's desires, wishes, and attitudes from user-generated texts. This sub-field of text mining has recently attracted research interest. This research paper provides an overview and an analysis of the latest studies in this field. These studies were categorized and summarized according to their contributions and the techniques they used. Several proposed approaches and some real applications were investigated in depth and presented in detail. Moreover, some related fields to intention analysis such as Transfer Learning (TL), Spam Detection (SD), and Building Resources (BR) were discussed in this survey of the literature dedicated to Intention Analysis. The aim of this survey is to give a comprehensive view of the intention analysis field supported by a number of graphics and summary tables about the literature. The paper concludes by identifying a number of research topics that can be promising for future research.																	0925-9902	1573-7675				DEC	2020	55	3					423	443		10.1007/s10844-020-00604-x		APR 2020											
J								Artificial virtue: the machine question and perceptions of moral character in artificial moral agents	AI & SOCIETY										Machine ethics; Virtue ethics; Artificial intelligence; Robot rights; Agents; Moral psychology	INTENTIONAL ACTION; MIND PERCEPTION; ATTRIBUTIONS; INTELLIGENCE; IMPRESSIONS; COMPUTER; PEOPLE	Virtue ethics seems to be a promising moral theory for understanding and interpreting the development and behavior of artificial moral agents. Virtuous artificial agents would blur traditional distinctions between different sorts of moral machines and could make a claim to membership in the moral community. Accordingly, we investigate the "machine question" by studying whether virtue or vice can be attributed to artificial intelligence; that is, are people willing to judge machines as possessing moral character? An experiment describes situations where either human or AI agents engage in virtuous or vicious behavior and experiment participants then judge their level of virtue or vice. The scenarios represent different virtue ethics domains of truth, justice, fear, wealth, and honor. Quantitative and qualitative analyses show that moral attributions are weakened for AIs compared to humans, and the reasoning and explanations for the attributions are varied and more complex. On "relational" views of membership in the moral community, virtuous machines would indeed be included, even if they are indeed weakened. Hence, while our moral relationships with artificial agents may be of the same types, they may yet remain substantively different than our relationships to human beings.																	0951-5666	1435-5655															10.1007/s00146-020-00977-1		APR 2020											
J								A new parallel fuzzy data envelopment analysis model for parallel systems with two components based on Stackelberg game theory	FUZZY OPTIMIZATION AND DECISION MAKING										Fuzzy data; Data envelopment analysis; Parallel system; Stackelberg game theory	DECISION-MAKING; MULTICOMPONENT EFFICIENCY; SHARED INPUTS; DEA; PERFORMANCE; DECOMPOSITION; SALES	This paper investigates the problem of efficiency measurement for parallel systems with two components based on Stackelberg game theory, while some inputs/outputs are fuzzy numbers. Conventional DEA models treat DMUs as "Black Boxes". While in this paper, we propose a new parallel fuzzy DEA model to calculate the efficiency scores for each DMU's whole system and its sub-systems. Through the Stackelberg (leader-follower) game theory, the whole system's efficiency score of each DMU is decomposed into a set of efficiency scores for its sub-systems. This approach is independent of the alpha-cut which reduces the computational efforts. In order to show our method, we use the data from Beasley (J Oper Res Soc 46(4):441-452, 1995) to measure the fuzzy efficiency of the teaching and research efficiencies of chemistry departments in UK universities.																	1568-4539	1573-2908				SEP	2020	19	3					311	332		10.1007/s10700-020-09320-1		APR 2020											
J								Object detection using Metaheuristic algorithm for volley ball sports application	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Object detection; Performance metrics; Firefly algorithm; TLBO algorithm; Cuckoo; Search algorithm	OPTIMIZATION	Object Detection has been a great challenge over the years. The reason behind is that, it is applied for numerous real time applications like vision based control, traffic control, video surveillance, sports analysis, etc. But, object detection in a video sequence is a highly challenging task. It has various problems like occlusion, fast moving objects, shadow, poor lighting, color contrast and other static background objects. This reason brought the object detection to be a thrust research area in the field of image processing. In the previous researches the conventional methods of object detection like Frame Difference, Mixture of Gaussian (MoG), Optical Flow etc., still have the above problems. Hence the research focuses on a different approach in object detection using Metaheuristic algorithm for the video sequence of volley ball player in the practice session. In this research three Metaheuristic Algorithms, namely Firefly, Teaching and Learning Based Optimization (TLBO) and Cuckoo Search Algorithm are used. These algorithms are evaluated and compared with the parameters like accuracy, precision, and recall. The result shows Cuckoo Search Algorithm is best suited to object detection especially in this application.																	1868-5137	1868-5145															10.1007/s12652-020-01981-5		APR 2020											
J								Surface EMG signal classification using TQWT, Bagging and Boosting for hand movement recognition	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Prosthetic hand control; Surface electromyography (sEMG); Multi-scale principle component analysis (MSPCA); Tunable Q wavelet transform (TQWT); Ensemble classifiers; Bagging; Boosting	BCI-COMPETITION-III; PATTERN-RECOGNITION; FEATURE-EXTRACTION; ENSEMBLE; ELECTROMYOGRAPHY; WAVELET; INTERFACE; SYSTEM; ROBUST; SOFT	Hands play a significant role in grasping and manipulating different objects. The loss of even a single hand have impact on the human activity. In this regard, a prosthetic hand is an appealing solution for the subjects who lost their hands. The surface electromyogram (sEMG) plays a vital role in the design of prosthesis hands. The ensemble classifiers achieve better performance by using a weighted combination of several classifier models. Hence, in this paper, the feasibility of the Bagging and the Boosting ensemble classifiers is assessed for the basic hand movement recognition by using sEMG signals, which were recorded during the grasping movements with various objects for the six hand motions. So, the novelty of the current study is the development of an ensemble model for hand movement recognition based on the tunable Q-factor wavelet transform (TQWT). The proposed method consists of three steps. In the first step, MSPCA is used for denoising. In the second step, a novel feature extraction method, TQWT is used for feature extraction from the sEMG signals, then, statistical values of TQWT sub-bands are calculated. In the last step, the obtained feature set is used as input to an ensemble classifier for the identification of intended hand movements. Performances of the Bagging and the Boosting ensemble classifiers are compared in terms of different performance measures. Using TQWT extracted features along with the presented the Adaboost with SVM and the Multiboost with SVM classifier results in a classification accuracy up to 100%. Hence, the results have shown that the proposed framework has achieved overall better performance and it is a potential candidate for the prosthetic hands control.																	1868-5137	1868-5145															10.1007/s12652-020-01980-6		APR 2020											
J								High-Precision Visual-Tracking using the IMM Algorithm and Discrete GPI Observers (IMM-DGPIO) Categories (4)(7)	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										High-precision; Visual tracking; Real-time; GPI observer; Interacting multiple models		In this work, we propose the integration of a bank of Discrete Generalized Proportional Integral Observers (DGPIO) within an Interacting Multiple Model (IMM) structure in order to improve the precision of visual-tracking tasks. Applications such as visual servoing, robotic assisted surgery and optronic weapon systems require accurate and high-precision measurements provided by real-time visual-tracking systems. In this case, the DGPIO-Bank was designed using two kinematic models based in constant velocity (CV) and constant acceleration (CA) motion profiles. The main feature of the DGPIO-Bank is the active disturbance rejection (ADR) feature which reduces noise in the position signal of a moving object. The resultant algorithm uses a fusion of four important features: state interaction, Kalman filtering, active disturbance rejection and multiple models combination. For performance comparison, we evaluated our proposed IMM-DGPIO algorithm and other state of the art IMM algorithms. Experimental results show that our proposed strategy had the best performance.																	0921-0296	1573-0409				SEP	2020	99	3-4			SI		815	835		10.1007/s10846-020-01164-6		APR 2020											
J								V3O2: hybrid deep learning model for hyperspectral image classification using vanilla-3D and octave-2D convolution	JOURNAL OF REAL-TIME IMAGE PROCESSING										HSI classification; Octave convolution; Convolutional neural network; Deep learning; Feature extraction	FRAMEWORK; NETWORK	Remote sensing image analysis is an emerging area of research and is used for various applications such as climate analysis, crop monitoring and change detection. Hyperspectral image (HSI) is one of the dominant remote sensing imaging modalities that captures information beyond the visible spectrum. The evolution of deep learning has made a significant impact on HSI analysis, mainly for its classification. The spatial-spectral feature-based classification model improves the classification accuracy of hyperspectral images (HSIs). However, these models are computationally expensive, and redundancy exists in the spatial dimension of features. This research work proposes a hybrid convolutional neural network (CNN) for HSI classification. The proposed model uses principal component analysis (PCA) as a preprocessing technique for optimal band extraction from HSIs. The hybrid CNN classification technique extracts the spectral and spatial features using three-dimensional CNN (3D CNN). These features are fed into a two-dimensional CNN (2D CNN) for further feature extraction and classification. The redundancy in spatial features of the hybrid CNN model is reduced by octave convolution (OctConv) instead of standard vanilla convolution. OctConv factorizes the spatial features into lower and higher spatial frequencies, and different convolutions are performed on them based on their frequencies. The hybrid model is compared against various state-of-the-art CNN-based techniques and found that the accuracy is boosted with a lesser computational cost.																	1861-8200	1861-8219															10.1007/s11554-020-00966-z		APR 2020											
J								Moral Gridworlds: A Theoretical Proposal for Modeling Artificial Moral Cognition	MINDS AND MACHINES										Artificial intelligence; Moral AI; Moral cognition; Machine ethics; Moral psychology; Reinforcement learning; Fairness	DECISION-MAKING; ULTIMATUM GAME; AUTONOMOUS VEHICLES; FAIRNESS; ETHICS; AI; MECHANISMS; EVOLUTION; REPRESENTATIONS; ARCHITECTURE	I describe a suite of reinforcement learning environments in which artificial agents learn to value and respond to moral content and contexts. I illustrate the core principles of the framework by characterizing one such environment, or "gridworld," in which an agent learns to trade-off between monetary profit and fair dealing, as applied in a standard behavioral economic paradigm. I then highlight the core technical and philosophical advantages of the learning approach for modeling moral cognition, and for addressing the so-called value alignment problem in AI.																	0924-6495	1572-8641				JUN	2020	30	2					219	246		10.1007/s11023-020-09524-9		APR 2020											
J								Improved convolutional neural network in remote sensing image classification	NEURAL COMPUTING & APPLICATIONS										Convolutional neural network; Algorithm improvement; Remote sensing image; Image classification; Image recognition		The classification of land cover is the first step in the analysis and application of remote sensing data in land resources. How to solve the multi-category image recognition and meet certain precision is a key issue in remote sensing image research, which has very important theoretical significance and practical application value. In this study, the algorithm is improved on the basis of convolutional neural network, and experiments are carried out on multi-source remote sensing images with different geomorphologies taken under three different weather conditions to verify the effectiveness and scalability of the improved convolutional neural network. The research results show that the improved algorithm proposed in this paper has certain results in remote sensing image classification and can provide theoretical reference for subsequent related research.																	0941-0643	1433-3058															10.1007/s00521-020-04931-6		APR 2020											
J								Multi-objective optimal medical data informatics standardization and processing technique for telemedicine via machine learning approach	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Telemedicine; Channel optimization; Multi-objective clustering; Medical data processing; Information processing	SYSTEM	Telemedicine is a blooming field with inter-disciplinary research and wide area of application refinement. Various techniques are proposed since last decade with a primary focus of improving telemedicine. The algorithms are either data dependent or application centric. In this paper, a multi-objective optimal medical (MooM) data processing technique is proposed under multi-dimensional data types of medial samples such as text files, image files, log files, electronic health records (EHR), audio signal files and graphic files. The technique proposes a dedicated methodology for independent data-type processing to retrieve on a standard protocol platform for transmission of data via telemedicine channel. The technique uses unsupervised and hybrid clustering approaches of machine learning to predict data-types attributes for processing, thus resulting in higher-order accuracy and data scalability on transmission channel of telemedicine environment. The MooM technique processed on medical images retrieve the compressed stream of data frames with QoS recorded 9.23, for medical textural data the QoS is 9.87 and audio signal pattern data, the QoS is recorded 9.76 on a scale of 10.																	1868-5137	1868-5145															10.1007/s12652-020-02016-9		APR 2020											
J								Deep neural networks to predict diabetic retinopathy	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Diabetic retinopathy; Deep neural networks; Dimensionality reduction; Principal component analysis; Grey Wolf Optimizer (GWO); Deep learning	PREVALENCE; VALIDATION; ALGORITHM	Diabetic retinopathy is a prominent cause of blindness among elderly people and has become a global medical problem over the last few decades. There are several scientific and medical approaches to screen and detect this disease, but most of the detection is done using retinal fungal imaging. The present study uses principal component analysis based deep neural network model using Grey Wolf Optimization (GWO) algorithm to classify the extracted features of diabetic retinopathy dataset. The use of GWO enables to choose optimal parameters for training the DNN model. The steps involved in this paper include standardization of the diabetic retinopathy dataset using a standardscaler normalization method, followed by dimensionality reduction using PCA, then choosing of optimal hyper parameters by GWO and finally training of the dataset using a DNN model. The proposed model is evaluated based on the performance measures namely accuracy, recall, sensitivity and specificity. The model is further compared with the traditional machine learning algorithms-support vector machine (SVM), Naive Bayes Classifier, Decision Tree and XGBoost. The results show that the proposed model offers better performance compared to the aforementioned algorithms.																	1868-5137	1868-5145															10.1007/s12652-020-01963-7		APR 2020											
J								Generation of maximum power in PV system using EHO based embedded controller	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										PV array; MPPT technique; Embedded controller; EHO algorithm; DC; DC converter; XSG	FUZZY-LOGIC CONTROLLER; MPPT METHOD; CONTROL DESIGN; PERFORMANCE; ALGORITHM; IMPLEMENTATION; SIMULATION; PERTURB	Photovoltaic (PV) framework's current-voltage qualities are nonlinear, since, the solar temperature and the resulting randomness in the light of the natural elements. PV for power generation maximum power point tracking (MPPT) is utilized to promote the ability of the setup. The elephant herding optimization (EHO) calculation is introduced by means of Xilinx system generator (XSG) based embedded controller, for get-together maximal power from the PV. Effectuated optimizing is done by focusing on the gain parameters of the proportional integral derivative (PID) using an EHO-based embedded controller to govern the boost converter switching pulse. Through the XSG domain and by validating the proposed approach, the embedded controller was developed; also the switching schemes are tested for the PV supported boost converter. The principal intention of this proposed approach is the maximal power is extricated from PV agglomeration subject to cell temperature and solar radiation in distinction numerous natural situation. Through the PV framework, the predictive framework model is built to the Matlab/Simulink stage and with converter switching signals and was designed utilizing the Xilinx domain interconnected to them. At last, the activity of proposed framework is analyzed with other existence controller.																	1868-5137	1868-5145															10.1007/s12652-020-01970-8		APR 2020											
J								Real-time image carrier generation based on generative adversarial network and fast object detection	JOURNAL OF REAL-TIME IMAGE PROCESSING										Steganography; CNN; GAN; Style transfer; GPU; Real-time processing	TEXTURE SYNTHESIS; CLASSIFICATION	Image steganography aims to conceal the secret information inside another carrier image. And by embedding the information into the carrier image, the carrier image may suffer certain image distortion. Thus, not only the hiding algorithm should be carefully designed, but also the carrier image should be meticulously selected during the hiding process. This paper follows the idea of creating suitable cover images instead of selecting the ones by presenting a unified architecture which combines real-time object detection based on convolutional neural network, local style transfer using generative adversarial network and steganography together to realize real-time carrier image generation. The object in the carrier image is first detected using a fast object detector and then the detected area is reconstructed through a local generative network. The secret message is embedded into the intermediate generated images during the training process in order to generate an image which is suitable as an image carrier. The experimental results show that the reconstructed stego images are nearly indistinguishable to both human eyes and steganalysis tools. Furthermore, the whole carrier image generation process with GPU implementation can achieve around 5 times faster than the regular CPU implementation which meets the requirement of real-time image processing.																	1861-8200	1861-8219				JUN	2020	17	3					655	665		10.1007/s11554-020-00969-w		APR 2020											
J								Motion-blurred star image restoration based on multi-frame superposition under high dynamic and long exposure conditions	JOURNAL OF REAL-TIME IMAGE PROCESSING										Multi-frame superposition; Motion recursive model; Star image restoration; Inertial navigation system	REGULARIZATION APPROACH; TRACKER; ALGORITHM	Under high dynamic and long exposure conditions, the number of recognized stars on motion-blurred star images decreases, thereby degrading the attitude accuracy of star sensors. To improve the attitude accuracy, a restoration method based on multi-frame superposition, which focuses on the noise removal and quality of restored star images, is proposed for a star sensor. During each short exposure time, the corrected coordinate variation of the same star spot between adjacent star images is determined using a motion recursive model. Subsequently, the corrected star spot region is obtained, and the noise is removed. A restoration algorithm based on multi-frame superposition is proposed, taking the time consumption and quality of restored star image considered simultaneously. Simulation results indicate that the proposed restoration method based on multi-frame superposition is effective in removing noise and improving the quality of restored star images. The star recognition rate in simulation experiments verifies the advantages of the proposed method.																	1861-8200	1861-8219															10.1007/s11554-020-00965-0		APR 2020											
J								Prediction and diagnosis of vertebral tumors on the Internet of Medical Things Platform using geometric rough propagation neural network	NEURAL COMPUTING & APPLICATIONS										Vertebral; Lumbar; Calibration; Genetic factor diagnosis; Neural network	STATISTICAL SHAPE; SPINE SEGMENTATION; MODELS; ORDER; 1ST; HIP	Vertebral tumors have a percentage of back pain that causes other vertebral region-born symptoms. Cancers that affect the vertebral column are visceral organ cancer metastases that are mostly seen in older patients. Vertebral dysfunction and neurological failure vertebral column cancers are the most important occurred cancers for patients. In the past, only few methods have been used to combat main and metastatic vertebral tumors. These methods are accessible for short-term monitoring and possess standardized classification consistency for vertebral diagnosis. In this paper, geometric rough propagation neural network has been used for the identification of genetic factors in the examination of a clinical sample with vertebral columns. The proposed neural network has C-statistics of 79.1%, a parameter pitch of 96.1%, and configuration for measurement in the study range with the Brier's score of 95.6%. The algorithm shows great net gain on the decision curve study, with promising performance results of 98.5% on internal testing for preoperative non-routine estimation of discharges with 0.5% error rate and 96% accuracy range. Also, these models have been externally validated by the online healthcare careers cloud-based open access web application on Internet of Medical Things Platform with 97.9% specificity ratio.																	0941-0643	1433-3058															10.1007/s00521-020-04935-2		APR 2020											
J								Classification of pavement crack types based on square bounding box diagonal matching method	NEURAL COMPUTING & APPLICATIONS										Crack classification; Regional similarity; Square bounding boxes	3D ASPHALT SURFACES; ALGORITHM	With the increase in traffic volume, it brings new problems to people's live, that is, the task of inspection and maintenance of road pavements has become increasingly arduous. Therefore, the detection and identification of the road surface has become particularly urgent. This paper proposes a crack classification method based on diagonal matching of square bounding boxes. This method is used to match the geometric features of different types of cracks and gives an evaluation criterion for the regional similarity metrics that can be used to construct the classification of crack types. It provides quantitative criteria for refining the intrinsic relationship of complex types of crack areas. The experimental results show that the classification function of the crack type of the pavement is better. The results show that the accuracy of the identification and classification of block and crack cracks is obviously better than the traditional classification methods such as random forest and support vector machine.																	0941-0643	1433-3058															10.1007/s00521-020-04929-0		APR 2020											
J								A fuzzy variational model for segmentation of images having intensity inhomogeneity and slight texture	SOFT COMPUTING										Image segmentation; Fuzzy sets; Pseudo-level set; Kernel metric	LEVEL SET MODEL; ACTIVE CONTOURS; ALGORITHMS; ENERGY; MUMFORD	Segmentation of real images having unwanted outliers, inhomogeneity or complex background is always very challenging for active contour models. In this paper, we propose a novel model for segmentation of such type of images. The proposed model is based on fuzzy energy functional, which uses coefficient of variation as a region statistics. The proposed model is convex due to introduction of fuzzy membership functions in the energy functional and hence converges to the absolute minima and avoids local minima. Convexity of the proposed model is proved, and hence, the model is independent of initial placement of the contour. Experimental results of the proposed model are compared with other state-of-the-art existing models both qualitatively and quantitatively. For quantitative comparison, we have used Jaccard similarity index and computational complexity. The proposed model is tested on various data sets containing noisy images, images having intensity inhomogeneity and slight texture. In all experimental results, performance of the proposed model can be seen in the experimental section.																	1432-7643	1433-7479				OCT	2020	24	20					15491	15506		10.1007/s00500-020-04878-9		APR 2020											
J								Fuzzy structural element method for solving fuzzy dual medium seepage model in reservoir	SOFT COMPUTING										Partial differential equations; Oil and gas reservoir; Fuzzy permeability; Fuzzy differential equation; Fuzzy double medium model; Fuzzy structural element	ROUGH SET MODELS; QUOTIENT SPACE; (I	This study primarily introduces fuzzy set theory in the reservoirs modeling to enhance the accuracy of the model. The conventional seepage models of dual medium in reservoirs have several limitations. To be specific, it considers the complexity of the model during the modeling process and idealizes certain reservoir parameters to be constant. By adopting fuzzy set theory to study reservoir seepage theory, on the one hand, the seepage model is capable of fully considering the complex and variability of the reservoir; on the other hand, it can avoid parameter errors attributed to laboratory measurements. A numerical example is given in this study to illustrate the accuracy and superiority of the fuzzy seepage model. Studies suggest that fuzzy set theory is capable of effectively addressing the limitations in conventional seepage models.																	1432-7643	1433-7479				NOV	2020	24	21					16097	16110		10.1007/s00500-020-04926-4		APR 2020											
J								Online scheduling of dependent tasks of cloud's workflows to enhance resource utilization and reduce the makespan using multiple reinforcement learning-based agents	SOFT COMPUTING										Cloud computing; Task scheduling; Resource provisioning; Reinforcement learning; Multi-agent systems; Workflow	KRILL HERD ALGORITHM; COMPUTING ENVIRONMENTS; OPTIMIZATION; MANAGEMENT; PERFORMANCE; SIMULATION; PLACEMENT; CLUSTER; EDGE	Due to different heterogeneous cloud resources and diverse and complex applications of the users, an optimal task scheduling, which can satisfy users and cloud service providers with energy-saving and cost-effective use of resources, is a major issue in cloud computing. On the one hand, network users are demanding the quality assurance of their requested services, minimizing their costs, and their own data security, and on the other hand, the service providers consider less power consumption, more efficient use of resources, and optimal utilization. In dependent tasks dealing with massive data, resource scheduling is considered as an important challenge. Due to the time limitation of online scheduling process of dependent tasks, many existing methods of the literature are not able to guarantee the best resource utilization. In this paper, a reinforcement learning approach is exploited in a multi-agent system for task scheduling and resource provisioning, in order to reduce the makespan, minimize the required power, optimize the cost of using the resources, and maximize the utilization of the resources (considering their expiration time), simultaneously. The proposed algorithm has two phases. In the first phase, the tasks are scheduled using reinforcement learning techniques, and in the second one, considering the information obtained from the scheduling phase, resources are allocated in a multi-agent environment. The results of experiments show that this method improves the efficiency of the use of resources and reduces their costs. Moreover, the expiration time of the tasks is observed and the total execution time and energy consumption will be significantly reduced.																	1432-7643	1433-7479				NOV	2020	24	21					16177	16199		10.1007/s00500-020-04931-7		APR 2020											
J								A novel prediction method of complex univariate time series based on k-means clustering	SOFT COMPUTING										Time series; Change trend prediction; K-means clustering; Attention mechanism; Gated recurrent unit	NETWORKS	Time-series prediction has been widely studied and applied in various fields. For the time series with high acquisition frequency and high noise, it is very difficult to establish a prediction model directly. Therefore, it is necessary to study how to obtain the change trend information of time series accurately, and then build a prediction model for its change trend. To obtain the change trend information of the original time series effectively and establish an accurate prediction model, this paper proposes a novel prediction method of complex univariate time series based on K-means clustering. This method first obtains the change trend information of the original time series based on the K-means clustering idea, and then, a gated recurrent unit based on the input attention mechanism is used to establish a prediction model for the obtained time-series change trend information. Extensive experiments on the electromagnetic radiation dataset we collected, the AEP_hourly dataset, and the Wind Turbine Scada dataset published online, demonstrate that our proposed K-means clustering method can effectively reduce noise interference and accurately obtain the time-series change trend information. Comparative experiments of different prediction models demonstrate that our prediction model has the best prediction accuracy, and our proposed complex univariate time-series prediction algorithm has great practical value.																	1432-7643	1433-7479				NOV	2020	24	21					16425	16437		10.1007/s00500-020-04952-2		APR 2020											
J								Instance space analysis for a personnel scheduling problem	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Personnel scheduling; Combinatorial optimization; Algorithm selection; Instance space	ALGORITHM PERFORMANCE; GENERATION	This paper considers the Rotating Workforce Scheduling Problem, and shows how the strengths and weaknesses of various solution methods can be understood by the in-depth evaluation offered by a recently developed methodology known as Instance Space Analysis. We first present a set of features aiming to describe hardness of test instances. We create a new, more diverse set of instances based on an initial instance space analysis that reveals gaps in the instance space, and offers the opportunity to generate additional instances to add diversity to the test suite. The results of three algorithms on our extended instance set reveal insights based on this visual methodology. We observe different regions of strength and weakness in the instance space for each algorithm, as well as a phase transition from feasible to infeasible instances with more challenging instances at the phase transition boundary. This rigorous and insightful approach to analyzing algorithm performance highlights the critical role played by the choice of test instances, and the importance of ensuring diversity and unbiasedness of test instances to support valid conclusions.																	1012-2443	1573-7470															10.1007/s10472-020-09695-2		APR 2020											
J								DGPose: Deep Generative Models for Human Body Analysis	INTERNATIONAL JOURNAL OF COMPUTER VISION										Deep generative models; Semi-supervised learning; Human pose estimation; Variational autoencoders; Generative adversarial networks	PERCEPTION; VISION; PEOPLE; REAL	Deep generative modelling for human body analysis is an emerging problem with many interesting applications. However, the latent space learned by such approaches is typically not interpretable, resulting in less flexibility. In this work, we present deep generative models for human body analysis in which the body pose and the visual appearance are disentangled. Such a disentanglement allows independent manipulation of pose and appearance, and hence enables applications such as pose-transfer without specific training for such a task. Our proposed models, the Conditional-DGPose and the Semi-DGPose, have different characteristics. In the first, body pose labels are taken as conditioners, from a fully-supervised training set. In the second, our structured semi-supervised approach allows for pose estimation to be performed by the model itself and relaxes the need for labelled data. Therefore, the Semi-DGPose aims for the joint understanding and generation of people in images. It is not only capable of mapping images to interpretable latent representations but also able to map these representations back to the image space. We compare our models with relevant baselines, the ClothNet-Body and the Pose Guided Person Generation networks, demonstrating their merits on the Human3.6M, ChictopiaPlus and DeepFashion benchmarks.																	0920-5691	1573-1405				MAY	2020	128	5					1537	1563		10.1007/s11263-020-01306-1		APR 2020											
J								Assessing contemporary legislative proposals for their compatibility with a natural law case for AI legal personhood	AI & SOCIETY										Legal personhood; Natural law; AI; Rights; Gewirth		The question of the moral status of AI and the extent to which that status ought to be recognised by societal institutions is one that has not yet received a satisfactory answer from lawyers. This paper seeks to provide a solution to the problem by defending a moral foundation for the recognition of legal personhood for AI, requiring the status to be granted should a threshold criterion be reached. The threshold proposed will be bare, noumenal agency in the Kantian sense. Agency has been identified by Alan Gewirth as the source of the rights claims of our own species and, at risk of contradiction, is a foundation that must be expanded to all agents or else we contradict the foundation of our own rights. This is something that ought to be recognised through the granting of legal personhood to all noumenal agents by any system that requires such personhood for the enforcement of rights, or else the rule restricting legal personhood cannot be seen as a valid legal norm. Having laid out the case, the paper will move on to defend this natural law conception against the narrower definition of legal personhood proposed by Bryson et al. with regards to AI. It will argue that bare agency is a sufficient, though not necessary, criterion for the ascription of legal personhood in any system that sees the status as necessary for the ascription of legal rights. The paper will conclude by analysing the proposals currently making their way through the legislatures of the UK and European Union. They will be assessed for their compatibility with the claim that a functioning legal system necessarily must recognise the legal personhood of all noumenal agents regardless of their origins, and whether they are future-proofed for the possibility that AI may meet this threshold.																	0951-5666	1435-5655															10.1007/s00146-020-00979-z		APR 2020											
J								An efficient versatile fast correlation vector quantization scheme based reversible data hiding on image processing	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Reversible data hiding; Versatile fast correlation vector quantization (VFCVQ); Embedded secret data; Image processing; Hierarchical state codebook mapping (HSCM)		The Reversible Data Hiding (RHD) is the one of the useful technologies to solve the issues in multimedia security. The lossless data hiding is the most promising data technique; it needs to provide the complete data with a cover media after embedded data is retrieved. For solving this issue, reversible data masking method like Versatile Speed Correlation Vector (VFCVQ) compressed images have been proposed. The two main concerns about data masking are the ability to hide and stereo image quality. The higher efficiency of the VFCVQ images sends more confidential data. A VFCVQ compressed image means it will store the secret data with the original image size, and it will keep avoiding the unwanted attention of the compressed image. The scheme proposed an RHD method based on the concept of a VFCVQ. The proposed coding system uses a specific value to determine the ability to embed confidential data into an image and creates a VFCVQ encoded image or output compressed code stream. Progression of integrating a sensitive data with an original VFCVQ encoded images will be retrieved when output is needed. The proposed method will be implemented to process the secret data using three ways progress which is typically identical in a VFCVQ encoded image; initially the system obtained four-layer connection for embedding the data bit, second it uses the Hierarchical Code-Code Mapping (HSCM) reconstruction to match the first word's encoded-word in image, Finally it will calculate the difference between the new code of the two adjacent blocks, After progressing all of this procedure the encryption of secret bit data values are analyses with each compressed output block. The proposed system is executed in the MATLAB simulation environment; the result of an output data is compared with the proposed methods with previous RHD techniques. The performance analysis of the proposed RHD techniques provides an efficient in terms of like compression ratio 39.2 (%), embedding rate 1.50 (kb), PSNR rate 38.9 (dB) compared with conventional schemes.																	1868-5137	1868-5145															10.1007/s12652-020-01978-0		APR 2020											
J								Stock market analysis using candlestick regression and market trend prediction (CKRM)	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Machine learning; Stock price; Time-series data; Candle-stick pattern; Regression analysis		Stock market data is a time-series data in which stock value varies depends on time. Prediction of the stock market is an endeavor to assess the future value of a company's stock rate which will increase the investor's profit. The accurate prediction of stock market analysis is still a challenging task. The proposed system predicts stock price of any company mentioned by the user for the next few days. Using the predicted stock price and datasets collected from various sources regarding a certain equity, the overall sentiment of the stock is predicted. The prediction of stock price is done by regression and candlestick pattern detection. The proposed system generates signals on the candlestick graph which allows to predict market movement to a sufficient level of accuracy so that the user is able to judge whether a stock is a 'Buy/Sell' and whether to short the stock or go long by delivery. The prediction accuracy of the stock exchange has analyzed and improved to 85% using machine learning algorithms.																	1868-5137	1868-5145															10.1007/s12652-020-01892-5		APR 2020											
J								IoT-inspired smart home based urine infection prediction	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Internet of Things (IoT); Smart urine infection monitoring; Fog computing; Temporal-artificial neural network model	CARE	Urine infection (UI) has been the most prevalent diseases among the global population. With large emphasize on smart healthcare in modern era, UI monitoring has been a major concern for medical industry. Motivated from these aspects, this paper proposes a novel Internet of Things-inspired framework in the form of a smart UI monitoring and prediction system for regular analysis of UI in home-centric environment. The overall framework has been structured in 5-layered model for determination of UI at early stages. These layers include urine perception layer, urine analysis layer, urine extraction layer, Urine prediction layer and visualization layer. These layers are designed to provide effective UI monitoring and analysis in real-time based on mathematical quantification of UI parameters in terms of infection degree and infection index measure for early prediction of UI. For prediction purposes, temporal-artificial neural network model is proposed. The visualization of the UI can be performed in time-sensitive manner over a handheld device for feasible analysis by the user. The validation of the proposed framework was done over real-time data of 12 patients acquired from nearby clinical laboratories. Results were compared with several state-of-the-art prediction approaches which show that the presented technique attain significant enhancement and high efficacy.																	1868-5137	1868-5145															10.1007/s12652-020-01952-w		APR 2020											
J								Fuzzy metrics and cost optimization of a fault-tolerant system with vacationing and unreliable server	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Non-markov fuzzy model; Fault-tolerance; Harmony search; Machining system; Server breakdown; Imperfect recovery	MACHINE INTERFERENCE PROBLEM; REPAIRABLE SYSTEM; RELIABILITY-ANALYSIS; IMPERFECT COVERAGE; QUEUING SYSTEM; SENSITIVITY-ANALYSIS; WORKING VACATIONS; RECURSIVE METHOD; GENERAL REPAIR; WARM STANDBYS	A finite population fuzzy model for the fault-tolerant system (FTS) is studied by considering the general distributed repair time, server vacation, and server breakdown. The concept of imperfect recovery, along with reboot process, is considered for the evaluation of fuzzy performance indices of an FTS supported by warm standbys. If the faults in the system are not detected successfully, then FTS reconfigures itself automatically by rebooting. When the system becomes free from the assigned repair jobs, the idle server can take a vacation and returns from the vacation in case a machine fails and requires repair. The single failure-prone server can provide the repair of failed machines with a slower rate in the breakdown state also. The parametric non-linear programming approach is implemented for the evaluation of performance metrics in a fuzzified environment using system parameters as a trapezoidal fuzzy number. The supplementary variable and recursive approaches are employed to obtain the system size distribution of the M/G/1 model by taking remaining repair time as a supplementary variable. The cost analysis is performed in order to determine the suitable control parameters using harmony search approach. The impacts of the sensitive system parameters on the performance of FTS are explored by evaluating numerical results for specific distributions of repair time.																	1868-5137	1868-5145															10.1007/s12652-020-01951-x		APR 2020											
J								Multiset task related component analysis (M-TRCA) for SSVEP frequency recognition in BCI	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Brain computer interface (BCI); Steady-state visual evoked potential (SSVEP); Electroencephalogram (EEG); Task; related component analysis (TRCA) and multiset TRCA (m-TRCA)	CANONICAL CORRELATION-ANALYSIS; BRAIN	To optimize the frequency recognition of SSVEP signals, various methods were proposed. The optimal frequency recognition methods follow the use of reference signal. Despite its efficiency, an issue identified is inaccurate detection of frequency since the reference signal used is the sine-cosine waves which is pre developed and this seems to lack the exact features present in the original electroencephalogram (EEG) data. To address this inaccuracy of frequency recognition, we propose to advance the reference signal by extricating the basic features (electrical activity of the brain) present in the EEG signals. This study also uses a spatial filtering mechanism to separate the task related activities called task related component analysis (TRCA). Having this TRCA has base, the proposed work functions on recording multiple sets of EEG data called, Multiset TRCA (M-TRCA). This streamlines the reference signal by removing the features that are common in the data sets. Also, an Ensemble M-TRCA that integrates the spatial filters to produce high speed and improved accuracy on frequency recognition is experimented. The proposed M-TRCA and Ensemble M-TRCA methods are compared to the existing TRCA and Ensemble TRCA methods under various parameters.																	1868-5137	1868-5145															10.1007/s12652-020-01962-8		APR 2020											
J								A novel-designed fuzzy logic control structure for control of distinct chaotic systems	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Fuzzy logic control; Nonlinear systems; Lyapunov stability theory	FAULT-DETECTION; POL SYSTEMS; MATHIEU-VAN; SYNCHRONIZATION; STABILIZATION; STABILITY	In this paper, a Lyapunov-based fuzzy logic control (FLC) system is developed for controlling of complicated as well as distinct nonlinear systems. According to Lyapunov Stability Theory, a candidate Lyapunov function with simple quadratic form is designed. Via constructing the fuzzy IF-THEN rules through referencing the statuses of errors states and error derivatives in each sub-space of the Lyapunov function derivatives, the proposed structure of FLC system is able to appropriately as well as flexibly adjust the control forces with minimum magnitude compensating the nonlinear systems in real-time. In addition, the designed FLC system can be applied to different kinds of control systems without further information and operation experience. Two nonlinear systems with distinct structures, classical Lorentz system and Mathieu-van der Pol system, are illustrated for simulation examples. In comparison of the previous research work, the simulation results reveal the effectiveness, flexibility and convenient-design of the proposed method.																	1868-8071	1868-808X				OCT	2020	11	10					2391	2406		10.1007/s13042-020-01125-3		APR 2020											
J								Dynamic dominance-based multigranulation rough sets approaches with evolving ordered data	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Granular computing; Dynamic updating; Ordered data; Knowledge acquisition; Multigranulation	INCREMENTAL ATTRIBUTE REDUCTION; UPDATING APPROXIMATIONS; INFORMATION-SYSTEMS; KNOWLEDGE REDUCTION; SELECTION; MAINTENANCE; GRANULARITY; ACCELERATOR; ALGORITHM; DECISIONS	In practical applications, there exist lots of ordered information systems (OISs). In the process of dealing with OISs, dominant preference, which plays a significant role in decision making, should be taken into consideration. With the increasing of data capacity, OISs often evolve with time. In order to extract updated knowledge from evolving ordered data, we have to elaborate computation efforts to re-calculate entire data, which consumes a significant computational cost. Therefore, the computational efficiency is extremely low. In response to this challenge, matrix-based dynamic dominance-based multigranulation rough sets (DMGRSs) approaches, which can improve computational efficiency for updating knowledge, are explored to update multigranulation approximations in dynamic ordered information systems with evolving data. To begin with, we present a matrix representation of dominance-based multigranulation approximations according to the dominant relation matrix and relevant column vectors of each granular structure. Afterwards, the incremental strategies to update dominance-based multigranulation approximations in OISs are proposed when adding or deleting objects. Furthermore, the corresponding dynamic algorithms, which avoid some unnecessary calculations, are explored in DMGRSs. Finally, extensive experiments carried out on nine UCI data sets indicate that the explored dynamic algorithms can achieve promising performance.																	1868-8071	1868-808X															10.1007/s13042-020-01119-1		APR 2020											
J								A short survey on end-to-end simple question answering systems	ARTIFICIAL INTELLIGENCE REVIEW										Survey; Neural networks; Simple question answering; Deep learning	LONG-TERM DEPENDENCIES	Searching for a specific and meaningful piece of information in the humongous textual data volumes found on the internet and knowledge repositories is a very challenging task. This problem is usually constrained to answering simple, factoid questions by resorting to a question answering (QA) system built on top of complex approaches such as heuristics, information retrieval, and machine learning. More precisely, deep learning methods became into sharp focus of this research field because such purposes can realize the benefits of the vast amounts of data to boost the practical results of QA systems. In this paper, we present a systematic survey on deep learning-based QA systems concerning factoid questions, with particular focus on how each existing system addresses their critical features in terms of learning end-to-end models. We also detail the evaluation process carried out on these systems and discuss how each approach differs from the others in terms of the challenges tackled and the strategies employed. Finally, we present the most prominent research problems still open in the field.																	0269-2821	1573-7462				OCT	2020	53	7					5429	5453		10.1007/s10462-020-09826-5		APR 2020											
J								Product Quantization Network for Fast Visual Search	INTERNATIONAL JOURNAL OF COMPUTER VISION										Product quantization; Image retrieval; Deep learning; Video retrieval	FEATURES	Product quantization has been widely used in fast image retrieval due to its effectiveness of coding high-dimensional visual features. By constructing the approximation function, we extend the hard-assignment quantization to soft-assignment quantization. Thanks to the differentiable property of the soft-assignment quantization, the product quantization operation can be integrated as a layer in a convolutional neural network, constructing the proposed product quantization network (PQN). Meanwhile, by extending the triplet loss to the asymmetric triplet loss, we directly optimize the retrieval accuracy of the learned representation based on asymmetric similarity measurement. Utilizing PQN, we can learn a discriminative and compact image representation in an end-to-end manner, which further enables a fast and accurate image retrieval. By revisiting residual quantization, we further extend the proposed PQN to residual product quantization network (RPQN). Benefited from the residual learning triggered by residual quantization, RPQN achieves a higher accuracy than PQN using the same computation cost. Moreover, we extend PQN to temporal product quantization network (TPQN) by exploiting temporal consistency in videos to speed up the video retrieval. It integrates frame-wise feature learning, frame-wise features aggregation and video-level feature quantization in a single neural network. Comprehensive experiments conducted on multiple public benchmark datasets demonstrate the state-of-the-art performance of the proposed PQN, RPQN and TPQN in fast image and video retrieval.																	0920-5691	1573-1405				SEP	2020	128	8-9			SI		2325	2343		10.1007/s11263-020-01326-x		APR 2020											
J								Model-based Control with Interaction Predicting for Human-coupled Lower Exoskeleton Systems	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Model-based control with interaction predicting; Sensitivity amplification control; Inaccurate dynamic model; Physical human-robot interaction; Strength augmentation; Lower exoskeleton	EXTREMITY; IDENTIFICATION	Sensitivity Amplification Control (SAC) algorithm was first proposed in the augmentation application of the Berkeley Lower Extremity Exoskeleton (BLEEX). Since the SAC algorithm can greatly reduce the complexity of exoskeleton system, it is widely used in human augmentation applications. Nevertheless, the performance of the SAC algorithm depends on the accuracy of dynamic model parameters. In this paper, we propose a novel Model-based control with Interaction Predicting (MIP) strategy to lower dependency on the accurate dynamic model of the exoskeleton. The MIP consists of an interaction predictor and a model-based controller. The interaction predictor can predict motion trajectories of the pilot and substitute for the pilot to drive the exoskeleton through an impedance model. In proposed strategy, the model-based controller not only amplify the forces initiated by the interaction predictor, but more importantly the forces imposed by the pilot to correct the errors between the predictive motion trajectory and the intended motion trajectory of the pilot. Illustrative simulations and experimental results are presented to demonstrate the efficiency of the proposed strategy. Additionally, the comparisons with traditional model-based control algorithm are also presented to demonstrate the efficiency and superiority of the proposed control strategy for lowering dependency on dynamic models.																	0921-0296	1573-0409				NOV	2020	100	2					389	400		10.1007/s10846-020-01200-5		APR 2020											
J								Dynamic Path Planning of the UAV Avoiding Static and Moving Obstacles	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Dynamic path planning; Local path planning; Obstacle avoidance; Cubic spline; Cost function	WINDOW APPROACH; OPTIMIZATION; SIMULATION	This paper introduces a dynamic path planning method for the UAV that can avoid both static and moving obstacles. The condition with sudden threats can better reflect the real situation of the UAV in the real environment. First of all, the A* algorithm is adopted to generate an optimal path in a known environment in this method. Then, in the situation of static sudden threats, a series of candidate paths are generated by the principle of cubic spline second-order continuity. In order to make the static sudden threat at the center of a cluster of candidate paths, they need to be adjusted. After that, this path cluster completely surrounds the sudden threat and has symmetry about the sudden threat. When encountering a sudden threat of movement, factors such as the speed, acceleration and certain parameters of the movement obstacle or the UAV are considered, and a correlation model of the dynamic sudden threat is established. Finally, the total cost function is established to select the optimal obstacle avoidance path, and the total cost function contains four sub-cost functions, they are static security cost function, smoothness cost function, consistency cost function and dynamic security cost function. The simulation results demonstrate the effectiveness of the proposed method.																	0921-0296	1573-0409				SEP	2020	99	3-4			SI		909	931		10.1007/s10846-020-01151-x		APR 2020											
J								Tell me something my friends do not know: diversity maximization in social networks	KNOWLEDGE AND INFORMATION SYSTEMS										Diversity maximization; Filter bubble; Quadratic knapsack; Combinatorial optimization; Greedy algorithms		Social media have a great potential to improve information dissemination in our society, yet they have been held accountable for a number of undesirable effects, such as polarization and filter bubbles. It is thus important to understand these negative phenomena and develop methods to combat them. In this paper, we propose a novel approach to address the problem of breaking filter bubbles in social media. We do so by aiming to maximize the diversity of the information exposed to connected social-media users. We formulate the problem of maximizing the diversity of exposure as a quadratic-knapsack problem. We show that the proposed diversity-maximization problem is inapproximable, and thus, we resort to polynomial nonapproximable algorithms, inspired by solutions developed for the quadratic-knapsack problem, as well as scalable greedy heuristics. We complement our algorithms with instance-specific upper bounds, which are used to provide empirical approximation guarantees for the given problem instances. Our experimental evaluation shows that a proposed greedy algorithm followed by randomized local search is the algorithm of choice given its quality-vs.-efficiency trade-off.																	0219-1377	0219-3116				SEP	2020	62	9					3697	3726		10.1007/s10115-020-01456-1		APR 2020											
J								Single-image super-resolution with multilevel residual attention network	NEURAL COMPUTING & APPLICATIONS										Image super-resolution; Deep convolutional neural network; Attention mechanism; Residual learning	NEURAL-NETWORK	Recently, a great variety of image super-resolution (SR) algorithms based on convolutional neural network (CNN) have been proposed and achieved significant improvement. But how to restore more high-frequency details such as edges and textures is still an unsolved issue. The low-frequency information is similar in a pair of low-resolution and high-resolution images. So the SR model is supposed to pay more attention to the high-frequency features to restore more realistic images. But most CNN-based methods don't consider the different types of features and think the features in different channels and regions contribute equally to the reconstruction performance, which limits the representation capacity of the model. In the meantime, most of these deep networks only simply stack blocks like residual block, which only capture the local features. In this paper, we propose a deep multilevel residual attention network (MRAN) for image SR to focus on the high-frequency features and improve the flow of information. Specially, we propose a channel-wise attention module and a spatial attention module to rescale the channel-wise and spatial weights adaptively, which makes our MRAN focus more on the high-frequency information. Meanwhile, to improve the flow of information and ease the training process, the multilevel residual learning is adopted. Extensive experimental results on five benchmark datasets demonstrate that our MRAN is superior to those state-of-the-art methods for both accuracy and visual comparisons.																	0941-0643	1433-3058				OCT	2020	32	19			SI		15615	15628		10.1007/s00521-020-04896-6		APR 2020											
J								Two hybrid metaheuristic approaches for the covering salesman problem	NEURAL COMPUTING & APPLICATIONS										Covering salesman problem; Traveling salesman problem; Heuristic; Genetic algorithm; Artificial bee colony algorithm	BEE COLONY ALGORITHM; TOUR; OPTIMIZATION	This paper addresses the covering salesman problem (CSP), which is an extension of the classical traveling salesman problem (TSP). Given a set of cities and a coverage radius associated with each one of them, the CSP seeks a Hamiltonian cycle over a subset of cities such that each city not in the subset is within the coverage radius of at least one city in the subset and that has minimum length among all Hamiltonian cycles over such subsets. To solve this problem, one has to deal with the aspects of subset selection and permutation. The CSP finds application in emergency and disaster management and rural healthcare. This paper presents two hybrid metaheuristic approaches for the CSP. The first approach is based on the artificial bee colony algorithm, whereas the latter approach is based on the genetic algorithm. Both the approaches make use of several new first improvement or best improvement based local search strategies defined over various neighborhood structures. Computational results on a wide range of benchmark instances demonstrate the effectiveness of the proposed approaches. We are able to improve the best known solution values on majority of the large instances.																	0941-0643	1433-3058				OCT	2020	32	19			SI		15643	15663		10.1007/s00521-020-04898-4		APR 2020											
J								The design of multiple feedback topology Chebyshev low-pass active filter with average differential evolution algorithm	NEURAL COMPUTING & APPLICATIONS										Tenth-order active filter design; Multiple feedback topology; Chebyshev low-pass active filter; Average differential evolution algorithm; Metaheuristics	COMPONENT VALUE SELECTION; BAND; OPTIMIZATION	This study presents the design of a tenth-order multiple feedback Chebyshev low-pass filter (MF-C-LPF). Component selection and gain calculation of filters are generally achieved over long periods of time using traditional methods. For 1-dB and 3-dB gains, the component values of the filter were optimized for both continuous and discrete values using four different metaheuristic algorithms. In the first case where continuous values were used, component values were accepted as ideal and unlimited in order to minimize gains. In the second case, industrial E196 series component values were used to transform the design problem into a discrete optimization problem. In this case where the design problem became more complex, the performance of the metaheuristic algorithms was compared. The literature review shows that this study is the first attempt to design a 10th-order MF-C-LPF for E196 series values. The average differential evolution algorithm is proposed to determine the optimal component values of the tenth-order MF-C-LPF. The performance of the proposed method was compared with three commonly used algorithms (PSO, CSS and DE). The optimal filter component values and quality factors (Q) were presented for each stage. We believe that the quality factor values will be a reference for future studies.																	0941-0643	1433-3058				NOV	2020	32	22			SI		17097	17113		10.1007/s00521-020-04922-7		APR 2020											
J								Electronic word-of-mouth effects on studio performance leveraging attention-based model	NEURAL COMPUTING & APPLICATIONS										Electronic word-of-mouth; Audience review; Stock market; Deep learning; Attention mechanism	MEDIA; SENTIMENT; REVIEWS; HELPFULNESS; DYNAMICS; MARKETS; MOVIES; EWOM	While existing studies have established the relationship between electronic word-of-mouth (eWOM) and studio performance, limited research has been conducted to demonstrate how the attention-based model applies to the motion picture industry. In this study, examining a review corpus of seven Hollywood studios, we proved that deep learning with the attention mechanism has the best accuracy in both eWOM and stock price movement. We present both a hierarchical two-layer attention network and hierarchical convoluted attention network (HCAN), which quantify the importance of crucial eWOM features in capturing valuable information from audience members' reviews. Further, comparing the two case studies, we determined that the HCAN model is superior to both machine learning and attention-based models. Our work helps to highlight the business value of the attention-based model and has implications for studio business decisions.																	0941-0643	1433-3058															10.1007/s00521-020-04937-0		APR 2020											
J								Deep Learning Neural Network for Unconventional Images Classification	NEURAL PROCESSING LETTERS										Content filtering; Pornographic material recognition; Deep learning; Convolutional neural networks	INTERNET PORNOGRAPHY USE; VISUAL-ATTENTION MODEL; ITERATIVE FUSION; ENSEMBLE; RECOGNITION; DIVERSITY; CLUSTERS; QUALITY	The pornographic materials including videos and images are easily in reach for everyone, including under-age youths, allover Internet. It is also an aim for popular social network applications to contain no public pornographic materials. However, their frequent existence throughout all the Internet and huge amount of available images and videos there, make it impossible for manual monitoring to discriminate positive items (porn image or video) from benign images (non-porn image or video). Therefore, automatic detection techniques can be very useful here. But, the traditional machine learning models face many challenges. For example, they need to tune their many parameters, to select the suitable feature set, to select a suitable model. Therefore, this paper proposes an intelligent filtering system model based on a recent convolutional neural networks where it bypasses the aforementioned challenges. We show that the proposed model outperforms the recent machine learning based models. It also outperforms the state of the art deep learning based models.																	1370-4621	1573-773X				AUG	2020	52	1			SI		169	185		10.1007/s11063-020-10238-3		APR 2020											
J								Unsupervised visual domain adaptation via discriminative dictionary evolution	PATTERN ANALYSIS AND APPLICATIONS										Cross-domain visual classification; Domain adaptation; Discriminative dictionary evolution; Feature representation learning; Transfer learning	ALIGNMENT; KERNEL	This work focuses on unsupervised visual domain adaptation which is still challenging in visual recognition. Most of the attention has been dedicated to seeking the domain-invariant features of cross-domain data, but they ignores the valuable discriminative information in the source domain. In this paper, we propose a Discriminative Dictionary Evolution (DDE) approach to seek discriminative features robust to domain shift. Specifically, DDE gradually adapts a discriminative dictionary learned from the source domain to the target domain through a dictionary evolving procedure, in which self-selected atoms of the dictionary are updated with l(2,1)-norm-based regularization. DDE produces domain-invariant representations for cross-domain visual recognition meanwhile promotes the discriminativeness of the dictionary. Empirical results on real-world data sets demonstrate the advantages of the proposed approach over existing competitive methods.																	1433-7541	1433-755X				NOV	2020	23	4					1665	1675		10.1007/s10044-020-00881-w		APR 2020											
J								Temporal convolutional neural (TCN) network for an effective weather forecasting using time-series data from the local weather station	SOFT COMPUTING										Localized weather forecasting; Time-series data analysis; Temporal convolution networks (TCN); Long short-term memory (LSTM); Precision farming	PREDICTION; PARAMETERS	Non-predictive or inaccurate weather forecasting can severely impact the community of users such as farmers. Numerical weather prediction models run in major weather forecasting centers with several supercomputers to solve simultaneous complex nonlinear mathematical equations. Such models provide the medium-range weather forecasts, i.e., every 6 h up to 18 h with grid length of 10-20 km. However, farmers often depend on more detailed short-to medium-range forecasts with higher-resolution regional forecasting models. Therefore, this research aims to address this by developing and evaluating a lightweight and novel weather forecasting system, which consists of one or more local weather stations and state-of-the-art machine learning techniques for weather forecasting using time-series data from these weather stations. To this end, the system explores the state-of-the-art temporal convolutional network (TCN) and long short-term memory (LSTM) networks. Our experimental results show that the proposed model using TCN produces better forecasting compared to the LSTM and other classic machine learning approaches. The proposed model can be used as an efficient localized weather forecasting tool for the community of users, and it could be run on a stand-alone personal computer.																	1432-7643	1433-7479				NOV	2020	24	21					16453	16482		10.1007/s00500-020-04954-0		APR 2020											
J								Intelligent fuzzy rule-based approach with outlier detection for secured routing in WSN	SOFT COMPUTING										Fuzzy rules; Trust score; Energy efficiency; Secure routing and cluster-based routing; Temporal reasoning; Wireless sensor networks	WIRELESS SENSOR NETWORKS; ENERGY-EFFICIENT; PROTOCOL; ALGORITHM; CLUSTER; KNOWLEDGE	In wireless sensor networks (WSNs), energy optimization and the provision of security are the major design challenges. Since the wireless sensor devices are energy constrained, the issue of high energy consumption by the malicious nodes must be addressed well in order to enhance the network performance by making increased network lifetime, reduced energy consumption and delay. In the past, many researchers worked in the provision of new techniques for providing improved security to WSN in order to enhance the reliability in the routing process. However, most of the existing routing techniques are not able to achieve the required security through the use of intelligent techniques for safeguarding the sensor nodes from malicious attacks. In order to address these problems, a new fuzzy temporal clustering-based secured communication model with trust analysis and outlier detection has been developed in this research work. For this purpose, a new fuzzy temporal rule-based cluster-based routing algorithm with trust modelling and outlier detection for monitoring the nodes participating in the communication has been proposed. In addition, a fuzzy temporal rule- and distance-based outlier detection algorithm is also proposed in this paper for distinguishing the malicious nodes from other nodes within each cluster of the network and has been used in the secured routing algorithm. The proposed secure routing algorithm uses the temporal reasoning tasks of explanation-based learning and prediction as well as spatial constraints for making efficient routing decisions through the application of trust and key management techniques for performing effective authentication of nodes and thereby isolating the malicious nodes from communication through outlier detection. By applying these two proposed algorithms for communication in the proposed work, it is proved through experiments that the proposed secure routing algorithm and the outlier detection algorithm are able to perform secured and reliable routing through genuine cluster head nodes more effectively. Moreover, these two algorithms provide improved quality of service with respect to the reliability of communication, packet delivery ratio, reduction in end-to-end delay and reduced energy consumption.																	1432-7643	1433-7479				NOV	2020	24	21					16483	16497		10.1007/s00500-020-04955-z		APR 2020											
J								Engineering problems in machine learning systems	MACHINE LEARNING										Machine learning; Software engineering; Systems engineering; Safety critical systems; Automated driving; Quality models		Fatal accidents are a major issue hindering the wide acceptance of safety-critical systems that employ machine learning and deep learning models, such as automated driving vehicles. In order to use machine learning in a safety-critical system, it is necessary to demonstrate the safety and security of the system through engineering processes. However, thus far, no such widely accepted engineering concepts or frameworks have been established for these systems. The key to using a machine learning model in a deductively engineered system is decomposing the data-driven training of machine learning models into requirement, design, and verification, particularly for machine learning models used in safety-critical systems. Simultaneously, open problems and relevant technical fields are not organized in a manner that enables researchers to select a theme and work on it. In this study, we identify, classify, and explore the open problems in engineering (safety-critical) machine learning systems-that is, in terms of requirement, design, and verification of machine learning models and systems-as well as discuss related works and research directions, using automated driving vehicles as an example. Our results show that machine learning models are characterized by a lack of requirements specification, lack of design specification, lack of interpretability, and lack of robustness. We also perform a gap analysis on a conventional system quality standard SQuaRE with the characteristics of machine learning models to study quality models for machine learning systems. We find that a lack of requirements specification and lack of robustness have the greatest impact on conventional quality models.																	0885-6125	1573-0565				MAY	2020	109	5					1103	1126		10.1007/s10994-020-05872-w		APR 2020											
J								Implementation of an optimized binary classification byGMDH-typeneural network algorithm for predicting the blast produced ground vibration	EXPERT SYSTEMS										blasting; GMDH; ground vibration; optimized binary classification	NEURAL-NETWORKS; INDUCED FLYROCK; PERFORMANCE; ANN	Ground vibration is one of the most important undesired phenomena resulting from blasting operations imposing damages to facilities and buildings on the one hand, and creating environmental problems in open pit mining on the other. Therefore, the present study aims to provide an optimized classification binary model to identify the blasting patterns with an acceptable ground vibration intensity to reduce the damages resulting from this artificial phenomenon. This study uses a binary method to provide an optimized classification model for predicting and evaluating the blasting patterns with the minimum ground vibration. Group Method of Data Handling-Type Neural Network is used as one of the most practical optimization algorithms to solve complicated and uncertain problems in this modelling. In this study, by collecting the data of 52 different blasting patterns from Soungun copper mine, some of the most important geometric properties and the amount of ammonium nitrate fuel oil consumed in each blasting pattern are recorded. In addition, based on expertise and experience of experts, the degree of ground vibration produced by each blasting is qualitatively classified into four different ranges of very high, high, normal and low in the form of unacceptable (very high and High) and acceptable (normal and low) clusters. Based on the results obtained from the analyses, the developed model has a high flexibility and ability in the binary prediction of blasting patterns with an acceptable vibration magnitude.																	0266-4720	1468-0394				OCT	2020	37	5			SI				e12563	10.1111/exsy.12563		APR 2020											
J								Machine learning explainability via microaggregation and shallow decision trees	KNOWLEDGE-BASED SYSTEMS										Explainability; Machine learning; Data protection; Microaggregation; Privacy		Artificial intelligence (AI) is being deployed in missions that are increasingly critical for human life. To build trust in AI and avoid an algorithm-based authoritarian society, automated decisions should be explainable. This is not only a right of citizens, enshrined for example in the European General Data Protection Regulation, but a desirable goal for engineers, who want to know whether the decision algorithms are capturing the relevant features. For explainability to be scalable, it should be possible to derive explanations in a systematic way. A common approach is to use simpler, more intuitive decision algorithms to build a surrogate model of the black-box model (for example a deep learning algorithm) used to make a decision. Yet, there is a risk that the surrogate model is too large for it to be really comprehensible to humans. We focus on explaining black-box models by using decision trees of limited depth as a surrogate model. Specifically, we propose an approach based on microaggregation to achieve a trade-off between the comprehensibility and the representativeness of the surrogate model on the one side and the privacy of the subjects used for training the black-box model on the other side. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 22	2020	194								105532	10.1016/j.knosys.2020.105532													
J								Construction and exploitation of an historical knowledge graph to deal with the evolution of ontologies	KNOWLEDGE-BASED SYSTEMS										Knowledge graphs; Ontology evolution; Biomedical ontology; Versioning	MAPPINGS; SYSTEM	With the advances of Artificial Intelligence, the need for annotated data increases. However, the quality of these annotations can be impacted by the evolution of domain knowledge since the relations between successive versions of ontologies are rarely described and the history of concepts is not kept at the ontology level. As a consequence, using datasets annotated at different times becomes a real challenge for data- and knowledge-intensive systems. This work presents a way to address this problem. We introduce a Historical Knowledge Graph (HKG), where information from previous versions of an ontology can be found inside a single graph, reducing storage space (no need for versioning) and data treatment time (no need for laborious analysis of each version of the ontology). The HKG proposed in this work represents the evolutionary aspects of the knowledge in a structural way. Examples of the applicability of an HKG for information retrieval and the maintenance of semantic annotations show the capability of our approach for improving the quality of existing techniques. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 22	2020	194								105508	10.1016/j.knosys.2020.105508													
J								Probabilistic quantum clustering	KNOWLEDGE-BASED SYSTEMS										Quantum clustering; Mixture of Gaussians; Probabilistic framework; Unsupervised assessment; Manifold Parzen window	ALGORITHM; NUMBER	Quantum Clustering is a powerful method to detect clusters with complex shapes. However, it is very sensitive to a length parameter that controls the shape of the Gaussian kernel associated with a wave function, which is employed in the Schrodinger equation with the role of a density estimator. In addition, linking data points into clusters requires local estimates of covariance which requires further parameters. This paper proposes a Bayesian framework that provides an objective measure of goodness-of-fit to the data, to optimise the adjustable parameters. This also quantifies the probabilities of cluster membership, thus partitioning the data into a specific number of clusters, where each cluster probability is estimated through an aggregated density function composed of the data samples that generate the cluster, having each cluster an associated probability density function P(K|X); this probability can be used as a measure of how well the clusters fit the data. Another main contribution of the work is the adaptation of the Schrodinger equation to deal with local length parameters for cluster discrimination by density. The proposed framework is tested on real and synthetic data sets, assessing its validity by measuring concordance with the Jaccard score. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 22	2020	194								105567	10.1016/j.knosys.2020.105567													
J								Novel chaotic grouping particle swarm optimization with a dynamic regrouping strategy for solving numerical optimization tasks	KNOWLEDGE-BASED SYSTEMS										Particle swarm optimization; Chaotic map; Chaotic grouping mechanism; Dynamic regrouping strategy	BEE COLONY ALGORITHM; GLOBAL OPTIMIZATION; TOPOLOGY; SEARCH; STABILITY; EVOLUTION; SELECTION	Particle swarm optimization (PSO) has been widely applied to address various optimization problems, since it is easy to implement and only has a few control parameters. However, PSO often suffers from a lack of population diversity during the search process and is ineffective in balancing exploration and exploitation, especially in solving complex numerical optimization tasks. To overcome these disadvantages of PSO for complex numerical optimization problems, a new chaotic grouping PSO algorithm with a dynamic regrouping strategy (CGPSO-DRS) is proposed in this paper. The newly proposed CGPSO-DRS is based on a dynamic multiswarm PSO framework that cooperates with the chaotic grouping mechanism (CGM) and the dynamic regrouping strategy (DRS). First, the CGM divides the entire population into many subswarms via a chaotic sequence. The CGM not only improves the population grouping quality in the search process but also increases the diversity of the population. Second, the DRS is used to guide the regrouping of the population, and the population starts searching with a new configuration. Here, the DRS changes with the number of function evaluations. The DRS facilitates the effective utilization of information to balance the early exploration and the later exploitation performances. In addition, the DRS can increase the diversity of the population in the search process. Experiments have been conducted on 41 benchmark functions, and the numerical results demonstrate that the proposed CGPSO-DRS method outperforms similar population-based approaches and state-of-the-art PSO variants in accelerating the convergence speed and finding the global optimum. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 22	2020	194								105568	10.1016/j.knosys.2020.105568													
J								A topic-sensitive trust evaluation approach for users in online communities	KNOWLEDGE-BASED SYSTEMS										Topic-sensitive analysis; Trust evaluation; Trust propagation; Context-dependency; Labeled LDA	PROPAGATION; REPUTATION; MODEL	In order to facilitate human decision making, trust evaluation has received widespread attention in many fields, especially for online services. Most of the existing methods consider trust in a person as a value which does not vary across different scenarios without any attention to the distinction of domains or communities where trust is derived. However, the notion of context is a significant and indispensable factor for trust evaluation in practice. Due to the lack of the consideration of context, traditional methods cannot resolve the issue that arises when a highly trustworthy person in one domain is likely to dominate the results of trust assessment in others where the person is in fact less authoritative. To solve this problem, in this paper, we develop a general approach to accomplish topic-sensitive trust evaluation by considering the context of trust. We first propose a general framework which presents the well-organized architecture of topic-sensitive trust evaluation in online communities. Then, a user-topic model is proposed to automatically extract topic data from user-generated content based on the Labeled Latent Dirichlet Allocation (LLDA) model. To compare the topic differences between users, we design a topic coverage function for revealing their trust relationships in diverse topics. Moreover, we employ two traditional methods and extend them to accomplish trust prediction for people with multiple domain knowledge. Experiments based on a realworld dataset show that extended topic-sensitive approaches are more adaptive and accurate than those topic-free trust evaluation approaches, especially when the trust application scenario features multiple topics. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 22	2020	194								105546	10.1016/j.knosys.2020.105546													
J								Graph-regularized least squares regression for multi-view subspace clustering	KNOWLEDGE-BASED SYSTEMS										Multi-view clustering; Subspace clustering; Least squares regression; Column-sparsity norm; Manifold constraint	ALGORITHM	Many works have proven that the consistency and differences in multi-view subspace clustering make the clustering results better than the single-view clustering. Therefore, this paper studies the multi-view clustering problem, which aims to divide data points into several groups using multiple features. However, existing multi-view clustering methods fail to capturing the grouping effect and local geometrical structure of the multiple features. In order to solve these problems, this paper proposes a novel multi-view subspace clustering model called graph-regularized least squares regression (GLSR), which uses not only the least squares regression instead of the nuclear norm to generate grouping effect, but also the manifold constraint to preserve the local geometrical structure of multiple features. Specifically, the proposed GLSR method adopts the least squares regression to learn the globally consensus information shared by multiple views and the column-sparsity norm to measure the residual information. Under the alternating direction method of multipliers framework, an effective method is developed by iteratively update all variables. Numerical studies on eight real databases demonstrate the effectiveness and superior performance of the proposed GLSR over eleven state-of-the-art methods. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 22	2020	194								105482	10.1016/j.knosys.2020.105482													
J								Texture synthesis quality assessment using perceptual texture similarity	KNOWLEDGE-BASED SYSTEMS										Texture; Texture synthesis quality assessment; Perceptual texture similarity; Random forests	FEATURES; CLASSIFICATION; INFORMATION	Texture synthesis plays an important role in computer game and movie industries. Although it has been widely studied, the assessment of the quality of the synthesised textures has received little attention. Inspired by the research progress in perceptual texture similarity estimation, we propose a Texture Synthesis Quality Assessment (TSQA) approach. To our knowledge, this is the first attempt to exploit perceptual texture similarity for the TSQA task. In particular, we introduce two perceptual similarity principles for synthesis quality assessment. Correspondingly, we train two Random Forest (RF) regressors. Given a pair of sample and synthesised textures, the two regressors can be used to predict the global and local quality scores of the synthesised texture respectively. An overall score is generated from the two scores. Our results show that the deep Bag-of-Words (BoW) descriptors, extracted by a pre-trained Convolutional Neural Network (CNN), perform better than, or comparably to, the other nine types of hand-crafted or CNN descriptors and an image quality assessment measure, together with the proposed TSQA approach. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 22	2020	194								105591	10.1016/j.knosys.2020.105591													
J								Helping university students to choose elective courses by using a hybrid multi-criteria recommendation system with genetic optimization	KNOWLEDGE-BASED SYSTEMS										Recommendation system; Course recommendation; Hybrid multi-criteria filtering; Genetic algorithm		The wide availability of specific courses together with the flexibility of academic plans in university studies reveal the importance of Recommendation Systems (RSs) in this area. These systems appear as tools that help students to choose courses that suit to their personal interests and their academic performance. This paper presents a hybrid RS that combines Collaborative Filtering (CF) and Content-based Filtering (CBF) using multiple criteria related both to student and course information to recommend the most suitable courses to the students. A Genetic Algorithm (GA) has been developed to automatically discover the optimal RS configuration which include both the most relevant criteria and the configuration of the rest of parameters. The experimental study has used real information of Computer Science Degree of University of Cordoba (Spain) including information gathered from students during three academic years, counting on 2500 entries of 95 students and 63 courses. Experimental results show a study of the most relevant criteria for the course recommendation, the importance of using a hybrid model that combines both student information and course information to increase the reliability of the recommendations as well as an excellent performance compared to previous models. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 22	2020	194								105385	10.1016/j.knosys.2019.105385													
J								Rule-based granular classification: A hypersphere information granule-based method	KNOWLEDGE-BASED SYSTEMS										Granular classification; Hypersphere information granules; Granular computing	DECISION TREE CLASSIFICATION; FUZZY RULES; DATA ANALYTICS; MACHINE; CLASSIFIERS; PRINCIPLE; DESIGN	As fundamental abstract constructs supporting the human-centered way of Granular Computing (GrC), information granules can be used to distinguish different classes of data from the perspective of easily understood geometrical structure. In this study, a three-stage rule-based granular classification method is proposed using a union of a series of hypersphere information granules. The first stage focuses on dividing each class of data into a series of chunks. The second stage concerns the construction of some hyperspheres around these chunks. These resulting hyperspheres form a union information granule to depict the key structural characteristics of the corresponding data through their union operation. At the final stage, the union information granules are refined and the rule-based granular classification model is emerged through using a series of "If-Then'' rules to articulate the refined union information granule formed for each class with the corresponding class label. A number of experiments involving several synthetic and publicly available datasets are implemented to exhibit the advantages of the resulting classifier. The impacts of critical parameters on the performance of the constructed classifier are also revealed. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 22	2020	194								105500	10.1016/j.knosys.2020.105500													
J								Kernel compositional embedding and its application in linguistic structured data classification	KNOWLEDGE-BASED SYSTEMS										Structured data representation; Kernel methods; Compositional embedding; Structured object classification	WORD EMBEDDINGS; SPACE	In many applications such as natural language processing, speech recognition, and computer vision, there are inputs with hierarchical compositional structure and long relation among their subcomponents. Introducing this information in the definition of a model can improve its performance in dealing with this type of data. On the other side, the high generalization power of kernel methods is proven in traditional machine learning problems. If we can employ some idea of these methods on handling structured objects, we can benefit from improving the performance and the generalization capability of compositional models. Accordingly, a new approach is introduced in this paper to realize the idea of simultaneously leveraging advantages of both kernel methods and compositional embedding to provide powerful representations and classifiers for structured data. Based on this approach, which is named Kernel Compositional Embedding (KCE), we propose two methods: Direct KCE (DKCE), and Indirect KCE (IKCE). In the DKCE method, we directly deal with a potentially infinite dimensional embedding space, i.e., the embeddings are used implicitly in the classification task. In IKCE method, instead of implicitly performing all operations inside embedding space, only some elements of one or more reproducing kernel Hilbert spaces are used to embed structured objects into low-dimensional Euclidean spaces. To evaluate the performance of the proposed methods, we apply them on two common computational linguistic tasks, i.e, sentiment analysis and natural language inference. The experimental results illustrate that the classification performance of the proposed methods is higher than or competitive to some well-known competing methods. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 22	2020	194								105553	10.1016/j.knosys.2020.105553													
J								Evolutionary and adaptive inheritance enhanced Grey Wolf Optimization algorithm for binary domains	KNOWLEDGE-BASED SYSTEMS										Binary optimization; Grey wolf optimization; Knapsack problem; Facility location problem; Multi-parent crossover	BEE COLONY ALGORITHM; DESIGN OPTIMIZATION; FIREFLY ALGORITHM; LOCATION; INTELLIGENCE; SEARCH; CHAOS	This paper introduces a new binary Grey Wolf Optimization (GWO) algorithm, which is one of the recent swarm intelligence-based metaheuristic algorithms. Its various extensions have been reported in the related literature. Despite numerous successful applications in real-valued optimization problems, the canonical GWO algorithm cannot directly handle binary optimization problems. To this end, transformation functions are commonly employed to map the real-valued solution vector to the binary values; however, this approach brings about the undesired problem of spatial disconnect. In this study, evolutionary and adaptive inheritance mechanisms are employed in the GWO algorithm so as to operate in the binary domain directly. To mimic the leadership hierarchy procedure of the GWO, multi-parent crossover with two different dominance strategies is developed while updating the binary coordinates of the wolf pack. Furthermore, adaptive mutation with exponentially decreasing step-size is adopted to avoid premature convergence and to establish a balance between intensification and diversification. The performance of the proposed algorithm is tested on the well-known binary benchmark suites comprised of the Set-Union Knapsack Problem (SUKP) that extends the 0-1 Knapsack Problem and the Uncapacitated Facility Location Problem (UFLP). Comprehensive experimental study including real-life applications and statistical analyses demonstrate the effectiveness of the proposed algorithm. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 22	2020	194								105586	10.1016/j.knosys.2020.105586													
J								RuleKit: A comprehensive suite for rule-based learning	KNOWLEDGE-BASED SYSTEMS										Rule learning; Classification; Regression; Survival analysis; User-guided induction; Knowledge discovery		Rule-based models are often used for data analysis as they combine interpretability with predictive power. We present RuleKit, a versatile tool for rule learning. Based on a sequential covering induction algorithm, it is suitable for classification, regression, and survival problems. The presence of a user-guided induction facilitates verifying hypotheses concerning data dependencies which are expected or of interest. The powerful and flexible experimental environment allows straightforward investigation of different induction schemes. The analysis can be performed in batch mode, through RapidMiner plug-in, or R package. (C) 2020 The Authors. Published by Elsevier B.V.																	0950-7051	1872-7409				APR 22	2020	194								105480	10.1016/j.knosys.2020.105480													
J								Knowledge of words: An interpretable approach for personality recognition from social media	KNOWLEDGE-BASED SYSTEMS										Personality recognition; Big five; Lexicon; Social media	ATTRIBUTES	Personality is one of the fundamental and stable individual characteristics that can be detected from human behavioral data. With the rise of social media, increasing attention has been paid to the ability to recognize personality traits by analyzing the contents of user-generated text. Existing studies have used general psychological lexicons or machine learning, and even deep learning models, to predict personality, but their performance has been relatively poor or they have lacked the ability to interpret personality. In this paper, we present a novel interpretable personality recognition model based on a personality lexicon. First, we use word embedding techniques and prior-knowledge lexicons to automatically construct a Chinese semantic lexicon suitable for personality analysis. Based on this personality lexicon, we analyze the correlations between personality traits and semantic categories of words, and extract the semantic features of users' microblogs to construct personality recognition models using classification algorithms. Extensive experiments are conducted to demonstrate that the proposed model can achieve significantly better performances compared to previous approaches. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 22	2020	194								105550	10.1016/j.knosys.2020.105550													
J								Solving dynamic multi-objective problems with an evolutionary multi-directional search approach	KNOWLEDGE-BASED SYSTEMS										Dynamic; Dynamic multi-objective optimization; Local search; Multi-directional search strategy	LOCAL SEARCH; OPTIMIZATION ALGORITHM; ENVIRONMENTS; PREDICTION	The challenge of solving dynamic multi-objective optimization problems is to effectively and efficiently trace the varying Pareto optimal front and/or Pareto optimal set. To this end, this paper proposes a multi-direction search strategy, aimed at finding the dynamic Pareto optimal front and/or Pareto optimal set as quickly and accurately as possible before the next environmental change occurs. The proposed method adopts a multi-directional search approach which mainly includes two parts: an improved local search and a global search. The first part uses individuals from the current population to produce solutions along each decision variables direction within a certain range and updates the population using the generated solutions. As a result, the first strategy enhances the convergence of the population. In part two, individuals are generated in a specific random method along every dimensions orientation in the decision variable space, so as to achieve good diversity as well as guarantee the avoidance of local optimal solutions. The proposed algorithm is measured on several benchmark test suites with various dynamic characteristics and different difficulties. Experimental results show that this algorithm is very competitive in dealing with dynamic multi-objective optimization problems when compared with four state-of-the-art approaches. (C) 2019 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 22	2020	194								105175	10.1016/j.knosys.2019.105175													
J								Batch allocation for decomposition-based complex task crowdsourcing e-markets in social networks	KNOWLEDGE-BASED SYSTEMS										Crowdsourcing e-markets; Task allocation; Complex task; Decomposition; Social network	QUALITY	In existing studies on decomposition-based complex task crowdsourcing e-markets, a complex task is first decomposed into a flow of simple subtasks and then the decomposed subtasks are allocated independently to different individual workers. However, such retail-style independent allocation of decomposed subtasks costs much time and the intermediate results of subtasks cannot be utilized by each other; moreover, the independent allocation does not consider the cooperation among assigned workers and the time-dependency relations among subtasks. To solve such a problem, this paper presents a novel batch allocation approach for decomposition-based complex task crowdsourcing in social networks, in which the similar subtasks of complex tasks are integrated into a batch that will be allocated to the same workers. In the presented approach, it is preferable that a batch of subtasks will be allocated to the workers within the same group or the workers with closer relations in a social network; moreover, the allocation will consider the time constraints of subtasks so that the deadlines of the whole complex tasks can be satisfied. This batch allocation optimization problem is proved to be NP-hard. Then, two types of heuristic approaches are designed: the lateral approach that does not consider the subordination relationship between subtasks and complex tasks and the longitudinal approach that considers such relationships. The experiments on real-world crowdsourcing datasets show that the two presented heuristic approaches outperform traditional retail-style allocation approach in terms of total payment by requesters, average income of assigned workers, cooperation efficiency of assigned workers, and task allocation time. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 22	2020	194								105522	10.1016/j.knosys.2020.105522													
J								Collaborative learning based on centroid-distance-vector for wearable devices	KNOWLEDGE-BASED SYSTEMS										Internet-of-Things (IoT); Wearable devices; Collaborative learning		In typical application scenarios of Internet-of-Things (IoT), human activity recognition capability of a wearable device is usually limited to the body segment on which the device is worn. We believe that an efficient approach where wearable devices collaboratively learn can lead to better performance of activity recognition. In this paper, we first propose the concept of centroid distance vector to fully exploit and express the hidden knowledge of available data. Then, we introduce our centroid-distancevector-based collaborative learning framework which consists of two key steps: data expansion with centroid distance vector and automatic model construction. In order to achieve automatic model construction, we further propose two collaborative learning methods: a centroid-distance-vectorbased label propagation method (CD-LP) and a centroid-distance-vector-based knowledge distillation method (CD-KD). Experiments have shown that by using our proposed algorithms, the human activity recognition accuracy can achieve an average improvement of 5.24% compared to the state-of-the-art machine learning methods, and 5.88 % compared to the neural network with two fully-connected hidden layers. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 22	2020	194								105569	10.1016/j.knosys.2020.105569													
J								Three-way conflict analysis: A unification of models based on rough sets and formal concept analysis	KNOWLEDGE-BASED SYSTEMS										Conflict analysis; Formal concept analysis; Rough sets; Three-valued situation table; Three-way decision	DECISION; APPROXIMATIONS; INFORMATION; CONTEXTS	The Pawlak model of conflict analysis uses three-valued ratings (i.e., positive, neutral, and negative) of a set of agents on a set of issues. Several extensions to the Pawlak model, namely, rough sets based qualitative and quantitative models, formal concept analysis based quantitative models, and three-way conflict analysis models, have been proposed in recent years. The main objective of this paper is to propose a more general model that unifies these existing models in an evaluation-based framework of three-way decision. The proposed model uses a pair of evaluations, one for support and the other for opposition, for trisecting the set of agents. By considering qualitative and quantitative evaluations, we derive a qualitative model and a quantitative model of three-way conflict analysis, respectively. The corresponding two models built based on rough sets and the corresponding two models built based on formal concept analysis are special cases. A unification of existing models provides insights into a common structure in formulating three-way conflict analysis with different choices of evaluations. We illustrate an application of the three-way conflict analysis model in making development plans for Gansu Province in China. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 22	2020	194								105556	10.1016/j.knosys.2020.105556													
J								Estimation of missing values in heterogeneous traffic data: Application of multimodal deep learning model	KNOWLEDGE-BASED SYSTEMS										Autoencoder; Feature fusion; Deep learning; Traffic missing data; Imputation	LOOP DETECTOR DATA; DATA IMPUTATION; FLOW; PREDICTION; COMPLETION; ALGORITHM; NETWORK; TIME	With the development of sensing technology, a large amount of heterogeneous traffic data can be collected. However, the raw data often contain corrupted or missing values, which need to be imputed to aid traffic condition monitoring and the assessment of the system performance. Several existing studies have reported imputation models used to impute the missing values, and most of these models aimed to capture the spatial or temporal dependencies. However, the dependencies of the heterogeneous data were ignored. To this end, we propose a multimodal deep learning model to enable heterogeneous traffic data imputation. The model involves the use of two parallel stacked autoencoders that can simultaneously consider the spatial and temporal dependencies. In addition, a latent feature fusion layer is developed to capture the dependencies of the heterogeneous traffic data. To train the proposed imputation model, a hierarchical training method is introduced. Using a real world dataset, the performance of the proposed model is evaluated and compared with that of several widely used temporal imputation models, spatial imputation models, and spatial-temporal imputation models. The experimental and evaluation results indicate that the values of the evaluation criteria of the proposed model are smaller, indicating a better performance. The results also show that the proposed model can accurately impute the continuously missing data. Furthermore, the sensitivity of the parameters used in the proposed deep multimodal deep learning model is investigated. This study clearly demonstrates the effectiveness of deep learning for heterogeneous traffic data synthesis and missing data imputation. The dependencies of the heterogeneous traffic data should be considered in future studies to improve the performance of the imputation model. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 22	2020	194								105592	10.1016/j.knosys.2020.105592													
J								A comprehensive exploration of semantic relation extraction via pre-trained CNNs	KNOWLEDGE-BASED SYSTEMS										Relation extraction; Semantic relation; Natural language processing; Convolutional neural networks		Semantic relation extraction between entity pairs is a crucial task in information extraction from text. In this paper, we propose a new pre-trained network architecture for this task, and it is called the XM-CNN. The XM-CNN utilizes word embedding and position embedding information. It is designed to reinforce the contextual output from the MT-DNNKD pre-trained model. Our model effectively utilized an entity-aware attention mechanisms to detected the features and also adopts and applies more relation-specific pooling attention mechanisms applied to it. The experimental results show that the XM-CNN achieves state-of-the-art results on the SemEval-2010 task 8, and a thorough evaluation of the method is conducted. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 22	2020	194								105488	10.1016/j.knosys.2020.105488													
J								Multi-view clustering by non-negative matrix factorization with co-orthogonal constraints	KNOWLEDGE-BASED SYSTEMS										Multi-view clustering; Co-orthogonal constraints; Non-negative matrix factorization		Non-negative matrix factorization (NMF) has attracted sustaining attention in multi-view clustering, because of its ability of processing high-dimensional data. In order to learn the desired dimensionalreduced representation, a natural scheme is to add constraints to traditional NMF. Motivated by that the clustering performance is affected by the orthogonality of inner vectors of both the learned basis matrices and the representation matrices, a novel NMF model with co-orthogonal constraints is designed to deal with the multi-view clustering problem in this paper. For solving the proposed model, an efficient iterative updating algorithm is derived. And the corresponding convergence is proved, together with the analysis to its computational complexity. Experiments on five datasets are performed to present the advantages of the proposed algorithm against the state-of-the-art methods. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 22	2020	194								105582	10.1016/j.knosys.2020.105582													
J								A region division based decomposition approach for evolutionary many-objective optimization	KNOWLEDGE-BASED SYSTEMS										Region division; Many-objective optimization; Many-objective 0/1 knapsack problems; Decomposition; Evolutionary computation	ALGORITHM; SELECTION; INDICATOR; DIVERSITY; MOEA/D	A region division based decomposition approach for evolutionary many-objective optimization (denoted as RD-EMO) is proposed in this paper. In the proposed RD-EMO, a set of reference points are generated and the objective space is divided into a set of regions through angle bisectors between adjacent reference lines. Then two attributions of regions are defined, which are region degree and region sparse rate, respectively. Region attributions based select operator is designed to choose solutions in sparse regions of objective space as mating solutions so that new solutions created by mating solutions can be located in sparser regions. In addition, region sparse rate is also applied to the population update process so that solutions in sparse regions of objective space are reserved and those in dense regions are discarded. Hence, two attributions of regions can better guarantee population diversity. Moreover, those solutions with better scalar function values are reserved in the same intensity regions so that population convergence is also guaranteed. In the study of the performance of the proposed algorithm, the performance comparison of RD-EMO with some state-of-the-art algorithms including NSGA-III, MOEA/D-PBI, MOEA/DD, RVEA and MOEA/D-M2M in solving a set of well-known multi-objective optimization problems (MOPs) having 3 to 15 objectives shows that the proposed RD-EMO is superior in converging to approximate Pareto Front (PF) with a standout distribution. We also apply it to solve nine many-objective 0/1 knapsack problems (MKPs), with a good performance obtained. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 22	2020	194								105518	10.1016/j.knosys.2020.105518													
J								GEV-NN: A deep neural network architecture for class imbalance problem in binary classification	KNOWLEDGE-BASED SYSTEMS										Neural networks; Auto-encoder; Gumbel distribution; Imbalanced classification	SAMPLING APPROACH; SMOTE; HYPERTENSION; REGRESSION	Class imbalance is a common issue in many applications such as medical diagnosis, fraud detection, web advertising, etc. Although standard deep learning method has achieved remarkably high-performance on datasets with balanced classes, its ability to classify imbalanced dataset is still limited. This paper proposes a novel end-to-end deep neural network architecture and adopts Gumbel distribution as an activation function in neural networks for class imbalance problem in the application of binary classification. Our proposed architecture, named GEV-NN, consists of three components: the first component serves to score input variables to determine a set of suitable input, the second component is an auto-encoder that learns efficient explanatory features for the minority class, and in the last component, the combination of the scored input and extracted features are then used to make the final prediction. We jointly optimize these components in an end-to-end training. Extensive experiments using real-world imbalanced datasets showed that GEV-NN significantly outperforms the state-of-the-art baselines by around 2% at most. In addition, the GEV-NN gives a beneficial advantage to interpret variable importance. We find key risk factors for hypertension, which are consistent with other scientific researches, using the first component of GEV-NN. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 22	2020	194								105534	10.1016/j.knosys.2020.105534													
J								Object Detection Binary Classifiers methodology based on deep learning to identify small objects handled similarly: Application in video surveillance	KNOWLEDGE-BASED SYSTEMS										Detection; Convolutional neuronal networks; One-Versus-All; One-Versus-One	HANDGUN DETECTION; MULTICLASS; CLASSIFICATION	The capability of distinguishing between small objects when manipulated with hand is essential in many fields, especially in video surveillance. To date, the recognition of such objects in images using Convolutional Neural Networks (CNNs) remains a challenge. In this paper, we propose improving robustness, accuracy and reliability of the detection of small objects handled similarly using binarization techniques. We propose improving their detection in videos using a two level methodology based on deep learning, called Object Detection with Binary Classifiers. The first level selects the candidate regions from the input frame and the second level applies a binarization technique based on a CNN-classifier with One-Versus-All or One-Versus-One. In particular, we focus on the video surveillance problem of detecting weapons and objects that can be confused with a handgun or a knife when manipulated with hand. We create a database considering six objects: pistol, knife, smartphone, bill, purse and card. The experimental study shows that the proposed methodology reduces the number of false positives with respect to the baseline multi-class detection model. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 22	2020	194								105590	10.1016/j.knosys.2020.105590													
J								A novel hybrid grey wolf optimizer algorithm for unmanned aerial vehicle (UAV) path planning	KNOWLEDGE-BASED SYSTEMS										Hybrid meta-heuristic algorithm; Unmanned aerial vehicle (UAV); Path planning	DIFFERENTIAL EVOLUTION ALGORITHM; SYMBIOTIC ORGANISMS SEARCH; GLOBAL OPTIMIZATION; DESIGN	Unmanned aerial vehicle (UAV) path planning problem is an important component of UAV mission planning system, which needs to obtain optimal route in the complicated field. To solve this problem, a novel hybrid algorithm called HSGWO-MSOS is proposed by combining simplified grey wolf optimizer (SGWO) and modified symbiotic organisms search (MSOS). In the proposed algorithm, the exploration and exploitation abilities are combined efficiently. The phase of the GWO algorithm is simplified to accelerate the convergence rate and retain the exploration ability of the population. The commensalism phase of the SOS algorithm is modified and synthesized with the GWO to improve the exploitation ability. In addition, the convergence analysis of the proposed HSGWO-MSOS algorithm is presented based on the method of linear difference equation. The cubic B-spline curve is used to smooth the generated flight route and make the planning path be suitable for the UAV. The simulation experimental results show that the HSGWO-MSOS algorithm can acquire a feasible and effective route successfully, and its performance is superior to the GWO, SOS and SA algorithm. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 22	2020	194								105530	10.1016/j.knosys.2020.105530													
J								Dense connection and depthwise separable convolution based CNN for polarimetric SAR image classification	KNOWLEDGE-BASED SYSTEMS										DSNet; Polarimetric SAR image classification; Convolutional neural networks; Depthwise separable convolution; Dense connection	MODEL	Convolution neural networks (CNN) have achieved great success in natural image processing where large amounts of training data are available. However, for the polarimetric synthetic aperture radar (PolSAR) image classification problem, the number of labeled training samples is typically limited. To improve the performance of CNN on limited training data, we propose a new network, the densely connected and depthwise separable convolutional neural network (DSNet). According to characteristics of PolSAR data, DSNet uses depthwise separable convolution to replace standard convolution, to independently extract features over each channel in PolSAR images. DSNet also introduces dense connections to directly connect non-adjacent layers. With the depthwise separable convolution and dense connections, DSNet can avoid extracting redundant features, reuse the hierarchical feature maps of PolSAR images and reduce the number of training parameters. Compared with normal CNN, DSNet is more lightweight and its training parameters decrease to less than 1/9. We compare DSNet against several popular algorithms on three different data sets, and show that DSNet achieves better results while using less training samples. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 22	2020	194								105542	10.1016/j.knosys.2020.105542													
J								Modeling and multi-neighborhood iterated greedy algorithm for distributed hybrid flow shop scheduling problem	KNOWLEDGE-BASED SYSTEMS										Distributed hybrid flow shop scheduling problem; Iterated greedy algorithm; Multi-neighborhood; Decomposition based heuristic; Makespan	MINIMIZING MAKESPAN; GENETIC ALGORITHM; SEARCH ALGORITHM; OPTIMIZATION; HEURISTICS; METAHEURISTICS; FLOWSHOPS	As economic globalization, large manufacturing enterprises build production centers in different places to maximize profit. Therefore, scheduling problems among multiple production centers should be considered. This paper studies a distributed hybrid flow shop scheduling problem (DHFSP) with makespan criterion, which combines the characteristic of distributed flow shop scheduling and parallel machine scheduling. In the DHFSP, a set of jobs are assigned into a set of identical factories to process. Each job needs to be through same route with a set of stages, and each stage has several machines in parallel and at least one of stage has more than one machine. For solving the DHFSP, this paper proposes two algorithms: DNEH with smallest-medium rule and multi-neighborhood iterated greedy algorithm. The DNEH with smallest-medium rule constructive heuristic first generates a seed sequence by decomposition and smallest-medium rule, and then uses a greedy iteration to assign jobs to factories. In the iterated greedy algorithm, a multi-search construction is proposed, which applies the greedy insertion to the factory again after inserting a new job. Then, a multi-neighborhood local search is utilized to enhance local search ability. The proposed algorithms are evaluated by a comprehensive comparison, and the experimental results demonstrate that the proposed algorithms are very competitive for solving the DHFSP. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 22	2020	194								105527	10.1016/j.knosys.2020.105527													
J								Incremental feature selection for dynamic hybrid data using neighborhood rough set	KNOWLEDGE-BASED SYSTEMS										Feature selection; Rough sets; Attribute reduction; Incremental algorithm; Dynamic data	ATTRIBUTE REDUCTION; UPDATING APPROXIMATIONS; UNCERTAINTY MEASURES; DECISION SYSTEMS; MAINTENANCE; KNOWLEDGE; MODEL; ALGORITHMS	Feature selection with rough sets aims to delete redundant conditional features from static data by considering single type features. However, traditional feature selection methods generally ignore real-world scenarios: hybrid conditional feature set including missing, categorical and numerical ones coexists in the data, and the object set may change dynamically in one by one over time. To deal with dynamic hybrid data with mixed-type features, we propose a neighborhood entropy-based incremental feature selection framework by neighborhood rough set model. In this paper, the dynamics of an object set involves the change of a single object and multiple objects. Therefore, two incremental feature selection algorithms are developed for hybrid data with the dynamic change of a single object and multiple objects, respectively. At first, an incremental manner is utilized to compute the neighborhood entropy as feature criterion. On this basis, the incremental computations of feature significance are used to select candidate features in a descending order. Meanwhile, a deletion strategy is employed to filter out redundant features from the selection results. Finally, experimental results on different real-life data sets demonstrate the proposed incremental algorithms can outperform the non-incremental algorithm for feature selection in speed within comparable classification accuracy. Especially for multiple objects adding into and deleting from the hybrid data, the incremental algorithm is more efficient to select a subset of features than the algorithm for handling the dynamic change of a single object. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 22	2020	194								105516	10.1016/j.knosys.2020.105516													
J								SACPC: A framework based on probabilistic linguistic terms for short text sentiment analysis	KNOWLEDGE-BASED SYSTEMS										Semantic change; Sentiment analysis; Probabilistic linguistic terms; Polarity classification	SETS	Short text sentiment analysis is challenging because short texts are limited in length and lack context. Short texts are usually rather ambiguous because of polysemy and the typos these texts contain. Polysemy is the coexistence of multiple word meanings and commonly appears in every language. Various uses of a word may assign the word both positive and negative meanings. In previous studies, the variability of words is often ignored, which may cause analysis errors. In this study, to resolve this problem, we proposed a novel text representation model named Word2PLTS for short text sentiment analysis by introducing probabilistic linguistic terms sets (PLTSs) and the relevant theory. In this model, every word is represented as a PLTS that fully describes the possibilities for the sentiment polarity of the word. Then, by using support vector machines (SVM), a novel sentiment analysis and polarity classification framework named SACPC is obtained. This framework is a technique that combines supervised learning and unsupervised learning. We compare SAPCP to lexicon-based approaches and machine learning approaches on three benchmark datasets. A noticeable improvement in performance is achieved. To further verify the superiority of SAPCP, state of the art performance comparisons are conducted, and the results are impressive. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 22	2020	194								105572	10.1016/j.knosys.2020.105572													
J								Zero-shot learning by mutual information estimation and maximization	KNOWLEDGE-BASED SYSTEMS										Zero-shot learning; Mutual information; Noise-contrastive estimation; Visual-semantic embedding	MODEL	The key of zero-shot learning is to use the visual-semantic embedding to transfer the knowledge from seen classes to unseen classes. In this paper, we propose to build the visual-semantic embedding by maximizing the mutual information between visual features and corresponding attributes. Then, the mutual information between visual and semantic features can be utilized to guide the knowledge transfer from seen domain to unseen domain. Since we are primarily interested in maximizing mutual information, we introduce the noise-contrastive estimation to calculate lower-bound value of mutual information. Through the noise-contrastive estimation, we reformulate zero-shot learning as a binary classification problem, i.e., classifying the matching visual-semantic pairs (positive samples) and mismatching visual-semantic pairs (negative/noise samples). Experiments conducted on five datasets demonstrate that the proposed mutual information estimators outperforms current state-of-the-art methods both in conventional and generalized zero-shot learning settings. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 22	2020	194								105490	10.1016/j.knosys.2020.105490													
J								Interlayer link prediction in multiplex social networks: An iterative degree penalty algorithm	KNOWLEDGE-BASED SYSTEMS										Social networks; Multiplex network; Interlayer link prediction; Scale-free	USER IDENTIFICATION	Online social network (OSN) applications provide different experiences; for example, posting a short text on Twitter and sharing photographs on Instagram. Multiple OSNs constitute a multiplex network. For privacy protection and usage purposes, accounts belonging to the same user in different OSNs may have different usernames, photographs, and introductions. Interlayer link prediction in multiplex network aims at identifying whether the accounts in different OSNs belong to the same person, which can aid in tasks including cybercriminal behavior modeling and customer interest analysis. Many real-world OSNs exhibit a scale-free degree distribution; thus, neighbors with different degrees may exert different influences on the node matching degrees across different OSNs. We developed an iterative degree penalty (IDP) algorithm for interlayer link prediction in the multiplex network. First, we proposed a degree penalty principle that assigns a greater weight to a common matched neighbor with fewer connections. Second, we applied node adjacency matrix multiplication for efficiently obtaining the matching degree of all unmatched node pairs. Thereafter, we used the approved maximum value method to obtain the interlayer link prediction results from the matching degree matrix. Finally, the prediction results were inserted into the priori interlayer node pair set and the above processes were performed iteratively until all unmatched nodes in one layer were matched or all matching degrees of the unmatched node pairs were equal to 0. Experiments demonstrated that our advanced IDP algorithm significantly outperforms current network structure-based methods when the multiplex network average degree and node overlapping rate are low. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 22	2020	194								105598	10.1016/j.knosys.2020.105598													
J								Learning hierarchical concepts based on higher-order fuzzy semantic cell models through the feed-upward mechanism and the self-organizing strategy	KNOWLEDGE-BASED SYSTEMS										Higher-order fuzzy semantic cell models; Self-organizing map; Feed-upward; Hierarchical abstract concept graphs	CONCEPT LATTICES; INFORMATION GRANULES; COGNITIVE MAPS; MIXTURE-MODELS; REPRESENTATIONS; REDUCTION; PRINCIPLE	Concept representation and learning is a basic topic of artificial intelligence. The aim of this paper is to explore the representation issue and the learning issue of abstract concepts. In this paper, we first introduce higher-order fuzzy semantic cell models to represent abstract concepts, based on which we develop a hierarchical representation of concepts called abstract concept graphs. Then, we put forward an unsupervised algorithm to learn a second-order abstract concept graph from a given data set. This method combines the feed-upward mechanism and the self-organizing strategy. In addition, we provide an evaluation metric for this learning algorithm. A series of experiments is provided to demonstrate the feasibility and validity of the proposed method. We also conduct a preliminary exploration of the potential application of this method to image segmentation. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 22	2020	194								105506	10.1016/j.knosys.2020.105506													
J								CNN tracking based on data augmentation	KNOWLEDGE-BASED SYSTEMS										Visual tracking; Data augmentation; Convolutional neural network	VISUAL TRACKING; OBJECT TRACKING	Correlation filter based tracking methods have aroused increasing attention due to the appealing performance on tracking benchmark datasets. For each frame, a filter is trained to separate the object from its background. Considering that the object always undergoes challenging situations, the trained filter should consider both external and internal distractions. In this paper, we propose a data augmentation based robust visual tracking algorithm to better generalize the training data. Specifically, data augmentation technique is utilized to generate training samples to improve the robustness of the training filter. Then hierarchical convolutional neural network (CNN) features are utilized to encode the target and the augmented sample. Different from previous work, we exploit to use a hash matrix to reduce the dimension of the CNN features. Next, the correlation filter tracking method is employed. The tracking results of multiple hash features are combined to locate the target. Extensive experiments on five large scale datasets show that the proposed method achieves comparable results to state-of-the-art trackers. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 22	2020	194								105594	10.1016/j.knosys.2020.105594													
J								Attention deep neural network for lane marking detection	KNOWLEDGE-BASED SYSTEMS										Deep learning; Autonomous drive; Lane markings detection; Self-attention; Semantic segmentation		Deep learning lane marking detection algorithms based on vision in a complex scene face many challenges, such as absent markings and shadow and dazzle light. The following are the two particularly significant reasons: (1) the empirical size of the receptive fields in the deep neural network (DNN) is considerably smaller than the theoretical one; and (2) the importance of each channel in DNN is not being considered. To address both problems, we propose an attention module that combines self-attention and channel attention (called AMSC) by using a learnable coefficient in parallel. In addition, we apply AMSC in LargeFOV and propose an attention DNN for lane marking detection (modified LargeFOV). Long-range dependencies amongst pixels and channel dependencies are synchronously modelled to capture the global context and strengthen important features in the modified LargeFOV. In comparison with state-of-the-art methods that model dependencies of pixels and channels, our proposed module manifests certain properties, such as inherent parallel computing advantage and needs fewer parameters and convolution operations. Tests on the CULane dataset show that the modified LargeFOV outperforms recurrent neural network and DenseCRF by 3.7% and 5.6%, respectively, with at least 1.6x faster in computation speed, and the AMSC is 10.4x faster than SCNN with minimal performance loss. The modified LargeFOV outperforms the baseline network based on LargeFOV by 1.27% with negligible computational cost and is 1.6x faster than SCNN-LargeFOV(apply SCNN in LargeFOV) with 0.1% performance loss on TuSimple lane marking challenge dataset. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 22	2020	194								105584	10.1016/j.knosys.2020.105584													
J								Graph convolutional networks with multi-level coarsening for graph classification	KNOWLEDGE-BASED SYSTEMS										Graph convolutional networks; Multi-level coarsening; Graph classification		Graph convolutional networks (GCNs) have attracted increasing attention in recent years. Many important tasks in graph analysis involve graph classification which aims to map a graph to a certain category. However, as the number of convolutional layers increases, most existing GCNs suffer from the problem of over-smoothing, which makes it difficult to extract the hierarchical information and global patterns of graphs when learning its representations. In this paper, we propose a multi-level coarsening based GCN (MLC-GCN) for graph classification. Specifically, from the perspective of graph analysis, we develop new insights into the convolutional architecture of image classification. Inspired by this, the two-stage MLC-GCN architecture is presented. In the architecture, we first introduce an adaptive structural coarsening module to produce a series of coarsened graphs and then construct the convolutional network based on these graphs. In contrast to existing GCNs, MLC-GCN has the advantages of learning graph representations at multiple levels while preserving the local and global information of graphs. Experimental results on multiple benchmark datasets demonstrate that the proposed MLC-GCN method is competitive with the state-of-the-art graph classification methods. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 22	2020	194								105578	10.1016/j.knosys.2020.105578													
J								Heterogeneous graph neural networks for noisy few-shot relation classification	KNOWLEDGE-BASED SYSTEMS										Relation extraction; Heterogeneous graph neural networks; Few-shot learning; Adversarial learning		Relation classification is an essential and fundamental task in natural language processing. Distant supervised methods have achieved great success on relation classification, which improve the performance of the task through automatically extending the dataset. However, the distant supervised methods also bring the problem of wrong labeling. Inspired by people learning new knowledge from only a few samples, we focus on predicting formerly unseen classes with a few labeled data. In this paper, we propose a heterogeneous graph neural network for few-shot relation classification, which contains sentence nodes and entity nodes. We build the heterogeneous graph based on the message passing between entity nodes and sentence nodes in the graph, which can capture rich neighborhood information of the graph. Besides, we introduce adversarial learning for training a robust model and evaluate our heterogeneous graph neural networks under the scene of introducing different rates of noise data. Experimental results have demonstrated that our model outperforms the state-of-the-art baseline models on the FewRel dataset. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 22	2020	194								105548	10.1016/j.knosys.2020.105548													
J								An improved emperor penguin optimization based multilevel thresholding for color image segmentation	KNOWLEDGE-BASED SYSTEMS										Color image segmentation; Emperor penguin optimization; Levy flight; Gaussian mutation; Opposition-based learning; Kapur entropy	ALGORITHM; SEARCH; REGION; MODEL	This paper proposes a multi-threshold image segmentation method based on improved emperor penguin optimization (EPO). The calculation complexity of multi-thresholds increases with the increase of the number of thresholds. To overcome this problem, the EPO is used to find the optimal multilevel threshold values for color images. Then, the Gaussian mutation, the Levy flight and the opposition-based learning are employed to increase the search ability of EPO algorithm and balance the exploitation and exploration. The IEPO algorithm optimizes the Kapur's multi-threshold method to conduct experiments on Berkeley images, Satellite images and plant canopy images. As the experimental results show, the IEPO is the effective method for color image segmentation and have higher segmentation accuracy and less CPU time. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 22	2020	194								105570	10.1016/j.knosys.2020.105570													
J								DGI: Recognition of Textual Entailment via dynamic gate Matching	KNOWLEDGE-BASED SYSTEMS										Textual entailment; Dynamic gate inference; Attention mechanism; Natural language processing; Long short-term memory		Recognizing Textual Entailment (RTE) is an integral part of intelligent machines which is able to understand and reason with natural languages. Some special embedding methods such as the attention mechanism exploit the semantic information without considering the features of sentence interaction, which also affect the word-level attention weight when a word appears at multiple positions of a sentence. In this study, we propose a Dynamic Gate Inference model (DGI) to fulfill the RTE task. In the DGI model, different aspects of semantic information are extracted from a premise sentence and a hypothesis sentence by a proposed dynamic gate Matching LSTM structure (gMatch), which combines the word-level fine-grained reasoning mechanism with the sentence-level gating structure to capture the global semantics. The textual relationship between the premise and the hypothesis is inferred by the three categories of attention including direct concatenation, similarity and difference. Extensive experiments were conducted to evaluate the performance of the proposed DGI model in two popular corpus by the metric of accuracy, and the results demonstrate that our approach outperforms the state-of-the-art baseline models in textual entailment in an effective manner. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 22	2020	194								105544	10.1016/j.knosys.2020.105544													
J								Semantics of soft sets and three-way decision with soft sets	KNOWLEDGE-BASED SYSTEMS										Three-way decision; Three-way approximation; Soft set; Fuzzy set; Shadowed set; Interval set	ROUGH SET; CONFLICT-ANALYSIS; SHADOWED SETS; FEATURE-SELECTION; FUZZY-SETS; APPROXIMATIONS; MODEL; COMBINATION; GRANULATION	The theory of three-way decision provides an effective tool for decision-making under uncertainty and incomplete information, when a two-way decision is difficult to make. Soft sets conceptualize and represent special types of uncertainty. In this paper, we suggest two plausible semantics of soft sets for modeling uncertainty, namely, a multi-context semantics and a possible-world semantics. Under the two semantics, we investigate three-way decision under uncertainty represented by soft sets. We introduce a qualitative model of three-way decision based on the core and support of a soft set. We formulate a quantitative model by a) transforming a soft set into a fuzzy set, b) transforming the resulting fuzzy set into a shadowed set (i.e., a three-way approximation of the fuzzy set), and c) making three-way decision with the shadowed set. The results bring additional insights into soft sets, fuzzy sets, interval sets, and shadowed sets for decision making under uncertainty. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 22	2020	194								105538	10.1016/j.knosys.2020.105538													
J								Visual object tracking with adaptive structural convolutional network	KNOWLEDGE-BASED SYSTEMS										Visual tracking; Convolution neural network; Structural filters; Adaptive weighting	REGION	Convolutional Neural Networks (CNN) have been demonstrated to achieve state-of-the-art performance in visual object tracking task. However, existing CNN-based trackers usually use holistic target samples to train their networks. Once the target undergoes complicated situations (e.g., occlusion, background clutter, and deformation), the tracking performance degrades badly. In this paper, we propose an adaptive structural convolutional filter model to enhance the robustness of deep regression trackers (named: ASCT). Specifically, we first design a mask set to generate local filters to capture local structures of the target. Meanwhile, we adopt an adaptive weighting fusion strategy for these local filters to adapt to the changes in the target appearance, which can enhance the robustness of the tracker effectively. Besides, we develop an end-to-end trainable network comprising feature extraction, decision making, and model updating modules for effective training. Extensive experimental results on large benchmark datasets demonstrate the effectiveness of the proposed ASCT tracker performs favorably against the state-of-the-art trackers. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 22	2020	194								105554	10.1016/j.knosys.2020.105554													
J								Learning target-focusing convolutional regression model for visual object tracking	KNOWLEDGE-BASED SYSTEMS										Visual object tracking; Discriminative correlation filters; Target-focusing model; Convolutional regression		Discriminative correlation filters (DCFs) have been widely used in the tracking community recently. DCFs-based trackers utilize samples generated by circularly shifting from an image patch to train a ridge regression model, and estimate target location using a response map generated by the correlation filters. However, the generated samples produce some negative effects and the response map is vulnerable to noise interference, which degrades tracking performance. In this paper, to solve the aforementioned drawbacks, we propose a target-focusing convolutional regression (CR) model for visual object tracking tasks (called TFCR). This model uses a target-focusing loss function to alleviate the influence of background noise on the response map of the current tracking image frame, which effectively improves the tracking accuracy. In particular, it can effectively balance the disequilibrium of positive and negative samples by reducing some effects of the negative samples that act on the object appearance model. Extensive experimental results illustrate that our TFCR tracker achieves competitive performance compared with state-of-the-art trackers. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 22	2020	194								105526	10.1016/j.knosys.2020.105526													
J								Finding influential nodes in social networks based on neighborhood correlation coefficient	KNOWLEDGE-BASED SYSTEMS										Social networks; Influential nodes; Influence range; Information propagation; Susceptible-Infected-Recovered model	COMPLEX NETWORKS; INFLUENCE MAXIMIZATION; H-INDEX; RANKING; IDENTIFICATION; SPREADERS; CENTRALITY; USERS; IDENTIFY; FEATURES	Finding the most influential nodes in social networks has significant applications. A number of methods have been recently proposed to estimate influentiality of nodes based on their structural location in the network. It has been shown that the number of neighbors shared by a node and its neighbors accounts for determining its influence. In this paper, an improved cluster rank approach is presented that takes into account common hierarchy of nodes and their neighborhood set. A number of experiments are conducted on synthetic and real networks to reveal effectiveness of the proposed ranking approach. We also consider ground-truth influence ranking based on Susceptible-Infected-Recovered model, on which performance of the proposed ranking algorithm is verified. The experiments show that the proposed method outperforms state-of-the-art algorithms. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 22	2020	194								105580	10.1016/j.knosys.2020.105580													
J								An attention-guided and prior-embedded approach with multi-task learning for shadow detection	KNOWLEDGE-BASED SYSTEMS										Shadow detection; Multi-task learning; High-level prior; Channel attention		Shadow detection is a fundamental and challenging task, requiring understanding accurately the visual semantic context of the shadow region and backgrounds. In this paper, we propose an attention-guided and prior-embedded approach with multi-task learning for shadow detection task. Different from most existing works, we introduce the effective multi-task learning into this target detection task to add the high-level prior into the detection process, instead of using the pertained weighting network as the front-end module and complex recurrent network. Especially, we also employ a channel attention-guided module to complement the high-level feature and low-level feature. Moreover, for the proposed approach with multi-task learning, we design the weighted loss function for effective training. Experimental results on two public available benchmarks demonstrate our approach achieves competitive results than the existing typical shadow detection approaches. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 22	2020	194								105540	10.1016/j.knosys.2020.105540													
J								PGNet: A Part-based Generative Network for 3D object reconstruction	KNOWLEDGE-BASED SYSTEMS										3D reconstruction; Point cloud generation; Part-based; Semantic reconstruction	SHAPE	Deep-learning generative methods have developed rapidly. For example, various single- and multiview generative methods for meshes, voxels, and point clouds have been introduced. However, most 3D single-view reconstruction methods generate whole objects at one time, or in a cascaded way for dense structures, which misses local details of fine-grained structures. These methods are useless when the generative models are required to provide semantic information for parts. This paper proposes an efficient part-based recurrent generative network, which aims to generate object parts sequentially with the input of a single-view image and its semantic projection. The advantage of our method is its awareness of part structures; hence it generates more accurate models with fine-grained structures. Experiments show that our method attains high accuracy compared with other point set generation methods, particularly toward local details. (C) 2020 Published by Elsevier B.V.																	0950-7051	1872-7409				APR 22	2020	194								105574	10.1016/j.knosys.2020.105574													
J								Obtaining accurate estimated action values in categorical distributional reinforcement learning	KNOWLEDGE-BASED SYSTEMS										Distributional reinforcement learning; Estimated action value; Bootstrapping; Interval estimation		Categorical Distributional Reinforcement Learning (CDRL) uses a categorical distribution with evenly spaced outcomes to model the entire distribution of returns and produces state-of-the-art empirical performance. However, using inappropriate bounds with CDRL may generate inaccurate estimated action values, which affect the policy update step and the final performance. In CDRL, the bounds of the distribution indicate the range of the action values that the agent can obtain in one task, without considering the policy's performance and state-action pairs. The action values that the agent obtains are often far from the bounds, and this reduces the accuracy of the estimated action values. This paper describes a method of obtaining more accurate estimated action values for CDRL using adaptive bounds. This approach enables the bounds of the distribution to be adjusted automatically based on the policy and state-action pairs. To achieve this, we save the weights of the critic network over a fixed number of time steps, and then apply a bootstrapping method. In this way, we can obtain confidence intervals for the upper and lower bound, and then use the upper and lower bound of these intervals as the new bounds of the distribution. The new bounds are more appropriate for the agent and provide a more accurate estimated action value. To further correct the estimated action values, a distributional target policy is proposed as a smoothing method. Experiments show that our method outperforms many state-of-the-art methods on the OpenAI gym tasks. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 22	2020	194								105511	10.1016/j.knosys.2020.105511													
J								A cooperative coevolution algorithm for multi-objective fuzzy distributed hybrid flow shop	KNOWLEDGE-BASED SYSTEMS										Multi-objective fuzzy distributed hybrid flow shop; Fuzzy processing times and due dates; Robustness; Cooperative coevolution algorithm; Estimation of distribution algorithm; Iterated greedy search	ITERATED GREEDY ALGORITHM; SCHEDULING PROBLEM; GENETIC ALGORITHM; TOTAL TARDINESS; OPTIMIZATION; MAKESPAN; MACHINE	With consideration of uncertainty in the distributed manufacturing systems, this paper addresses a multi-objective fuzzy distributed hybrid flow shop scheduling problem with fuzzy processing times and fuzzy due dates. To optimize the fuzzy total tardiness and robustness simultaneously, a cooperative coevolution algorithm with problem-specific strategies is proposed by reasonably combining the estimation of distribution algorithm (EDA) and the iterated greedy (IG) search. In the EDA-mode search, a problem-specific probability model is established to reduce the solution space and a sample mechanism is proposed to generate new individuals. To enhance exploitation, a specific local search is designed to improve performance of non-dominated solutions. Moreover, destruction and reconstruction methods in the IG-mode search are employed for further exploiting better solutions. To balance exploration and exploitation capabilities, a cooperation scheme for mode switching is designed based on the information entropy and the diversity of elite solutions. The effect of the key parameters on the performances of the proposed algorithm is investigated by Taguchi design of experiment method. Comparative results and statistical analysis demonstrate the effectiveness of the proposed algorithm in solving the problem. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 22	2020	194								105536	10.1016/j.knosys.2020.105536													
J								Constrained bilinear factorization multi-view subspace clustering	KNOWLEDGE-BASED SYSTEMS										Multi-view clustering; Subspace clustering; Bilinear factorization; Low-rank representation	ALGORITHM; SYSTEM; SCALE	Multi-view clustering is an important and fundamental problem. Many multi-view subspace clustering methods have been proposed, and most of them assume that all views share a same coefficient matrix. However, the underlying information of multi-view data are not fully exploited under this assumption, since the coefficient matrices of different views should have the same clustering properties rather than be uniform among multiple views. To this end, this paper proposes a novel Constrained Bilinear Factorization Multi-view Subspace Clustering (CBF-MSC) method. Specifically, the bilinear factorization with an orthonormality constraint and a low-rank constraint is imposed for all coefficient matrices to make them have the same trace-norm instead of being equivalent, so as to explore the consensus information of multi-view data more fully. Finally, an Augmented Lagrangian Multiplier (ALM) based algorithm is designed to optimize the objective function. Comprehensive experiments tested on nine benchmark datasets validate the effectiveness and competitiveness of the proposed approach compared with several state-of-the-arts. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 22	2020	194								105514	10.1016/j.knosys.2020.105514													
J								Cross Multi-Type Objects Clustering in Attributed Heterogeneous Information Network	KNOWLEDGE-BASED SYSTEMS										Heterogeneous information network; Clustering; Attributed network		Real-world networks usually consist of a large number of interacting, multi-typed components which are usually referred as heterogeneous information networks (HIN). HIN that associated with various attributes on nodes is defined as attributed HIN (or AHIN). Clustering is a fundamental task for HIN and AHIN. However, most of the current existing methods focus on single type nodes and there is very limited existing work that groups objects of different types into the same cluster. This is largely due to the reasons that object similarities can either be attribute-based or link-based between same type of nodes and it is challenging to incorporate both in a unified framework. To bridge this gap, in this paper, we propose a framework, namely Cross Multi-Type Objects Clustering in Attributed Heterogeneous Information Network, or CMOC-AHIN, to integrate both the attribute information and multi-type node clustering in a principled way. We empirically show superior performances of CMOC-AHIN on three large scale challenging data sets and also summarize insights on the performances compared to other state-of-the-arts methodologies. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				APR 22	2020	194								105458	10.1016/j.knosys.2019.105458													
J								Development of an Improved Rotational Orthosis for Walking With Arm Swing and Active Ankle Control	FRONTIERS IN NEUROROBOTICS										interlimb neural coupling; adjustable admittance control; pole-placement design; active ankle control; gait rehabilitation	GAIT; STROKE	Based on interlimb neural coupling, gait robotic systems should produce walking-like movement in both upper and lower limbs for effective walking restoration. Two orthoses were previously designed in our lab to provide passive walking with arm swing. However, an active system for walking with arm swing is desirable to serve as a testbed for investigation of interlimb neural coupling in response to voluntary input. Given the important function of the ankle joint during normal walking, this work aimed to develop an improved rotational orthosis for walking with arm swing, which is called ROWAS II, and especially to develop and evaluate the algorithms for active ankle control. After description of the mechanical structure and control schemes of the overall ROWAS II system, the closed-loop position control and adjustable admittance control algorithms were firstly deduced, then simulated in Matlab/Simulink and finally implemented in the ROWAS II system. Six able-bodied participants were recruited to use the ROWAS II system in passive mode, and then to estimate the active ankle mechanism. It was showed that the closed-loop position control algorithms enabled the ROWAS II system to track the target arm-leg walking movement patterns well in passive mode, with the tracking error of each joint <0.7 degrees. The adjustable admittance control algorithms enabled the participants to voluntarily adjust the ankle movement by exerting various active force. Higher admittance gains enabled the participants to more easily adjust the movement trajectory of the ankle mechanism. The ROWAS II system is technically feasible to produce walking-like movement in the bilateral upper and lower limbs in passive mode, and the ankle mechanism has technical potential to provide various active ankle training during gait rehabilitation. This novel ROWAS II system can serve as a testbed for further investigation of interlimb neural coupling in response to voluntary ankle movement and is technically feasible to provide a new training paradigm of walking with arm swing and active ankle control.																	1662-5218					APR 22	2020	14								17	10.3389/fnbot.2020.00017													
J								My avatar and I. A study on avatars, personality traits, self-attributes, and their perceived importance	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										3D virtual environments; Avatar; Big-five; Importance of self-attributes; Personality traits; Self-attributes	ATTRACTIVENESS; PERCEPTION; IDENTITY; GENDER; MATTER	The study's objective was to examine the effects of gender, personality traits, and self-views on self-representation through avatars. The importance users attach to self-attributes was also taken into account. The target group was 268 university students. Data for both self- and avatar-attributes were collected using the short version of the Self-Attributes Questionnaire and the users' personality was assessed using the Greek version of the 50-item International Personality Item Pool. The results indicated that the avatars depicted "better" versions of their creators, with females focusing on intellectuality and males focusing on attractiveness. Neurotics and introverts created more socially skilled avatars than themselves. Neurotics also amplified their avatars' attractiveness, whereas extroverts' exaggerated their athletic abilities. Additionally, the avatars of individuals high in openness were more intellectually gifted than themselves, while individuals low in openness created avatars more athletic than themselves. The inclusion of the importance of self-attributes allowed gender differences to emerge and highlighted differences in personality traits. The study's implications are also discussed.																	1868-5137	1868-5145															10.1007/s12652-020-01977-1		APR 2020											
J								Fuzzy-logic threat classification for multi-level selective encryption over real-time video streams	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Classification model; CDN; Edge servers; Fuzzy rule-based system; HEVC and H; 264; AVC codecs; Confidentiality levels; Selective encryption	DELIVERY NETWORKS CHALLENGES; AUTHENTICATION PROTOCOL; IOT DEVICES; H.264/AVC; SYSTEM; CABAC; CRYPTANALYSIS; PERFORMANCE; PROTECTION; ALGORITHM	This paper proposes a Fuzzy-logic Threat Classification (FTC) model as the basis of a method to auto-detect three different confidentiality levels for videos streamed from heterogeneous, mobile devices via web edge servers, possibly part of a Content Distribution Network (CDN). The FTC consists of three parallel Fuzzy Inference Systems (FIS) corresponding to device, network, and type of video application, for the real-time, intelligent selection of an appropriate confidentiality level for a specific end-user. After selection of the level, an encryption module implements the corresponding form of encryption. In tests to demonstrate the concept, there were three increasing confidentiality levels, namely (1) low-level with no encryption, (2) Medium level with an in-house cipher [variant of eXclusive OR (XOR)], named P-XOR (XOR with additional rounds of permutation) applied to Selective Encryption (SE) and (3) high level with the Advanced Encryption Standard again for SE of compressed video syntax components. Results were obtained by considering realistic specifications of multiple digital devices, networks, and differing real-time streaming applications. Visual analysis of encrypted test video clips established that the FTC outputs an appropriate privacy level by reason of the implemented FISs. Absolute encryption times across the privacy levels were distinguished by their real-time response level, which is proportionate to the required degree of confidentiality.																	1868-5137	1868-5145															10.1007/s12652-020-01895-2		APR 2020											
J								Customer behavior analysis using Naive Bayes with bagging homogeneous feature selection approach	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Bagging; BHFS (Bagging Homogenous feature selection); CRM; Feature selection (FS); Naive Bayes (NB); Prediction	CLASSIFICATION	The significant success of an organization greatly depends upon the consumers and their relationship with the organization. The knowledge of consumer behavioral and a excellent understanding of consumer expectations is important for the development of strategic management decisions in support of improving the business value. CRM is intensively applied in the analysis of consumer behavior patterns with the use of Machine Learning (ML) Techniques. Naive Bayes (NB) one of the ML supervised classification models is used to analyze customer behavior predictions. In some domain, the NB performance degrades which involves the existence of redundant, noisy and irrelevant attributes in the dataset, which is a violation of underlying assumption made by naive Bayes. Different enhancements have been suggested to enhance the primary assumption of the NB classifier-independence assumption between the attributes of given class label. In this research, we suggest a simple, straight forward and efficient approach called BHFS (Bagging Homogeneous Feature Selection) which is based upon Ensemble data perturbation feature selection methods. The BHFS method is applied to eliminate the correlated, irrelevant attributes in the dataset and selecting a stable feature subset for improving performance prediction of the NB model. The advantage of the BHFS method requires less running time and selects the best relevant attributes for the evaluation of naive Bayes. The Experimental outcomes demonstrate that the BHFS-naive Bayes model makes better predictions compared to the standard NB. The running time complexity is also less with BHFS-NB since the naive Bayes is constructed using selected features obtained from BHFS.																	1868-5137	1868-5145															10.1007/s12652-020-01961-9		APR 2020											
J								Using Neuro-Evolutionary Techniques to Tune Odometric Navigational System of Small Biomimetic Autonomous Underwater Vehicle - Preliminary Report	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Underwater navigation; Odometry; Underwater vehicle; Evolutionary algorithms		Autonomous underwater vehicles (AUVs) are robots that operate in underwater environment and do not need involvement of an operator when performing some tasks. In order to move independently in water environment, AUVs need navigation capabilities, on the one hand, they have to be able to detect obstacles and avoid them, and on the other hand, they also have to know their own position and spatial orientation, at least course. With regard to the orientation, there are many various solutions like inertial systems, inclinometers, magnetic compasses, optical gyro-compasses, whereas, position due to unavailability of GPS requires solutions dedicated to underwater environment such as inertial navigation. To this end, information about spatial orientation and velocity is necessary. When the vehicle is not equipped with a device to measure velocity, e.g. because of small size of the vehicle itself, the only solution is to use odometry, that is, to apply information from the drive to estimate the velocity. The paper presents Odometric Navigational System (ONS) designed for a small biomimetic autonomous underwater vehicle (BAUV) and tuned by means of neuro-evolutionary techniques. To verify system performance, data from the real BAUV were applied.																	0921-0296	1573-0409				OCT	2020	100	1					363	376		10.1007/s10846-020-01191-3		APR 2020											
J								Automatic plagiarism detection in obfuscated text	PATTERN ANALYSIS AND APPLICATIONS										Automatic plagiarism detection; Obfuscated text; Machine learning; Support vector machine		Plagiarism is a serious problem in education, research, publishing and other fields. Automatic plagiarism detection systems are crucial for ensuring the integrity and genuineness of intellectual work. There are different types of plagiarism, such as copy-paste, obfuscation and translation. In particular, obfuscated text is one of the hardest types of plagiarism to detect. In this paper, we propose an automatic plagiarism detection system for obfuscated text based on a support vector machine classifier that exploits a set of lexical, syntactic and semantic features. We evaluated the performance of the proposed system on benchmark English and Arabic corpora made available by the PAN Workshop series: PAN 2012, PAN 2013, PAN 2014 and PAN@FIRE2015. We also compared the performance of our system to the performances of other systems that participated in the PAN competitions. The obtained results show that our system had the best performance in terms of the F-measure on the PAN 2012 and on the PAN@FIRE2015 obfuscated sub-corpora, was among the top four on the PAN 2013 corpus and was among the top two on the PAN 2014 corpus.																	1433-7541	1433-755X				NOV	2020	23	4					1627	1650		10.1007/s10044-020-00882-9		APR 2020											
J								ASCRClu: an adaptive subspace combination and reduction algorithm for clustering of high-dimensional data	PATTERN ANALYSIS AND APPLICATIONS										High-dimensional data; Subspace clustering; Cluster similarity	DENSITY	The curse of dimensionality in high-dimensional data is one of the major challenges in data clustering. Recently, a considerable amount of literature has been published on subspace clustering to address this challenge. The main objective of the subspace clustering is to discover clusters embedded in any possible combination of the attributes. Previous studies have mostly been generating redundant subspace clusters, leading to clustering accuracy loss and also increasing the running time. In this paper, a bottom-up density-based approach is proposed for clustering of high-dimensional data. We employ the cluster structure as a similarity measure to generate the optimal subspaces which result in raising the accuracy of the subspace clustering. Using this idea, we propose an iterative algorithm to discover similar subspaces using the similarity in the features of subspaces. At each iteration of this algorithm, it first determines similar subspaces, then combines them to generate higher-dimensional subspaces, and finally re-clusters the subspaces. The algorithm repeats these steps and converges to the final clusters. Experiments on various synthetic and real datasets show that the results of the proposed approach are significantly better in both quality and runtime comparing to the state of the art on clustering high-dimensional data. The accuracy of the proposed method is around 34% higher than the CLIQUE algorithm and around 6% higher than DiSH.																	1433-7541	1433-755X				NOV	2020	23	4					1651	1663		10.1007/s10044-020-00884-7		APR 2020											
J								Fuzzy integrated salp swarm algorithm-based RideNN for prostate cancer detection using histopathology images	EVOLUTIONARY INTELLIGENCE										Prostate cancer detection; Local optimal oriented pattern descriptor; Optimization; Histopathology images; Rider neural network		One of the dreadful diseases in the medical industry is prostate cancer and it is growing at a higher rate among men. Hence, it is a necessity to detect cancer in an early stage due to the alarming increase in the reports. Various techniques are introduced for effective prostate cancer detection using histopathology images. Accordingly, an automatic method is proposed for segmenting and classifying prostate cancer. This paper presents the prostate cancer detection method using histopathology images by proposing the fuzzy-based salp swarm algorithm-based rider neural network (SSA-RideNN) classifier. At first, the input image is fed to the pre-processing step and then the segmentation is performed using Color Space transformation and thresholding. Once the segmentation is performed, the feature extraction is done by extracting multiple kernel scale invariant feature transform features along with the texture features that are extracted based on local optimal oriented pattern descriptor to improve the classification accuracy. Finally, the prostate cancer detection is done based on the proposed fuzzy-based SSA-RideNN, which is developed by integrating fuzzy approach with SSA-RideNN. The performance of the proposed fuzzy-based SSA-RideNN is analyzed using sensitivity, specificity, and accuracy. The proposed fuzzy-based SSA-RideNN produces the maximum accuracy of 0.9190, a maximum sensitivity of 0.9084, and maximum specificity of 0.9, indicating its superiority.																	1864-5909	1864-5917															10.1007/s12065-020-00402-y		APR 2020											
J								Learning Description Logic Ontologies: Five Approaches. Where Do They Stand?	KUNSTLICHE INTELLIGENZ										Ontology learning; Description logic; Logic and learning		The quest for acquiring a formal representation of the knowledge of a domain of interest has attracted researchers with various backgrounds into a diverse field called ontology learning. We highlight classical machine learning and data mining approaches that have been proposed for (semi-)automating the creation of description logic (DL) ontologies. These are based on association rule mining, formal concept analysis, inductive logic programming, computational learning theory, and neural networks. We provide an overview of each approach and how it has been adapted for dealing with DL ontologies. Finally, we discuss the benefits and limitations of each of them for learning DL ontologies.																	0933-1875	1610-1987				SEP	2020	34	3			SI		317	327		10.1007/s13218-020-00656-9		APR 2020											
J								Deep learning system to screen coronavirus disease 2019 pneumonia	APPLIED INTELLIGENCE										Pandemic; Viral pneumonias; Convolutional neural networks; Deep learing		Radiographic patterns on CT chest scans have shown higher sensitivity and specificity compared to RT-PCR detection of COVID-19 which, according to the WHO has a relatively low positive detection rate in the early stages. We technically review a study that compared multiple convolutional neural network (CNN) models to classify CT samples with COVID-19, Influenza viral pneumonia, or no-infection. We compare this mentioned study with one that is developed on existing 2D and 3D deep-learning models, combining them with the latest clinical understanding, and achieved an AUC of 0.996 (95%CI: 0.989-1.00) for Coronavirus vs Non-coronavirus cases per thoracic CT studies. They calculated a sensitivity of 98.2% and a specificity of 92.2%.																	0924-669X	1573-7497															10.1007/s10489-020-01714-3		APR 2020											
J								Recommendation decision-making algorithm for sharing accommodation using probabilistic hesitant fuzzy sets and bipartite network projection	COMPLEX & INTELLIGENT SYSTEMS										Bipartite network projection; Probabilistic hesitant fuzzy set; Satisfaction degree; Recommendation decision-making; Sharing accommodation	SENTIMENT ANALYSIS; ONLINE REVIEWS	In recent years, with the uninterrupted development of sharing accommodation, it not only caters to the diversified accommodation of tourists, but also takes an active role in expanding employment and entrepreneurship channels, enhancing the income of urban and rural residents, and promoting the revitalization of rural areas. However, with the continuous expansion of the scale of sharing accommodation, it is fairly complicated for users to search appropriate services or information. The decision-making problems become more and more complicated. Hence, a probabilistic hesitant fuzzy recommendation decision-making algorithm based on bipartite network projection is proposed in this paper. First of all, combining the users' decision-making information and the experts' evaluation information, a bipartite graph connecting users and alternatives is established. Then, the satisfaction degree of probabilistic hesitant fuzzy element is defined. Besides, the recommended alternative is obtained by the allocation of resources. Finally, a numerical case of Airbnb users is given to illustrate the feasibility and effectiveness of the proposed method.																	2199-4536	2198-6053				JUL	2020	6	2					431	445		10.1007/s40747-020-00142-7		APR 2020											
J								Evaluation of feature learning for anomaly detection in network traffic	EVOLVING SYSTEMS										Anomaly detection; Feature learning; Network intrusion detection	SYSTEMS; SUPPORT	The application of anomaly detection approaches to network intrusion detection in real scenarios is difficult. The ability of techniques such as deep learning to estimate new data representations with higher levels of abstraction can be useful to address data analysis of network traffic data. For that reason, the performance of different anomaly detection techniques on feature representations obtained by an autoencoder and a variational autoencoder is compared. We have employed a variety of well-known anomaly detection algorithms, which addresses intrusion detection as a semi-supervised problem where patterns that deviate from a baseline model, estimated only from normal traffic, are labelled as anomalous. Furthermore, this assessment is performed on four publicly available benchmarks. The results show that the effect of feature representation on performance is highly dependent on the anomaly detection technique.																	1868-6478	1868-6486															10.1007/s12530-020-09342-5		APR 2020											
J								A minimum spanning tree based partitioning and merging technique for clustering heterogeneous data sets	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Partitioning and merging approach; Minimum spanning tree based clustering; Box-plot method; Clustering multi-scale datasets	DENSITY; GRAPH; ALGORITHM; ROBUST	Clustering being an unsupervised learning technique, has been used extensively for knowledge discovery due to its less dependency on domain knowledge. Many clustering techniques were proposed in the literature to recognize the cluster of different characteristics. Most of them become inadequate either due to their dependency on user-defined parameters or when they are applied on multi-scale datasets. Hybrid clustering techniques have been proposed to take the advantage of both Partitional and Hierarchical techniques by first partitioning the dataset into several dense sub-clusters and merging them into actual clusters. However, the universality of the partition and merging criteria are not sufficient to capture many characteristics of the clusters. Minimum spanning tree (MST) has been used extensively for clustering because it preserves the intrinsic nature of the dataset even after the sparsification of the graph. In this paper, we propose a parameter-free, minimum spanning tree based efficient hybrid clustering algorithm to cluster the multi-scale datasets. In the first phase, we construct a MST of the dataset to capture the neighborhood information of data points and employ box-plot, an outlier detection technique on MST edge weights for effectively selecting the inconsistent edges to partition the data points into several dense sub-clusters. In the second phase, we propose a novel merging criterion to find the genuine clusters by iteratively merging only the pairs of adjacent sub-clusters. The merging technique involves both dis-connectivity and intra-similarity using the topology of two adjacent pairs which helps to identify the arbitrary shape and varying density clusters. Experiment results on various synthetic and real world datasets demonstrate the superior performance of the proposed technique over other popular clustering algorithms.																	0925-9902	1573-7675				DEC	2020	55	3					587	606		10.1007/s10844-020-00602-z		APR 2020											
J								Toward a robust and fast real-time point cloud registration with factor analysis and Student's-t mixture model	JOURNAL OF REAL-TIME IMAGE PROCESSING										Point cloud registration; Factor analysis; Student's-t mixture model	GO-ICP	Three-dimensional (3D) point cloud registration generally involves in unsatisfied situations like Gaussian white noise, data missing and disorder in affine. This paper proposes a robust and real-time point cloud registration, which combines the Student's-t mixture model (SMM) with factor analysis. The proposed method extending the point cloud mathematical model to the orthogonal factor model and employs the SMM to fit the point cloud data, because the degree of freedom of Student's t-distribution makes it more flexible in fitting the probability distribution of data. Since the Expectation Maximization (EM) algorithm has a stable estimation ability for the mixture model, the EM algorithm is used to estimate the factor load matrix. The filed data and experimental results show that the proposed algorithm can achieve accurate registration and fast convergence even in the case of point cloud disorder, data occlusion, incomplete loss and noise.																	1861-8200	1861-8219															10.1007/s11554-020-00964-1		APR 2020											
J								Channel-independent spatially regularized discriminative correlation filter for visual object tracking	JOURNAL OF REAL-TIME IMAGE PROCESSING										Visual object tracking; Discriminative correlation filter; Alternating direction method of multipliers; Embedded devices		The study proposes the improvements for visual object trackers based on discriminative correlation filters. These improvements consist in the development of the channel-independent spatially regularized method for filter calculation, which is based on the alternating direction method of multipliers as well as in the use of additional features that are the result of the backprojection of normalized weighted object histogram. The VOT Challenge 2018 benchmark has confirmed that the proposed approaches allow to increase the tracking robustness. Particularly, by the value of expected average overlap (EAO = 0.1828), the tracker that uses these approaches (CISRDCF) can reach the level of more computationally complex competitors that utilize convolutional neural features. At the same time, the software-optimized version of the CISRDCF tracker, which implements the suggested improvements has moderate computational complexity and can operate in the real-time both on the PC and on the mid-range ARM-based processors, making the CISRDCF tracker promising for embedded applications.																	1861-8200	1861-8219															10.1007/s11554-020-00967-y		APR 2020											
J								Grey wolf optimization-tuned convolutional neural network for transmission line protection with immunity against symmetrical and asymmetrical power swing	NEURAL COMPUTING & APPLICATIONS										Transmission line protection; Power swings; Convolutional neural network; Grey wolf optimization; Monte Carlo simulations; Real-time validation	FAULT-DETECTION; DISTANCE RELAY; SCHEME; IDENTIFICATION; PERFORMANCE; LOCATION	The similar current-voltage profile during power swing and fault quite often leads to maloperation of distance relays. As compared to symmetrical power swings, discriminating a swing scenario from a fault becomes more challenging during asymmetrical swings arising due to single-pole tripping. Unlike symmetrical power swings, the presence of zero-sequence and negative-sequence current during asymmetrical swing scenarios hinders the application of classical power swing blocking schemes. In this regard, a convolutional neural network (CNN)-based protection scheme has been proposed in this paper, which, in addition to detecting, classifying, and locating faults, is also able to discriminate between power swing (both symmetrical and asymmetrical) and faults. The discrimination avoids possible maloperation during the non-faulty stressed conditions, thereby overcoming the limitation of the existing protection scheme. With the convolutional neural network, the raw signals are directly fed to the classifier, thus avoiding the computational cost associated with feature extraction in time and frequency domains. With the aim of achieving improved input-output mapping capability of CNN for larger datasets, an evolutionary optimization technique, i.e., grey wolf optimization, has been utilized for determining the optimal values of CNN tuning parameters. The performance of the proposed scheme has been extensively validated for a wide range of fault and power swing conditions in terms of standard indices, i.e., dependability, security, and accuracy. The effectiveness of the proposed scheme has also been evaluated for practical setting by performing real-time simulation on OPAL-RT digital simulator.																	0941-0643	1433-3058				NOV	2020	32	22			SI		17059	17076		10.1007/s00521-020-04938-z		APR 2020											
J								Supervised and semi-supervised twin parametric-margin regularized extreme learning machine	PATTERN ANALYSIS AND APPLICATIONS										Extreme learning machine; Semi-supervised learning; Laplacian twin extreme learning machine; Manifold regularization; Successive over-relaxation technology	SUPPORT VECTOR MACHINE; SUCCESSIVE OVERRELAXATION; TSVM	Twin extreme learning machine (TELM) has attracted considerable attention and achieved great success in the machine learning field. However, its performance will be severely affected when outliers exist in the dataset since TELM does not consider heteroscedasticity in practical applications. To improve the performance of TELM, a novel learning framework called twin parametric-margin extreme learning machine (TPMELM) was proposed. Further, to enhance the classification performance of our TPMELM in a semi-supervised learning setting, a Laplacian TPMELM (Lap-TPMELM) was developed by introducing manifold regularization into TPMELM. Using the geometric information of the marginal distribution embedded in unlabeled samples, Lap-TPMELM can effectively construct a more reasonable classifier. The TPMELM and Lap-TPMELM are suitable for many situations, especially when the data has heteroscedastic error structure. Moreover, the TPMELM and Lap-TPMELM are helpful in clarifying theoretical interpretation of parameters which control the bounds on proportions of support vectors and boundary errors. An efficient technique (successive over-relaxation, SOR) is applied in TPMELM and Lap-TPMELM, respectively. Experimental results show the effectiveness and reliability of the proposed methods.																	1433-7541	1433-755X				NOV	2020	23	4					1603	1626		10.1007/s10044-020-00880-x		APR 2020											
J								An Innovative synthesis of deep learning techniques (DCapsNet & DCOM) for generation electrical renewable energy from wind energy	SOFT COMPUTING										DCapsNet; DCOM; Search method; Optimization; Multi-objective optimization; Green energy	OPTIMIZATION; PREDICTION; SYSTEM	Renewable energy becomes one of the main resources that help the world to safety the environment from pollution and provide the people of new type of energy; therefore, this paper presents model called multi-objectives renewable energy-generation (MORE-G) for generating electrical energy from the wind. In general, this model consists of five basic phases: in a first phase collecting and preparing the data, so to make it in format suitable for the decision-making stage, this phase split into multi-steps (i.e., handle missing values and normalization dataset), and the second phase focuses on building constraints for each dataset and develops one of the optimization algorithms called cuckoo based on horizontal combination and multi-objective optimization used in third phase to generate the energy. Another model is developed as multi-layer neural network called (DCapsNet) based on linear combination and multi-objective functions used in the fourth phase to generate the energy. Final phase is related to evaluation of both models (DCOM and DCapsNet) to determine the best. The MORE-G is characterized by addressing one of the real problems, saving on material costs (i.e., reducing the need for manpower and reducing dependence on other countries in importing electric power) and upgrading the scope of the ministry of electricity.																	1432-7643	1433-7479				JUL	2020	24	14					10943	10962		10.1007/s00500-020-04905-9		APR 2020											
J								Time series data analysis of stock price movement using machine learning techniques	SOFT COMPUTING										Stock market; Machine learning; Support vector machine; Artificial neural network; Logistic regression; Technical indicators	ALGORITHM	Stock market also called as equity market is the aggregation of the sellers and buyers. It is concerned with the domain where the shares of various public listed companies are traded. For predicting the growth of economy, stock market acts as an index. Due to the nonlinear nature, the prediction of the stock market becomes a difficult task. But the application of various machine learning techniques has been becoming a powerful source for the prediction. These techniques employ historical data of the stocks for the training of machine learning algorithms and help in predicting their future behavior. The three machine learning algorithms used in this paper are support vector machine, perceptron, and logistic regression, for predicting the next day trend of the stocks. For the experiment, dataset from about fifty stocks of Indian National Stock Exchange's NIFTY 50 index was taken, by collecting stock data from January 1, 2013, to December 31, 2018, and lastly by the calculation of some technical indicators. It is reported that the average accuracy for the prediction of the trend of fifty stocks obtained by support vector machine is 87.35%, perceptron is 75.88%, and logistic regression is 86.98%. Since the stock data are time series data, another dataset is prepared by reorganizing previous dataset into the supervised learning format which improves the accuracy of the prediction process which reported the results with support vector machine of 89.93%, perceptron of 76.68%, and logistic regression of 89.93%, respectively.																	1432-7643	1433-7479				NOV	2020	24	21					16509	16517		10.1007/s00500-020-04957-x		APR 2020											
J								Generalize Robot Learning From Demonstration to Variant Scenarios With Evolutionary Policy Gradient	FRONTIERS IN NEUROROBOTICS										learning from demonstration; generalization; exploration; reinforcement learning; evolutionary algorithms	VIEW	There has been substantial growth in research on the robot automation, which aims to make robots capable of directly interacting with the world or human. Robot learning for automation from human demonstration is central to such situation. However, the dependence of demonstration restricts robot to a fixed scenario, without the ability to explore in variant situations to accomplish the same task as in demonstration. Deep reinforcement learning methods may be a good method to make robot learning beyond human demonstration and fulfilling the task in unknown situations. The exploration is the core of such generalization to different environments. While the exploration in reinforcement learning may be ineffective and suffer from the problem of low sample efficiency. In this paper, we present Evolutionary Policy Gradient (EPG) to make robot learn from demonstration and perform goal oriented exploration efficiently. Through goal oriented exploration, our method can generalize robot learned skill to environments with different parameters. Our Evolutionary Policy Gradient combines parameter perturbation with policy gradient method in the framework of Evolutionary Algorithms (EAs) and can fuse the benefits of both, achieving effective and efficient exploration. With demonstration guiding the evolutionary process, robot can accelerate the goal oriented exploration to generalize its capability to variant scenarios. The experiments, carried out in robot control tasks in OpenAI Gym with dense and sparse rewards, show that our EPG is able to provide competitive performance over the original policy gradient methods and EAs. In the manipulator task, our robot can learn to open the door with vision in environments which are different from where the demonstrations are provided.																	1662-5218					APR 21	2020	14								21	10.3389/fnbot.2020.00021													
J								Exploiting deep representations for natural language processing	NEUROCOMPUTING										Natural language processing; Deep neural networks; Deep representations; Layer aggregation; Routing-by-agreement		Advanced neural network models generally implement systems as multiple layers to model complex functions and capture complicated linguistic structures at different levels [1]. However, only the top layers of deep networks are leveraged in the subsequent process, which misses the opportunity to exploit the useful information embedded in other layers. In this work, we propose to expose all of these embedded signals with two types of mechanisms, namely deep connections and iterative routings. While deep connections allow better information and gradient flow across layers, iterative routings directly combine the layer representations to form a final output with iterative routing-by-agreement mechanism. Experimental results on both machine translation and language representation tasks demonstrate the effectiveness and universality of the proposed approaches, which indicates the necessity of exploiting deep representations for natural language processing tasks. While the two strategies individually boost performance, combining them can further improve performance. (c) 2019ElsevierB.V. Allrightsreserved.																	0925-2312	1872-8286				APR 21	2020	386						1	7		10.1016/j.neucom.2019.12.060													
J								The expressivity and training of deep neural networks: Toward the edge of chaos?	NEUROCOMPUTING										Deep neural networks; Expressivity; Criticality theory; Convergence; Activation function; Hilbert transform	PROPAGATION	Expressivity is one of the most significant issues in assessing neural networks. In this paper, we provide a quantitative analysis of the expressivity for the deep neural network (DNN) from its dynamic model, where the Hilbert space is employed to analyze the convergence and criticality. We study the feature mapping of several widely used activation functions obtained by Hermite polynomials, and find sharp declines or even saddle points in the feature space, which stagnate the information transfer in DNNs. We then present a new activation function design based on the Hermite polynomials for better utilization of spatial representation. Moreover, we analyze the information transfer of DNNs, emphasizing the convergence problem caused by the mismatch between input and topological structure. We also study the effects of input perturbations and regularization operators on critical expressivity. Our theoretical analysis reveals that DNNs use spatial domains for information representation and evolve to the edge of chaos as depth increases. In actual training, whether a particular network can ultimately arrive the edge of chaos depends on its ability to overcome convergence and pass information to the required network depth. Finally, we demonstrate the empirical performance of the proposed hypothesis via multivariate time series prediction and image classification examples. (c) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 21	2020	386						8	17		10.1016/j.neucom.2019.12.044													
J								Multiple people tracking with articulation detection and stitching strategy	NEUROCOMPUTING										Multiple people tracking; Articulation detection; Stitching strategy		Multiple people tracking in a monocular video of crowded scenes is a challenging problem, methods of which are mostly based on tracking-by-detection strategies. The result of detection preprocessing used by many tracking methods to avoid creating wrong targets, is likely to be contaminated when there are defective detections in datasets of benchmark. We propose an articulation-based detection selecting method to screen out detections unqualified for further processing. For the association part of tracking workflow, applying minimax operation can minimize the max intra-distance but results in discontinuous trajectories. We design a stitching strategy to link the tracklets created by minimax algorithm. The experimental results will demonstrate that the proposed method outperforms or is comparable to previous approaches. (c) 2019PublishedbyElsevierB.V.																	0925-2312	1872-8286				APR 21	2020	386						18	29		10.1016/j.neucom.2019.12.037													
J								No-reference screen content image quality assessment based on multi-region features	NEUROCOMPUTING										Image quality assessment; Screen content image; No-reference; Convolutional neural network; Multi-region features; Multi-task training; Ranking loss	NEURAL-NETWORKS; SIMILARITY	Unlike natural image captured with cameras, screen content image (SCI) is a composite image including textual and pictorial regions. The different characteristics lead to many difficulties in image quality assessment (IQA). Most existing models based on the convolutional neural network (CNN) divide large SCIs into image patches to increase training samples for CNN training. This brings two problems: (1) a single image patch can not represent the quality of the entire image, especially in IQA of SCI; (2) SCI patches of an entire image degraded by the same distortion type and strength may have drastically different quality. In addition, these models adopt the mean square error (MSE) between the predicted quality and the subjective differential mean opinion score (DMOS) to train the CNN, without considering quality ranking between different SCIs. In this paper, we propose a novel no-reference (NR) IQA model based on the convolutional neural network (CNN). The contributions of our algorithm can be concluded as follows: (1) considering a large difference exists in different regions in a SCI, the pseudo global features generated with multi-region local features are utilized for quality evaluation, which better reflect image quality than local features of each image patch; (2) the noise classification task is used as an auxiliary task which aids the quality score prediction task to improve the representation ability; (3) the Siamese networks are used to predict the quality scores of two different SCIs, and a new ranking loss is proposed to rank the predicted scores, aiming to enhance the ability of the model to rank image in terms of quality. Experimental results verify that our model outperforms all test NR IQA methods and full-reference (FR) IQA methods on the screen content image quality assessment database (SIQAD). (c) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 21	2020	386						30	41		10.1016/j.neucom.2019.12.027													
J								Incorporating context -relevant concepts into convolutional neural networks for short text classification	NEUROCOMPUTING										Short text classification; Knowledge base; Attention mechanism; Neural networks		Text classification is an important task in natural language processing. Previous text classification models do not perform well on short texts due to the data sparsity problem. In order to solve this problem, recent research extracts concepts of words to enrich text representation. However, this approach might bring general concepts, which might not be helpful in discriminating categories in text classification. Furthermore, it might bring noise into text representation and lead to performance degradation. To tackle these problems, we propose a neural network called DE-CNN, which can incorporate context-relevant concepts into a convolutional neural network for short text classification. Our model firstly utilizes two layers to extract concepts and context respectively and then employs an attention layer to extract contextrelevant concepts. Then the concepts are incorporated into text representation for short text classification. The experimental results on three text classification tasks show that our proposed model outperforms compared state-of-the-art models. (c) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				APR 21	2020	386						42	53		10.1016/j.neucom.2019.08.080													
J								Face inpainting network for large missing regions based on weighted facial similarity	NEUROCOMPUTING										Image inpainting; Face image; Face inpainting; Face similarity; Face inpainting network	OBJECT REMOVAL; IMAGE	Recently, deep learning has made great achievements in the field of image inpainting, especially filling the large missing regions based on generative adversarial net (GAN). However, GAN is the model to capture the data distribution rather than image content. Therefore, for the corrupted face image with large holes, it may generate a new image which is greatly different from the original one. To solve above problem, we present a face inpainting network based on weighted face similarity (WFS-Net) to generate a better restoration. Firstly, according to the structural similarity index (SSIM), a weighted similar face set (WSFS) can be built to provide more reference information for the recovery of missing regions. And then, WFS-Net is designed to fill the damaged face image by exploring the relationship between the missing region and the available information, which include the known parts of the damaged image and its WSFS. Furthermore, a new loss function is presented in pixel level and texture level. The experimental results show that our proposed approach outperforms other state-of-the-arts. (c) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 21	2020	386						54	62		10.1016/j.neucom.2019.12.079													
J								Evaluating QoE in VoIP networks with QoS mapping and machine learning algorithms	NEUROCOMPUTING										Quality of Service; VoIP; Association Test; Quality of Experience; Machine Learning	SPEECH QUALITY; PREDICTION; MODEL; SYSTEM; JITTER	The quality of experience (QoE) of the end-users is a critical criterion of measurement in VoIP (Voice over Internet Protocol) systems for technical and commercial purposes. We investigate how quality of service (QoS) influences QoE and assesses the QoE in VoIP communication. Our contributions are three-fold. First, the impacts of QoS on QoE are comprehensively analyzed by experimental means and an association test method, instead of independently studying each parameter. Second, an algorithm is proposed to integrate the effects of QoS parameters with spatial or temporal characteristics on QoE. Third, we apply machine learning regression algorithms with QoS impairments, noise and echo impairments to nonintrusive voice quality prediction in different network environments. The results from numerous experiments show that fairly accurate prediction can be obtained from these models. Our work will achieve a more accurate evaluation of the QoE in VoIP by using QoS parameters, clarify the influence of IP network environments, noise and echo impairments on the quality and reliability of VoIP traffic, and provide QoS parameter requirements for the VoIP application that runs at the desired QoE level. (c) 2019ElsevierB.V. Allrightsreserved.																	0925-2312	1872-8286				APR 21	2020	386						63	83		10.1016/j.neucom.2019.12.072													
J								Unsupervised Deep Cross -modal Hashing with Virtual Label Regression	NEUROCOMPUTING										Cross-modal retrieval; Deep discrete hashing; Virtual labels		Unsupervised cross-modal hashing learns hash codes without dependence on the semantic labels. It has the desirable advantage of well scalability and thus can effectively support large-scale cross-media retrieval. However, how to directly learn discriminative discrete hash codes under the unsupervised learning paradigm is still an open challenging problem. In this paper, we aim to attack this problem by proposing an Unsupervised Deep Cross-modal Hashing with Virtual Label Regression (UDCH-VLR) method. We propose a novel unified learning framework to jointly perform deep hash function training, virtual label learning and regression. Specifically, we learn unified hash codes via collaborative matrix factorization on the multi-modal deep representations to preserve the multi-modal shared semantics. Further, we incorporate the virtual label learning into the objective functions and simultaneously regress the learned virtual labels to the hash codes. Finally, instead of simply exploiting the existing shallow features and relaxing the binary constraints, we devise an alternative optimization strategy to directly update the deep hash functions and discrete binary codes. Under such circumstance, the discriminative capability of hash codes can be progressively enhanced with iterative learning. Extensive experiments on three publicly available cross-media retrieval datasets demonstrate that our approach outperforms state-of-the-art methods. (c) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 21	2020	386						84	96		10.1016/j.neucom.2019.12.058													
J								Hetero-Center loss for cross -modality person Re-identification	NEUROCOMPUTING										Cross-modality person re-identification; Hetero-Center loss; Local feature		Cross-modality person re-identification is a challenging problem which retrieves a given pedestrian image in RGB modality among all the gallery images in infrared modality. The task can address the limitation of RGB-based person Re-ID in dark environments. Existing researches mainly focus on enlarging inter-class differences of feature to solve the problem. However, few studies investigate improving intraclass cross-modality similarity, which is important for this issue. In this paper, we propose a novel loss function, called Hetero-Center loss (HC loss) to reduce the intra-class cross-modality variations. Specifically, HC loss can supervise the network learning the cross-modality invariant information by constraining the intra-class center distance between two heterogenous modalities. With the joint supervision of Cross-Entropy (CE) loss and HC loss, the network is trained to achieve two vital objectives, inter-class discrepancy and intra-class cross-modality similarity as much as possible. Besides, we propose a simple and high- performance network architecture to learn local feature representations for cross-modality person re-identification, which can be a baseline for future research. Extensive experiments indicate the effectiveness of the proposed methods, which outperform state-of-the-art methods by a wide margin. (c) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 21	2020	386						97	109		10.1016/j.neucom.2019.12.100													
J								A comparative study of general fuzzy min -max neural networks for pattern classification problems	NEUROCOMPUTING										General fuzzy min-max; Fuzzy min-max neural network; Hyperbox fuzzy sets; Pattern classification; Comparative study	LEARNING ALGORITHMS; CLASSIFIERS	General fuzzy min-max (GFMM) neural network is a generalization of fuzzy neural networks formed by hyperbox fuzzy sets for classification and clustering problems. Two principle algorithms are deployed to train this type of neural network, i.e., incremental learning and agglomerative learning. This paper presents a comprehensive empirical study of performance influencing factors, advantages, and drawbacks of the general fuzzy min-max neural network on pattern classification problems. The subjects of this study include (1) the impact of maximum hyperbox size, (2) the influence of the similarity threshold and measures on the agglomerative learning algorithm, (3) the effect of data presentation order, (4) comparative performance evaluation of the GFMM with other types of fuzzy min-max neural networks and prevalent machine learning algorithms. The experimental results on benchmark datasets widely used in machine learning showed overall strong and weak points of the GFMM classifier. These outcomes also informed potential research directions for this class of machine learning algorithms in the future. (c) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 21	2020	386						110	125		10.1016/j.neucom.2019.12.090													
J								Discrete -time zeroing neural network for solving time -varying Sylvester -transpose matrix inequation via exp-aided conversion	NEUROCOMPUTING										Zeroing neural network; Time-varying Sylvester-transpose matrix inequation; ZeaD formula; Discrete-time model; Exp-aided conversion	ONLINE SOLUTION; EQUATION; DYNAMICS; MODELS	Time-varying linear matrix equations and inequations have been widely studied in recent years. Time-varying Sylvester-transpose matrix inequation, which is an important variant, has not been fully investigated. Solving the time-varying problem in a constructive manner remains a challenge. This study considers an exp-aided conversion from time-varying linear matrix inequations to equations to solve the intractable problem. On the basis of zeroing neural network (ZNN) method, a continuous-time zeroing neural network (CTZNN) model is derived with the help of Kronecker product and vectorization technique. The convergence property of the model is analyzed. Two discrete-time ZNN models are obtained with the theoretical analyses of truncation error by using two Zhang et al.'s discretization (ZeaD) formulas with different precision to discretize the CTZNN model. The comparative numerical experiments are conducted for two discrete-time ZNN models, and the corresponding numerical results substantiate the convergence and effectiveness of two ZNN discrete-time models. (c) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 21	2020	386						126	135		10.1016/j.neucom.2019.12.053													
J								Object -based multi -modal convolution neural networks for building extraction using panchromatic and multispectral imagery	NEUROCOMPUTING										Multi-modal convolution neural networks; Building extraction; Remote sensing imagery; Superpixel	CLASSIFICATION	Building extraction is one of the important tasks for urbanization monitoring, city planning, and urban change detection. It is not an easy task due to spectral heterogeneity and structural diversity of the complex backgrounds. In this paper, an object-based multi-modal convolution neural networks (OMM-CNN) is proposed for building extraction using panchromatic and multispectral imagery. Specifically, a multi-modal deep CNN (the multispectral CNN and the panchromatic CNN) architecture is designed which can mine multiscale spectral-spatial contextual information. In order to fully explore the spatial-spectral information embedded in panchromatic and multispectral images, the complex convolution and complex self-adaption pooling layer are developed. Furthermore, to improve the building extraction accuracy and efficiency, a simple linear iterative clustering (SLIC) algorithm is used to segment the panchromatic and multispectral remote sensing imagery simultaneous. Results demonstrated that the proposed method can extract different types of buildings, and the result is more accurate and effective than that of the recent building extraction methods. (c) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 21	2020	386						136	146		10.1016/j.neucom.2019.12.098													
J								Multi-exposure high dynamic range imaging with informative content enhanced network	NEUROCOMPUTING										High dynamic range imaging; Multiple exposure; Informative content enhanced network; Residual channel attention module		For High Dynamic Range (HDR) imaging systems, a new multi-exposure HDR imaging method based on Informative Content Enhanced Network (ICEN) is proposed to overcome the disadvantage that the existing deep learning based methods fails to fully exploit the differently exposed Low Dynamic Range (LDR) image contents to recover the details of the under/over-exposed regions. Specifically, the key contents in differently exposed LDR images that contribute to HDR imaging are firstly defined as Informative Contents (ICs). Then, the ICs are enhanced by the proposed Residual Channel Attention Module (RCAM), and then fused to generate HDR image. Furthermore, a generation scheme is designed for constructing HDR image labels and producing a multi-exposure HDR imaging dataset for training the proposed ICEN. The experimental results show that the proposed method is superior to the existing HDR imaging methods in quantitative and qualitative analysis, and can quickly generate high-quality HDR images. (c) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 21	2020	386						147	164		10.1016/j.neucom.2019.12.093													
J								Robust principal component analysis with intra-block correlation	NEUROCOMPUTING										Robust principle component analysis; Block sparse; Intra-block correlation; Localized low-rank promoting method; Alternating direction method of multipliers; Image denoising	LOW-RANK; ALGORITHMS; IMAGE; PCA	This paper proposes a novel optimization program for solving the Robust Principle Component Analysis (RPCA) problem, which decomposes a data matrix into a conventional low-rank part plus a particular block-sparse residual. This kind of block-sparse residual often scattered in the source signal scope as contaminants, and often existed in many practical applications, such as an ordinary imaging system, a Hyper Spectral Imaging system, EEG and MEG, and types of physiological signals. Different from most currently existing approaches, the study perceived especially a highly spatial correlation among the inner structure of the neighbouring pixels in this contiguously block-sparse residual. The high intra-block correlation is then introduced as prior information to deal the governing optimization problem. In order to enhance the block-sparsity and maintain the local smoothness simultaneously, a localized low-rank promoting method is introduced with a theoretical guarantee. An efficient solving algorithm is designed accordingly with a convergence analysis by adopting the classical Alternating Direction Method of Multipliers (ADMM) framework. In addition to the theoretical model derivation, several synthetic simulations together with a real application on image denoising experiment have been conducted to validate the proposed model. As expected, the models outperforms significantly the compared state-of-the-arts. (c) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 21	2020	386						165	178		10.1016/j.neucom.2019.12.092													
J								Deep learning based QoE evaluation for internet video	NEUROCOMPUTING										QoE; Neural networks; User behavior; Time sequence; Graph-based ranking	QUALITY	The issue of quality of experience (QoE) evaluation in Internet video is challenging for mainly two facts. One is the sophisticated interactions between features from different entities: user, system and context. The other is the heterogeneity of feature type, i.e., sequential and non-sequential characteristics of features. To address above challenges, we propose a hybrid network model that integrates a deep neural network (DNN) and an improved recurrent neural network (RNN) for representation learning of viewlevel QoE evaluation. Non-sequential side information and time difference of sequential features are incorporated to different layers of RNN. Attention mechanism is applied for further improvement of RNN. Based on the output of attention network, we propose a graph-based ranking algorithm to conduct user behavior analyzing, which is useful to online decision of service providers. We conduct experiments on a real-world dataset from a large-scale VoD services provider. The results demonstrate that our method is more effective over the state-of-the-art methods for QoE evaluation. (c) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 21	2020	386						179	190		10.1016/j.neucom.2019.12.082													
J								Towards event-triggere d extende d state observer for multi -agent systems	NEUROCOMPUTING										Multi-agent systems; Extended state observer; Event-triggered; Consensus	DISTURBANCE REJECTION; TIME CONSENSUS; TRACKING; LEADER	This paper deals with the event-triggered leader-follower consensus problem of linear multi-agent systems with disturbances over the directed graph. By using relative output information of the neighboring agents, we develop a novel class of distributed extended state observers. With the estimated values of tracking error and coupled states of the leader's input and disturbances, event-triggered protocols are designed to solve the leader-follower consensus problem. The proposed protocols update intermittently and can suppress disturbances actively. Subsequently, we prove the systems don't exhibit Zeno behavior. Furthermore, both the input of leader and disturbances are considered as unmodeled and not available to any follower. Finally, a simulation example is provided to demonstrate the theoretical results. (c) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 21	2020	386						191	197		10.1016/j.neucom.2019.12.080													
J								Unsupervised feature selection based extreme learning machine for clustering	NEUROCOMPUTING										Extreme learning machine; Unsupervised feature selection; Clustering	REGRESSION; FRAMEWORK	For data with various complicated distribution in the original feature space, it is difficult to find the clusters of the data. Extreme learning machine (ELM) is famous for its universal approximation capability and the hidden space created by random nonlinear feature mapping. Existing ELM based clustering methods address this by constructing an embedding space, in which the cluster are easily revealed. A commonality of them is the final results have to be subsequently derived by k-means clustering. In this paper, we propose an unsupervised feature selection based extreme learning machine (UFSELM) for clustering, which integrates ELM with L2,1 norm regularization to remove the worthless hidden neurons and clusters the data directly without building an embedding. Specifically, the proposed method conducts feature selection by minimizing the L2,1 norm of output weights, and the clustering results is computed by eigendecomposition. By solving the formulated optimization problem in an iterative fashion, we improved the accuracy of clustering. We conducted experiments on several public datasets to demonstrate the effectiveness of the proposed method and further analyzed the properties of the proposed method. (c) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 21	2020	386						198	207		10.1016/j.neucom.2019.12.065													
J								Joint Personalized Markov Chains with social network emb e dding for cold -start recommendation	NEUROCOMPUTING										Markov chains; User cold-start; Temporal information; Social network embedding		The primary objective of recommender systems is to help users select their desired items, where a key challenge is providing high-quality recommendations to users in a "cold-start" situation. Recent advances in tackling this problem combine social relations and temporal information and integrate them into a unified framework. However, these methods suffer from a limitation that there not always exist links for the newcomers, thus these users are filtered in related studies. To break the boundary, in this paper, we propose a Joint Personalized Markov Chains (JPMC) model to address the cold-start issues for implicit feedback recommendation system. In our study, we first utilize user embedding to mine Network Neighbors, so that newcomers without relations can be represented by similar users, then we designed a two-level model based on Markov chains at both user level and user group level respectively to model user preferences dynamically. Experimental results on three real-world datasets have shown that our model can significantly outperform the state-of-the-art models. (c) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 21	2020	386						208	220		10.1016/j.neucom.2019.12.046													
J								Synchronization of semi-Markov coupled neural networks with impulse effects and leakage delay	NEUROCOMPUTING										Coupled neural networks; Memory state feedback controller; Resilient control; Impulse effect	TIME-VARYING DELAYS; H-INFINITY; JUMP SYSTEMS; FEEDBACK-CONTROL; EXPONENTIAL STABILITY; STATE ESTIMATION; ROBUST; STABILIZATION; SATURATION; CONTROLLER	This paper examines the robust synchronization problem for a class of coupled neural networks in mean-square sense subject to semi-Markov jump, outer coupling, leakage and time-varying delay through a resilient memory state feedback controller, wherein the effect of impulse strategy is considered in the system design. Particular, impulsive perturbation and leakage delay are incorporated in the system model cannot be neglected since they make huge impact on the stability behaviour. By endowing Lyapunov's direct method, a newest set of necessary conditions is enhanced in the form of linear matrix inequalities (LMIs) which guarantee that the addressed system can realize synchronization criterion. Based on these obtained LMIs, the explicit characterization of the designed controller is retrieved interms of linear matrix inequalities by using the convex optimization algorithm. Eventually, the analytical design are voiced by a numerical example which substantiate the superiority and efficiency of the proposed control design strategy. (c) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 21	2020	386						221	231		10.1016/j.neucom.2019.12.097													
J								Pairwise registration of TLS point clouds by deep multi -scale local features	NEUROCOMPUTING										MSSNet; Point cloud registration; Terrestrial laser scanning (TLS); Data augmentation; Geometric constraints	AUTOMATED REGISTRATION; GO-ICP	Because of the mechanism of TLS system, noise, outliers, various occlusions, varying cloud densities, etc. inevitably exist in the collection of TLS point clouds. To achieve automatic TLS point cloud registration, many methods, based on the hand-crafted features of keypoints, have been proposed. Despite significant progress, the current methods still face great challenges in accomplishing TLS point cloud registration. In this paper, we propose a multi-scale neural network to learn local shape descriptors for establishing correspondences between pairwise TLS point clouds. To train our model, data augmentation, developed on pairwise semi-synthetic 3D local patches, is to extend our network to be robust to rotation transformation. Then, based on varying local neighborhoods, multi-scale subnetworks are constructed and fused to learn robust local features. Experimental results demonstrate that our proposed method successfully registers two TLS point clouds and outperforms state-of-the-art methods. Besides, our learned descriptors are invariant to translation and tolerant to changes in rotation. (c) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 21	2020	386						232	243		10.1016/j.neucom.2019.12.074													
J								BLF-based adaptive DSC for a class of stochastic nonlinear systems of full state constraints with time delay and hysteresis input	NEUROCOMPUTING										Symmetric and asymmetric BLFs; DSC; Full state constraints; Time delay; Hysteresis input; Neural networks	BARRIER LYAPUNOV FUNCTIONS; STRICT-FEEDBACK SYSTEMS; NEURAL-NETWORK CONTROL; FUZZY CONTROL; INTERCONNECTED SYSTEMS; TRACKING CONTROL; CONTROL DESIGN; STABILIZATION; ORDER; STABILITY	This paper is concerned about barrier Lyapunov function(BLF)-based adaptive dynamic surface control(DSC) for a class of stochastic nonlinear systems of full state constraints with hysteresis input and time delay. All states of the system are guaranteed to be constrained in bounded compact set. The pure feedback nonlinear system is transformed into a strictly feedback nonlinear system with nonaffine nonlinear terms by using the mean value theorem. By using backstepping design of BLF combine with DSC technique, the explosion of complexity is avoided. Based on the neural network with the finite covering lemma, the time-delay functions are solved. Simultaneously, the backlash-like hysteresis input control in this paper is considered. The two cases of symmetric and asymmetric BLFs are discussed separately. An adaptive controller is designed to ensure that the output tracking error converges on a small region of the origin. Finally, the control scheme ensures that all signals in the closed-loop systems are semi-global uniformly ultimately bounded. Results of two simulation cases are presented to prove the effectivity of the theoretical analysis. (c) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 21	2020	386						244	256		10.1016/j.neucom.2019.12.102													
J								Multi -objective evolutionary framework for non-linear system identification: A comprehensive investigation	NEUROCOMPUTING										Evolutionary algorithms; Multi-criteria decision making; Model structure selection; NARX model; Pareto optimality; System identification	NON-LINEAR SYSTEMS; OUTPUT PARAMETRIC MODELS; MULTIOBJECTIVE OPTIMIZATION; ORTHOGONAL REGRESSION; STRUCTURE SELECTION; DYNAMIC-SYSTEMS; NARX MODELS; PART I; ALGORITHM; INPUT	The present study proposes a multi-objective framework for structure selection of nonlinear systems which are represented by polynomial NARX models. This framework integrates the key components of Multi-Criteria Decision Making (MCDM) which include preference handling, Multi-Objective Evolutionary Algorithms (MOEAs) and a posteriori selection. To this end, three well-known MOEAs such as NSGA-II, SPEA-II and MOEA/D are thoroughly investigated to determine if there exists any significant difference in their search performance. The sensitivity of all these MOEAs to various qualitative and quantitative parameters, such as the choice of recombination mechanism, crossover and mutation probabilities, is also studied. These issues are critically analyzed considering seven discrete-time and a continuous-time benchmark nonlinear system as well as a practical case study of non-linear wave-force modeling. The results of this investigation demonstrate that MOEAs can be tailored to determine the correct structure of nonlinear systems. Further, it has been established through frequency domain analysis that it is possible to identify multiple valid discrete-time models for continuous-time systems. A rigorous statistical analysis of MOEAs via performance sweet spots in the parameter space convincingly demonstrates that these algorithms are robust over a wide range of control parameters. (c) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 21	2020	386						257	280		10.1016/j.neucom.2019.12.095													
J								Reading into the mind?s eye: Boosting automatic visual recognition with EEG signals	NEUROCOMPUTING										EEG; BCI; Object recognition; Computer vision; Deep learning; Neural networks; LSTMs; CNNs; Gabor filters	COMPUTER; CLASSIFICATION; COMMUNICATION	Classifying visual information is an apparently simple and effortless task in our everyday routine, but can we automatically predict what we see from signals emitted by the brain? While other researchers have already attempted to answer this question, we are the first to show that a commercially available BCI could be effectively used for visual image classification in real-world scenarios - when testing takes place at a completely different time than training data collection. The task is difficult, as it requires relating the noisy and low-level EEG signals to complex and highly semantic visual categories. In this paper, we propose different learning approaches and show that simpler classifiers such as Ridge Regression with Gabor filtering of the input EEG signal could be more effective than the powerful Long Short Term Memory Networks and Convolutional Neural Networks in this case of limited and noisy training data. We analyzed the importance of each electrode for the visual classification task and noticed that the sensors with the highest accuracy were the ones that recorded brain activity from regions known to be correlated more with higher level recognition and cognitive processes and less to lower-level visual signal processing. The result is also in accordance with research in computer vision with deep neural networks, which shows that semantic visual features are learned only at higher levels of neural depth. While EEG signals are weaker by themselves for the task of visual classification, we demonstrate that they could be powerful when combined with deep visual features extracted from the image, improving performance from 91% to over 97% in a multi-class recognition setting. Our tests show that EEG input brings additional information that is not learned by artificial deep networks on the given image training set. Thus, a commercially available BCI could be effectively used in conjunction with a deep learning based vision system to form together a stronger visual recognition system that is suitable for real-world applications. (c) 2020 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license. (http://creativecommons.org/licenses/by-nc-nd/4.0/)																	0925-2312	1872-8286				APR 21	2020	386						281	292		10.1016/j.neucom.2019.12.076													
J								Traffic scene semantic segmentation using self -attention mechanism and bi-directional GRU to correlate context	NEUROCOMPUTING										Semantic segmentation; Context; Self-attention; Gated recurrent unit	NETWORK; VIDEO	Context information plays an important role in semantic segmentation of urban traffic scenes, which is one of the key tasks of the intelligent platform's (such as unmanned vehicles) perceiving environment, and has inspired a wide range of interests from researchers. This paper synthesizes three considerations: feature space correlation, information distributed in the long distance of image plane and long distance sequence information, and proposes a combination of self-attention mechanism and bi-directional gated recurrent unit (GRU) neural network to extract various contextual information on the basis of deep feature network, so as to achieve better semantic segmentation performance. In order to explore the optimal implementation, two kinds of topological connections are attempted. One is self-attention branch and bidirectional GRU branch in series, and the other is in parallel. In addition, in order to train the network better and achieve more precise segmentation results, a cascade refinement supervised method using two losses is proposed. Experiments carried out on Cityscapes, Mapillary, CamVid and KITTI semantic segmentation datasets demonstrate the outstanding performance and robust generalization ability of our method. (c) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 21	2020	386						293	304		10.1016/j.neucom.2019.12.007													
J								Normalization of face illumination with photorealistic texture via deep image prior synthesis	NEUROCOMPUTING										Face illumination normalization; Photorealistic texture; Deep image prior synthesis; Illumination regression filter; Low-rank decomposition	RECOGNITION; REFLECTANCE; MODELS	Illumination normalization for face recognition is of great importance when the face image is taken under a harsh lighting condition and only one single image is available. In this paper, an illumination normalization method that can preserve face identity and make the processed image have photorealistic face texture is proposed. First, an illumination regression filter is proposed to remove a large number of illumination components of the input image. Then, the accelerated proximal gradient algorithm is used for low-rank decomposition of the filtered image, which further reduces the residual illumination and noise of the face image. However, after these two operations, the face image has a peeled-offappearance. To avoid the unrealistic appearance, a deep image prior synthesis process is developed to synthesize the photorealistic face texture. To accomplish this task, the filtered image, the low-rank decomposed image, the original image and any a uniformly illuminated frontal face image as reference are used as synthetic ingredients. These ingredients, as prior knowledge, synthesize a face image that has the texture features of the reference image and retains the identity of the input image. We try to bridge the appearance gap between the synthetic ingredients and the final synthetic image by a deep neural network. Instead of training the network on a large dataset, the final synthetic image is generated by iterating the parameters of the generator-structured network according to a certain proportion of the synthetic ingredients. Systematic evaluations conducted on public databases demonstrate that the proposed method is robust to illumination with better performance than state-of-the-art methods. (c) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 21	2020	386						305	316		10.1016/j.neucom.2019.12.101													
J								Spatio-temporal DenseNet for real-time intent prediction of pedestrians in urban traffic environments	NEUROCOMPUTING										Pedestrian; Intent; Autonomous vehicles	APPEARANCE	Autonomous ground vehicles are increasingly finding their way into real-life applications, ranging from food/parcel delivery to self-driving vehicles. Given that, understanding the behaviours and intentions of humans are still one of the main challenges autonomous ground vehicles faced with. More specifically, when it comes to complex environments such as urban traffic scenes, inferring the intentions and actions of vulnerable road users such as pedestrians become even harder. In this paper, we address the problem of intent action prediction of pedestrians in urban traffic environments using only image sequences from a monocular RGB camera. We propose a real-time framework that can accurately detect, track and predict the intended actions of pedestrians based on a tracking-by-detection technique in conjunction with a novel spatio-temporal DenseNet model. We trained and evaluated our framework based on real data collected from urban traffic environments. Our framework has shown resilient and competitive results in comparison to other baseline approaches. Overall, we achieved an average precision score of 84.76% with a real-time performance at 20 FPS. (c) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				APR 21	2020	386						317	324		10.1016/j.neucom.2019.12.091													
J								Nonlinear gated channels networks for action recognition	NEUROCOMPUTING										Convolutional neural network; Action recognition; Network training; Network convergence; Long range temporal property	TERM	Video-based convolutional neural networks (CNNs) involve a large amount of training parameters, leading to the enormous computational complexity, which thereby delays the network convergence. Therefore, training successful CNN for action recognition rapidly is non-trivial. In this paper, a novel encoding called nonlinear gated channels unit (NGCU) is proposed to facilitate network training by encoding the global channel-level relationship. Based on this, nonlinear gated channels network (NGCN) is constructed for the end-to-end encoding, and the corresponding convergence performance is evaluated on the standard benchmarks UCF101 and HMDB51. Experimental results demonstrate that the proposed method is conducive to the convergence process of CNN based action recognition models. (c) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				APR 21	2020	386						325	332		10.1016/j.neucom.2019.12.077													
J								Efficient data sensing with group key management for intelligent automation system by one-way key derivation in wireless networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Multicast security; Multiple logical key trees; Group key management; One-way key derivation; Rekeying process		Over the last century, Wireless Sensor Network (WSN) has given major advances in several distinguishable fields. In addition, it has also evolved many features such as high device heterogeneity, high scale and supporting multiple applications. In WSN, Key management is employed (involves) as a significant component of network security, especially in multi-cast based applications and services. Group key management has a vital role in multi-cast secure communication with huge number of group members. Group key must always be updated dynamically to all groups in the network and keys to be redistributed currently to active members only. Consequently, a secret key called session key for a group of members is stored in key trees that are shared efficiently between them, in order to achieve secure group communication. The key trees are used to encrypt with other keys and data are transmitted to ensure security. Logical key hierarchy (LKH) is a hierarchical structure of multiple keys to provide a scalable and secrecy for group communication. This research work proposes a new hierarchical group key management (HGKM) using multiple logical key trees for dynamic groups in order to enhance the Quality of Service of the network. Among the multiple logical trees, primary tree can be selected by tree selection algorithm. In WSN, there are high numbers of members joining or leaving the group at any time, resulting with rekeying process becoming too large. This leads to degraded efficiency of tree-based key management system. Further, communication overhead of rekeying process can be mitigated by one-way key derivation method which integrates with multiple logical key trees. In this method, tree server does not need to be encrypted. Rather, it involves in sending a new key to members who are capable of derive their own keys in same way the server does. This results with a need for less number of encrypted keys for each rekeying process within the group tree. The fixed interval time is called batch time, which is applied in each rekeying process after nodes join or leave the group. The proposed approach can improve the bandwidth utilization effectively. The proposed HGKM method is implemented in Network Simulator (NS-2) environment and obtained results are compared with existing two methods such as LKH and multiple logical tree key management (MLT-KM). QOS of proposed approach is evaluated in terms of Bandwidth Efficiency.																	1868-5137	1868-5145															10.1007/s12652-020-01862-x		APR 2020											
J								Robust angle estimation algorithm for MIMO radar based on tensor analysis and reduced-dimension subspace reconstruction	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Angle estimation; MIMO radar; Reduced-dimension subspace reconstruction; High-order singular value decomposition (HOSVD); Robustness	MULTIDIMENSIONAL HARMONIC RETRIEVAL; ESTIMATION ACCURACY; DOA ESTIMATION; IMPROVE; ESPRIT	A robust angle estimation algorithm based on tensor analysis and reduced-dimension subspace reconstruction is proposed for the noncircular signals with unknown mutual coupling in MIMO radar. The proposed algorithm constructs a special tensor analysis to capture the non-circular characteristics and multi-dimensional structure of non-circular signals, and can eliminate the influence of mutual coupling in the tensor domain. The signal subspace is obtained by using high-order singular value decomposition (HOSVD), and the direction-of-departure (DOD) of the target is obtained by combining the real value subspace estimation of direction of arrival (DOA). In addition, according to the working principle and echo model of bistatic MIMO radar, combined with array signal processing, the angle estimation algorithm of reduced-dimension subspace reconstruction is proposed, which can effectively solve the angle estimation problem of bistatic MIMO radar, reduce the calculation cost, improve the robustness of the algorithm, and solve the problem that angle matching cannot be realized after dimension reduction. The simulation results show that the algorithm proposed in this paper has good performance in recognition.																	1868-5137	1868-5145															10.1007/s12652-020-01959-3		APR 2020											
J								Joint model of entity recognition and relation extraction based on artificial neural network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Deep learning; Long short-term memory; Natural language processing; Neural network		Entity and relationship extraction is an important step in building a knowledge base, which is the basis for many artificial intelligence products to be used in life, such as Amazon Echo and Intelligent Search. We propose a new artificial neural network model to identify entities and their relationships without any handcrafted features. The neural network model mainly includes the CNN module for extracting text features and relationship classifications, and a bidirectional LSTM module for obtaining context information of the entity. The context information and entity tags between the entities obtained in the entity identification process are further passed to the CNN module of the relationship classification to improve the effectiveness of the relationship classification and achieve the purpose of joint processing. We conducted experiments on the public datasets CoNLL04 (Conference on Computational Natural Language Learning), ACE04 and ACE05 (Automatic Content Extraction program) to verify the effectiveness of our approach. The method we proposed achieves the state-of-the-art results on entity and relation extraction task.																	1868-5137	1868-5145															10.1007/s12652-020-01949-5		APR 2020											
J								Sound quality prediction for power coupling mechanism of HEV based on CEEMD-HT and RVM	NEURAL COMPUTING & APPLICATIONS										Sound quality; Complementary ensemble empirical mode decomposition; Hilbert transform; Relevance vector machine	EMPIRICAL MODE DECOMPOSITION; NOISE	The sound quality evaluation model based on complementary ensemble empirical mode decomposition (CEEMD), Hilbert transform (HT) and relevance vector machine (RVM) is proposed to predict the sound quality of acoustical signals at power coupling mechanism of a hybrid electric vehicle (HEV). The technique is applied with wave filtering and CEEMD of the acoustical signals acquired at power coupling mechanism of HEV to achieve the intrinsic mode function (IMF) components. Built upon this is the calculation of the instantaneous frequencies of the IMF components by HT. Then, the critical frequency bands are used as the weight to calculate the weighted energy values of the IMF components. The weighted energy values are used as the new inputs of the sound quality evaluation model. Afterward, the subjective evaluation experiment of the acoustical signals at the power coupling mechanism is carried out based on pairwise comparison method. The subjective evaluation values are used as the outputs of the evaluation model. Finally, the new evaluation model is established based on RVM. In addition, the second RVM model is built for sound quality evaluation with the psychoacoustic objective parameters as the inputs. In this paper, the sound samples acquired under steady- and unsteady-state operating conditions are tested, respectively, in two models to obtain the prediction results. The prediction result suggests that the prediction accuracy of the evaluation model based on CEEMD-HT is higher than the evaluation model based on psychoacoustic objective parameters, and the prediction accuracy of the steady sound samples is higher than the unsteady sound samples.																	0941-0643	1433-3058															10.1007/s00521-020-04934-3		APR 2020											
J								A reformative teaching-learning-based optimization algorithm for solving numerical and engineering design optimization problems	SOFT COMPUTING										Evolutionary algorithm; Swarm intelligence based algorithm; Unconstrained numerical optimization; Constrained engineering optimization; Teaching-learning-based optimization algorithm	DIFFERENTIAL EVOLUTION; GLOBAL OPTIMIZATION	Teaching-learning-based optimization (TLBO) algorithm, which simulates the process of teaching-learning in the classroom, has been studied by many researchers, and a number of experiments have shown that it has great performance in solving optimization problems. However, it has an inherent origin bias in teacher phase and may fall into local optima for solving complex high-dimensional optimization problems. Therefore, an improved teaching method is proposed to eliminate the bias of converging toward the origin and enhance the ability of exploration during the convergence process. And a self-learning phase is presented to maintain the ability of exploration after convergence. Besides, a mutation phase is introduced to provide a good mixing ability among the population, preventing premature convergence. As a result, a reformative TLBO (RTLBO) algorithm with three modifications, an improved teaching method, a self-learning phase and a mutation phase, is proposed to significantly improve the performance of the TLBO algorithm. Ten unconstrained benchmark functions and three constrained engineering design problems are employed to evaluate the performance of the RTLBO algorithm. The results of the experiments show that the RTLBO algorithm is of excellent performance and better than, or at least comparable to, other available optimization algorithms in literature.																	1432-7643	1433-7479				OCT	2020	24	20					15889	15906		10.1007/s00500-020-04918-4		APR 2020											
J								XACBench: a XACML policy benchmark	SOFT COMPUTING										Access control; XACML; Synthetic policy; Security policy; Policy evaluation		XACML standard defines a declarative language to determine access control policies which are critical for deploying security solutions. It is important to evaluate the performance of policies defined by XACML, for applications such as policy enforcement efficiency, policy refinement, anomaly detection, conflict resolution, and policy similarity assessment. Due to security and confidentiality reasons, at hands policy sets for such evaluations are very rare. Moreover, these policy sets are created gradually, thus access to large and effective policy sets in a short time is challenging and daunting task. In this paper, we present XACBench, a suite of tools for both generating synthetic XACML policies and benchmarking the policy evaluation algorithms. To this end, XACBench first extracts, models and generalizes some statistical properties of an input policy which is called policy profile. Such profile helps generating policies in a way that accurately simulates the statistic properties of the input policy. XACBench then generates synthetic policies of any desired length based on the profile. It also provides a simple mechanism for controlling the correlation between the generated policies and the input policy with respect to the extracted policy profile. Experimental results demonstrate that our approach is efficient and scalable to various policy lengths as well as input policies.																	1432-7643	1433-7479				NOV	2020	24	21					16081	16096		10.1007/s00500-020-04925-5		APR 2020											
J								New similarity and entropy measures of single-valued neutrosophic sets with applications in multi-attribute decision making	SOFT COMPUTING										Single-valued neutrosophic set; Inclusion relation; Similarity measure; Entropy; Multi-attribute decision making	AGGREGATION OPERATORS; CROSS-ENTROPY; ALGORITHM; FILTERS	Information measures play a fundamental role in single-valued neutrosophic set (SVNS) theory. The main purpose of this paper is to study the similarity and entropy measures of SVNS with applications in multi-attribute decision making. We proposed the axiomatic definitions of similarity and entropy for single-valued neutrosophic values (SVNVs) with respect to a new kind of inclusion relation between SVNVs. On the basis of Hamming distance, cosine function and cotangent function, three similarity measures and three entropies for SVNVs are constructed. Then, we extended the definitions and construction methods of similarity and entropy for SVNVs to SVNSs by using some aggregation operators. Finally, by using the new similarity and entropy measures we presented a SVNSs based multi-attribute decision making method. It demonstrated that the new information measures presented in this study are applicable and efficient.																	1432-7643	1433-7479				NOV	2020	24	21					16165	16176		10.1007/s00500-020-04930-8		APR 2020											
J								A survey of the recent architectures of deep convolutional neural networks	ARTIFICIAL INTELLIGENCE REVIEW										Deep learning; Convolutional neural networks; Taxonomy; Representational capacity; Residual learning; Channel boosted CNN	ACTION RECOGNITION; RECEPTIVE-FIELDS; FUNCTIONAL ARCHITECTURE; IMAGE; ENSEMBLE; DATASET; NETS; LSTM	Deep Convolutional Neural Network (CNN) is a special type of Neural Networks, which has shown exemplary performance on several competitions related to Computer Vision and Image Processing. Some of the exciting application areas of CNN include Image Classification and Segmentation, Object Detection, Video Processing, Natural Language Processing, and Speech Recognition. The powerful learning ability of deep CNN is primarily due to the use of multiple feature extraction stages that can automatically learn representations from the data. The availability of a large amount of data and improvement in the hardware technology has accelerated the research in CNNs, and recently interesting deep CNN architectures have been reported. Several inspiring ideas to bring advancements in CNNs have been explored, such as the use of different activation and loss functions, parameter optimization, regularization, and architectural innovations. However, the significant improvement in the representational capacity of the deep CNN is achieved through architectural innovations. Notably, the ideas of exploiting spatial and channel information, depth and width of architecture, and multi-path information processing have gained substantial attention. Similarly, the idea of using a block of layers as a structural unit is also gaining popularity. This survey thus focuses on the intrinsic taxonomy present in the recently reported deep CNN architectures and, consequently, classifies the recent innovations in CNN architectures into seven different categories. These seven categories are based on spatial exploitation, depth, multi-path, width, feature-map exploitation, channel boosting, and attention. Additionally, the elementary understanding of CNN components, current challenges, and applications of CNN are also provided.																	0269-2821	1573-7462				DEC	2020	53	8					5455	5516		10.1007/s10462-020-09825-6		APR 2020											
J								An Adaptive Fuzzy Predictive Controller with Hysteresis Compensation for Piezoelectric Actuators	COGNITIVE COMPUTATION										Adaptive fuzzy model; Feedforward-feedback control; Hysteresis compensation; Model predictive control (MPC); Piezoelectric actuators (PEAs)	DESIGN; SYSTEMS	Piezoelectric actuators (PEAs) are the pivotal components of many nanopositioning systems because of their superiorities in bandwidth, mechanical force, and precision. Unfortunately, the intrinsic nonlinear property, hysteresis, makes it difficult to achieve the precise control of PEAs. Considering this drawback, diversified feedback control approaches have been studied in the literature. Inspired by the idea that the involvement of feedforward terms can upgrade the tracking performance, our previous conference paper proposed a novel feedforward-feedback control approach (model predictive control with hysteresis compensation). Following the previous work, an adaptive fuzzy predictive controller with hysteresis compensation is further studied in this paper. The major improvement of the proposed method is the employment of adaptive fuzzy model, by which the dynamic model of PEAs is able to adjust in real time, resulting in a better control performance. To validate the effectiveness of the proposed method, extensive experiments are conducted on a Physik Instrumente P-753.1CD piezoelectric nanopositioning stage. Comparisons with several existing control approaches are carried out, and the root mean square tracking error of the proposed method is reduced to 30% of that under the previously proposed neural network model-based predictive control, when tracking 100 Hz sinusoidal reference.																	1866-9956	1866-9964				JUL	2020	12	4					736	747		10.1007/s12559-020-09722-8		APR 2020											
J								Grey Absolute Decision Analysis (GADA) Method for Multiple Criteria Group Decision-Making Under Uncertainty	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Multi-attribute decision-making; Group decision-making; Absolute degree grey incidence; GADA Index; GADA weights; GADA method; Incomplete data	MCDM; MANAGEMENT; MODEL; WEIGHTS; NUMBERS	The study proposes a novel, convenient and dimensionless model of multi-criteria decision-making (MCDM), hereby referred to as Grey Absolute Decision Analysis (GADA) method. The foundation of the GADA method rests upon the Absolute Grey Relational Analysis (Absolute GRA) model and the system that the method follows to produce GADA Indexes and GADA Weights. The GADA Weights represent the relative weights of decision alternatives under given criteria. The method can handle both positive ("higher the better") and negative ("lower the better") criteria simultaneously in its algorithm. The method can deal with problems involving uncertainty and incomplete data. Two practical cases have been presented in the study to demonstrate the feasibility of the method. Furthermore, the GADA Weights obtained for the cases show that these values are comparable to the relative weights obtained through the traditional methods like AHP and SAW thus signifying the feasibility of the method. However, the conventional methods do not consider the mutual association between the judgments of the members of decision-making group (experts' opinions), a weakness that the proposed method overwhelms. Therefore, the overall ranking obtained from the proposed method is acceptable, especially under the uncertain environment where the nature of mutual association between the judgments is not precise. The key benefit of the method lies in its adaptability to different scales of measurement. Also, it can provide relative weights and rankings of experts, criterion and alternatives. These benefits make the GADA method significant among the class of MCDM methods.																	1562-2479	2199-3211				JUN	2020	22	4					1073	1090		10.1007/s40815-020-00827-8		APR 2020											
J								Towards Universal Control System for Powered Ankle-Foot Prosthesis: A Simulation Study	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Powered ankle-foot; Robotic ankle-foot; Genetic fuzzy rule-based control system; Impedance fuzzy control system; Hierarchical control; system; Adaptive Fuzzy System	GENETIC FUZZY-SYSTEMS; HYBRID ZERO DYNAMICS; WALKING; TAXONOMY; NOISE	In powered ankle-foot prostheses, multilevel hierarchical control systems are usually used with predetermined parameters (tuned during the prosthesis trial). Therefore, control systems cannot adaptively interact with terrains variation where control systems is most effective for ground level walking and less effective for ascending or descending stair/slop. In order to address the control system performance in ever-changing terrains, an adaptive mechanism should be included the control system structure. Here, we present a pilot study to illustrate the applicability of a genetic algorithm-based adaptive fuzzy logic control system. The design method could be divided into two stages: initial knowledge base and membership functions for the genetic pool on the basis of the analysis of biological ankle-foot behaviour. Additionally, the construction of genetic optimization mechanism rules and constraints (fitness function, mutation rats, replacement rate, etc.). Takagi-Sugeno-Kang fuzzy (TSK-fuzzy) inference system is selected because the system structure could depict the character of simple impedance control system. The control system and dynamic model were developed using C code and evaluated using MATLAB/Simulink (2019a).																	1562-2479	2199-3211				JUN	2020	22	4					1299	1313		10.1007/s40815-020-00855-4		APR 2020											
J								Simple, robust and secure data hiding based on CRT feature extraction and closed-loop chaotic encryption system	JOURNAL OF REAL-TIME IMAGE PROCESSING										Robust data hiding; Discrete Tchebichef transform (DTT); Chinese remainder theorem (CRT); Chaotic maps; Secure data hiding	TCHEBICHEF MOMENTS; IMAGE; DCT; SCHEME	Robust data hiding methods are basically used when rightful ownership is intended and/or transmitting channel is prone to noise. In this paper, a robust and secure data hiding method is proposed in Tchebichef domain. For feature extraction, remainders of Tchebichef coefficients are calculated according to Chinese remainder theorem (CRT). Distances of these remainders are utilized as the extracted features. These distances are then modified according to the hidden bits, to complete the embedding process. To reduce the chance of illegal access and increase the security of the proposed method, a closed-loop chaotic encryption system (CLCES) has been introduced. The proposed CLCES requires very little side information and produces extremely sensitive noise-like sequences with high security. Robustness and security of the proposed method have been analyzed. Comparison results with other related methods in the current literature confirm the superiority of the proposed method. Due to its simplicity, the proposed method is a good candidate for real-time data hiding applications.																	1861-8200	1861-8219															10.1007/s11554-020-00971-2		APR 2020											
J								A survey on context awareness in big data analytics for business applications	KNOWLEDGE AND INFORMATION SYSTEMS										Big data; Context awareness; Business applications; Enterprise level systems	SUPPLY CHAIN; SYSTEM; IMPACT; ERP	The concept of context awareness has been in existence since the 1990s. Though initially applied exclusively in computer science, over time it has increasingly been adopted by many different application domains such as business, health and military. Contexts change continuously because of objective reasons, such as economic situation, political matter and social issues. The adoption of big data analytics by businesses is facilitating such change at an even faster rate in much complicated ways. The potential benefits of embedding contextual information into an application are already evidenced by the improved outcomes of the existing context-aware methods in those applications. Since big data is growing very rapidly, context awareness in big data analytics has become more important and timely because of its proven efficiency in big data understanding and preparation, contributing to extracting the more and accurate value of big data. Many surveys have been published on context-based methods such as context modelling and reasoning, workflow adaptations, computational intelligence techniques and mobile ubiquitous systems. However, to our knowledge, no survey of context-aware methods on big data analytics for business applications supported by enterprise level software has been published to date. To bridge this research gap, in this paper first, we present a definition of context, its modelling and evaluation techniques, and highlight the importance of contextual information for big data analytics. Second, the works in three key business application areas that are context-aware and/or exploit big data analytics have been thoroughly reviewed. Finally, the paper concludes by highlighting a number of contemporary research challenges, including issues concerning modelling, managing and applying business contexts to big data analytics.																	0219-1377	0219-3116				SEP	2020	62	9					3387	3415		10.1007/s10115-020-01462-3		APR 2020											
J								Multi-view region-adaptive multi-temporal DMM and RGB action recognition	PATTERN ANALYSIS AND APPLICATIONS										Action recognition; DMM; 3D CNN; Region adaptive	ENSEMBLE	Human action recognition remains an important yet challenging task. This work proposes a novel action recognition system. It uses a novel multi-view region-adaptive multi-resolution-in-time depth motion map (MV-RAMDMM) formulation combined with appearance information. Multi-stream 3D convolutional neural networks (CNNs) are trained on the different views and time resolutions of the region-adaptive depth motion maps. Multiple views are synthesised to enhance the view invariance. The region-adaptive weights, based on localised motion, accentuate and differentiate parts of actions possessing faster motion. Dedicated 3D CNN streams for multi-time resolution appearance information are also included. These help to identify and differentiate between small object interactions. A pre-trained 3D-CNN is used here with fine-tuning for each stream along with multi-class support vector machines. Average score fusion is used on the output. The developed approach is capable of recognising both human action and human-object interaction. Three public-domain data-sets, namely MSR 3D Action, Northwestern UCLA multi-view actions and MSR 3D daily activity, are used to evaluate the proposed solution. The experimental results demonstrate the robustness of this approach compared with state-of-the-art algorithms.																	1433-7541	1433-755X				NOV	2020	23	4					1587	1602		10.1007/s10044-020-00886-5		APR 2020											
J								The fixed set search applied to the power dominating set problem	EXPERT SYSTEMS										combinatorial optimization; dominating set; fixed set search; GRASP; power dominating set		In this article, we focus on solving the power dominating set problem and its connected version. These problems are frequently used for finding optimal placements of phasor measurement units in power systems. We present an improved integer linear program (ILP) for both problems. In addition, a greedy constructive algorithm and a local search are developed. A greedy randomised adaptive search procedure (GRASP) algorithm is created to find near optimal solutions for large scale problem instances. The performance of the GRASP is further enhanced by extending it to the novel fixed set search (FSS) metaheuristic. Our computational results show that the proposed ILP has a significantly lower computational cost than existing ILPs for both versions of the problem. The proposed FSS algorithm manages to find all the optimal solutions that have been acquired using the ILP. In the last group of tests, it is shown that the FSS can significantly outperform the GRASP in both solution quality and computational cost.																	0266-4720	1468-0394														e12559	10.1111/exsy.12559		APR 2020											
J								Multi-scale crowd feature detection using vision sensing and statistical mechanics principles	MACHINE VISION AND APPLICATIONS										Crowd dynamics; Crowd behaviour detection; Multi-scale crowd features; Group detection and tracking; Video analysis; Statistical mechanics	COHERENT MOTIONS; BEHAVIORS; SCENES; MODEL; FLOW	Crowd behaviour analysis using vision has been subject to many different approaches. Multi-purpose crowd descriptors are one of the more recent approaches. These descriptors provide an opportunity to compare and categorize various types of crowds as well as classify their respective behaviours. Nevertheless, the automated calculation of descriptors which are expressed as measurements with accurate interpretation is a challenging problem. In this paper, analogies between human crowds and molecular thermodynamics systems are drawn for the measurement of crowd behaviour. Specifically, a novel descriptor is defined and measured for crowd behaviour at multiple scales. This descriptor uses the concept of Entropy for evaluating the state of crowd disorder. By results, the descriptor Entropy does indeed appear to capture the desired outcome for crowd entropy while utilizing easily detectable image features. Our new approach for machine understanding of crowd behaviour is promising, while it offers new complementary capabilities to the existing crowd descriptors, for example, as will be demonstrated, in the case of spectator crowds. The scope and performance of this descriptor are further discussed in detail in this paper.																	0932-8092	1432-1769				APR 21	2020	31	4							26	10.1007/s00138-020-01075-4													
J								Fuzzy model-based optimal energy control during the electrical discharge machining	NEURAL COMPUTING & APPLICATIONS										EDM; Discharge energy; Material removal rate; Optimal control; MIMO fuzzy system	MATERIAL REMOVAL RATE; SURFACE-ROUGHNESS; ELECTRODE WEAR; TAGUCHI METHOD; EDM PROCESS; OPTIMIZATION; PARAMETERS; LOGIC; EFFICIENCY; WEDM	This research aimed to describe a fuzzy optimal energy control strategy, so the electrical discharge machining (EDM) process can be optimized as a whole. The purpose is to develop a fuzzy logic-based multi-input-multi-output optimal control system to characterize the discharge energy efficiency of the EDM process. Methodology used in this paper is specific experimental design covering final and rough operations. Two controllable parameters were selected, and they were discharge current and pulse duration. The discharge energy and material removal rate were chosen as quality characteristics. The proposed approach uses the concept of fuzzy optimal control which is formulated on the basis of fuzzy logic technology with the use of knowledge-based system control. The finding that resulted in this research is that the effect of the discharge energy developed was observed on the performance of material removal rate. The material removal of the electrical discharge is directly proportional to the amount of the discharge energy applied during the on time of the pulse. The experimental results revealed that the effect of the EDM is increased as electrical discharge input parameters increase, but their interaction is limited and still somewhat unclear. The fuzzy optimal control model provides a highly advantageous option of the EDM input parameters, to achieve the best machining performance: the minimum discharge energy and the maximum material removal rate.																	0941-0643	1433-3058				NOV	2020	32	22			SI		17011	17026		10.1007/s00521-020-04909-4		APR 2020											
J								l(2,p)-norm sequential bilateral 2DPCA: a novel robust technology for underwater image classification and representation	NEURAL COMPUTING & APPLICATIONS										Underwater image analysis; l2; p-norm; Two-dimensional principal component analysis (2DPCA); Sequential bilateral	2-DIMENSIONAL PCA; FACE REPRESENTATION; PROJECTION; L1-NORM	Many recently proposed robust two-dimensional principal component analysis (2DPCA) approaches can suppress the sensitivity to outliers in images to some extent. However, most approaches can neither perfectly minimize the reconstruction error nor use fewer coefficients to conveniently represent image information. To alleviate these deficiencies, we developed a novel robust 2DPCA approach for underwater image analysis, called l(2,p)-sequential bilateral-2DPCA (l(2,p)-SB-2DPCA). The outstanding advantages of l(2,p)-SB-2DPCA are as follows. First, our model uses the l(2,p)-norm as the metric criterion of the objective function, which not only improves the robustness of the algorithm but also preserves the basic properties of 2DPCA. Second, we establish the relationship between the variance of the projection data and the corresponding input data in both the row and column directions, which makes the model achieve good recognition ability while using fewer coefficients and further improves the interpretability of the model. Finally, to obtain the optimal value of l(2,p)-SB-2DPCA, we present an iterative algorithm. The experimental results show that the proposed algorithm achieves the best performance on three underwater datasets and one extended face dataset compared with other robust 2DPCA approaches.																	0941-0643	1433-3058				NOV	2020	32	22			SI		17027	17041		10.1007/s00521-020-04936-1		APR 2020											
J								Deep Q-network-based multi-criteria decision-making framework for virtual simulation environment	NEURAL COMPUTING & APPLICATIONS										Deep learning; Big data; Motivation system; Behavior planning; Nature inspired algorithm		Deep learning improves the realistic expression of virtual simulations specifically to solve multi-criteria decision-making problems, which are generally rely on high-performance artificial intelligence. This study was inspired by the motivation theory and natural life observations. Recently, motivation-based control has been actively studied for realistic expression, but it presents various problems. For instance, it is hard to define the relation among multiple motivations and to select goals based on multiple motivations. Behaviors should generally be practiced to take into account motivations and goals. This paper proposes a deep Q-network (DQN)-based multi-criteria decision-making framework for virtual agents in real time to automatically select goals based on motivations in virtual simulation environments and to plan relevant behaviors to achieve those goals. All motivations are classified according to the five-level Maslow's hierarchy of needs, and the virtual agents train a double DQN by big social data, select optimal goals depending on motivations, and perform behaviors relying on a predefined hierarchical task networks (HTNs). Compared to the state-of-the-art method, the proposed framework is efficient and reduced the average loss from 0.1239 to 0.0491 and increased accuracy from 63.24 to 80.15%. For behavioral performance using predefined HTNs, the number of methods has increased from 35 in the Q network to 1511 in the proposed framework, and the computation time of 10,000 behavior plans reduced from 0.118 to 0.1079 s.																	0941-0643	1433-3058															10.1007/s00521-020-04918-3		APR 2020											
J								Development of optimal water supply plan using integrated fuzzy Delphi and fuzzy ELECTRE III methods-Case study of the Gamasiab basin	EXPERT SYSTEMS										basin; fuzzy Delphi; fuzzy ELECTRE III; group decision-making; optimal water supply; water resources	GROUP DECISION-MAKING; ANALYTIC HIERARCHY PROCESS; MULTICRITERIA APPROACH; CLIMATE-CHANGE; MANAGEMENT; AHP; SELECTION; CRITERIA; RANKING; SYSTEMS	This paper presents a novel method for the development of an optimal water supply plan showcased using data from the Gamasiab basin, located in Kermanshah province, Iran, concerning new dams that are being constructed in this semi-arid region. In this paper, a new group multi-criteria decision-making (MCDM) plan is proposed by combining two MCDM methods based on the fuzzy Delphi and fuzzy ELECTRE III methods that convert the experts' opinions to triangular fuzzy numbers based on the level of uncertainty associated with various quantitative and qualitative criteria. Considering the opinions of four non-stakeholder experts and data analysis using the fuzzy Delphi method, the criteria were evaluated. Then, by analysing the results using the fuzzy ELECTRE III method, the final ranking of scenarios is obtained. A sensitivity analysis was conducted to assess the effect of uncertainty on the performance of the decision-making system in scenarios ranking. The total expense, flood control, reservoir capacity and diversion and water transfer played a significant role in selecting the optimal scenario. Additionally, a hydrologic model was developed to evaluate the performance of the optimal scenario in terms of qualitative criteria. The data indicated that there was a good agreement between the results obtained from the hydrological model and the scenario ranking by the employed method. Altogether, a comparison of the proposed method with other MCDM methods, including fuzzy analytic hierarchy process and fuzzy technique for order preference by simulation of ideal solution, indicated that the results of the employed method matched more closely to the local experts' opinion.																	0266-4720	1468-0394				OCT	2020	37	5			SI				e12568	10.1111/exsy.12568		APR 2020											
J								Real time detection of acoustic anomalies in industrial processes using sequential autoencoders	EXPERT SYSTEMS										acoustic anomaly detection; audio feature extraction; convolutional autoencoder; convolutional long short-term memory autoencoder; industrial processes	NOVELTY DETECTION	Development of intelligent systems with the pursuit of detecting abnormal events in real world and in real time is challenging due to difficult environmental conditions, hardware limitations, and computational algorithmic restrictions. As a result, degradation of detection performance in dynamically changing environments is often encountered. However, in the next-generation factories, an anomaly detection system based on acoustic signals is especially required to quickly detect and interfere with the abnormal events during the industrial processes due to the increased cost of complex equipment and facilities. In this study we propose a real time Acoustic Anomaly Detection (AAD) system with the use of sequence-to-sequence Autoencoder (AE) models in the industrial environments. The proposed processing pipeline makes use of the audio features extracted from the streaming audio signal captured by a single-channel microphone. The reconstruction error generated by the AE model is calculated to measure the degree of abnormality of the sound event. The performance of Convolutional Long Short-Term Memory AE (Conv-LSTMAE) is evaluated and compared with sequential Convolutional AE (CAE) using sounds captured from various industrial manufacturing processes. In the experiments conducted with the real time AAD system, it is shown that the Conv-LSTMAE-based AAD demonstrates better detection performance than CAE model-based AAD under different signal-to-noise ratio conditions of sound events such as explosion, fire and glass breaking.																	0266-4720	1468-0394														e12564	10.1111/exsy.12564		APR 2020											
J								Uncertain location-allocation decisions for a bi-objective two-stage supply chain network design problem with environmental impacts	EXPERT SYSTEMS										environmental impact; fuzzy programming; multi-objective optimization; supply chain network design problem; uncertainty theory	TRANSPORTATION PROBLEM; PROGRAMMING-MODELS; SELECTION	In the cases that the historical data of an uncertain event is not available, belief degree-based uncertainty theory is a useful tool to reflect such uncertainty. This study focuses on uncertain bi-objective supply chain network design problem with cost and environmental impacts under uncertainty. As such network may be designed for the first time in a geographical region, this problem is modelled by the concepts of belief degree-based uncertainty theory. This article is almost the first study on belief degree-based uncertain supply chain network design problem with environmental impacts. Two approaches such as expected value model and chance-constrained model are applied to convert the proposed uncertain problem to its crisp form. The obtained crisp forms are solved by some multi-objective optimization approaches of the literature such as TH, Niroomand, MMNV. A deep computational study with several test problems are performed to study the performance of the crisp models and the solution approaches. According to the results, the obtained crisp formulations are highly sensitive to the changes in the value of the cost parameters. On the other hand, Niroomand and MMNV solution approaches perform better than other solution approaches from the solution quality point of view.																	0266-4720	1468-0394				OCT	2020	37	5			SI				e12558	10.1111/exsy.12558		APR 2020											
J								Modular neural networks for quality of transmission prediction in low-margin optical networks	JOURNAL OF INTELLIGENT MANUFACTURING										Low margin optical networks; Modular neural networks; Optical power management; Quality of transmission	AUTOMATIC MICROSTRUCTURAL CHARACTERIZATION; MODEL; IDENTIFICATION; DESIGN; OUTPUT	It is estimated that the emergence of digital businesses, and growing dynamic traffic demands for Internet applications, including cloud computing and the Internet of Things, will impact the traffic at unprecedented rates. Thus, to cost-efficiently accommodate these challenging requirements, network operators are motivated to maximize the achieved capacity over deployed links, and design more efficient networks capable of handling a wide range of applications, as well as operate more closely to optimality. On the other hand, as the network elements age, nonlinear impairments increases (which can be translated to increasing the network load) and hence, the system margin and maximum achievable rate decrease. Therefore, in order to increase the capacity of optical networks and extend their life time, an accurate physical model is required to estimate the quality of signal and quantify the margin of lightpaths. In this regard, a machine learning (ML) method is proposed based on the modular neural networks to account the cross channel nonlinear effects in estimating noise power and OSNR of lightpaths. For this, the dependence of the contributed nonlinear component of noise power on the transmitted powers as well as the modulation formats of all channels are considered in this data-model. Indeed, this ML-based model provides a useful tool for per-lightpath power management, as a fundamental element for reducing margins. Results of evaluating various proposed modular models show that, in general, all modular models are able to estimate OSNR of lightpaths, with average accuracy of more than 96.6%. However, the proposed MM4 modular model is the model presenting better generalization, being able to correctly estimate OSNR of lightpaths, with average accuracy of 99.2%. Also, MM3 model performs better for partially loaded network scenarios (99.3%).																	0956-5515	1572-8145															10.1007/s10845-020-01576-z		APR 2020											
J								On the Logical Philosophy of Assertive Graphs	JOURNAL OF LOGIC LANGUAGE AND INFORMATION										Assertion; Assertive graphs; Existential graphs; Peirce; Classical vs; non-classical logical graphs; Deep inference; Inferentialism		The logic of assertive graphs (AGs) is a modification of Peirce's logic of existential graphs (EGs), which is intuitionistic and which takes assertions as its explicit object of study. In this paper we extend AGs into a classical graphical logic of assertions (ClAG) whose internal logic is classical. The characteristic feature is that both AGs and ClAG retain deep-inference rules of transformation. Unlike classical EGs, both AGs and ClAG can do so without explicitly introducing polarities of areas in their language. We then compare advantages of these two graphical approaches to the logic of assertions with a reference to a number of topics in philosophy of logic and to their deep-inferential nature of proofs.																	0925-8531	1572-9583				DEC	2020	29	4					375	397		10.1007/s10849-020-09315-6		APR 2020											
J								European option pricing under multifactor uncertain volatility model	SOFT COMPUTING										Uncertainty theory; Liu process; Multifactor model; European option	STOCK MODEL; STOCHASTIC VOLATILITY	This paper presents an uncertain stock model under the multifactor uncertain volatility framework. Based on the uncertainty theory, some closed-form and analytical formulas presented to value a European call and put option under the multifactor uncertain volatility model. Numerical tests are reported to highlight how the proposed model provides interesting results on pricing a European option. Finally, we summarize the theoretical results and some numerical experiments.																	1432-7643	1433-7479				JUN	2020	24	12			SI		8781	8792		10.1007/s00500-020-04919-3		APR 2020											
J								Pruning of genetic programming trees using permutation tests	EVOLUTIONARY INTELLIGENCE										Genetic programming; Permutation testing; Tree pruning		We present a novel approach based on statistical permutation tests for pruning redundant subtrees from genetic programming (GP) trees that allows us to explore the extent of effective redundancy . We observe that over a range of regression problems, median tree sizes are reduced by around 20% largely independent of test function, and that while some large subtrees are removed, the median pruned subtree comprises just three nodes; most take the form of an exact algebraic simplification. Our statistically-based pruning technique has allowed us to explore the hypothesis that a given subtree can be replaced with a constant if this substitution results in no statistical change to the behavior of the parent tree-what we term approximate simplification. In the eventuality, we infer that more than 95% of the accepted pruning proposals are the result of algebraic simplifications, which provides some practical insight into the scope of removing redundancies in GP trees.																	1864-5909	1864-5917				DEC	2020	13	4					649	661		10.1007/s12065-020-00379-8		APR 2020											
J								Automatic acoustic identification of respiratory diseases	EVOLVING SYSTEMS										Respiratory sound classification; Acoustic signal processing; Respiratory diseases; Directed acyclic graph; Hidden Markov model	DIRECTED ACYCLIC GRAPHS; FRAMEWORK; SOUND	Several disease affecting the human respiratory system, such as asthma, pneumonia, etc. are associated with distinctive sounds. Here, we propose a methodology towards their automatic identification by means of signal processing and pattern recognition algorithms. We designed a suitable feature set based on wavelet packet analysis characterizing data coming from diverse classes of respiratory sounds following the logic of the challenge organised within the International Conference on Biomedical Health Informatics in 2017. The patterns revealed by the feature extraction stage are modelled by hidden Markov models. Automatic identification is carried out via a directed acyclic graph (DAG) scheme limiting the problem space while based on decisions made by the available HMMs. Thorough experiments following a well-established protocol demonstrate the efficacy of the proposed solution. Indeed, such a DAG-based structure outperforms the current state of the art including deep networks based on convolutional kernels. Importantly, the presented solution offers a high level of explainability as one is able to investigate the effective DAG path and understand both correct and incorrect predictions.																	1868-6478	1868-6486															10.1007/s12530-020-09339-0		APR 2020											
J								Twin support vector machine based on adjustable large margin distribution for pattern classification	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Pattern classification; Margin distribution; Twin support vector machine; Generalization performance		This paper researches the value of the margin distribution in binary classifier. The central idea of large margin distribution machine (LDM) is to optimize the margin distribution, such as maximizing the margin mean and minimizing the margin variance. Compared to support vector machine (SVM), LDM demonstrates the good generalization performance. In order to improve the generalization performance of twin support vector machine (TSVM), a twin support vector machine based on adjustable large margin distribution (ALD-TSVM) is proposed in this paper. Firstly, the margin distribution is redefined to construct a pair of adjustable supporting hyperplanes. Then, the redefined margin distribution is introduced onto TSVM to obtain the models of ALD-TSVM, including linear case and nonlinear case. ALD-TSVM is a general learning method which can be used in any place where TSVM and LDM can be applied. Finally, the novel method is compared with other classification algorithms by doing experiments on toy dataset, UCI datasets and image datasets. The experimental results show that ALD-TSVM obtains better classification performance.																	1868-8071	1868-808X				OCT	2020	11	10					2371	2389		10.1007/s13042-020-01124-4		APR 2020											
J								A new BAT optimization algorithm based feature selection method for electrocardiogram heartbeat classification using empirical wavelet transform and Fisher ratio	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Heartbeats feature selection; BAT based feature selection; Heartbeat classification; Arrhythmia classification; EWR with BAT feature selection	FEATURE-EXTRACTION; ECG	In this paper, a novel feature selection method is proposed for the categorization of electrocardiogram (ECG) heartbeats. The proposed technique uses the Fisher ratio and BAT optimization algorithm to obtain the best feature set for ECG classification. The MIT-BIH arrhythmia database contains sixteen classes of the ECG heartbeats. The MIT-BIH ECG arrhythmia database divided into intra-patient and inter-patient schemes to be used in this study. The proposed feature selection methodology works in following steps: firstly, features are extracted using empirical wavelet transform (EWT) and then higher-order statistics, as well as symbolic features, are computed for each decomposed mode of EWT. Thereafter, the complete feature vector is obtained by the conjunction of EWT based features and RR interval features. Secondly, for feature selection, the Fisher ratio is utilized. It is optimized by using BAT algorithm so as to have maximal discrimination of the between classes. Finally, in the classification step, the k-nearest neighbor classifier is used to classify the heartbeats. The performance measures i.e., accuracy, sensitivity, positive predictivity, specificity for intra-patient scheme are 99.80%, 99.80%, 99.80%, 99.987% and for inter-patient scheme are 97.59%, 97.589%, 97.589%, 99.196% respectively. The proposed feature selection technique outperforms the other state of art feature selection methods.																	1868-8071	1868-808X				NOV	2020	11	11					2439	2452		10.1007/s13042-020-01128-0		APR 2020											
J								A novel permission ranking system for android malware detection-the permission grader	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Android; Malware detection; Permission; Static analysis; Deep learning		Android is a profound, vanguard mobile operating system of the contemporary era. The quantity of mobile phone emptor dependent on Android platform is rising expeditiously, which expands its prominence everywhere throughout the world. The facts demonstrate that there are different sides to everything, with the remarkable achievement of the Android; attacks on Android operating system have been on the rise, as there are a lot of apps on the Internet that encompass malware. Malware is a segment of code composed with the aim of hurting a gadget or stealing the information in it. The proposed Permission Grading System performs static analysis of the apps to extract the requested permissions and to identify the permissions that are unique to malware and benign apps, by calculating the contribution of each of the permission. This helps identify the risk of the permissions that are requested by the apps. The results show an increase of about 20% in the detection of malware, with True Positive Rate values more than 0.85 and False Positive Rate values nearly fall below 0.03. These values are improved on using the familiar Term Frequency-Inverse Document Frequency weighting after the identification of unique permissions. This has led to achieve a True Positive Rate of more than 0.90 and False Positive Rate values were only 0.01.																	1868-5137	1868-5145															10.1007/s12652-020-01957-5		APR 2020											
J								An empirical model (EM: CCO) for clustering, convergence and center optimization in distributive databases	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Clustering; Distributive data mining; Convergence; Cluster center and memory-resident		Conventional clustering methods have an assumption that data is stored centrally and are memory resident which made it tough to arrive at solutions when dealing with large data. Centralizing huge data from multiple locations are always a challenging task owing to the large memory space and computational time required by traditional mining methods. Traditional k-means type of clustering were used for the identification of clusters' prototype that can serve as a representative point in a large dataset and the major setback is that the cluster centers tend to distort the distribution of the underlying data making the representative points incapable of handling the complete distribution of the data leading to poor pattern generation. With the aim to resolve this issue, this paper proposes an empirical model (EM) that ensures the centers of the cluster for capturing the data distribution which lies under. In the proposed methodology, the asymptotic convergence is centered on the data which is distributed. Secondly, an efficient mechanism for measuring the cluster centers in practice. Finally, a methodology for distributive convergence and center optimization is proposed. The model is compared with that of other methods in the literature and the results are discussed.																	1868-5137	1868-5145															10.1007/s12652-020-01955-7		APR 2020											
J								A context-aware system using mobile applications and beacons for on-premise security environments	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Beacons; Indoor localization; Context-aware computing; BLE mobile applications; Mobile computing; IoT		In this paper, we describe a mobile solution for on-premise security monitoring using Android devices and Bluetooth Low Energy beacons. The system incorporates two mobile based applications, beacons and supporting cloud services. One mobile application was developed for security managers to easily configure beacons while in close proximity to a beacon for setting up a route, while the other is for security guards as they perform their rounds. We developed this system in collaboration with a security firm and an industry partner located in Waterloo, Ontario, Canada. We present the relevant background of work in this area, the architectural framework that we designed and developed to support the system, the applications themselves and a report on the findings of real use test case scenarios involving security managers and guards. Our system was tested in a variety of conditions and performed at an accuracy of 98%, achieved SUS usability scores of 85% and 82% for Security Managers and Security Guards respectively. It provides a low-cost, easily deployed scalable solution for security monitoring in a range of environment configurations.																	1868-5137	1868-5145															10.1007/s12652-020-01906-2		APR 2020											
J								A Global/Local Path Planner for Multi-Robot Systems with Uncertain Robot Localization	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Multi-robot systems; Path planning; Genetic algorithm; Multiple traveling salesmen problem		This paper proposes a path planner for multi-robot systems based on the solution of the multiple traveling salesman problem by genetic algorithm. The main planning goal is to build an efficient path to each robot of the system which jointly ensures that several a priori known points (ways-points) in the environment can be attained by at least one of the robots. During navigation, each robot try to follow its planned path while performing tasks and deviating from unknown static and dynamic obstacles. As the robots have limited sensing and communication skills, their positions can easily become uncertain and the robots will be unable to follow their planned paths. To circumvent this problem, each robot has a local planner, able to recalculate its position and then to resume its planned path. This computation is based on the knowledge about way-points positions, information exchanged with other robots and a self localization algorithm by triangulation. The proposed global/local path planner is firstly validated by computer simulations. An experimental study is also carried out with a small system with very simple mobile robots with differential drive and Bluetooth communication. The obtained results confirm the efficacy of the proposed path planner.																	0921-0296	1573-0409				OCT	2020	100	1					311	333		10.1007/s10846-020-01196-y		APR 2020											
J								Decision model change patterns for dynamic system evolution	KNOWLEDGE AND INFORMATION SYSTEMS										DMN; Decision model and notation; Model evolution; Change patterns	DMN; MANAGEMENT	In the modern digital era, information systems must operate in increasingly interconnected and dynamic environments, which force them to be changeable yet consistent. Such modern information systems are usually decision- and knowledge-intensive. A recently introduced standard, the decision model and notation (DMN), has been adopted in both industry and academia as a suitable method for modelling decisions and decision rules. Noteworthy is that, despite the dynamic nature of modern knowledge-intensive systems, DMN was only studied and implemented in a static fashion, as decision schema change patterns have not received any attention so far. This paper identifies and analyses the change patterns that can occur in a DMN decision model. A change in the decision model can require the triggering of other changes in order to safeguard consistency. As such, this paper will also investigate for each change pattern which further changes should be performed to ensure model consistency. The patterns presented in this paper will not only facilitate the understanding of decision change management and within-model consistency, but can also be capitalised on for developing and implementing flexible decision management systems. To illustrate this, we present a modelling environment prototype that provides modelling support when applying the proposed change patterns.																	0219-1377	0219-3116				SEP	2020	62	9					3665	3696		10.1007/s10115-020-01469-w		APR 2020											
J								Thresholded ConvNet ensembles: neural networks for technical forecasting	NEURAL COMPUTING & APPLICATIONS										Technical analysis; Machine learning; Deep neural networks	ALGORITHMS	Much of modern practice in financial forecasting relies on technicals, an umbrella term for several heuristics applying visual pattern recognition to price charts. Despite its ubiquity in financial media, the reliability of its signals remains a contentious and highly subjective form of 'domain knowledge'. We investigate the predictive value of patterns in financial time series, applying machine learning and signal processing techniques to 22 years of US equity data. By reframing technical analysis as a poorly specified, arbitrarily preset feature-extractive layer in a deep neural network, we show that better convolutional filters can be learned directly from the data, and provide visual representations of the features being identified. We find that an ensemble of shallow, thresholded convolutional neural networks optimised over different resolutions achieves state-of-the-art performance on this domain, outperforming technical methods while retaining some of their interpretability.																	0941-0643	1433-3058				SEP	2020	32	18			SI		15249	15262		10.1007/s00521-020-04877-9		APR 2020											
J								Generalized robust graph-Laplacian PCA and underwater image recognition	NEURAL COMPUTING & APPLICATIONS										Graph-Laplacian principal component analysis (gLPCA); Distance metric; Robust; Underwater image recognition	PRINCIPAL COMPONENT ANALYSIS; NEIGHBORHOOD PRESERVING PROJECTION; DISCRIMINANT-ANALYSIS; FACE; REGULARIZATION; SIMILARITY; NOISE; 2DPCA; NORM	Recently, given the importance of the structure-preserving ability of features, many principal component analysis (PCA) methods based on manifold learning theory, such as graph-Laplacian PCA (gLPCA), have been developed to protect the geometrical structure of the original data space. However, many methods do not best minimize the reconstruction error, which is great significance for underwater image recognition and representation. To alleviate this deficiency, a novel idea for gLPCA-generalized robust graph-Laplacian PCA (GRgLPCA)-was proposed. GRgLPCA not only employs the l2,p-norm on regarding the correlation between the reconstruction error and variance in the projection data to suppress the influence of underwater noise, but it also employs it regarding the graph-Laplacian regularization term to better protect the intrinsic geometric information embedded in the data. Moreover, GRgLPCA preserves the rotational invariance well, and the solution of the model is related to image covariance matrix, which are the two desired properties of PCA-based method. Finally, we design a fast and effective non-greedy iterative algorithm to obtain the GRgLPCA solution. A series of experiments on several underwater image databases and one face image extension database illustrated the effectiveness of our proposed method.																	0941-0643	1433-3058				NOV	2020	32	22			SI		16993	17010		10.1007/s00521-020-04927-2		APR 2020											
J								The effect of reduced training in neural architecture search	NEURAL COMPUTING & APPLICATIONS										Neural architecture search; Deep learning; Ranking; NASBench		As neural architecture search becomes an increasingly studied field, it has become apparent that it demands a great number of computational resources. These are usually devoted to computations and utilized to train and evaluate intermediate solutions during the search phase. Although most researchers focus on developing more efficient search methods, the main computational cost in terms of execution time percentage concerns the evaluation of candidate architectures. As such, many works utilize a smaller number of training epochs during search phase evaluations. In this work, we study the effect of reduced training in neural architecture search. We focus on the retention of relative rankings between architectures when they are trained with different optimizers and for various epochs. We discover relatively high rank correlations between various fully and partially trained, arbitrarily connected architectures (Kendall's tau-b > 0.7). These are generated by mutating a simple convolutional architecture for the CIFAR-10 image recognition dataset. Furthermore, we observe similar behaviors in networks sampled from the NASBench neural architecture dataset, consisting of a fixed outer skeleton and variable cell module composition. Finally, we demonstrate the ability of genetic algorithms to find optimal solutions in noisy environments, by simulating the previous findings with perturbed n-dimensional Rastrigin functions.																	0941-0643	1433-3058															10.1007/s00521-020-04915-6		APR 2020											
J								Retrieval of colour and texture images using local directional peak valley binary pattern	PATTERN ANALYSIS AND APPLICATIONS										LBP value; Image retrieval; Image classification; Feature vector; Texture database; Face database; Local binary pattern	MULTIRESOLUTION GRAY-SCALE; COOCCURRENCE PATTERN; FEATURE DESCRIPTOR; FACE; ROTATION; CLASSIFICATION	Many content-based image retrieval (CBIR) methods are being developed to store more and more information about images in shorter feature vectors and to improve image retrieval rate. In the proposed method, two-step approach to CBIR has been developed. The first step generates an image mask from local binary pattern (LBP). This LBP mask is then utilized to draw comparison between the centre pixel and the eight surrounding pixels. The second step involves drawing the peak and valley patterns of local directional binary pattern for each image which is then combined with the colour histogram to retrieve similar images. Existing methods suffer from lower average image retrieval accuracy even with larger feature vectors. The proposed method overcomes such problems through shorter feature vectors that can store more information about the image. As illustrated through experimental results, the proposed method produces promising results with shorter feature vector of length 56 and improved image retrieval rate of about 5-10%. Our method outperforms similar techniques when tested with public data sets.																	1433-7541	1433-755X				NOV	2020	23	4					1569	1585		10.1007/s10044-020-00879-4		APR 2020											
J								Minimizing information loss in shared data: Hiding frequent patterns with multiple sensitive support thresholds	STATISTICAL ANALYSIS AND DATA MINING										information loss; itemset mining; privacy preserving itemset mining	ITEMSET; SANITIZATION; ALGORITHM	Privacy preserving data mining (PPDM) is the process of protecting sensitive knowledge from being discovered by data mining techniques in case of data sharing. Privacy preserving frequent itemset mining (PPFIM) is a subtask and NP-hard problem of PPDM. Its objective is to modify a given database in such a way that none of the sensitive itemsets of the database owner can be obtained by any frequent itemset mining technique from the modified database. The main challenge of PPFIM is to minimize the distortion given to the data and nonsensitive knowledge while sanitizing all given sensitive itemsets. Distortion-based sensitive itemset hiding algorithms decrease the support of each sensitive itemset under a predefined sensitive threshold through sanitization. Most of the distortion-based itemset hiding algorithms allow database owner to define a single sensitive threshold for each sensitive itemset. However, this is a limitation to the database owner since the importance of each sensitive itemset varies. In this paper we propose a distortion-based itemset hiding algorithm that allows database owner to assign multiple sensitive thresholds, namely itemset oriented pseudo graph based sanitization (IPGBS) algorithm. The purpose of IPGBS algorithm is to give minimum distortion to the nonsensitive knowledge and data while hiding all sensitive itemsets. For this reason, the IPGBS algorithm modifies least amount of transaction and transaction content. The performance evaluation of the IPGBS algorithm is conducted by using two different counterparts on four different databases. The results show that the IPGBS algorithm is more efficient in terms of nonsensitive frequent itemset loss on both dense and sparse databases. It has considerable good results in terms of number of transactions modified, number of items deleted, execution time and total memory allocation as well.																	1932-1864	1932-1872				AUG	2020	13	4					309	323		10.1002/sam.11458		APR 2020											
J								Knot selection in sparse Gaussian processes with a variational objective function	STATISTICAL ANALYSIS AND DATA MINING										knot selection; machine learning; nonparametric regression; sparse Gaussian processes; variational inference		Sparse, knot-based Gaussian processes have enjoyed considerable success as scalable approximations of full Gaussian processes. Certain sparse models can be derived through specific variational approximations to the true posterior, and knots can be selected to minimize the Kullback-Leibler divergence between the approximate and true posterior. While this has been a successful approach, simultaneous optimization of knots can be slow due to the number of parameters being optimized. Furthermore, there have been few proposed methods for selecting the number of knots, and no experimental results exist in the literature. We propose using a one-at-a-time knot selection algorithm based on Bayesian optimization to select the number and locations of knots. We showcase the competitive performance of this method relative to optimization of knots simultaneously on three benchmark datasets, but at a fraction of the computational cost.																	1932-1864	1932-1872				AUG	2020	13	4					324	336		10.1002/sam.11459		APR 2020											
J								Recommender system for home automation using IoT and artificial intelligence	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Artificial intelligence; Machine learning; Natural language processing; Automation		Smart home automation system is the current and upcoming trending technology in the market which makes life simpler and easier to control. Internet of things (IoT) Based home automation system is designed for old and disabled people. This system design is not only using IoT technology but also using the feature of artificial intelligence (AI) as well as cloud. Due to this advancement people get an assistant to manage home and their needs, based on the commands they told them to do. The main control system using wireless communication technology is to provide remote access from tablet or Smartphone. Here, natural language processing (NLP) plays a vital role since it acts as an interface between human interaction and machines. Through NLP users can either command or control devices at home even though disabled persons command or request varies from presets. Home controls like door monitoring, home appliances monitoring, and bed movement monitoring will be assisted through IoT which in turn is controlled by AI and the information is stored in the cloud. All the controls of the home are thrown at AI. The user assists all the IoT functions using voice control and all related information is sent to the cloud. Predictions can be done through a predictive engine which in turn can be used in the near future. This work can be used as a recommendation system for old people as well as physically impaired people those who didn't do their work independently and easily.																	1868-5137	1868-5145															10.1007/s12652-020-01968-2		APR 2020											
J								Divide-and-conquer ensemble self-training method based on probability difference	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Ensemble self-training; Probability difference; Low-fuzzy instances; High-fuzzy instances; Divide-and-conquer strategy	PERFORMANCE	Self-training method can train an effective classifier by exploiting labeled instances and unlabeled instances. In the process of self-training method, the high confidence instances are usually selected iteratively and added to the training set for learning. Unfortunately, the structure information of high confidence instances is so similar that it leads to local over-fitting during the iterations. In order to avoid the over-fitting phenomenon, and improve the classification effect of self-training methods, a novel divide-and-conquer ensemble self-training framework based on probability difference is proposed. Firstly, the probability difference of instances is calculated by the category probability of each classifier, the low-fuzzy and high-fuzzy instances of each classifier are divided through the probability difference. Then, a divide-and-conquer strategy is adopted. That is, the low-fuzzy instances determined by all the classifiers are directly labeled and high-fuzzy instances are manually labeled. Finally, the labeled instances are added to the training set for iteration self-training. This method expands the training set by selecting low-fuzzy instances with accurate structure information and high-fuzzy instances with more comprehensive structure information, and it improves the generalization performance of the method effectively. The method is more suitable for noise data sets and it can obtain structure information even in a few labeled instances. The effectiveness of the proposed method is verified by comparative experiments on the University of California Irvine (UCI).																	1868-5137	1868-5145															10.1007/s12652-020-01971-7		APR 2020											
J								An intelligent knowledge system for designing, modeling, and recognizing the behavior of elderly people in smart space	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Human activity description; Human activity recognition; Intelligent reasoning; Recognition model; Knowledge system; Smart environment; E-health	ACTIVITY RECOGNITION; FRAMEWORK	In this paper, a context-sensitive descriptive language is proposed to design and model the daily living activities of elderly people. The objective is to simplify and represent correctly the knowledge collected by sensors (low level) and to have a relevant recognition of the person's knowledge (high level). The proposed language is based on several rules and constraints through intelligent meaning. It is dedicated to a better understanding and semantic design and description of the behavior of elderly people. Subsequently, in order to provide a powerful knowledge recognition system, a hybrid Markov model is proposed to recognize and predict the activities designed by the proposed language. The proposed model is adapted to the reasoning of the new language. This allows providing a hierarchical and temporal relationship within the knowledge. It is responsible to recognize and predict the behavior of the elderly people efficiently. The flexibility and the intelligibility of the proposed language is proven and the accuracy of the recognition model is demonstrated which ensures the efficiency of the proposed knowledge recognition system.																	1868-5137	1868-5145															10.1007/s12652-020-01876-5		APR 2020											
J								A mix-supervised unified framework for salient object detection	APPLIED INTELLIGENCE										Salient object detection; Mix-supervised framework; Computing vision	VISUAL-ATTENTION; CONTRAST	Recently, although deep learning network has shown its advantages in supervised salient object detection, supervised models often require massive pixel-wise annotations and learnable parameters, which seriously manacle training and testing of models. In this paper, we present a mix-supervised unified framework for salient object detection to avoid the insufficient training labels and speed training and testing up, which is composed of a region-wise stream and a pixel-wise stream. In the region-wise stream, to avoid the requirement of expensive pixel-wise annotations, an improved energy equation based manifold learning algorithm is employed, by which accurate object location and prior knowledge are introduced by the unsupervised learning. In the pixel-wise stream, to alleviate the problem of time-consuming, a simplified bi-directional reuse network is introduced, which can obtain clear object contour and competitive performance with fewer parameters. To relieve the bottleneck pressure of parallel training and testing, each steam is directly connected to its pre-processed color feature and post-processing refinement. Extensive experiments demonstrate that each component contributes to the final results and complement each other perfectly.																	0924-669X	1573-7497				SEP	2020	50	9					2945	2958		10.1007/s10489-020-01700-9		APR 2020											
J								A novel framework of fuzzy oblique decision tree construction for pattern classification	APPLIED INTELLIGENCE										Fuzzy oblique decision tree; Feature selection; Fuzzy numbers; Fuzzy rule extraction	RULE; GRANULATION; INDUCTION	In this paper, some significant efforts on fuzzy oblique decision tree (FODT) have been done to improve classification accuracy and decrease tree size. Firstly, to eliminate data redundancy and improve classification efficiency, a forward greedy fast feature selection algorithm based on neighborhood rough set (NRS_FS_FAST) is introduced. Then, a new fuzzy rule generation algorithm (FRGA) is proposed to generate fuzzy rules. These fuzzy rules are used to construct leaf nodes for each class in each layer of the FODT. Different from the traditional axis-parallel decision trees and oblique decision trees, the FODT takes dynamic mining fuzzy rules as decision functions. Moreover, the parameter delta, which can control the size of the tree, is optimized by genetic algorithm. Finally, a series of comparative experiments are carried out with five traditional decision trees (C4.5, Best First Tree (BFT), amulti-class alternating decision tree (LAD), Simple Cart (SC), Naive Bayes Tree (NBT)), and recently proposed decision trees (FRDT, HHCART, and FMMDT-HB) on UCI machine learning datasets. The experimental results demonstrate that the FODT exhibits better performance on classification accuracy and tree size than the chosen benchmarks.																	0924-669X	1573-7497				SEP	2020	50	9					2959	2975		10.1007/s10489-020-01675-7		APR 2020											
J								Selective ensemble of uncertain extreme learning machine for pattern classification with missing features	ARTIFICIAL INTELLIGENCE REVIEW										Ensemble classification; Missing data; uncertainty; Robustness; Extreme learning machine; DC programming	SUPPORT VECTOR MACHINE; REGRESSION; VALUES; MARGIN	Ensemble learning is an effective technique to improve performance and stability compared to single classifiers. This work proposes a selective ensemble classification strategy to handle missing data classification, where an uncertain extreme learning machine with probability constraints is used as individual (or base) classifiers. Then, three selective ensemble frameworks are developed to optimize ensemble margin distributions and aggregate individual classifiers. The first two are robust ensemble frameworks with the proposed loss functions. The third is a sparse ensemble classification framework with the zero-norm regularization, to automatically select the required individual classifiers. Moreover, the majority voting method is applied to produce ensemble classifier for missing data classification. We demonstrate some important properties of the proposed loss functions such as robustness, convexity and Fisher consistency. To verify the validity of the proposed methods for missing data, numerical experiments are implemented on benchmark datasets with missing feature values. In experiments, missing features are first imputed by using expectation maximization algorithm. Numerical experiments are simulated in filled datasets. With different probability lower bounds of classification accuracy, experimental results under different proportion of missing values show that the proposed ensemble methods have better or comparable generalization compared to the traditional methods in handling missing-value data classifications.																	0269-2821	1573-7462				DEC	2020	53	8					5881	5905		10.1007/s10462-020-09836-3		APR 2020											
J								Speech and web-based technology to enhance education for pupils with visual impairment	JOURNAL ON MULTIMODAL USER INTERFACES										Pupils with visual impairments; Educational technology interface; Text-to-speech synthesis; Improving classroom teaching	ASSISTIVE TECHNOLOGY; STUDENTS; DISABILITIES	This paper describes a new web-based system specially adapted to the education of Czech pupils with visual impairment. The system integrates speech and language technologies with a web framework in lower secondary education, especially in mathematics and physics subjects. A new interface utilized the text-to-speech (TTS) synthesis for online automatic reading of educational texts. The interface provides several TTS voices, synthesized data caching, and automatic processing of formulas in mathematics and physics. The system was designed to enable teachers create and manage teaching materials. It also enables the pupils to view and listen to the read forms of these documents online. A school for pupils with visual impairment participated in the development and implementation of the system. After one year of using the system daily, the user experience and evaluation data were collected. The results indicate a positive reception and frequent use of the system as well as a preference over classical educational materials.																	1783-7677	1783-8738				JUN	2020	14	2			SI		219	230		10.1007/s12193-020-00323-1		APR 2020											
J								LPG: a novel approach for medical forgery detection in image transmission	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Machine learning algorithm; Local patterns of gray level (LPG); Medical image forgery detection; MIAS database; IOMIT-internet medical image transmission		Medical image transmission using IoT has become the hot field in the today's world of research, but the attacks or manipulating the images, has become the real threat to the medical field. Physicians diagnose always depends on the digital image(s). Small change in the medical image may threaten patient's life. Early detection of forgery may help a patient's life from danger. Hence the most intelligent algorithm developing is required for the above mentioned attacks. To meet the above criteria, most intelligent LPG algorithm has been proposed. LPG algorithm has been integrated with the cognitive extreme learning machines for detection. The proposed algorithm has been evaluated with the mammograms breast cancer images and accuracy detection is found to be more accuracy based on activation function compared with the other existing recent papers.																	1868-5137	1868-5145															10.1007/s12652-020-01932-0		APR 2020											
J								An efficient human face verification system based on ELBP: a high precision feature	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										LBP; ELBP; Radial filter; Face verification; Face recognition	LOCAL BINARY PATTERNS; RECOGNITION; CLASSIFICATION	Human Face Verification (HFV) is used for distinguishing the human as an authorized or unauthorized Person in security and biometric related applications. HFV has huge applications in many areas. HFV process involves the collection, detection, pre-processing, extraction, classifying and Verification stages. The main problem in HFV system is to detect the faces automatically from front view with variation in illumination, expression, and occlusion and so on. This work is mainly focused on extraction and recognition stages. LBP is widely used method for texture analysis. LBP usually compares the center pixel to its neighbors over a fixed radius. LBP is generally sensitive to noise. In order to overcome this problem ELBP method had been proposed which deals with the rotation invariant of face images. The proposed method is evaluated for noise environment, pose variations and emotions. For each one, the experimental results are tabulated and analyzed. From the result analysis, the proposed method provides better accuracy than existing methods.																	1868-5137	1868-5145															10.1007/s12652-020-01965-5		APR 2020											
J								A General Framework for Deep Supervised Discrete Hashing	INTERNATIONAL JOURNAL OF COMPUTER VISION										Supervised discrete hashing; l(2) loss; Hinge loss; Alternating minimization method		With the rapid growth of image and video data on the web, hashing has been extensively studied for image or video search in recent years. Benefiting from recent advances in deep learning, deep hashing methods have shown superior performance over the traditional hashing methods. However, there are some limitations of previous deep hashing methods (e.g., the semantic information is not fully exploited). In this paper, we develop a general deep supervised discrete hashing framework based on the assumption that the learned binary codes should be ideal for classification. Both the similarity information and the classification information are used to learn the hash codes within one stream framework. We constrain the outputs of the last layer to be binary codes directly, which is rarely investigated in deep hashing algorithms. Besides, both the pairwise similarity information and the triplet ranking information are exploited in this paper. In addition, two different loss functions are presented: l(2) loss and hinge loss, which are carefully designed for the classification term under the one stream framework. Because of the discrete nature of hash codes, an alternating minimization method is used to optimize the objective function. Experimental results have shown that our approach outperforms current state-of-the-art methods on benchmark datasets.																	0920-5691	1573-1405				SEP	2020	128	8-9			SI		2204	2222		10.1007/s11263-020-01327-w		APR 2020											
J								Multi-parameter online optimization algorithm of BP neural network algorithm in Internet of Things service	NEURAL COMPUTING & APPLICATIONS										BP neural network; NNA; MP online measurement; IOT system		With the development of science and technology, the application of the Internet of Things (IOT) is becoming more and more widespread. Applying BP neural network algorithms (NNA) to the IOT system will help improve the performance of the IOT system. The research purpose of this paper is to solve the problems of long-parameter measurement cycle and untimely feedback of the existing IOT online measurement system. In this paper, a multi-parameter (MP) IOT online measurement system based on BP NNA is designed, and a simulation test experiment is performed. The MP online measurement IOT system based on the BP NNA completes the parameter collection, analysis, and display through the perception layer, network transmission layer, and application layer. The core is that the system application layer adds the BP NNA to optimize real-time acquisition parameters, processing to reduce parameter measurement time. It can be known from algorithm simulation experiments that the online measurement system based on the BP NNA proposed in this paper uses the BP NNA to predict the absolute error value of the final moisture content and the measured moisture content within 0.3, and the absolute error of the moisture content value in actual production. It is acceptable in the range of 0.5, which speeds up data collection time. This system has a very good effect on improving the feedback adjustment speed of the manufacturing process system.																	0941-0643	1433-3058															10.1007/s00521-020-04913-8		APR 2020											
J								An enhanced Moth-flame optimization algorithm for permutation-based problems	EVOLUTIONARY INTELLIGENCE										Metaheuristics; Combinatorial optimization problems (COP); Moth-flame optimization (MFO); Travelling salesman problem (TSP); Crossover function; Local minima	ANT COLONY OPTIMIZATION; MUTATION; SEARCH	Moth-flame optimizer (MFO) is one of the recently proposed metaheuristic optimization techniques which has been successfully used in wide range of applications. However, there are two issues with the MFO algorithm. First, as a stochastic technique, MFO may prematurely converge at some local minima during the search process. Second, the original MFO was developed for continuous search space problems and is not directly applicable to, e.g., permutation-based problems (PBP). In this paper, a novel perturbation strategy is introduced to the MFO algorithm to avoid probable local minima regions. This strategy works as follows: if the best solution obtained so far doesn't improve for a given number of consecutive iterations, the current population of solutions is perturbed using some crossover mechanism as an attempt to explore new promising neighbourhoods in the search space. In addition, smallest position values mapping technique is employed in order for the proposed, termed CrossMFO (COMFO), algorithm to be applicable to PBP problems. It is noticed that, despite these modifications, the proposed COMFO has the same time complexity order as the original MFO. Extensive simulation experiments are conducted to compare the proposed COMFO to the MFO, other enhanced versions of MFO, and some metaheuristic optimizers in solving the well-known Travelling Salesman Problem (TSP). Empirical results show that the solutions obtained using MFO are improved by a factor of 24-47% on average for large TSP instances having more than 100 cities using COMFO and can even reach 38-58% using different settings. In addition, compared to other algorithms in the literature, the proposed algorithm provides, on average, better solutions. Hence, it can be considered a promising and efficient technique for this type of problems.																	1864-5909	1864-5917				DEC	2020	13	4					741	764		10.1007/s12065-020-00389-6		APR 2020											
J								Query-by-visual-search: multimodal framework for content-based image retrieval	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Content-based image retrieval; Multifractal features; Soft label SVM; Salient objects; Bag-of-words model	FEATURE INTEGRATION; COLOR; FEATURES; TEXTURE; DESCRIPTOR; TRANSFORM; WORDS; MODEL	Content-based image retrieval (CBIR) states the procedure of recovering images having similar visual content against a query image from image datasets. In CBIR, the selection of redundant and irrelevant features from images results in the semantic gap issue, which occurs during feature representation and machine learning process. The robust image representation for effective and efficient image retrieval mainly depends upon robust feature selection and classification, which also reduces the semantic gap problem of CBIR. This paper proposed an innovative method for effective and efficient CBIR. The method uses sparse complementary features for vigorous image representation, optimal feature selection based on locality-preserving projection, fuzzy c-means clustering, and soft label support vector machine for robust image classification. In CBIR, smaller and larger sizes of codebook improve the recall and precision (accuracy) of the system, respectively. Due to this reason, the proposed method introduces complementary features based on a larger size codebook, which is assembled using two small sizes of codebooks to increase CBIR performance. The three well-known image datasets (i.e. Corel-1000, Corel-1500, and Holidays) are used to assess the performance of the proposed method. The experimental evaluation highlights promising results as compared to recent methods of CBIR.																	1868-5137	1868-5145															10.1007/s12652-020-01923-1		APR 2020											
J								Robotic-based nonlinear device fault detection with sensor fault and limited capacity for communication	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Interval type-2; Robotics; Sensor; Fault detection	NETWORKED SYSTEMS; DEPLOYMENT; MODEL	This paper proposes the fault detection filter for wireless sensor network. The problem of time-varying nonlinear filter is causing randomly varying nonlinearity due to environment, quantization error and packet dropout are solved by combining Bernoulli distributed white sequence and Robotics based Type-2 T-S (Takagi-Sugeno) Fuzzy method by taking output from sensor and neighbor sensor by solving recursive linear matrix inequalities. The random link failure and uncertainty caused by measurement missing and stochastic communication failure so it can be solved by Interval Type-2 T-S Fuzzy Method Systems (IT-2 T-S FMS) with robotics by closing the feedback loop and Lypunov theory asymptotically stable and satisfying average performance level. The filters are designed for a total defect detection system with a robust mean-square asymptotic stability. So these filters are used in the truck-trailer system for practical output.																	1868-5137	1868-5145															10.1007/s12652-020-01946-8		APR 2020											
J								Improved many-to-one architecture based on discrete wavelet packet transform for industrial IoT applications using channel coding	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Industrial internet of things (IIoT); Industrial wireless sensor networks (IWSN); Impulse radio; Discrete wavelet packet transform (DWPT); Wireless channel; Channel coding		Nowadays, the Industrial Internet of Things (IIoT), plays an important role in transforming industrial environment by opening up a new area for economic growth and competitiveness in digital Industry 4.0. Using intelligent communication system based on IIoT, will help factories to obtain better profits in the industrial manufacturing markets. For this purpose, this article presents performances in terms of binary error rate of a wide-band IIoT multi-users (or sensors) system under an industrial medium. This medium is simulated as a fading channel including industrial noise, through which the robustness of our system will be studied and improved. This multi-users system is based on IDWPT and DWPT for many-to-one applications. To face the harshness of the industrial environment, an error-correcting code is added to the architecture to improve its robustness. Two error-correcting codes are used, convolutional coding and RS code for 8, 16 and 32 multi-sensors system. This will lead to a comparative analysis of the performance between this two coding techniques.																	1868-5137	1868-5145															10.1007/s12652-020-01972-6		APR 2020											
J								IOT based statistical performance improvement technique on the power output of photovoltaic system	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										PV array-photovoltaic array; IOT-internet of things; Performance analysis; Soiling; MPPT-maximum power point technique	DUST	Green electricity gains emphasis not only due to technological progress but also because of deployment of non-renewable energy resources and abundant of renewable energy resources. Global demand for the power increasing yearly is met with the help of green energy. The sources which paved the way for green power generation are solar, wind, tidal etc. The parameters of green power generation to be monitored becomes essential to enhance the power output. Without human interaction there is a simple and easy way for monitoring the parameters, with the help of Internet of Things (IOT). Among available resources PV (Photovoltaic) plant generation is advantageous among all other resources because of easy installation and power production. PV plants of MW capacity are sometimes installed in very remote locations. Interfacing the plant using internet helps to monitor and evaluate the performance of PV arrays continuously. Performance impairment could be due to several factors like drop in efficiency, hotspots, manufacturing defects, insufficient insolation, shadow, orientation of panels, soiling etc. Quality control and good engineering practice helps in overcoming most of these factors. Soiling or accumulation of dust is one important factor which requires frequent attention as dust accumulation will hinder the light reaching the solar panels. PV panel orientation varies throughout the dawn and based on geographical location tilt angle varies throughout the year. For curtailment in losses due to orientation, tilt angle, accumulation of dust, an integrated smart system with wireless technology is proposed in this paper.																	1868-5137	1868-5145															10.1007/s12652-020-01954-8		APR 2020											
J								Image segmentation evaluation: a survey of methods	ARTIFICIAL INTELLIGENCE REVIEW										Image segmentation; Segmentation evaluation; Unsupervised evaluation; Supervised evaluation; Evaluation application	QUANTITATIVE-EVALUATION; OBJECT SEGMENTATION; UNSUPERVISED EVALUATION; PERFORMANCE EVALUATION; QUALITY EVALUATION; MULTISCALE; ALGORITHMS; FRAMEWORK; DISTANCE; FUSION	Image segmentation is a prerequisite for image processing. There are many methods for image segmentation, and as a result, a great number of methods for evaluating segmentation results have also been proposed. How to effectively evaluate the quality of image segmentation is very important. In this paper, the existing image segmentation quality evaluation methods are summarized, mainly including unsupervised methods and supervised methods. Based on hot issues, the application of metrics in natural, medical and remote sensing image evaluation is further outlined. In addition, an experimental comparison for some methods were carried out and the effectiveness of these methods was ranked. At the same time, the effectiveness of classical metrics for remote sensing and medical image evaluation is also verified.																	0269-2821	1573-7462				DEC	2020	53	8					5637	5674		10.1007/s10462-020-09830-9		APR 2020											
J								Intelligent modelling to predict heat transfer coefficient of vacuum glass insulation based on thinking evolutionary neural network	ARTIFICIAL INTELLIGENCE REVIEW										Neural network intelligence model; Back-propagation neural network; Genetic algorithm; Evolutionary algorithm; Vacuum glass insulation performance; Numerical simulation; Heat transfer coefficient	GENETIC ALGORITHMS; SYSTEMS	Vacuum glass is widely used in many construction applications, including single-family homes, as a proven energy-saving method with outstanding heat preservation characteristics. The thermal insulation performance of vacuum glass is closely related to its heat transfer coefficient. In this study, we applied neural network methods to predict the heat transfer coefficients of vacuum glass. Using MATLAB, a neural network intelligence model was established, and the traditional back-propagation neural network (BPNN) was optimised. First, a genetic algorithm was used to reduce the dimensions of the independent variable. Then, the Mind Evolutionary Computation algorithm was used to optimise the initial weight and threshold. Using the optimised BPNN intelligence model to predict the heat transfer coefficient of vacuum glass insulation, we derived an average absolute error of 0.0076.																	0269-2821	1573-7462				DEC	2020	53	8					5907	5928		10.1007/s10462-020-09837-2		APR 2020											
J								Artificial intelligence, machine learning and process automation: existing knowledge frontier and way forward for mining sector	ARTIFICIAL INTELLIGENCE REVIEW										Machine learning; Artificial intelligence; Deep learning; Autonomous technology; Process automation; Mining; Autonomous haul trucks; Fuzzy logic	SUPPORT VECTOR MACHINE; FUZZY-LOGIC; NEURAL-NETWORKS; ROCK FRAGMENTATION; ORIENTED GRADIENTS; FROTH FLOTATION; PREDICTION; PERFORMANCE; EQUIPMENT; MODELS	Machine learning and artificial intelligence are the two fields of computer science dealing with the innovative idea of inducing smartness and intelligence in machines and automating complex tasks and operations through modern learning algorithms. While the rest of the operational fields have been diligent in developing new technologies, the mining industry has been lacking when it comes to applying these innovative methodologies to achieve operation autonomy with intelligence. However, this trend is beginning to change with a few researchers adopting the fields of machine learning and artificial intelligence to improve the existing technologies. This study was an attempt to review and analyze all the recent automation related work in every sector of the mining industry including mineral prospecting and exploration, mine planning, equipment selection, underground and surface equipment operation, drilling and blasting, mineral processing, etc., for establishing the existing frontiers of technological advancement. Shortcomings and challenges were identified within the current research work. Recommendations were provided to progress the existing technology by implementing deep learning, machine learning, and artificial intelligence for smart and intelligence-based evolution in the mining sector. With all of this innovative development and implementation of smart automation systems, the foundation for the mine of the future could be built, thus creating efficient, effective, and safer machines with sustainable mining operations.																	0269-2821	1573-7462				DEC	2020	53	8					6025	6042		10.1007/s10462-020-09841-6		APR 2020											
J								Simulation and modeling of microblog-based spread of public opinions on emergencies	NEURAL COMPUTING & APPLICATIONS										Microblog; Emergencies; Spread of public opinions; Simulation modeling	SOCIOPHYSICS	The crux to emergency management is public opinion control, while the key to public opinion control is the control of online spread of information about emergencies, premised on the study of online information flow about emergencies in order to effectively warn against and channel emergencies and prevent the spread and deterioration of information about a crisis in the spread of such information. Based on the complex network theory, information spread theory and disease spread theory, as well as a study of the Wenling doctor murdering case, a model about the spread of information on emergencies is constructed by combining empirical data and simulation experiments. Through the verification and analysis of the simulation results, the rationality of the model and the simulation results is proved, thus providing an approach of modeling and analysis that can be used for reference in subsequent research on the spread of information about emergencies through multi-information spread sources. This research has certain reference value for establishing an information spread control and guidance mechanism.																	0941-0643	1433-3058															10.1007/s00521-020-04919-2		APR 2020											
J								Binary Output Layer of Extreme Learning Machine for Solving Multi-class Classification Problems	NEURAL PROCESSING LETTERS										Extreme learning machines (ELM); Multi-class classification problems; One-to-one approach; Binary approach; Accuracies	SMOOTHING L-1/2 REGULARIZATION; FEEDFORWARD NEURAL-NETWORKS; GRADIENT-METHOD; ACCURACY	Considered in this paper is the design of output layer nodes of extreme learning machine (ELM) for solving multi-class classification problems with r (r >= 3) classes of samples. The common and conventional setting of output layer, called "one-to-one approach" in this paper, is as follows: The output layer contains r output nodes corresponding to the r classes. And for an input sample of the ith class (1 <= i <= r), the ideal output is 1 for the ith output node, and 0 for all the other output nodes. We propose in this paper a new "binar y approach": Suppose 2(q-1) < r <= 2(q) with q >= 2, then we let the output layer contain q output nodes, and let the ideal outputs for the r classes be designed in a binary manner. Numerical experiments carried out in this paper show that our binary approach does equally good job as, but uses less output nodes and hidden-output weights than, the traditional one-to-one approach.																	1370-4621	1573-773X				AUG	2020	52	1			SI		153	167		10.1007/s11063-020-10236-5		APR 2020											
J								Single image deraining via nonlocal squeeze-and-excitation enhancing network	APPLIED INTELLIGENCE										Single image de-raining; Convolutional Neural Network (CNN); Squeeze-and-excitation; Non-local mean; Dense network	RAIN; REMOVAL	Raindrop blur or rain streaks can severely degrade the visual quality of the images, which causes many practical vision systems to fail to work, such as autonomous driving and video surveillance. Hence, it is important to address the problem of single image de-raining. In this paper, we propose a novel deep network for single image de-raining. The proposed network consists of three stages, including encoder stage, Dense Non-Local Residual Block (DNLRB) stage, and decoder stage. As spatial contextual information has been analyzed to be meaningful for image de-raining (Huang et al. ??), we adopt squeeze-and-excitation enhancing on feature maps in each convolution layer for capturing spatial contextual information. In addition, to better leverage spatial contextual information for extracting rain components, the non-local mean operation has been embed in DNLRB. Both quantitative and qualitative experimental results demonstrate the proposed method performs favorably against the state-of-the-art de-raining methods. The source codes will be available at .																	0924-669X	1573-7497				SEP	2020	50	9					2932	2944		10.1007/s10489-020-01693-5		APR 2020											
J								Non-convex low-rank representation combined with rank-one matrix sum for subspace clustering	SOFT COMPUTING										Subspace clustering; Non-convex low-rank representation; Block coordinate descent; Rank-one matrix	DECISION-MAKING; ALGORITHM	Exploring the multiple subspace structures of data such as low-rank representation is effective in subspace clustering. Non-convex low-rank representation (NLRR) via matrix factorization is one of the state-of-the-art techniques for subspace clustering. However, NLRR cannot scale to problems with large n (number of samples) as it requires either the inversion of an nxn linear system. To address this issue, we propose a novel approach, NLRR++, which reformulates NLRR as a sum of rank-one components, and apply a column-wise block coordinate descent to update each component iteratively. NLRR++ reduces the time complexity per iteration from O(n3) and the memory complexity from O(n2), where m is the dimensionality and d is the target rank (usually dMUCH LESS-THANmMUCH LESS-THANn). Our experimental results on simulations and real datasets have shown the efficiency and effectiveness of NLRR++. We demonstrate that NLRR++ is not only much faster than NLRR, but also scalable to large datasets such as the ImageNet dataset with 120K samples.																	1432-7643	1433-7479				OCT	2020	24	20					15317	15326		10.1007/s00500-020-04865-0		APR 2020											
J								Frequency domain CNN and dissipated energy approach for damage detection in building structures	SOFT COMPUTING										Structural health monitoring; Damage detection; Convolutional neural network; Identification system	CONVOLUTIONAL NEURAL-NETWORKS; SYSTEM-IDENTIFICATION; MODE SHAPE; VIBRATION; RECOGNITION; HYSTERESIS; EXTRACTION	Recent developments tools and techniques for structural health monitoring allow the design of early warning systems for the damage diagnosis and structural assessment. Most methods to damage detection involve vibration data analysis by using identification systems that generally require a mathematical model and much information about the system, such as parameters and states that are mostly unknown. In this paper, a novel frequency domain convolutional neural network (FDCNN) proposed aims to design an identification system for damage detection based on Bouc-Wen hysteretic model. FDCNN, unlike other works, only requires acceleration measurements for damage diagnosis that are very sensitive to environmental noise. In contrast to neural network (NN) and time domain convolutional neural network, FDCNN reduces the computational time required for the learning stage and adds robustness against noise in data. The FDCNN includes random filters in the frequency domain to avoid measurement noise using a spectral pooling operation, which is useful when the system bandwidth is unknown. Incorrect filtering can produce unwanted results, as a shifted and attenuation signal relative to the original. Moreover, FDCNN allows overcoming the parameterization problem in nonlinear systems, which is often difficult to achieve. In order to validate the proposed methodology, a comparison between two different architectures of convolutional neural networks is made, showing that proposed CNN in frequency domain brings better performance in the identification system for damage diagnosis in building structures. Experimental results from reducing scale two-storey building confirm the effectiveness of the proposed.																	1432-7643	1433-7479				OCT	2020	24	20					15821	15840		10.1007/s00500-020-04912-w		APR 2020											
J								A modified method of generating Z-number based on OWA weights and maximum entropy	SOFT COMPUTING										Z-number; OWA; Maximum entropy; Reliability; Decision making	FUZZY AGGREGATION OPERATORS; SETS; MODEL	How to generate Z-number is an important and open issue in the uncertain information processing of Z-number. In Kang et al. (Int J Intell Syst 33(8):1745-1755, 2018), a method of generating Z-number using OWA weight and maximum entropy is investigated. However, the meaning of the method in Kang et al. (2018) is not clear enough according to the definition of Z-number. Inspired by the methodology in Kang et al. (2018), we modify the method of determining Z-number based on OWA weights and maximum entropy, which is more clear about the meaning of Z-number. In addition, the model of generating Z-number under the environment of group decision making is well investigated based the modified model. Some numerical examples are used to illustrate the effectiveness of the proposed methodology.																	1432-7643	1433-7479				OCT	2020	24	20					15841	15852		10.1007/s00500-020-04914-8		APR 2020											
J								Physical significance of chemical processes and Lorentz's forces aspects on Sisko fluid flow in curved configuration	SOFT COMPUTING										Unsteady flow; Sisko fluid; Homogeneous-heterogeneous reactions; Non-uniform heat sink; source	HOMOGENEOUS-HETEROGENEOUS REACTIONS; GENERALIZED FOURIERS; CARREAU FLUID; MODEL; CONVECTION; ALGORITHM; NANOFLUID; SURFACE	Current determination is committed to characterize the features of curved surface for Sisko fluid in the presence of Lorentz's forces. Heat-mass relocation exploration is conducted in the presence of homogeneous-heterogeneous processes and non-uniform heat sink/source. Similarity variables are designated to transmute nonlinear PDEs into ODEs. These intricate ordinary differential expressions assessing the flow situation are handled efficaciously by manipulating bvp4c scheme. Graphical demonstration is deliberated to scrutinize the variation in pressure, velocity, temperature and concentration profiles with respect to flow regulating parameters. Numerical data are displayed through tables in order to surmise variation in surface drag force and heat transport rate. It is noted that radius of curvature and temperature-dependent heat sink/source significantly affect heat-mass transport mechanisms for curved surface. Furthermore, graphical analysis reveals that velocity profile of Sisko magneto-fluid enhances for augmented values of curvature parameter. Additionally, it is evaluated that increasing values of heat source parameter and Lorentz's forces, pressure profile exhibited the diminishing behavior.																	1432-7643	1433-7479				NOV	2020	24	21					16213	16223		10.1007/s00500-020-04935-3		APR 2020											
J								Necessary and Sufficient Polynomial Constraints on Compatible Triplets of Essential Matrices	INTERNATIONAL JOURNAL OF COMPUTER VISION										Multiview geometry; Essential matrix; Compatible triplet; Polynomial constraints	RELATIVE POSE; MOTION	The essential matrix incorporates relative rotation and translation parameters of two calibrated cameras. The well-known algebraic characterization of essential matrices, i.e. necessary and sufficient conditions under which an arbitrary matrix (of rank two) becomes essential, consists of a single matrix equation of degree three. Based on this equation, a number of efficient algorithmic solutions to different relative pose estimation problems have been proposed in the last two decades. In three views, a possible way to describe the geometry of three calibrated cameras comes from considering compatible triplets of essential matrices. The compatibility is meant the correspondence of a triplet to a certain configuration of calibrated cameras. The main goal of this paper is to give an algebraic characterization of compatible triplets of essential matrices. Specifically, we propose necessary and sufficient polynomial constraints on a triplet of real rank-two essential matrices that ensure its compatibility. The constraints are given in the form of six cubic matrix equations, one quartic and one sextic scalar equations. An important advantage of the proposed constraints is their sufficiency even in the case of cameras with collinear centers. The applications of the constraints may include relative camera pose estimation in three and more views, averaging of essential matrices for incremental structure from motion, multiview camera auto-calibration, etc.																	0920-5691	1573-1405				DEC	2020	128	12					2781	2793		10.1007/s11263-020-01330-1		APR 2020											
J								A novel NURBS surface approach to statistically monitor manufacturing processes with point cloud data	JOURNAL OF INTELLIGENT MANUFACTURING										High-density measurements; Non-contact scanning systems; NURBS surfaces; Statistical process control; Surface monitoring	CONTROL CHARTS; QUALITY	As sensor and measurement technologies advance, there is a continual need to adapt and develop new Statistical Process Control (SPC) techniques to effectively and efficiently take advantage of these new datasets. Currently high-density noncontact measurement technologies, such as 3D laser scanners, are being implemented in industry to rapidly collect point clouds consisting of millions of data points to represent a manufactured parts' surface. For their potential to be realized, SPC methods capable of handling these datasets need to be developed. This paper presents an approach for performing SPC using high-density point clouds. The proposed approach is based on transforming the high-dimensional point clouds into Non-Uniform Rational Basis Spline (NURBS) surfaces. The control parameters for these NURBS surfaces are then monitored using a surface monitoring technique. In this paper point clouds are simulated to determine the performance of the proposed approach under varying fault scenarios.																	0956-5515	1572-8145															10.1007/s10845-020-01574-1		APR 2020											
J								Utilization of a convolutional method for Alzheimer disease diagnosis	MACHINE VISION AND APPLICATIONS										Computer-assisted diagnosis (CAD); Alzheimer's disease (AD); Neuroimaging; Machine learning	COMPUTER-AIDED DIAGNOSIS; MILD COGNITIVE IMPAIRMENT; FEATURE REPRESENTATION; HIPPOCAMPAL ATROPHY; NEURAL-NETWORKS; MRI; CLASSIFICATION; DEMENTIA; FUSION; ALGORITHM	With the increasing number of cases as well as care costs, Alzheimer's disease has gained more interest in several scientific communities especially medical and computer science. Clinical and analytical tests are widely accepted techniques for detecting Alzheimer cases. However, early detection can help prevent damage to brain tissue and heal it with proper treatment. Interpreting brain images is considered as a time-consuming task with a high error-prone. Recently, advanced machine learning methods have successfully proved high performance in various fields including brain image analysis. These existing techniques, which become more used for clinical disease detection, present challenging wrongness sensibility to detect aberrant values or areas in the human brain. We conducted our work to automate the detection of the damaged areas and diagnose Alzheimer's disease. Our method can segment MRI images, identify brain lesions and the different stages of Alzheimer's disease. We evaluated our method using ample cases form public databases to demonstrate that our proposition performed reliable and effective results. Our proposal achieved an accuracy of 94.73%, a recall rate of 93.82%, and an F1-score of 92.8%. Also, the detection precision reached 91.76% with a sensitivity of 92.48% and a specificity rate of 90.64%. Our method creates an important way to optimize the imaging process via an automated computer-assisted diagnosis using potential deep learning methods to increase the consistency and accuracy of Alzheimer's disease diagnosis worldwide.																	0932-8092	1432-1769				APR 18	2020	31	4							25	10.1007/s00138-020-01074-5													
J								Coupling cell detection and tracking by temporal feedback	MACHINE VISION AND APPLICATIONS										Tracking; Detection; Segmentation; Probabilistic models; Cell populations	FRAMEWORK; ALGORITHM	The tracking-by-detection strategy is the backbone of many methods for tracking living cells in time-lapse microscopy. An object detector is first applied to the input images, and the resulting detection candidates are then linked by a data association module. The performance of such methods strongly depends on the quality of the detector because detection errors propagate to the linking step. To tackle this issue, we propose a joint model for segmentation, detection and tracking. The model is defined implicitly as limiting distribution of a Markov chain Monte Carlo algorithm and contains a temporal feedback, which allows to dynamically alter detector parameters using hints given by neighboring frames and, in this way, correct detection errors. The proposed method can integrate any detector and is therefore not restricted to a specific domain. The parameters of the model are learned using an objective based on empirical risk minimization. We use our method to conduct large-scale experiments for confluent cultures of endothelial cells and evaluate its performance in the ISBI Cell Tracking Challenge, where it consistently scored among the best three methods.																	0932-8092	1432-1769				APR 18	2020	31	4							24	10.1007/s00138-020-01072-7													
J								Bipolar fuzzy Petri nets for knowledge representation and acquisition considering non-cooperative behaviors	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Fuzzy Petri net (FPN); Bipolar fuzzy set (BFS); Knowledge acquisition; Knowledge representation; Expert system	FAULT-DIAGNOSIS; RISK-ASSESSMENT; FAILURE MODE; ALGORITHM; SETS	Fuzzy Petri nets (FPNs) are a promising modeling tool for knowledge representation and reasoning. As a new type of FPNs, bipolar fuzzy Petri nets (BFPNs) are developed in this article to overcome the shortcomings and improve the performance of traditional FPNs. In order to depict expert knowledge more accurately, the BFPN model adopts bipolar fuzzy sets (BFSs), which are characterized by the satisfaction degree to property and the satisfaction degree to its counter property, to represent knowledge parameters. Because of the increasing scale of expert systems, a concurrent hierarchical reasoning algorithm is introduced to simplify the structure of BFPNs and reduce the computation complexity of knowledge reasoning algorithm. In addition, a large group expert weighting method is proposed for knowledge acquisition by taking experts' non-cooperative behaviors into account. A realistic case of risk index evaluation system is presented to show the effectiveness and practicality of the proposed BFPNs. The result shows that the new BFPN model is feasible and efficient for knowledge representation and acquisition.																	1868-8071	1868-808X				OCT	2020	11	10					2297	2311		10.1007/s13042-020-01118-2		APR 2020											
J								Accelerated inexact matrix completion algorithm via closed-form q-thresholding (q=1/2,2/3) operator	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Matrix completion; l(q) regularization; q-thresholding operator; Nesterov's rule; Power method	LOW-RANK; VARIABLE SELECTION; LEAST-SQUARES; REGULARIZATION; RELAXATION	l(q) (0 < q < 1) regularization is a dominating strategy for matrix completion problems. The main goal of nonconvex l(q) regularization based algorithm is to find a so-called low-rank solution.Unfortunately, most existing algorithms suffer from full singular value decomposition (SVD), and thus become inefficient for large-scale matrix completion problems. To alleviate this limitation, in this paper we propose an accelerated inexact algorithm to handle such problem. The key idea is to employ the closed-form q-thresholding (q = 1/2, 2/3) operator to approximate the rank of a matrix. The power method and the special "sparse plus low-rank" structure of the matrix iterates are adopted to allow efficient SVD. Besides, we employ Nesterov's accelerated gradient method and continuation technique to further accelerate the convergence speed of our proposed algorithm. A convergence analysis shows that the sequence {X-t} generated by our proposed algorithm is bounded and has at least one accumulation point. Extensive experiments have been conducted to study its recovery performance on synthetic data, image recovery and recommendation problems. All results demonstrate that our proposed algorithm is able to achieve comparable recovery performance, while being faster and more efficient than state-of-the-art methods.																	1868-8071	1868-808X				OCT	2020	11	10					2327	2339		10.1007/s13042-020-01121-7		APR 2020											
J								Inter-vehicle distance-based location aware multi-hop routing in vehicular ad-hoc network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										VANET; LAMHR; Next-hop; Inter-vehicle distance; Location prediction	PROTOCOL	The vehicular ad-hoc network (VANET) is a self-organized wireless ad-hoc network created by movable vehicles of the limited transmission rangeRto transmit traffic-related information to nearby vehicles on demand. It is worthy to mention that in VANET the vehicles are the source of transmission of data packets so that it works as a node. Vehicles restricted transmission range causes the route to participate in data transmission from the sourceSto destinationDmay vanish seldom. Therefore, a stable route from source to destination needs to deliver data packets at the intended destination. This paper presented an inter-vehicle distance-based location-aware multi-hop routing (LAMHR) in the vehicular ad-hoc network to enhance the vehicle's connectivity. LAMR predicts the future location of the nodes to select an optimal next forwarder towards the destination to establish a stable route from source to destination. In this paper, to obtain the inter-vehicle distance, a geometry-based localization technique has developed that impact on the vehicle's connectivity. Performance of the LAMHR has evaluated in terms of the path vanish, node broadcasting time, packet delivery ratio and throughput. Through the simulated results, it has shown that the proposed LAMHR model gives high performance comparatively the existing fuzzy logic-based directional location routing (FLDLR), directional-location aided routing (D-LAR) and location aided routing (LAR) protocols.																	1868-5137	1868-5145															10.1007/s12652-020-01947-7		APR 2020											
J								Text tendency analysis based on multi-granularity emotional chunks and integrated learning	NEURAL COMPUTING & APPLICATIONS										Text orientation analysis; Multi-granularity emotional chunking; Machine learning; BPSO		Internet is a new type of information exchange tool that has developed with the times. Now, it has been integrated into all aspects of our study and life. At the same time, in the era of the rise of social media, there are more and more platforms on the Internet. A variety of critical texts have also exploded in these platforms. In these opinions and comments, the subjective opinions of the presenters are included, and the emotional tendencies of the commenters are expressed. Nowadays, subjective text information resources are huge. One of the major problems to be solved by management objects facing information management is how to manage them effectively so that users can quickly and accurately find the required information. Therefore, classifying these text tendencies and mining the potential value in the text has broad application prospects. Based on the text granularity and processing efficiency, this paper conducts multi-granularity emotional block partitioning on network texts and compares the sentiment analysis under different granularities. Furthermore, a random subspace integrated learning text sentiment classification method based on BPSO (binary particle swarm optimization) is proposed to analyze text orientation. By simulating news site comments and e-commerce website reviews such as Taobao, the convergence analysis of BPSO in the optimization of the number of base classifiers shows that the BPSO algorithm can be applied to the random subspace method well, and the accuracy of the classification results is high.																	0941-0643	1433-3058															10.1007/s00521-020-04901-y		APR 2020											
J								Individual credit ranking by an integrated interval type-2 trapezoidal fuzzy Electre methodology	SOFT COMPUTING										Credit ranking model; Electre; AHP; Interval type-2 trapezoidal fuzzy numbers	DECISION-MAKING; TOPSIS METHOD; AHP; FIRMS; TECHNOLOGY; QUALITY; DELPHI; MODEL	In this paper, first, it is aimed to determine the most important criteria which affect the credit evaluation process. A type-2 trapezoidal fuzzy analytic hierarchy process method is proposed to analyze the criteria influencing the credit evaluation. Then, a ranking of experts is obtained using a type-2 trapezoidal fuzzy Electre (elimination and choice translating reality English) method. Lastly, the applicants' ranking is determined as a real case. This method is aimed to be used by public and private banks to improve their credit ranking and evaluation strategies. Finally, the applicability and feasibility of the proposed approach are demonstrated by providing the results and sensitivity analysis.																	1432-7643	1433-7479				NOV	2020	24	21					16149	16163		10.1007/s00500-020-04929-1		APR 2020											
J								Lightning attachment procedure optimization algorithm for nonlinear non-convex short-term hydrothermal generation scheduling	SOFT COMPUTING										Short-term hydrothermal scheduling; Non-convex optimization problem; Lightning attachment procedure optimization; Valve point loading effect	CODED GENETIC ALGORITHM; CHAOTIC DIFFERENTIAL EVOLUTION; PARTICLE SWARM OPTIMIZATION; BEE COLONY ALGORITHM; SEARCH ALGORITHM; SYSTEMS; HYBRID	Short-term hydrothermal scheduling (STHS) is considered an important problem in the field of power system economics. The solution of this problem gives the hourly output of power generation schedule of the available hydro and thermal power units, which leads to minimization of the total fuel cost of thermal units for a given period of a time. The optimal generation of STHS is considered as a complicated and nonlinear optimization problem with a set of equality and inequality constraints such as the valve point loading effect of thermal units, the power transmission loss and the load balance. This paper proposes lightning attachment procedure Optimization (LAPO) algorithm for solving the nonlinear non-convex STHS optimization problem in order to minimize the operating fuel cost of thermal units with satisfying the operating constraints of the system. The performance of LAPO algorithm is validated using three different test systems considering the valve point loading effects of thermal units and the power transmission losses. The obtained results prove the effectiveness and superiority of LAPO algorithm for solving the STHS problem compared with other well-known optimization techniques.																	1432-7643	1433-7479				NOV	2020	24	21					16225	16248		10.1007/s00500-020-04936-2		APR 2020											
J								Multi-label classification and knowledge extraction from oncology-related content on online social networks	ARTIFICIAL INTELLIGENCE REVIEW										Cancer; Social networks; Natural language processing; Machine learning; Classification; Knowledge extraction	BREAST-CANCER; PROSTATE-CANCER; SUPPORT GROUPS; SKIN-CANCER; MEDIA USE; TWITTER; ENGAGEMENT; BEHAVIORS; EDUCATION; MOTIVES	This study aims at automatic processing and knowledge extraction from large amounts of oncology-related content from online social networks (OSN). In this context, a large number of OSN textual posts concerning major cancer types are automatically scraped and structured using natural language processing techniques. Machines are trained to assign multiple labels to these posts based on the type of knowledge enclosed, if any. Trained machines are used to automatically classify large-scale textual posts. Statistical inferences are made based on these predictions to extract general concepts and abstract knowledge. Different approaches for constructing document feature vectors showed no tangible effect on the classification accuracy. Among different classifiers, logistic regression achieved the highest overall accuracy (96.4%) and (F1) over bar (73.4) in a 13-way multi-label classification of textual posts. The most common topic was seeking or providing moral support for cancer patients, followed by providing technical information about cancer causes and treatments. The most common causes and treatments of different types of cancer on OSN are also automatically detected in this study. Seeking or providing moral support for cancer patients shared the largest overlap with other topics, i.e. moral support tends to be present even in OSN posts which focus on other topics. On the other hand, providing technical information about cancer diagnosis or prevention were the most isolated topics, where OSN posts tend not to allude to other topics. OSN posts which seek financial support only overlap with the moral support topic, if any. Our methodology and results provide public health professionals with an opportunity to monitor what topics and to which extent are being discussed on OSN, what specific information and knowledge are being disseminated over OSN, and to assess their veracity in close to real time. This helps them to develop policies that encourage, discourage, or modify the consumption of viral oncology-related information on OSN.																	0269-2821	1573-7462				DEC	2020	53	8					5957	5994		10.1007/s10462-020-09839-0		APR 2020											
J								An improved case based reasoning method and its application in estimation of surface quality toward intelligent machining	JOURNAL OF INTELLIGENT MANUFACTURING										ICBR method; ANN model; GPR model; VPSO algorithm	NEURAL-NETWORK; CBR SYSTEM; PREDICTION; OPTIMIZATION; ROUGHNESS; DESIGN; MODEL	In the high speed milling process, the accurate predictions of surface roughness and residual stress can avoid the deterioration of machined surface quality. But it's hard to estimate the surface roughness and residual stress under different tool wear status and cutting parameters. In this work, a novel intelligent reasoning method-improved case based reasoning (ICBR) was proposed to predict the surface roughness and residual stress. The inputs of ICBR are cutting parameters and tool wear status. The corresponding outputs of ICBR are surface roughness and residual stress. In the ICBR, K-nearest neighbor method and artificial neural network (ANN) as case retrieval was introduced to retrieve the K similar cases to the inputs. Through retrieving K similar cases, the Gaussian process regression (GPR) model as case reuse was established to output the surface roughness and residual stress. The vibration particle swarm optimization algorithm is proposed to optimize the ANN and GPR models. The high speed milling experiments of Compacted Graphite Iron was performed to validate the performance of ICBR. The experimental results showed that the cutting speed is the most important factor affecting the surface roughness. The feed rate is the most important factor affecting the residual stress. The ICBR gives the accurate estimation of surface roughness with the Mean Absolute Percentage Error of 11.6%. As for residual stress, the prediction accuracy using ICBR is 87.5%. Compared with Back-Propagation neural network, standard CBR and GPR models, the ICBR has better predictive performance and can be used for estimations of surface roughness and residual stress in the actual machining process.																	0956-5515	1572-8145															10.1007/s10845-020-01573-2		APR 2020											
J								Content-based image retrieval using block truncation coding based on edge quantization	CONNECTION SCIENCE										Contest-based image retrieval (CBIR); edge detection; block truncation technique; edge structure feature; colour histogram feature	COLOR; FEATURES; SYSTEM; TEXT; DESCRIPTOR; SCHEME; QUERY	In this paper, we propose an effective image retrieval approach using block truncation coding compressed data stream based on edge-based quantization (EQBTC). First, an image is compressed into corresponding quantisers and a bitmap image by EQBTC. Then, the quantisers are used for colour feature extraction, whereby the bitmap image and grey image are used for luminance and edge feature extraction. Subsequently, two image features, the colour histogram feature (CHF) and the overall structure feature (OSF), are computed to measure the similarity between two images using a specific distance metric computation. The results presented in this paper demonstrate that the proposed model is superior to the block truncation coding image retrieval scheme and some earlier proposed methods.																	0954-0091	1360-0494															10.1080/09540091.2020.1753174		APR 2020											
J								A community partitioning algorithm based on network enhancement	CONNECTION SCIENCE										Community partitioning; network enhancement; graph convolution; connectivity; symmetric doubly stochastic matrix	COMPLEX NETWORKS	In recent years, as an effective method to mine information from the complex network, community discovery has been widely used in social network, financial risk control and other fields. However, the existing community discovery algorithms are not effective in dealing with complex network which always contains fuzzy community structure. With the help of graph convolution, the proposed algorithm defines the connectivity between any nodes in a network and constructs the symmetric doubly stochastic matrix. Then, the algorithm enhances the network by the nonlinear transformation of the eigenvalues of the symmetric doubly stochastic matrix and makes the original fuzzy community structure become clear. Experimental results show that this method can effectively sharpen the community structure of a network and improve the effect of community partitioning.																	0954-0091	1360-0494															10.1080/09540091.2020.1753172		APR 2020											
J								Multi-Keyword ranked search based on mapping set matching in cloud ciphertext storage system	CONNECTION SCIENCE										Cloud storage system; ciphertext retrieval; outsourced encrypted data; semantic extension; mapping set matching		Most of the existing outsourced encrypted data schemes are retrieved based on the query keyword entered by authorised users. However, with the increase of the data scale in the cloud storage system, the retrieval efficiency of existing solutions has not been significantly improved. In this paper, a multi-keyword ranked search scheme for ciphertext based on mapping set matching (MSMR) is proposed, where (1) The private cloud server matches the keyword numbering set corresponding to the document index vector and the keyword numbering set corresponding to the query vector and sends the document identifier of the matching keyword numbering to the public cloud server. The public cloud server filters the documents irrelevant to the query request according to the document identifier corresponding to the matching keyword numbering, which effectively reduces the time spent in calculating the correlation score, and (2) the document index vector and query vector are segmented before encrypting them out, reducing the time to construct such vectors. Theoretical analysis shows that the proposed scheme is secure in the known ciphertext model. Experimental results confirm that whenever the data scale grows, the improvement of MSMR retrieval efficiency is more significant.																	0954-0091	1360-0494															10.1080/09540091.2020.1753175		APR 2020											
J								Enhanced image no-reference quality assessment based on colour space distribution	IET IMAGE PROCESSING										feature extraction; image enhancement; image colour analysis; image resolution; reference image; enhanced image quality; grey-scale image quality; low-light images; no-reference quality assessment; colour space distribution; NR image quality assessment model; clear target image; hypothetical reference images; input images; X-ray images; underwater images; dust images; FSIM	SCENE	In this study, the authors investigate the problem of enhanced image no-reference (NR) quality assessment. For resolving the problem of the enhanced images, it is difficult to obtain reference images, this study proposes an NR image quality assessment (IQA) model based on colour space distribution. Given an enhanced image, our method first uses a gist to select a clear target image in which the scene, colour and quality are similar to the hypothetical reference images. And then, the colour transfer is used between the input images and target images to construct the reference image. Next, the appropriate IQA method is used to assess enhanced image quality. The absolute colour difference and feature similarity (FSIM) are used to measure the colour and grey-scale image quality, respectively. Extensive experiments demonstrate that the proposed method is good at evaluating enhanced image quality for X-ray, dust, underwater and low-light images. The experimental results are consistent with human subjective evaluation and achieve good assessment effects.																	1751-9659	1751-9667				APR 17	2020	14	5					807	817		10.1049/iet-ipr.2019.0856													
J								Improved strategy for human action recognition; experiencing a cascaded design	IET IMAGE PROCESSING										motion estimation; video signal processing; image classification; feature extraction; object detection; image colour analysis; image sequences; entropy; computer vision; image segmentation; dimensionality reduction; National Television System Committee colour; parallel design utilising attention-based motion estimation; segmentation module; feature selection technique; improved correct classification rate; human action recognition; human motion analysis; computer vision community; research domain; video surveillance; patient monitoring systems; pedestrian detection; improved cascaded design; frame segmentation; features extraction; red-green-blue colour space enhancement technique	FEATURE-SELECTION; FEATURES FUSION; SEGMENTATION; SYSTEM; CLASSIFICATION; SILHOUETTE; FRAMEWORK; MODEL	Human motion analysis has received a lot of attention in the computer vision community during the last few years. This research domain is supported by a wide spectrum of applications including video surveillance, patient monitoring systems, and pedestrian detection, to name a few. In this study, an improved cascaded design for human motion analysis is presented; it consolidates four phases: (i) acquisition and preprocessing, (ii) frame segmentation, (iii) features extraction and dimensionality reduction, and (iv) classification. The implemented architecture takes advantage of CIE-Lab and National Television System Committee colour spaces, and also performs contrast stretching using the proposed red-green-blue* colour space enhancement technique. A parallel design utilising attention-based motion estimation and segmentation module is also proposed in order to avoid the detection of false moving regions. In addition to these contributions, the proposed feature selection technique called entropy controlled principal components with weights minimisation, further improves the classification accuracy. The authors claims are supported with a comparison between six state-of-the-art classifiers tested on five standard benchmark data sets including Weizmann, KTH, UIUC, Muhavi, and WVU, where the results reveal an improved correct classification rate of 96.55, 99.50, 99.40, 100, and 100%, respectively.																	1751-9659	1751-9667				APR 17	2020	14	5					818	829		10.1049/iet-ipr.2018.5769													
J								Stereo matching for infrared images using guided filtering weighted by exponential moving average	IET IMAGE PROCESSING										stereo image processing; filtering theory; infrared imaging; image matching; stereo matching; infrared images; exponential moving average; infrared imaging; visible light imaging; infrared stereo imaging; guided-image techniques; advanced edge-aware filters; guided-image filtering scheme; infrared image pair	COST AGGREGATION; ALGORITHM; ACCURATE	Infrared imaging is less susceptible to illumination conditions and haze than visible light imaging. The advantage makes infrared sensing suitable for providing remote visibility with reduced distortion. However, infrared images tend to have low resolution and lack rich textures that facilitate stereo matching. To enhance the applicability of infrared stereo imaging, the authors re-examine the guided-image techniques to include advanced edge-aware filters for aggregation and propose a novel guided-image filtering scheme here. Based on the exponential moving average, the weights are recursively calculated such that all pixels on the infrared image pair can contribute to a discrepancy cost. The arrangement allows additional pixels to be involved in the cost aggregation to reduce the demand for rich texture. Experimental results using the colour and thermal stereo (CATS) benchmark testbed demonstrate that the proposed approach outperforms several state-of-art approaches in generating accurate disparity maps.																	1751-9659	1751-9667				APR 17	2020	14	5					830	837		10.1049/iet-ipr.2019.0144													
J								Image patch prior learning based on random neighbourhood resampling for image denoising	IET IMAGE PROCESSING										image denoising; mixture models; learning (artificial intelligence); Gaussian processes; expectation-maximisation algorithm; random neighbourhood resampling; Gaussian mixture model; natural image patches; expectation maximisation algorithm; parameter estimation; RNR; enhanced GMM learning algorithm; expected patch log-likelihood framework; EGMM algorithm; image denoising methods; image patch prior learning; EM algorithm; EPLL framework	GAUSSIAN MIXTURE MODEL; BOOTSTRAP METHODS; ESTIMATING UNCERTAINTY; PARAMETERS	Image patch priors become a popular tool for image denoising. The Gaussian mixture model (GMM) is remarkably effective in modelling natural image patches. However, GMM prior learning using the expectation maximisation (EM) algorithm is sensitive to the initialisation, often leading to low convergence rate of parameter estimation. In this study, a novel sampling method called random neighbourhood resampling (RNR) is proposed to improve the accuracy and efficiency of parameter estimation. An enhanced GMM (EGMM) learning algorithm is further developed by incorporating RNR into the EM algorithm to initialise and update the GMM prior. The learned EGMM prior is applied in the expected patch log-likelihood (EPLL) framework for image denoising. The effectiveness and performance of the proposed RNR and EGMM algorithm are demonstrated via extensive experimental results comparing with the state-of-the-art image denoising methods, the experimental results show the higher PSNR result of the denoised images using the proposed method. Meanwhile, the authors verified that the proposed method can efficiently reduce the time of image denoising compared with the basic EPLL method.																	1751-9659	1751-9667				APR 17	2020	14	5					838	844		10.1049/iet-ipr.2018.5403													
J								Blind video quality assessment via spatiotemporal statistical analysis of adaptive cube size 3D-DCT coefficients	IET IMAGE PROCESSING										discrete cosine transforms; distortion; regression analysis; video signal processing; video databases; feature extraction; spatiotemporal phenomena; blind video quality assessment; spatiotemporal statistical analysis; adaptive cube size 3D-DCT coefficients; robust video quality assessment model; video content; reference video; distorted video; spatiotemporal contents; human visual system properties; spatiotemporal frequency bands; EPFL-PoliMi video database; NR-VQA models; no reference model; three-dimensional-discrete cosine transform domain; HVS properties; feature extraction; linear regression analysis	NETWORKED VIDEO	There is an urgent need for a robust video quality assessment (VQA) model that can efficiently evaluate the quality of a video content varying in terms of the distortion and content type in the absence of the reference video. Considering this need, a novel no reference (NR) model relying on the spatiotemporal statistics of the distorted video in a three-dimensional (3D)-discrete cosine transform (DCT) domain is proposed in this study. While developing the model, as the first contribution, the video contents are adaptively segmented into the cubes of different sizes and spatiotemporal contents in line with the human visual system (HVS) properties. Then, the 3D-DCT is applied to these cubes. Following that, as the second contribution, different efficient features (i.e. spectral behaviour, energy variation, distances between spatiotemporal frequency bands, and DC variation) associated with the contents of these cubes are extracted. After that, these features are associated with the subjective experimental results obtained from the EPFL-PoliMi video database using the linear regression analysis for building the model. The evaluation results present that the proposed model, unlike many top-performing NR-VQA models (e.g. V-BLIINDS, VIIDEO, and SSEQ), achieves high and stable performance across the videos with different contents and distortions.																	1751-9659	1751-9667				APR 17	2020	14	5					845	852		10.1049/iet-ipr.2019.0275													
J								KSR-BOF: a new and exemplified method (as KSRs method) for image classification	IET IMAGE PROCESSING										computer vision; feature extraction; image classification; linear codes; image representation; image coding; pattern recognition; computer vision; max-pooling; sum-pooling; average-pooling; dictionary atoms; coding coefficients; pooling methods; image classification task; KSR method; classification accuracy; compact representation; discriminative final representation; two-part KSR; bag-of-features; locality-constrained linear coding; linear distance coding; sparse coding; scene classification; KSR-BOF method; K-strongest responses; UC Merced land; 19-class satellite scene		Image classification is very important in pattern recognition and computer vision, where, for integrating final representation, feature pooling methods of the max-pooling, sum-pooling and average-pooling have been widely used. In this study, the authors propose a new method called K-strongest responses (KSRs) on the dictionary atoms for integrating the coding coefficients to generate the final representation that is compared with the previous pooling methods, produces better performance for the image classification task. On the basis of the KSR method, to improve classification accuracy and generate more compact and discriminative final representation, a new framework consisting of two-part KSR and bag-of-features is proposed. To evaluate the performance of the proposed method and framework, they apply it to locality-constrained linear coding, linear distance coding and sparse coding by using two datasets from benchmarks of scene classification: 19-class satellite scene and UC Merced Land. The results show that the coding coefficients integrated by their method and framework are more discriminative than other methods.																	1751-9659	1751-9667				APR 17	2020	14	5					853	861		10.1049/iet-ipr.2019.0613													
J								Hybrid higher-order total variation model for multiplicative noise removal	IET IMAGE PROCESSING										image reconstruction; image restoration; variational techniques; iterative methods; image denoising; novel hybrid higher-order TV regularisation model; MNR; image prior information; second-order derivatives; novel higher-order regulariser; preferable equivalent formulation; derived equivalent formulation; HHTV method; image quality; convergence speed; image details; undesirable staircase effects; sharp edges; higher-order extensions; variational methods; image processing; multiplicative noise removal; higher-order total variation model; image edges; staircase effect	TOTAL GENERALIZED VARIATION; SPECKLE REDUCTION; IMAGE; FILTER	As an important, challenging, and difficult problem in image processing, multiplicative noise removal (MNR) has attracted great attention. To this end, many variational methods have been effectively proposed in the past few decades. Among these variational methods, total variation (TV) and its higher-order extensions are very effective, where the former can preserve sharp edges but cause some undesirable staircase effects and the latter can better reduce the staircase effects but sometimes smooth the image details. To overcome the drawbacks while taking full use of their merits, the authors propose a novel hybrid higher-order TV regularisation model for MNR, in which the novelty of the proposed model consists of combining the image prior information of first-order and second-order derivatives to propose a novel higher-order regulariser, named as hybrid higher-order TV (HHTV). More specifically, a more preferable equivalent formulation of HHTV is derived. Then, they use the derived equivalent formulation to design an efficient alternating iterative algorithm to solve the proposed model. Finally, the experimental results demonstrate that the proposed HHTV method outperforms several state-of-the-art methods in terms of image quality and convergence speed.																	1751-9659	1751-9667				APR 17	2020	14	5					862	873		10.1049/iet-ipr.2018.5930													
J								Privacy preserving search index for image databases based on SURF and order preserving encryption	IET IMAGE PROCESSING										image retrieval; visual databases; cloud computing; cryptography; data privacy; feature extraction; database indexing; image matching; efficient indexing methods; privacy preserving search approach; possibly curious cloud servers; secure inverted index; robust features; SURF; extracted visual words; accurate image retrieval; rotation invariant feature; local HSV histogram; weighted term frequency-inverse document frequency; OPE encryption; encryption time; normalised frequencies; search time; sequential search scheme; search schemes; privacy preserving search index; personal image databases	RETRIEVAL METHOD; EFFICIENT	Managing large personal image databases requires efficient privacy preserving indexing methods to allow for their outsourcing to possibly curious cloud servers. To construct a secure inverted index in this paper, first, visual words are extracted from stored images based on the Speeded-Up and Robust Features (SURF). Next, Order Preserving Encryption (OPE) is used to encipher the frequencies of occurrence of the extracted visual words. Another scale and rotation invariant feature, which is the local HSV histogram, is included for comparison. From the obtained results, it is apparent that SURF achieves more precise results. Aggregation of both features is considered to further improve the accuracy. The effects of the weighting scheme of the visual words and their number on the performance are investigated. Weighted term frequency inverse document frequency (tf-idf) together with the Jaccard similarity measure yield the best performance. OPE encryption is shown to have minor impact on the retrieval accuracy. To reduce encryption time, a lookup table is constructed. The inverted index reduces the search time significantly compared to a sequential search scheme as apparent from the results. A comparative study with recent related schemes demonstrates the competitiveness of the implemented system in terms of computational efficiency and accuracy.																	1751-9659	1751-9667				APR 17	2020	14	5					874	881		10.1049/iet-ipr.2019.0575													
J								CapsNet topology to classify tumours from brain images and comparative evaluation	IET IMAGE PROCESSING										medical image processing; image classification; neural nets; feature extraction; brain; biomedical MRI; gradient methods; learning (artificial intelligence); expectation-maximisation algorithm; tumours; optimisation; CapsNet topology; brain images; brain tissues; meningioma; ependymoma; computer-assisted brain tumour classification techniques; Capsule-based neural networks; tumour recognition; brain magnetic resonance images; glioma; CapsNet based methods; expectation-maximisation based dynamic routing; tumour boundary information; pituitary; Sobolev gradient-based optimisation; network topology; feature extraction; learned features	QUALITY-OF-LIFE; PITUITARY-TUMORS; CLASSIFICATION; SEGMENTATION; TEXTURE; SURGERY; GLIOMAS	Visual evaluation of many magnetic resonance images is a difficult task. Therefore, computer-assisted brain tumor classification techniques have been proposed. These techniques have several drawbacks or limitations. Capsule based neural networks are new approaches that can preserve spatial relationships of learned features using dynamic routing algorithm. By this way, not only performance of tumor recognition increases but also sampling efficiency and generalisation capability improves. Therefore, in this work, a Capsule Network (CapsNet) is used to achieve fully automated classification of tumors from brain magnetic resonance images. In this work, prevalent three types of tumors (pituitary, glioma and meningioma) have been handled. The main contributions in this paper are as follows: 1) A comprehensive review on CapsNet based methods is presented. 2) A new CapsNet topology is designed by using a Sobolev gradient-based optimisation, expectation-maximisation based dynamic routing and tumor boundary information. 3) The network topology is applied to categorise three types of brain tumors. 4) Comparative evaluations of the results obtained by other methods are performed. According to the experimental results, the proposed CapsNet based technique can achieve extraction of desired features from image data sets and provides tumor classification automatically with 92.65% accuracy.																	1751-9659	1751-9667				APR 17	2020	14	5					882	889		10.1049/iet-ipr.2019.0312													
J								Kernel-based Bayesian clustering of computed tomography images for lung nodule segmentation	IET IMAGE PROCESSING										medical image processing; fuzzy set theory; cancer; feature extraction; lung; image segmentation; Bayes methods; pattern clustering; computerised tomography; Lung Image Database Consortium; lung cancer diagnosis; effective lung nodule segmentation; lung nodule segmentation process; kernel-based Bayesian fuzzy clustering; input computed tomography image; effective segmentation; grid-based segmentation; kernel-based BFC; kernel functions; adaptive thresholding strategy; scale-invariant feature transform descriptor; Image Database Resource Initiative database; false positive rate	ACTIVE CONTOUR MODEL; PULMONARY NODULES	Lung nodule segmentation is an interesting research topic, and it serves as an effective solution for the diagnosis of Lung cancer. The existing methods of lung nodule segmentation suffer from accuracy issues due to the heterogeneity of the nodules in the lungs and the presence of visual deviations in the nodules. Thus, there is a requirement for an effective lung nodule segmentation, which assists the physicians in making accurate decisions. Accordingly, this study proposes a lung nodule segmentation process based on the kernel-based Bayesian fuzzy clustering (BFC), which is the integration of kernel functions in the BFC. Initially, the input computed tomography image is pre-processed for ensuring the effective segmentation, and the lobes are identified using the adaptive thresholding strategy. Then, the dominant areas in the lobes are identified using a scale-invariant feature transform descriptor, and the significant nodules are extracted using the grid-based segmentation. Finally, the lung nodules are segmented using the proposed kernel-based BFC. The proposed algorithm is evaluated using the Lung Image Database Consortium and Image Database Resource Initiative database, and it acquires the accuracy, sensitivity, and false positive rate of 0.955, 0.999, and 0.025, respectively.																	1751-9659	1751-9667				APR 17	2020	14	5					890	900		10.1049/iet-ipr.2018.5748													
J								Robust image hashing with visual attention model and invariant moments	IET IMAGE PROCESSING										feature extraction; wavelet transforms; image coding; image classification; robot vision; discrete wavelet transforms; cryptography; robust image hashing; visual attention model; invariant moments; image copy detection; image authentication; social event detection; Itti saliency model; attention focus; perceptual robustness; authors; weighted DWT representation; open image datasets; compared hashing algorithms	RING PARTITION; SALIENCY; SCHEME; RECOGNITION; SECURE	Image hashing is an efficient technique of multimedia processing for many applications, such as image copy detection, image authentication, and social event detection. In this study, the authors propose a novel image hashing with visual attention model and invariant moments. An important contribution is the weighted DWT (discrete wavelet transform) representation by incorporating a visual attention model called Itti saliency model into LL sub-band. Since the Itti saliency model can efficiently extract saliency map reflecting regions of attention focus, perceptual robustness of the proposed hashing is achieved. In addition, as invariant moments are robust and discriminative features, hash construction with invariant moments extracted from the weighted DWT representation ensures good classification performance between robustness and discrimination. Extensive experiments with open image datasets are done to validate the performances of the proposed hashing. The results demonstrate that the proposed hashing is robust and discriminative. Performance comparisons with some hashing algorithms are also conducted, and the receiver operating characteristic results illustrate that the proposed hashing outperforms the compared hashing algorithms in classification performance between robustness and discrimination.																	1751-9659	1751-9667				APR 17	2020	14	5					901	908		10.1049/iet-ipr.2019.1157													
J								Depth data and fusion of feature descriptors for static gesture recognition	IET IMAGE PROCESSING										support vector machines; feature extraction; image segmentation; gesture recognition; image motion analysis; complex background; depth data; feature descriptors; static gesture recognition; depth map; Microsoft's Kinect camera; public static gesture datasets; segmented hand; local binary patterns; hand palm centre; hand region; multiclass support vector machine kernels; extracted feature vector; recognition accuracy; public complex static gesture datasets; Gaussian SVM kernel function; high recognition accuracies; hand segmentation	KINECT	In this study, the authors propose a novel methodology for static gesture recognition in a complex background using only depth map from Microsoft's Kinect camera. Four different types of features are extracted and analysed on two public static gesture datasets. The features extracted from the segmented hand are geometrical, local binary patterns, number of fingers (Num) raised in a gesture and distance of hand palm centre from the fingertips and the valley between the fingers. The hand region is first segmented from the image using depth data followed by the forearm removal. Four multi-class support vector machine (SVM) kernels are also compared and used for recognition of gestures with extracted feature vector as an input. The experimental results achieved recognition accuracy of 99 and $95.7\%$95.7% on two public complex static gesture datasets using Gaussian SVM kernel function as a classifier. The proposed approach is found to be comparable and even outperforms some of the state-of-the-art techniques in terms of high recognition accuracies, even after using a single cue for hand segmentation and extraction of features in the complex background which results in non-dependency on too many cues and much hardware.																	1751-9659	1751-9667				APR 17	2020	14	5					909	920		10.1049/iet-ipr.2019.0230													
J								Active contour image segmentation model with de-hazing constraints	IET IMAGE PROCESSING										fog; image colour analysis; image denoising; computer vision; image enhancement; image segmentation; active contour image segmentation model; de-hazing constraints; hazy weather conditions; foggy weather conditions; atmospheric particles; computer vision systems; image de-hazing; weather factors; image visualisation; hazy scenes; easy image postprocessing; human assistance systems; variational segmentation model; de-hazing constraint terms; coupled dehazing-segmentation model; image structure boundaries; image quality; robust dehazing-segmentation scheme; vector-valued images; dehazing preprocessing	FRAMEWORK; VISION	Images captured in hazy or foggy weather conditions can be seriously degraded by scattering of atmospheric particles, which makes the objects and their features difficult to be identified by computer vision systems. In the past decades, image de-hazing is used to remove the influence of weather factors and improve image visualisation in hazy scenes by providing easy image post-processing towards human assistance systems benefit. In this study, the authors present a variational segmentation model equipped with de-hazing constraint terms in a new coupled dehazing-segmentation model. The proposed hybrid formulation not only recovers/restores the fog/haze degradation but at the same time segments image degraded object/objects by solving in this way the difficulties of simultaneously performed dehazing and segmentation pre/post-processing. This combination takes into account the image structure boundaries and the image quality, leading in this way to a robust dehazing segmentation scheme. The advantages of the proposed method are the suitability of the model for grey and vector-valued images, a small number of parameters involved, and a rather good speed of the algorithm. Experiments show that their approach outperforms the state-of-the-art algorithms in terms of segmentation accuracy while avoiding a dehazing preprocessing which reflects an extended CPU time.																	1751-9659	1751-9667				APR 17	2020	14	5					921	928		10.1049/iet-ipr.2018.5987													
J								Structure tensor-based SIFT algorithm for SAR image registration	IET IMAGE PROCESSING										transforms; image matching; image registration; geophysical image processing; remote sensing; radar imaging; feature extraction; synthetic aperture radar; SAR image registration; structure tensor-based SIFT algorithm; scale layers; input SAR images; correct matches; SAR image pairs; widely used feature extraction; feature matching method; remote sensing image registration; synthetic aperture radar images; correct matching rate	OPERATOR	The scale-invariant feature transform (SIFT) algorithm is the most widely used feature extraction as well as a feature matching method in remote sensing image registration. However, the performance of this algorithm is affected by the influence of speckle noise in synthetic aperture radar (SAR) images. It reduces the number of correct matches as well as the correct matching rate in SAR image registration. Moreover, SAR image registration is considered to be a challenging task as the images generally have significant geometric as well as intensity variations. To address these problems, a structure tensor-based SIFT algorithm is proposed to register the SAR images. At first, the tensor diffusion technique is used to construct the scale layers. Then, the features are extracted in the scale layers. Finally, feature matching is performed between the input SAR images and correct matches are identified. The proposed method can increase the number of correct matches as well as position accuracy in registration. Experiments have been conducted on five SAR image pairs to verify the effectiveness of the method.																	1751-9659	1751-9667				APR 17	2020	14	5					929	938		10.1049/iet-ipr.2019.0568													
J								Two-stage traffic sign detection and recognition based on SVM and convolutional neural networks	IET IMAGE PROCESSING										feature extraction; driver information systems; road traffic; object detection; road safety; object recognition; support vector machines; convolutional neural nets; image classification; German traffic sign recognition benchmark datasets; recognition rate; stage traffic sign detection; convolutional neural network; advanced driver assistance systems; safety; road traffic scenes; real-time traffic sign detection; traffic situation; linear support vector machines; German traffic sign detection benchmark datasets	ROAD-SIGN	Nowadays, traffic sign recognition is the most important task of advanced driver assistance systems since it improves the safety and comfort of drivers. However, it remains a challenging task due to the complexity of road traffic scenes. In this study, a novel two-stage approach for real-time traffic sign detection and recognition in a real traffic situation was proposed. The first stage aims to detect and classify the detected traffic signs into circular and triangular shape using HOG features and linear support vector machines (SVMs). The main objective of the second stage is to recognise the traffic signs using a convolutional neural network into their subclasses. The performance of the whole process is tested on German traffic sign detection benchmark (GTSDB) and German traffic sign recognition benchmark (GTSRB) datasets. Experimental results show that the obtained detection and recognition rate is comparable with those reported in the literature with much less complexity. Furthermore, the average processing time demonstrates its suitability for real-time processing applications.																	1751-9659	1751-9667				APR 17	2020	14	5					939	946		10.1049/iet-ipr.2019.0634													
J								CNN based localisation of forged region in object-based forgery for HD videos	IET IMAGE PROCESSING										video surveillance; image sequences; feature extraction; video coding; object detection; convolutional neural nets; CNN based localisation; forged region; object-based forgery; HD videos; tampering; traffic monitoring; scientific approach; spatio-temporal detection method; convolutional neural network; forged video frame; temporal CNN; spatial CNN; video forgery dataset; variable length; frame size videos	ALGORITHM	The location of the smallest object in a scene plays an essential role in the perception of a viewer. Any tampering with it, may evolve in adverse consequences especially with surveillance videos of banks, ATMs, traffic monitoring etc. Therefore, a scientific approach is required to thoroughly observe the fine details of tampering (forgery) in a video. A spatio-temporal detection method is proposed using convolutional neural network (CNN) to detect as well as localise the forged region in a forged video frame. The proposed method is employed in two stages. The first stage is detecting forged frames using proposed temporal CNN, while the second stage is localising the forged region in a novel way using proposed spatial CNN. The vital element of a video, i.e. motion residual is used to train the proposed network. Thus, making the network comprehensive in detecting the object-based forgery in HD videos. The performance of the proposed method is evaluated on SYSU-OBJFORG dataset (object-based video forgery dataset) and a derived test dataset of variable length and frame size videos. The results are compared with state-of-the-art methods to prove the efficacy of the proposed method.																	1751-9659	1751-9667				APR 17	2020	14	5					947	958		10.1049/iet-ipr.2019.0397													
J								Segmentation-based recognition system for handwritten Bangla and Devanagari words using conventional classification and transfer learning	IET IMAGE PROCESSING										optical character recognition; natural language processing; image segmentation; neural nets; learning (artificial intelligence); handwritten character recognition; feature extraction; document image processing; image classification; segmentation-based recognition system; offline recognition; handwritten text; Indian regional scripts; Indian script-based OCR system; statistical features; identified pseudocharacters; convolutional neural network-based transfer learning architectures; handwritten Devanagari words; handwritten Bangla words; transfer learning	CHARACTER-RECOGNITION	Offline recognition of handwritten text in Indian regional scripts is a major area of research as nearly 910 million people use such scripts in India. Most of the reported research works on Indian script-based optical character recognition (OCR) system have focused on a single script only. Research for developing methodologies that are capable of handling more than one Indian script is yet to be focused. As such, this has motivated us to study and experiment on creating a recognition system that can handle two most popular Indian scripts, namely Bangla and Devanagari. The authors propose a system that first detects and corrects skew present in Bangla and Devanagari handwritten words, estimates the headline, and further segments the words into meaningful pseudo-characters. This is followed by extraction of three different statistical features and combination of these features with off-the-shelf classifiers to study and identify the exemplary combination. Moreover, they employ state-of-the-art convolutional neural network-based transfer learning architectures and delineate a comparison with the extracted hand-crafted features. Finally, they amalgamate the identified pseudo-characters to provide the final result. On experimentation, the proposed segmentation methodology is discerned to provide good accuracy when compared with existing methods.																	1751-9659	1751-9667				APR 17	2020	14	5					959	972		10.1049/iet-ipr.2019.0208													
J								Image encryption with cross colour field algorithm and improved cascade chaos systems	IET IMAGE PROCESSING										cryptography; chaos; bifurcation; image colour analysis; matrix algebra; cross colour field diffusion algorithm; pixel correlation; plain text attack; image encryption; cascade chaos systems; encryption keyspace; cross colour field confusion method; R G B colour matrixes; improved cascade system; key sequence generator; pixels scrambling map; bifurcation diagrams; brute force attack; known plain attack; ciphertext attack; differential attack; security analysis	DNA-SEQUENCE OPERATION; CELLULAR-AUTOMATA; MAP; DESIGN	In this study, the critical factor of the chaos system has been analysed to improve the randomicity of the encryption keyspace. Several chaos systems have been integrated together with a linear function to form a much more efficient key sequence generator. This study also presents the cross colour field confusion method which scrambles the pixels among the R G B colour matrixes. In this way, the range of the pixels scrambling map is extended to three matrixes compared to the traditional pixels scrambling schemes which do pixels scrambling in its own colour matrix. The experiment results show that the improved cascade system has better bifurcation diagrams. The cross colour field diffusion algorithm makes the encrypted image has little pixel correlation and no information leakage. The experiment results justify that the novel cross colour confusion scheme with improved chaos systems has a good ability to resist brute force attack, known plain attack, chosen plain text attack, chosen ciphertext attack and differential attack. The security analysis demonstrates that the proposed approach has satisfactory properties in image encryption.																	1751-9659	1751-9667				APR 17	2020	14	5					973	981		10.1049/iet-ipr.2019.0310													
J								Multi-directional local adjacency descriptors (MDLAD) for heterogeneous face recognition	IET IMAGE PROCESSING										feature extraction; image representation; face recognition; convolutional neural nets; neighbourhood information; rotating spoke; concentric rings concept; multidirectional local adjacency descriptors; directional information; adjacency information; heterogeneous face recognition; heterogeneous face representation; image descriptors; central pixel; deep convolutional neural network; heterogeneous modalities; LFW; multi-PIE; extended Yale B; CASIA; CUHK face sketch; IIITD; CARL	VARYING ILLUMINATION; SKETCHES; DATABASE	This paper presents new image descriptors for heterogeneous face recognition (HFR). The proposed descriptors combine directional and neighborhood information using a rotating spoke and concentric rings concept. We name the descriptors as multi-directional local adjacency descriptors (MDLAD). This family of descriptor captures the directional information through successive rotations of a pair of orthogonal spokes. Likewise, they capture the adjacency information through a comparison against the central pixel of a window with concentric rings around the central pixel. The MDLAD is found to describe the face images well for recognition purposes, which when matched using the chi-squared distance. The face recognition performance with MDLAD improves with its use as a layer in a deep neural network, which yields a robust classification for heterogeneous face recognition with respect to the state-of-the-art methods. The MDLADNET deep network is easily trainable with few hyperparameters and limited data samples as compared to existing similar deep networks. We have experimented on different heterogeneous modalities viz. Extended Yale B, CASIA, CUFSF, IIITD, LFW, Multi-PIE, and CARL, and have found proficient results.																	1751-9659	1751-9667				APR 17	2020	14	5					982	994		10.1049/iet-ipr.2019.0199													
J								Assessing the importance of autistic attributes for autism screening	EXPERT SYSTEMS										autistic spectrum disorder; importance of autistic attributes; machine learning; recursive feature elimination; stability selection	SPECTRUM DISORDER; CLASSIFICATION; SELECTION; FEATURES	Autistic Spectrum Disorder (ASD) is a cognitive disease which leads to the loss of linguistic, communicative, cognitive, and social skills and abilities. Patients with ASD have diverse troubles such as sleeping problems. The role of genetic and environmental factors is of great importance in its pathophysiology. Early diagnosis provides an improved overall mental health of the patients. This study aimed to identify the important attributes for the best detection of this disorder in children, adolescents and adults. To achieve this aim, Recursive Feature Elimination and Stability Selection methods that consider important attributes for target class were proposed. The attributes collected from autism screening methods and other attributes such as age and gender were examined for the disease. The results verified the combining of feature selection method and machine learning algorithm within the frame of accuracy, sensitivity and specificity evaluation metrics.																	0266-4720	1468-0394				OCT	2020	37	5			SI				e12562	10.1111/exsy.12562		APR 2020											
J								Detection of distributed denial of service attack in cloud computing using the optimization-based deep networks	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Cloud computing; DDoS attack; log file; Bhattacharya distance; deep belief network	DDOS ATTACKS; MITIGATION; MECHANISM; TAXONOMY; SYSTEM	Cloud computing services provide a wide range of resource pool for maintaining a large amount of data. Cloud services are commonly used as the private or public data forum based on the demand, and the increase in usage has lead to security concerns. The information in the cloud comes under threat due to hackers, and the most common attack on the cloud data is considered as the Distributed Denial of Service (DDoS) attack. This work has concentrated on detecting the DDoS attack by developing the deep learning-based classifier. The service request from the users is collected and grouped as the log information. From the log file, some important features are selected for the classification using the Bhattacharya distance measure to reduce the training time of the classifier. Here, Taylor-Elephant Herd Optimisation based Deep Belief Network (TEHO-DBN), is developed by modifying the Elephant Herd Optimisation (EHO) with the Taylor series and the algorithm thus developed is adopted to train the Deep Belief Network (DBN) for the DDoS attack detection. From the simulation results, it can be concluded that the proposed TEHO based DBN classifier has improved performance with a maximum accuracy of 0.830.																	0952-813X	1362-3079															10.1080/0952813X.2020.1744196		APR 2020											
J								Tying the knot with a robot: legal and philosophical foundations for human-artificial intelligence matrimony	AI & SOCIETY										Artificial intelligence; Personhood; Marriage; Constitutional law; Roboethics	RIGHTS	Technological progress may eventually produce sophisticated robots with human-like traits that result in humans forming meaningful relationships with them. Such relationships would likely lead to a demand for human-artificial intelligence (AI) matrimony. U.S. Supreme Court decisions that expanded the definition of marriage to include interracial and same-sex couples, as well as those that have not extended marriage to polygamous relationships, provide guidance regarding the criteria that human-AI would have to meet to successfully assert a right to marry. Ultimately, robots will have to possess certain characteristics of personhood to marry, including the capacity to contract and to engage in an intimate relationship. Alternatively, if AIs can simulate these abilities sufficiently, we may believe that they have these capacities. Even if AIs genuinely possess the capabilities necessary to enter into a marriage, it is social acceptance of intelligent non-humans as life partners that will likely influence legal development is this realm rather than personhood criteria. However, AIs are likely to face bias due to their "artificial" rather than biological nature. Yet, Peter Singer's influential argument regarding speciesism in the context of animal rights implies that AIs with specific human-like qualities cannot be justifiably denied certain rights.																	0951-5666	1435-5655															10.1007/s00146-020-00973-5		APR 2020											
J								Discriminative feature learning and cluster-based defect label reconstruction for reducing uncertainty in wafer bin map labels	JOURNAL OF INTELLIGENT MANUFACTURING										Wafer bin map; Label uncertainty; Class label reconstruction; Unknown defect detection; Siamese network; G-means clustering	CLASSIFICATION; PATTERNS	Many studies have been conducted to improve wafer bin map (WBM) defect classification performance because accurate WBM classification can provide information about abnormal processes causing a decrease in yield. However, in the actual manufacturing field, the manual labeling performed by engineers leads to a high level of uncertainty. Label uncertainty has been a major cause of the reduction in WBM classification system performance. In this paper, we propose a class label reconstruction method for subdividing a defect class with various patterns into several groups, creating a new class for defect samples that cannot be categorized into known classes and detecting unknown defects. The proposed method performs discriminative feature learning of the Siamese network and repeated cross-learning of the class label reconstruction based on Gaussian means clustering in a learned feature space. We verified the proposed method using a real-world WBM dataset. In a situation where there the class labels of the training dataset were corrupted, the proposed method could increase the classification accuracy of the test dataset by enabling the corrupted sample to find its original class label. As a result, the accuracy of the proposed method was up to 7.8% higher than that of the convolutional neural network (CNN). Furthermore, through the proposed class label reconstruction, we found a new mixed-type defect class that had not been found until now, and we detected new types of unknown defects that were not used for learning with an average accuracy of over 73%.																	0956-5515	1572-8145															10.1007/s10845-020-01571-4		APR 2020											
J								A new irregular cellular learning automata-based evolutionary computation for time series link prediction in social networks	APPLIED INTELLIGENCE										Time-series link prediction; Irregular cellular learning automata; Evolutionary computation		Link prediction (LP), as an attempt to predict event-based future connections within a network, is the main task of social network analysis (SNA). Accordingly, common LP approaches to forecast future connections utilize similarity metrics of non-connected links in a static network representation. A general shortcoming of most existing research studies in this field is that they tap the present condition of a system and fail to take any temporal events into account. Moreover; social networks are innately evolutionary since they are assumed to be online, non-deterministic, and unforeseeable in most applications. Consequently, it is not appropriate to employ deterministic models for examining actual social network problems. With regard to time-series LP (TSLP) problems, temporal evolution of connection incidence is correspondingly exploited to predict connection chances at a particular time. In this paper, a new TSLP method based on irregular cellular learning automaton (ICLA) and evolutionary computation (EC) is proposed. In the evolutionary procedure suggested here, each vertex (i.e. cell) includes a genome as well as a set of learning automata (LAs). Accordingly, the genome residing in a cell represents predicted links for the corresponding cell. Local information among cells in successive time1toTin the network is then analyzed to predict future connections in timeT + 1. According to the distributed feature of the recommended approach, each genome is locally developed by a local search. The experiments in this study via e-mail and co-authorship networks ultimately show that the proposed algorithm leads to remarkable outcomes in predicting future connections.																	0924-669X	1573-7497															10.1007/s10489-020-01685-5		APR 2020											
J								Implementation and comparison of topic modeling techniques based on user reviews in e-commerce recommendations	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Coherence score; Collaborative filtering; Feature extraction; Gensim; PyLDAvis; Recommender system; Topic visualization; Tomotopy	SYSTEMS	These days users are able to save their time and effort by purchasing products online via various e-commerce websites. Their experience with a product exists in the form of textual reviews/feedbacks provided by them. Recommender systems offer personalized choices to users by capturing their interests and preferences. Through this paper identification of underlying topics using existing topic modeling techniques in user provided reviews of Moto e5 mobile on e-commerce website Amazon has been done and these techniques contrasted. Topic modeling is unsupervised learning technique used to identify hidden topics from a document (all the reviews of a product in this paper's context). Coherence score, a measure of goodness of a topic reflecting the quality of human judgment compares these techniques. The higher the coherence score, the topic is more coherent. Experiments performed reveal that LDA technique performed better on the scrapped dataset.																	1868-5137	1868-5145															10.1007/s12652-020-01956-6		APR 2020											
J								Designing assistive technology for getting more independence for blind people when performing everyday tasks: an auditory-based tool as a case study	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Smart home; Home automation; Visually-impaired	VISUAL IMPAIRMENT	Everyday activities and tasks should in theory be easily carried by everyone, including the blind. Information and Communication Technology (ICT) has been widely used for supporting solutions. However, the solutions can be problematic for the visually impaired since familiarity with digital devices is often required. Or, indeed the procedure can be perceived as fiddly or impractical particularly for repetitive tasks due to the number/type of steps required to complete the task. This paper introduces a simple audio-based tool aimed at supporting visually-impaired people in the seemingly simple activity of checking whether the light in a room is on or off. It is an example of potential low tech devices that can be designed without the need for specific skills or knowledge by the user, and that functions in a practical way. In this context, we discuss the main issues and considerations for totally blind users in identifying whether a light is switched on. The proposed prototype is based on a simple circuit and a form of auditory feedback which informs the user whether they are switching on or off the light. Two prototypes have been designed and built for two different kinds of installation. For the subsequent second prototype, three different versions are proposed to provide a blind person with further support in easily identifying the light status at home. The new design includes enhanced auditory feedback and modifications to the dimensions. The evaluation conducted by involving various groups of end-users revealed the usefulness of the proposed tool. In addition, a survey conducted with 100 visually-impaired people reported the limitations and difficulties encountered by the blind in using existing devices. Moreover, the study revealed the interest from 94% of the participants for a potential (new) basic tool integrable with the existing lighting system. This study gives a contribution in the ambient intelligence field by (1) showing how an auditory-based tool can be used to support totally blind people to check the lights in an autonomous and relatively simple way; (2) proposing an idea that can be exploited in other application cases that use light feedback; and (3) proposing seven potential recommendations for designing assistive technology tools and common everyday devices, based on information gathered from the online survey.																	1868-5137	1868-5145															10.1007/s12652-020-01944-w		APR 2020											
J								Strongly-resilient, well connected, non-interactive and gateway-free hierarchical deterministic key agreement schemes using Chinese remainder theorem	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Ad hoc network security; Efficient (Subset and Hybrid) KAS; CRT; Identity based cryptography (IBC); Indistinguishability of key in IBC; Random Oracle (RO) Model	PREDISTRIBUTION SCHEMES; DISTRIBUTION MECHANISMS; COMBINATORIAL DESIGN; ENCRYPTION; EFFICIENT	Efficient key agreement schemes (KAS) that manage symmetric key cryptographic (SKC) secrets (or keys) are essential for lightweight security solutions. There exists numerous cost-effective distributed KAS, either random or deterministic. Such solutions render inappropriate for large networks (with hierarchy). Thus arises the need for hierarchy supporting gateway-free KAS that is deterministic in nature. We propose a class of lightweight combinatorial subset scheme, H with predictable design features based on Chinese Remainder Theorem (CRT). This lightweight subset KAS, H can, in turn, combine with bilinear pairing (BLP) map to yield a scalable hybrid KAS, H'. We prove internal threshold resilience of the hybrid KAS is same as that of the subset KAS. Further, we prove the hybrid KAS, H' to be 'leaf resilient' using indistinguishability of keys in identity based cryptography settings (IND-ID-KA) in random oracle (RO) model. Rigorous analysis of vital design parameters exhibit that both the combinatorial subset scheme and its leaf resistant extension outperform state-of-the-art schemes.																	1868-5137	1868-5145															10.1007/s12652-020-01915-1		APR 2020											
J								Visual-Haptic Size Estimation in Peripersonal Space	FRONTIERS IN NEUROROBOTICS										visual; haptic; size; force-feedback; perceptual estimation; peripersonal space; virtual reality	ADAPTATION-LEVEL; FEEDBACK	In perceptual psychology, estimations of visual depth and size under different spatial layouts have been extensively studied. However, research evidence in virtual environments (VE) is relatively lacking. The emergence of human-computer interaction (HCI) and virtual reality (VR) has raised the question of how human operators perform actions based on the estimation of visual properties in VR, especially when the sensory cues associated with the same object are conflicting. We report on an experiment in which participants compared the size of a visual sphere to a haptic sphere, belonging to the same object in a VE. The sizes from the visual and haptic modalities were either identical or conflicting (with visual size being larger than haptic size, or vice versa). We used three standard haptic references (small, medium, and large sizes) and asked participants to compare the visual sizes with the given reference, by method of constant stimuli. Results show a dominant functional priority of the visual size perception. Moreover, observers demonstrated a central tendency effect: over-estimation for smaller haptic sizes but under-estimation for larger haptic sizes. The results are in-line with previous studies in real environments (RE). We discuss the current findings in the framework of adaptation level theory for haptic size reference. This work provides important implications for the optimal design of human-computer interactions when integrating 3D visual-haptic information in a VE.																	1662-5218					APR 16	2020	14								18	10.3389/fnbot.2020.00018													
J								Building a cognizant honeypot for detecting active fingerprinting attacks using dynamic fuzzy rule interpolation	EXPERT SYSTEMS										active fingerprinting attack; dynamic; D-FRI; FRI; fuzzy inference system; fuzzy rule interpolation; honeypot		Dynamic fuzzy rule interpolation (D-FRI) technique delivers a dynamic rule base through the utilisation of fuzzy rule interpolation to infer more accurate results for a given application problem. D-FRI offered dynamic rule base is very useful in security areas where network conditions are always volatile and require the most updated rule base. A honeypot is a vital part of any security infrastructure for directly investigating attacks and attackers in real-time to strengthen the overall security of the network. However, a honeypot as a concealed system can only function successfully while its identity is not revealed to any attackers. Attackers always attempt to uncover such honeypots for avoiding any trap and strengthening their attacks. Active fingerprinting attack is used to detect these honeypots by injecting purposefully designed traffic to a network. Such an attack can be prevented by controlling the traffic but this will make honeypot unusable system if its interaction with the outside world is limited. Alternatively, it is practically more useful if this fingerprinting attack is detected in real-time to manage its immediate consequences and preventing the honeypot. This article offers an approach to building a cognizant honeypot for detecting active fingerprinting attacks through the utilisation of the established D-FRI technique. It is based on the use of just a sparse rule base while remaining capable of detecting active fingerprinting attacks when the system does not find any matching rules. Also, it learns from current network conditions and offers a dynamic rule base to facilitate more accurate and efficient detection.																	0266-4720	1468-0394														e12557	10.1111/exsy.12557		APR 2020											
J								Using chemometric tools to investigate the quality of three- and four-way liquid chromatographic data obtained with two different fluorescence detectors and applied to the determination of quinolone antibiotics in animal tissues	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Liquid chromatography; Fluorescence detection; Three- and four-way data; Multivariate curve resolution; Unfolded partial least-squares; Quinolone antibiotics in animal tissues	GRAPHICAL INTERFACE TOOLBOX; LEAST-SQUARES; MULTIVARIATE	A comparison of two multi-way methodologies is presented regarding the simultaneous quantization of several analytes in complex samples. Both protocols are based on liquid chromatography with fluorescence detection, in the following modes: (1) collecting second-order/three-way data by fluorescence emission detection a a fixed excitation wavelength, and (2) measuring third-order/four-way data through excitation-emission fluorescence matrix detection. Ten quinolone antibiotics were simultaneously analyzed in edible animal tissues such as chicken liver and bovine liver and kidney. Multivariate curve resolution - alternating least-squares (MCR-ALS) provided excellent results with the second-order strategy, with average relative prediction errors in the range 4-12% for real samples, at analyze concentrations which are compatible with the corresponding maximum residue levels. For third-order data, however, the overall MCR-ALS analytical results were worse than for second-order data (relative errors were in the range 9-23%), and one analyze was not resolved. As an alternative, unfolded partial least-squares with residual bi- and trilinearization (U-PLS/RBL and U-PLS/RTL) were applied to both second- and third-order data, with relative errors of 7-18% and 5-27% respectively. The latter errors were significantly larger than those for MCR-ALS/second-order data, although the U-PLS/RTL model permitted the detection of all analytes when processing the third-order data. Relative advantages and disadvantages of the applied procedures are discussed on the basis of the analytical performances and the specific details of the instrumental setups.																	0169-7439	1873-3239				APR 15	2020	199								103972	10.1016/j.chemolab.2020.103972													
J								Data preprocessing for multiblock modelling - A systematization with new methods	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Multiblock methods; Data preprocessing; Partial Least Squares; Data science; Big data; Industry 4.0	SPECTROMETRY-BASED METABOLOMICS; MISSING DATA; SIGNAL CORRECTION; MULTIVARIATE DATA; DATA-DRIVEN; PLS; REGRESSION; PREDICTION; PCA; ALIGNMENT	With the advance of Industry 4.0, new data collectors are appearing at different points of the process generating blocks of data whose integrity should be preserved during data analysis. This is the scope of multiblock methods, whose potential has been recognized in several areas of application where they are becoming increasingly popular. Multiblock methods can be applied to a wide range of data-driven problems that practitioners face nowadays such as plant-wide process monitoring and diagnosis, process optimization and quality prediction of key product properties. These methods have the ability to find associations and interpretative connections between different data blocks from different sources and carrying complementary or overlapping information, as well as assessing the blocks' relative contributions to the final outcome. A critical stage in the application of multiblock methods is the selection of the appropriate preprocessing to apply to each block, before proceeding to the modelling. The preprocessing strategy can exponentiate the information extracted from the blocks and their mutual interactions or hide/mask/distort them if inappropriately done. In this article, we present a systematic workflow where both the intra-block and inter-block variation components are considered during preprocessing. We illustrate the application of the framework using two real case studies where a critical comparison is presented for the different preprocessing alternatives.																	0169-7439	1873-3239				APR 15	2020	199								103959	10.1016/j.chemolab.2020.103959													
J								Fault diagnosis of microbial pharmaceutical fermentation process with non-Gaussian and nonlinear coexistence	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Batch process; Non-Gaussian and nonlinear; Fault monitoring; Fault diagnosis; Multiway kernel entropy independent component analysis	INDEPENDENT COMPONENT ANALYSIS; STATISTICS; ICA	A large Proportion of batch processes commonly have traits of non-Gaussian and nonlinear. In this work, Multiway Kernel Entropy Independent Component Analysis (MKEICA) algorithm was developed to formulate more accurate model for process monitoring so as to enhance the monitoring performance. The original process data with three-dimension were first expanded into two-dimensional data matrix by using AT variable expansion method. The Kernel Entropy Component Analysis (KECA) was then employed to preprocess the data in order to reduce data redundancy. Such approach can also retain the information of cluster structure and maximize the essential characteristics of data. After that, a monitoring model of MKEICA was established for production process monitoring. Once a fault is detected, a nonlinear contribution plots method would be utilized to diagnose the fault variables. Consequently, to illustrate the superiority and feasibility, the proposed method was conducted on the penicillin simulation platform and the actual pharmaceutical production process.																	0169-7439	1873-3239				APR 15	2020	199								103931	10.1016/j.chemolab.2020.103931													
J								A new multiple kernel-based regularization method for identification of delay linear dynamic systems	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Linear system identification; Variable selection; Time delay estimation; Multiple kernel-based regularization; Iterative reweighting algorithm; ADMM	TIME-DELAY; REGRESSION; SELECTION; ESTIMATORS; STABILITY; CONVEX; ERROR	Model parameter estimation, model order selection, variable selection and time delay estimation are four important issues that receive increasing interests in dynamic system identification. However, previous work did not solve the four issues simultaneously. Motivated by the multiple kernel-based regularization method (MKRM), this paper proposes a new multiple kernel-based regularization method for joint model parameter estimation, model order selection, variable selection and time delay estimation for delay linear dynamic systems, referred to as the MKRM-D. Then, an efficient iterative reweighted algorithm is derived to solve the resulting difference of convex functions programming (DCP) problem. In addition, by exploiting the structure of the objective function in each iteration of this algorithm, the alternating direction method of multipliers (ADMM) is employed to decompose the centralized problem into a series of independent subproblems with lower variable dimension, which can be solved in a parallel and distributed manner. The performance of the proposed method is demonstrated by numerical experiments using both synthetic and real data.																	0169-7439	1873-3239				APR 15	2020	199								103971	10.1016/j.chemolab.2020.103971													
J								Real-time outlier detection for large datasets by RT-DetMCD	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Anomaly detection; Minimum covariance determinant; Parallel computing; Robust aggregation; Robust estimation	COVARIANCE; LOCATION; ALGORITHM; SCATTER; MATRIX; SIGN	Modern industrial machines can generate gigabytes of data in seconds, frequently pushing the boundaries of available computing power. Together with the time criticality of industrial processing this presents a challenging problem for any data analytics procedure. We focus on the deterministic minimum covariance determinant method (DetMCD), which detects outliers by fitting a robust covariance matrix. We construct a much faster version of DetMCD by replacing its initial estimators by two new methods and incorporating update-based concentration steps. The computation time is reduced further by parallel computing, with a novel robust aggregation method to combine the results from the threads. The speed and accuracy of the proposed real-time DetMCD method (RT-DetMCD) are illustrated by simulation and a real industrial application to food sorting.																	0169-7439	1873-3239				APR 15	2020	199								103957	10.1016/j.chemolab.2020.103957													
J								Deep convolutional neural networks for predicting leukemia-related transcription factor binding sites from DNA sequence data	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Transcription factor binding site; Deep learning; Machine-learning; DNA sequence; Leukemia	PROTEIN; CLASSIFICATION	Transcription factors are proteins that could bind to specific DNA sequences so as to regulate gene expressions. Currently, identification of transcription factor binding sites locating in DNA sequences is very important for building regulatory model in biological systems and identifying pathogenic variations. Traditional machine-learning methods have been successfully used for biological prediction problems based on DNA or protein sequences, but they all need to manually extract numerical features, which is not only tedious, but also would ignore effective information of first-order sequences. In this paper, based on the principle of deep learning (DL), we constructed prediction model for transcription factor binding sites only from DNA original base sequences. Here, a DL method based on convolutional neural network (CNN) and long short-term memory (LSTM) were proposed to investigate four leukemia categories from the perspective of transcription factor binding sites using four large non-redundant datasets for acute, chronic, myeloid and lymphatic leukemia, respectively. Compared with three widely used machine-learning methods of artificial neural network (ANN), support vector machine (SVM) and random forest (RF), our DL method exhibits significant superiority in terms of prediction performance, since the prediction accuracy of three machine-learning models either based on sequence feature or k-mer feature extraction are all lower than that of DL model. The available DL models for four leukemia categories gives an average prediction accuracy of 75% based only on sequence segments with 101 bases, which indicates that the DL based method is promising with unique advantages over the traditional machine learning methods. But focusing on leukemia-related transcription factor binding site prediction, further improvements would be implemented such as optimizing base segment length and CNN architecture, in order to improve the current prediction accuracy.																	0169-7439	1873-3239				APR 15	2020	199								103976	10.1016/j.chemolab.2020.103976													
J								Honey exposed to laser-induced breakdown spectroscopy for chaos-based botanical classification and fraud assessment	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Honey; LIBS; Chaotic parameters; Botanical origin; Adulteration; Classification	MINERAL-COMPOSITION; RICE SYRUP; AUTHENTICATION; IDENTIFICATION; ADULTERATION; PARAMETERS	Given that honey is among the top ten foods with the highest adulteration rate in the European Union, in this research, a tool has been developed to tackle this malpractice. The combination of laser-induced breakdown spectroscopy (LIBS) and chaotic parameters has been employed to classify six European honeys of different botanical origins as well as detect samples containing the usually elusive rice syrup adulteration in weight concentrations as low as 2%. The profiles of the LIBS emission spectra can be used to faithfully classify honey in terms of botanical origin by combining information extracted directly from the spectra with simple linear modeling. In contrast, the detection of low amounts of rice syrup in honey is not as straightforward, which is why algorithms based on chaotic parameters such as shifted (lag-k) autocorrelation coefficients were employed to extract underlying information representative of adulterated samples. Since these algorithms are capable of detecting slight changes in the composition of honeys, it has been possible to identify these adulterations with a success rate greater than 90% when samples from honeys of different botanical origins are combined into the same model, and over 95% when individual honey types are analyzed.																	0169-7439	1873-3239				APR 15	2020	199								103939	10.1016/j.chemolab.2020.103939													
J								Development of semi-supervised multiple-output soft-sensors with Co-training and tri-training MPLS and MRVM	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Soft sensor modeling; Semi-supervised learning; Tri-training method; Partial least squares; Relevance vector machines	PREDICTION; MODEL	Soft sensors are the most commonly used tools to estimate the hard-to-measure variables in the chemical processes and other industries, mainly due to unknown mechanism, significant measurement delay and highly unacceptable costs. However, a small number of labeled data and a large number of unlabeled data are not fully investigated and coordination of them to train the models and to improve the prediction accuracy is even rare. Multiple tasks learning or multiple outputs learning adds more complexity to this problem. In this light, this paper proposed semi-supervised multiple-output learning soft sensor models with co-training MPLS (Multiple-output partial least squares), co-training MRVM (Multiple-output relevance vector machines), tri-training MPLS and tritraining MRVM. Co-training MPLS model is developed by extending the traditional co-training PLS model. Cotraining MRVM is promoted by replacing MPLS with MRVM. Tri-training MPLS and tri-training MRVM are built by combining tri-training algorithm with MPLS and MRVM. The proposed four models can make full use of appropriate unlabeled data to optimize the regression model, and then to directly strengthen multiple-output variables prediction. These models were firstly demonstrated by a numerical example, then accessed by a wastewater plant (WWTP) simulated with well-established WWTP validation platform, Benchmark Simulation Model No. 1 (BSM1). The results proved that the proposed models were able to significantly improve the prediction performance and efficiency.																	0169-7439	1873-3239				APR 15	2020	199								103970	10.1016/j.chemolab.2020.103970													
J								Authentication of the geographical origin and the botanical variety of avocados using liquid chromatography fingerprinting and deep learning methods	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Avocado; Authentication; Chromatographic fingerprinting; Supervised pattern recognition; Deep learning methods	PATTERN-RECOGNITION; CLASSIFICATION	The lipid chromatographic fingerprint of different avocado fruits have been acquired and two classification multivariate methods, partial least squares-discriminant analysis (PLS-DA) and support vector machine (SVM), have been successfully tested in order to discriminate and classify a higher variability of avocado samples. Two authentication goals have been achieved attending to: (i) the geographical origin, and (ii) the botanical variety or cultivar. However, to our knowledge, there are no antecedents aimed at comparing and classifying avocado fruits. The pulp oil fraction of the avocado fruit was first extracted using pressurised liquid extraction from the previously lyophilised pulp. Then the 190-400 nm UV-absorption fingerprints were obtained from the avocado oils using normal phase high performance liquid chromatography coupled to an absorption diode-array detector ((NP) HPLC-DAD) and the 220 nm spectra were then selected for classification model building. Several input-class classification strategies were applied and the classification models were externally validated from the specific success/error contingencies. In addition, some quality metrics, i.e. sensitivity (or recall), specificity, precision, negative predictive values, efficiency (or accuracy), AUC (area under the receiver operating curve), Mathews correlation coefficient and Kappa coefficient, were determined to evaluate the performance of each classification model (PLS-DA and SVM) and the results clearly show that SVM method is the most proficient.																	0169-7439	1873-3239				APR 15	2020	199								103960	10.1016/j.chemolab.2020.103960													
J								Second-order calibration in combination with fluorescence fibre-optic data modelling as a novel approach for monitoring the maturation stage of plums	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Fibre-optic; Plums; Autofluorescence; Second-order algorithms	LEAST-SQUARES METHODS; SPECTRAL-ANALYSES; CHLOROPHYLL; SPECTROSCOPY; PRODUCTS; PIGMENTS; MATURITY	In this work, non-destructive autofluorescence of plums was employed to study the chlorophylls' concentration evolution along the maturation process. For that, excitation-emission matrices (EEMs), containing full fluorescence information, were collected with a fibre-optic, assembled to a spectrofluorometer. Data analysis was performed with several second-order multi-way algorithms, such as parallel factor analysis (PARAFAC), multi-way partial least-squares (N-PLS), unfolded partial least-squares (U-PLS), and multivariate curve resolution-alternating least-squares (MCR-ALS). Firstly, the EEMs of each plum, collected each week along the maturation process, were processed with PARAFAC. Two components were used to model the data and the excitation and emission loadings were obtained. Score values for the first PARAFAC component showed a clear evolution with time, increasing during the first five weeks, and decreasing for the last weeks. Also, the chlorophyll concentrations obtained by HPLC analysis, in the skin and the whole fruit, were compared with those obtained with different algorithms mentioned before. Best results were obtained in the case of skin for all algorithms. Similar correlation coefficients (r) were obtained in all cases (0.899 (PARAFAC); 0.940 (U-PLS); 0.936 (N-PLS) and 0.958 (MCR-ALS)). When the elliptical joint confidence region (EJCR), for the slope and intercept, were calculated, the theoretically expected values of 1 and 0, for the slope and intercept, respectively, were included in all ellipses. However, it was observed that for the skin data and U-PLS and N-PLS algorithms, the EJCR confidence region was smaller than in the other cases.																	0169-7439	1873-3239				APR 15	2020	199								103980	10.1016/j.chemolab.2020.103980													
J								A chemometric strategy to evaluate the comparability of PLS models obtained from quartz cuvettes and disposable glass vials in the determination of extra virgin olive oil quality parameters by NIR spectroscopy	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS											REGRESSION	In the present study, a chemometric strategy was deployed for comparing the analytical performances of quartz cuvettes and disposable glass vials in the analysis of olive oil by near infrared spectroscopy (NIRS). Nowadays, laboratories that perform 'highly frequent' analysis on extra virgin olive oil (EVOO) by NIRS usually employ quartz cuvettes. This results in time-consuming measurements, especially in the cleaning phase, and an increased cost for non-green cleaning solvents. The use of mono-use glass vials may reduce time and costs significantly, but their analytical performances in EVOO analysis, have not yet been investigated. In order to reach this goal, a set of 106 EVOO samples from different Italian olive-growing areas were collected and analysed using both quartz cuvettes and mono-use glass vials. From spectral data, multivariate calibration models were developed to estimate quality parameters of extra virgin olive oil: methyl esters of fatty acids (FAMEs) and triacylglycerols (TAGs) determined by a fast-GC approach and an UHPLC system, respectively. An optimisation procedure was performed in order to individuate the best pre-treatment and the optimal complexity for each model. The predictive ability of each PLS model was evaluated on an independent test set. The Passing-Bablok linear regression was lastly used to statistically compare the performances of the two different types of cuvettes. In light of the outcomes of the present study, analytical performances of quartz cuvettes and disposable glass vials were considered not significantly different in predicting the olive oil quality parameters taken into account.																	0169-7439	1873-3239				APR 15	2020	199								103974	10.1016/j.chemolab.2020.103974													
J								Sparse non-negative multivariate curve resolution: L-0, L-1, or L-2 norms?	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Sparsity constraint; L-p-norms; LC/GC-MS data; Hyperspectral image; Borgen-Rajko plot; Outer-polygon	FEASIBLE REGIONS; REGRESSION; DECONVOLUTION; SELECTION	Several constraints are designed to further restrict bilinear decompositions to a unique solution. Constraints are physico-chemical restrictions on the curve resolution task. Sparsity, as a constraint, was introduced to create solutions with zero elements. As neither the number of zeros nor the places of zeros are not initially available, sparsity constraint should be implemented with caution. Regarding sparsity constraint, two important issues can be addressed. The first issue is the effect of sparsity constraint on the possible solutions of bilinear decompositions, i.e., set of sparse solutions. The second issue is the type of Lp( )norm, {p = 0, 1, 2}, for the sparsity implementation. Self-modeling curve resolution (SMCR) tools (say Borgen-Rajko plot) draw a clear picture of the data micro-structure. Focusing on the geometry of bilinear data sets, outer-polygon as the non-negativity boundary of possible solutions contains all the sparse solutions. In this contribution, we shed light on all possible sparse solutions of a bilinear decomposition, and it was shown that outer-polygon is the set of sparse solutions. Finally, L-p-norms were calculated for the different feasible profiles, and it is revealed that L-0-norm minimization and L-2-norm maximization are the correct way toward the sparse/est solutions. Finally, this study targets LC/GC-MS, hyperspectral images, and all of the data sets which contain zero values in their profiles. Since omics researches use extensively, e.g., mass spectrometry, thus a wide community expected to be interested in our report on the limitations of sparsity.																	0169-7439	1873-3239				APR 15	2020	199								103969	10.1016/j.chemolab.2020.103969													
J								Sequential preprocessing through ORThogonalization (SPORT) and its application to near infrared spectroscopy	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Spectroscopy; Analytical techniques; Pretreatments; Ensemble; Boosting; Multi-block analysis; Sequential and orthogonalized partial least squares (SO-PLS); Sequential preprocessing through orthogonalization (SPORT)		In spectroscopy, multivariate calibrations more than often include a pre-processing step to reduce the effect of unwanted (not Y-related) sources of variability. Because there are many types of background noise, there are many pre-treatment methods. It is therefore tedious to select and/or combine the best pre-treatments. This article proposes to combine several pre-treatments through the use of sequential and orthogonalized partial least squares (SO-PLS), thus leading to a boosting method. The performances and properties of this new method, called Sequential Preprocessing through ORThogonalization (SPORT), are compared to those of a previously published stacking method. SPORT demonstrates very good calibration performances, but also the ability to make significant pretreatment selections.																	0169-7439	1873-3239				APR 15	2020	199								103975	10.1016/j.chemolab.2020.103975													
J								On the restrictiveness of equality constraints in multivariate curve resolution	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										multivariate curve resolution; Rotational ambiguity; Area of feasible solutions; Borgen plot; Equality constraint	POLYGON INFLATION ALGORITHM; FEASIBLE SOLUTIONS; ROTATIONAL AMBIGUITY; AREA; COMPUTATION; REDUCTION	Multivariate curve resolution methods suffer from the non-uniqueness of the solutions of the nonnegative matrix factorization problem. The solution ambiguity can be considerably reduced by equality constraints in the form of known spectra or concentration profiles. Two measures are suggested that indicate the impact of the equality constraints. The representation of these measures in the area of feasible solutions show strong variations in the restrictiveness of equality constraints. The measures are tested for a three-component model problem and experimental data sets from the hydroformylation process and a catalyst cluster formation.																	0169-7439	1873-3239				APR 15	2020	199								103942	10.1016/j.chemolab.2020.103942													
J								Chemometric outlier classification of 2D-NMR spectra to enable higher order structure characterization of protein therapeutics	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Nuclear magnetic resonance (NMR); Biopharmaceuticals; Chemometrics; Monoclonal antibody (mAb); NISTmAb; Spectral similarity metric; Higher order structure	MONOCLONAL-ANTIBODY THERAPEUTICS; NATURAL-ABUNDANCE; RATIONAL DESIGN; NMR METHODS; COMPARABILITY; AGGREGATION	Protein therapeutics are vitally important clinically and commercially, with monoclonal antibody (mAb) therapeutic sales alone accounting for $115 billion in revenue for 2018.[1] In order for these therapeutics to be safe and efficacious, their protein components must maintain their high order structure (HOS), which includes retaining their three-dimensional fold and not forming aggregates. As demonstrated in the recent NISTmAb Interlaboratory nuclear magnetic resonance (NMR) Study[2], NMR spectroscopy is a robust and precise approach to address this HOS measurement need. Using the NISTmAb study data, we benchmark a procedure for automated outlier detection used to identify spectra that are not of sufficient quality for further automated analysis. When applied to a diverse collection of all 252 H-1, C-13 gHSQC spectra from the study, a recursive version of the automated procedure performed comparably to visual analysis, and identified three outlier cases that were missed by the human analyst. In total, this method represents a distinct advance in chemometric detection of outliers due to variation in both measurement and sample.																	0169-7439	1873-3239				APR 15	2020	199								103973	10.1016/j.chemolab.2020.103973													
J								Chemometric handling of spectral-temporal dependencies for liquid chromatography data with online registering of excitation-emission fluorescence matrices	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Third-order data; Multi-way data analysis; Excitation-Emission fluorescence matrices; Liquid chromatography; Loss of trilinearity; Open-source	OCEAN WATER; SPECTROFLUOROMETER; HYDROCARBONS; QUANTITATION; GENERATION; UNIQUENESS; TOOLBOX; RANK	In this work, the generation and posterior chemometric resolution of third-order data, obtained from samples processed by liquid chromatography (LC) with online registering of excitation-emission fluorescence matrices (EEM) is reported. Samples were instrumentally processed in a relatively short time, and neither an intentional reduction of the linear flow rate nor an unconventional fluorescence instrument were required. Through the inclusion of external circuitry based on open-source hardware, the occurrence time of each individual fluorescence intensity reading was recorded. For the reported instrumental setup, irregular signal sampling was verified. In order to consider samples-specific time measurements, the PARAFAC (Parallel Factor Analysis) algorithm, and the derived APARAFAC (Augmented-PARAFAC) strategy, were adapted. The functional information was employed during the computational stages, through the development and implementation of smoothing strategies. To tackle differences between the rate of spectral acquisition and the rate of change in the concentration of the mobile fluorophores, Expectation Maximization was implemented. Data from samples with one calibrated analyze (Vitamin B6-Pyridoxine), in presence of uncalibrated interferents, were modeled. In order to preserve the original data structure, unfolding data operations were minimized. The resolved profiles of all species were in agreement with the corresponding chromatographic and spectral references. Results suggest that the effects derived from the loss of trilinearity previously reported in the literature for LC-EEM data, depend on interpretation and subsequent modeling of the data. The reported strategies can be useful with other flow techniques and kinetics.																	0169-7439	1873-3239				APR 15	2020	199								103961	10.1016/j.chemolab.2020.103961													
J								Feature selection and classification for gene expression data using novel correlation based overlapping score method via Chou's 5-steps rule	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Feature selection; Gene expression data; Correlation based overlapping score; Classifiers; Stability index	AMINO-ACID-COMPOSITION; FLEXIBLE WEB SERVER; SUBCELLULAR-LOCALIZATION; COMPUTATIONAL MODEL; PSEUDO; PREDICTION; PSEAAC; PSEKNC; TOOL	The analysis of omics data together with knowledge-based interpretation can help obtaining important information regarding different biological processes and to reflect the current physiological status of tissue and cells. The main challenge, however, is to analyze high-dimensional gene expression data consisting of a massive amount of redundant genes in extracting disease-related information. To address this problem, gene selection, that eliminates redundant and irrelevant genes, has been a key step. In current article, a feature selection technique is proposed that exploit correlation based overlapping analysis of expression data across classes. The proposed correlation based overlapping score (COS) technique is compared with state-of-the-art gene selection approaches using real-world benchmark microarray datasets. In an experimental evaluation, the COS algorithm outperforms the other methods with minimum misclassification errors obtained via boosting, random forest and k-nearest neighbour (kNN) classifiers. Moreover, the proposed technique is more stable than the other techniques in gene selection.																	0169-7439	1873-3239				APR 15	2020	199								103958	10.1016/j.chemolab.2020.103958													
J								An adaptive mode convolutional neural network based on bar-shaped structures and its operation modeling to complex industrial processes	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Convolutional neural networks; Bar-shaped; Operation modeling; Adaptive mode; Methanol production process	ENERGY EFFICIENCY; MECHANISM	Optimal operation modeling plays an important role in complex industrial processes; however, with the increasing complexity and high nonlinearity in industrial processes, it becomes more and more difficult to establish an accurate operation modeling using first-principles methods. In this paper, an adaptive mode convolutional neural network framework based on bar-shaped structures (BS-AMCNN) is proposed, which is a data-driven model. First, a bar-shaped structure is designed to deal with the industrial process data specifically. The bar-shaped structure can transfer the advantages of CNN on processing image data to processing industrial process data. Meanwhile, the convolution windows and pooling windows in the proposed BS-AMCNN algorithm is replaced by translation-only sliding bar-shaped windows. Therefore, the algorithm can adjust the CNN structure adaptively among three different modes depending on different process statuses. the optimal operation model can be obtained with the proposed BS-AMCNN method accordingly. An experiment on real complex industrial process, methanol production process, is carried out, which validates the effectiveness of the proposed method. The proposed method is further compared with the traditional CNN method, and the back propagation (BP) method. The results demonstrate the effectiveness of the proposed method.																	0169-7439	1873-3239				APR 15	2020	199								103932	10.1016/j.chemolab.2020.103932													
J								Combination of one-dimensional convolutional neural network and negative correlation learning on spectral calibration	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										One-dimensional convolution neural network; Negative correlation learning; Spectral calibration; Composite error function	NEAR-INFRARED SPECTRA; PARTIAL LEAST-SQUARES; SPECTROSCOPY; REGRESSION	The advantage of data-sensitive deep learning methods used in spectral calibration is not obvious when the amount of available data is insufficient. To solve this problem, this paper proposes a new method that combines one-dimensional convolution neural network (1-dim CNN) with negative correlation learning (NCL). First, we create several identical one-dimensional convolutional neural networks as subnetworks of the NCL system. Second, we add the error function of each subnetwork to a negative correlation penalty term that is related to the correlation between the networks and then use this composite error function to back-propagate these networks for parameter adjustment. Finally, after the model has converged, we take the average of the results of all subnetworks as the result of the whole model. We compare CNN_NCL with PLS,creating diversity partial least squares (CDPLS) and a single 1-dim CNN on the pharmaceutical tablet dataset and diesel fuels dataset. The experimental results show that CNN_NCL performs better than PLS and CDPLS when the number of samples is sufficient. Additionally, CNN_NCL can always be more effective than a single CNN regardless of the data scale. Therefore, in the context of the era of big data, CNN_NCL is a fairly efficient model for spectral calibration.																	0169-7439	1873-3239				APR 15	2020	199								103954	10.1016/j.chemolab.2020.103954													
J								LSSVM-based color prediction for cotton fabrics with reactive pad-dry-pad-steam dyeing	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Dyeing; Cotton fabric; LSSVM; Pad-dry-pad-steam; Prediction	NIR SPECTROSCOPY; CIELAB VALUES; LS-SVM; MODEL; CARBON	Least-square support vector machines (LSSVM) have been strongly moving from preliminary theoretical investigations into practical industrial applications. Successfully attaining a required color is a crucial task in the dyeing industry. In this paper, an LSSVM-based color quality prediction model for cotton fabrics dyed with a reactive dye using the conventional pad-dry-pad-steam (PDPS) process is proposed. The concentrations of the dye, sodium carbonate and sodium chloride, as well as the drying and steaming times were selected as the model inputs, while the CIE Lab value (L*, a*, b*) and the K/S value of the dyed fabric were used as the model outputs. The two prominent reactive dyes, Remazol Red and Remazol Navy, were used separately to dye cotton fabrics. The proposed LSSVM-based model gave mean absolute errors of less than 0.5 and 1.5 for predicting the K/S value and the CIE Lab value (L*, a*, b*), respectively. As well, the model had an acceptable average color difference Delta E* of less than 2.0 for the fabrics dyed using the PDPS process with both reactive dyes.																	0169-7439	1873-3239				APR 15	2020	199								103956	10.1016/j.chemolab.2020.103956													
J								A novel strategy for prediction of human plasma protein binding using machine learning techniques	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Plasma protein binding; Quantitative structure-activity relationship model; Machine learning techniques	DRUG DISCOVERY; APPLICABILITY DOMAIN; IN-SILICO; PHARMACOKINETICS; CLEARANCE; VITRO; RAT	Plasma protein binding (PPB) is a key player of drug ADME (absorption, distribution, metabolism, elimination) behaviors, enabling PPB to have significant impact on drug efficacy and toxicity. As drug discovery enters the era of rational drug design, it is desirable to use in silico model to predict PPB so as to achieve rapid initial screening for potential candidate compounds prior to further time-consuming and costly in vitro and in vivo experimental assay. In this study, a global quantitative structure-activity relationship (QSAR) model of PPB was built on the basis of a large training set comprising more than 5000 compounds to represent large structural diversity. The uneven distribution of PPB was often rectified by two mathematical transformations of PPB but this led to a decrease in prediction accuracy a the lower binding level. To resolve this problem, we proposed a novel strategy to build models for different binding levels. The best model yielded much lower mean absolute error (MAE) of 0.076 on the test set than published models and the MAE was further reduced to 0.041 a the high level of binding (0.8-1). The models also performed excellent in the validation set containing some compounds from traditional Chinese medicine. In addition, the applicability domain was determined to identify new compounds which are appropriate for prediction using our built models. In conclusion, this study developed a novel strategy to construct robust QSAR model for PPB prediction which could be used by chemists to predict the PPB of candidate compounds efficiently and make structural modification in the early stage of drug development.																	0169-7439	1873-3239				APR 15	2020	199								103962	10.1016/j.chemolab.2020.103962													
J								A generalized multi-snapshot model for 3D homing and route following	ADAPTIVE BEHAVIOR										Visual navigation; route following; flying insects; view-based homing; spherical harmonics	LEARNING-WALKS; LANDMARK GUIDANCE; DESERT ANTS; NAVIGATION; ORIENTATION; FLIGHT; INSECTS; HYMENOPTERA; VISION; VIEWS	Inspired by the learning walks of the ant Ocymyrmex robustior, the original multi-snapshot model was introduced, which-in contrast to the classical "single snapshot at the goal" model-collects multiple snapshots in the vicinity of the goal location that subsequently can be used for homing, that is, for guiding the return to the goal. In this study, we show that the multi-snapshot model can be generalized to homing in three dimensions. In addition to capturing snapshots at positions shifted in all three dimensions, we suggest to decouple the home direction from the orientation of snapshots and to associate a home vector with each snapshot. We then propose a modification of the multi-snapshot model for three-dimensional route following and evaluate its performance in an accurate reconstruction of a real environment. As an illumination-invariant alternative to grayscale images, we also examine sky-segmented images. We use spherical harmonics as efficient representation of panoramic images enabling low memory usage and fast similarity estimation of rotated images. The results show that our approach can steer an agent reliably along a route, making it also suitable for robotic applications using on-board computers with limited resources.																	1059-7123	1741-2633														1059712320911217	10.1177/1059712320911217		APR 2020											
J								Personalised healthcare model for monitoring and prediction of airpollution: machine learning approach	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Internet of Things (IoT); modified-air quality index; adaptive neuro-fuzzy inference system; differential evolution	DIFFERENTIAL EVOLUTION; IOT; OPTIMIZATION; ARCHITECTURE; FRAMEWORK; WORKOUTS; INTERNET; THINGS	The drastic increase in atmospheric pollutants has resulted in the prevalence of hazardous diseases like Asthma, Ischaemic heart disease, and Pulmonary disease around the world. IoT technology has the capability to acquire and monitor air quality parameters in the ambient environment of an individual. Inspired from these aspects, this paper proposes an IoT-based automated framework for monitoring and predicting air quality parameters like benzene using machine learning technology. Specifically, this study incorporates an Adaptive Neuro-Fuzzy Inference System (ANFIS) technique to predict the air quality in the form of Level of Pollutant (LoP) and modified Air Quality Index (m-AQI). Moreover, for the optimisation of ANFIS technique, Differential Evolution (DE)-inspired algorithm has been proposed for overall enhancement of system accuracy. In order to validate the proposed model, numerous experimental simulations were performed over four challenging datasets and results were compared with several state-of-the-art models. Comparative analysis shows improved statistical values for the presented model in terms of Accuracy (97.3%), Coefficient of Determination (94.01%), and Root Mean Square Error (2.4%). In addition to these, enhanced values of performance estimators like Reliability (92.36%) and Stability (76.00%) were estimated for depicting overall efficiency of the proposed system.																	0952-813X	1362-3079															10.1080/0952813X.2020.1744197		APR 2020											
J								Double-quantitative decision rough set over two universes and application to African swine fever decision-making	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										African swine fever (ASF) decision making; probabilistic graded rough set; double quantitative information; decision rough set	MODEL	In order to actively respond to the severe situation of the current African swine fever (ASF) epidemic, this paper proposed a double-quantitative decision rough set method over two universes. First, a double-quantitative decision rough set model over two universes based on compatibility relation is defined. Furthermore, the characteristics of ASF decision-making problems are fully considered, and the ASF decision-making process is transformed into a rough approximation decision problem. Then, we construct a double-quantitative decision rough set application method over two universes based on ASF decision-making problems. The upper and lower approximations of the ASF decision objects are calculated by the double quantification approximation space over two universes, and the positive region of the ASF decision object is given. The model proposes a method of ASF decision-making and the optimal decision rules based on the personal preference information of decision-makers. Finally, the theoretical model is applied to the ASF decision problem to illustrate the application process and its effectiveness of the model constructed in this paper. At the same time, the corresponding decision suggestions for dealing with the actual ASF problem are proposed based on the application results of numerical examples.																	0952-813X	1362-3079															10.1080/0952813X.2020.1744195		APR 2020											
J								A fog based ball tracking ((FBT)-T-2) system using intelligent ball bees	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Ball tracking; Fog computing; Quadcopter; Unmanned aerial vehicle (UAV)	VIDEO ANALYSIS; SOCCER; ALGORITHM; FILTER	Soccer is the most popular group game; it has a large number of fans all over the world. Although each complete soccer match lasts around one hour and a half, only have very few scenes attracting audiences been perfectly filmed. This filming has usually been done using static (fixed) cameras, and they cannot provide the same level of accuracy or entertainment given by mobile devices. Taking into our consideration that there are no schemes using mobile devices have been proposed to solve this problem till now. As periodic communication generates a huge amount of data; typical storage, computation, and communication resources are required. Hence, this paper aims at introducing a Fog computing mechanism that takes into consideration the requitrements of special mobility, low latency and location awareness. Our solution based on controlling the movement of mobile cameras mounted on a fleet of mobile bees. A bee is an autonomous camera-drone, which gives the audience the feeling of being a part of such sports competition. It is a special type of Unmanned Aerial Vehicle (UAV) equipped with a built-in processing unit, memory, high-speed camera and transceiver. The bee has the ability to film the movement of the ball, which has to be followed and filmed via object tracking principle. In fact, detecting and tracking the ball from the broadcast soccer video constitute a major challenge. In soccer matches, the ball moves most of the time and it is frequently occluded while its size and shape appearance vary over the time and between cameras. Moreover, the feature-based tracking methods are used to judge whether or not a sole object is the target as the features of the ball might be changed fast over frames and then we cannot manage to distinguish the ball from other objects by using these methods. Thus, the current study demonstrates an innovative technique for tracking a soccer ball from mobile cameras fixed on multiple ball bees.																	1868-5137	1868-5145															10.1007/s12652-020-01948-6		APR 2020											
J								Profile generation system using artificial intelligence for information recovery and analysis	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Information recovery; Information fusion; Big data; Profiling	NETWORK	The advances in data computing and analysis methodologies have contributed to the added value of data. Several years ago it was difficult to imagine that we would ever be able to extract such a large amount of information from the Internet. All this thanks to the ability of current techniques to process large volumes of data in a short period of time. The Internet provides access to a large amount of unstructured or unlabelled data, which are hard to retrieve for any human due to the lack of knowledge of the available sources of information. Moreover, in many cases people are unaware of the online availability of their personal data. This article presents a system for retrieving personal information from the Internet on the basis of several input criteria. The system is capable of differentiating the information of different people with the same name by using artificial intelligence techniques. In the conducted case study, the information has been gathered from sources containing information about people living in Spain, but it could be adapted to the specific sources of information of other countries. The system has been validated in a case study which included several participants and the obtained results have been quite satisfactory.																	1868-5137	1868-5145															10.1007/s12652-020-01942-y		APR 2020											
J								Multiclass normalized clustering and classification model for electricity consumption data analysis in machine learning techniques	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Classification; Multi class clustering; Energy consumption; Machine learning	FRAMEWORK	During the past two decades, a numerous amount of research applications has been taken for machine learning tasks. In this paper, both supervised and unsupervised learning techniques used to analyse the electricity consumption data in India. Multiclass clustering and classification model of the dataset provides more insight towards the consumption behaviour on different regions. The proposed Enhanced Multiclass Normalized Optimal Cluster Algorithm has been used to group the data objects and classification of the data into multiple classes. The classification model for electricity consumption evaluates and compares the accuracy of the input dataset. The results give an overview of demands that are existing in energy consumption in different regions of India and also indicate that the proposed method performance is significantly better.																	1868-5137	1868-5145															10.1007/s12652-020-01960-w		APR 2020											
J								Weather forecast prediction and analysis using sprint algorithm	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Weather forecasting; Data mining; WEKA tool; SPRINT algorithm; Climate data	DATA ANALYTICS	Weather forecasting is an emerging domain that predicts the weather condition at a particular location at a particular time. Weather forecasting is considered as the most sensitive research field which facing a lot of real-time issues such as inaccurate prediction, lack of handling in huge data volume and inadequate in technology advancement. In this paper, we propose the SPRINT algorithm which is works with the principle of the decision tree. The experimental work is carried out with climate dataset and applied on WEKA tool. Based on the climate parameters such as Outlook, Temperature, Humidity, and Windy the data is classified into sunny, overcast and rainy. From the obtained result the weather is predicted, to prove the proposed methods of proficiency in accuracy level. Performance comparison is done with the existing method navie Bayes, both results are plotted on the graph. The outcome proves SPRINT algorithm is efficient and accurate in predicting the weather conditions.																	1868-5137	1868-5145															10.1007/s12652-020-01928-w		APR 2020											
