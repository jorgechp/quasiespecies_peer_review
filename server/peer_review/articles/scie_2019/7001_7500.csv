PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	RP	EM	RI	OI	FU	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	D2	EA	PG	WC	SC	GA	UT	PM	OA	HC	HP	DA
J								Hybrid on-chip soft computing model for performance evaluation of 6T SRAM cell using 45-nm technology	SOFT COMPUTING										Static random access memory; Static noise margin; Data retention voltage; Cat swarm optimization; Fuzzy inference system; Deep learning; Deep backpropagation neural network	LOW-POWER; ARCHITECTURE; PROCESSOR	A hybrid on-chip approach based on the variants of soft computing techniques is proposed in this work to evaluate the performance metrics of a 6T static random access memory (SRAM) cell in 45-nm technology. The performance metric evaluated for the SRAM cell is the data retention voltage (DRV) with optimal memory requirements. Each of the SRAM memory cells intends the chip to possess low density but operates at high speed. This paper formulates a hybrid soft computing framework comprising a deep backpropagation neural network which trains the fuzzy inference system to determine the performance metrics of 6T SRAM cell. The weight and bias parameters of the deep learning neural framework are optimized through a cat swarm optimization algorithm so as to reduce the elapsed convergence time of the new hybrid soft computing model. Evaluation process is executed based on the driven outputs from deep learning model to the fuzzy inference system (FIS) module so as to achieve the best values of DRV. Data retention voltage plays a major role in reducing the substantial leakage current and static noise margin intends to retain the data without losing them. Deep backpropagation neural network gets trained with deep learning procedures and optimizes the rule parameters and membership parameters of the FIS design structure. The performance metric DRV under various constraints prove to be better and effective in comparison with the solutions from the existing literature works for the same configuration of 6T SRAM cell using 45-nm technology.																	1432-7643	1433-7479				JUL	2020	24	14					10785	10799		10.1007/s00500-019-04581-4													
J								A new overall quality indicator OQoC and the corresponding context inconsistency elimination algorithm based on OQoC and Dempster-Shafer theory	SOFT COMPUTING										Context-aware systems; Inconsistency; Overall quality of context indicator; Dempster-Shafer theory; Personal identity verification	AWARE; RELIABILITY; FRAMEWORK; SYSTEMS	With the rapid development of Internet of things technology, context-aware systems (CASs) are being gradually improved and widely applied to many fields such as digital home, smart health and so on. However, context information from sensor-rich CASs usually has inconsistency, which leads to wrong decisions made by systems, and even lowers user experience. Therefore, a new overall quality of context (OQoC) indicator is defined, which is the effective fusion of the parameters of reliability, up-to-dateness and modified correctness. Its accurate measurement is of great importance in inconsistency elimination. Moreover, we put forward a new context inconsistency elimination algorithm based on OQoC and Dempster-Shafer theory. The performance of the proposed algorithm is verified in personal identity verification scenario. Experimental results from multiple dimensions fully show the superiority of the proposed algorithm in solving context inconsistency problem, and quality of context information using the proposed algorithm has been greatly improved.																	1432-7643	1433-7479				JUL	2020	24	14					10829	10841		10.1007/s00500-019-04585-0													
J								An integrated optimization technique for optimal power flow solution	SOFT COMPUTING										Optimal power flow; Integrated optimization technique; Invasive weed optimization; Powell's pattern search	PARTICLE SWARM OPTIMIZATION; INVASIVE WEED OPTIMIZATION; FACTS DEVICES; DIFFERENTIAL EVOLUTION; COMPETITIVE ALGORITHM; HYBRID ALGORITHM; SEARCH ALGORITHM; DISPATCH; SYSTEMS; PERFORMANCE	The aim of the research work is to propose an integrated optimization technique, established with the integration of the invasive weed optimization (IWO) and Powell's pattern search (PPS) method. The IWO algorithm has been undertaken as a global search technique, which is inspired from the specific ecological behavior of weeds and has the ability to adapt to the changing environment. The local search PPS method is based upon a conjugate-based search and having excellent exploitation search capability, which helps to improve the solution obtained from IWO technique. The proposed technique is applied to solve optimal power flow (OPF) problem with the flexible AC transmission system devices. The OPF problem is a nonlinear, non-convex optimization problem and consists of continuous and discrete decision variables. The three objective functions comprise total fuel cost, pollutant emission and system transmission loss which are minimized sequentially. The proposed technique is tested on four standard IEEE test systems, and results are compared with the reported results in the literature and found promising. The results illustrate that the proposed technique performs better as compared to IWO technique in terms of the quality of solution and convergence characteristics. Further,ttest is performed to validate the statistical performance of the proposed integrated optimization technique.																	1432-7643	1433-7479				JUL	2020	24	14					10865	10882		10.1007/s00500-019-04590-3													
J								Statistical(C,1)(E,mu)\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$(C,1)(E,\mu )$$\end{document}-summability and associated fuzzy approximation theorems with statistical fuzzy rates	SOFT COMPUTING										Sequences of fuzzy numbers; Cesaro summability method; Euler's summability method; (C,1); (E, mu-summability mean; Korovkin's theory; Statistical convergence	CESARO SUMMABILITY; SEQUENCES; NUMBERS	The concept of statistical convergence has attracted the pervasive attention of the current researchers due basically to the fact that it is stronger than the ordinary convergence. Korovkin-type approximation theorem plays a vital role in the convergence of sequences of positive linear operators. Moreover, this type of approximation theorems has been extended through different statistical summability methods over general sequence spaces. The paper investigated statistical (C, 1)( E, mu) product summability mean for sequences of fuzzy numbers and proved a fuzzy Korovkin-type approximation theorem. Furthermore, we have established another result for the fuzzy rate of convergence which is uniform in fuzzy Korovkin-type approximation theorem under our proposed summability mean.																	1432-7643	1433-7479				JUL	2020	24	14					10883	10892		10.1007/s00500-019-04591-2													
J								Hybrid evolutionary algorithm (NNACOR) for energy minimization in a wireless mesh topology towards green computing	SOFT COMPUTING										Wireless mesh networks; Routing protocols; AODC; Evolutionary computing	SENSOR NETWORKS	Wireless sensor networks are a very rapidly emerging class of network structures being utilized in almost all computing environments. Energy consumption is an essential criterion especially with the recent need in conservation of energy and carbon footprint. The proposed research paper investigated the prospects of green computing towards implementation of a hybrid algorithm (NNACOR) based on evolutionary computing model deriving its behaviour from naturally occurring phenomenon for reducing the energy consumed by each node in the mesh topology. The experiments have been conducted in NS2 and compared against the conventional AODC routing protocol, and the observed results indicate a superior performance of the proposed evolutionary model by providing up to a 48% in energy saving as against a 44% energy saving reported in the literature.																	1432-7643	1433-7479				JUL	2020	24	14					10893	10902		10.1007/s00500-019-04592-1													
J								Robust credibilistic intuitionistic fuzzy clustering for image segmentation	SOFT COMPUTING										Credibilistic; Intuitionistic; Fuzzy clustering; Intuitionistic fuzzy entropy; Spatial relationship	ALGORITHM; FUZZINESS; NEGATION	To improve the anti-noise ability of credibilistic intuitionistic fuzzy c-means clustering method (CIFCM) for image segmentation, this paper proposes a robust credibilistic intuitionistic fuzzy c-means clustering method based on credibility of pixels and intuitionistic fuzzy entropy. Firstly, a new similarity measure is constructed by utilizing the grayscale and spatial relationship between the current pixel and its neighborhood pixels. Secondly, it is embedded into the objective function of credibilistic intuitionistic fuzzy c-means clustering, and a new robust clustering method with spatial constraints is presented to effectively solve the segmentation problem of image corrupted by high noise. In the end, the convergence of this proposed robust clustering method is strictly proved by iterated convergence theorem. Experimental results show that proposed algorithm has better noise-suppression ability and more satisfactory segmentation results than CIFCM algorithm.																	1432-7643	1433-7479				JUL	2020	24	14					10903	10932		10.1007/s00500-019-04593-0													
J								Supervised feature selection by constituting a basis for the original space of features and matrix factorization	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Feature selection; Subspace learning; Linear independence; Basis of features; Microarray datasets	UNSUPERVISED FEATURE-SELECTION; GENE-EXPRESSION; CLASSIFICATION; REDUCTION; PATTERNS; TUMOR	Most of existing research works in the field of feature selection via matrix factorization techniques have been employed for unsupervised learning problems. This paper introduces a new framework for the supervised feature selection, called supervised feature selection by constituting a basis for the original space of features and matrix factorization (SFS-BMF). To this end, SFS-BMF is a guided search to find a basis for the original space of features that inherently contains linearly independent features and can be replaced with the original space. For finding the best subset of features regarding the class attribute, information gain is utilized for the process of constructing a basis. In fact, a basis for the original features is constructed according to the most informative features in terms of the information gain. Then, this basis is decomposed through a matrix factorization form in order to select a subset of features. Our proposed method guarantees the maximum relevancy of selected features to the output by using the information gain while simultaneously secures the minimum redundancy among them based on the linear independence property. Several experiments on high-dimensional microarray datasets are conducted for illustrating the efficiency of SFS-BMF. The experimental results show that the proposed SFS-BMF method outperforms some state-of-the-art feature selection methods with respect to classification performance and also according to the computational complexity.																	1868-8071	1868-808X				JUL	2020	11	7					1405	1421		10.1007/s13042-019-01046-w													
J								Orientation space code and multi-feature two-phase sparse representation for palmprint recognition	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Palmprint recognition; Biometric; Orientation space; Sparse representation	FACE	Orientation based coding method is one of the most important approaches in palmprint recognition, which achieves impressive performance by extracting one or more dominant orientation features, and calculating the distance between features of two palmprints. However, simply using the orientation features may be vulnerable to noisy and rotation. In this paper, we proposed a novel orientation-space code scheme to represent the orientation space feature of palmprint and designed a novel multi-feature two-phase sparse representation (MTPSR) scheme for feature matching. Extensive experiments on three benchmark palmprint databases are conducted to demonstrate the high effectiveness of the proposed method																	1868-8071	1868-808X				JUL	2020	11	7					1453	1461		10.1007/s13042-019-01049-7													
J								Gaining-sharing knowledge based algorithm for solving optimization problems: a novel nature-inspired algorithm	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Evolutionary computation; Global optimization; Meta-heuristics; Nature-inspired algorithms; Population-based algorithm	META-HEURISTIC OPTIMIZATION; GLOBAL OPTIMIZATION; SEARCH ALGORITHM; SWARM; DESIGN; EVOLUTION; SYSTEM; COLONY; SIMULATION; DYNAMICS	This paper proposes a novel nature-inspired algorithm called Gaining Sharing Knowledge based Algorithm (GSK) for solving optimization problems over continuous space. The GSK algorithm mimics the process of gaining and sharing knowledge during the human life span. It is based on two vital stages, junior gaining and sharing phase and senior gaining and sharing phase. The present work mathematically models these two phases to achieve the process of optimization. In order to verify and analyze the performance of GSK, numerical experiments on a set of 30 test problems from the CEC2017 benchmark for 10, 30, 50 and 100 dimensions. Besides, the GSK algorithm has been applied to solve the set of real world optimization problems proposed for the IEEE-CEC2011 evolutionary algorithm competition. A comparison with 10 state-of-the-art and recent metaheuristic algorithms are executed. Experimental results indicate that in terms of robustness, convergence and quality of the solution obtained, GSK is significantly better than, or at least comparable to state-of-the-art approaches with outstanding performance in solving optimization problems especially with high dimensions.																	1868-8071	1868-808X				JUL	2020	11	7					1501	1529		10.1007/s13042-019-01053-x													
J								Stochastic trust region inexact Newton method for large-scale machine learning	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Stochastic optimisation; Subsampling; Second order methods; Inexact Newton; Large-scale learning		Nowadays stochastic approximation methods are one of the major research direction to deal with the large-scale machine learning problems. From stochastic first order methods, now the focus is shifting to stochastic second order methods due to their faster convergence and availability of computing resources. In this paper, we have proposed a novel stochastic trust region inexact Newton method, called as STRON, to solve large-scale learning problems which uses conjugate gradient to inexactly solve trust region subproblem. The method uses progressive subsampling in the calculation of gradient and Hessian values to take the advantage of both, stochastic and full-batch regimes. We have extended STRON using existing variance reduction techniques to deal with the noisy gradients and using preconditioned conjugate gradient as subproblem solver, and empirically proved that they do not work as expected, for the large-scale learning problems. Finally, our empirical results prove efficacy of the proposed method against existing methods with bench marked datasets.																	1868-8071	1868-808X				JUL	2020	11	7					1541	1555		10.1007/s13042-019-01055-9													
J								Unsupervised feature learning with sparse Bayesian auto-encoding based extreme learning machine	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										ELM; ELM auto-encoder; Multi-layer ELM; Bayesian learning	REGRESSION	Extreme learning machine (ELM) is a popular method in machine learning with extremely few parameters, fast learning speed and model efficiency. Unsupervised feature learning based ELM receives rising research focus. Recently the ELM auto-encoder (ELM-AE) was proposed for this task, which develops the ELM based compact feature learning without sacrificing elegant solution. Compared with ELM-AE and followingl1-regularized ELM-AE, we introduce a sparse Bayesian learning scheme into ELM-AE for better generalization capability. A parallel training strategy is also integrated to improve time-efficiency of multi-output sparse Bayesian learning. Furthermore, pruning hidden nodes for better performance and efficiency according to estimated variances of prior distribution of output weights is achieved. Experiments on several datasets verify the effectiveness and efficiency of our proposed ELM-AE for unsupervised feature learning, compared with PCA, NMF, ELM-AE andl1-regularized ELM-AE.																	1868-8071	1868-808X				JUL	2020	11	7					1557	1569		10.1007/s13042-019-01057-7													
J								Regularization on a rapidly varying manifold	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Manifold; Regularization; Laplacian; Hessian; Jerk; Inflection point; Jerk regularization	DIMENSIONALITY REDUCTION; CLASSIFICATION; FRAMEWORK; MACHINE	The presence of inflection points on data manifold or rapid variation makes it difficult for the second order derivative based graph Laplacian and Hessian regularization techniques to accurately approximate the marginal distribution parameters. Moreover, in general, function over-fitting on seen unlabeled instances due to lack of extrapolation power which makes graph Laplacian regularization based solution biased towards constant. Hessian solves this problem by opting a generic function based on the function's divergence in more than one direction. However, due to the presence of inflection points in the dense region, the function remains unpenalized by Hessian manifold regularization. We propose a Jerk based manifold regularization (JR) for dense, oscillating and manifolds with inflection points. JR approximates the rate of change of curvature from the underlying manifold which appropriately identifies the unpenalized geodesic deviating functions and accurately penalizes them. It also helps in identifying the optimal function in the presence of inflection points. Extensive experiments on synthetic and real-world datasets show that the proposed JR technique approximates accurate and generic input space geometrical constraints to outperform existing state-of-the-art manifold regularization techniques by a significant margin.																	1868-8071	1868-808X				JUL	2020	11	7					1571	1590		10.1007/s13042-019-01059-5													
J								An ORESTE approach for multi-criteria decision-making with probabilistic hesitant fuzzy information	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Multi-criteria decision making; Probabilistic hesitant fuzzy element; Normalized probabilistic hesitant fuzzy element; Euclidean distance; ORESTE	PREFERENCE RELATIONS; CONSENSUS MODEL; SETS; SUPPORT; SUSTAINABILITY; AGGREGATION; FRAMEWORK; SELECTION	As an important extension of fuzzy number, the probabilistic hesitant fuzzy element (PHFE) shows the flexibility of decision makers in expressing hesitant information in multi-criteria decision-making (MCDM) processes. Accordingly, numerous research findings have been obtained since PHFE introduction. However, a few important issues in PHFE utilization remain to be addressed. This study introduces the French organization Rangement Et Synthese De Ronnees Relationnelles' (ORESTE) approach for MCDM with probabilistic hesitant fuzzy information. First, the limitations of normalized PHFE (NPHFE), Euclidean distance, and several operations in previous studies are discussed. Subsequently, an algorithm is designed to derive the new NPHFE. A new Euclidean distance and several operations are developed on the basis of the proposed NPHFE. Second, the ORESTE approach is extended to probabilistic hesitant fuzzy environments. Lastly, the problem of selecting best research topic is presented to demonstrate that the proposed approach is effective. A comparative study with other approaches is conducted with identical illustrative example.																	1868-8071	1868-808X				JUL	2020	11	7					1591	1609		10.1007/s13042-020-01060-3													
J								Study of naturalness in tone-mapped images	COMPUTER VISION AND IMAGE UNDERSTANDING										Image unnaturalness study; High dynamic range image; Standard dynamic range image; Tone mapping operator; Multiple exposure fusion; Handcrafted features; Learned features; Transfer learning; Convolutional neural network	QUALITY ASSESSMENT; COLOR IMAGES; REPRODUCTION; DISPLAY; FUSION	Nowadays, images can be obtained in various ways such as capturing photos in single-exposure mode, applying Multiple Exposure Fusion algorithms to generate an image from multiple shoots of the same scene, mapping High Dynamic Range (HDR) images to Standard Dynamic Range (SDR) images, converting raw formats to displayable formats, or applying post-processing techniques to enhance image quality, aesthetic quality, ... When looking at some photos, one might have a feeling of unnaturalness. This paper deals with the problem of developing a model firstly to estimate if an image looks natural or not to humans and the second purpose is to try to understand how the unnaturalness feeling is induced by a photo: Are there specific unnaturalness clues or is unnaturalness a general feeling when looking at a photo? The study focuses on SDR images, especially on tone-mapped images. The first contribution of the paper is the setting of an experiment gathering human naturalness opinions on 1900 SDR images mainly obtained from tone mapping operators. Based on the collected data, the second contribution is to study the efficiency of different feature types including handcrafted features and learned features for image naturalness analysis. A binary classification model is then developed based on the determined features to classify if an image looks natural or unnatural.																	1077-3142	1090-235X				JUL	2020	196								102971	10.1016/j.cviu.2020.102971													
J								Product image recognition with guidance learning and noisy supervision	COMPUTER VISION AND IMAGE UNDERSTANDING											CLASSIFICATION	This paper considers to recognize products from daily photos, which is an important problem in real-world applications but also challenging due to background clutters, category diversifies, noisy labels, etc. We address this problem by two contributions. First, we introduce a novel large-scale product image dataset, termed as Product-90. Instead of collecting product images by laborious and time-intensive image capturing, we take advantage of the web and download images from the reviews of several e-commerce websites where the images are casually captured by consumers. Labels are assigned automatically by the categories of e-commerce websites. Totally the Product-90 consists of more than 140K images with 90 categories. Due to the fact that consumers may upload unrelated images, it is inevitable that our Product-90 introduces noisy labels. As the second contribution, we develop a simple yet efficient guidance learning (GL) method for training convolutional neural networks (CNNs) with noisy supervision. The GL method first trains an initial teacher network with the full noisy dataset, and then trains a target/student network with both large-scale noisy set and small manually-verified clean set in a multi-task manner. Specifically, in the stage of student network training, the large-scale noisy data is supervised by its guidance knowledge which is the combination of its given noisy label and the soften label from the teacher network. We conduct extensive experiments on our Products-90 and four public datasets, namely Food101, Food-101N, Clothing1M and synthetic noisy CIFAR-10. Our guidance learning method achieves performance superior to state-of-the-art methods on these datasets.																	1077-3142	1090-235X				JUL	2020	196								102963	10.1016/j.cviu.2020.102963													
J								Color edge preserving image colorization with a coupled natural vectorial total variation	COMPUTER VISION AND IMAGE UNDERSTANDING										Colorization; Color edge preserving; Natural vectorial total variation; Convexity; Primal-dual algorithm	MINIMIZATION; MODELS	In this paper, a novel colorization model based on the natural vectorial total variation TVJ is proposed. TVJ supports a common edge direction for all channels, it leads to a better preservation of color edges. We give the existence of the minimizer of the proposed model and use the primal-dual algorithm to numerically solve this model. Experiment results both on structure images and texture images show that the proposed model can preserve the contours well and reduce color bleeding effects at edges compared with the known methods.																	1077-3142	1090-235X				JUL	2020	196								102981	10.1016/j.cviu.2020.102981													
J								Age estimation from faces using deep learning: A comparative analysis	COMPUTER VISION AND IMAGE UNDERSTANDING										Automatic age estimation; Deep ageing patterns Learning; Convolutional neural network; Comparative analysis; Cross-domain age estimation; Knowledge transfer	CONVOLUTIONAL NEURAL-NETWORKS; DATABASE; RECOGNITION; GENDER	Automatic Age Estimation (AAE) has attracted attention due to the wide variety of possible applications. However, it is a challenging task because of the large variation of facial appearance and several other extrinsic and intrinsic factors. Most of the proposed approaches in the literature use hand-crafted features to encode ageing patterns. Deeply learned features extracted by Convolutional Neural Networks (CNNs) algorithms usually perform better than hand-crafted features. The main contribution of this paper is an extensive comparative analysis of several frameworks for real AAE based on deep learning architectures. Different well-known CNN architectures are considered and their performances are compared. MORPH, FGNET, FACES, PubFig and CASIA-web Face datasets are used in our experiments. The robustness of the best deep estimator is evaluated under noise, expression changes, "crossing" ethnicity and "crossing" gender. The experimental results demonstrate the high performances of the popular CNNs frameworks against the stateof-art methods of automatic age estimation. A Layer-wise transfer learning evaluation is done to study the optimal number of layers to fine-tune on AAE task. An evaluation framework of Knowledge transfer from face recognition task across AAE is performed. We have made our best-performing CNNs models publicly available that would allow one to duplicate the results and for further research on the use of CNNs for AAE from face images.																	1077-3142	1090-235X				JUL	2020	196								102961	10.1016/j.cviu.2020.102961													
J								Intelligent video analysis: A Pedestrian trajectory extraction method for the whole indoor space without blind areas	COMPUTER VISION AND IMAGE UNDERSTANDING										Fisheye camera; Pedestrian detection; Object tracking; Height estimation; Trajectory extraction		Pedestrian trajectory extraction is an important part of intelligent monitoring, which is of great significance to many fields such as statistics on pedestrian flow and density, population behavior analysis, abnormal behavior detection, etc. However, it is quite challenging to extract pedestrian trajectory without blind areas in the whole space due to the limited view angle of ordinary cameras. So far, no efficient method has been proposed to deal with this problem. In this paper, we propose a pedestrian trajectory extraction method based on a single fisheye camera, which can realize no blind areas pedestrian trajectory extraction in the whole interior space. First, the fisheye camera with a perspective of 180 degrees is adopted in our work which can realize the entire space monitoring without blind areas and avoid object matching among multiple cameras. Then, the deep convolutional neural network, the Kalman Filter algorithm, and the Hungarian algorithm are combined for pedestrian head detection and tracking. In order to calculate the coordinates of the trajectory points according to the obtained head position, we propose a novel pedestrian height estimation method for fisheye cameras. Finally, the pedestrian trajectory points are calculated based on the detected head position and the estimated height. The performance of the proposed pedestrian trajectory extraction method has been evaluated by a variety of experiments. The experimental results show that the trajectories of multiple pedestrians can be extracted simultaneously through the method proposed in this paper, and the average error of the trajectory points is less than 5.07 pixels in the 512x512 images.																	1077-3142	1090-235X				JUL	2020	196								102968	10.1016/j.cviu.2020.102968													
J								Classifier-agnostic saliency map extraction	COMPUTER VISION AND IMAGE UNDERSTANDING										Saliency map; Convolutional neural networks; Image classification; Weakly supervised localization		Currently available methods for extracting saliency maps identify parts of the input which are the most important to a specific fixed classifier. We show that this strong dependence on a given classifier hinders their performance. To address this problem, we propose classifier-agnostic saliency map extraction, which finds all parts of the image that any classifier could use, not just one given in advance. We observe that the proposed approach extracts higher quality saliency maps than prior work while being conceptually simple and easy to implement. The method sets the new state of the art result for localization task on the ImageNet data, outperforming all existing weakly-supervised localization techniques, despite not using the ground truth labels at the inference time. The code reproducing the results is available at hops://github.com/kondiz/casme.																	1077-3142	1090-235X				JUL	2020	196								102969	10.1016/j.cviu.2020.102969													
J								Word sense disambiguation using implicit information	NATURAL LANGUAGE ENGINEERING										Word sense disambiguation; Common sense; ConceptNet; Implicit information; Lexical priming	ACQUISITION	Humans proficiently interpret the true sense of an ambiguous word by establishing association among words in a sentence. The complete sense of text is also based on implicit information, which is not explicitly mentioned. The absence of this implicit information is a significant problem for a computer program that attempts to determine the correct sense of ambiguous words. In this paper, we propose a novel method to uncover the implicit information that links the words of a sentence. We reveal this implicit information using a graph, which is then used to disambiguate the ambiguous word. The experiments show that the proposed algorithm interprets the correct sense for both homonyms and polysemous words. Our proposed algorithm has performed better than the approaches presented in the SemEval-2013 task for word sense disambiguation and has shown an accuracy of 79.6 percent, which is 2.5 percent better than the best unsupervised approach in SemEval-2007.																	1351-3249	1469-8110				JUL	2020	26	4					413	432	PII S1351324919000421	10.1017/S1351324919000421													
J								Measuring diachronic language distance using perplexity: Application to English, Portuguese, and Spanish	NATURAL LANGUAGE ENGINEERING										Corpus linguistics; Language resources; Similarity		The objective of this work is to set a corpus-driven methodology to quantify automatically diachronic language distance between chronological periods of several languages. We apply a perplexity-based measure to written text representing different historical periods of three languages: European English, European Portuguese, and European Spanish. For this purpose, we have built historical corpora for each period, which have been compiled from different open corpus sources containing texts as close as possible to its original spelling. The results of our experiments show that a diachronic language distance based on perplexity detects the linguistic evolution that had already been explained by the historians of the three languages. It is remarkable to underline that it is an unsupervised multilingual method which only needs a raw corpora organized by periods.																	1351-3249	1469-8110				JUL	2020	26	4					433	454	PII S1351324919000378	10.1017/S1351324919000378													
J								Two approaches to compilation of bilingual multi-word terminology lists from lexical resources	NATURAL LANGUAGE ENGINEERING										Language resources; Machine translation; Terminology extraction; Text classification	STATISTICAL MACHINE TRANSLATION	In this paper, we present two approaches and the implemented system for bilingual terminology extraction that rely on an aligned bilingual domain corpus, a terminology extractor for a target language, and a tool for chunk alignment. The two approaches differ in the way terminology for the source language is obtained: the first relies on an existing domain terminology lexicon, while the second one uses a term extraction tool. For both approaches, four experiments were performed with two parameters being varied. In the experiments presented in this paper, the source language was English, and the target language Serbian, and a selected domain was Library and Information Science, for which an aligned corpus exists, as well as a bilingual terminological dictionary. For term extraction, we used theFlexiTermtool for the source language and a shallow parser for the target language, while for word alignment we used GIZA++. The evaluation results show that for the first approach the F(1)score varies from 29.43% to 51.15%, while for the second it varies from 61.03% to 71.03%. On the basis of the evaluation results, we developed a binary classifier that decides whether a candidate pair, composed of aligned source and target terms, is valid. We trained and evaluated different classifiers on a list of manually labeled candidate pairs obtained after the implementation of our extraction system. The best results in a fivefold cross-validation setting were achieved with the Radial Basis Function Support Vector Machine classifier, giving a F(1)score of 82.09% and accuracy of 78.49%.																	1351-3249	1469-8110				JUL	2020	26	4					455	479	PII S1351324919000615	10.1017/S1351324919000615													
J								Natural language generation: The commercial state of the art in 2020	NATURAL LANGUAGE ENGINEERING												It took a while, but natural language generation is now an established commercial software category. It's commented upon frequently in both industry media and the mainstream press, and businesses are willing to pay hard cash to take advantage of the technology. We look at who's active in the space, the nature of the technology that's available today and where things might go in the future.																	1351-3249	1469-8110				JUL	2020	26	4					481	487		10.1017/S135132492000025X													
J								Tackling challenges of neural purchase stage identification from imbalanced twitter data	NATURAL LANGUAGE ENGINEERING										Purchase stage identification; Neural networks; Social media data		Twitter and other social media platforms are often used for sharing interest in products. The identification of purchase decision stages, such as in the AIDA model (Awareness, Interest, Desire, and Action), can enable more personalized e-commerce services and a finer-grained targeting of advertisements than predicting purchase intent only. In this paper, we propose and analyze neural models for identifying the purchase stage of single tweets in a user's tweet sequence. In particular, we identify three challenges of purchase stage identification: imbalanced label distribution with a high number of non-purchase-stage instances, limited amount of training data, and domain adaptation with no or only little target domain data. Our experiments reveal that the imbalanced label distribution is the main challenge for our models. We address it with ranking loss and perform detailed investigations of the performance of our models on the different output classes. In order to improve the generalization of the models and augment the limited amount of training data, we examine the use of sentiment analysis as a complementary, secondary task in a multitask framework. For applying our models to tweets from another product domain, we consider two scenarios: for the first scenario without any labeled data in the target product domain, we show that learning domain-invariant representations with adversarial training is most promising, while for the second scenario with a small number of labeled target examples, fine-tuning the source model weights performs best. Finally, we conduct several analyses, including extracting attention weights and representative phrases for the different purchase stages. The results suggest that the model is learning features indicative of purchase stages and that the confusion errors are sensible.																	1351-3249	1469-8110				JUL	2020	26	4							PII S1351324919000433	10.1017/S1351324919000433													
J								Augmented Context-Based Conceptual User Modeling for Personalized Recommendation System in Online Social Networks	INTERNATIONAL JOURNAL OF COGNITIVE INFORMATICS AND NATURAL INTELLIGENCE										Knowledge Graph; Recommendation; Social Networks; User Modeling		As the data on the online social networks is getting larger, it is important to build personalized recommendation systems that recommend suitable content to users, there has been much research in this field that uses conceptual representations of text to match user models with best content. This article presents a novel method to build a user model that depends on conceptual representation of text by using ConceptNet concepts that exceed the named entities to include the common-sense meaning of words and phrases. The model includes the contextual information of concepts as well, the authors also show a novel method to exploit the semantic relations of the knowledge base to extend user models, the experiment shows that the proposed model and associated recommendation algorithms outperform all previous methods as a detailed comparison shows in this article.																	1557-3958	1557-3966				JUL-SEP	2020	14	3					1	19		10.4018/IJCINI.2020070101													
J								Fitness Distance Correlation Strategy for Solving the RGV Dynamic Scheduling Problem	INTERNATIONAL JOURNAL OF COGNITIVE INFORMATICS AND NATURAL INTELLIGENCE										Computer Numerical Controller; Dynamic Scheduling; Fitness Distance Correlation; Fitness Landscape; Rail Guide Vehicle	AISLE AUTOMATED STORAGE; GUIDED VEHICLES; ALGORITHMS; LANDSCAPE	Rail guide vehicle (RGV) problems have the characteristics of fast running, stable performance, and high automation. RGV dynamic scheduling has a great impact on the working efficiency of an entire automated warehouse. However, the relative intelligent optimization research of different workshop components for RGV dynamic scheduling problems are insufficient scheduling in the previous works. They appear idle when waiting, resulting in reduced operating efficiency during operation. This article proposes a new distance landscape strategy for the RGV dynamic scheduling problems. In order to solve the RGV dynamic scheduling problem more effectively, experiments are conducted based on the type of computer numerical controller (CNC) with two different procedures programming model in solving the RGV dynamic scheduling problems. The experiment results reveal that this new distance landscape strategy can provide promising results and solves the considered RGV dynamic scheduling problem effectively.																	1557-3958	1557-3966				JUL-SEP	2020	14	3					20	40		10.4018/IJCINI.2020070102													
J								Fuzzy Ontology-Based Querying User' Requests Under Uncertain Environment	INTERNATIONAL JOURNAL OF COGNITIVE INFORMATICS AND NATURAL INTELLIGENCE										Fuzzy Ontology; Imprecision; Knowledge Extraction; Uncertainty; User Requests		Assistance with the use of technical devices is required as soon as the tasks to be performed become complex. This assistance is also needed as soon as the authors provide assistance to users to find solutions to incidents that occur during the application of unsuitable procedures. The goal of this work is then to provide a knowledge extraction approach that can interpret user requests into valid system requests to respond appropriately to novice user requests. This approach is based on a fuzzy semantic network for the modeling of imprecise and uncertain knowledge and the automatic construction of temporary fuzzy ontology for the identification and interpretation of user requests. The proposed approach has the advantage of being able to integrate the notion of uncertain and imprecise knowledge into the representation of system objects and procedures. The experimental results show the efficiency and the effectiveness of the approach.																	1557-3958	1557-3966				JUL-SEP	2020	14	3					41	59		10.4018/IJCINI.2020070103													
J								An Improved Firefly Algorithm-Based 2-D Image Thresholding for Brain Image Fusion	INTERNATIONAL JOURNAL OF COGNITIVE INFORMATICS AND NATURAL INTELLIGENCE										Firefly Algorithm; Image Fusion; Image Thresholding; Interval Type-2 Fuzzy; Renyi Entropy; Scale Invariant Feature Transform	ADAPTIVE DIFFERENTIAL EVOLUTION; CONTOURLET; TRANSFORM; HISTOGRAM	In this article, an attempt is made to diagnose brain diseases like neoplastic, cerebrovascular, Alzheimer's, and sarcomas by the effective fusion of two images. The two images are fused in three steps. Step 1. Segmentation: The images are segmented on the basis of optimal thresholding, the thresholds are optimized with an improved firefly algorithm (pFA) by assuming Renyi entropy as an objective function. Earlier, image thresholding was performed with a 1-D histogram, but it has been recently observed that a 2-D histogram-based thresholding is better. Step 2: the segmented features are extracted with the scale invariant feature transform (SIFT) algorithm. The SIFT algorithm is good in extracting the features even after image rotation and scaling. Step 3: The fusion rules are made on the basis of an interval type-2 fuzzy set (IT2FL), where uncertainty effects are minimized unlike type-1. The novelty of the proposed work is tested on different benchmark image fusion data sets and has proven better in all measuring parameters.																	1557-3958	1557-3966				JUL-SEP	2020	14	3					60	96		10.4018/IJCINI.2020070104													
J								DNA Epidemic Model Construction and Dynamics Optimization	INTERNATIONAL JOURNAL OF COGNITIVE INFORMATICS AND NATURAL INTELLIGENCE										Bionic Optimization; Epidemic Dynamics; Evolution Algorithm; Immune Genes; Meta-Heuristic Search; Population Intelligent Optimization; Random Search; SIR Epidemic Model	DIFFERENTIAL EVOLUTION; MATHEMATICAL-THEORY; ALGORITHM; MUTATION	In order to solve some complex optimization problems, the SIR-DNA algorithm was constructed based on the DNA-based SIR (susceptible-infectious-recovered) infectious disease model. Since infectious diseases attack a very small part of the individual's genes, the number of variables per treatment is small; thus, the natural dimensionality reduction of the algorithm is achieved. Based on the DNA-SIR infectious disease model, different infections can be distinguished in the pathogenesis of viruses. The mechanisms of disease transmission are described by the SIR model, and these are used to construct operators such as SS, SI, II, IR, RR, and RS, so that individuals can naturally exchange information naturally through disease transmission. The test results show that the algorithm has the characteristics of strong search ability and has a high convergence speed for solving complex optimization problems.																	1557-3958	1557-3966				JUL-SEP	2020	14	3					97	117		10.4018/IJCINI.2020070105													
J								Optimizing for High Resolution ADC Model With Combined Architecture	INTERNATIONAL JOURNAL OF COGNITIVE INFORMATICS AND NATURAL INTELLIGENCE										Analog-Digital Conversion (ADC); Architecture; Data Acquisition System; Matlab; Noise Analysis; Pipeline; Simulation; Sigma-Delta Modulation	DELTA-SIGMA ADC	High resolution analog-digital conversion (ADC) is a key instrument to convert analog signals to digital signals, which is deployed in data acquisition system to match high resolution analog signals from seismometers systems. To achieve high resolution, architecture of Sigma-Delta oversampling or pipeline ADC architecture have following disadvantages: high power consumption, low linearity of modulators, and complex structure. This work presents a novel model architecture, which design principle is validated by mathematical formulations which combined advantages of both pipeline and Sigma-Delta oversampling ADC architecture. By discussing the adverse effects of the whole ADC architecture with an external noise theoretically, an amended theoretical model is proposed according to the assessment result of a noise simulation algorithm. The simulation results represent that the whole performance of combined architecture is determined by the noise level of integrator and subtractor. Using these two components with a noise index no more than 10-7 V/root Hz, the resolution of the prototype can achieve a reservation of 144.5 dB.																	1557-3958	1557-3966				JUL-SEP	2020	14	3					118	132		10.4018/IJCINI.2020070106													
J								DR vertical bar GRADUATE: Uncertainty-aware deep learning-based diabetic retinopathy grading in eye fundus images	MEDICAL IMAGE ANALYSIS										Diabetic retinopathy grading; Deep learning; Uncertainty; Explainability	SCREENING-PROGRAM; IDENTIFICATION; CLASSIFICATION; PREVALENCE	Diabetic retinopathy (DR) grading is crucial in determining the adequate treatment and follow up of patient, but the screening process can be tiresome and prone to errors. Deep learning approaches have shown promising performance as computer-aided diagnosis (CAD) systems, but their black-box behaviour hinders clinical application. We propose DR vertical bar GRADUATE, a novel deep learning-based DR grading CAD system that supports its decision by providing a medically interpretable explanation and an estimation of how uncertain that prediction is, allowing the ophthalmologist to measure how much that decision should be trusted. We designed DR vertical bar GRADUATE taking into account the ordinal nature of the DR grading problem. A novel Gaussian-sampling approach built upon a Multiple Instance Learning framework allow DR vertical bar GRADUATE to infer an image grade associated with an explanation map and a prediction uncertainty while being trained only with image-wise labels. DR vertical bar GRADUATE was trained on the Kaggle DR detection training set and evaluated across multiple datasets. In DR grading, a quadratic-weighted Cohen's kappa (kappa) between 0.71 and 0.84 was achieved in five different datasets. We show that high kappa values occur for images with low prediction uncertainty, thus indicating that this uncertainty is a valid measure of the predictions' quality. Further, bad quality images are generally associated with higher uncertainties, showing that images not suitable for diagnosis indeed lead to less trustworthy predictions. Additionally, tests on unfamiliar medical image data types suggest that DR vertical bar GRADUATE allows outlier detection. The attention maps generally highlight regions of interest for diagnosis. These results show the great potential of DR vertical bar GRADUATE as a second-opinion system in DR severity grading. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				JUL	2020	63								101715	10.1016/j.media.2020.101715													
J								Fully automatic brain tumor segmentation with deep learning-based selective attention using overlapping patches and multi-class weighted cross-entropy	MEDICAL IMAGE ANALYSIS										Brain tumor segmentation; Class-imbalance; Convolutional neural networks; Fully automatic; Glioblastomas; Overlapping patches	HEALTH-ORGANIZATION CLASSIFICATION; CENTRAL-NERVOUS-SYSTEM; MR-IMAGES; MODEL; FIELDS; GLIOMA; CRF	In this paper, we present a new Deep Convolutional Neural Networks (CNNs) dedicated to fully automatic segmentation of Glioblastoma brain tumors with high- and low-grade. The proposed CNNs model is inspired by the Occipito-Temporal pathway which has a special function called selective attention that uses different receptive field sizes in successive layers to figure out the crucial objects in a scene. Thus, using selective attention technique to develop the CNNs model, helps to maximize the extraction of relevant features from MRI images. We have also addressed two more issues: class-imbalance, and the spatial relationship among image Patches. To address the first issue, we propose two steps: an equal sampling of images Patches and an experimental analysis of the effect of weighted cross-entropy loss function on the segmentation results. In addition, to overcome the second issue, we have studied the effect of Overlapping Patches against Adjacent Patches where the Overlapping Patches show better segmentation results due to the introduction of the global context as well as the local features of the image Patches compared to the conventionnel Adjacent Patches. Our experiment results are reported on BRATS-2018 dataset where our End-to-End Deep Learning model achieved state-of-the-art performance. The median Dice score of our fully automatic segmentation model is 0.90, 0.83, 0.83 for the whole tumor, tumor core, and enhancing tumor respectively compared to the Dice score of radiologist, that is in the range 74%-85%. Moreover, our proposed CNNs model is not only computationally efficient at inference time, but it could segment the whole brain on average 12 seconds. Finally, the proposed Deep Learning model provides an accurate and reliable segmentation result, and that makes it suitable for adopting in research and as a part of different clinical settings. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				JUL	2020	63								101692	10.1016/j.media.2020.101692													
J								Surgical spectral imaging	MEDICAL IMAGE ANALYSIS										Multispectral imaging; Hyperspectral imaging; Minimally-invasive surgery; Computational imaging	INVERSE MONTE-CARLO; IN-VIVO; OXYGEN-SATURATION; OPTICAL SPECTROSCOPY; MULTISPECTRAL IMAGES; TISSUE OXYGENATION; RENAL OXYGENATION; SYSTEM; CANCER; MODEL	Recent technological developments have resulted in the availability of miniaturised spectral imaging sensors capable of operating in the multi- (MSI) and hyperspectral imaging (HSI) regimes. Simultaneous advances in image-processing techniques and artificial intelligence (AI), especially in machine learning and deep learning, have made these data-rich modalities highly attractive as a means of extracting biological information non-destructively. Surgery in particular is poised to benefit from this, as spectrally-resolved tissue optical properties can offer enhanced contrast as well as diagnostic and guidance information during interventions. This is particularly relevant for procedures where inherent contrast is low under standard white light visualisation. This review summarises recent work in surgical spectral imaging (SSI) techniques, taken from Pubmed, Google Scholar and arXiv searches spanning the period 2013-2019. New hardware, optimised for use in both open and minimally-invasive surgery (MIS), is described, and recent commercial activity is summarised. Computational approaches to extract spectral information from conventional colour images are reviewed, as tip-mounted cameras become more commonplace in MIS. Model-based and machine learning methods of data analysis are discussed in addition to simulation, phantom and clinical validation experiments. A wide variety of surgical pilot studies are reported but it is apparent that further work is needed to quantify the clinical value of MSI/HSI. The current trend toward data-driven analysis emphasises the importance of widely-available, standardised spectral imaging datasets, which will aid understanding of variability across organs and patients, and drive clinical translation. (C) 2020 The Authors. Published by Elsevier B.V.																	1361-8415	1361-8423				JUL	2020	63								101699	10.1016/j.media.2020.101699													
J								Multi-atlas image registration of clinical data with automated quality assessment using ventricle segmentation	MEDICAL IMAGE ANALYSIS										Registration; ventricles; Segmentation; Deep learning; Quality; Multi-atlas; age; White matter hyperintensity; ADNI	WHITE-MATTER HYPERINTENSITY; STROKE; ALGORITHMS; LESIONS	Registration is a core component of many imaging pipelines. In case of clinical scans, with lower resolution and sometimes substantial motion artifacts, registration can produce poor results. Visual assessment of registration quality in large clinical datasets is inefficient. In this work, we propose to automatically assess the quality of registration to an atlas in clinical FLAIR MRI scans of the brain. The method consists of automatically segmenting the ventricles of a given scan using a neural network, and comparing the segmentation to the atlas ventricles propagated to image space. We used the proposed method to improve clinical image registration to a general atlas by computing multiple registrations - one directly to the general atlas and others via different age-specific atlases - and then selecting the registration that yielded the highest ventricle overlap. Finally, as an example application of the complete pipeline, a voxelwise map of white matter hyperintensity burden was computed using only the scans with registration quality above a predefined threshold. Methods were evaluated in a single-site dataset of more than 1000 scans, as well as a multi-center dataset comprising 142 clinical scans from 12 sites. The automated ventricle segmentation reached a Dice coefficient with manual annotations of 0.89 in the single-site dataset, and 0.83 in the multi-center dataset. Registration via age-specific atlases could improve ventricle overlap compared to a direct registration to the general atlas (Dice similarity coefficient increase up to 0.15). Experiments also showed that selecting scans with the registration quality assessment method could improve the quality of average maps of white matter hyperintensity burden, instead of using all scans for the computation of the white matter hyperintensity map. In this work, we demonstrated the utility of an automated tool for assessing image registration quality in clinical scans. This image quality assessment step could ultimately assist in the translation of automated neuroimaging pipelines to the clinic. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				JUL	2020	63								101698	10.1016/j.media.2020.101698													
J								Accelerating Cartesian MRI by domain-transform manifold learning in phase-encoding direction	MEDICAL IMAGE ANALYSIS										Magnetic resonance imaging; Acceleration; Domain transform; Manifold learning	RESONANCE IMAGE-RECONSTRUCTION; NETWORKS	This study developed a domain-transform framework comprising domain-transform manifold learning with an initial analytic transform to accelerate Cartesian magnetic resonance imaging (DOTA-MRI). The proposed method directly transforms undersampled Cartesian k-space data into a reconstructed image. In Cartesian undersampling, the k-space is fully or zero sampled in the data-acquisition direction (i.e., the frequency-encoding direction or the x-direction); one-dimensional (1D) inverse Fourier transform (IFT) along the x-direction on the undersampled k-space does not induce any aliasing. To exploit this, the algorithm first applies an analytic x-direction 1D IFT to the undersampled Cartesian k-space input, and subsequently transforms it into a reconstructed image using deep neural networks. The initial analytic transform (i.e., 1D IFT) allows the fully connected layers of the neural network to learn 1D global transform only in the phase-encoding direction (i.e., the y-direction) instead of 2D transform. This drastically reduces the number of parameters to be learned from O(N-2) to O(N) compared with the existing manifold learning algorithm (i.e., automated transform by manifold approximation) (AUTOMAP). This enables DOTA-MRI to be applied to high-resolution MR datasets, which has previously proved difficult to implement in AUTOMAP because of the enormous memory requirements involved. After the initial analytic transform, the manifold learning phase uses a symmetric network architecture comprising three types of layers: front-end convolutional layers, fully connected layers for the 1D global transform, and back-end convolutional layers. The front-end convolutional layers take 1D IFT of the undersampled k-space (i.e., undersampled data in the intermediate domain or in the ky-x domain) as input and performs data-domain restoration. The following fully connected layers learn the 1D global transform between the ky-x domain and the image domain (i.e., the y-x domain). Finally, the back-end convolutional layers reconstruct the final image by denoising in the image domain. DOTA-MRI exhibited superior performance over nine other existing algorithms, including state-of-the-art deep learning-based algorithms. The generality of the algorithm was demonstrated by experiments conducted under various sampling ratios, datasets, and noise levels. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				JUL	2020	63								101689	10.1016/j.media.2020.101689													
J								Asymmetric fiber trajectory distribution estimation using streamline differential equation	MEDICAL IMAGE ANALYSIS										Diffusion MRI; Fiber orientation estimation; Asymmetric fiber trajectory distribution; Streamline differential equation	DIFFUSION TENSOR; TRACTOGRAPHY; RESOLUTION; TRACKING; VALIDATION; BRAIN	Fiber orientation distribution estimation with diffusion magnetic resonance imaging (dMRI) is critical in white matter fiber tractography which is most commonly based on symmetric crossing structures. Ambiguous spatial correspondence between estimated diffusion directions and fiber geometry, such as asymmetric crossing, bending, fanning, or kissing, makes tractography challenging. Consequently, numerous tracts suggest intertwined connections in unexpected regions of the white matter or actually stop prematurely in the white matter. In this work, we propose a novel asymmetric fiber trajectory distribution (FTD) function defined on neighboring voxels based on a streamline differential equation from fluid mechanics. The spatial consistency with intra- and inter-voxel constraints is derived for FTD estimation by introducing the concept of divergence. At a local level, the FTD is a series of curve flows that minimize the energy function, characterizes the relations between fibers and joint fiber fragments within the same fiber bundle. Experiments are performed on FiberCup phantom, ISMRM 2015 Tractography challenge data, and in vivo brain dMRI data for qualitative and quantitative evaluations. Results show that our approach can reveal continuous asymmetric FTD details that are potentially useful for robust tractography. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				JUL	2020	63								101686	10.1016/j.media.2020.101686													
J								Dense biased networks with deep priori anatomy and hard region adaptation: Semi-supervised learning for fine renal artery segmentation	MEDICAL IMAGE ANALYSIS										Renal artery segmentation; Semi-supervised learning; Dense biased network; Dense biased connection; Deep priori anatomy; Hard region adaptation loss function; 3D fine segmentation; CT angiography image	COMPUTED-TOMOGRAPHY ANGIOGRAPHY; CENTERLINE EXTRACTION	Fine renal artery segmentation on abdominal CT angiography (CTA) image is one of the most important tasks for kidney disease diagnosis and pre-operative planning. It will help clinicians locate each interlobar artery's blood-feeding region via providing the complete 3D renal artery tree masks. However, it is still a task of great challenges due to the large intra-scale changes, large inter-anatomy variation, thin structures, small volume ratio and small labeled dataset of the fine renal artery. In this paper, we propose the first semi-supervised 3D fine renal artery segmentation framework, DPA-DenseBiasNet, which combines deep prior anatomy (DPA), dense biased network (DenseBiasNet) and hard region adaptation loss (HRA): 1) Based on our proposed dense biased connection, the DenseBiasNet fuses multi-receptive field and multi-resolution feature maps for large intra-scale changes. This dense biased connection also obtains a dense information flow and dense gradient flow so that the training is accelerated and the accuracy is enhanced. 2) DPA features extracted from an autoencoder (AE) are embedded in DenseBiasNet to cope with the challenge of large inter-anatomy variation and thin structures. The AE is pre-trained (unsupervised) by numerous unlabeled data to achieve the representation ability of anatomy features and these features are embedded in DenseBiasNet. This process will not introduce incorrect labels as optimization targets and thus contributes to a stable semi-supervised training strategy that is suitable for sensitive thin structures. 3) The HRA selects the loss value calculation region dynamically according to the segmentation quality so the network will pay attention to the hard regions in the training process and keep the class balanced. Experiments demonstrated that DPA-DenseBiasNet had high predictive accuracy and generalization with the Dice coefficient of 0.884 which increased by 0.083 compared with 3D U-Net (Cicek et al., 2016). This revealed our framework with great potential for the 3D fine renal artery segmentation in clinical practice. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				JUL	2020	63								101722	10.1016/j.media.2020.101722													
J								Self-weighted adaptive structure learning for ASD diagnosis via multi-template multi-center representation	MEDICAL IMAGE ANALYSIS										Autism spectrum disorder; Pearson's correlation (PC) -based sparse low-rank representation; Multi-template multi-center; Self-weighted adaptive structure learning	FUNCTIONAL CONNECTIVITY; AUTISM; NETWORKS; CLASSIFICATION; FEATURES	As a kind of neurodevelopmental disease, autism spectrum disorder (ASD) can cause severe social, communication, interaction, and behavioral challenges. To date, many imaging-based machine learning techniques have been proposed to address ASD diagnosis issues. However, most of these techniques are restricted to a single template or dataset from one imaging center. In this paper, we propose a novel multi-template multi-center ensemble classification scheme for automatic ASD diagnosis. Specifically, based on different pre-defined templates, we construct multiple functional connectivity (FC) brain networks for each subject based on our proposed Pearson's correlation-based sparse low-rank representation. After extracting features from these FC networks, informative features to learn optimal similarity matrix are then selected by our self-weighted adaptive structure learning (SASL) model. For each template, the SASL method automatically assigns an optimal weight learned from the structural information without additional weights and parameters. Finally, an ensemble strategy based on the multi- template multi-center representations is applied to derive the final diagnosis results. Extensive experiments are conducted on the publicly available Autism Brain Imaging Data Exchange (ABIDE) database to demonstrate the efficacy of our proposed method. Experimental results verify that our proposed method boosts ASD diagnosis performance and outperforms state-of-the-art methods. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				JUL	2020	63								101662	10.1016/j.media.2020.101662													
J								Cellular community detection for tissue phenotyping in colorectal cancer histology images	MEDICAL IMAGE ANALYSIS										Computational pathology; Tissue phenotyping; Tumor microenvironment; Cellular communities	PATHOLOGY; REPRESENTATION; CLASSIFICATION; VALIDATION; ALGORITHM; NETWORKS; STROMA	Classification of various types of tissue in cancer histology images based on the cellular compositions is an important step towards the development of computational pathology tools for systematic digital profiling of the spatial tumor microenvironment. Most existing methods for tissue phenotyping are limited to the classification of tumor and stroma and require large amount of annotated histology images which are often not available. In the current work, we pose the problem of identifying distinct tissue phenotypes as finding communities in cellular graphs or networks. First, we train a deep neural network for cell detection and classification into five distinct cellular components. Considering the detected nuclei as nodes, potential cell-cell connections are assigned using Delaunay triangulation resulting in a cell-level graph. Based on this cell graph, a feature vector capturing potential cell-cell connection of different types of cells is computed. These feature vectors are used to construct a patch-level graph based on chi-square distance. We map patch-level nodes to the geometric space by representing each node as a vector of geodesic distances from other nodes in the network and iteratively drifting the patch nodes in the direction of positive density gradients towards maximum density regions. The proposed algorithm is evaluated on a publicly available dataset and another new large-scale dataset consisting of 280K patches of seven tissue phenotypes. The estimated communities have significant biological meanings as verified by the expert pathologists. A comparison with current state-of-the-art methods reveals significant performance improvement in tissue phenotyping. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				JUL	2020	63								101696	10.1016/j.media.2020.101696													
J								Designing weighted correlation kernels in convolutional neural networks for functional connectivity based brain disease diagnosis	MEDICAL IMAGE ANALYSIS										Functional connectivity network; Correlation kernel; Convolutional neural network; Alzheimer's disease; Classification	MILD COGNITIVE IMPAIRMENT; DEFAULT-MODE NETWORK; ALZHEIMERS-DISEASE; IDENTIFICATION; VARIABILITY; STATES	Functional connectivity networks (FCNs) based on functional magnetic resonance imaging (fMRI) have been widely applied to analyzing and diagnosing brain diseases, such as Alzheimer's disease (AD) and its prodrome stage, i.e., mild cognitive impairment (MCI). Existing studies usually use Pearson correlation coefficient (PCC) method to construct FCNs, and then extract network measures (e.g., clustering coefficients) as features to learn a diagnostic model. However, the valuable observation information in network construction (e.g., specific contributions of different time points), as well as high-level and high-order network features are neglected in these studies. In this paper, we first define a novel weighted correlation kernel (called wc-kernel) to measure the correlation of brain regions, by which weighting factors are learned in a data-driven manner to characterize the contributions of different time points, thus conveying the richer interaction information among brain regions compared with the PCC method. Furthermore, we build a wc-kernel based convolutional neural network (CNN) (called wck-CNN) framework for learning the hierarchical (Le., from local to global and also from low-level to high-level) features for disease diagnosis, by using fMRI data. Specifically, we first define a layer to build dynamic FCNs using our proposed wc-kernels. Then, we define another three layers to sequentially extract local (brain region specific), global (brain network specific) and temporal features from the constructed dynamic FCNs for classification. Experimental results on 174 subjects (a total of 563 scans) with rest-state fMRI (rs-fMRI) data from ADNI database demonstrate the efficacy of our proposed method. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				JUL	2020	63								101709	10.1016/j.media.2020.101709													
J								DeepDistance: A multi-task deep regression model for cell detection in inverted microscopy images	MEDICAL IMAGE ANALYSIS										Multi-task learning; Feature learning; Fully convolutional network; Cell detection; Cell segmentation; Inverted microscopy image analysis	SEGMENTATION; NUCLEI; CLASSIFICATION	This paper presents a new deep regression model, which we call DeepDistance, for cell detection in images acquired with inverted microscopy. This model considers cell detection as a task of finding most probable locations that suggest cell centers in an image. It represents this main task with a regression task of learning an inner distance metric. However, different than the previously reported regression based methods, the DeepDistance model proposes to approach its learning as a multi-task regression problem where multiple tasks are learned by using shared feature representations. To this end, it defines a secondary metric, normalized outer distance, to represent a different aspect of the problem and proposes to define its learning as complementary to the main cell detection task. In order to learn these two complementary tasks more effectively, the DeepDistance model designs a fully convolutional network (FCN) with a shared encoder path and end-to-end trains this FCN to concurrently learn the tasks in parallel. For further performance improvement on the main task, this paper also presents an extended version of the DeepDistance model that includes an auxiliary classification task and learns it in parallel to the two regression tasks by also sharing feature representations with them. DeepDistance uses the inner distances estimated by these FCNs in a detection algorithm to locate individual cells in a given image. In addition to this detection algorithm, this paper also suggests a cell segmentation algorithm that employs the estimated maps to find cell boundaries. Our experiments on three different human cell lines reveal that the proposed multi-task learning models, the DeepDistance model and its extended version, successfully identify the locations of cell as well as delineate their boundaries, even for the cell line that was not used in training, and improve the results of its counterparts. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				JUL	2020	63								101720	10.1016/j.media.2020.101720													
J								Generative-based airway and vessel morphology quantification on chest CT images	MEDICAL IMAGE ANALYSIS										Lung; Airway; Vessel; Deep-learning regression; Subvoxel resolution	COMPUTED-TOMOGRAPHY; ACCURATE MEASUREMENT; WALL THICKNESS; SEGMENTATION; EMPHYSEMA; DISEASE; SMOKERS; COPD; LUNG; DIMENSIONS	Accurately and precisely characterizing the morphology of small pulmonary structures from Computed Tomography (CT) images, such as airways and vessels, is becoming of great importance for diagnosis of pulmonary diseases. The smaller conducting airways are the major site of increased airflow resistance in chronic obstructive pulmonary disease (COPD), while accurately sizing vessels can help identify arterial and venous changes in lung regions that may determine future disorders. However, traditional methods are often limited due to image resolution and artifacts. We propose a Convolutional Neural Regressor (CNR) that provides cross-sectional measurement of airway lumen, airway wall thickness, and vessel radius. CNR is trained with data created by a generative model of synthetic structures which is used in combination with Simulated and Unsupervised Generative Adversarial Network (SimGAN) to create simulated and refined airways and vessels with known ground-truth. For validation, we first use synthetically generated airways and vessels produced by the proposed generative model to compute the relative error and directly evaluate the accuracy of CNR in comparison with traditional methods. Then, in-vivo validation is performed by analyzing the association between the percentage of the predicted forced expiratory volume in one second (FEVI%) and the value of the Pi10 parameter, two well-known measures of lung function and airway disease, for airways. For vessels, we assess the correlation between our estimate of the small-vessel blood volume and the lungs' diffusing capacity for carbon monoxide (DLCO). The results demonstrate that Convolutional Neural Networks (CNNs) provide a promising direction for accurately measuring vessels and airways on chest CT images with physiological correlates. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				JUL	2020	63								101691	10.1016/j.media.2020.101691													
J								A scalable approach to T2-MRI colon segmentation	MEDICAL IMAGE ANALYSIS										Colon segmentation; MRI; Graph-cuts; Tubularity	AUTOMATIC SEGMENTATION; SPINAL-CORD; ENHANCEMENT	The study of the colonic volume is a procedure with strong relevance to gastroenterologists. Depending on the clinical protocols, the volume analysis has to be performed on MRI of the unprepared colon without contrast administration. In such circumstances, existing measurement procedures are cumbersome and time-consuming for the specialists. The algorithm presented in this paper permits a quasi-automatic segmentation of the unprepared colon on T2-weighted MRI scans. The segmentation algorithm is organized as a three-stage pipeline. In the first stage, a custom tubularity filter is run to detect colon candidate areas. The specialists provide a list of points along the colon trajectory, which are combined with tubularity information to calculate an estimation of the colon medial path. In the second stage, we delimit the region of interest by applying custom segmentation algorithms to detect colon neighboring regions and the fat capsule containing abdominal organs. Finally, within the reduced search space, segmentation is performed via 3D graph-cuts in a three-stage multigrid approach. Our algorithm was tested on MRI abdominal scans, including different acquisition resolutions, and its results were compared to the colon ground truth segmentations provided by the specialists. The experiments proved the accuracy, efficiency, and usability of the algorithm, while the variability of the scan resolutions contributed to demonstrate the computational scalability of the multigrid architecture. The system is fully applicable to the colon measurement clinical routine, being a substantial step towards a fully automated segmentation. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				JUL	2020	63								101697	10.1016/j.media.2020.101697													
J								Automatic spatial estimation of white matter hyperintensities evolution in brain MRI using disease evolution predictor deep neural networks	MEDICAL IMAGE ANALYSIS										White matter hyperintensities (WMH); WMH Evolution; Disease evolution predictor (DEP) models; DEP Generative adversarial network (DEP-GAN); DEP U-Residual Network (DEP-UResNet); Small vessel disease (SVD)	SMALL VESSEL DISEASE; STROKE LESION SEGMENTATION; COGNITIVE CONSEQUENCES; RISK-FACTORS; PROGRESSION; IMAGES; REGRESSION; DYNAMICS; INSIGHTS; VOLUME	Previous studies have indicated that white matter hyperintensities (WMH), the main radiological feature of small vessel disease, may evolve (i.e., shrink, grow) or stay stable over a period of time. Predicting these changes are challenging because it involves some unknown clinical risk factors that leads to a non-deterministic prediction task. In this study, we propose a deep learning model to predict the evolution of WMH from baseline to follow-up (i.e., 1-year later), namely "Disease Evolution Predictor" (DEP) model, which can be adjusted to become a non-deterministic model. The DEP model receives a baseline image as input and produces a map called "Disease Evolution Map" (DEM), which represents the evolution of WMH from baseline to follow-up. Two DEP models are proposed, namely DEP-UResNet and DEP-GAN, which are representatives of the supervised (i.e., need expert-generated manual labels to generate the output) and unsupervised (i.e., do not require manual labels produced by experts) deep learning algorithms respectively. To simulate the non-deterministic and unknown parameters involved in WMH evolution, we modulate a Gaussian noise array to the DEP model as auxiliary input. This forces the DEP model to imitate a wider spectrum of alternatives in the prediction results. The alternatives of using other types of auxiliary input instead, such as baseline WMH and stroke lesion loads are also proposed and tested. Based on our experiments, the fully supervised machine learning scheme DEP-UResNet regularly performed better than the DEP-GAN which works in principle without using any expert-generated label (i.e., unsupervised). However, a semi-supervised DEP-GAN model, which uses probability maps produced by a supervised segmentation method in the learning process, yielded similar performances to the DEP-UResNet and performed best in the clinical evaluation. Furthermore, an ablation study showed that an auxiliary input, especially the Gaussian noise, improved the performance of DEP models compared to DEP models that lacked the auxiliary input regardless of the model's architecture. To the best of our knowledge, this is the first extensive study on modelling WMH evolution using deep learning algorithms, which deals with the non-deterministic nature of WMH evolution. (C) 2020 The Authors. Published by Elsevier B.V.																	1361-8415	1361-8423				JUL	2020	63								101712	10.1016/j.media.2020.101712													
J								Artificial neural networks for magnetic resonance elastography stiffness estimation in inhomogeneous materials	MEDICAL IMAGE ANALYSIS										Magnetic resonance elastography; Inversion; Stiffness; Artificial neural networks	MR ELASTOGRAPHY; INVERSION; BRAIN; VISCOELASTICITY; RECONSTRUCTION; ALGORITHM; FREQUENCY; PATTERNS; FIBROSIS; DISEASE	Purpose: To test the hypothesis that removing the assumption of material homogeneity will improve the spatial accuracy of stiffness estimates made by Magnetic Resonance Elastography (MRE). Methods: An artificial neural network was trained using synthetic wave data computed using a coupled harmonic oscillator model. Material properties were allowed to vary in a piecewise smooth pattern. This neural network inversion (Inhomogeneous Learned Inversion (ILI)) was compared against a previous homogeneous neural network inversion (Homogeneous Learned Inversion (HLI)) and conventional direct inversion (DI) in simulation, phantom, and in-vivo experiments. Results: In simulation experiments, ILI was more accurate than HLI and DI in predicting the stiffness of an inclusion in noise-free, low-noise, and high-noise data. In the phantom experiment, ILI delineated inclusions <= 2.25 cm in diameter more clearly than HLI and DI, and provided a higher contrast-to-noise ratio for all inclusions. In a series of stiff brain tumors, ILI shows sharper stiffness transitions at the edges of tumors than the other inversions evaluated. Conclusion: ILI is an artificial neural network based framework for MRE inversion that does not assume homogeneity in material stiffness. Preliminary results suggest that it provides more accurate stiffness estimates and better contrast in small inclusions and at large stiffness gradients than existing algorithms that assume local homogeneity. These results support the need for continued exploration of learning-based approaches to MRE inversion, particularly for applications where high resolution is required. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				JUL	2020	63								101710	10.1016/j.media.2020.101710													
J								Towards multi-center glaucoma OCT image screening with semi-supervised joint structure and function multi-task learning	MEDICAL IMAGE ANALYSIS										Optical coherence tomography; Deep learning; Glaucoma screening; Semi-supervised multi-task learning	NERVE-FIBER LAYER; DIAGNOSIS; NETWORK; SEGMENTATION; CLASSIFIERS	Glaucoma is the leading cause of irreversible blindness in the world. Structure and function assessments play an important role in diagnosing glaucoma. Nowadays, Optical Coherence Tomography (OCT) imaging gains increasing popularity in measuring the structural change of eyes. However, few automated methods have been developed based on OCT images to screen glaucoma. In this paper, we are the first to unify the structure analysis and function regression to distinguish glaucoma patients from normal controls effectively. Specifically, our method works in two steps: a semi-supervised learning strategy with smoothness assumption is first applied for the surrogate assignment of missing function regression labels. Subsequently, the proposed multi-task learning network is capable of exploring the structure and function relationship between the OCT image and visual field measurement simultaneously, which contributes to classification performance improvement. It is also worth noting that the proposed method is assessed by two large-scale multi-center datasets. In other words, we first build the largest glaucoma OCT image dataset (i.e., HK dataset) involving 975,400 B-scans from 4,877 volumes to develop and evaluate the proposed method, then the model without further fine-tuning is directly applied on another independent dataset (i.e., Stanford dataset) containing 246,200 B-scans from 1,231 volumes. Extensive experiments are conducted to assess the contribution of each component within our framework. The proposed method outperforms the baseline methods and two glaucoma experts by a large margin, achieving volume-level Area Under ROC Curve (AUC) of 0.977 on HK dataset and 0.933 on Stanford dataset, respectively. The experimental results indicate the great potential of the proposed approach for the automated diagnosis system. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				JUL	2020	63								101695	10.1016/j.media.2020.101695													
J								Convolutional neural networks for classification of Alzheimer's disease: Overview and reproducible evaluation	MEDICAL IMAGE ANALYSIS										Convolutional neural network; Reproducibility; Alzheimer's disease classification Magnetic resonance imaging	MILD COGNITIVE IMPAIRMENT; DIFFEOMORPHIC IMAGE REGISTRATION; EARLY-DIAGNOSIS; MRI; EXTRACTION; PREDICTION; ENSEMBLE; SYSTEM; MCI	Numerous machine learning (ML) approaches have been proposed for automatic classification of Alzheimer's disease (AD) from brain imaging data. In particular, over 30 papers have proposed to use convolutional neural networks (CNN) for AD classification from anatomical MRI. However, the classification performance is difficult to compare across studies due to variations in components such as participant selection, image preprocessing or validation procedure. Moreover, these studies are hardly reproducible because their frameworks are not publicly accessible and because implementation details are lacking. Lastly, some of these papers may report a biased performance due to inadequate or unclear validation or model selection procedures. In the present work, we aim to address these limitations through three main contributions. First, we performed a systematic literature review. We identified four main types of approaches: i) 2D slice-level, ii) 3D patch-level, iii) ROI-based and iv) 3D subject-level CNN. Moreover, we found that more than half of the surveyed papers may have suffered from data leakage and thus reported biased performance. Our second contribution is the extension of our open-source framework for classification of AD using CNN and T1-weighted MRI. The framework comprises previously developed tools to automatically convert ADNI, AIBL and OASIS data into the BIDS standard, and a modular set of image preprocessing procedures, classification architectures and evaluation procedures dedicated to deep learning. Finally, we used this framework to rigorously compare different CNN architectures. The data was split into training/validation/test sets at the very beginning and only the training/validation sets were used for model selection. To avoid any overfitting, the test sets were left untouched until the end of the peer-review process. Overall, the different 3D approaches (3D-subject, 3D-ROI, 3D-patch) achieved similar performances while that of the 2D slice approach was lower. Of note, the different CNN approaches did not perform better than a SVM with voxel-based features. The different approaches generalized well to similar populations but not to datasets with different inclusion criteria or demographical characteristics. All the code of the framework and the experiments is publicly available: general-purpose tools have been integrated into the Clinica software (www.clinica.run) and the paper-specific code is available at: https://github.com/aramis-lab/AD-DL. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				JUL	2020	63								101694	10.1016/j.media.2020.101694													
J								Identifying Cross-individual Correspondences of 3-hinge Gyri	MEDICAL IMAGE ANALYSIS										Gyral hinge; Structural connectivity; Cortical folding pattern; Cross-subject correspondence	SURFACE-BASED ANALYSIS; DEEP SULCAL LANDMARKS; CEREBRAL-CORTEX; SPATIAL-DISTRIBUTION; FOLDING PATTERNS; CORTICAL ROIS; HUMAN BRAIN; LOCALIZATION; SEGMENTATION; REGISTRATION	Human brain alignment based on imaging data has long been an intriguing research topic. One of the challenges is the huge inter-individual variabilities, which are pronounced not only in cortical folding patterns, but also in the underlying structural and functional patterns. Also, it is still not fully understood how to link the cross-subject similarity of cortical folding patterns to the correspondences of structural brain wiring diagrams and brain functions. Recently, a specific cortical gyral folding pattern was identified, which is the conjunction of gyri from multiple directions and termed a "gyral hinge". These gyral hinges are characterized by the thickest cortices, the densest long-range fibers, and the most complex functional profiles in contrast to other gyri. In addition to their structural and functional importance, a small portion of 3-hinges found correspondences across subjects and even species by manual labeling. However, it is unclear if such cross-subject correspondences can be found for all 3-hinges, or if the correspondences are interpretable from structural and functional aspects. Given the huge variability of cortical folding patterns, we proposed a novel algorithm which jointly uses structural MRI-derived cortical folding patterns and diffusion-MRI-derived fiber shape features to estimate the correspondences. This algorithm was executed in a group-wise manner, whereby 3-hinges of all subjects were simultaneously aligned. The effectiveness of the algorithm was demonstrated by higher cross-subject 3-hinges' consistency with respect to structural and functional metrics, when compared with other methods. Our findings provide a novel approach to brain alignment and an insight to the linkage between cortical folding patterns and the underlying structural connective diagrams and brain functions. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				JUL	2020	63								101700	10.1016/j.media.2020.101700													
J								Image reconstruction with low-rankness and self-consistency of k-space data in parallel MRI	MEDICAL IMAGE ANALYSIS										Parallel imaging; Image reconstruction; Low-rank; Structured Hankel matrix; SPIRiT	SPARSIFYING TRANSFORMS; HANKEL MATRIX; DYNAMIC MRI; NEIGHBORHOODS; FACTORIZATION; ALGORITHMS; RECOVERY; LORAKS	Parallel magnetic resonance imaging has served as an effective and widely adopted technique for accelerating data collection. The advent of sparse sampling offers aggressive acceleration, allowing flexible sampling and better reconstruction. Nevertheless, faithfully reconstructing the image from limited data still poses a challenging task. Recent low-rank reconstruction methods are superior in providing high-quality images. Nevertheless, none of them employ the routinely acquired calibration data to improve image quality in parallel magnetic resonance imaging. In this work, an image reconstruction approach named STDLR-SPIRiT is proposed to explore the simultaneous two-directional low-rankness (STDLR) in the k-space data and to mine the data correlation from multiple receiver coils with the iterative self-consistent parallel imaging reconstruction (SPIRiT). The reconstruction problem is then solved with a singular value decomposition-free numerical algorithm. Experimental results of phantom and brain imaging data show that the proposed method outperforms the state-of-the-art methods in terms of suppressing artifacts and achieving the lowest error. Moreover, the proposed method exhibits robust reconstruction even when the auto-calibration signals are limited in parallel imaging. Overall the proposed method can be exploited to achieve better image quality for accelerated parallel magnetic resonance imaging. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				JUL	2020	63								101687	10.1016/j.media.2020.101687													
J								Tripartite-GAN: Synthesizing liver contrast-enhanced MRI to improve tumor detection	MEDICAL IMAGE ANALYSIS										Contrast-enhanced MRI synthesis; Tripartite-GAN; Dual attention module; Tumor detection	CT	Contrast-enhanced magnetic resonance imaging (CEMRI) is crucial for the diagnosis of patients with liver tumors, especially for the detection of benign tumors and malignant tumors. However, it suffers from high-risk, time-consuming, and expensive in current clinical diagnosis due to the use of the gadolinium-based contrast agent (CA) injection. If the CEMRI can be synthesized without CA injection, there is no doubt that it will greatly optimize the diagnosis. In this study, we propose a Tripartite Generative Adversarial Network (Tripartite-GAN) as a non-invasive, time-saving, and inexpensive clinical tool by synthesizing CEMRI to detect tumors without CA injection. Specifically, our innovative Tripartite-GAN combines three associated-networks (an attention-aware generator, a convolutional neural network-based discriminator, and a region-based convolutional neural network-based detector) for the first time, which achieves CEMRI synthesis and tumor detection promoting each other in an end-to-end framework. The generator facilitates detector for accurate tumor detection via synthesizing tumor-specific CEMRI. The detector promotes the generator for accurate CEMRI synthesis via the back-propagation. In order to synthesize CEMRI of equivalent clinical value to real CEMRI, the attention-aware generator expands the receptive field via hybrid convolution, and enhances feature representation and context learning of multi-class liver MRI via dual attention mechanism, and improves the performance of convergence of loss via residual learning. Moreover, the attention maps obtained from the generator newly added into the detector improve the performance of tumor detection. The discriminator promotes the generator to synthesize high-quality CEMRI via the adversarial learning strategy. This framework is tested on a large corpus of axial T1 FS Pre-Contrast MRI and axial T1 FS Delay MRI of 265 subjects. Experimental results and quantitative evaluation demonstrate that the Tripartite-GAN achieves high-quality CEMRI synthesis that peak signal-to-noise rate of 28.8 and accurate tumor detection that accuracy of 89.4%, which reveals that Tripartite-GAN can aid in the clinical diagnosis of liver tumors. (C) 2020 Elsevier B.V. All rights reserved.																	1361-8415	1361-8423				JUL	2020	63								101667	10.1016/j.media.2020.101667													
J								FinPathlight: Framework for an multiagent recommender system designed to increase consumer financial capability	DECISION SUPPORT SYSTEMS										Knowledge-based recommender systems; Ontology-based recommendation agents; Multiagent systems; Financial capability; Consumer finance; Financial planning	SCIENCE RESEARCH; INFORMATION; METHODOLOGY; AGENTS	In consideration of the general lack of trust in human professional financial advisors due to conflicts of interest, and given inadequacies in terms of the utility of FinTech alternatives for financial goal recommendations, this study establishes a framework for an ontology-based, multiagent recommender system designed to improve financial capability through the recommendation of financial goals, called FinPathlight. The FinPathlight framework provides an architecture for a personal financial recommender system designed to identify and recommend specific, achievable financial goals appropriate to a wide range of financially situated users. This framework contributes principles of implementation for a novel financial technology (FinTech) application aimed at addressing a pervasive lack of trust surrounding traditional financial advisory services, as well as utility inadequacies within the current landscape for FinTech applications, providing a comprehensive set of practical and explicit financial goal recommendations. Considering the importance of users' adoption of an innovation, this study empirically tests its utility in terms of trust and perceived usefulness. The experimental evaluation results show that an application built using this framework would likely be perceived as trustworthy and useful to users for identification and selection of financial capability enhancing objectives.																	0167-9236	1873-5797				JUL	2020	134								113306	10.1016/j.dss.2020.113306													
J								Automated discovery of business process simulation models from event logs	DECISION SUPPORT SYSTEMS										Process simulation; Process mining; Automated process discovery		Business process simulation is a versatile technique to estimate the performance of a process under multiple scenarios. This, in turn, allows analysts to compare alternative options to improve a business process. A common roadblock for business process simulation is that constructing accurate simulation models is cumbersome and error-prone. Modern information systems store detailed execution logs of the business processes they support. Previous work has shown that these logs can be used to discover simulation models. However, existing methods for log-based discovery of simulation models do not seek to optimize the accuracy of the resulting models. Instead they leave it to the user to manually tune the simulation model to achieve the desired level of accuracy. This article presents an accuracy-optimized method to discover business process simulation models from execution logs. The method decomposes the problem into a series of steps with associated configuration parameters. A hyper-parameter optimization method is used to search through the space of possible configurations so as to maximize the similarity between the behavior of the simulation model and the behavior observed in the log. The method has been implemented as a tool and evaluated using logs from different domains.																	0167-9236	1873-5797				JUL	2020	134								113284	10.1016/j.dss.2020.113284													
J								A dynamic shipment matching problem in hinterland synchromodal transportation	DECISION SUPPORT SYSTEMS										Hinterland synchromodal transportation; Dynamic shipment matching; Rolling horizon approach; Heuristic algorithm	INTERMODAL FREIGHT TRANSPORTATION; SERVICE NETWORK DESIGN; ALLOCATION; SYSTEM; BARGE; PORT	Hinterland intermodal transportation is the movement of containers between deep-sea ports and inland terminals by using trucks, trains, barges, or any combination of them. Synchromodal transportation, as an extension of intermodal transportation, refers to transport systems with dynamic updating of plans by incorporating real-time information. The trend towards spot markets and digitalization in hinterland intermodal transportation gives rise to online synchromodal transportation problems. This paper investigates a dynamic shipment matching problem in which a centralized platform provides online matches between shipment requests and transport services. We propose a rolling horizon approach to handle newly arrived shipment requests and develop a heuristic algorithm to generate timely solutions at each decision epoch. The experiment results demonstrate the solution accuracy and computational efficiency of the heuristic algorithm in comparison to an exact algorithm. The proposed rolling horizon approach outperforms a greedy approach from practice in total costs under various scenarios of the system.																	0167-9236	1873-5797				JUL	2020	134								113289	10.1016/j.dss.2020.113289													
J								Transparency and accountability in AI decision support: Explaining and visualizing convolutional neural networks for text information	DECISION SUPPORT SYSTEMS										Convolutional neural network; Machine learning interpretability; Class activation mapping; Explainable artificial intelligence	BIG DATA; DATA QUALITY; ANALYTICS; GO	Proliferating applications of deep learning, along with the prevalence of large-scale text datasets, have revolutionized the natural language processing (NLP) field, thereby driving the recent explosive growth. Nevertheless, it is argued that state-of-the-art studies focus excessively on producing quantitative performances superior to existing models, by playing "the Kaggle game." Hence, the field requires more effort in solving new problems and proposing novel approaches and architectures. We claim that one of the promising and constructive efforts would be to design transparent and accountable artificial intelligence (AI) systems for text analytics. By doing so, we can enhance the applicability and problem-solving capacity of the system for real-world decision support. It is widely accepted that deep learning models demonstrate remarkable performances compared to existing algorithms. However, they are often criticized for being less interpretable, i.e., the "black box." In such cases, users tend to hesitate to utilize them for decision-making, especially in crucial tasks. Such complexity obstructs transparency and accountability of the overall system, potentially debilitating the deployment of decision support systems powered by AI. Furthermore, recent regulations are emphasizing fairness and transparency in algorithms to a greater extent, turning explanations more compulsory than voluntary. Thus, to enhance the transparency and accountability of the decision support system and preserve the capacity to model complex text data at the same time, we propose the Explaining and Visualizing Convolutional neural networks for Text information (EVCT) framework. By adopting and ameliorating cutting-edge methods in NLP and image processing, the EVCT framework provides a human-interpretable solution to the problem of text classification while minimizing information loss. Experimental results with large-scale, real-world datasets show that EVCT performs comparably to benchmark models, including widely used deep learning models. In addition, we provide instances of human-interpretable and relevant visualized explanations obtained from applying EVCT to the dataset and possible applications for real-world decision support.																	0167-9236	1873-5797				JUL	2020	134								113302	10.1016/j.dss.2020.113302													
J								Uplift Modeling for preventing student dropout in higher education	DECISION SUPPORT SYSTEMS										Learning analytics; Uplift modeling; Student dropout; Educational data mining	RETENTION; ATTRITION	Uplift modeling is an approach for estimating the incremental effect of an action or treatment at the individual level. It has gained attention in the marketing and analytics communities due to its ability to adequately model the effect of direct marketing actions via predictive analytics. The main contribution of our study is the implementation of the uplift modeling framework to maximize the effectiveness of retention efforts in higher education institutions i.e., improvement of academic performance by offering tutorials. The objective is to improve the design of retention programs by tailoring them to students who are more likely to be retained if targeted. Data from three different bachelor programs from a Chilean university were collected. Students who participated in the tutorials are considered the treatment group, otherwise, they are assigned to the nontreatment group. Our results demonstrate the virtues of uplift modeling in tailoring retention efforts in higher education over conventional predictive modeling approaches.																	0167-9236	1873-5797				JUL	2020	134								113320	10.1016/j.dss.2020.113320													
J								Employees recruitment: A prescriptive analytics approach via machine learning and mathematical programming	DECISION SUPPORT SYSTEMS										Recruitment; Machine learning; Human resource analytics; Explainable artificial intelligence; Interpretable AI; Mathematical programming	STATISTICAL PROCESS-CONTROL; PERFORMANCE; PERSONNEL; TURNOVER	In this paper, we propose a comprehensive analytics framework that can serve as a decision support tool for HR recruiters in real-world settings in order to improve hiring and placement decisions. The proposed framework follows two main phases: a local prediction scheme for recruitments' success at the level of a single job placement, and a mathematical model that provides a global recruitment optimization scheme for the organization, taking into account multilevel considerations. In the first phase, a key property of the proposed prediction approach is the interpretability of the machine learning (ML) model, which in this case is obtained by applying the Variable-Order Bayesian Network (VOBN) model to the recruitment data. Specifically, we used a uniquely large dataset that contains recruitment records of hundreds of thousands of employees over a decade and represents a wide range of heterogeneous populations. Our analysis shows that the VOBN model can provide both high accuracy and interpretability insights to HR professionals. Moreover, we show that using the interpretable VOBN can lead to unexpected and sometimes counter-intuitive insights that might otherwise be overlooked by recruiters who rely on conventional methods. We demonstrate that it is feasible to predict the successful placement of a candidate in a specific position at a pre-hire stage and utilize predictions to devise a global optimization model. Our results show that in comparison to actual recruitment decisions, the devised framework is capable of providing a balanced recruitment plan while improving both diversity and recruitment success rates, despite the inherent trade-off between the two.																	0167-9236	1873-5797				JUL	2020	134								113290	10.1016/j.dss.2020.113290													
J								IoT-based location and quality decision-making in emerging shared parking facilities with competition	DECISION SUPPORT SYSTEMS										Sharing economy; Shared parking; Electric vehicle; Sharing decision support	FEATURE-SELECTION; DESIGN-MODEL; OPTIMIZATION; STRATEGIES; ALGORITHM; HYBRID; SITE	Shared parking firms offer a double-sided platform for parking space sharing. Many of these firms provide differentiated service levels to both suppliers and buyers. This new phenomenon in the parking industry materialized thanks to recent innovations in IoT-enabled automation and electric vehicle charging technologies. We study shared parking firms. Specifically, we formulate the firm's location and quality decision problem by using a multiplicative interaction model with competition. A non-cooperative game renders the optimized quality levels and location selections at Nash equilibrium in the presence of competition. We illustrate managerial insights with a small-sized problem. For industry practitioners, we propose a tailored branch and bound based exact algorithm and a problem-specific genetic algorithm for large-sized problems. Simulated computational results confirm the effectiveness and efficiency of the proposed shared-parking decision support model.																	0167-9236	1873-5797				JUL	2020	134								113301	10.1016/j.dss.2020.113301													
J								Deterioration control decision support for perishable inventory management	DECISION SUPPORT SYSTEMS										Decision support; Perishable inventory management; Deterioration-dependent demand; Controlled deterioration rate	STOCK-DEPENDENT DEMAND; STOCHASTIC DEMAND; SHELF-SPACE; PRODUCTS; MODEL; INVESTMENT; TIME	Deterioration rate is an important characteristic of perishable products. While deterioration in perishables is unavoidable, there are proven ways to lower their deterioration rates. Deterioration rate is commonly treated as an exogenous parameter in extant inventory management literature. We define deterioration rate as one of the controllable variables and propose a novel freshness-preservation effort (FPE) indicator. We consider perishable inventory management with FPE for deterioration control decision support. Our results show that there exists an optimal order quantity that minimizes total cost for a controlled deterioration scenario. The optimal FPE indicator value and the optimal order quantity are investigated through numerical analysis. We perform sensitivity analysis and conclude with related managerial implications.																	0167-9236	1873-5797				JUL	2020	134								113308	10.1016/j.dss.2020.113308													
J								New Caledonian crow learning algorithm: A new metaheuristic algorithm for solving continuous optimization problems	APPLIED SOFT COMPUTING										Social learning; Swarm intelligence; Observational learning; Nature-inspired computing; Socio-inspired optimization	EVOLUTIONARY; MECHANISMS	Several metaheuristic algorithms have been introduced to solve different optimization problems. Such algorithms are inspired by a wide range of natural phenomena or behaviors. We introduced a new metaheuristic algorithm called New Caledonian (NC) crow learning algorithm (NCCLA), inspired by efficient social, asocial, and reinforcement mechanisms that NC-crows use to learn behaviors for developing tools from Pandanus trees to obtain food. Such mechanisms were modeled mathematically to develop NCCLA, whose performance was subsequently evaluated and statistically analyzed using 23 classical benchmark functions and 4 engineering problems. The results verify NCCLA's performance efficiency and highlight its accelerated convergence and ability to escape from local minima. An extensive comparative study was conducted to demonstrate that the solution accuracy and convergence rate of NCCLA were better than those of other state-of-the-art metaheuristics. The results also indicate that NCCLA is a promising algorithm that can be applied to solve other optimization and real-world problems. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106325	10.1016/j.asoc.2020.106325													
J								A new fuzzy approach based on BWM and fuzzy preference programming for hospital performance evaluation: A case study	APPLIED SOFT COMPUTING										Group decision-making; Best-worst method; Fuzzy preference programming; Hospital performance evaluation	INFORMATION-SYSTEM; SELECTION; DECISION; DEA; TECHNOLOGY; EFFICIENCY; QUALITY; MODEL	Hospital performance evaluation (HPE) has a major role in improving the quality, safety, and effectiveness of health care services, so it is indispensable for proper and continuous operation of hospitals. Although several studies have been performed on HPE, few have used group decision-making (GDM). This study presents a comprehensive multi-criteria GDM model for HPE under uncertain conditions. In this model, we have combined the group best-worst method (GBWM) and fuzzy preference programming (FPP) method to create an applicable framework for GDM in which members of a decision-making group including decision-makers (DMs) have different expertise and the importance of criteria and DMs opinions are determined by a supervisor. The advantages of the proposed method include the integration of the GDM process in the form of a single model and there is no need to calculate separately the consistency of the decisions of the decision-making team members. Finally, a case study conducted on 5 hospitals in Tehran is presented to demonstrate the applicability and effectiveness of the proposed method. The results show that Sina Hospital, Baharloo Hospital, and Tehran Heart Center were ranked first to third, respectively. Also, we can conclude from this study that the proposed integrated framework is capable to address the HPE problem by using a GDM and considering the uncertainty of the comparisons made by decision-making team members. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106279	10.1016/j.asoc.2020.106279													
J								Discrete artificial electric field algorithm for high-order graph matching	APPLIED SOFT COMPUTING										High order graph matching; Artificial intelligence; Artificial electric field algorithm	PARTICLE SWARM OPTIMIZATION; STEREO	High-order graph matching is a problem of establishing the correspondences between two sets of visual features subject to high-order matching constraints. This is an NP-hard combinatorial optimization problem and formulated as a maximization problem of matching score over all permutations of features. Artificial electric field algorithm (AEFA) (Yadav et al., 2019) is a proven optimization algorithm in the family of meta-heuristic and performed well for continuous optimization problems. In this article, we extended the AEFA algorithm for combinatorial high-order graph matching problems and introduced a discrete artificial electric field algorithm (DAEFA). This framework incorporates the redefine position and velocity representation scheme, addition-subtraction operation, velocity and position update rules, and a problem specific initialization by using heuristic information. The efficiency of the proposed algorithm is tested over three well-known datasets: synthetic, CMU house and real-world datasets. The computational results measured the matching score, accuracy of matching and established the correspondences between two graphs. The computational results show the outperformance of the proposed algorithm over the other state-of-art algorithms in terms of good matching score and accuracy both. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106260	10.1016/j.asoc.2020.106260													
J								Modeling energy flow in natural gas networks using time series disaggregation and fuzzy systems tuned by particle swarm optimization	APPLIED SOFT COMPUTING										Gas network; Gas consumption; Low frequency and high frequency time series; Time series disaggregation; TSK fuzzy system; Particle swarm optimization	NONINTRUSIVE LOAD DISAGGREGATION; SUPPORT VECTOR REGRESSION; FORECASTING ALGORITHM; PSO VARIANT; C-MEANS; CONSUMPTION; DEMAND; PREDICTION; CLASSIFICATION; SIMULATION	Natural gas is widely used in industrial, residential, and commercial sectors and is delivered to consumption nodes via gas distribution networks. For efficient management and utilization of this nonrenewable energy resource, High Frequency (HF) response of gas networks to nodal consumption is needed that requires solution of the network governing equations. This high frequency response could either be measured with expensive high technology hardware at each consumption node or alternatively calculated from Low Frequency (LF) data collected by inexpensive low technology gas meters, which are usually preferred. Solution of the governing equations itself requires nodal gas consumption that is recorded by LF gas meters installed at consumption nodes. The recording frequency differs from one meter to another. Gas companies use these LF meter data just for billing. This paper presents a methodology for HF study of gas networks response to nodal consumption using these LF data. A Time Series Disaggregation (TSD) method is formulated that disaggregates the LF meter readings to HF gas consumption all with the same frequency by which the network governing equations are solved. HF gas consumption of each node is employed to train Takagi-Sugeno-Kang (TSK) fuzzy system to forecast HF consumption of that node in forthcoming days. The governing equations are then solved using this forecasted nodal consumption to predict the gas network behavior in the days ahead. This enables gas companies to recognize areas of the network with high pressure drop in cold days and manage the network accordingly. The paper also presents two techniques to prevent pressure drop in the areas of the network with high gas consumption. The proposed methods are applied to a gas network with 4258 customers using available LF data recorded during three years. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106332	10.1016/j.asoc.2020.106332													
J								PRRAT_AM-An advanced ant-miner to extract accurate and comprehensible classification rules	APPLIED SOFT COMPUTING										Classification; Ant-miner; Adaptive gamma; Ants selection; Pheromone update; Convergence speed; Generic rules	NEURAL-NETWORKS; PREDICTION; ALIGNMENT	Ant-Miner, a rule-based classification algorithm, has been successfully applied for classification tasks but it has some limitations such as getting stuck in local optima, high selective pressure, fixed exploration and exploitation rate, and premature convergence. In this paper, we have proposed a novel Ant-Miner based technique based on new Pheromone update method, Rule Rejection threshold, Adaptive gamma, and altered Tournament selection (PRRAT_AM) that caters to these limitations. The proposed algorithm introduced an adaptive gamma parameter to avoid fixed exploration and exploitation rate. To decrease the selective pressure, pheromone is updated by weighted average of rule length, rule quality and heuristic of the path. Ants are selected using improved tournament selection strategy to update the pheromone. Rules that covered less than one percent of the training examples are rejected to generate generic rules. These improvements aid PRRAT_AM in avoiding premature convergence and high selective pressure. We have tested the proposed approach on eight publicly available data-sets on standard benchmark performance measures that include accuracy and F1-score. The proposed approach has been compared with state of the art versions of Ant-Miner and with various data mining algorithms. The experimental results showed that the proposed approach achieved better results when compared with other techniques in terms of standard performance measures and convergence speed. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106326	10.1016/j.asoc.2020.106326													
J								Research on feature selection for rotating machinery based on Supervision Kernel Entropy Component Analysis with Whale Optimization Algorithm	APPLIED SOFT COMPUTING										Multi-features; WOSKECA; PSOSVM; Fault diagnosis; Feature selection	FAULT-DIAGNOSIS; PERMUTATION ENTROPY	Aimed at finding a scientific and effective method to diagnose faults in rotating machinery, the algorithm based on Supervision Kernel Entropy Component Analysis with Whale Optimization Algorithm (WOSKECA) for feature selection has been proposed in this paper. Firstly, for ensure sufficient information gathering from the rotating machinery, multi-features parameters of the time domain, frequency domain, time-frequency domain and entropy domain are extracted, and a high-dimensional feature matrix is constructed from these features. Secondly, the WOSKECA for feature selection is applied to eliminate any redundant information. The algorithm takes the class information as the supervised information to improve the recognition accuracy of Kernel Entropy Component Analysis (KECA) and can extract low-dimensional features with discriminative ability from the high-dimensional feature space. Meanwhile, the Whale Optimization Algorithm (WOA) as a new meta-heuristic optimization algorithm is applied to optimize the kernel parameters in KECA, which reduces the interference of subjective factors and reduces the professionalism of obtaining fault information. Finally, Support Vector Machine based on the Particle Swarm Optimization (PSOSVM) is used to classify the fault type as well as assess the severity of the faults. The feature extraction algorithm is entirely evaluated through experimentation and comparative. The results show that the proposed method is able to detect and classify the faults of rotating machinery more successfully and more accurately than traditional manifold learning. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106245	10.1016/j.asoc.2020.106245													
J								A learnheuristic approach for the team aerial drone motion constraints orienteering problem with	APPLIED SOFT COMPUTING										Team orienteering problem; Metaheuristics; Machine learning; Learnheuristics; Aerial drones; Route-dependent edge times	FACILITY LOCATION PROBLEM; VEHICLE-ROUTING PROBLEM; EVOLUTIONARY ALGORITHM; BIASED RANDOMIZATION; OPTIMIZATION; CURVATURE; GRASP; AIR	This work proposes a learnheuristic approach (combination of heuristics with machine learning) to solve an aerial-drone team orienteering problem. The goal is to maximise the total reward collected from information gathering or surveillance observations of a set of known targets within a fixed amount of time. The aerial drone team orienteering problem has the complicating feature that the travel times between targets depend on a drone's flight path between previous targets. This path-dependence is caused by the aerial surveillance drones flying under the influence of air-resistance, gravity, and the laws of motion. Sharp turns slow drones down and the angle of ascent and air-resistance influence the acceleration a drone is capable of. The route dependence of inter-target travel times motivates the consideration of a learnheuristic approach, in which the prediction of travel times is outsourced to a machine learning algorithm. This work proposes an instance-based learning algorithm with interpolated predictions as the learning module. We show that a learnheuristic approach can lead to higher quality solutions in a shorter amount of time than those generated from an equivalent metaheuristic algorithm, an effect attributed to the search-diversity enhancing consequence of the online learning process. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106280	10.1016/j.asoc.2020.106280													
J								DenseAttentionSeg: Segment hands from interacted objects using depth input	APPLIED SOFT COMPUTING										Hand-object interaction; Semantic segmentation; Artificial neural networks; Human-computer interaction		Hand segmentation is an important task in computer vision, which is usually the foundation of hand pose recognition, hand tracking, and reconstruction. For hand segmentation, it is more challenging when the hand is interacting with objects, but handling interacting motions is more important for applications like HCI and VR. In this paper, we propose a real-time DNN-based technique to segment hand and object in interacting motions from a single depth input. Our model is called DenseAttentionSeg, which contains a dense attention mechanism which effectively fuses information in different scales and improves the quality of result with skip-connections. Besides, we introduce a contour loss in model training, which helps to generate accurate hand and object boundaries. Finally, we propose our InterSegHands dataset, a fine-scale hand segmentation dataset containing about 52k depth maps of hand-object interactions, with the ground truth segmentation masks. Our experiments evaluate the effectiveness of our techniques and datasets, and indicate that our method outperforms the current state-of-the-art deep segmentation methods in handling hand-object interactions. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106297	10.1016/j.asoc.2020.106297													
J								Real-time location systems selection by using a fuzzy MCDM approach: An application in humanitarian relief logistics	APPLIED SOFT COMPUTING										Fuzzy sets; Decision making; Humanitarian relief logistics; Real time location systems; Performance management; Technology selection	GROUP DECISION-MAKING; MODEL; MANAGEMENT; SETS; AHP	The real-time location systems (RTLSs) with different positioning technologies allow real-time and high-precision localization of assets. Since the usage of RTLSs technologies and their population increase, RTLSs technology selection problem that addresses many factors should be considered. Therefore, it is important to determine the performance criteria and evaluation of these technologies should be investigated before RTLSs technology is applied to the system. This paper aims to select the most appropriate RTLSs technology by using a combined fuzzy based decision-making approach. Thus, the first paper for selection of RTLSs systems in a holistic approach by combining benefit and risk factors has been revealed. The developed approach is applied to humanitarian relief logistics warehouse with four alternatives which are given as Ultra-Wide Band, Wi-Fi, UHF RFID and Active RFID for the selection. The proposed approach has been integrated with interval-valued intuitionistic fuzzy (IVIF) sets that allow to deal with fuzziness inherent in decision making processes. For this aim, firstly IVIF DEMATEL is used to determine the inner and outer dependencies of the sub and main criteria; secondly, weights of the sub-criteria are obtained by using IVIF ANP. Finally, the best RTLSs technology to be used in humanitarian logistics warehouse is selected by using IVIF TOPSIS. As a result of the calculations, the best system is determined as the "Wi-Fi RTLS'' system based on the given context. By the way, a sensitivity analysis has been also implemented to test and validate the developed methodology. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106322	10.1016/j.asoc.2020.106322													
J								Local gradient full-scale transform patterns based off-line text-independent writer identification	APPLIED SOFT COMPUTING										Handwritten documents; Off-line mode; Feature algorithm; Dissimilarity measure; Connected components	ORIENTED GRADIENTS; CLASSIFICATION; DESCRIPTORS; HISTOGRAMS; EXTRACTION; RETRIEVAL; FEATURES	Handwriting based writer identification is one of the reliable components of behavioral biometrics. A huge effort has been done in recent years to improve the writer identification performance. Our paper presents a new and effective off-line text-independent system for writer identification. Extracting features from handwriting substantially impacts the ability of the classification process to identify the query writers. With the use of suitable classifier, a well-designed and discriminative feature extraction improves the classification performance. For that, we introduce a discriminative yet simple feature method, referred to as Local gradient full-Scale Transform Patterns (LSTP). The proposed LSTP algorithm captures salient local writing structure at small regions of interest of the writing. These writing regions are termed as connected components. In the classification stage, we perform Hamming distance based NN classifier to compare and match LSTP feature vectors. The proposed framework is evaluated on 9 well-known handwritten benchmarks. Experimental results show high identification performance against the current state-of-the-art. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106277	10.1016/j.asoc.2020.106277													
J								Application of modified pigeon-inspired optimization algorithm and constraint -objective sorting rule on multi-objective optimal power flow problem	APPLIED SOFT COMPUTING										Modified pigeon-inspired optimization algorithm; Optimal power flow problem; Constraint-objective sorting rule; Penalty function method	ECONOMIC-DISPATCH; EMISSION; COST; LOSSES; MOEA/D	To solve the non-differentiable optimal power flow (OPF) problems with multiple contradictory objectives, a modified pigeon-inspired optimization algorithm (MPIO) is put forward in this paper. Combining with the common-used penalty function method (PFM), the MPIO-PFM algorithm is proposed and applied to optimize the active power loss, emission and fuel cost (with valve-point loadings) of power system. Eight simulation trials carried out on MATLAB software validate MPIO-PFM algorithm can obtain superior Pareto Frontier (PF) comparing with the typical NSGA-II algorithm. Nevertheless, some Pareto solutions obtained by MPIO-PFM algorithm cannot satisfy all system constraints due to the difficulty in choosing the proper penalty coefficients. Thus, an innovative approach named as constraint-objective sorting rule (COSR) is presented in this paper. The bi-objective and tri-objective trials implemented on IEEE 30-node, 57-node and 118-node systems demonstrate that the Pareto optimal set (POS) obtained by MPIO-COSR algorithm realizes zero-violation of various system constraints. Furthermore, the generational-distance and hyper-volume indexes quantitatively illustrate that in contrast to NSGA-II and MPIO-PFM methods, the MPIO-COSR algorithm can determine the evenly-distributed PFs with satisfactory-diversity. The intelligent MPIO-COSR algorithm provides an effective way to handle the non-convex MOOPF problems. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106321	10.1016/j.asoc.2020.106321													
J								A new combined model based on multi-objective salp swarm optimization for wind speed forecasting	APPLIED SOFT COMPUTING										Combined model; Wind speed forecasting; Multi-objective optimization algorithm; Data preprocessing technique	TIME-SERIES; ALGORITHM; SYSTEM; DECOMPOSITION; POWER; PREDICTION; MULTISTEP; MACHINE; NETWORK; ARIMA	Wind energy as the representative renewable energy sources attracted the global attention and wind power plays a significant role in power system. Thus, wind speed forecasting is highly critical in wind power grid management. The short-term wind speed prediction can effectively support power grid-management to reduce wind curtailments. In the past, lots of researches had often considered how to enhance the accuracy or stability in short wind speed forecasting. Nevertheless, just focus on one criterion is the inability to build an effective predictive system. In this paper, a novel combined forecasting system was proposed and effectively applied to address the issue of wind speed prediction while obtaining high precision and strong stability simultaneously at the same time. Four ANNs (artificial neural networks) were combined by the optimal weighting coefficients determined by MSSO (multi-objective salp swarm optimizer) in this system and data decomposition and denoising are included in the data preprocessing stage. The multi-objective optimization algorithm overcomes the weakness of the single-objective optimization algorithm that can only achieve one criterion. It can simultaneously optimize accuracy and stability. The 10-minute wind speed data of three data sets of Penglai, China were selected for multi-step forecasting to evaluate the effectiveness of the proposed combined model. And experimental results show that the proposed model not only achieves excellent precision and stability but also outperforms other proposed combined models. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106294	10.1016/j.asoc.2020.106294													
J								Feature selection via normative fuzzy information weight with application into tumor classification	APPLIED SOFT COMPUTING										Fuzzy rough set; Feature selection; Tumor classification; Conditional mutual information; Normative fuzzy information weight; Knowledge granularity	ROUGH SET-THEORY; ATTRIBUTE REDUCTION; MUTUAL INFORMATION; UNCERTAINTY; PREDICTION; ENTROPY; GRANULARITY; GRANULATION	Feature selection via mutual information has been widely used in data analysing. Mutual information with monotonous is an effective tool to analyse the correlation and redundancy of features. However, the mutual information, which adopted in most of existing feature selection criterions, can't explain the correlation and redundancy of features in the fuzzy situation well. Therefore, we propose feature selection strategy via normative fuzzy information weight based on fuzzy conditional mutual information in this paper. Firstly, the monotone fuzzy metric structure is defined, and some theoretical properties are proved. Secondly, we put forward the concept of fuzzy independent classification information based on fuzzy conditional mutual information, and propose a feature selection method via fuzzy independent classification information. Thirdly, considering the proportion of new classification information provided by the selected feature in its own information, we introduce the concept of normative fuzzy information weight and propose an improved feature selection method. Finally, the availability of the two proposed methods is tested by comparative experiments, and the improved feature selection method is applied to tumor classification. This work provides an alternative strategy for feature selection in real-world data applications. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106299	10.1016/j.asoc.2020.106299													
J								Integrated fabric procurement and multi-site apparel production planning with cross-docking: A hybrid fuzzy-robust stochastic programming approach	APPLIED SOFT COMPUTING										Procurement planning; Purchasing-production decisions; Global supply chain; Fuzzy programming; Robust two-stage stochastic programming; Textile industry	SUSTAINABLE SUPPLIER SELECTION; ORDER ALLOCATION; QUANTITY DISCOUNTS; OPTIMIZATION MODEL; MULTIPLE CRITERIA; ROUTING PROBLEM; CHAIN; UNCERTAINTY; DECISIONS; SEARCH	In this paper, we study the integrated inbound logistics decisions and multi-site aggregate production planning (APP) over the tactical planning horizon in a textile industry. In this regard, supplier selection, order allocation, inbound transportation logistics, and multi-site APP problems are addressed in a multi-period, multi-product and multiple transportation modes under uncertainty. To make procurement and production decisions simultaneously, a mixed-integer nonlinear mathematical programming model is proposed in a hybrid fuzzy-stochastic environment. To solve the proposed model, an efficient multi-stage algorithm is developed by re-formulating with a linearization scheme and employing a novel defuzzification process to cope with the proposed possibilistic programming. Then, a robust two-stage stochastic programming method is applied. Finally, the application of the model and the effectiveness of the solution method are examined through comprehensive numerical studies for the apparel industry. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106267	10.1016/j.asoc.2020.106267													
J								Multi-robot path planning using improved particle swarm optimization algorithm through novel evolutionary operators	APPLIED SOFT COMPUTING										IPSO-EOPs; Energy utilization; Path planning; Multiple mobile robots; Run time; Average untraveled trajectory target distance	ROBOTS; SYSTEM	The highlight of this paper is to propose an innovative approach to compute an optimal collision free trajectory path for each robot in a known and complex environment. The problem under consideration has been solved by employing an improved version of particle swarm optimization (IPSO) with evolutionary operators (EOPs). In the present context, PSO is improved with the concept of governance in human society and two evolutionary operators such as multi-crossover inherited from the genetic algorithm, and bee colony operator to enhance the intensification capability of the IPSO algorithm. The algorithm proposed to compute the deadlock free subsequent coordinate of an individual robot from their present coordinate, in addition, to minimize the path length for each robot by maintaining a good balance between intensification and diversification. Results obtained from the proposed IPSO-EOPs have been compared with competitors such as DE and IPSO in a similar environment to substantiate the robustness and usefulness of the algorithm. It perceives from the result obtained from simulation and experimentation that IPSO-EOPs is succeeding IPSO, and DE in terms of arrival time, generating a safe optimal path, and energy utilization during the travel. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106312	10.1016/j.asoc.2020.106312													
J								Improving the reliability of test functions generators	APPLIED SOFT COMPUTING										Benchmarking; Continuous optimization; Test function selection; Design of experiments; Statistical analysis; ANOVA	GLOBAL OPTIMIZATION	Computational intelligence methods have gained importance in several real-world domains such as process optimization, system identification, data mining, or statistical quality control. Tools are missing, which determine the performance of computational intelligence methods in these application domains in an objective manner. Statistics provide methods for comparing algorithms on certain data sets. In the past, several test suites were presented and considered as state of the art. However, there are several drawbacks of these test suites, namely: (i) problem instances are somehow artificial and have no direct link to real-world settings; (ii) since there is a fixed number of test instances, algorithms can be fitted or tuned to this specific and very limited set of test functions; (iii) statistical tools for comparisons of several algorithms on several test problem instances are relatively complex and not easily to analyze. We propose a methodology to overcome these difficulties. It is based on standard ideas from statistics: analysis of variance and its extension to mixed models. This paper combines essential ideas from two approaches: problem generation and statistical analysis of computer experiments. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106315	10.1016/j.asoc.2020.106315													
J								A two-stage fuzzy neural approach for credit risk assessment in a Brazilian credit card company	APPLIED SOFT COMPUTING										Predictive modeling; Credit scoring; Fuzzy inference system; Neural networks; Classification	NETWORKS; BANKING; SYSTEMS	This study explores and evaluates the use of soft computing systems for clients' credit risk assessment in a Brazilian private credit card provider through the development of an innovative two-stage process, both involving soft computing techniques (fuzzy and neural networks). We use commercially available credit score ratings both in the development of our method and for benchmarking. After describing the development of our method, we present a discussion about the comparison of performances of our method and a number of other credit scoring methods described in literature (for e.g. statistical and soft computing-based). One of the analyzed existing methods for instance involves the use of a soft computing algorithm only - Artificial Neural Networks (ANN) - for client classification into solvent or non-solvent, having a market available credit score rating as input. One of the most relevant contributions of this study however is the development of what we consider an innovative approach for credit scoring. This is a two-stage process that involves the use of a fuzzy inference model as input for an ANN model (what we call a fuzzy-neural approach), using commercially available credit score ratings as response in order to conduct the fuzzy reasoning step of the analysis. The main conclusion of our research is that, in general, our fuzzy-neural method had better results than the pure application of some market available score rating method as input to a Multi-Layer Perceptron (MLP) since it was able to reduce uncertainty by improving predictability and reducing variability of the outcomes when compared to a model with no scores. The performance of a combination of a fuzzy and a neural method was very satisfactory; the vagueness usually present in the information of a company' database was to a certain extent, incorporated by our method with good results. From the practical perspective, although our method has not proved to be substantially better than market available options, we demonstrated that it is possible for companies to develop credit score rating mechanisms internally based on past data by using fuzzy inference systems. Under certain circumstances, companies may find this option preferable than the usual option of paying high fees to large credit scoring agencies for the use of their proprietary systems. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106329	10.1016/j.asoc.2020.106329													
J								A multi-objective pharmaceutical supply chain network based on a robust fuzzy model: A comparison of meta-heuristics	APPLIED SOFT COMPUTING										Pharmaceutical supply chain network; Uncertainty; Robust fuzzy approach; Meta-heuristic algorithms	INVENTORY MANAGEMENT STRATEGIES; PROGRAMMING APPROACH; OPTIMIZATION; DESIGN; SUBSTITUTABILITY; UNCERTAINTY; ALGORITHMS; OPERATIONS; COMPANY; ISSUES	The pharmaceutical supply chain has features that distinguish it from other supply chains. Medicine is considered a strategic commodity, and the smallest disruption in its supply chain may cause severe crises. This is why the distribution of pharmaceutical products needs to combine the minimization of costs with strong compliance with service standards while taking into account risks due to uncertainty. In this study, we present a new multi-objective multi-echelon multi-product multi-period pharmaceutical supply chain network (PSCN) along with the production-distribution-purchasing-ordering-inventory holding-allocation-routing problem under uncertainty. We formulate the problem as a Mixed-Integer Non-Linear Programming model and develop a novel robust fuzzy programming method to cope with uncertainty parameters. To find optimal solutions, several multi-objective metaheuristic algorithms, namely, MOSEO, MOSAM MOKA, and MOFFA considering different criteria and multi-objective assessment metrics are suggested. Since there are no benchmarks existing in the literature, 10 numerical instances in large and small sizes are generated and also the trapezoidal fuzzy numbers of the uncertain parameters were randomly generated based on a uniform distribution. The required parameters were set and also the simulated data were examined in an exact method and by metaheuristic algorithms. The results confirm the efficiency of the MOFFA algorithm to detect a near-optimal solution within a logical CPU time. The solution methods are complemented with several sensitivity analyses on the input parameters of the proposed model. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106331	10.1016/j.asoc.2020.106331													
J								Highly interpretable hierarchical deep rule-based classifier	APPLIED SOFT COMPUTING										Deep rule-based; Hierarchical; Prototype-based; Self-organizing	FUZZY INFERENCE SYSTEM; SCENE CLASSIFICATION; NEURAL-NETWORKS; EVOLVING FUZZY; IMAGE; IDENTIFICATION; PERFORMANCE; FEATURES; MODELS	Pioneering the traditional fuzzy rule-based (FRB) systems, deep rule-based (DRB) classifiers are able to offer both human-level performance and transparent system structure on image classification problems by integrating zero-order fuzzy rule base with a multi-layer image-processing architecture that is typical for deep learning. Nonetheless, it is frequently observed that the inner structure of DRB can become over sophisticated and not interpretable for humans when applied to largescale, complex problems. To tackle the issue, one feasible solution is to construct a tree structural classification model by aggregating the possibly huge number of prototypes identified from data into a much smaller number of more descriptive and highly abstract ones. Therefore, in this paper, we present a novel hierarchical deep rule-based (H-DRB) approach that is capable of summarizing the less descriptive raw prototypes into highly generalized ones and self-arranging them into a hierarchical prototype-based structure according to their descriptive abilities. By doing so, H-DRB can offer high-level performance and, most importantly, full transparency and human-interpretability on various problems including large-scale ones. The proposed concept and generical principles are verified through numerical experiments based on a wide variety of popular benchmark image sets. Numerical results demonstrate that the promise of H-DRB. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106310	10.1016/j.asoc.2020.106310													
J								Simultaneous use of two normalization methods in decomposition-based multi-objective evolutionary algorithms	APPLIED SOFT COMPUTING										Evolutionary multi-objective optimization (EMO); Objective space normalization; MOEA/D; Decomposition-based algorithm; Many-objective optimization	MANY-OBJECTIVE OPTIMIZATION; NONDOMINATED SORTING APPROACH; MOEA/D; SELECTION	In real-world applications, the order of magnitude in each objective varies, whereas most of fitness evaluation methods in many-objective solvers are scaling dependent. Objective space normalization has a large effect on the performance of each algorithm (i.e., on the practical applicability of each algorithm to real-world problems). In order to put equal emphasis on each objective, a normalization mechanism is always encouraged to be employed in the framework of the algorithm. Decomposition-based algorithms have become more and more popular in many-objective optimization. MOEA/D is a representative decomposition-based algorithm. Recently, some negative effects of normalization have been reported, which may deteriorate the practical applicability of MOEA/D to real-world problems. In this paper, to remedy the performance deterioration introduced by normalization in MOEA/D, we propose an idea of using two types of normalization methods in MOEA/D simultaneously (denoted as MOEA/D-2N). The proposed idea is compared with the standard MOEA/D and MOEA/D with normalization (denoted as MOEA/D-N) via two widely-used test suites (as well as their variants) and a real-world optimization problem. Experimental results show that MOEA/D-2N can effectively evolve a more diverse set of solutions and achieve robust and comparable performance compared with the standard MOEA/D and MOEA/D-N. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106316	10.1016/j.asoc.2020.106316													
J								Soft computing models for predicting blast-induced air over-pressure: A novel artificial intelligence approach	APPLIED SOFT COMPUTING										Soft computing model; Meteorological conditions; Air over-pressure; Artificial intelligence; Hybrid model	SUPPORT VECTOR MACHINE; NEURAL-NETWORK; AIRBLAST-OVERPRESSURE; GAUSSIAN-PROCESSES; DECISION TREE; OPTIMIZATION; REGRESSION; SLOPE; PARAMETERS; DEPENDENCE	Applying soft computing models for solving real-life problems has yielded many significant benefits, especially in the mining industry. This study proposed a novel soft computing model for estimating blast-induced air over-pressure (AOp) with high accuracy. Accordingly, the boosted smoothing spline (BSTSM) and genetic algorithm (GA) were considered and combined, namely GA-BSTSM model. One hundred twenty-one blasts were collected at the Coc Sau open-pit coal mine (Vietnam) for this aim. The explosive capacity used (W) and distance (R) were considered as the primary input variables for the aiming of AOp prediction. Also, the meteorological conditions such as temperature (T), relative humidity (RH), atmospheric pressure (AP), wind speed (WS), and wind direction (WD) were taken into account to predict AOp in this study. To confirm the performance of the proposed GA-BSTSM model, an empirical model and six other artificial intelligence models were also developed to predict AOp based on the same dataset, including CART (classification and regression tree), KNN (k-nearest neighbors), ANN (artificial neural network), BRR (Bayesian ridge regression), SVR (support vector regression), and Gaussian process (GP). The developed models were then evaluated through three performance indices (i.e., RMSE, R-2, and VAF) using the testing dataset. In addition, a Taylor diagram was also developed to evaluate the quality of the models. The evaluation results showed that the proposed GA-BSTSM model yielded a robust performance with high accuracy for AOp prediction herein. The findings also disclosed that meteorological factors have a strong influence on the accuracy of AOp predictive models, especially RH and WS. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106292	10.1016/j.asoc.2020.106292													
J								A new wind power interval prediction approach based on reservoir computing and a quality-driven loss function	APPLIED SOFT COMPUTING										Interval prediction; Reservoir computing; Loss function; Wind power	NEURAL-NETWORK; SPEED; DECOMPOSITION; MODEL; CONSTRUCTION; OPTIMIZATION	Understanding the uncertainty of wind power forecasting is crucial for its practical application. This paper proposes a new forecasting approach to estimate the wind power prediction intervals (PIs) to quantify the prediction uncertainty. This approach integrates the reservoir computing methodology into a three-layer neural network architecture, and outputs the final PIs by minimizing a quality-driven loss function. The reservoir computing methodology can help study the nonlinear relationship implicit in data as well as accelerate computational time, while the quality-driven loss function is assumption-free and can help improve the forecasting capability of the proposed model. The proposed model is applied to real wind power data to test its effectiveness. Case studies show that for the data used in this paper, the proposed model can reduce the mean prediction interval width (MPIW) by up to 16.69%, reduce root mean square error (RMSE) by up to 7.36%, and save up to 5 times computation time compared to the benchmark models, these indicate that the proposed model has strong predictive capability. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106327	10.1016/j.asoc.2020.106327													
J								Enhancing tree-seed algorithm via feed-back mechanism for optimizing continuous problems	APPLIED SOFT COMPUTING										Optimization algorithm; Tree-seeds algorithm; TSA; Continuous problems	ENGINEERING OPTIMIZATION; MULTIOBJECTIVE OPTIMIZATION; DIFFERENTIAL EVOLUTION; GLOBAL OPTIMIZATION; HEURISTIC ALGORITHM; SWARM OPTIMIZATION; HARMONY SEARCH; GA ALGORITHM; DESIGN; PSO	Tree-Seed Algorithm (TSA) is a novel population-based random search algorithm with its advantages in continuous optimization problems. However, there are some problems in its searching procedure. Problem (1): its balance mechanism of exploration and exploitation is implemented with a constant ST, and this fixed value is unreasonable in the random search procedure; Problem (2): the seed generation mechanism is achieved randomly without considering different searching phases based on function evaluations. To overcome these two problems, the feedback mechanism should be enhanced. Firstly, the st_TSA is proposed to solve the Problem (1); secondly, the ns_TSA is proposed to further solve the Problem (2); finally, in order to inherit these feedback mechanisms, a novel fb_TSA has been proposed and verified by standard 30 test benchmark functions from IEEE CEC 2014 with the basic TSA and its variants, such as STSA. In addition, GWO, ABC, SCA, DE, PSO and CLPSO are adopted for some comparative experiments with different dimensions. The computational results demonstrate that the enhanced feedback mechanism on ST and ns parameters can improve the optimization capability of the basic TSA significantly, especially in global optimum. The applicability of the proposed fb_TSA is proved by the 4 real engineering problems when compared with TSA, SCA, ABC and PSO. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106314	10.1016/j.asoc.2020.106314													
J								Inbound tourism demand forecasting framework based on fuzzy time series and advanced optimization algorithm	APPLIED SOFT COMPUTING										Inbound tourism demand; Small sample forecasting; Fuzzy time series model; Information optimization technology; Atom search optimization algorithm	NEURAL-NETWORKS; MODEL; SYSTEM; ARRIVALS; COMBINATION; TERM	The tourism industry has been integrated into the national strategic system in China. Thus, tourism demand forecasting has become a concern for the sustainable development of the tourism industry. Unfortunately, the sample size for tourism in China is always small and cannot satisfy the hypothesis test of an economic model or the data volume for a traditional time series model. In this study, a novel hybrid forecasting framework combining fuzzy time series (FTS) and an atom search optimization (ASO) algorithm is proposed for inbound tourism demand forecasting; this forecasting framework is particularly suitable for small sample sizes. Specifically, information optimization technology is applied in the FTS to improve the recognition ability of the system and effectively identify small sample information. The ASO algorithm is applied to search the optimal parameters of FTS that can further improve forecasting performance. All comparison experiments and tests verify the effectiveness and superiority of our proposed model, which provides excellent forecasting results for tourism demand and a basis for policymakers and managers to plan appropriately for the tourism market. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106320	10.1016/j.asoc.2020.106320													
J								Acceleration harmonics estimation and elimination with MABC-RLS algorithm: Simulation and experimental analyses on shaking table	APPLIED SOFT COMPUTING										Signal processing; Acceleration harmonics; Estimation; Elimination; Modified artificial bee colony	ARTIFICIAL BEE COLONY; KALMAN FILTER; IDENTIFICATION; CANCELLATION; OPTIMIZATION	In this paper, to estimate and eliminate acceleration harmonics in shaking table system, a novel approach combining modified artificial bee colony (MABC) algorithm and recursive least square (RLS) algorithm is proposed. MABC algorithm is employed in amplitude and phase estimation of acceleration harmonics while RLS algorithm is used to eliminate harmonics from the acceleration signal. In order to analyse the performance of the proposed algorithm, simulation and experimental studies are realized and the results are compared with the well-known algorithm reported in literature. In addition, harmonic frequencies and numbers are increased and robustness of proposed approach is investigated. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106377	10.1016/j.asoc.2020.106377													
J								Multi-scale single image rain removal using a squeeze-and-excitation residual network	APPLIED SOFT COMPUTING										Rain removal; Single image; Multi-scale residual network		Rain adversely affects the performance of collaborative robots in outdoor applications. In machine vision, single image rain removal is an extremely difficult problem due to the disordered and irregular rain streaks in the image. Existing methods either fail to achieve satisfactory rain removal results or destroy image details. In this paper, we propose a novel multi-scale rain removal model to address these problems by decomposing images into base layers and detail layers. The proposed method adapts a two-branch squeeze-and-excitation residual network architecture that learns the basic structure and texture details of the corresponding clean image. By decomposing the image into multiple layers and merging these layers, the network can effectively remove rain streaks from an image to restore its structural information and details. Extensive experiments on synthetic and real datasets demonstrate that the proposed method significantly outperforms recent state-of-the-art algorithms in terms of both qualitative and quantitative measures. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106296	10.1016/j.asoc.2020.106296													
J								A fast surrogate-assisted particle swarm optimization algorithm for computationally expensive problems	APPLIED SOFT COMPUTING										Computationally expensive problems; Particle swarm optimization (PSO); Surrogate model; Uncertainty	GLOBAL OPTIMIZATION; ENSEMBLE; MODEL; APPROXIMATION; REGRESSION; HYBRID	Although many surrogate-assisted evolutionary algorithms (SAEAs) have been proposed to solve computationally expensive problems, they usually need to consume plenty of expensive evaluations to obtain an acceptable solution. In this paper, we proposed a fast surrogate-assisted particle swarm optimization (FSAPSO) algorithm to solve medium scaled computationally expensive problems through a small number of function evaluations (FEs). Two criteria are applied in tandem to select candidates for exact evaluations. The performance-based criterion is used to exploit the current global best and accelerate the convergence rate, while the uncertainty-based criterion is used to enhance the exploration of the algorithm. The distance-based uncertainty criterion in SAEAs does not consider the fitness landscape of different problems. Therefore, we developed a criterion to estimate uncertainty by considering the distance and fitness value information simultaneously. This criterion can make up for the disadvantage of the conventional distance-based uncertainty criterion by considering the fitness landscape of a problem. In addition, it can be applied in any surrogate-assisted evolutionary algorithm irrespective of the used surrogate model. Twenty-three benchmark functions widely adopted in the literature and a 10-dimension propeller design problem are used to test the proposed approach. Experimental results demonstrate the superiority of the proposed FSAPSO algorithm over seven state-of-the-art algorithms. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106303	10.1016/j.asoc.2020.106303													
J								Preference-driven Pareto front exploitation for bloat control in genetic programming	APPLIED SOFT COMPUTING										Preference; Pareto front; Multi-objective optimization; Bloat control; Genetic programming	PARSIMONY PRESSURE; ALGORITHM	As one of evolutionary algorithms (EAs), genetic programming (GP) has been applied in a wide range of areas, e.g. bioinformatics and robotics. Different from other EAs, GP can represent problems with variable length (e.g. trees), which makes it more flexible in evolving solutions, yet leads to a serious problem, bloat. It can cause evolving redundant parts and slowing down search. Multi-objective techniques are popularly used for reducing bloat in GP (termed as MOGP). Specifically, MOGP methods evolve trade-off solutions of all objectives, which constitute the so-called Pareto front. Then users select solutions on the front based on their preference for specific tasks. However, existing MOGP methods rarely consider users' preference during evolution, which wastes computation power and time to search for useless solutions and cannot generate fine-grained interested regions on the Pareto front. Therefore, this paper investigates introducing users' preference to guide multi-objective techniques to focus on the interested regions on Pareto front during evolution. Specifically, Pareto dominance is an important notion in multi-objective techniques for comparing two solutions. We design two preference-driven Pareto dominance mechanisms, scPd (static constraint Pareto dominance) and dcPd (dynamic constraint Pareto dominance), which are introduced in a base multi-objective technique and then are incorporated with GP respectively to form two new bloat control MOGP methods, i.e. scPd_MOGP and dcPd_MOGP. They are tested on benchmark symbolic regression tasks comparing with GP, two existing bloat control methods (i.e. a parsimony GP method (pGP) and a standard multi-objective GP method (sMOGP)), and four popularly-used symbolic regression methods. Results show that the proposed methods can reduce bloat in GP and outperform pGP in bloat control, and comparison with sMOGP shows that they can search front regions based on users' preference where the solutions have better functionality, yet relatively larger sizes. In addition, compared with four popularly-used symbolic regression methods, scPd_MOGP is generally better; while dcPd_MOGP achieves varied results, yet it performs better or similar to the reference methods on the majority of the given test functions. Moreover, comparison between the two proposed methods suggests that the constraint in the Pareto dominance of scPd_MOGP is more relaxed than that of dcPd_MOGP. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106254	10.1016/j.asoc.2020.106254													
J								A robust dynamic scheduling approach based on release time series forecasting for the steelmaking-continuous casting production	APPLIED SOFT COMPUTING										Scheduling; Dynamic scheduling; Robust optimization; Hybrid flow shop; Time series forecasting	CONSTRAINED PROGRAMMING APPROACH; MEMETIC ALGORITHM; HYBRID FLOWSHOP; LOCAL SEARCH; OPTIMIZATION; SHOP	In this work, the dynamic scheduling problem is investigated considering the uncertainty of the job release time in steelmaking-continuous casting production processes. In contrast to existing dynamic scheduling strategies, a novel robust dynamic scheduling approach based on release time series forecasting (RDSA_RTSF) is proposed. The proposed RDSA_RTSF consists of two stages, i.e., offline preparation and online robust dynamic scheduling. In the offline preparation stage, a release time series forecasting model is established using historical data, and the forecasting accuracy of the model is calculated. In the online robust dynamic scheduling stage, a chance constrained programming (CCP) model is built first for rescheduling based on the predicted release time series and the statistical information of the forecasting accuracy. A robust schedule is subsequently generated by using a Monte Carlo simulation and an evolutionary algorithm to solve the CCP model. The evolutionary algorithm is formulated by combining a genetic algorithm with a local search strategy based on the problem characteristics. Computational experiments based on real data from a steel plant in China show that the proposed RDSA_RTSF strategy performs better than classical approaches in obtaining robust schedule results under a dynamic environment with uncertain release times. (C) 2020 Published by Elsevier B.V.																	1568-4946	1872-9681				JUL	2020	92								106271	10.1016/j.asoc.2020.106271													
J								Deep belief network and linear perceptron based cognitive computing for collaborative robots	APPLIED SOFT COMPUTING										Collaborative robot; Cognitive computing; Deep belief network; Simulation; Multilayer perceptron	INTERNET; PREDICTION; EDGE	Objective: This paper is to analyze the performance of the control system of collaborative robots based on cognitive computing technology. Methods: This study combines cognitive computing and deep belief network algorithms with collaborative robots to construct a cognitive computing system model based on deep belief networks, which is applied to the control system of collaborative robots. Further, the simulation is used to compare and analyze the algorithm performance of deep belief network (DBN), multilayer perceptron (MLP) and the cognitive computing system model of deep belief network and linear perceptron (DBNLP) proposed in this study. Results: The results show that compared with the DBN and MLP algorithms, the DBNLP algorithm model has a significantly lower error rate in the number of repetitions of the training set, the number of hidden neurons, and the number of network layers. And the number of task backlog, the number of resources to be allocated and the time consumption are less, as well as the accuracy is high. After comparing and analyzing the changes in the estimated value of Ex (expected value), En (entropy value) and He (hyper entropy value), it is found that the estimated value of the DBNLP algorithm model is closer to the true value than that of the DBN and MLP algorithms. Conclusion: The application of the DBNLP algorithm model to collaborative robots can significantly improve its accuracy and safety, providing an experimental basis for the performance improvement of later collaborative robots. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106300	10.1016/j.asoc.2020.106300													
J								Hybrid FORM-Sampling simulation method for finding design point and importance vector in structural reliability	APPLIED SOFT COMPUTING										Reliability analysis; Importance sampling method; Standard deviation; Artificial neural network; Most probable point	ARTIFICIAL NEURAL-NETWORKS; PARAMETRIC SENSITIVITY; ROBUST; MODEL; PREDICTION; ALGORITHM	It is still a big challenge to calculate the structural reliability index. Although first order reliability method (FORM) is effective in calculating the reliability index, it encounters many obstacles due to the need for differentiation of the limit state function (LSF) and using an optimization method, particularly when the LSF is nonlinear and non-differentiable. On the other hand, although simulation methods do not suffer from none of these problems, they require a large number of random samples. Moreover, simulation methods can only calculate the failure probability (rho(f)) directly, and they are not capable of calculating the design point. In the present paper, a new hybrid FORM-sampling simulation algorithm has been proposed to calculate the reliability index, design point, and importance vector. The proposed algorithm is viable to analysis the structural reliability with few random samples by using superior capabilities of importance sampling and step-by-step correction of the standard deviation (SD) of variables associated with the sampling density function. Furthermore, the LSF has been well approximated using the artificial neural network (ANN), leading to a significant reduction in the computation time. The efficiency of the present algorithm is illustrated through some examples in comparison to conventional methods. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106313	10.1016/j.asoc.2020.106313													
J								A Novel Image Steganographic Method based on Integer Wavelet Transformation and Particle Swarm Optimization	APPLIED SOFT COMPUTING										Image steganography; Particle Swarm Optimization; Discrete wavelet transform; Integer Wavelet Transform; Data hiding in image; Secure communication	DATA HIDING SCHEME; ALGORITHM; SUBSTITUTION; CONVERGENCE; ENCRYPTION; VISIBILITY; STABILITY; ISSUES; ROBUST; MODEL	Image steganography is a technique of hiding secret data into a cover image and so as to prevent the intruders from accessing the secret data. The efficiency of image steganography techniques are usually evaluated based on perceptual transparency, payload capacity, security, temper resistance and computational costs. Though there has been significant improvement in related research over the decades, available steganographic techniques usually satisfy only a subset of these criteria. This paper presents a novel IWT (Integer Wavelet Transform) based steganography method using PSO (Particle Swarm Optimization) to find the optimal substitution matrix for converting secret data into their substituted forms. In the proposed method, optimal pixel adjustment process is used to improve perceptual transparency so that the obtained stego image has low distortion. The proposed method improves the security, imperceptibility, and robustness of the secret data by hiding them into the wavelet coefficients of an image. Thus, the paper provides a detailed study of the use of PSO in three different image steganographic methods based on (i) LSB (Least Significant Bit) substitution, (ii) DWT (Discrete Wavelet Transform), and (iii) IWT. Experiments are conducted using well-known benchmark images and results are comparatively analyzed. It is found that our proposed approach of PSO based IWT outperforms both PSO based LSB and PSO based DWT from the context of standard quality metrics, statistical analysis, security level estimation, payload capacity, imperceptibility etc. We have also provided a comparative overview of the existing data hiding methods including the proposed approach. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106257	10.1016/j.asoc.2020.106257													
J								Bayesian optimization algorithm for multi-objective scheduling of time and precedence constrained tasks in heterogeneous multiprocessor systems	APPLIED SOFT COMPUTING										Multiprocessor systems; Real-time systems; Task scheduling; Precedence constraints; Multi-objective Bayesian optimization algorithm	HYBRID GENETIC ALGORITHM	This paper presents a Bayesian optimization based novel approach for multi-objective scheduling of time and precedence constrained tasks in heterogeneous multiprocessing environments. The proposed approach, termed as multi-objective Bayesian optimization algorithm (moBOA) for real-time scheduling, can suitably produce optimal task schedules without any violation of the timing and precedence constraints. The moBOA utilizes Bayesian networks to learn the task graphs that represent the precedence relationships among the tasks. It first allocates tasks to individual processors and then decides the order of execution on each processor based on the latest deadline first policy. The proposed moBOA may be applied to both homogeneous and heterogeneous multiprocessor systems. Extensive comparative analysis has been made by considering two other existing evolutionary algorithms, namely the multi-objective genetic algorithm (moGA) and multi-objective hybrid genetic algorithm (mohGA) through experimental simulations with benchmark datasets. From the results of the simulation experiments, it is observed that moBOA outperforms both moGA and mohGA in terms of quality of solutions, Pareto-optimalilty and standard performance measures. We demonstrate that the task schedules produced by moBOA are optimal and comply with the timing as well as the precedence constraints. Statistical significant analyses of the results are conducted. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106274	10.1016/j.asoc.2020.106274													
J								Noise gradient strategy for an enhanced hybrid convolutional-recurrent deep network to control a self-driving vehicle	APPLIED SOFT COMPUTING										Hybrid convolutional-recurrent deep network; Noise gradient strategy; Self-driving cars		In this paper a noise gradient strategy on the Adam optimizer is introduced, in order to reduce the training time of our enhanced Chauffeur hybrid deep model. This neural network was modified to take into account the time dependence of the input visual information from a time-distributed convolution, with the aim of increasing the autonomy of a self-driving vehicle. The effectiveness of the proposed optimizer and model was evaluated and quantified during training and validation with a higher performance than the original Chauffeur model in combination with the comparative optimizers. In terms of the autonomy, it can be seen that our enhanced Hybrid Convolutional-Recurrent Deep Network was better trained, achieving autonomy greater than 95% with a minimum number of human interventions. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106258	10.1016/j.asoc.2020.106258													
J								Pareto optimality and game theory approach for optimal deployment of DG in radial distribution system to improve techno-economic benefits	APPLIED SOFT COMPUTING										Swarm intelligence algorithms; Improved raven roosting optimization; Distributed generation; Radial distribution system; Technical and economical issues; Game theory; Pareto optimality	PARTICLE SWARM OPTIMIZATION; VOLTAGE STABILITY; MULTIOBJECTIVE OPTIMIZATION; OPTIMAL PLACEMENT; GENERATION; ALGORITHM; LOCATION; DESIGN; HYBRID	This paper discusses the application of nature-inspired swarm intelligence methods for optimal allocation and sizing of distributed generation (DG) in the radial distribution system (RDS). Introducing DG units in the RDS will enhance the technical and economic benefits if they are optimally deployed. The objective functions considered are to improve the technical aspects and net economical saving cost with DG units integration on RDS. In this paper, a weighted multi-objective index considers a wide range of technical issues such as active and reactive power losses of the system, voltage profile, line loading, and the voltage stability, these are assumed as technical improvement aspects in the RDS. A recent optimization method, i.e. improved raven roosting optimization (IRRO) algorithm has been implemented for optimal deployment of DG in RDS. The state of the art of IRRO algorithm parameters will improve the ability for exploration and prevent premature convergence. Pareto optimality is used in making a set of the best solutions between two conflicting objectives considered, i.e. technical and economical aspects. The main contribution in this paper is to utilize a game theory based (minimax) algorithm in taking the best decision from a set of non-dominated solutions obtained by Pareto optimality criteria. IEEE 33-bus and 69-bus RDS's are considered as the test systems for verifying the effectiveness of the IRRO algorithm. A comparative analysis with other nature-inspired swarm optimization techniques such as particle swarm optimization (PSO), modified teaching learning based optimization (MTLBO), Jaya algorithm (JAYA), and grey wolf optimizer (GWO) is also presented in this work. The simulation results of IRRO are compared with similar existing papers. It is observed that the IRRO algorithm can produce better results for the considered multi-objective functions. The MATLAB software is employed for the purpose. The novelty of the paper lies in the use of Pareto optimal and game theory in obtaining better results to the problem of optimal deployment of DG in RDS to improve technical as well as economic benefits. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106234	10.1016/j.asoc.2020.106234													
J								Generalised fuzzy cognitive maps: Considering the time dynamics between a cause and an effect	APPLIED SOFT COMPUTING										Generalised fuzzy cognitive maps; Time lags; Complex systems; Time relations; Fuzzy logic; Recurrent neural networks	DIAGNOSIS; KNOWLEDGE; MODEL	Fuzzy Cognitive Maps (FCMs) have been used to quantitatively model the dynamics of complex systems and predict their behaviours. However, they are usually unable to address the issues arising from time lags between causes and effects. Accordingly, Generalised Fuzzy Cognitive Maps (GFCMs) have been introduced to overcome this problem. This article deals with a breed of GFCMs that addresses time lags between cause(s) and effect(s), demonstrated by a case-study that deals with the social, economic and technological consequences of heavy rainfall in Kampala, Uganda. The results show that the inclusion of time lags alters both, the final steady-state values of the social, economic and technological consequences of heavy rainfall and the time taken to stabilise. Thus, the inclusion of time lags increases the reliability of GFCMs as a means to quantitatively model the dynamics of complex systems. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106309	10.1016/j.asoc.2020.106309													
J								Improved metaheuristics for the two-dimensional strip packing problem	APPLIED SOFT COMPUTING										Packing; Metaheuristics	INTELLIGENT SEARCH ALGORITHM; BIN-PACKING; GENETIC ALGORITHM; PERFORMANCE BOUNDS	Given a fixed set of rectangular items and a single rectangular object of fixed width and unlimited height, the two-dimensional strip packing problem consists of packing all the items into the object in a non-overlapping manner, such that the resulting packing height is a minimum. Two improved strip packing metaheuristics are proposed in this paper. The first algorithm is a hybrid approach in which the method of simulated annealing is combined with a heuristic construction algorithm, while the second algorithm involves application of the method of simulated annealing directly in the space of completely defined packing layouts, without an encoding of solutions. These two algorithms are compared with a representative sample of metaheuristics from the literature in terms of solution quality achieved in the context of a large set of 1 718 benchmark instances, clustered into four sets of test problems, each with differing characteristics. It is found that the new algorithms indeed compare favourably with, and in some cases outperform, existing strip packing metaheuristics in the literature. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106268	10.1016/j.asoc.2020.106268													
J								An improved sine-cosine algorithm for simultaneous network reconfiguration and DG allocation in power distribution systems	APPLIED SOFT COMPUTING										sine-cosine algorithm; Levy flights; Network reconfiguration; Voltage stability; Distributed generator	LOSS MINIMIZATION; CUCKOO SEARCH; PARTICLE SWARM; PLACEMENT; GENERATION; MAXIMIZATION; ENHANCEMENT; INTEGRATION	This paper presents a recently proposed meta-heuristic sine-cosine algorithm combined with levy flights to reconfigure the distribution network with simultaneous allocation (placement and size) of multiple distributed generators (DGs). The algorithm is proposed to be adaptive with an exponentially decreasing conversion parameter and a self-controlled levy mutation in order to explore the solution space more efficiently during the course of iterations. The effectiveness of the algorithm is verified on 10 standard benchmark functions. Later, it is used to address the issues of a real combinatorial optimization, such as network reconfiguration (NR) in the presence of DGs. In order to enhance the effectiveness of the system, a multi-objective function is developed considering total active power loss and overall voltage stability of the network with suitable weights without violating the system limitations. To evaluate the objective function, a depth fast search integrated forward-backward sweep based load flow technique that is capable of managing any topological alterations owing to the NR and DG integration is developed. In order to demonstrate the efficiency of the system, four distinct cases of NR and DG installation are investigated. The proposed algorithm is contrasted with other well-known algorithms that exist in the literature, namely, harmony search algorithm (HSA), fireworks algorithm (FWA), genetic algorithm (GA), refined genetic algorithm (RGA) and firefly (FF) algorithm considering 33 and 69-bus distribution systems at three different load levels and its superiority is established. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106293	10.1016/j.asoc.2020.106293													
J								Online RBM: Growing Restricted Boltzmann Machine on the fly for unsupervised representation	APPLIED SOFT COMPUTING										Restricted Boltzmann Machine; Online learning; Unsupervised representation	ART CLASSIFICATION ALGORITHMS	In this work, we endeavor to investigate and propose a novel unsupervised online learning algorithm, namely the Online Restricted Boltzmann Machine (O-RBM). The O-RBM is able to construct and adapt the architecture of a Restricted Boltzmann Machine (RBM) artificial neural network, according to the statistics of the streaming input data. Specifically, for a training data that is not fully available at the onset of training, the proposed O-RBM begins with a single neuron in the hidden layer of the RBM, progressively adds and suitably adapts the network to account for the variations in streaming data distributions. Such an unsupervised learning helps to effectively model the probability distribution of the entire data stream, and generates robust features. We will demonstrate that such unsupervised representations can be used for discriminative classifications on a set of multi-category and binary classification problems for unstructured image and structured signal data sets, having varying degrees of class-imbalance. We first demonstrate the O-RBM algorithm and characterize the network evolution using the simple and conventional multi-class MNIST image dataset, aimed at recognizing hand-written digit. We then benchmark O-RBM performance to other machine learning, neural network and Class RBM techniques using a number of public non-stationary datasets. Finally, we study the performance of the O-RBM on a real-world problem involving predictive maintenance of an aircraft component using time series data. In all these studies, it is observed that the O-RBM converges to a stable, concise network architecture, wherein individual neurons are inherently discriminative to the class labels despite unsupervised training. It can be observed from the performance results that on an average O-RBM improves accuracy by 2.5%-3% over conventional offline batch learning techniques while requiring at least 24%-70% fewer neurons. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106278	10.1016/j.asoc.2020.106278													
J								Modeling and analysis of factors affecting repair effectiveness of network	APPLIED SOFT COMPUTING										Repairable systems; Repair Effectiveness; Repair quality factors; Kijima model; Imperfect repair; Bayesian networks; Repair Effectiveness Index	BAYESIAN NETWORKS; RENEWAL PROCESS; SYSTEMS; MAINTENANCE; RELIABILITY; SAFETY	Imperfect maintenance modeling and analysis of repairable systems involves an additional parameter called repair effectiveness index (REI) `q', along with shape and scale parameters in generalized renewal process (GRP), which is a measure of the quality of repair or repair effectiveness (RE). The quantitative measure of this parameter as proposed by Kijima through his virtual age models attracted enough attention and is extensively used by the researchers. But, RE could be dependent on many subjective factors and also requires qualitative analysis as well for better understanding of repair effects and performance. Quantitative assessment of RE is necessary but not sufficient to analyze it completely. The paper brings out the limitations of the quantitative assessment of RE and highlights the need for further examining it qualitatively. This paper conducts an extensive study with the help of field experts on selection and analysis of various factors affecting RE and proposes eleven primary factors and their sub-factors (total 55 factors/sub-factors) which affect it the most. After due selection of the factors and sub-factors, the paper then proposes a Bayesian network (BN) to model their dependency on each other and with RE. As a result, the proposed BN model provides the measurable effect of all the selected factors and sub factors on RE in percentage form. The results are demonstrated with the help of two examples inspired by practical industrial applications. The presented work could be extremely useful for industries in undertaking reliability improvement of repairable systems by analyzing their repair quality in detail. The proposed methodology can also be used as fundamental guidelines to develop basic understanding of the repair effectiveness and how it can be improved for a particular system leading to an overall improvement in the reliability of the system. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106261	10.1016/j.asoc.2020.106261													
J								Applying deep learning algorithms to enhance simulations of large-scale groundwater flow in IoTs	APPLIED SOFT COMPUTING										Deep learning; Numerical simulation; Groundwater; Internet of Things	MODELS; INTERNET; SYSTEM; THINGS	Deep learning for enhancing simulation IoTs groundwater flow is a good solution for gaining insights into the behavior of aquifer systems. In previous studies, corresponding results give a basis for the rational management of groundwater resources. The users generally require special skills or knowledge and massive observations in representing the field reality to perform the deep learning algorithms and simulations. To simplify the procedures for performing the numerical and large-scale groundwater flow simulations, we apply the deep learning algorithms which combine both the numerical groundwater model and large-scale IoTs, groundwater flow measuring equipment and various complex groundwater numerical models. The mechanism has the capability to show spatial distributions of in-situ data, analyze the spatial relationships of observed data, generate meshes, update users' databases with in-situ observed data, and create professional reports. According to the numerical simulation results, we revealed that the deep learning algorithms are high computational efficiency, and we can enhance precise variance estimations for large-scale groundwater flow problems. The findings help users to best apply the deep learning algorithms in an easier way, get more accurate simulation results, and manage the groundwater resources rationally. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106298	10.1016/j.asoc.2020.106298													
J								A novel harmony search algorithm and its application to data clustering	APPLIED SOFT COMPUTING										Harmony search; CEC 2017; Data clustering	PARTICLE SWARM OPTIMIZATION; KRILL HERD ALGORITHM; DIFFERENTIAL EVOLUTION; K-MEANS; IMPROVE; RELIABILITY; DESIGN; HYBRID; CONVERGENCE	This paper presents a variant of harmony search algorithm (HS), called best-worst-mean harmony search (BWM_HS). The main difference between the proposed algorithm and the canonical HS is that it employs a modified memory consideration procedure to utilize more efficiently the accumulated knowledge and experience in harmony memory (HM). To this aim, the random harmony selection scheme of this procedure is replaced with three novel pitch selection and production rules. These rules use the information of the current best and worst harmonies as well as the mean of all harmonies to guide the search process. To further utilize the valuable information of HM, two new harmonies are generated at each iteration where the better one will compete with the current worst harmony. The mean of all harmonies is always employed to produce a new harmony. On the other hand, each pitch of the second one is obtained by the rules that consider the information of the best and worst harmonies. These rules can present either explorative or exploitative search behaviors at different stages of search. Thus, a probabilistic self-adaptive selection scheme decides to choose between them to properly balance the exploration and exploitation abilities. The general performance of BWM_HS for solving optimization problems is evaluated against CEC 2017 benchmark functions and its results are compared with HS and eight state-of-the-art variants of HS. The comparison indicates that the performance of BWM_HS is better than or equal to the compared algorithms with respect to the accuracy, robustness, and convergence speed criteria. Moreover, the performance of BWM_HS in solving clustering problems is investigated by applying it for clustering several well-known benchmark datasets. The experimental results show that, in general, the BWM_HS outperforms other well-known algorithms in the literature and in particular, it significantly improves the statistical results for one dataset. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106273	10.1016/j.asoc.2020.106273													
J								A multi-objective fuzzy robust optimization approach for designing sustainable and reliable power systems under uncertainty	APPLIED SOFT COMPUTING										Power system; Sustainability; Reliability; Renewable energy; Robust optimization; Fuzzy multi-objective	POSSIBILISTIC PROGRAMMING APPROACH; SUPPLY CHAIN NETWORKS; RELIABILITY; MODEL; PLANT	A sustainable and reliable power system is extremely important to ensure the prosperity of a country and its society. Traditional power systems are facing serious environmental and social issues while renewable energy systems possess low reliability due to the intermittent nature of energy sources. This paper addresses the sustainable and reliable power system design problem in an uncertain environment by using an approach called multi-objective fuzzy robust programming. The proposed approach, which is an integration of robust programming and two main branches of fuzzy programming (possibilistic and flexible programming), solves the presented multi-objective problem by simultaneously improving both sustainability and reliability, as well as by capturing uncertain factors. The objective is to determine the optimal number, location, capacity, and technology of the generation units as well as the electricity generated and transmitted through the network while minimizing the sustainability and reliability costs of the system. The proposed model considers uncertainties in the demand, the intermittent nature of renewable energy resources, and cost parameters. A case study in Vietnam was conducted to demonstrate the efficacy and efficiency of the proposed model. Results show that the proposed model improves the total cost of the power system, including sustainability and reliability costs, by approximately 4.2% and reduces the computational time by 20% compared to the scenario-based stochastic programming approach. Our findings also show that due to risk disruption, the reliability cost of the power system increases to 56.72% when more electric power is generated. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106317	10.1016/j.asoc.2020.106317													
J								Semi-supervised image depth prediction with deep learning and binocular algorithms	APPLIED SOFT COMPUTING										Depth prediction; Convolution neural network; Semi-supervised learning		Combining the advantages and disadvantages of supervised learning and unsupervised learning strategies in convolution neural networks, this paper proposes a semi-supervised single-image depth prediction model based on binocular information and sparse laser data. The model improves the depth prediction accuracy by introducing sparse depth monitoring information, which provides a better convergence of the model with a local optimal solution. In the experiment, we validate the effectiveness of the model on the KITTI data set. Compared to the supervised algorithm, the root mean square error is reduced by 41.6% and, compared to the unsupervised algorithm, the root mean square error is reduced by 26.9%. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106272	10.1016/j.asoc.2020.106272													
J								Imprecise weighted extensions of random forests for classification and regression	APPLIED SOFT COMPUTING										Classification; Regression; Random forest; Decision tree; Imprecise; Dirichlet model; Linear-vacuous mixture; Confidence intervals; Quadratic programming	DECISION TREES; PROBABILITIES; ALGORITHM; MODEL	One of the main problems of using the random forests (RF) in classification and regression tasks is a lack of sufficient data which fall into certain leaves of trees in order to estimate the tree predicted values. To cope with this problem, robust imprecise classification and regression RF models, called the imprecise RF, are proposed. They are based on the following ideas. First, imprecision of the tree estimates is taken into account by means of imprecise statistical inference models and confidence interval models. Secondly, we introduce weights assigned to trees or to groups of trees, which are computed in order to correct the RF estimates under condition of imprecise tree predicted values. In fact, the weights can be regarded as a robust meta-learner controlling the imprecision of estimates. Special modifications of loss functions to compute optimal weights for the classification and regression tasks are proposed in order to simplify maximin optimization problems. As a result, simple linear and quadratic optimization problems are obtained, whose solution does not meet any difficulties. Various numerical examples with real datasets illustrate the proposed robust models and show outperforming results when datasets are rather small or noisy. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106324	10.1016/j.asoc.2020.106324													
J								Optimizing the DNA fragment assembly using metaheuristic-based overlap layout consensus approach	APPLIED SOFT COMPUTING										Metaheuristic; DNA fragment assembly; Hybrid genetic algorithm; Overlap layout consensus; Optimization	LOCAL SEARCH ALGORITHM	Nucleotide sequencing finds the exact order of nucleotides present in a DNA molecule. The correct DNA sequence is required to obtain the desired information about the complete genetic makeup of an organism. The DNA fragment assembly correctly combines the DNA information present in the form of fragments as a sequence. Reconstruction of the original DNA sequence from large fragments is a challenging task due to the limitations of the available technologies that reads the DNA sequence. Objective of the DNA fragment assembly is to find the correct order of the fragments which is further used in the generation of a consensus sequence that represents the original DNA sequence. Power Aware Local Search (PALS) algorithm proposed for the DNA fragment assembly is an efficient method that orders the fragments in a correct sequence by minimizing the number of contigs. This work presents a hybrid approach on the basis of Overlap Layout Consensus for the DNA fragment assembly, where Restarting and Recentering Genetic Algorithm (RRGA) with integrated PALS is utilized as an evolutionary operator. Quality of the current proposal is quantified using overlap scores and the number of contigs. This work is evaluated using 25 benchmark datasets with three types of experiments. The results are compared with four state-of-the-art methods for the same task, namely, Recentering-Restarting Genetic Algorithm variation for DNA fragment assembly, PALS, Genetic Algorithm, and Hybrid Genetic Algorithm. Results show better average performance of the proposed solution. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106256	10.1016/j.asoc.2020.106256													
J								Application of a genetic algorithm based model selection algorithm for identification of carbide-based hot metal desulfurization	APPLIED SOFT COMPUTING										Hot metal desulfurization; Mathematical modeling; Calcium carbide; Variable selection; Neural networks; Genetic algorithm	PREDICTION MODEL; OBJECTIVE FUNCTION; ANN MODELS; OPTIMIZATION; NETWORKS; KINETICS	Sulfur is considered as one of the main impurities in hot metal. Hot metal desulfurization is often carried out with pneumatic injection of a fine-grade desulfurization reagent using a submerged lance. The aim of this study was to develop a data-driven model for the process. The model selection algorithm carries out a simultaneous variable selection and optimization of number of hidden neurons with a combination of binary and integer coded Genetic Algorithm. The objective function applied in the search is repeated Leave-Multiple-Out cross-validation. The model considered is a feedforward neural network with a single hidden layer. In the inner loop of the algorithm, the computational load is reduced by making use of Extreme Learning Machine (ELM) architecture. The final model is trained using the Bayesian regularization. The results show that a well-generalizing data-driven model with good prediction performance can be repeatedly selected based on noisy industrial data with the help of a Genetic Algorithm, provided that the model is validated comprehensively with internal and external data sets. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106330	10.1016/j.asoc.2020.106330													
J								Transfer stacking from low-to high-fidelity: A surrogate-assisted bi-fidelity evolutionary algorithm	APPLIED SOFT COMPUTING										Bi-fidelity optimization; Transfer stacking; Surrogate; Evolutionary computation; Radial basis function network	BUILDING-BLOCKS; OPTIMIZATION; SIMULATIONS; KNOWLEDGE	Optimization of many real-world optimization problems relies on numerical simulations for function evaluations. In some cases, both high- and low-fidelity simulations are available, where the high fidelity evaluation is accurate but time-consuming, whereas the low-fidelity evaluation is less accurate but computationally cheap. To find an acceptable optimum within a limited budget, it is economical for evolutionary algorithms to use both high- and low-fidelity evaluations in a single optimization search. This paper proposes a novel surrogate-assisted evolutionary algorithm using the transfer stacking technique for bi-fidelity optimization. To this end, a radial basis function network is firstly built to approximate the high-fidelity fitness function as additional low-fidelity evaluation, then a surrogate model transferring the original and additional low-fidelity evaluations to the expensive high-fidelity evaluation is adapted to guide the search. The simulation results on a series of bi-fidelity optimization benchmark problems with resolution, stochastic, and instability errors and a beneficiation processes optimization problem show that the proposed algorithm is both effective and efficient for solving bi-fidelity optimization problems, when their low-fidelity evaluations have resolution and stochastic errors. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106276	10.1016/j.asoc.2020.106276													
J								Robust fuzzy c-means clustering algorithm with adaptive spatial & intensity constraint and membership linking for noise image segmentation	APPLIED SOFT COMPUTING										FCM; Noise image segmentation; Local information; Robustness	LOCAL INFORMATION; DATA SET; NUMBER	The fuzzy C-means (FCM) clustering method is proven to be an efficient method to segment images. However, the FCM method is not robustness and less accurate for noise images. In this paper, a modified FCM method named FCM_SICM for noise image segmentation is proposed. Firstly, fast bilateral filter is used to acquire local spatial & intensity information; secondly, absolute difference image between the original image and the bilateral filtered image is employed and the reciprocal of the difference image and the difference image itself constrain conventional FCM as well as the local spatial & intensity information respectively; finally, membership linking is achieved by summing all membership degrees calculated from previous iteration within every cluster in squared logarithmic form as the denominator of objective function. Experiments show that this proposed method achieves superior segmentation performance in terms of segmentation accuracy (SA), average intersection-overunion (mIoU), E-measure and number of iteration steps on mixed noise images compared with several state-of-the-art methods. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106318	10.1016/j.asoc.2020.106318													
J								Robust fusion for RGB-D tracking using CNN features	APPLIED SOFT COMPUTING										RGB-D tracking; Robust fusion; Hierarchical convolutional neural network; Correlation filter tracking	DEEP CONVOLUTIONAL NETWORKS; VISUAL TRACKING; OBJECT TRACKING; MODEL; TIME	Recently, RGB-D sensors have become popular. Many computer vision problems can be better dealt with depth data. It is a challenging problem to integrate depth data into a visual object tracker to address the problems such as scale change and occlusion. In this paper, we propose a robust fusion based RGB-D tracking method. Specifically, hierarchical convolutional neural network (CNN) features are first adopted to encode RGB and depth images separately. Next, target is tracked based on correlation filter tracking framework. Then the results of each CNN feature are localized according to the tracking results in a short period of time. Finally, the target is localized by jointly fusing the results of RGB and depth images. Model updating is finally carried out according to the differences between RGB and depth images. Experiments on the University of Birmingham RGB-D Tracking Benchmark (BTB) and the Princeton RGB-D Tracking Benchmark (PTB) achieve comparable results to state-of-the-art methods. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106302	10.1016/j.asoc.2020.106302													
J								Imbalanced sample fault diagnosis of rotating machinery using conditional variational auto-encoder generative adversarial network	APPLIED SOFT COMPUTING										Planetary gearbox fault diagnosis; Imbalanced sample dataset; Conditional variational generative; adversarial network; Sample generation; Adversarial learning	DEEP NEURAL-NETWORKS; INTELLIGENT DIAGNOSIS; PREDICTION	In many real applications of planetary gearbox fault diagnosis, the number of fault samples is much less than normal samples while fault samples are hard to collected in different working conditions, so many traditional diagnosis methods will get low accuracy. To solve this problem, a method based on conditional variational auto-encoder generative adversarial network (CVAE-GAN) is proposed for imbalanced fault diagnosis. Firstly, new method uses encoder network of conditional variational auto-encoder to obtain the distribution of fault samples, and then a large number of similar fault samples can be generated through decoder network. Secondly, the parameters of generator, discriminator and classifier may be continuously optimized using adversarial learning mechanism. Finally, the trained CVAE-GAN is applied for intelligent fault diagnosis of planetary gearbox. The experimental results show that CVAE-GAN can generate fault samples in different working conditions, which improve the fault diagnosis performance of planetary gearbox. The sample generating ability of CVAE-GAN is significantly higher than other methods in two cases of imbalanced dataset. (C) 2020 Published by Elsevier B.V.																	1568-4946	1872-9681				JUL	2020	92								106333	10.1016/j.asoc.2020.106333													
J								Selecting a diverse set of benchmark instances from a tunable model problem for black-box discrete optimization algorithms	APPLIED SOFT COMPUTING										Experimentation; Benchmarking; Optimization; Runtime behavior; Black-box optimization; Discrete optimization	FITNESS LANDSCAPES; EVOLUTIONARY OPTIMIZATION; PHASE-TRANSITION; NK; NEUTRALITY; PERFORMANCE; ONEMAX; MAXSAT	As the number of practical applications of discrete black-box metaheuristics is growing faster and faster, the benchmarking of these algorithms is rapidly gaining importance. While new algorithms are often introduced for specific problem domains, researchers are also interested in which general problem characteristics are hard for which type of algorithm. The W-Model is a benchmark function for discrete black-box optimization, which allows for the easy, fast, and reproducible generation of problem instances exhibiting characteristics such as ruggedness, deceptiveness, epistasis, and neutrality in a tunable way. We conduct the first large-scale study with the W-Model in its fixed-length singleobjective form, investigating 17 algorithm configurations (including Evolutionary Algorithms and local searches) and 8372 problem instances. We develop and apply a machine learning methodology to automatically discover several clusters of optimization process runtime behaviors as well as their reasons grounded in the algorithm and model parameters. Both a detailed statistical evaluation and our methodology confirm that the different model parameters allow us to generate problem instances of different hardness, but also find that the investigated algorithms struggle with different problem characteristics. With our methodology, we select a set of 19 diverse problem instances with which researchers can conduct a fast but still in-depth analysis of algorithm performance. The bestperforming algorithms in our experiment were Evolutionary Algorithms applying Frequency Fitness Assignment, which turned out to be robust over a wide range of problem settings and solved more instances than the other tested algorithms. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106269	10.1016/j.asoc.2020.106269													
J								A topology-based single-pool decomposition framework for large-scale global optimization	APPLIED SOFT COMPUTING										Large-scale global optimization; Problem decomposition; Cooperative coevolution; Topology information	DIFFERENTIAL EVOLUTION; COOPERATIVE COEVOLUTION; ALGORITHM; ENSEMBLE; STRATEGY	Identification of variable interaction plays a crucial role in applying a divide-and-conquer algorithm for large-scale black-box optimization. However, most of the existing decomposition methods are less efficient in decomposing the overlapping problems. This drawback diminishes the practicality of the existing methods. In this paper, we propose an efficient single-pool decomposition framework (SPDF). The interactions of decision variables are identified in an ordinal fashion. The unbalanced grouping efficiency of the existing decomposition methods can be significantly alleviated. Furthermore, we find that the grouping efficiency can be further improved by integrating the topological information into the decomposition process. In many real-world problems, this information can be 1-, 2- or 3-dimensional coordinates, which represent the geometric structure of the large-scale systems. Based on this, we propose a topology-based decomposition method, which we call Topology-based Single-Pool Differential Grouping (TSPDG). The efficacy of our proposed methods is demonstrated on the CEC'2010 and the CEC'2013 large-scale benchmark suites, as well as a practical case study in production optimization. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106295	10.1016/j.asoc.2020.106295													
J								Prediction of bank telephone marketing results based on improved whale algorithms optimizing S_Kohonen network	APPLIED SOFT COMPUTING										Whale optimization algorithm; Kohonen neural network; Supervised learning	OPTIMIZATION ALGORITHM	Time deposit has the characteristics of strong stability and low cost. It is a stable source of funds for banks. In this paper, S_Kohonen network is used to predict the success rate of fixed deposit in bank telephone marketing. Firstly, the output layer is added after the competition layer of unsupervised Kohonen network, which makes Kohonen network become S_Kohonen network with supervised learning. Because the improved S_Kohonen network is similar to other feedforward neural networks, each adjacent layer is connected by weights, and the initial weights are random, which easily leads to the unstable output of the network, and still has the disadvantage of relatively low prediction accuracy. Therefore, an improved whale optimization algorithm (IWOA) is proposed to optimize the weights between the input layer and the competition layer of S_Kohonen network. In this paper, the inertia weight of whale optimization algorithm is introduced into random factor on the basis of non-linear decline, and then the random search pattern of Levy flight is introduced into whale algorithm. Finally, the empirical results show that the improved S_Kohonen network can more intuitively represent the classification results of the network, and the classification accuracy of S_Kohonen network optimized by IWOA is significantly higher than that of S_Kohonen network optimized by GA, WOA and LWOA algorithm. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106259	10.1016/j.asoc.2020.106259													
J								RBFNN based terminal sliding mode adaptive control for electric ground vehicles after tire blowout on expressway	APPLIED SOFT COMPUTING										Lumped uncertainties; Saturated velocity planning; Terminal sliding mode control; Tire blowout; Radial basis function neural network	TRACKING CONTROL; PREDICTIVE CONTROL; STABILIZATION; MOTOR	This paper proposes a radial basis function neural network (RBFNN) based terminal sliding mode control scheme for electric ground vehicles subject to tire blowout on expressway in presence of tire nonlinearities, unmodeled dynamics and external disturbances. For enhancing the longitudinal and lateral stability of the vehicle after tire blowout, a saturated velocity planner is firstly constructed for tracking the original motion trajectory, by which the longitudinal velocity and yaw rate saturation constraints can be effectively handled. Afterwards, a terminal sliding mode controller (TSMC) is designed for tracking the planned velocity signals because of its inherent finite time convergence rate and superior steady-state property, by which the adverse dynamic behaviors can be timely suppressed. Further, to strengthen the adaptability and robustness of the control scheme, a RBFNN approximator is developed for identifying the lumped uncertainty, such as tire nonlinearities, unmodeled dynamics and external disturbances, etc., and then compensated into the controller. Lastly, simulations with front-right tire blowout on expressway are performed to validate the effectiveness and efficiency of presented control scheme and methods, and the comprehensive performance of TSMC+RBFNN and TSMC schemes in maintaining original trajectory tracking capacity is evaluated and discussed. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106304	10.1016/j.asoc.2020.106304													
J								A novel case adaptation method based on differential evolution algorithm for disaster emergency	APPLIED SOFT COMPUTING										Case-based reasoning; Differential evolution algorithm; Disaster emergency; Adaptation	LINGUISTIC TERM SETS; GLOBAL OPTIMIZATION; REASONING APPROACH; NEURAL-NETWORK; EXPERT-SYSTEM; CBR; INTELLIGENT; PREDICTION; MANAGEMENT; PERFORMANCE	When disasters happen, time is often very urgent. Case-based reasoning (CBR) is one of the most effective approaches to support disaster emergency management. CBR takes good use of historical case data, which is one of the typical data-driven decision-making methods. Among the steps of CBR, adaptation is the core. To improve the adaptation, a hybrid mutation operator is implemented, and a new differential evolution (DE) algorithm is developed. An adaptation method based on the proposed algorithm is put forward to achieve case adaptation in the CBR system. The comparison results have shown that the proposed algorithm is superior compared with the state-of-art algorithms. Then, experiments of CBR have revealed that the adaptation method can effectively generate appropriate solutions with the help of the proposed algorithm. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106306	10.1016/j.asoc.2020.106306													
J								Convolutional descriptors aggregation via cross-net for skin lesion recognition	APPLIED SOFT COMPUTING										Dermoscopy image; Melanoma recognition; Residual network; Cross-net model; Deep learning	IMAGE CLASSIFICATION; MALIGNANT-MELANOMA; ABCD RULE; COLOR; DERMATOSCOPY; FEATURES	Malignant melanoma is one of the rare but deadliest types of skin cancers. Clinically, the early diagnosis of this disease is based on human visual inspection with dermoscopy imaging. However, human observations are subjective and prone to errors due to huge variations within dermoscopy images. To address it, we propose a framework for automatic skin lesion recognition using cross-net based aggregation of multiple convolutional networks. The output activation maps of each network are extracted as indicator maps to select the local deep convolutional descriptors (i.e., local patterns and color) in dermoscopy images. Also, this map of a convolutional layer captures the semantic regions of the input image and localizes the target object. These selected features are aggregated into an informative feature map, which are potentially better preserved in the convolutional feature maps. Finally, we use Fisher vector (FV) to encode the selected features. Extensive experiments demonstrate the effectiveness of our proposed method. Comparing with aggregation strategy using pooling approaches, the proposed method learns more robust and discriminative representations based on the publicly available skin lesion challenge datasets from the International Symposium on Biomedical Imaging (ISBI) 2016 and 2017. (C) 2020 Published by Elsevier B.V.																	1568-4946	1872-9681				JUL	2020	92								106281	10.1016/j.asoc.2020.106281													
J								Load frequency regulation by de-loaded tidal turbine power plant units using fractional fuzzy based PID droop controller	APPLIED SOFT COMPUTING										Tidal energy; Fractional order fuzzy PID controller; Frequency regulation; De-loading operation; Imperialist competitive algorithm	AUTOMATIC-GENERATION CONTROL; SEARCH ALGORITHM; PERFORMANCE; OPTIMIZATION; CALCULUS; SYSTEMS; DESIGN	This study presents a model to analyse and simulate tidal power generation for hybrid power system in the presence of highly infiltrated tidal units. The response of the hybrid system may be at risk without suitable frequency enhancement techniques. The complete load frequency model is a combination of conventional automatic generation control (AGC) and automatic voltage regulator (AVR). AVR is employed to keep the output voltage magnitude of conventional generator at a particular level. The purpose of integrating AGC with AVR is to maintain the equilibrium between system's generation and load as well as to keep the frequency of system within suitable range. Owing to inclusion of nonconventional sources, the total inertia of the power system is eventually diminished. Conventional PID droop controller demonstrates inefficacy in diminishing the frequency deviations due to slow controlling action. This research proposes a fractional order (FO) fuzzy PID droop in de-loaded area to improvise the frequency excursion over fixed/Fuzzy PID/PID droop controllers. Imperialist competitive algorithm (ICA) is employed to tune the parameters of the controllers. ICA optimized fractional order PID droop control strategy unveils best performance (settling time = 11.65 s, undershoot amplitude = 0.26pu, performance index = 0.072e-6) over the integer order fuzzy PID control (settling time = 12.02 s, undershoot amplitude = 0.278 pu, performance index = 0.189e-6) and PID droop control (settling time = 30.68 s, undershoot amplitude = 0.95 pu, performance index = 0.215e-6). Examination of dynamic responses for abrupt changes in load request divulges the pre-eminence of proposed droop controller strategy with others controllers. The comprehensive study carried out in this paper implies that ICA optimized controller operates properly and has proven its robustness for +/- 10% variations in system parameters and physical constraints (Generation rate constraints, governor dead band, and time delay). This study also analyses the effect of tidal power plant contribution in the frequency regulation by inertia, primary and secondary frequency control. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106338	10.1016/j.asoc.2020.106338													
J								Managing individual evaluator's personalized semantic environment of linguistic term with improved vector expression in multi-granularity linguistic group decision making	APPLIED SOFT COMPUTING										Linguistic decision making; Group decision analysis; Improved vector expression; Semantic environment of linguistic term; Multi-granularity linguistic group decision making	FUZZY ONTOLOGIES; CONSENSUS; HIERARCHY; MODELS; SCALE; DEAL; SETS	Group linguistic assessment with the vector symbolic of linguistic evaluation information has been recently proposed for qualitative group decision making. Due to various individualized characteristics and knowledge levels, evaluators in group assessment often provide linguistic terms based on different individual linguistic evaluation scales to express their preferences on alternatives. In some situations, decision maker needs to distinguish different meanings of the same linguistic term in different individual evaluators' understandings. To further develop the resolution and the operational performance of the vector expression of linguistic term in multi-granularity linguistic group decision making (MGLGDM), in this study, we present the concept of improved vector expression of linguistic term. First, we present a method of rewriting the numerical symbolic of linguistic term into the improved vector expression based on the individual linguistic evaluation scale. Based on this, we introduce an approach to compare individual linguistic evaluation scales in MGLGDM. Then, an algorithm with improved vector expression is proposed for ranking alternatives in MGLGDM. Finally, a case illustration and some comparative studies have shown that the new proposed algorithm with improved vector expression of linguistic term is accurate and efficient in distinguishing and computing linguistic evaluation information in MGLGDM. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106334	10.1016/j.asoc.2020.106334													
J								Intelligent optic disc segmentation using improved particle swarm optimization and evolving ensemble models	APPLIED SOFT COMPUTING										Image segmentation; Particle swarm optimization; Evolutionary algorithm; Convolutional neural network and ensemble segmentation model	FIREFLY ALGORITHM; REGRESSION; LOCALIZATION	In this research, we propose Particle Swarm Optimization (PSO)-enhanced ensemble deep neural networks for optic disc (OD) segmentation using retinal images. An improved PSO algorithm with six search mechanisms to diversify the search process is introduced. It consists of an accelerated super-ellipse action, a refined super-ellipse operation, a modified PSO operation, a random leader-based search operation, an average leader-based search operation and a spherical random walk mechanism for swarm leader enhancement. Owing to the superior segmentation capabilities of Mask R-CNN, transfer learning with a PSO-based hyper-parameter identification method is employed to generate the fine-tuned segmenters for OD segmentation. Specifically, we optimize the learning parameters, which include the learning rate and momentum of the transfer learning process, using the proposed PSO algorithm. To overcome the bias of single networks, an ensemble segmentation model is constructed. It incorporates the results of distinctive base segmenters using a pixel-level majority voting mechanism to generate the final segmentation outcome. The proposed ensemble network is evaluated using the Messidor and Drions data sets and is found to significantly outperform other deep ensemble networks and hybrid ensemble clustering models that are incorporated with both the original and state-of-the-art PSO variants. Additionally, the proposed method statistically outperforms existing studies on OD segmentation and other search methods for solving diverse unimodal and multimodal benchmark optimization functions and the detection of Diabetic Macular Edema. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106328	10.1016/j.asoc.2020.106328													
J								An optimal service selection approach for service-oriented business collaboration using crowd-based cooperative computing	APPLIED SOFT COMPUTING										Service-oriented computing; Crowd-based cooperative computing; Crowd behaviors; Service selection; Particle swarm optimization	PARTICLE SWARM OPTIMIZATION; GENETIC ALGORITHM; QOS; INTELLIGENCE; INTERNET	Crowd-based cooperative computing (CBCC) emerges as a new computing paradigm, the core issue of which is the effective management and the coordinated use of crowd resources, including Internet users, application services, and smart devices. The service-oriented architecture (SOA) provides interoperability among crowd resources to support service-oriented business collaboration (SOBC). To address such a common issue of the coordinated use of crowd resources for SOBC, this paper studies a collaborative service computing model by considering the competition and cooperation among crowd resources. Then, a multi-objective optimization mathematical model is established for optimal service selection (OSS). Specifically, the methodology is resorted to an improved particle swarm optimization (IPSO) algorithm to find suitable collaborative services that optimally balance the quality of service (QoS) and synergy effect (SE). Furthermore, a flexible rescheduling strategy is presented for faulty services. The experimental results show that the proposed methodology is effective and feasible to obtain better-quality solutions for fulfilling the SOBC. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106270	10.1016/j.asoc.2020.106270													
J								The automatic segmentation of residential solar panels based on satellite images: A cross learning driven U-Net method	APPLIED SOFT COMPUTING										Data mining; Satellite images; Solar panels; Neural networks; Computational intelligence	FAULT-DETECTION; CLASSIFICATION; EXTRACTION; PROGRESS; SYSTEMS	Segmenting small-scale residential solar panels (RSPs) based on satellite images is an emerging data science problem in the renewable energy field. In this paper, we develop a cross learning driven U-Net (CrossNets) method and its extension, adaptive CrossNets, to automatically segment RSPs in satellite images. Proposed methods employ a group of generic U-Nets as a community and target to enhance the RSP segmentation performance. First, parameters of each generic U-Net in the community of CrossNets are initialized individually via the initialization with transfer learning and the classical initialization methods. Next, a novel training mechanism, cross learning, is developed to serve as a constraint for better optimizing CrossNets. Based on cross learning, each generic U-Net in the community first individually updates parameters at every epoch and next learns parameters from the best individual at specific epochs. Cross learning relieves the reliance of generic U-Nets on a careful initialization and better optimizes U-Nets. In testing, the result of the best performed generic U-Net in the community is selected as the final segmentation result of CrossNets. Adaptive CrossNets, a variant of CrossNets, is developed by applying an additional threshold to reduce the possibility of over-learning caused by cross learning. Satellite images collected from one city in U.S. are utilized to validate the performance of proposed methods. These images cover a large area of 135 km(2) with 2794 RSPs. Compared with two generic U-Nets based benchmarks, our method can enhance the overall segmentation IoU by around 34% and 1.5%. Moreover, the segmentation robustness is improved from 1.191e-2 and 1.286e-4 to 2.481e-5. In addition, two new image datasets collected from other two cities in U.S. are applied to further examine the applicability of proposed methods. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				JUL	2020	92								106283	10.1016/j.asoc.2020.106283													
J								Interval-valued fuzzy reasoning full implication algorithms based on the t-representable t-norm	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Interval-valued fuzzy sets; t-representable t-norm; Triple I algorithms; Robustness	ROBUSTNESS; OPERATORS; SYSTEMS; DESIGN	Interval-valued fuzzy reasoning plays a vital role in intelligent systems with the performance of effectively reducing the loss of fuzzy information and reflecting the vagueness and uncertainty in information processing. However the existing reasoning algorithms were developed based on some special interval-valued t-norms which limits the usability and adaptation of these algorithms. This study proposes general reasoning algorithms on the basis of interval-valued fuzzy sets, that is the interval-valued fuzzy reasoning triple I algorithms based on the left-continuous t-representable t-norm T-T1,T-T2. Furthermore, the interval-valued R.-type triple I solutions of the interval-valued fuzzy reasoning triple I algorithms are given. We show that the proposed algorithms possess the reducibility. Finally, some robustness results of the interval-valued fuzzy reasoning triple I algorithms based on the left-continuous interval-valued t-representable t-norm are proved. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				JUL	2020	122						1	8		10.1016/j.ijar.2020.03.009													
J								Fubini theorem and generalized Minkowski inequality for the pseudo-integral	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Nonadditive measure; Pseudo-analysis; Pseudo-integral; Fubini theorem; Generalized Minkowski inequality	DECOMPOSABLE MEASURES; PRODUCT; CHOQUET	This paper is devoted to generalizations of Fubini theorem, Transformation theorem, and generalized Minkowski inequality for the so called pseudo-integral. The approach is based on the relation of double pseudo-integrals and iterated pseudo-integrals. Since the pseudo-integral covers Lebesgue integral, Sugeno's fuzzy integral, and Zhao's (N)fuzzy integral (with a respect to special nonadditive measure: v-measure), the obtained results generalize the corresponding previously known results. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				JUL	2020	122						9	23		10.1016/j.ijar.2020.03.010													
J								On the quantification and efficient propagation of imprecise probabilities with copula dependence	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Imprecise probability; Uncertainty quantification; Copula; Bayesian inference; Small data; Multimodel inference	BAYESIAN MODEL SELECTION; VINE-COPULA; COMPOSITE-MATERIALS; HIGH DIMENSIONS; UNCERTAINTY; DISTRIBUTIONS; PREDICTION; DESIGN; SYSTEM; SETS	This paper addresses the problem of quantification and propagation of uncertainties associated with dependence modeling when data for characterizing probability models are limited. Practically, the system inputs are often assumed to be mutually independent or correlated by a multivariate Gaussian distribution. However, this subjective assumption may introduce bias in the response estimate if the real dependence structure deviates from this assumption. In this work, we overcome this limitation by introducing a flexible copula dependence model to capture complex dependencies. A hierarchical Bayesian multimodel approach is proposed to quantify uncertainty in dependence model-form and model parameters that result from small data sets. This approach begins by identifying, through Bayesian multimodel inference, a set of candidate marginal models and their corresponding model probabilities, and then estimating the uncertainty in the copula-based dependence structure, which is conditional on the marginals and their parameters. The overall uncertainties integrating marginals and copulas are probabilistically represented by an ensemble of multivariate candidate densities. A novel importance sampling reweighting approach is proposed to efficiently propagate the overall uncertainties through a computational model. Through an example studying the influence of constituent properties on the out-of-plane properties of transversely isotropic E-glass fiber composites, we show that the composite property with copula-based dependence model converges to the true estimate as data set size increases, while an independence or arbitrary Gaussian correlation assumption leads to a biased estimate. (C) 2020 Published by Elsevier Inc.																	0888-613X	1873-4731				JUL	2020	122						24	46		10.1016/j.ijar.2020.04.002													
J								Multi-granulation method for information fusion in multi-source decision information system	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Multi-source decision information system; Decision support characteristic function; Decision related characteristic function; Fixed aggregation operator; Possible aggregation operator	MULTIGRANULATION ROUGH SETS; MODEL; REDUCTION	Most of the existing multi-source fusion methods are to choose the most reliable information from the multi-source information system to form a single-source information system. Obviously, this process is accompanied by information loss. In order to solve this problem, the multi-granulation method of information fusion in multi-source decision information system is studied in this paper. Firstly, decision support characteristic function and decision related characteristic function are constructed. Secondly, a pair of aggregation operators, including fixed aggregation operator and possible aggregation operator, is defined through two characteristic functions. Meanwhile, the two cases when thresholds alpha and beta take special values are discussed. Finally, the relevant properties of aggregation operators in different situations are proposed and proved. What is more, two groups of comparative experiments are carried out to illustrate the effect of the aggregation operators. The experimental results show that the proposed multi-source fusion method can always find a set of thresholds (alpha, beta), which makes the fusion effect better than the mean fusion. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				JUL	2020	122						47	65		10.1016/j.ijar.2020.04.003													
J								Probabilistic stability analyses of slope reinforced with piles in spatially variable soils	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Probabilistic assessment; Soil slope; Stabilizing pile; RLEM; Spatial variability; Convergence	RELIABILITY-BASED DESIGN; RESPONSE-SURFACE METHODS; SLIP SURFACES; VARIABILITY; PARAMETERS; REGRESSION; FAILURE; TUNNEL	In this paper, deterministic stability analyses for soil slopes reinforced with pile of different locations and lengths were conducted firstly to calculate the factor of safety through limit equilibrium method (LEM). Since the effect of inherent uncertainties as well as the spatial variability of soils cannot be reflected via the deterministic factor of safety, probabilistic stability analyses of slopes reinforced with pile in spatially constant soils and spatially variable soils were successively carried out, respectively. The failure probability was determined by random limit equilibrium method (RLEM) considering the influence of different pile locations, pile length and soil statistical parameters. The optimal locations along slope and length of pile were analyzed based on the reliability analyses results in spatially constant soils and spatially variable soils, which were inconsistent with the results via the traditional deterministic methods, with a lower probability of sliding failure. Finally, method was proposed to determine the minimum samples of simulation iterations for investigating the convergence of failure probability of soil slope reinforced with piles. The results revealed that the multiple potential sliding surfaces introduced by the installations of pile would result in the increased uncertainty of slope failures. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				JUL	2020	122						66	79		10.1016/j.ijar.2020.04.006													
J								EVOLUTIONARY ALGORITHM WITH A CONFIGURABLE SEARCH MECHANISM	JOURNAL OF ARTIFICIAL INTELLIGENCE AND SOFT COMPUTING RESEARCH										evolutionary algorithm; population-based algorithm; optimization; operator pool; operator selection; individual selection	OPTIMIZATION ALGORITHM; DIFFERENTIAL EVOLUTION; PSO; DISPATCH; MODEL	In this paper, we propose a new population-based evolutionary algorithm that automatically configures the used search mechanism during its operation, which consists in choosing for each individual of the population a single evolutionary operator from the pool. The pool of operators comes from various evolutionary algorithms. With this idea, a flexible balance between exploration and exploitation of the problem domain can be achieved. The approach proposed in this paper might offer an inspirational alternative in creating evolutionary algorithms and their modifications. Moreover, different strategies for mutating those parts of individuals that encode the used search operators are also taken into account. The effectiveness of the proposed algorithm has been tested using typical benchmarks used to test evolutionary algorithms.																	2083-2567	2449-6499				JUL	2020	10	3					151	171		10.2478/jaiscr-2020-0011													
J								AN ALGORITHM FOR THE EVOLUTIONARY-FUZZY GENERATION OF ON-LINE SIGNATURE HYBRID DESCRIPTORS	JOURNAL OF ARTIFICIAL INTELLIGENCE AND SOFT COMPUTING RESEARCH										biometrics; on-line signature; dynamic signature; dynamic signature verification; evolutionary-fuzzy signature partitioning; horizontal and vertical partitioning	DIFFERENTIAL EVOLUTION; VERIFICATION; SYSTEMS; ANFIS	In biometrics, methods which are able to precisely adapt to the biometric features of users are much sought after. They use various methods of artificial intelligence, in particular methods from the group of soft computing. In this paper, we focus on on-line signature verification. Such signatures are complex objects described not only by the shape but also by the dynamics of the signing process. In standard devices used for signature acquisition (with an LCD touch screen) this dynamics may include pen velocity, but sometimes other types of signals are also available, e.g. pen pressure on the screen surface (e.g. in graphic tablets), the angle between the pen and the screen surface, etc. The precision of the on-line signature dynamics processing has been a motivational springboard for developing methods that use signature partitioning. Partitioning uses a well-known principle of decomposing the problem into smaller ones. In this paper, we propose a new partitioning algorithm that uses capabilities of the algorithms based on populations and fuzzy systems. Evolutionary-fuzzy partitioning eliminates the need to average dynamic waveforms in created partitions because it replaces them. Evolutionary separation of partitions results in a better matching of partitions with reference signatures, eliminates disproportions between the number of points describing dynamics in partitions, eliminates the impact of random values, separates partitions related to the signing stage and its dynamics (e.g. high and low velocity of signing, where high and low are imprecise-fuzzy concepts). The operation of the presented algorithm has been tested using the well-known BioSecure DS2 database of real dynamic signatures.																	2083-2567	2449-6499				JUL	2020	10	3					173	187		10.2478/jaiscr-2020-0012													
J								MULTI AGENT DEEP LEARNING WITH COOPERATIVE COMMUNICATION	JOURNAL OF ARTIFICIAL INTELLIGENCE AND SOFT COMPUTING RESEARCH										multi-agent systems; deep reinforcement learning; centralized learning	ALGORITHMS	We consider the problem of multi agents cooperating in a partially-observable environment. Agents must learn to coordinate and share relevant information to solve the tasks successfully. This article describes Asynchronous Advantage Actor-Critic with Communication (A3C2), an end-to-end differentiable approach where agents learn policies and communication protocols simultaneously. A3C2 uses a centralized learning, distributed execution paradigm, supports independent agents, dynamic team sizes, partially-observable environments, and noisy communications. We compare and show that A3C2 outperforms other state-of-the-art proposals in multiple environments.																	2083-2567	2449-6499				JUL	2020	10	3					189	207		10.2478/jaiscr-2020-0013													
J								A NEW METHOD FOR AUTOMATIC DETERMINING OF THE DBSCAN PARAMETERS	JOURNAL OF ARTIFICIAL INTELLIGENCE AND SOFT COMPUTING RESEARCH										clustering algorithms; DBSCAN; data mining	ALGORITHM; INDEX	Clustering is an attractive technique used in many fields in order to deal with large scale data. Many clustering algorithms have been proposed so far. The most popular algorithms include density-based approaches. These kinds of algorithms can identify clusters of arbitrary shapes in datasets. The most common of them is the Density-Based Spatial Clustering of Applications with Noise (DBSCAN). The original DBSCAN algorithm has been widely applied in various applications and has many different modifications. However, there is a fundamental issue of the right choice of its two input parameters, i.e the eps radius and the MinPts density threshold. The choice of these parameters is especially difficult when the density variation within clusters is significant. In this paper, a new method that determines the right values of the parameters for different kinds of clusters is proposed. This method uses detection of sharp distance increases generated by a function which computes a distance between each element of a dataset and its k-th nearest neighbor. Experimental results have been obtained for several different datasets and they confirm a very good performance of the newly proposed method.																	2083-2567	2449-6499				JUL	2020	10	3					209	221		10.2478/jaiscr-2020-0014													
J								DETECTING VISUAL OBJECTS BY EDGE CRAWLING	JOURNAL OF ARTIFICIAL INTELLIGENCE AND SOFT COMPUTING RESEARCH										content-based image retrieval; crawler; edge detection; image descriptor; object extraction	IMAGE RETRIEVAL; DESCRIPTOR; COLOR; ENHANCEMENT; FEATURES; SURF	Content-based image retrieval methods develop rapidly with a growing scale of image repositories. They are usually based on comparing and indexing some image features. We developed a new algorithm for finding objects in images by traversing their edges. Moreover, we describe the objects by histograms of local features and angles. We use such a description to retrieve similar images fast. We performed extensive experiments on three established image datasets proving the effectiveness of the proposed method.																	2083-2567	2449-6499				JUL	2020	10	3					223	237		10.2478/jaiscr-2020-0015													
J								A tight upper bound on the generalization error of feedforward neural networks	NEURAL NETWORKS										Feedforward neural networks; Generalization error	VC-DIMENSION; STABILITY	We give a tight upper bound on the generalization error of 2-times continuously differentiable feedforward neural networks if the loss function is 2-times continuously differentiable as well. The upper bound consists of two terms, the first term indicates how well the empirical error estimates the error at the mean of the sample space. The second term indicates the expected sensitivity of the error to the changes of the input. Furthermore, we provide explicit formulas for the calculation of the second term. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				JUL	2020	127						1	6		10.1016/j.neunet.2020.04.001													
J								Finite-time synchronization of memristor neural networks via interval matrix method	NEURAL NETWORKS										Nonlinear feedback controllers; Memristor neural networks; Finite-time synchronization; Interval matrix method	EXPONENTIAL SYNCHRONIZATION; DISCRETE	In this paper, the finite-time synchronization problems of two types of driven-response memristor neural networks (MNNs) without time-delay and with time-varying delays are investigated via interval matrix method, respectively. Based on interval matrix transformation, the driven-response MNNs are transformed into a kind of system with interval parameters, which is different from the previous research approaches. Several sufficient conditions in terms of linear matrix inequalities (LMIs) are driven to guarantee finite-time synchronization for MNNs. Correspondingly, two types of nonlinear feedback controllers are designed. Meanwhile, the upper-bounded of the settling time functions are estimated. Finally, two numerical examples with simulations are given to illustrate the correctness of the theoretical results and the effectiveness of the proposed controllers. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				JUL	2020	127						7	18		10.1016/j.neunet.2020.04.003													
J								Generative adversarial networks with decoder-encoder output noises	NEURAL NETWORKS										Image generation; Generative models; Generative adversarial networks; Variational autoencoders; Noise	DIMENSIONALITY	In recent years, research on image generation has been developing very fast. The generative adversarial network (GAN) emerges as a promising framework, which uses adversarial training to improve the generative ability of its generator. However, since GAN and most of its variants use randomly sampled noises as the input of their generators, they have to learn a mapping function from a whole random distribution to the image manifold. As the structures of the random distribution and the image manifold are generally different, this results in GAN and its variants difficult to train and converge. In this paper, we propose a novel deep model called generative adversarial networks with decoder-encoder output noises (DE-GANs), which take advantage of both the adversarial training and the variational Bayesian inference to improve GAN and its variants on image generation performances. DE-GANs use a pre-trained decoder-encoder architecture to map the random noise vectors to informative ones and feed them to the generator of the adversarial networks. Since the decoder-encoder architecture is trained with the same data set as the generator, its output vectors, as the inputs of the generator, could carry the intrinsic distribution information of the training images, which greatly improves the learnability of the generator and the quality of the generated images. Extensive experiments demonstrate the effectiveness of the proposed model, DE-GANs. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				JUL	2020	127						19	28		10.1016/j.neunet.2020.04.005													
J								Randomized sketches for kernel CCA	NEURAL NETWORKS										Canonical correlation analysis; Covariance/cross-covariance operator; Kernel method; Random projection	CANONICAL CORRELATION-ANALYSIS; DIVIDE-AND-CONQUER; ALGORITHM; RATES	Kernel canonical correlation analysis (KCCA) is a popular tool as a nonlinear extension of canonical correlation analysis. Consistency and optimal convergence rate have been established in the literature. However, the time complexity of KCCA scales as O(n(3)) and is thus prohibitive when n is large. We propose an m-dimensional randomized sketches approach for KCCA with m << n, based on the recent work on randomized sketches for kernel ridge regression (KRR). Technically we establish our theoretical results relying on an interesting connection between KCCA and KRR by utilizing a novel "duality tracking" device that alternates between the infinite-dimensional operator-theory-based view of KCCA and the finite-dimensional kernel-matrix-based view. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				JUL	2020	127						29	37		10.1016/j.neunet.2020.04.006													
J								Synchronization of coupled neural networks under mixed impulsive effects: A novel delay inequality approach	NEURAL NETWORKS										Mixed impulses; Coupled neural network; Delayed impulsive inequality; Average delay impulsive gain; Exponential synchronization	EXPONENTIAL STABILITY; SYSTEMS; STABILIZATION	In this paper, the synchronization problems of an array of coupled neural networks with mixed impulses are considered. Here mixed impulses contain desynchronizing delay-free impulses, synchronizing delay-free impulses, desynchronizing delayed impulses and synchronizing delayed impulses. A novel concept named average delayed impulsive gain is proposed to quantify the effects of mixed impulses. Besides, we establish a delayed impulsive differential inequality which extends famous Halanay inequality, and apply it to study the synchronization problems of delayed neural networks with mixed impulses. It is interesting to notice that both delay-free impulses and delayed impulses can contribute to the synchronization of coupled neural networks. Meanwhile, we also discuss the synchronization of neural networks only with delay-dependent impulses. Some sufficient conditions are derived to ensure the exponential synchronization of delayed neural networks. Finally, some numerical examples are provided to illustrate the validity and superiority of the obtained results. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				JUL	2020	127						38	46		10.1016/j.neunet.2020.04.002													
J								Further results on finite-time synchronization of delayed inertial memristive neural networks via a novel analysis method	NEURAL NETWORKS										Finite-time synchronization; Inertial memristive neural networks; New inequality methods; Mixed time-varying delays	VARYING DELAYS; STABILITY; DYNAMICS; PASSIVITY; CHAOS	In this paper, we propose a novel analysis method to investigate the finite-time synchronization (FTS) control problem of the drive-response inertial memristive neural networks (IMNNs) with mixed time-varying delays (MTVDs). Firstly, an improved control scheme is proposed under the delay-independent conditions, which can work even when the past state cannot be measured or the specific time delay function is unknown. Secondly, based on the assumption of bounded activation functions, we establish a new Lemma, which can effectively deal with the difficulties caused by memristive connection weights and MTVDs. Thirdly, by constructing a suitable Lyapunov functions and using a new inequality method, novel sufficient conditions to ensure the FTS for the discussed IMNNs are obtained. Compared with the existing results, our results obtained in a more general framework are more practical. Finally, some numerical simulations are given to substantiate the effectiveness of the theoretical results. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				JUL	2020	127						47	57		10.1016/j.neunet.2020.04.009													
J								Generative Adversarial Networks are special cases of Artificial Curiosity (1990) and also closely related to Predictability Minimization (1991)	NEURAL NETWORKS										Artificial Curiosity; Predictability Minimization; Generative Adversarial Networks	CREATIVITY; ALGORITHM	I review unsupervised or self-supervised neural networks playing minimax games in game-theoretic settings: (i) Artificial Curiosity (AC, 1990) is based on two such networks. One network learns to generate a probability distribution over outputs, the other learns to predict effects of the outputs. Each network minimizes the objective function maximized by the other. (ii) Generative Adversarial Networks (GANs, 2010-2014) are an application of AC where the effect of an output is 1 if the output is in a given set, and 0 otherwise. (iii) Predictability Minimization (PM, 1990s) models data distributions through a neural encoder that maximizes the objective function minimized by a neural predictor of the code components. I correct a previously published claim that PM is not based on a minimax game. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				JUL	2020	127						58	66		10.1016/j.neunet.2020.04.008													
J								Neural memory plasticity for medical anomaly detection	NEURAL NETWORKS										Neural Memory Networks; Anomaly detection; Neural plasticity; Abnormal EEG identification; MRI tumour type classification; Schizophrenia risk detection	PUTATIVE ANTECEDENTS; CLASSIFICATION; BRAIN; NETWORKS; SCHIZOPHRENIA; ARTMAP	In the domain of machine learning, Neural Memory Networks (NMNs) have recently achieved impressive results in a variety of application areas including visual question answering, trajectory prediction, object tracking, and language modelling. However, we observe that the attention based knowledge retrieval mechanisms used in current NMNs restrict them from achieving their full potential as the attention process retrieves information based on a set of static connection weights. This is suboptimal in a setting where there are vast differences among samples in the data domain; such as anomaly detection where there is no consistent criteria for what constitutes an anomaly. In this paper, we propose a plastic neural memory access mechanism which exploits both static and dynamic connection weights in the memory read, write and output generation procedures. We demonstrate the effectiveness and flexibility of the proposed memory model in three challenging anomaly detection tasks in the medical domain: abnormal EEG identification, MRI tumour type classification and schizophrenia risk detection in children. In all settings, the proposed approach outperforms the current state-of-the-art. Furthermore, we perform an in-depth analysis demonstrating the utility of neural plasticity for the knowledge retrieval process and provide evidence on how the proposed memory model generates sparse yet informative memory outputs. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				JUL	2020	127						67	81		10.1016/j.neunet.2020.04.011													
J								Novel deep neural network based pattern field classification architectures	NEURAL NETWORKS										Neural network; Field classification; Deep learning	STYLE	Field classification is a new extension of traditional classification frameworks that attempts to utilize consistent information from a group of samples (termed fields). By forgoing the independent identically distributed (i.i.d.) assumption, field classification can achieve remarkably improved accuracy compared to traditional classification methods. Most studies of field classification have been conducted on traditional machine learning methods. In this paper, we propose integration with a Bayesian framework, for the first time, in order to extend field classification to deep learning and propose two novel deep neural network architectures: the Field Deep Perceptron (FDP) and the Field Deep Convolutional Neural Network (FDCNN). Specifically, we exploit a deep perceptron structure, typically a 6-layer structure, where the first 3 layers remove (learn) a 'style' from a group of samples to map them into a more discriminative space and the last 3 layers are trained to perform classification. For the FDCNN, we modify the AlexNet framework by adding style transformation layers within the hidden layers. We derive a novel learning scheme from a Bayesian framework and design a novel and efficient learning algorithm with guaranteed convergence for training the deep networks. The whole framework is interpreted with visualization features showing that the field deep neural network can better learn the style of a group of samples. Our developed models are also able to achieve transfer learning and learn transformations for newly introduced fields. We conduct extensive comparative experiments on benchmark data (including face, speech, and handwriting data) to validate our learning approach. Experimental results demonstrate that our proposed deep frameworks achieve significant improvements over other state-of-the-art algorithms, attaining new benchmark performance. (C) 2020 Published by Elsevier Ltd.																	0893-6080	1879-2782				JUL	2020	127						82	95		10.1016/j.neunet.2020.03.011													
J								On the boundary conditions of avoidance memory reconsolidation: An attractor network perspective	NEURAL NETWORKS										Attractor network; Inhibitory avoidance; Reconsolidation; Extinction; Boundary condition; Synaptic plasticity	FREE-ENERGY PRINCIPLE; PROTEIN-SYNTHESIS; PATTERN SEPARATION; FEAR; EXTINCTION; HIPPOCAMPUS; MECHANISMS; SYSTEMS; CONSOLIDATION; PLASTICITY	The reconsolidation and extinction of aversive memories and their boundary conditions have been extensively studied. Knowing their network mechanisms may lead to the development of better strategies for the treatment of fear and anxiety-related disorders. In 2011, Osan et al. developed a computational model for exploring such phenomena based on attractor dynamics, Hebbian plasticity and synaptic degradation induced by prediction error. This model was able to explain, in a single formalism, experimental findings regarding the freezing behavior of rodents submitted to contextual fear conditioning. In 2017, through the study of inhibitory avoidance in rats, Radiske et al. showed that the previous knowledge of a context as non-aversive is a boundary condition for the reconsolidation of the shock memory subsequently experienced in that context. In the present work, by adapting the model of Osan et al. (2011) to simulate the experimental protocols of Radiske et al. (2017), we show that such boundary condition is compatible with the dynamics of an attractor network that supports synaptic labilization common to reconsolidation and extinction. Additionally, by varying parameters such as the levels of protein synthesis and degradation, we predict behavioral outcomes, and thus boundary conditions that can be tested experimentally. (C) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				JUL	2020	127						96	109		10.1016/j.neunet.2020.04.013													
J								Dendrite P systems	NEURAL NETWORKS										P systems; Neural-like P systems; Dendrite P systems; Computational power	RULES; SIMULATION; ALGORITHM; POWER	It was recently found that dendrites are not just a passive channel. They can perform mixed computation of analog and digital signals, and therefore can be abstracted as information processors. Moreover, dendrites possess a feedback mechanism. Motivated by these computational and feedback characteristics, this article proposes a new variant of neural-like P systems, dendrite P (DeP) systems, where neurons simulate the computational function of dendrites and perform a firing-storing process instead of the storing-firing process in spiking neural P (SNP) systems. Moreover, the behavior of the neurons is characterized by dendrite rules that are abstracted by two characteristics of dendrites. Different from the usual firing rules in SNP systems, the firing of a dendrite rule is controlled by the states of the corresponding source neurons. Therefore, DeP systems can provide a collaborative control capability for neurons. We discuss the computational power of DeP systems. In particular, it is proven that DeP systems are Turing-universal number generating/accepting devices. Moreover, we construct a small universal DeP system consisting of 115 neurons for computing functions. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				JUL	2020	127						110	120		10.1016/j.neunet.2020.04.014													
J								Training of deep neural networks for the generation of dynamic movement primitives	NEURAL NETWORKS										Training of deep neural networks; Dynamic movement primitives; Robot skill learning		Dynamic movement primitives (DMPs) have proven to be an effective movement representation for motor skill learning. In this paper, we propose a new approach for training deep neural networks to synthesize dynamic movement primitives. The distinguishing property of our approach is that it can utilize a novel loss function that measures the physical distance between movement trajectories as opposed to measuring the distance between the parameters of DMPs that have no physical meaning. This was made possible by deriving differential equations that can be applied to compute the gradients of the proposed loss function, thus enabling an effective application of backpropagation to optimize the parameters of the underlying deep neural network. While the developed approach is applicable to any neural network architecture, it was evaluated on two different architectures based on encoder-decoder networks and convolutional neural networks. Our results show that the minimization of the proposed loss function leads to better results than when more conventional loss functions are used. (c) 2020 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).																	0893-6080	1879-2782				JUL	2020	127						121	131		10.1016/j.neunet.2020.04.010													
J								Micro-cracks detection of solar cells surface via combining short-term and long-term deep features	NEURAL NETWORKS										Solar cell; Micro-cracks detection; Short-term deep features; Long-term deep features; Stacked denoising auto encoder; Convolutional neural networks	SWITCHED NEURAL-NETWORKS; CLASSIFICATION; DEFECTS	The machine vision based methods for micro-cracks detection of solar cells surface have become one of the main research directions with its efficiency and convenience. The existed methods are roughly classified into two categories: current viewing information based methods, prior knowledge based methods, however, the former usually adopt hand-designed features with poor generality and lacks the guidance of prior knowledge, the latter are usually implemented through the machine learning, and the generalization ability is also limited since the large-scale annotation dataset is scarce. To resolve above problems, a novel micro-cracks detection method via combining short-term and long-term deep features is proposed in this paper. The short-term deep features which represent the current viewing information are learned from the input image itself through stacked denoising auto encoder (SDAE), the long-term deep features which represent the prior knowledge are learned from a large number of natural scene images that people often see through convolutional neural networks (CNNs). The subjective and objective evaluations demonstrate that: 1) the performance of combining the short-term and long-term deep features is better than any of them alone, 2) the performance of proposed method is superior to the shallow learning based methods, 3) the proposed method can effectively detect various kinds of micro-cracks. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				JUL	2020	127						132	140		10.1016/j.neunet.2020.04.012													
J								Linear embedding by joint Robust Discriminant Analysis and Inter-class Sparsity	NEURAL NETWORKS										Linear discriminant analysis; Feature extraction; Feature selection; Inter-class sparsity; Image classification	FEATURE-SELECTION; CLASSIFICATION; RECOGNITION; REGRESSION; PROJECTION	Linear Discriminant Analysis (LDA) and its variants are widely used as feature extraction methods. They have been used for different classification tasks. However, these methods have some limitations that need to be overcome. The main limitation is that the projection obtained by LDA does not provide a good interpretability for the features. In this paper, we propose a novel supervised method used for multi-class classification that simultaneously performs feature selection and extraction. The targeted projection transformation focuses on the most discriminant original features, and at the same time, makes sure that the transformed features (extracted features) belonging to each class have common sparsity. Our proposed method is called Robust Discriminant Analysis with Feature Selection and Interclass Sparsity (RDA_FSIS). The corresponding model integrates two types of sparsity. The first type is obtained by imposing the l(2,1) constraint on the projection matrix in order to perform feature selection. The second type of sparsity is obtained by imposing the inter-class sparsity constraint used for ensuring a common sparsity structure in each class. An orthogonal matrix is also introduced in our model in order to guarantee that the extracted features can retain the main variance of the original data and thus improve the robustness to noise. The proposed method retrieves the LDA transformation by taking into account the two types of sparsity. Various experiments are conducted on several image datasets including faces, objects and digits. The projected features are used for multi-class classification. Obtained results show that the proposed method outperforms other competing methods by learning a more compact and discriminative transformation. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				JUL	2020	127						141	159		10.1016/j.neunet.2020.04.018													
J								Graph Convolution Networks with manifold regularization for semi-supervised learning	NEURAL NETWORKS										Graph-based semisupervised learning; Graph Convolution Networks (GCN); Label prediction; Manifold regularization; Semisupervised image classification	FRAMEWORK	In recent times, Graph Convolution Networks (GCN) have been proposed as a powerful tool for graph-based semi-supervised learning. In this paper, we introduce a model that enhances label propagation of Graph Convolution Networks (GCN). More precisely, we propose GCNs with Manifold Regularization (GCNMR). The objective function of the proposed GCNMR is composed by a supervised term and an unsupervised term. The supervised term enforces the fitting term between the predicted labels and the known labels. The unsupervised term imposes the smoothness of the predicted labels of the whole data samples. By learning a Graph Convolution Network with the proposed objective function, we are able to derive a more powerful semi-supervised learning. The proposed model retains the advantages of the classic GCN, yet it can improve it with no increase in time complexity. Experiments on three public image datasets show that the proposed model is superior to the GCN and several competing existing graph-based semi-supervised learning methods. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				JUL	2020	127						160	167		10.1016/j.neunet.2020.04.016													
J								Vulnerability of classifiers to evolutionary generated adversarial examples	NEURAL NETWORKS										Supervised learning; Neural networks; Kernel methods; Genetic algorithms; Adversarial examples	NETWORKS	This paper deals with the vulnerability of machine learning models to adversarial examples and its implication for robustness and generalization properties. We propose an evolutionary algorithm that can generate adversarial examples for any machine learning model in the black-box attack scenario. This way, we can find adversarial examples without access to model's parameters, only by querying the model at hand. We have tested a range of machine learning models including deep and shallow neural networks. Our experiments have shown that the vulnerability to adversarial examples is not only the problem of deep networks, but it spreads through various machine learning architectures. Rather, it depends on the type of computational units. Local units, such as Gaussian kernels, are less vulnerable to adversarial examples. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				JUL	2020	127						168	181		10.1016/j.neunet.2020.04.015													
J								DART: Domain-Adversarial Residual-Transfer networks for unsupervised cross-domain image classification	NEURAL NETWORKS										Transfer learning; Residue network; Adversarial domain adaptation	RECURRENT NEURAL-NETWORKS	The accuracy of deep learning (e.g., convolutional neural networks) for an image classification task critically relies on the amount of labeled training data. Aiming to solve an image classification task on a new domain that lacks labeled data but gains access to cheaply available unlabeled data, unsupervised domain adaptation is a promising technique to boost the performance without incurring extra labeling cost, by assuming images from different domains share some invariant characteristics. In this paper, we propose a new unsupervised domain adaptation method named Domain-Adversarial Residual-Transfer (DART) learning of deep neural networks to tackle cross-domain image classification tasks. In contrast to the existing unsupervised domain adaption approaches, the proposed DART not only learns domain-invariant features via adversarial training, but also achieves robust domain-adaptive classification via a residual-transfer strategy, all in an end-to-end training framework. We evaluate the performance of the proposed method for cross-domain image classification tasks on several well-known benchmark data sets, in which our method clearly outperforms the state-of-the-art approaches. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				JUL	2020	127						182	192		10.1016/j.neunet.2020.03.025													
J								A classification-based approach to semi-supervised clustering with pairwise constraints	NEURAL NETWORKS										Semi-supervised clustering; Deep learning; Neural networks; Pairwise constraints; Siamese neural networks		In this paper, we introduce a neural network framework for semi-supervised clustering with pairwise (must-link or cannot-link) constraints. In contrast to existing approaches, we decompose semi-supervised clustering into two simpler classification tasks: the first stage uses a pair of Siamese neural networks to label the unlabeled pairs of points as must-link or cannot-link; the second stage uses the fully pairwise-labeled dataset produced by the first stage in a supervised neural-network-based clustering method. The proposed approach is motivated by the observation that binary classification (such as assigning pairwise relations) is usually easier than multi-class clustering with partial supervision. On the other hand, being classification-based, our method solves only well-defined classification problems, rather than less well specified clustering tasks. Extensive experiments on various datasets demonstrate the high performance of the proposed method. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				JUL	2020	127						193	203		10.1016/j.neunet.2020.04.017													
J								LiDAR-based vehicle localization on the satellite image via a neural network	ROBOTICS AND AUTONOMOUS SYSTEMS										Localization; Deep learning; LiDAR; Satellite image matching		We present a novel method to localize the vehicle on an easily accessible geo-referenced satellite image based on LiDAR. We first design a neural network to extract and compare the spatial-discriminative feature maps of the satellite image patch and the LiDAR points, and obtain the probability of correspondence. Then based on the outputs of the network, a particle filter is used to obtain the probability distribution of the vehicle pose. This method can use LiDAR points and any type of odometry as input to localize the vehicle. The experimental results show that our model can generalize well on several datasets. Compared with other methods, ours is more robust in some challenging scenarios such as the occluded or shadowed area on the satellite image. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				JUL	2020	129								103519	10.1016/j.robot.2020.103519													
J								Tuning and sensitivity analysis of a hexapod state estimator	ROBOTICS AND AUTONOMOUS SYSTEMS										Extended Kalman filter; Indirect EKF; EKF tuning; Hexapod; State estimation; Particle swarm optimisation (PSO)	LEGGED ODOMETRY; FUSION; ROBOT; OPTIMIZATION; MULTISENSOR; LOCOMOTION; POSE	An important envisaged application of legged robots is the exploration and mapping of extreme environments with an unknown terrain. Corin is a hexapod designed at the University of Manchester, which is able to perform motions using footholds on surfaces perpendicular to the ground plane. This allows it to be able to navigate through confined and narrow spaces. The hexapod requires an accurate estimate of its pose in order to be able to perform these motions. Current state-of-the-art state estimators for legged robots that solely use proprioceptive sensors, fuse inertial and leg kinematic measurements through an extended Kalman filter (EKF). This paper describes the implementation and validation of a state estimator on the Corin hexapod whilst performing motions using surface perpendicular to the ground plane. Another novelty of the work is the analysis of the algorithm sensitivity to the filter parameters and motion variables, and the tuning of the EKF using particle swarm optimisation (PSO). The results show that the average error achieved was below 6% for both position and orientation estimates. (C) 2020 The Authors. Published by Elsevier B.V.																	0921-8890	1872-793X				JUL	2020	129								103509	10.1016/j.robot.2020.103509													
J								A connectivity preserving node permutation local method in limited range robotic networks	ROBOTICS AND AUTONOMOUS SYSTEMS										Connectivity maintenance; Local connectivity; Multi-robot network; Potential function	DISTRIBUTED CONTROL; MAINTENANCE; SYSTEMS; COOPERATION	Limited communication range, together with mobility of robots, makes it crucial to design the control plans such that connectivity of a multi-robot network is maintained. Recently, many local and global connectivity maintenance schemes have been proposed to preserve connectivity of a robotic network. The traditional local connectivity maintenance method (LCM) is known to preserve every existing link, even though some of the existing connections might not be necessary for maintaining a path between each pair of robots, which is the aim of global connectivity maintenance (GCM) methods. However, the flexibility of movement provided by the global method costs restriction on speed and bandwidth. In this paper, a modified local connectivity maintenance method is provided to gain more flexibility of movement, while preserving the properties and simplicity of a local method. The proposed method is based on traditional local connectivity maintenance equipped with a basic operation to exchange the neighbors between two adjacent robots. Permutation of robots could be beneficial in many robotic applications such as exchanging the leader role in a V-formed robotic group or providing a path for a robot to reach its desired position while preserving the networks connectivity. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				JUL	2020	129								103540	10.1016/j.robot.2020.103540													
J								Optimal communication relay positioning in mobile multi-node networks	ROBOTICS AND AUTONOMOUS SYSTEMS										Communication relay; Gaussian process regression; Indoor communication model; Wireless mesh network	UNMANNED AERIAL VEHICLES; ROBOTIC ROUTER FORMATION; GAUSSIAN-PROCESSES; CONNECTIVITY	This paper presents an optimal communication relay positioning method to improve the communication performance of mobile multi-node networks in complex environments. The communication channel quality prediction between nodes is of primary concern to find the optimal relay node positions while considering the uncertain and dynamic nature of environments. To this end, the learning-based or the distance model-based method is used for the channel prediction depending on the mobility of the communication nodes of interest. The global message connectivity and the worst case connectivity are introduced as the communication performance metric of networked agents. The optimal relay positions are found by maximizing the performance with respect to the relay positions through a heuristic optimization technique. This algorithm outperforms a recently-developed relay positioning algorithm in the simulations. The indoor experiments are conducted to show that the proposed approach using mobile relays improves the communication performance of the complex network significantly with the accurate channel prediction. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				JUL	2020	129								103517	10.1016/j.robot.2020.103517													
J								A new approach for the estimation of non-cooperative satellites based on circular feature extraction	ROBOTICS AND AUTONOMOUS SYSTEMS										Pose estimation; Non-cooperative target; Adapter ring; Ellipse detection; Space robot; On-orbit service	RELATIVE NAVIGATION; POSE DETERMINATION; ORBIT	Pose estimation of non-cooperative satellites has been a hot topic in the study of astronautics as the visual feedback will highly enhance the safety of on-orbit services. A stereo vision system is proposed in this paper. It works as an eye-to-hand vision camera in the final approach phase Based on circular feature extraction, a closed-form solution is presented. The position and orientation of the adapter ring can be figured out in real-time as well as the unknown radius. Neither additional sensors nor prior knowledge is required, and the orientation-duality problem has been solved. It works well on the partial ellipses and is robust to outliers, noise and occlusions. Experimental results on both synthetic and real images have demonstrated the effectiveness and efficiency of the proposed method. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				JUL	2020	129								103532	10.1016/j.robot.2020.103532													
J								Fine-grained action plausibility rating	ROBOTICS AND AUTONOMOUS SYSTEMS											ACTION SELECTION; OBJECTS; AGENTS	An essential capability of humans is the effortless identification of useful tasks based on visual cues in everyday situations. Objects and their surroundings are integrated and processed to differentiate plausible from implausible actions. In this work, we study how to teach this ability to robots. In contrast to many tasks in computer vision where the goal is an accurate description (object labels, caption, scene class) of the present situation here the challenge is to make reasonable guesses about which forms of plausible and implausible actions can be conducted. To this end, we collect a dataset that associates images with probabilities over a set of actions. A convolutional neural network is trained to match these ground truth plausibility scores using this dataset. We compare the performance of state-of-the-art encoder architectures and specifically analyze the role of contextual cues quantitatively. While the object recognition capabilities of the encoder have a strong impact on performance, using context did not lead to substantial improvements. We show qualitatively the utility of such a system for robotic action selection in a household setting. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				JUL	2020	129								103511	10.1016/j.robot.2020.103511													
J								Robust shape estimation with false-positive contact detection	ROBOTICS AND AUTONOMOUS SYSTEMS										Tactile sensing; Shape estimation; Gaussian processes	TACTILE; EXPLORATION	We propose a means of omni-directional contact detection using accelerometers instead of tactile sensors for object shape estimation using touch. Unlike tactile sensors, our contact-based detection method tends to induce a degree of uncertainty with false-positive contact data because the sensors may react not only to actual contact but also to the unstable behavior of the robot. Therefore, it is crucial to consider a robust shape estimation method capable of handling such false-positive contact data. To realize this, we introduce the concept of heteroscedasticity into the contact data and propose a robust shape estimation algorithm based on Gaussian process implicit surfaces (GPIS). We confirmed that our algorithm not only reduces shape estimation errors caused by false-positive contact data but also distinguishes false-positive contact data more clearly than the GPIS through simulations and actual experiments using a quadcopter. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				JUL	2020	129								103527	10.1016/j.robot.2020.103527													
J								Development of a robotic finger with a branching tendon mechanism and sensing based on the moment-equivalent point	ROBOTICS AND AUTONOMOUS SYSTEMS										Wire-driven robotic finger; Branching tendon mechanism; Force sensing; Moment-equivalent point	DESIGN	In this paper, we developed an underactuated robotic finger with three joints having a branching tendon mechanism and a sensing system to estimate the wrench applied to the fingertip based on the moment-equivalent point (MEP). We proposed the design to combine the branching tendon mechanism and the principle of wrench sensing based on the MEP. The proposed system realized the measurement of the wrench applied to the fingertip using a simple force sensor and a wire-driven system. Furthermore, we experimentally confirmed their functioning. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				JUL	2020	129								103538	10.1016/j.robot.2020.103538													
J								A novel coordinated motion planner based on capability map for autonomous mobile manipulator	ROBOTICS AND AUTONOMOUS SYSTEMS										Autonomous mobile manipulator; Coordinated motion; Capability map; Motion planning	RESOLUTION	With the development of the robotic technology, Autonomous Mobile Manipulator (AMM) is increasingly used in more applications. Reasonable motion planning for AMM to maintain high manipulation capability is the prerequisite for the success of the mobile manipulation task. In this paper, the Capability Map (CM) of AMM that gives the distribution of the manipulability in cartesian space is first built. Then given the path of the end effector, we design a novel path planner for the mobile robot by querying CM online so that AMM keeps high manipulability. Moreover, a task-priority coordinated motion controller is developed to control the mobile robot and the manipulator to track their trajectories. In this controller, the trajectory of the manipulator is used as the primary task, and the trajectory of the mobile robot is treated as the constrained task. Simulation results show that the path of the mobile robot can be found online, and AMM follows the trajectories well. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				JUL	2020	129								103554	10.1016/j.robot.2020.103554													
J								Grasp prediction and evaluation of multi-fingered dexterous hands using deep learning	ROBOTICS AND AUTONOMOUS SYSTEMS										Grasp prediction; Grasp evaluation; Multi-fingered dexterous hands; Convolutional neural networks; Mixture density networks	OBJECTS	Learning from human skills has become one of the popular inspirations in grasp prediction and evaluation, but lack of effective methods on groups of grasp points for multi-fingered dexterous hands yields an open challenge. When facing an object, humans firstly predict a variety of options for grasps, which can be concerned as a complex multi-valued problem. After prediction, humans evaluate grasps and then choose the optimal one. Inspired by human skills, we propose Grasp Prediction Networks (GPNs) based on Convolutional Neural Networks (CNNs) and Mixture Density Networks (MDNs). The proposed GPNs map from a depth image to a set of parameters for Gaussian Mixture Model (GMM), from which candidate groups of grasp points can be sampled for prediction. Besides, we also propose Grasp Evaluation Networks (GENs) to evaluate candidate groups and then choose the optimal group of grasp points. The proposed GENs consider force-closure metric as grasp quality for evaluation. Different from other related work, our method (1) utilizes a probabilistic model to predict multiple groups of grasp points from a monocular depth image and (2) evaluates grasp quality with force-closure metric given a monocular depth image and a group of grasp points. Furthermore, we built a grasp dataset which consists of depth images, groups of grasp points and each group's grasp quality. Herein, three different experiments were designed to validate our approach. The first one was a comparative experiment and revealed that GPNs show equivalent performance as Grasplt! in terms of high-quality grasp planning. The second one was also a comparative experiment, which validated that GENs can evaluate grasps as precisely as Grasplt!. Moreover, the last one was an actual experiment implemented on Shadow Hand Lite, and experimental results indicated that our approach achieved finely grasp of novel objects. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				JUL	2020	129								103550	10.1016/j.robot.2020.103550													
J								NOMA based CR for QAM-64 and QAM-256	EGYPTIAN INFORMATICS JOURNAL										Matched filter; Cognitive radio (Cr); NOMA-OFDM	COGNITIVE RADIO; SYSTEM	Non-Orthogonal multiple access (NOMA) and Cognitive radio (Cr) are seen as one of the most promising techniques, which improves the utilization of the spectrum in 5G. The expanding number of wireless applications like new gadgets, IOT brought about developing a block in the ISM groups. The FCC requested to permit unlicensed clients to work in the void area without obstruction to an authorized guest. Cr gives an answer for an extra range prerequisite issue for productive spectrum usage. The foremost condition for permitting CRs to utilize spectrum is not causing obstruction to licensed users. Spectrum sensing permit secondary users (Su) to separately recognize the idle portions of the spectrum, and thus evade obstruction to licensed users. In existing spectrum sensing techniques, SU can only utilize the unused spectrum when PU is not present. Therefore, spectrum exploitation of the conventional system is very low. In recent times NOMA has been projected to utilize the spectrum in an efficient manner. The proposed work permits the SU to utilize a spectrum of PU, both at its absence. Spectrum sensing in NOMA is not explored so far. Hence, in this paper, NOMA based matched filter detection is designed for QAM-64 and QAM-256. Matlab simulation is applied to study the operation of the proposed detection technique in NOMA in respect of several parameters like bit error rate (BER) Vs signal to noise ratio (SNR), the probability of detection (Pd), and probability of false alarm (Pfa). (C) 2019 Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Information, Cairo University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).																	1110-8665	2090-4754				JUL	2020	21	2					67	71		10.1016/j.eij.2019.10.004													
J								Extractive Arabic Text Summarization Using Modified PageRank Algorithm	EGYPTIAN INFORMATICS JOURNAL										Extractive Arabic text summarization; PageRank; Morphological analyzer; Graph based		This paper proposed an approach for Arabic text summarization. Text summarization is one of the natural language processing's applications which is used for reducing the original text amount and retrieving only the important information from the original text. The Arabic language has a complex morphological structure which makes it very difficult to extract nouns to be used as a feature for summarization process. Therefore, Al-Khalil morphological analyzer is used to solve the problem of nouns extraction. The proposed approach is a graph-based system, which represents the document as a graph where the vertices of the graph are the sentences. A Modified PageRank algorithm is applied with an initial score for each node that is the number of nouns in this sentence. More nouns in the sentence mean more information, so nouns count used here as initial rank for the sentence. Edges between sentences are the cosine similarity between the sentences, to get a final summary that contains sentences with more information and well connected with each other. The process of text summarization consists of three major stages: preprocessing stage, features extraction and graph construction stage, and finally applying the Modified PageRank algorithm and summary extraction. The Modified PageRank algorithm used a different number of iterations to find the number returns the best summary results, and the extracted summary depends on compression ratio, taking into account removing redundancy depending on the overlapping between the sentences. To evaluate the performance of this approach EASC Corpus is used as a standard. LexRank and TextRank algorithms were used under the same circumstances, the proposed approach provides better results when compared with other Arabic text summarization techniques. The proposed approach performs efficiently with the number of iteration 10,000. (C) 2019 Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Information, Cairo University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).																	1110-8665	2090-4754				JUL	2020	21	2					73	81		10.1016/j.eij.2019.11.001													
J								A novel crypto technique based ciphertext shifting	EGYPTIAN INFORMATICS JOURNAL										Cryptography; Ciphertext; Shifting algorithm; HMAC; AES		One of the significant issues in information security areas is a hidden exchange of data. There are several techniques for this purpose such as cryptography, steganography, etc. Generally, in cryptography, the secret message content is scrambled. In another hand in steganography, the secret message is embedded inside the cover medium. In this paper, a new crypto technique-based ciphertext shifting algorithm has been designed to improve the security for our previous work that combining cryptographic and steganographic. The improvement in the security of the secret message is done by changing the ciphertext value. The proposed shifting algorithm is used to rearranges the location of each character of ciphertext based on key value, in which the final ciphertext length is equivalent to encryption value but it different in value. The key strength of this method is two side one is trick the attacker from notice any change in ciphertext which is the same length as the original, so when used the common cryptanalysis will not get anything due to the original ciphertext has been changed. The second key strength of this method is that the shifting value is variable and dependent on key length. This method is inspected to be a very strong technique that can prevent common cryptography attacks such as a dictionary or brute-force attacks, etc. (C) 2019 Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Information, Cairo University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).																	1110-8665	2090-4754				JUL	2020	21	2					83	90		10.1016/j.eij.2019.11.002													
J								A novel solution for a Wireless Body Sensor Network: Telehealth elderly people monitoring	EGYPTIAN INFORMATICS JOURNAL										Tele-monitoring chronic heart patients; Data transmission; WBSN		Event-based data transfer through Wireless Body Sensor Networks (WBSN) for monitoring the health of the elderly has so far not been successfully implemented due to limitations arising forms unreliable data, end-to-end delay during data transmission and the high energy consumption by sensors. This paper aims to improve reliability and latency and to reduce energy consumption by sensors during data transmission in WSBN. The proposed system consists of an Enhanced Reliability, Energy-Efficient and Latency (EREEAL) algorithm to reduce data losses and end-to-end delay as well as improve the transmission reliability in WBSN by sending the sensor data during different time slots using Time Division Multiple Access (TDMA) analysis and by minimizing redundant sensitive data. The result shows that the new algorithm improves reliability to 98% over the data bits generated within 8 similar to 12 min and reduces latency to 0.635 compared to 0.875 ms in the 'state of the art' system. Furthermore, the reduction in latency leads to lower power consumption by sensors, reduced to 315.638*10 3JS/bits during patient data transmission using a tele-monitoring process. The proposed system concentrates on reducing interference with data between sensors and focuses on minimizing data loss during transmission. Thus, this study provides an acceptable range of reliability with reduced delay and lower power consumption due to which doctors at a remote site can obtain reliable data value for smooth monitoring. (C) 2019 Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Information, Cairo University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).																	1110-8665	2090-4754				JUL	2020	21	2					91	103		10.1016/j.eij.2019.11.004													
J								A novel enhanced hybrid recursive algorithm: Image processing based augmented reality for gallbladder and uterus visualisation	EGYPTIAN INFORMATICS JOURNAL										Augmented reality; 3D-2D image registration; Gallbladder surgery; Bowel surgery; Surgical and invasive medical procedures	REGISTRATION	Background: Current Augmented Reality systems in liver and bowel surgeries, are not accurate enough to classify the hidden parts such as gallbladder and uterus which are behind the liver and bowel. Therefore, we aimed to improve the visualization accuracy of bowel and liver augmented videos to avoid the unexpected cuttings on the hidden parts. Methodology: The proposed system consists of an Enhanced hybrid recursive matching and k-parameterization techniques to improve the visualization. In addition, Mean Shift Filter is also added to improve the matching process while image registration. Results: Results proved that, the accuracy is improved in terms of liver and bowel surgeries Visualization errors about 0.53 mm and 0.22 mm respectively. Similarly, it can produce 2 more frames/sec compared to the current system. Conclusion: The proposed system worked towards the visualization of gallbladder and uterus while liver and bowel surgeries. So, this study solved the visualization issues, which are caused by neighbouring and hidden parts. (C) 2019 Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Information, Cairo University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).																	1110-8665	2090-4754				JUL	2020	21	2					105	118		10.1016/j.eij.2019.11.003													
J								Introducing languid particle dynamics to a selection of PSO variants	EGYPTIAN INFORMATICS JOURNAL										Particle swarm optimization; Inertia weight; Fitness based inertia; Swarm intelligence; Pipe network optimization	SWARM OPTIMIZER; INERTIA	Previous research showed that conditioning a PSO agent's movement based on its personal fitness improvement enhances the standard PSO method. In this article, Languid Particle Dynamics (LPD) technique is used on five adequate and widely used PSO variants. Five unmodified PSO variants were tested against their LPD-enabled counterparts on three search space dimensionalities (10, 20, and 50 dimensions) and 30 test functions of the CEC 2014 benchmark test. In the preliminary phase of the testing four of the five tested PSO variants showed improvement in accuracy. The worst and best-achieving variants from preliminary test went through detailed investigation on 220 and 770 combinations of method parameters, where both variants showed overall gains in accuracy when enhanced with LPD. The results obtained with best achieving PSO parameters were subject to statistical analysis which showed that the two variants give statistically significant improvements in accuracy for 13-50% of the test functions. Lastly, an engineering application test-case of water distribution system optimization was used, in which 151 pipe diameters were optimized with the tested PSO variants. Two of the five PSO variants manifested significantly improved accuracy when enhanced with LPD, with a LPD-enabled variant producing the overall best results, as being the only variant capable of finding the optimal pipe network configuration. (C) 2019 Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Information, Cairo University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).																	1110-8665	2090-4754				JUL	2020	21	2					119	129		10.1016/j.eij.2019.11.005													
J								Multi-stage multi-attribute decision-making method based on the prospect theory and triangular fuzzy MULTIMOORA	SOFT COMPUTING										Multi-attribute decision-making; Triangular fuzzy number; MULTIMOORA; The dominance theory	AGGREGATION OPERATORS; MOORA METHOD; OPTIMIZATION; SELECTION	This paper proposed a novel multi-stage multi-attribute decision-making method, in which the period weights and attribute weights are completely unknown, and attribute values are triangular fuzzy numbers. At first, the triangular fuzzy prospect decision matrices of different periods are constructed and the period weights optimization model is built according to the time degree and the prospect value deviation of alternatives in different periods. The attribute weights are obtained by the maximum deviation method. Then, MULTIMOORA is extended into the triangular fuzzy environment. Based on the new form of MULTIMOORA and dominance theory, the final ranking of alternatives can be got. Finally, a numerical example is presented to illustrate the feasibility and effectiveness of the proposed method.																	1432-7643	1433-7479				JUL	2020	24	13					9429	9440		10.1007/s00500-018-3017-0													
J								A fusion of decision-making method and neutrosophic linguistic considering multiplicative inverse matrix for coastal erosion problem	SOFT COMPUTING										Decision-making; DEMATEL; Inverse matrix; Neutrosophic set; Coastal erosion	MODEL COMBINING DEMATEL; INTEGRATED FUZZY AHP; HUMAN-RESOURCE; AGGREGATION OPERATORS; MCDM APPROACH; SELECTION; TOPSIS; SETS; ANP; CRITERIA	The recent boom of decision-making under uncertain information has attracted many researchers to the field of integrating various types of sets with decision-making methods. In this paper, a combined decision-making trial and evaluation laboratory (DEMATEL) with single-valued neutrosophic sets is proposed to solve the decision problem. This new model combines the advantages of multiplicative inverse of decision matrix in DEMATEL and neutrosophic numbers in linguistic variables, which can find the interrelationship among factors of decision problem. Differently from the typical multiplicative inverse of DEMATEL, which directly used inverse of matrix using real numbers, this method introduces the concept of inverse of matrix using the proposed left-right neutrosophic numbers. This step will enhance the validity of multiplicative inverse of decision matrix in the DEMATEL with neutrosophic numbers. The proposed neutrosophic DEMATEL is also be compared with the DEMATEL and fuzzy DEMATEL. This paper includes a case study that demonstrates the applicability of the neutrosophic DEMATEL in establishing the relationship among influential factors of coastal erosion. Extensive empirical studies using 12 factors of coastal erosion were presented to study the benefits of the proposed method. The result unveils the cause-and-effect relationships among the factors, where seven factors are identified as cause factors and five factors are grouped as effect factors. It is discovered that the factor 'imbalance sediment supply' gives a significant influence to coastal erosion. It is also shown that the degree of importance of the factors is almost consistent with the other two methods despite differences in type of numbers used in defining linguistic variables.																	1432-7643	1433-7479				JUL	2020	24	13					9595	9609		10.1007/s00500-019-04467-5													
J								An innovative user-attentive framework for supporting real-time detection and mining of streaming microblog posts	SOFT COMPUTING										Attentive user management systems; Real-time detection of streaming microblog posts; Real-time mining of streaming microblog posts; Opinion mining; Sentiment mining	SENTIMENT; CLASSIFICATION; FACE	In this paper, we present a modular system capable of catching the attention of a new user, to detect in real-time events and emotions related to them in a stream of microblog posts. The system is capable of making social sensing and exploiting the information arising on the Internet through user-generated contents, and it is equipped with a conversational engine that manages the interaction with the human user. The whole approach can be applied either by a human user or a robot, which remains a future application to be further improved in the context of our proposed system.																	1432-7643	1433-7479				JUL	2020	24	13					9663	9682		10.1007/s00500-019-04478-2													
J								A cuckoo search optimization-based forward consecutive mean excision model for threshold adaptation in cognitive radio	SOFT COMPUTING										Adaptive threshold; Autonomous; Cognitive radio; FCME; Metaheuristic algorithm; Parameter tuning	ENERGY; PERFORMANCE; ALGORITHM; NETWORKS	The forward consecutive mean excision (FCME) algorithm is one of the most effective adaptive threshold estimation algorithms presently deployed for threshold adaptation in cognitive radio (CR) systems. However, its effectiveness is often limited by the manual parameter tuning process and by the lack of prior knowledge pertaining to the actual noise distribution considered during the parameter modeling process of the algorithm. In this paper, we propose a new model that can automatically and accurately tune the parameters of the FCME algorithm based on a novel integration with the cuckoo search optimization (CSO) algorithm. Our model uses the between-class variance function of the Otsu's algorithm as the objective function in the CSO algorithm in order to auto-tune the parameters of the FCME algorithm. We compared and selected the CSO algorithm based on its relatively better timing and accuracy performance compared to some other notable metaheuristics such as the particle swarm optimization, artificial bee colony (ABC), genetic algorithm, and the differential evolution (DE) algorithms. Following close performance values, our findings suggest that both the DE and ABC algorithms can be adopted as favorable substitutes for the CSO algorithm in our model. Further simulation results show that our model achieves reasonably lower probability of false alarm and higher probability of detection as compared to the baseline FCME algorithm under different noise-only and signal-plus-noise conditions. In addition, we compared our model with some other known autonomous methods with results demonstrating improved performance. Thus, based on our new model, users are relieved from the cumbersome process involved in manually tuning the parameters of the FCME algorithm; instead, this can be done accurately and automatically for the user by our model. Essentially, our model presents a fully blind signal detection system for use in CR and a generic platform deployable to convert other parameterized adaptive threshold algorithms into fully autonomous algorithms.																	1432-7643	1433-7479				JUL	2020	24	13					9683	9704		10.1007/s00500-019-04481-7													
J								Trade credit policy of an inventory model with imprecise variable demand: an ABC-GA approach	SOFT COMPUTING										Inventory; Imprecise variable demand; Fuzzy differential equation; Artificial bee colony (ABC); Genetic algorithm (GA)	ECONOMIC ORDER QUANTITY; BEE COLONY ALGORITHM; FUZZY DIFFERENTIAL-EQUATION; EOQ MODEL; PERMISSIBLE DELAY; SUPPLY CHAIN; DETERIORATING ITEM; 2 WAREHOUSES; LEAD-TIME; PRICE	In this research work, an inventory model with fuzzy promotional effort induced dynamic demand under two level partial trade credit policy has been developed in an imprecise planning horizon. Here, it is assumed that in the planning horizon a retailer completed a finite number full cycles. In each of the retailer's cycle, a wholesaler offers a credit period to the retailer on the full purchased amount and in turn the retailer offers a credit period to its customers on a part of his/her purchased amount. The imprecise marketing demand is influenced by the retailer's fuzzy promotional effort, customers' credit period, customers' credit amount and retail selling price. Goal of this study is to find the optimal business strategy for the retailer with respect to his/her total fuzzy financial gain from the system. Due to imprecise nature of the demand, the problem is mathematically represented following fuzzy differential equation and fuzzy Riemann integration and alpha-cut of the entire fuzzy gain from the system is derived. Its graded mean integration representation is computed and optimized with respect to customer's credit amount credit period, and retailer's order quantity for most appropriate marketing decision. Hence, the problem reduced to a multivariate crisp optimization problem and a heuristic, multichoice artificial bee genetic algorithm (MCABGA) has been proposed for it. The efficiency of MCABGA is tested against some other existing artificial bee colony variants using a list of benchmark test functions available in the literature. The model is illustrated with some hypothetical test problems and some managerial insights are outlined. Finally, a conclusion is drawn and some future research directions are proposed.																	1432-7643	1433-7479				JUL	2020	24	13					9857	9874		10.1007/s00500-019-04502-5													
J								Some distance measures for type 2 hesitant fuzzy sets and their applications to multi-criteria group decision-making problems	SOFT COMPUTING										Hesitant fuzzy set; Type 2 hesitant fuzzy set; Distance measure; Decision making; TOPSIS method	LOGIC SYSTEMS; OPERATORS; MODELS; MCDM	The fuzzy set has an important role in the modeling of uncertainties. However, the fuzzy set is not sufficient in modeling of the problems, when the decision makers do not have the same opinion about membership degree of an element. To overcome this problem, the concept of hesitant fuzzy set was defined by Torra and Narukawa. Recently, the concept of the type 2 hesitant fuzzy set was defined by Feng and a ranking method among elements of a type 2 hesitant fuzzy element was given. In this paper, firstly, we point out some shortcomings in the ranking method given by Feng and then we give a new ranking method among elements of a type 2 hesitant fuzzy element. The distance and similarity measures are the effective mathematical tools to solve the problems such as medical diagnosis, decision making, pattern recognition and marketing strategy selection. Therefore, we introduce some distance measure methods between two type 2 hesitant fuzzy sets based on Hamming, Euclidean and Hausdorff distance measures. We obtain some properties of the proposed distance measure methods. We also develop a multi-criteria group decision-making method by integrating the TOPSIS method and the proposed distance measure methods under the type 2 hesitant fuzzy environment. Furthermore, we present a numerical example of multi-criteria group decision-making problem to choose the best alternative among firms to invest in order to illustrate the process and validate of the proposed method.																	1432-7643	1433-7479				JUL	2020	24	13					9965	9980		10.1007/s00500-019-04509-y													
J								Development of an intuitionistic fuzzy ranking model for nontraditional machining processes	SOFT COMPUTING										Nontraditional machining approaches; Multi-criteria decision making (MCDM); Intuitionistic fuzzy NTMP ranking model; Triangular fuzzy NTMP ranking model	DECISION-MAKING; SELECTION; SYSTEM; AHP	Nontraditional machining processes (NTMPs) are capable of processing very small parts, producing intricate geometries, operating on very narrow machining areas and machining high strength materials. These capabilities lead to a very diverse and large application area for NTMPs. Such a diverse and large application area along with more than one hundred NTMPs requires development of systematic and comprehensive models to help manufacturing engineers in their NTMP selection decisions. Furthermore, fuzzy models instead of crisp ones are being used in the literature in recent years to represent preferences of decision makers more realistically. This study proposes intuitionistic and triangular fuzzy NTMP ranking models and compares their ranking results with the crisp ranking model. The comparisons show that there are statistically significant differences among all three ranking models' NTMP ranking results.																	1432-7643	1433-7479				JUL	2020	24	13					10095	10110		10.1007/s00500-019-04523-0													
J								Writer identification system for pre-segmented offline handwritten Devanagari characters using k-NN and SVM	SOFT COMPUTING										Forensic record examination; Writer identification; Devanagari character recognition; Feature extraction; k-NN; SVM	ALGORITHM; RECOGNITION	A biometric identification system based on single and multiple modalities has been an evolving concept for solving criminal issues, security and privacy maintenance and for checking the authentication of an individual. The writer identification system is a type of biometric identification in which handwriting of an individual is taken as a biometric identifier. It is a system in which the writer can be identified based on his handwritten text. These systems employ machine learning and pattern recognition algorithms for the generation of a framework. In this paper, the authors have presented a novel system for the writer identification based upon the pre-segmented characters of Devanagari script and also presenting comprehensive state-of-the-art work. The experiment is performed on the corpus consisting of five copies of each character of Devanagari script written by 100 different writers, selected randomly at the public places and consisting of total 24,500 samples of Devanagari characters. Four feature extraction methodologies such as zoning, diagonal, transition and peak extent-based features and classification methods such as k-NN and linear SVM are used with identification accuracy of 91.53% when using zoning, transition and peak extent-based features with a linear SVM classifier.																	1432-7643	1433-7479				JUL	2020	24	13					10111	10122		10.1007/s00500-019-04525-y													
J								Uncertain interval programming model for multi-objective multi-item fixed charge solid transportation problem with budget constraint and safety measure	SOFT COMPUTING										Multi-objective multi-item fixed charge solid transportation problem; Budget constraint; Safety measure; Deterioration of item; Theories of interval		This paper presents uncertain interval programming models for multi-objective multi-item fixed charge solid transportation problem with budget constraint and safety measure (MOMIFCSTPBCSM). The human languages usually involve imperfect or unknown information and are in the lack of certainty, and often, it is impossible to exactly describe an existing state or a future outcome. In using the probability theory, we must have enough historical information to estimate the probability distributions and in the case of fuzzy theory, we must have a trustworthy membership function, which is not easy to do. Thus, we often estimate the degree of belief with some hesitation that each condition may occur. To deal with such a situation, the uncertain interval theory may be very useful. Based on these facts, the parameters of the formulated problem are chosen as uncertain intervals. We consider unit transportation costs, fixed charges, transportation times, deterioration of items, supplies at origins, demands at destinations, conveyance capacities, budget at each destination, selling prices and purchasing costs, and we assume the safety factor and the desired safety measure are interval uncertain parameters. To formulate the proposed MOMIFCSTPBCSM, we use interval theory and uncertain programming techniques to develop two different models: an Expected Value Model and a Chance-Constrained Model. The equivalent deterministic models are formulated and solved using a linear weighted method, a fuzzy programming method and the goal programming method.																	1432-7643	1433-7479				JUL	2020	24	13					10123	10147		10.1007/s00500-019-04526-x													
J								Model uncertainty quantification for diagnosis of each main coronary artery stenosis	SOFT COMPUTING										Data mining; Machine learning; Coronary artery disease; SVM; Gini index; LAD; Feature selection	NEURAL-NETWORK; EXPERT-SYSTEM; 2ND OPINIONS; DISEASE	One of the main causes of death in the world is coronary artery disease (CAD). CAD occurs when there is stenosis in one or more of the three major coronary arteries: right coronary artery (RCA), left circumflex (LCX) artery, and left anterior descending (LAD) artery. The gold standard or CAD diagnosis is angiography, but it is invasive, costly, and time consuming. Therefore, researchers continually seek new machine learning methods that can screen for CAD non-invasively. For reliable and cost-effective CAD diagnosis, several algorithms have been developed. Most prior studies analyzed the presence or absence of CAD in a dichotomous manner. Herein, we studied the more complex problem of classification of stenosis in individual LAD, LCX, and RCA by applying machine learning algorithms on the Z-Alizadeh Sani dataset that comprised 303 subjects, each with 54 features. In addition, our new methodology is developed to handle model uncertainty in the prediction of individual artery stenosis. It uses the hyperplane distance from a sample and accuracy rate of the classifier during the training phase to enhance its performance. Our results demonstrate high diagnostic performance of the proposed method for diagnosis of stenosis in individual RCA, LCX, and LAD, achieving accuracy rates of 82.67%, 83.67% and 86.43%, respectively. This is the best performance of ML techniques applied to the Z-Alizadeh Sani dataset.																	1432-7643	1433-7479				JUL	2020	24	13					10149	10160		10.1007/s00500-019-04531-0													
J								Deep perceptron neural network with fuzzy PID controller for speed control and stability analysis of BLDC motor	SOFT COMPUTING										BLDC motor; Fuzzy system; Deep learning neural network; Perceptron model; Stability; PID controller; Speed control; Multi-swarm particle swarm optimization; Lyapunov stability	LYAPUNOV STABILITY; IMPLEMENTATION; STRATEGY	Speed regulation is one of the significant characteristics to be adopted in the field of brushless DC motor drive for effective and accurate speed and position control operations. In this paper, stability analysis and performance characteristics of brushless direct current motor are studied and implemented with a new deep learning neural network-fuzzy-tuned proportional integral derivative (PID) speed controller. Deep learning architecture is designed for the multi-layer perceptron network, and the output from the neural module fires the rules of the fuzzy inference system mechanism. The parameters of deep perceptron neural network (DPNN) are tuned for near optimal solutions using the unified multi-swarm particle swarm optimization, and in turn the optimized DPNN selects the parameters of the fuzzy inference system. Deep learning neural network with the fuzzy inference system tunes the gain values of the PID controller and performs an effective speed regulation. The performance characteristics of the designed speed controller are tested for a step change in input speed and also for impulsive load disturbances. Further, the stability analysis of the new proposed controller is investigated with Lyapunov stability criterion by deriving the positive definite functions. The weight parameters of DPNN model and the number of rules of fuzzy system are tuned for their near optimal solutions using multi-swarm particle swarm optimization. From the results, it is well proven that the proposed controller is more stable and guarantees consistent performance than other considered controllers in all aspects. Simulation-based comparisons illustrate that the design methodologies outperform other controller designs from the literature.																	1432-7643	1433-7479				JUL	2020	24	13					10161	10180		10.1007/s00500-019-04532-z													
J								ACO-IM: maximizing influence in social networks using ant colony optimization	SOFT COMPUTING										Information diffusion; Influence maximization; Social networks; Ant colony optimization	INFLUENCE MAXIMIZATION; INFORMATION; MODELS	Online social networks play an essential role in propagating information, innovation, and ideas via word-of-mouth spreading. This word-of-mouth phenomenon leads to a fundamental problem, known as influence maximization (IM) or subset selection problem. The IM problem aims to identify a small subset of users, viz. seed nodes such that overall influence spread can be maximized. The seed selection problem is NP-hard, unfortunately. A greedy solution of IM problem is not sufficient due to the use of time-consuming Monte Carlo simulations, which is limited to small-scale networks. However, the greedy solution ensures a good approximation guarantee. In this paper, a local influence evaluation heuristic is adopted to approximate local influence within the two-hope area. With this heuristic, an expected diffusion value under the traditional diffusion models is evaluated. To optimize local influence evaluation heuristic, an influence maximization algorithm based on ant colony optimization (ACO-IM) is presented. ACO-IM redefines the representation and updates the rule of pheromone deposited by ants and heuristic information. The algorithm uses the probabilistic environment to avoid premature convergence. Finally, the experimental results show the superiority of the proposed algorithm. The statistical tests are also performed to distinguish the proposed method from the state-of-the-art methods.																	1432-7643	1433-7479				JUL	2020	24	13					10181	10203		10.1007/s00500-019-04533-y													
J								A hybrid OpenFlow with intelligent detection and prediction models for preventing BGP path hijack on SDN	SOFT COMPUTING										BGP; SDN; Inter-domain routing; Autonomous systems; CUSUM; Machine learning; Pattern Sequence Forecast; Network security	ALGORITHM; SECURITY	The Border Gateway Protocol (BGP) is a path vector protocol whose fundamental aim is to exchange the information across the Internet, which directs data between autonomous systems. The significant drawback of the BGP is that it does not address security; path hijacking is one of the top-rated cyber hijacks. Existing methods such as sBGP, soBGP and PGBGP have focused more on detecting path hijacking rather than preventing. Hence, we propose an intelligent model to detect abnormal behavior of a network and to predict and prevent BGP path hijacking (DPPBGP) in software-defined networks. The main objective of our proposed model is to reduce detection time and the controller workload with SFlow-integrated OpenFlow. Three modules of our model are as follows: (1) Based on the abnormal behavior of the network, we evaluated the statistics. We use the statistic features in the cumulative sum abnormal detection algorithm to detect abnormal behavior and flows proficiently and perfectly with less detection time. (2) An intelligent machine learning approach knows as a Pattern Sequence Forecasting algorithm is used to forecast the behavior of the network. (3) After the detection or the forecast of abnormality, path hijack is prevented by killing the appropriate PID based on SFlow analyzer. Simulation results show how large the network of this model can perform accurately and effectively.																	1432-7643	1433-7479				JUL	2020	24	13					10205	10214		10.1007/s00500-019-04534-x													
J								Enhancing PROMETHEE method with intuitionistic fuzzy soft sets	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										decision making; intuitionistic fuzzy set; outranking; PROMETHEE; soft set	ATTRIBUTE DECISION-MAKING; MAXIMIZING DEVIATION METHOD; VALUES; RULE	The notion of intuitionistic fuzzy soft sets (IFSSs) provides an effective tool for solving multiple attribute decision making with intuitionistic fuzzy information. The most crucial issue in decision making based on IFSSs is how to derive the ranking of alternatives from the information quantified in terms of intuitionistic fuzzy values. In this study, we propose a new extension of the preference ranking organization method for enrichment evaluation (PROMETHEE), by taking advantage of IFSSs. In addition to presenting a myriad of new notions, such as intuitionistic fuzzy membership (or nonmembership) deviation matrices, intuitionistic fuzzy membership (or nonmembership) preference matrices, and aggregated intuitionistic fuzzy preference matrices, we put more emphasis on the construction of three distinct preference structures and related utility functions on the corresponding weakly ordered sets by considering the positive, negative, and net flows of the alternatives based on the aggregated intuitionistic fuzzy preference matrix. We present a new algorithm for solving multiple attribute decision-making problems with the extended PROMETHEE method based on IFSSs. Moreover, a benchmark problem concerning risk investment is investigated to give a comparative analysis and show the feasibility of our approach.																	0884-8173	1098-111X				JUL	2020	35	7					1071	1104		10.1002/int.22235													
J								Sensor fusion based on Dempster-Shafer theory of evidence using a large scale group decision making approach	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										classification; Dempster-Shafer theory of evidence; large scale group decision making; sensor fusion; sensor weighting	TARGET CLASSIFICATION; REASONING APPROACH; BELIEF; ALGORITHM; COMBINATION; UNCERTAINTY; RELIABILITY; TBM	In group decision making (GDM), the quality of the solution relies primarily on the quality and the expertize of decision makers. At that point, deriving the weights, which reflects their importance or perceived reliability of decision makers, presents as a new challenge. In addition to that, the uncertainty is also a common problem for GDM. These problems are also faced in the sensor fusion problem where information from multiple sources must be aggregated. Therefore, in this study, a large scale GDM approach for sensor fusion is proposed. Since the proposed method is a clustering-based method, it provides acceptable results in the sensor networks consisting of multiple sensors. It can work under uncertainty as a result of converting the raw data obtained from sensors to the basic probability assignments. It also considers the reliability of the sensors clusters by assigning three objective weights. In addition to these objective weights, the proposed method enables to assign subjective weights to integrate supervisors/intelligence analyst experiences and knowledge in the problem field. The applicability and the validity of the proposed method are checked through two real classification data sets: ionosphere and forest type mapping data set. Experiments show that the classification rate is increased significantly when the proposed method is applied to two data sets. Finally, effect of extension parameter, objective weights, reliability threshold, number of clusters and clustering method on the classification rate and the detection probability are examined, and future studies are provided in conclusion.																	0884-8173	1098-111X				JUL	2020	35	7					1126	1162		10.1002/int.22237													
J								Correlation coefficients of dual type-2 hesitant fuzzy sets and their applications in clustering analysis	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										correlation coefficient; decision making; dual type-2 hesitant fuzzy sets; hesitant fuzzy sets; type-2 fuzzy set	AGGREGATION OPERATORS; ALGORITHM	Recently, many researchers have studied some types of sets which are extension of fuzzy sets widely. Some of them are interval-valued fuzzy set, intuitionistic fuzzy sets, type-2 fuzzy sets, type-n fuzzy sets, hesitant fuzzy sets (HFSs), dual fuzzy sets, and neutrosophic sets. In solving decision-making problems, these sets have more advantages than classical sets. In this paper, we introduce a new concept called dual type-2 hesitant fuzzy sets (DT2HFSs) by combining concepts of the dual HFS and the type-2 fuzzy set. Then we give correlation coefficient formulas and weighted correlation coefficient formulas between two DT2HFSs and obtain some results related to the proposed correlation coefficient formulas. On the basis of the proposed correlation coefficient formulas, we develop the clustering method under DT2HF environment. Finally, we present an application of the proposed method for a problem to illustrate the process and validate the proposed method.																	0884-8173	1098-111X				JUL	2020	35	7					1200	1229		10.1002/int.22239													
J								On the complexity of reasoning about opinion diffusion under majority dynamics	ARTIFICIAL INTELLIGENCE										Opinion diffusion; Stability; Computational complexity; Tree decompositions	INFLUENCE MAXIMIZATION; COMPETITIVE INFLUENCE; THRESHOLD MODELS; GRAPH MINORS; APPROXIMATION; MONOPOLIES; APPROXIMABILITY; CONSENSUS	We study opinion diffusion on social graphs where agents hold binary opinions and where social pressure leads them to conform to the opinion manifested by the majority of their neighbors. We provide bounds relating the number of agents that suffice to spread an opinion to all other agents with the number of required propagation steps. Bounds are established constructively, via polynomial time algorithms that identify the agents that must act as seeds. In particular, we show that, on any given social graph G =(N, E), it is possible to efficiently identify a set formed by half of the agents that can lead to consensus in min{right perpendicular vertical bar N vertical bar/2left perpendicular, even(G) + 1} propagation steps, where even(G) is the number of agents with an even number of neighbors in G. The result marks the boundary of tractability, since we show that the existence of sets of seeds consisting of less than half of the agents depends on certain features of the underlying graphs, which are NP-hard to identify. In fact, other NP-hardness results emerge from our analysis. In particular, by closing a problem left open in the literature, we show that it is intractable to decide whether further stable configurations exist in addition to the "consensus" ones (where all agents hold the same opinion). Eventually, for all these problems related to reasoning about opinion diffusion, we show that islands of tractability can be identified by focusing on classes of tree-like social graphs. (C) 2020 Elsevier B.V. All rights reserved.																	0004-3702	1872-7921				JUL	2020	284								103288	10.1016/j.artint.2020.103288													
J								Knowing the price of success	ARTIFICIAL INTELLIGENCE										Strategies; Modal logic; Know-how; Resources; Axiomatization; Completeness; Knowledge; Epistemic logic	MODEL-CHECKING; KNOW-HOW; LOGIC	If an agent, or a coalition of agents, knows that it has a strategy to achieve a certain outcome, it does not mean that the agent knows what the strategy is. Even if the agent knows what the strategy is, she might not know the price of executing this strategy. The article considers modality "the coalition not only knows the strategy, but also knows an upper bound on the price of executing the strategy". The main technical result is a sound and complete bimodal logical system that describes the interplay between this modality and the distributed knowledge modality. (C) 2020 Elsevier B.V. All rights reserved.																	0004-3702	1872-7921				JUL	2020	284								103287	10.1016/j.artint.2020.103287													
J								On quasi-inconsistency and its complexity	ARTIFICIAL INTELLIGENCE										Knowledge representation; Inconsistency; Computational complexity		We address the issue of analyzing potential in consistencies in knowledge bases. This refers to knowledge bases that contain rules which will always be activated together, and the knowledge base will become inconsistent, should these rules be activated. We investigate this problem in the context of the industrial use-case of business rule management, where it is often required that sets of (only) rules are analyzed for potential inconsistencies, e.g., during business rule modelling. To this aim, we introduce the notion of quasi-inconsistency, which is a formalization of the above-mentioned problem of potential inconsistencies. We put a specific focus on the analysis of computational complexity of some involved problems and show that many of them are intractable. (C) 2020 Elsevier B.V. All rights reserved.																	0004-3702	1872-7921				JUL	2020	284								103276	10.1016/j.artint.2020.103276													
J								CPCES: A planning framework to solve conformant planning problems through a counterexample guided refinement	ARTIFICIAL INTELLIGENCE										Conformant planning; Classical planning; Propositional satisfiability	COMPILING UNCERTAINTY; ALGORITHMS	We introduce CPCES, a novel planner for the problem of deterministic conformant planning. CPCES solves the problem by producing candidate plans based on a sample of the initial belief state, searching for counter-examples to these plans, and assigning these counterexamples to the sample, until a valid plan has been produced or the problem has been proved unfeasible. On top of providing a means to compute a conformant plan, the sample can also be understood as a justification for the plan being found, or relevant reasons why a plan cannot be found. We study the theoretical properties that CPCES enjoys-correctness, completeness, and optimality-and how the several variants of CPCES we describe differ in behaviour. Moreover, we establish a theoretical connection between the CPCES framework and well-known concepts from the literature such as tags and width. With this connection we prove the worst case complexity for some variants of CPCES. Finally, we show how CPCES can be used in a more incremental fashion by learning sequencing of actions from the previous plan being found. Such a technique mimics the use of macro-operators, widely used in automated planning to speedup resolution. Our theoretical analysis is accompanied with a thorough experimental evaluation of the (many) possible incarnations of CPCES. This not only proves our theoretical findings from an empirical perspective, but also shows that CPCES is able to handle problems that have been traditionally hard to solve by the existing conformant planners, whilst remaining competitive over "easier" conformant planning problems. Importantly, CPCES is able to prove many unsolvable conformant planning problems as such, extending substantially the reach of conformant planners. (C) 2020 Elsevier B.V. All rights reserved.																	0004-3702	1872-7921				JUL	2020	284								103271	10.1016/j.artint.2020.103271													
J								Combining gaze and AI planning for online human intention recognition	ARTIFICIAL INTELLIGENCE										Intention recognition; Gaze; Planning	HUMAN-ROBOT INTERACTION; EYE-TRACKING; ATTENTION; PREDICTION; MOVEMENTS	Intention recognition is the process of using behavioural cues, such as deliberative actions, eye gaze, and gestures, to infer an agent's goals or future behaviour. In artificial intelligence, one approach for intention recognition is to use a model of possible behaviour to rate intentions as more likely if they are a better 'fit' to actions observed so far. In this paper, we draw from literature linking gaze and visual attention, and we propose a novel model of online human intention recognition that combines gaze and model-based AI planning to build probability distributions over a set of possible intentions. In human-behavioural experiments (n = 40) involving a multi-player board game, we demonstrate that adding gaze-based priors to model-based intention recognition improved the accuracy of intention recognition by 22% (p < 0.05), determined those intentions approximate to 90 seconds earlier (p < 0.05), and at no additional computational cost. We also demonstrate that, when evaluated in the presence of semi-rational or deceptive gaze behaviours, the proposed model is significantly more accurate (9% improvement) (p < 0.05) compared to a model-based or gaze only approaches. Our results indicate that the proposed model could be used to design novel human-agent interactions in cases when we are unsure whether a person is honest, deceitful, or semi-rational. (C) 2020 Elsevier B.V. All rights reserved.																	0004-3702	1872-7921				JUL	2020	284								103275	10.1016/j.artint.2020.103275													
J								Fundamental sampling patterns for low-rank multi-view data completion	PATTERN RECOGNITION										Multi-view learning; Low-rank matrix completion; Sampling pattern; Sampling rate; Non-convex optimization; Rank decomposition	SUBSPACE	We consider the multi-view data completion problem, i.e., to complete a matrix U = [U-1 vertical bar U-2] where the ranks of U, U-1, and U-2 are given. In particular, we investigate the fundamental conditions on the sampling pattern, i.e., locations of the sampled entries for finite completability of such a multi-view data given the corresponding rank constraints. We provide a geometric analysis on the manifold structure for multi-view data to incorporate more than one rank constraint. We derive a probabilistic condition in terms of the number of samples per column that guarantees finite completability with high probability. Finally, we derive the guarantees for unique completability. Numerical results demonstrate reduced sampling complexity when the multi-view structure is taken into account as compared to when only low-rank structure of individual views is taken into account. Then, we propose an apporach using Newton's method to almost achieve these information-theoretic bounds for mulit-view data retrieval by taking advantage of the rank decomposition and the analysis in this work. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107307	10.1016/j.patcog.2020.107307													
J								IOS-Net: An inside-to-outside supervision network for scale robust text detection in the wild	PATTERN RECOGNITION										Text detection; Various sizes; Diverse aspect ratios; Inside-to-outside supervision; Position-sensitive segmentation	VIDEO; IMAGES	Accurately detecting scene text is a challenging task due to perspective distortion, scale variance, varied orientations, uneven illumination. Among them, scale variance has always been a core issue and generally involves two types: various size and diverse aspect ratios of the text regions. In contrast to most existing approaches focusing on addressing one type of scale variance, this paper presents a novel inside-to-outside supervision network (IOS-Net) that can well tackle both two. Specifically, we design a hierarchical supervision module (HSM), which consists of a new inception unit with parallel asymmetric convolution and a skip-layer fusion structure. Inside the HSM, we introduce hierarchical supervision into the new inception unit to effectively capture the texts with diverse aspect ratios. Outside the HSM, we adopt multiple-scale supervision on the stacked HSMs to accurately detect the texts with various sizes. Moreover, a position-sensitive segmentation is used to enhance the representation of difficult text objects and the discrimination of adjacent ones. The proposed method achieves state-of-the-art performance on representative public benchmarks, reaching 86% F-score and 11.5 frames per second (FPS) on the ICDAR 2015 incidental text dataset, 47% F-score and 16.1 FPS on the COCO-Text dataset, 69% F-score and 11.7 FPS on the ICDAR 2013 video text dataset. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107304	10.1016/j.patcog.2020.107304													
J								Online tracking of ants based on deep association metrics: method, dataset and evaluation	PATTERN RECOGNITION										Ant tracking; ResNet model; Mahalanobis distance; Appearance descriptors	INDIVIDUALS	Tracking movement of insects in a social group (such as ants) is challenging, because the individuals are not only similar in appearance but also likely to perform intensive body contact and sudden movement adjustment (start/stop, direction changes). To address this challenge, we introduce an online multi-object tracking framework that combines both the motion and appearance information of ants. We obtain the appearance descriptors by using the ResNet model for offline training on a small (N=50) sample dataset. For online association, a cosine similarity metric computes the matching degree between historical appearance sequences of the trajectory and the current detection. We validate our method in both indoor (lab setup) and outdoor video sequences. The results show that our model obtains 99.3% % +/- 0.5% MOTA and 91.9% +/- 2.1% MOTP across 24,050 testing samples in five indoor sequences, with real-time tracking performance. In an outdoor sequence, we achieve 99.3% MOTA and 92.9% MOTP across 22,041 testing samples. The datasets and code are made publicly available for future research in relevant domains. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107233	10.1016/j.patcog.2020.107233													
J								Automatic characteristic-calibrated registration (ACC-REG): Hippocampal surface registration using eigen-graphs	PATTERN RECOGNITION										Feature extraction; Feature correspondence; Surface registration; Surface deformation; Eigen-graph		In this paper, we propose an efficient algorithm, the ACC-REG, to automatically extract intrinsic key characteristics on hippocampal mesh surfaces and hence compute an accurate registration mapping between them. Given a pair of hippocampal surface mesh, the proposed algorithm constructs the eigen-graphs, an intrinsic feature on the surface, on each surface as its representative. The eigen-graphs are then calibrated along the longitudinal direction of the hippocampal surfaces. Accurately corresponded intrinsic characteristics on each hippocampus can thus be extracted. As a result, the two surfaces can be registered with improved accuracy and low computation cost. Experiments on ADNI data demonstrate the effectiveness of the proposed ACC-REG model over existing methods. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107142	10.1016/j.patcog.2019.107142													
J								Graph convolutional network with structure pooling and joint-wise channel attention for action recognition	PATTERN RECOGNITION										Graph convolutional network; Structure graph pooling; Joint-wise channel attention		Recently, graph convolutional networks (GCNs) have achieved state-of-the-art results for skeleton based action recognition by expanding convolutional neural networks (CNNs) to graphs. However, due to the lack of effective feature aggregation method, e.g. max pooling in CNN, existing GCN-based methods only learn local information among adjacent joints and are hard to obtain high-level interaction features, such as interactions between five parts of human body. Moreover, subtle differences of confusing actions often hide in specific channels of key joints' features, this kind of discriminative information is rarely exploited in previous methods. In this paper, we propose a novel graph convolutional network with structure based graph pooling (SGP) scheme and joint-wise channel attention UCA) modules. The SGP scheme pools the human skeleton graph according to the prior knowledge of human body's typology. This pooling scheme not only leads to more global representations but also reduces the amount of parameters and computation cost. The JCA module learns to selectively focus on discriminative joints of skeleton and pays different levels of attention to different channels. This novel attention mechanism enhance the model's ability to classify confusing actions. We evaluate our SGP scheme and JCA module on three most challenging skeleton based action recognition datasets: NTU-RGB+D, Kinetics-M, and SYSU-3D. Our method outperforms the state-of-art methods on three benchmarks. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107321	10.1016/j.patcog.2020.107321													
J								Automatic soccer field of play registration	PATTERN RECOGNITION										Soccer; Football; Field of play; Playing field; Registration; Segmentation; Line segment detection; Line classification; Validation	CAMERA CALIBRATION; ALGORITHM	This paper proposes a strategy for the automatic registration of soccer images on a model of the field of play. First, a robust and efficient preprocessing is applied to discard the areas of the image that do not belong to the field of play and eliminate most edge points that are not part of the line marks. Then, a novel probabilistic decision tree is used to identify the most probable classification for the set of all the straight lines in the image. Additionally, the line surrounding the center circle is also modeled for providing results when only few straight lines are visible. Finally, a three-step analysis stage is applied to ensure the validity of the results. To assess its quality, the strategy has been tested on several sequences corresponding to three stadiums with different characteristics. The results obtained have shown that the registration is successful in most images (94%). (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107278	10.1016/j.patcog.2020.107278													
J								Open-set face identification with index-of-max hashing by learning	PATTERN RECOGNITION										Secure open-set face identification; Index-of-max hashing; Fusion; Privacy		Large-scale face identification or 1-to-N matching where N is huge, plays a vital role in biometrics and surveillance. The system demands accurate and speedy matching where compact facial feature representation and a simple matcher are favored. On the other hand, most research considers closed-set identification that assumes that all identities of probe samples are enclosed in the gallery. On the contrary, openset identification expects that some probe identities are not known to the system. This setup poses an additional challenge, where the system should be able to reject those probes that correspond to unknown identities. In this paper, we address the large-scale open-set face identification problem with a compact facial representation that is based on the index-of-maximum (IoM) hashing, which was designed for biometric template protection. To be specific, the existing random IoM hashing is advanced to a data-driven based hashing technique, where the hashed face code can be made compact and matching can be easily performed by the Hamming distance, which can offer highly efficient matching. Furthermore, since IoM hashing transforms the original facial features non-invertibly, the privacy of users can also be preserved. Along with IoM hashed face code, we explore several fusion strategies to address the open-set face identification problem. The comprehensive evaluations are carried out with three large-scale unconstrained face datasets, namely LFW, VGG2 and UB-C. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107277	10.1016/j.patcog.2020.107277													
J								Circular object arrangement using spherical embeddings	PATTERN RECOGNITION										Combinatorial data analysis; Data sequencing; Circular seriation; Quadratic assignment problem; Spherical embeddings	QUADRATIC ASSIGNMENT; DIMENSIONALITY REDUCTION; PROXIMITY MATRICES; VISUALIZATION; ALGORITHM	We consider the problem of recovering a circular arrangement of data instances with respect to some proximity measure, such that nearby instances are more similar. Applications of this problem, also referred to as circular seriation, can be found in various disciplines such as genome sequencing, data visualization and exploratory data analysis. Circular seriation can be expressed as a quadratic assignment problem, which is in general an intractable problem. Spectral-based approaches can be used to find approximate solutions, but are shown to perform well only for a specific class of data matrices. We propose a bilevel optimization framework where we employ a spherical embedding approach together with a spectral method for circular ordering in order to recover circular arrangements of the embedded data. Experiments on real and synthetic datasets demonstrate the competitive performance of the proposed method. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107192	10.1016/j.patcog.2019.107192													
J								DevsNet: Deep Video Saliency Network using Short-term and Long-term Cues	PATTERN RECOGNITION										Video saliency detection; Spatiotemporal saliency; 3D convolution network (3D-ConvNet); Bidirectional convolutional long-short term memory network (B-ConvLSTM)	VISUAL-ATTENTION; OBJECT DETECTION; OPTIMIZATION; SEGMENTATION; FUSION; TREE	Recently, there have been various saliency detection methods proposed for still images based on deep learning techniques. However, the research on saliency detection for video sequences is still limited. In this study, we introduce a novel deep learning framework of saliency detection for video sequences, namely Deep Video Saliency Network (DevsNet). DevsNet mainly consists of two components: 3D Convolutional Network (3D-ConvNet) and Bidirectional Convolutional Long-Short Term Memory Network (B-ConvLSTM). 3D-ConvNet is constructed to learn short-term spatiotemporal information and the long-term spatiotemporal features are learned by B-ConvLSTM. The designed B-ConvLSTM can extract the temporal information not just from the previous video frames but also from the next frames, which demonstrates that the proposed model considers both the forward and backward temporal information. By combining the short-term and long-term spatiotemporal cues, the proposed DevsNet can extract saliency information for video sequences effectively and efficiently. Extensive experiments have been conducted to show that the proposed model can obtain better performance in spatiotemporal saliency prediction than the state-of-the-art models. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107294	10.1016/j.patcog.2020.107294													
J								Frobenius correlation based u-shapelets discovery for time series clustering	PATTERN RECOGNITION										Clustering; UShapelet; Correlation; Time series	UNCERTAINTY; KERNEL	An u-shapelet is a sub-sequence of a time series used for the clustering of time series datasets. The purpose of this paper is to discover u-shapelets on uncertain time series. To achieve this goal, we propose a dissimilarity score called FOTS whose computation is based on the eigenvector decomposition and the comparison of the autocorrelation matrices of the time series. This score is robust to the presence of uncertainty; it is not very sensitive to transient changes; it allows capturing complex relationships between time series such as oscillations and trends, and it is also well adapted to the comparison of short time series. The FOTS score is used with the Scalable Unsupervised Shapelet Discovery algorithm for the clustering of 63 datasets, and it has shown a substantial improvement in the quality of the clustering with respect to the Rand Index. This work defines a novel framework for the clustering of uncertain time series. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107301	10.1016/j.patcog.2020.107301													
J								Learning variable-length representation of words	PATTERN RECOGNITION										Word embedding; Compression and sparsity; Lexical semantics	NETWORK	A standard word embedding algorithm, such as 'word2vec', embeds each word as a dense vector of a preset dimensionality, the components of which are learned by maximizing the likelihood of predicting the context around it. However, as an inherent linguistic phenomenon, it is evident that there is a varying degree of difficulty in identifying words from their contexts. This suggests that a variable granularity in word vector representation may be useful to obtain sparser and more compressed word representations, requiring less storage space. To that end, in this paper, we propose a word vector training algorithm that uses a variable number of components to represent words. Given a text collection of documents, our algorithm, similar to the skip-gram approach of word2vec, learns to predict the context of a word given the current instance of a word. However, in contrast to skip-gram, which uses a static number of dimensions for each word vector, we propose to dynamically increase the dimensionality as a stochastic function of the prediction error. Our experiments with standard test collections demonstrate that our word representation method is able to achieve comparable (and sometimes even better) effectiveness than skip-gram word2vec, using a significantly smaller number of parameters (achieving compression ratio of around 65%). (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107306	10.1016/j.patcog.2020.107306													
J								Simplifying TugGraph using zipping algorithms	PATTERN RECOGNITION										Graph visualisation; TugGraph; Cluster hierarchy; Zipping algorithms	PATH-PRESERVING HIERARCHIES; GRAPH VISUALIZATION; SEGMENTATION; REPRESENTATION; SYSTEM	Graphs are an invaluable modelling tool in many domains, but visualising large graphs in their entirety can be difficult. Hierarchical graph visualisation - recursively clustering a graph's nodes to view it at a higher level of abstraction - has thus become popular. However, this can hide important information that a user needs to understand a graph's topology, e.g. nodes' neighbourhoods. TugGraph addressed this by 'separating out' a given node's neighbours from their hierarchy ancestors to visualise them independently. Its original implementation was straightforward, but copied parts of the hierarchy, making it slow and memory-hungry. An optimised later version, which we refer to as FastTug, avoided this, but at a cost in clarity. Optimising TugGraph without sacrificing clarity is difficult because of the need to keep every hierarchy node connected, a common challenge for graph hierarchy editing algorithms. Recently, this problem has been addressed by 'zipping' algorithms, multi-level split/merge algorithms that preserve hierarchy node connectedness and can be built upon for higher-level editing. In this paper, we generalise the original unzipping algorithms to implement SimpleTug, a simple, modular version of TugGraph that is easy to understand and implement, and even faster and more memory-efficient than FastTug. We formally prove its equivalence to FastTug, and show how both can be parallelised. Using our millipede hierarchical image segmentation system, we show experimentally that both the serial and parallel versions of SimpleTug are around 25% faster than their FastTug counterparts, whilst using considerably less memory. Finally, we discuss the interesting theoretical connections between TugGraph and zipping, and suggest ideas for further work. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107257	10.1016/j.patcog.2020.107257													
J								TVENet: Temporal variance embedding network for fine-grained action representation	PATTERN RECOGNITION										Fine-grained action representation; temporal variance embedding network (TVENet); joint optimization; temporal triplet loss; action search	DEEP; MODEL	With the breakthroughs in general action understanding, it has become an inevitable trend to analyze the actions in finer granularity. However, related researches have been largely hindered by the lack of fine-grained datasets and the difficulty of capturing subtle differences between fine-grained actions that are highly similar overall. In this paper, we address the above challenges by constructing a fine-grained action dataset, i.e., Figure Skating, which can be used for end-to-end network training and presenting a framework for the joint optimization of classification and similarity constraints. We propose to incorporate the triplet loss into the training of Convolutional Neural Network, which learns a mapping from fine-grained actions to a compact Euclidean space where distances directly correspond to a measure of action similarity. Triplet loss compels actions of distinct classes to have larger distances than actions of the same class. Besides, to boost the discrimination of the fine-grained actions, we further propose a temporal variance embedding network (TVENet) embedding temporal context variances into the feature embeddings during the joint network training. The experimental results on Figure Skating dataset, HMDB51 dataset as well as UCF101 dataset demonstrate the effectiveness of TVENet representation for fine-grained action search. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107267	10.1016/j.patcog.2020.107267													
J								An improved GrabCut on multiscale features	PATTERN RECOGNITION										Image segmentation; GrabCut; Multiscale feature; Total variation regularization; Inhomogeneity	FOREGROUND EXTRACTION; IMAGE; SEGMENTATION	the GrabCut can effectively extract the foreground according to features in a cartoon image; however, the performance is not so effective for a real image, because the feature extraction is independent of segmentation. To improve segmentation performance, this paper proposes an improved GrabCut which combines the segmentation and multiscale feature extraction into a unified model. In this model, the segmentation relies on multiscale features, and the multiscale features depend on multiscale decomposition. A novel total variation regularization is proposed in multiscale decomposition to preserve edges and remove the region inhomogeneity, by which the generalization of features for segmentation is improved. The features obtained by the multiscale decomposition are integrated into the segmentation process, and the foreground can be easily extracted from a proper scale. Experimental results indicate that, compared to the existing GrabCut and improved techniques, this method provides competitive performance in terms of the segmentation accuracy, while being insensitive to inhomogeneity. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107292	10.1016/j.patcog.2020.107292													
J								New fractional-order Legendre-Fourier moments for pattern recognition applications	PATTERN RECOGNITION										Color image descriptors; Pattern recognition; Rotation invariance; Fractional-order moments; Legendre-Fourier moments	COLOR IMAGES REPRESENTATION; ACCURATE COMPUTATION; ORTHOGONAL MOMENTS; MELLIN MOMENTS; INVARIANTS; MULTICHANNEL; SET	Orthogonal moments enable computer-based systems to discriminate between similar objects. Mathematicians proved that the orthogonal polynomials of fractional-orders outperformed their corresponding counterparts in representing the fine details of a given function. In this work, novel orthogonal fractional-order Legendre-Fourier moments are proposed for pattern recognition applications. The basis functions of these moments are defined and the essential mathematical equations for the recurrence relations, orthogonality and the similarity transformations (rotation and scaling) are derived. The proposed new fractional-order moments are tested where their performance is compared with the existing orthogonal quaternion, multi-channel and fractional moments. New descriptors were found to be superior to the existing ones in terms of accuracy, stability, noise resistance, invariance to similarity transformations, recognition rates and computational times. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107324	10.1016/j.patcog.2020.107324													
J								Novel dimensionality reduction approach for unsupervised learning on small datasets	PATTERN RECOGNITION										Unsupervised learning; Dimensionality reduction; PCA; F-transform; Image classification; Autoencoder	F-TRANSFORM; REGRESSION	We focus on an image classification task in which only several unlabeled images per class are available for learning and low computational complexity is required. We recall the state-of-the-art methods that are used to solve the task: autoencoder-based approaches and manifold-decomposition-based approaches. Next, we introduce our proposed method, which is based on a combination of the F-transform and (kernel) principal component analysis. F-transform significantly reduces the computation time of PCA and increases the robustness of PCA to translation, while PCA proposes more descriptive features. This combination performs 3D reduction: the F-transform reduces dimensionality over a single 2D image, while PCA reduces dimensionality through the whole set of processed images. Based on the benchmark results, our method may outperform deep-learning-based methods in limited settings. For completeness, we also address other image resampling algorithms that can be used instead of the F-transform, and we find that the F-transform is the most suitable. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107291	10.1016/j.patcog.2020.107291													
J								Bit-string representation of a fingerprint image by normalized local structures	PATTERN RECOGNITION										Bit-oriented; Fingerprint; Clustering; Fixed-length bit-string representation; Bit-training; Minutiae-based local structure	TEMPLATE PROTECTION; MINUTIAE; CODE; SECURITY	Conventional minutia-based fingerprint recognition requires a complicated geometric matching and hard to be adopted in the bit-string based cancellable biometrics or bio-encryption, as the minutia data representing a fingerprint image is geometrical, unordered and variable in size. In this paper, we propose a new method to represent a fingerprint image by an ordered and fixed-length bit-string to cope with those difficulties with providing a faster matching, compressibility and improved accuracy performance as well. Firstly, we devised a novel minutia-based local structure modeled by a mixture of 2D elliptical Gaussian functions to represent a minutia in the image pixel space. Then, each local structure was mapped to a point in a Euclidean space by normalizing the local structure by the number of minutiae in it. This simple yet crucial computation for converting the image space to the Euclidean-space enabled the fast dissimilarity computation of two local structures and all followed processes in our proposed method. A complementary texture-based local structure to the minutia-based local structure was also introduced, whereby both were compressed via principal component analysis and fused in the compressed Euclidean space. The fused local structures were then converted to a K-bit ordered string using the K-means clustering algorithm. This chain of computations with the sole use of Euclidean distance was vital for speedy and discriminative bit-string conversion. The accuracy was further improved by the finger-specific bit-training algorithm, in which two criteria were leveraged to select the useful bit positions for matching. Experiments were performed on Fingerprint Verification Competition (FVC) databases for comparisons with the existing techniques to show the superiority of the proposed method. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107323	10.1016/j.patcog.2020.107323													
J								Three-stream fusion network for first-person interaction recognition	PATTERN RECOGNITION										First-person vision; First-person interaction recognition; Three-stream fusion network; Three-stream correlation fusion; Camera ego-motion		First-person interaction recognition is a challenging task because of unstable video conditions resulting from the camera wearer's movement. For human interaction recognition from a first-person viewpoint, this paper proposes a three-stream fusion network with two main parts: three-stream architecture and three-stream correlation fusion. The three-stream architecture captures the characteristics of the target appearance, target motion, and camera ego-motion. Meanwhile the three-stream correlation fusion combines the feature map of each of the three streams to consider the correlations among the target appearance, target motion, and camera ego-motion. The fused feature vector is robust to the camera movement and compensates for the noise of the camera ego-motion. Short-term intervals are modeled using the fused feature vector, and a long short-term memory (LSTM) model considers the temporal dynamics of the video. We evaluated the proposed method on two public benchmark datasets to validate the effectiveness of our approach. The experimental results show that the proposed fusion method successfully generated a discriminative feature vector, and our network outperformed all competing activity recognition methods in first-person videos where considerable camera ego-motion occurs. (C) 2020 Published by Elsevier Ltd.																	0031-3203	1873-5142				JUL	2020	103								107279	10.1016/j.patcog.2020.107279													
J								Handling Gaussian blur without deconvolution	PATTERN RECOGNITION										Gaussian blur; Semi-group; Projection operator; Blur invariants; Image moments; Affine transformation; Combined invariants	DEGRADED IMAGE-ANALYSIS; COMBINED INVARIANTS; MOMENT INVARIANTS; FACE RECOGNITION; ORDER; SET	The paper presents a new theory of invariants to Gaussian blur. Unlike earlier methods, the blur kernel may be arbitrary oriented, scaled and elongated. Such blurring is a semi-group action in the image space, where the orbits are classes of blur-equivalent images. We propose a non-linear projection operator which extracts blur-insensitive component of the image. The invariants are then formally defined as moments of this component but can be computed directly from the blurred image without an explicit construction of the projections. Image description by the new invariants does not require any prior knowledge of the blur kernel parameters and does not include any deconvolution. The invariance property could be extended also to linear transformation of the image coordinates and combined affine-blur invariants can be constructed. Experimental comparison to three other blur-invariant methods is given. Potential applications of the new invariants are in blur/position invariant image recognition and in robust template matching. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107264	10.1016/j.patcog.2020.107264													
J								Exploring temporal consistency for human pose estimation in videos	PATTERN RECOGNITION										Video-based pose estimation; Convolution neural network; Temporal information	MODELS; OCCLUSION	In this paper, we introduce a method of exploring temporal information for estimating human poses in videos. The current state-of-the-art methods utilizing temporal information can be categorized into two major branches. The first category is a model-based method that captures the temporal information entirely by using a learnable function such as RNN or 3D convolution. However, these methods are limited in exploring temporal consistency, which is essential for estimating human joint positions in videos. The second category is the posterior enhancement method, where an independent post-processing step (e.g., using optical flow) is applied to enhance the prediction. However, operations such as optical flow estimation can be susceptible to the occlusion and motion blur problems, which will adversely affect the final performance. We propose a novel Temporal Consistency Exploration (TCE) module to address both shortcomings. Compared to previous approaches, the TCE module is more efficient as it captures the temporal consistency at the feature level without having to post-process and calculate extra optical flow. Further, to capture the rich spatial context in video data, we design a multi-scale TCE to explore the time consistency information at multi-scale spatial levels. Finally, a video-based pose estimation network is designed, which is based on the encoder-decoder architecture and extended with the powerful multiscale TCE module. We comprehensively evaluate the proposed model on two video datasets, Sub-JHMDB and Penn, and our model achieves state-of-the-art performance on both datasets. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107258	10.1016/j.patcog.2020.107258													
J								Learning shape and motion representations for view invariant skeleton-based action recognition	PATTERN RECOGNITION										Human action recognition; Skeleton sequence; Representation learning; View invariant; Geometric Algebra	MODEL	Skeleton-based action recognition is an increasing attentioned task that analyses spatial configuration and temporal dynamics of a human action from skeleton data, which has been widely applied in intelligent video surveillance and human-computer interaction. How to design an effective framework to learn discriminative spatial and temporal characteristics for skeleton-based action recognition is still a challenging problem. The shape and motion representations of skeleton sequences are the direct embodiment of spatial and temporal characteristics respectively, which can well address for human action description. In this work, we propose an original unified framework to learn comprehensive shape and motion representations from skeleton sequences by using Geometric Algebra. We firstly construct skeleton sequence space as a subset of Geometric Algebra to represent each skeleton sequence along both the spatial and temporal dimensions. Then rotor-based view transformation method is proposed to eliminate the effect of viewpoint variation, which remains the relative spatio-temporal relations among skeleton frames in a sequence. We also construct spatio-temporal view invariant model (STVIM) to collectively integrate spatial configuration and temporal dynamics of skeleton joints and bones. In STVIM, skeleton sequence shape and motion representations which mutually compensate are jointly learned to describe skeletonbased actions comprehensively. Furthermore, a selected multi-stream Convolutional Neural Network is employed to extract and fuse deep features from mapping images of the learned representations for skeleton-based action recognition. Experimental results on NTU RGB+D, Northwestern-UCLA and UTD-MHAD datasets consistently verify the effectiveness of our proposed method and the superior performance over state-of-the-art competitors. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107293	10.1016/j.patcog.2020.107293													
J								A concave optimization algorithm for matching partially overlapping point sets	PATTERN RECOGNITION										Concave optimization; Point matching; Branch-and-bound; Linear assignment; Global optimization	MIXTURE MODEL; REGISTRATION	Matching partially overlapping point sets is a challenging problem in computer vision. To achieve this goal, we model point matching as a mixed linear assignment - least square problem. By eliminating the transformation variable, we reduce the minimization problem to a concave optimization problem with the property that the objective function can be converted into a form with few nonlinear terms. We then use a heuristic variant of the branch-and-bound algorithm for optimization where convergence of the upper bound is used as the stopping criterion. We also propose a new lower bounding scheme which involves solving a k-cardinality linear assignment problem. Two cases of transformations, transformation output being linear with respect to parameters and 2D/3D similarity transformations, are discussed, resulting in ability to handle unknown arbitrary translation and similarity, respectively. Experimental results demonstrate better robustness of the algorithm over state-of-the-art methods. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107322	10.1016/j.patcog.2020.107322													
J								Webly-supervised learning for salient object detection	PATTERN RECOGNITION										Salient object detection; Webly-supervised learning; Deep learning	FRAMEWORK; FUSION	End-to-end training of a deep CNN-Based model for salient object detection usually requires a huge number of training samples with pixel-level annotations, which are costly and time-consuming to obtain. In this paper, we propose an approach that can utilize large amounts of web data for learning a deep salient object detection model. With thousands of images collected from the Web, we first employ several bottom-up saliency detection techniques to generate salient object masks for all images, and then use a novel quality evaluation method to pick out a subset of images with reliable masks for training. After that, we develop a self-training approach to boost the performance of our initial network, which iterates between the network training process and the training set updating process. Importantly, different from existing webly-supervised or weakly-supervised methods, our approach is able to automatically select reliable images for network training without requiring any human intervention (e.g., dividing images into different difficulty levels). Results of extensive experiments on several widely-used benchmarks demonstrate that our method has achieved state-of-the-art performance. It significantly outperforms existing unsupervised and weakly-supervised salient object detection methods, and achieves competitive or even better performance than fully supervised approaches. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107308	10.1016/j.patcog.2020.107308													
J								Shape-from-focus reconstruction using nonlocal matting Laplacian prior followed by MRF-based refinement	PATTERN RECOGNITION										Shape from focus; Depth reconstruction; Matting Laplacian; Image denoising; Markov random field; Edge-preserving	3D SHAPE; IMAGE; DEPTH; RECOVERY	In this paper, we address the problem of depth recovery from a sequence of multi-focus images, known as shape-from-focus (SFF). The conventional SFF techniques typically exhibit poor performance over textureless regions, and it is difficult to preserve depth edges and fine details while maintaining spatial consistency. Therefore, we propose an SFF depth recovery framework composed of depth reconstruction and refinement processes. We first formulate the depth reconstruction as a maximum a posterior (MAP) estimation problem with the inclusion of matting Laplacian prior. The nonlocal principle is adopted in matting Laplacian matrix construction to preserve depth edges and fine details. As the nonlocal principle breaks the spatial consistency, the reconstructed depth image is spatially inconsistent and suffers from the texture-copy artifacts. To smooth the noise and suppress the texture-copy artifacts, a closed-form edge-preserving depth refinement is proposed, which is formulated as a MAP estimation problem using Markov random fields (MRFs). Experimental results over synthetic and real scene datasets demonstrate the superiority of our algorithm in terms of robustness, and the ability to preserve edges and fine details while maintaining spatial consistency compared to existing approaches. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107302	10.1016/j.patcog.2020.107302													
J								CAGNet: Content-Aware Guidance for Salient Object Detection	PATTERN RECOGNITION										Saliency detection; Fully convolutional neural networks; Attention guidance	MODEL	Beneficial from Fully Convolutional Neural Networks (FCNs), saliency detection methods have achieved promising results. However, it is still challenging to learn effective features for detecting salient objects in complicated scenarios, in which i) non-salient regions may have "salient-like" appearance; ii) the salient objects may have different-looking regions. To handle these complex scenarios, we propose a Feature Guide Network which exploits the nature of low-level and high-level features to i) make foreground and background regions more distinct and suppress the non-salient regions which have "salient-like" appearance; ii) assign foreground label to different-looking salient regions. Furthermore, we utilize a Multi-scale Feature Extraction Module (MFEM) for each level of abstraction to obtain multi-scale contextual information. Finally, we design a loss function which outperforms the widely used Cross-entropy loss. By adopting four different pre-trained models as the backbone, we prove that our method is very general with respect to the choice of the backbone model. Experiments on six challenging datasets demonstrate that our method achieves the state-of-the-art performance in terms of different evaluation metrics. Additionally, our approach contains fewer parameters than the existing ones, does not need any post-processing, and runs fast at a real-time speed of 28 FPS when processing a 480 x 480 image. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107303	10.1016/j.patcog.2020.107303													
J								A parallel fuzzy rule-base based decision tree in the framework of map-reduce	PATTERN RECOGNITION										Parallel computing; Fuzzy classifier; Decision trees; Fuzzy rules; Map-Reduce	CLASSIFIERS; GENERATION	Decision trees are commonly used for learning and extracting classification rules from data. The fuzzy rule based decision tree (FRDT) is very representative owing to its better robustness and generalization. However, FRDT cannot work well on the analysis of large-scale data sets. One solution for this problem is parallel computing. A proved effective parallel computing model is Map-Reduce. Ensemble learning is an effective strategy which can significantly improve the generalization ability of machine learning systems. The objective of this paper is to develop a fuzzy rule-base based decision tree on the strategies of parallel computing and ensemble learning. First, we implement a parallel fusing fuzzy rule based classification system via Map-Reduce (MR-FMCS) to display how to extract fuzzy rules from data in parallel and how to evaluate the fuzzy rules in an ensemble learning way. Then, taking MR-FMCS as a fundamental module, we propose a parallel fuzzy rule-base based decision tree (MR-FRBDT) to improve the original FRDT algorithm. The experimental studies mainly focus on feasibility and parallelism. Compared with FRDT on 23 UCI benchmark data sets, the proposed MR-FRBDT algorithm with fewer parameters is effective and has the ability to handle large-scale data sets. Furthermore, some numerical experiments conducted on several large-scale data sets verify the parallel performance on reducing computing time and avoiding memory restrictions. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107326	10.1016/j.patcog.2020.107326													
J								Modelling visual impressions for Chinese and Pakistani ethnic groups	PATTERN RECOGNITION										Visual impression; Facial features; Symmetry; Golden ratio; Neoclassical canons; Ethnic group difference; Attractive; Feminine; Mature	FACIAL ATTRACTIVENESS; SYMMETRY; FACE; COMPUTATION; FEATURES; HEALTH; BEAUTY	This work describes an investigation into the relationship between the visual impressions of a number of facial images each described by a set of parameters related to the position and size of discrete components: the eyes, the nose and the lips. Observations were made by members of two ethnic groups, Chinese and Pakistani, and the images comprised female faces from the same two groups. The observers used 16 impression scales to assess each image and a forced-choice scaling technique. From the factor analysis, the results showed that three visual impressions, attractive, feminine and mature, can well represent all the 16 scales. In the second experiment, the observers assessed visual impressions of more images using only these three impressions. Data are presented relating to the differences between the observations for the various facial locations, as well as between observers from different ethnic groups. Models have been developed that describe the data and their predictions outperformed traditional models, i.e.symmetry, golden ratio and neoclassical canons. The differences between the results of the two ethnic groups were found to be small; there were however, some significant differences in the responses to different facial features. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107259	10.1016/j.patcog.2020.107259													
J								Training bidirectional generative adversarial networks with hints	PATTERN RECOGNITION										Generative Modeling; Generative Adversarial Networks; Unsupervised Learning; Autoencoders; Neural Networks; Deep Learning		The generative adversarial network (GAN) is composed of a generator and a discriminator where the generator is trained to transform random latent vectors to valid samples from a distribution and the discriminator is trained to separate such "fake" examples from true examples of the distribution, which in turn forces the generator to generate better fakes. The bidirectional GAN (BiGAN) also has an encoder working in the inverse direction of the generator to produce the latent space vector for a given example. This added encoder allows defining auxiliary reconstruction losses as hints for a better generator. On five widely-used data sets, we showed that BiGANs trained with the Wasserstein loss and augmented with hints learn better generators in terms of image generation quality and diversity, as measured numerically by the 1-nearest neighbor test, Frechet inception distance, and reconstruction error, and qualitatively by visually analyzing the generated samples. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107320	10.1016/j.patcog.2020.107320													
J								Learning to infer human attention in daily activities	PATTERN RECOGNITION										Human attention; Deep neural network; Attentional objects	OBJECT-BASED ATTENTION; VISUAL-ATTENTION; MODEL	The first attention model in the computer science community is proposed in 1998. In the following years, human attention has been intensively studied. However, these studies mainly refer human attention as the image regions that draw the attention of a human (outside the image) who is looking at the image. In this paper, we infer the attention of a human inside a third-person view video where the human is doing a task, and define human attention as attentional objects that coincide with the task the human is doing. To infer human attention, we propose a deep neural network model that fuses both low-level human pose cue and high-level task encoding cue. Due to the lack of appropriate public datasets for studying this problem, we newly collect a video dataset in complex Virtual-Reality (VR) scenes. In the experiments, we widely compare our method with three other methods on this VR dataset. In addition, we re-annotate a public real dataset and conduct the extensional experiments on this real dataset. The experiment results validate the effectiveness of our method. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107314	10.1016/j.patcog.2020.107314													
J								Hyper-parameter optimization in classification: To-do or not-to-do	PATTERN RECOGNITION										Hyper-parameter optimization; Framework; Bayesian optimization; Machine learning; Incremental learning	ALGORITHM	Hyper-parameter optimization is a process to find suitable hyper-parameters for predictive models. It typically incurs highly demanding computational costs due to the need of the time-consuming model training process to determine the effectiveness of each set of candidate hyper-parameter values. A priori, there is no guarantee that hyper-parameter optimization leads to improved performance. In this work, we propose a framework to address the problem of whether one should apply hyper-parameter optimization or use the default hyper-parameter settings for traditional classification algorithms. We implemented a prototype of the framework, which we use a basis for a three-fold evaluation with 486 datasets and 4 algorithms. The results indicate that our framework is effective at supporting modeling tasks in avoiding adverse effects of using ineffective optimizations. The results also demonstrate that incrementally adding training datasets improves the predictive performance of framework instantiations and hence enables "life-long learning." (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107245	10.1016/j.patcog.2020.107245													
J								Shallow2Deep: Indoor scene modeling by single image understanding	PATTERN RECOGNITION										Scene understanding; Image-based modeling; Semantic modeling; Relational reasoning	SUPPORT	Dense indoor scene modeling from 2D images has been bottlenecked due to the absence of depth information and cluttered occlusions. We present an automatic indoor scene modeling approach using deep features from neural networks. Given a single RGB image, our method simultaneously recovers semantic contents, 3D geometry and object relationship by reasoning indoor environment context. Particularly, we design a shallow-to-deep architecture on the basis of convolutional networks for semantic scene understanding and modeling. It involves multi-level convolutional networks to parse indoor semantics/geometry into non-relational and relational knowledge. Non-relational knowledge extracted from shallow-end networks (e.g. room layout, object geometry) is fed forward into deeper levels to parse relational semantics (e.g. support relationship). A Relation Network is proposed to infer the support relationship between objects. All the structured semantics and geometry above are assembled to guide a global optimization for 3D scene modeling. Qualitative and quantitative analysis demonstrates the feasibility of our method in understanding and modeling semantics-enriched indoor scenes by evaluating the performance of reconstruction accuracy, computation performance and scene complexity. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107271	10.1016/j.patcog.2020.107271													
J								Deep support vector machine for hyperspectral image classification	PATTERN RECOGNITION										Remote sensing; Hyperspectral image; Deep support vector machine; Image classification	SVM; MULTICLASS; MODEL	To improve on the robustness of traditional machine learning approaches, emphasis has recently shifted to the integration of such methods with Deep Learning techniques. However, the classification problems, complexity and inconsistency in several spectral classifiers developed for hyperspectral images are some reasons warranting further research. This study investigates the application of Deep Support Vector Machine (DSVM) for hyperspectral image classification. Two hyperspectral images, Indian Pines and University of Pavia are used as tentative test beds for the experiment. The DSVM is implemented with four kernel functions: Exponential Radial Basis Function (ERBF), Gaussian Radial Basis Function (GRBF), neural and polynomial. Stand-alone Support Vector Machines form the interconnecting weights of the entire network. The network is trained with one hundred input datasets, and the interconnecting weights of the network are initialised using the regularisation parameter of the model. Numerical results show that the classification accuracies of the DSVM for Indian Pines and University of Pavia based on each DSVM kernel functions are: ERBF (98.87%, 98.16%), GRBF (98.90%, 98.47%), neural (98.41%, 97.27%), and polynomial (99.24%, 98.79%). By comparing the DSVM algorithm against well-known classifiers, Support Vector Machine (SVM), Deep Neural Network (DNN), Gaussian Mixture Model (GMM), K Nearest Neighbour (KNN), and K Means (KM) classifiers, the mean classification accuracies for Indian Pines and University of Pavia are: DSVM (98.86%, 98.17%), SVM (76.03%, 73.52%), DNN (94.45%, 93.79%), GMM (76.82%, 78.35%), KNN (76.87%, 78.80%), and KM (21.65%, 18.18%). These results indicate that the DSVM outperformed the other classification algorithms. The high accuracy obtained with the DSVM validates its efficacy as state-of-the-art algorithm for hyperspectral image classification. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107298	10.1016/j.patcog.2020.107298													
J								Joint image deblurring and matching with feature-based sparse representation prior	PATTERN RECOGNITION										Blurred image matching; Joint image deblurring and matching; Sparse representation priorsparse; (2D)(2)PCA feature	FACE RECOGNITION; REGISTRATION; ALGORITHMS	Image matching aims to find a similar area of the small image in the large image, which is one of the key steps in image fusion and vision-based navigation; however, most matching methods perform poorly when the images to be matched are blurred. Traditional approaches for blurred image matching usually follow a two-stage framework - first resorting to image deblurring and then performing image matching with the recovered image. However, the matching accuracy of these methods often suffers greatly from the deficiency of image deblurring. Recently, a joint image deblurring and matching method that utilizes the sparse representation prior to exploit the correlation between deblurring and matching was proposed to address this problem and found to obtain a higher matching accuracy. Yet, that technique is not efficient when the image is seriously blurred, and the method's time complexity is excessive. In this paper, we propose a joint image deblurring and matching approach with a feature-based sparse representation prior. Our approach utilizes two-directional two-dimensional (2D)(2) PCA to extract feature vectors from images and obtains a sparse representation prior in a robust feature space rather than the original pixel space, thus mitigating the influence of image blur. Moreover, the reduction in the feature dimension can also increase the computational efficiency. Extensive experiments show that our approach significantly outperforms state-of-the-art approaches in terms of both accuracy and speed. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107300	10.1016/j.patcog.2020.107300													
J								Driving maneuver early detection via sequence learning from vehicle signals and video images	PATTERN RECOGNITION										Driving maneuver early detection; Deep neural networks; Sequence learning; Advanced driver assistance systems; Computer vision		Driving Maneuver Early Detection (DMED) is particularly useful for many applications of intelligent vehicle systems, including driver warning and collision avoidance systems. In this paper, we introduce a robust DMED model, denoted as University of Michigan Dearborn (UMD)-DMED, developed using innovative features and deep learning techniques. The UMD-DMED model contains three major computational components, distance based representation of driving context, combined vehicle trajectory features and visual features, and a Long Short-Term Memory (LSTM)-based neural network that captures temporal dependencies of driving maneuvers. To properly evaluate the performances of UMD-DMED, we developed two DMED systems based on the UMD-DMED model, one system is based on partially observed evidence of maneuver events, and another on features observed ahead of the time that driving maneuvers take place. We conducted the extensive experiments using a data set containing 1078 maneuver events extracted from 37 hours of real world driving trips. The results demonstrate that the UMD-DMED model is capable of learning the latent features of five different classes of driving maneuvers, i.e. left turn, right turn, left lane change, right lane change, driving straight. Comparing to four different state-of-the-art DMED systems, the UMD-DMED achieved better detection performances in both, the detection based on partial observations of driver maneuvering, and based on driving context observed ahead-of-time. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107276	10.1016/j.patcog.2020.107276													
J								Saliency detection using a deep conditional random field network	PATTERN RECOGNITION										Saliency detection; Conditional random field; Convolutional neural network	OPTIMIZATION; FEATURES	Saliency detection has made remarkable progress along with the development of deep learning. While how to integrate the low-level intrinsic context with high-level semantic information to keep the object boundary sharp and restrain the background noise is still a challenging problem. Many attempts on network structures and refinement strategies have been explored, such as using Conditional Random Field (CRF) to improve the accuracy of saliency map, but it is independent from the deep network and cannot be trained end-to-end. To tackle this issue, we propose a novel Deep Conditional Random Field network (DCRF) to take both deep feature and neighbor information into consideration. First, Multi-scale Feature Extraction Module (MFEM) is adopted to capture the low level texture and high level semantic features, multi-stacks of deconvolution layers are employed to improve the spatial resolution of deep layers. Then we employ Backward Optimization Module (BOM) to guide shallower layers by high-level location and shape information derived from deeper layers, which intrinsically enhance the representational capacity of low-level features. Finally, a Deep Conditional Random Field Module (DCRFM) with unary and pairwise potentials is designed to concentrate on spatial neighbor relations to obtain a compact and uniformed saliency map. Extensive experimental results on 5 datasets in terms of 6 evaluation metrics demonstrate that the proposed method achieves state-of-the-art performance. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107266	10.1016/j.patcog.2020.107266													
J								LG: A clustering framework supported by point proximity relations	PATTERN RECOGNITION										Clustering; Proximity relation; Local energy; Guide point; Face clustering	MULTIBODY FACTORIZATION; ALGORITHM; SELECTION	Clustering is a research problem based on the data's proximity relationship which is not made full use of by all the existing algorithms. In this paper, we present a novel two-stage LG framework consisting of the proposed Local Energy Gradient Oppression (LEGO) and the Guide Point Assignation (GPA) strategies which are closely related to the data points' proximity relations. In the LG framework, it is crucial to locate the appropriate centers for the subsequent data label assignment, and therefore we introduce the nuclear model viewing the dataset as a collection of charged particles, which is the basis of LEGO, and the points with local maximum potential energy are ascertained as the cluster centers. Besides, the GPA strategy innovatively adopts the idea that the cluster center actively selects data points as the same cluster, enabling the LG framework still to be effective when dealing with datasets of arbitrary shape distribution. Superiorities of the proposed framework and the two strategies are demonstrated on four synthetic datasets and three real-world faces image datasets in terms of two clustering performance metrics. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107265	10.1016/j.patcog.2020.107265													
J								A survey on image and video cosegmentation: Methods, challenges and analyses	PATTERN RECOGNITION										Image cosegmentation; Video cosegmentation	OBJECT CO-SEGMENTATION; BUILDING CHANGE DETECTION; SALIENCY; SHAPE; GRAPH; PROPAGATION; TRACKING	Image and video cosegmentation is a newly emerging and rapidly progressing area, which aims at delineating common objects at pixel-level from a group of images or a set of videos. Plenty of related works have been published and implemented in varied applications, but there lacks a systematic survey on both image and video cosegmentation. This paper provides a comprehensive overview including the existing methods, applications, and challenges. Specifically, different cosegmentation problem settings are described, the formulation details of the methods are summarized and their potential applications are listed. Moreover, the benchmark datasets and standard evaluation metrics are also given; and the future directions and unsolved challenges are discussed. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107297	10.1016/j.patcog.2020.107297													
J								Unsupervised learning of optical flow with patch consistency and occlusion estimation	PATTERN RECOGNITION										Patch consistency; Optical flow estimation; Occlusion estimation; Unsupervised learning; Deep learning	FEATURES	Recent works have shown that deep networks can be trained for optical flow estimation without supervision. Based on the photometric constancy assumption, most of these methods adopt the reconstruction loss as the supervision by point-based backward warping. Inspired by the traditional patch matching based approaches, we propose a patch-based consistency to improve the vanilla unsupervised learning method Ren et al. [1]. Instead of only comparing the corresponding pixel intensity, we locate the correspondence by using the image patches with census transform, which is more robust for the illumination variation and occlusion. Moreover, a novel parallel branch is devised to estimate a soft occlusion mask jointly in an unsupervised way. The mask is adopted to weight our patch-based consistency loss to alleviate the influence of the occlusion. The plenty of experiments have been implemented on Flying Chairs, KITTI and MPI-Sintel benchmarks. The results show that our method is efficient and outperforms the peer unsupervised learning methods that are using the FlowNet-liked network. (C) 2019 Published by Elsevier Ltd.																	0031-3203	1873-5142				JUL	2020	103								107191	10.1016/j.patcog.2019.107191													
J								Fusion of complex networks and randomized neural networks for texture analysis	PATTERN RECOGNITION										Randomized neural networks; Complex networks; Texture analysis; Feature extraction	CLASSIFICATION; FEATURES; REGULARIZATION; DESCRIPTORS	This paper presents a high discriminative texture analysis method based on the fusion of complex networks and randomized neural networks. In this approach, the input image is modeled as a complex network and its topological properties as well as the image pixels are used to train randomized neural networks to create a signature that represents the deep characteristics of the texture. The results obtained surpassed the accuracy of many methods available in the literature. This performance demonstrates that our proposed approach opens a promising source of research, which consists of exploring the synergy of neural networks and complex networks in the texture analysis field. (C) 2019 Published by Elsevier Ltd.																	0031-3203	1873-5142				JUL	2020	103								107189	10.1016/j.patcog.2019.107189													
J								Textual data summarization using the Self-Organized Co-Clustering model	PATTERN RECOGNITION										Co-Clustering; Document-term matrix; Latent block model	LATENT BLOCK MODEL; FACTORIZATION; MATRIX	Recently, different studies have demonstrated the use of co-clustering, a data mining technique which simultaneously produces row-clusters of observations and column-clusters of features. The present work introduces a novel co-clustering model to easily summarize textual data in a document-term format. In addition to highlighting homogeneous co-clusters as other existing algorithms do we also distinguish noisy co-clusters from significant co-clusters, which is particularly useful for sparse document-term matrices. Furthermore, our model proposes a structure among the significant co-clusters, thus providing improved interpretability to users. The approach proposed contends with state-of-the-art methods for document and term clustering and offers user-friendly results. The model relies on the Poisson distribution and on a constrained version of the Latent Block Model, which is a probabilistic approach for co-clustering. A Stochastic Expectation-Maximization algorithm is proposed to run the model's inference as well as a model selection criterion to choose the number of co-clusters. Both simulated and real data sets illustrate the efficiency of this model by its ability to easily identify relevant co-clusters. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107315	10.1016/j.patcog.2020.107315													
J								Unsupervised hashing based on the recovery of subspace structures	PATTERN RECOGNITION										Semantic hashing; Subspace learning; Low-rank representation; Discrete optimization	SEGMENTATION; ALGORITHM	Unsupervised semantic hashing should in principle keep the semantics among samples consistent with the intrinsic geometric structures of the dataset. In this paper, we propose a novel multiple stage unsupervised hashing method, named "Unsupervised Hashing based on the Recovery of Subspace Structures" (RSSH) for image retrieval. Specifically, we firstly adapt the Low-rank Representation (LRR) model into a new variant which treats the real-world data as samples drawn from a union of several low-rank subspaces. Then, the pairwise similarities are represented in a space-and-time saving manner based on the learned low-rank correlation matrix of the modified LRR. Next, the challenging discrete graph hashing is employed for binary hashing codes. Notably, we convert the original graph hashing model into an optimization-friendly formalization, which is addressed with efficient closed-form solutions for its sub-problems. Finally, the devised linear hash functions are fast achieved for out-of-samples. Retrieval experiments on four image datasets testify the superiority of RSSH to several state-of-the-art hashing models. Besides, it's worth mentioning that RSSH, a shallow model, significantly outperforms two recently proposed unsupervised deep hashing methods, which further confirms its effectiveness. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107261	10.1016/j.patcog.2020.107261													
J								Corner detection based on shearlet transform and multi-directional structure tensor	PATTERN RECOGNITION										Shearlet transform; Multi-directional structure tensor; Corner detection	SCALE-INVARIANT; FEATURES	Image corners have been widely used in various computer vision tasks. Current multi-scale analysis based corner detectors do not make full use of the multi-scale and multi-directional structural information. This degrades their detection accuracy and capability of refining corners. In this work, an improved shearlet transform with a flexible number of directions and a reasonable support is proposed to extract accurate multi-scale and multi-directional structural information from images. To make full use of the structural information from the improved shearlets, a novel multi-directional structure tensor is constructed for corner detection, and a multi-scale corner measurement function is proposed to remove false candidate corners. Experimental results demonstrate that the proposed corner detector performs better than existing corner and interest point detectors in terms of detection accuracy, localization accuracy, and robustness to affine transformations, illumination changes, noise, viewpoint changes, etc. It has a great potential for extension as a descriptor and for applications in computer vision tasks. Crown Copyright (C) 2020 Published by Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107299	10.1016/j.patcog.2020.107299													
J								Global and local sensitivity guided key salient object re-augmentation for video saliency detection	PATTERN RECOGNITION										Video saliency detection; Ranking saliency; Semantical guidance	VISUAL-ATTENTION; DETECTION MODEL; NETWORKS; RANKING	Image saliency is determined by spatial semantic features, while video saliency is affected by multiple factors such as spatial and temporal information. Since human eyes stay extremely short on each frame, the dynamic salient area is more focused and concentrated on one salient object. In order to better simulate the human visual attention mechanism in dynamic scenes, we propose a key salient object re-augmentation method (KSORA) based on the guidance of both bottom-up weighted features and topdown semantic knowledge. The bottom-up feature weighting strategy effectively eliminates noisy and redundancy, and provide accurate local spatiotemporal features for saliency inference. The top-down key object enhancement strategy ranks salient candidates based on global statistical knowledge, so as to explicitly enhance the saliency proportion of the key object. The fusion of the local weighted spatiotemporal features and the global key object augmentation features not only ensures spatiotemporal consistency, but also facilitates obtaining more concentrated salient prediction. Results on three large datasets validate that our proposed method has the capability of improving the detection accuracy in complex scenes. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107275	10.1016/j.patcog.2020.107275													
J								Depth upsampling based on deep edge-aware learning	PATTERN RECOGNITION										Upsampling; CNN; Edge-aware; Depth map	SUPERRESOLUTION; RECOGNITION	Depth map upsampling will unavoidably smoothen the edges leading to blurry results on the depth boundaries, especially at large upscaling factors. Given that edges represent the most important cue in addressing the task of depth upsampling, we propose a novel depth upsampling framework based on deep edge-aware learning. Unlike existing CNN-based approaches that directly predict depth values from low resolution (LR) depth input, our framework firstly learns edge information of depth boundaries from the known LR depth map and its corresponding high resolution (HR) color image as reconstruction cues. Then, two depth restoration modules, i.e., a fast depth filling strategy and a cascaded restoration network, are proposed to recover HR depth map by leveraging the predicted edge map and the HR color image. Extensive comparisons on both edge inference and depth upsampling under noisy and noiseless cases demonstrate the superiority of the proposed approaches. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107274	10.1016/j.patcog.2020.107274													
J								Pedestrian detection in underground mines via parallel feature transfer network	PATTERN RECOGNITION										Pedestrian detection; Underground mine; Deep learning network; Parallel feature transfer; Gated unit; Unmanned driving	FASTER R-CNN	Pedestrian detection has been one of the key technologies in computer vision for autonomous driving in underground mines. However, such pedestrian detection is easily affected by complex environmental factors, such as uneven light, dense dust and cable interference. Recently, the problem of pedestrian detection is solved as an object detection task, which has achieved significant advances with the framework of deep neural networks. In this paper, we propose a novel parallel feature transfer network based detector called PftNet that achieves better efficiency than one-stage methods and maintains comparable accuracy of two-stage methods. PftNet consists of two interconnected modules, i.e., the pedestrian identification module and the pedestrian location module. The former aims to roughly adjust the location and size of the anchor box, filter out the negative anchor box, and provide better initialization for the regression. The latter enables PftNet to adapt to different scales and aspect ratios of objects and further improves the regression accuracy. Meanwhile, a feature transfer block compromising gated units is well designed to transmit the pedestrian characteristics between two modules. Extensive experiments on self-annotated underground dataset as well as INRIA and ETH datasets show that PftNet achieves state-of-the-art detection efficiency with high accuracy, which is significant to realizing unmanned driving systems in mines. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107195	10.1016/j.patcog.2020.107195													
J								Cross-view hashing via supervised deep discrete matrix factorization	PATTERN RECOGNITION										Matrix factorization; Cross-view hashing; Similarity search	IMAGE; QUANTIZATION	Matrix factorization has been utilized for the task of cross-view hashing, where basis functions are learned to map data from different views to the same hamming embedding. It is possible that the basis functions between the hamming embedding and the original data matrix contain rather complex hierarchical information, which existing work can not capture. In addition, previous work employs relaxation technique in the matrix factorization based hashing which may lead to large quantization error. To address these issues, this paper presents a novel Supervised Discrete Deep Matrix Factorization (SDDMF) for cross-view hashing. We introduce deep matrix factorization so that SDDMF is able to learn a set of hierarchical basis functions and unified binary codes from different views. In addition, a classification error term is incorporated into the objective to learn discriminative binary codes. We then employ a linearization technique to directly optimize the discrete constraints which can significantly reduce the quantization error. Experimental results on three standard datasets with image-text modalities verify that SDDMF significantly outperforms several state-of-the-art methods. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107270	10.1016/j.patcog.2020.107270													
J								Adaptive quantile low-rank matrix factorization	PATTERN RECOGNITION										Low-rank matrix factorization; Mixture of asymmetric Laplace distributions; Expectation maximization algorithm; Skew noise	MISSING DATA; APPROXIMATIONS; ALGORITHMS; NETWORKS	Low-rank matrix factorization (LRMF) has received much popularity owing to its successful applications in both computer vision and data mining. By assuming noise to come from a Gaussian, Laplace or mixture of Gaussian distributions, significant efforts have been made on optimizing the (weighted) L-1 or L-2-norm loss between an observed matrix and its bilinear factorization. However, the type of noise distribution is generally unknown in real applications and inappropriate assumptions will inevitably deteriorate the behavior of LRMF. On the other hand, real data are often corrupted by skew rather than symmetric noise. To tackle this problem, this paper presents a novel LRMF model called AQ-LRMF by modeling noise with a mixture of asymmetric Laplace distributions. An efficient algorithm based on the expectation-maximization (EM) algorithm is also offered to estimate the parameters involved in AQ-LRMF. The AQ-LRMF model possesses the advantage that it can approximate noise well no matter whether the real noise is symmetric or skew. The core idea of AQ-LRMF lies in solving a weighted L-1 problem with weights being learned from data. The experiments conducted on synthetic and real data sets show that AQ-LRMF outperforms several state-of-the-art techniques. Furthermore, AQ-LRMF also has the superiority over the other algorithms in terms of capturing local structural information contained in real images. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107310	10.1016/j.patcog.2020.107310													
J								Single image-based head pose estimation with spherical parametrization and 3D morphing	PATTERN RECOGNITION										Head pose estimation; Spherical parameterization; 3D facial model	VISUAL FOCUS; GAZE; ATTENTION	Head pose estimation plays a vital role in various applications, e.g., driver-assistance systems, human-computer interaction, virtual reality technology, and so on. We propose a novel geometry-based method for accurately estimating the head pose from a single 2D face image at a very low computational cost. Specifically, the rectangular coordinates of only four non-coplanar feature points from a predefined 3D facial model as well as the corresponding ones automatically/manually extracted from a 2D face image are first normalized to exclude the effect of external factors (i.e., scale factor and translation parameters). Then, the four normalized 3D feature points are represented in spherical coordinates with reference to the uniquely determined sphere by themselves. Due to the spherical parametrization, the coordinates of feature points can then be morphed along all the three directions in the rectangular coordinates effectively. Finally, the rotation matrix indicating the head pose is obtained by minimizing the Euclidean distance between the normalized 2D feature points and the 2D re-projections of the morphed 3D feature points. Comprehensive experimental results over two popular datasets, i.e., Pointing'04 and Biwi Kinect, demonstrate that the proposed method can estimate head poses with higher accuracy and lower run time than state-of-the-art geometry-based methods. Even compared with start-of-the-art learning-based methods or geometry-based methods with additional depth information, our method still produces comparable performance. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107316	10.1016/j.patcog.2020.107316													
J								Learning motion representation for real-time spatio-temporal action localization	PATTERN RECOGNITION										Spatio-Temporal Action Localization; Real-time Computation; Optical Flow Sub-network; Pyramid Hierarchical Fusion	OPTICAL-FLOW ESTIMATION; RECOGNITION; ACCURACY	The current deep learning based spatio-temporal action localization methods that using motion information (predominated is optical flow) obtain the state-of-the-art performance. However, since the optical flow is pre-computed, leading to these methods face two problems - the computational efficiency is low and the whole network is not end-to-end trainable. We propose a novel spatio-temporal action localization approach with an integrated optical flow sub-network to address these two issues. Specifically, our designed flow subnet can estimate optical flow efficiently and accurately by using multiple consecutive RGB frames rather than two adjacent frames in a deep network, simultaneously, action localization is implemented in the same network interactive with flow computation end-to-end. To faster the speed, we exploit a neural network based feature fusion method in a pyramid hierarchical manner. It fuses spatial and temporal features at different granularities via combination function (Le. concatenation) and point-wise convolution to obtain multiscale spatio-temporal action features. Experimental results on three publicly available datasets, e.g. UCF101-24, JHMDB and AVA show that with both RGB appearance and optical flow cues, the proposed method gets the state-of-the-art performance in both efficiency and accuracy. Noticeably, it gets a significant improvement on efficiency. Compared to the currently most efficient method, it is 1.9 times faster in the running speed and 1.3% video-mAP more accurate on the UCF101-24. Our proposed method reaches real-time computation for the first time (up to 38 FPS). (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107312	10.1016/j.patcog.2020.107312													
J								Radical analysis network for learning hierarchies of Chinese characters	PATTERN RECOGNITION										Radical; Attention; Chinese character; Few-/zero-shot learning	NEURAL-NETWORK; RECOGNITION; ONLINE	Chinese characters have a valuable property, this is, numerous Chinese characters are composed of a compact set of fundamental and structural radicals. This paper introduces a radical analysis network (RAN) that makes full use of this valuable property to implement radical-based Chinese character recognition. The proposed RAN employs an attention mechanism to extract radicals from Chinese characters and to detect spatial structures among the radicals. Then, the decoder in RAN generates a hierarchical composition of Chinese characters based on the knowledge of the extracted radicals and their internal structures. The method of treating a Chinese character as a composition of radicals rather than as a single character category is a human-like method that can reduce the size of the vocabulary, ignore redundant information among similar characters and enable the system to recognize unseen Chinese character categories, i.e., zero-shot learning. Through experiments, we assess the practicality of RAN for recognizing Chinese characters in natural scenes. Furthermore, a RAN framework can be proposed for scene text recognition with the extension of a dense recurrent neural network (denseRNN) encoder, a multihead coverage attention model and HSV representations. The proposed approach achieved the best performance in the ICPR MTWI 2018 competition. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107305	10.1016/j.patcog.2020.107305													
J								HoPPF: A novel local surface descriptor for 3D object recognition	PATTERN RECOGNITION										Local feature descriptor; 3D representation; Feature matching; Shape retrieval; Object recognition	PERFORMANCE EVALUATION; HISTOGRAMS; EFFICIENT	Three-dimensional feature descriptors play an important role in 3D computer vision because they are widely employed in many 3D perception applications to extract point correspondences between two point clouds. However, most existing description methods suffer from either weak robustness, low descriptiveness, or costly computation. Thus, a 3D local feature descriptor named Histograms of Point Pair Features (HoPPF) is proposed in this paper, and it is aimed at robust representation, high descriptiveness, and efficient computation. First, we propose a novel method to redirect surface normals and use the Poisson-disk sampling strategy to solve the problem of data redundancy in data pre-processing. Second, a new technique is applied to divide the local point pair set of each keypoint into eight regions. Then, the distribution of local point pairs of each region is used to construct the corresponding sub-features. Finally, the proposed HoPPF is generated by concatenating all sub-features into a vector. The performance of the HoPPF method is rigorously evaluated on several standard datasets. The results of the experiments and comparisons with other state-of-the-art methods validate the superiority of the HoPPF descriptor in term of robustness, descriptiveness, and efficiency. Moreover, the proposed technique for division of point pair sets is used to modify the other typical point-pair-based descriptor (i.e., PFH) to show its generalization ability. The proposed HoPPF is also applied to object recognition on real datasets captured by different devices (e.g., Kinect and LiDAR) to verify the feasibility of this method for 3D vision applications. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107272	10.1016/j.patcog.2020.107272													
J								Fast minutiae extractor using neural network	PATTERN RECOGNITION										Minutiae extraction; Neural network; Fingerprint; Biometric technology	FINGERPRINT MINUTIAE	In this paper, we propose a fast and reliable neural network-based algorithm for fingerprint minutiae extraction. In particular, our algorithm involves a two-stage process: in the first stage, a network generates candidate patches in which minutiae may exist; in the second stage, another network extracts minutiae from every patch.These two networks share a common part to reduce the running time. Moreover, we analyze the properties of fingerprint images and propose a principle for designing efficient networks for minutiae extraction. For efficiency, our algorithm extracts minutiae directly from raw fingerprint images, without traditional pre-processes. Another benefit of this design is that the networks only require datasets with minutiae labels for training. On the public fingerprint datasets (FVC 2002 and 2004), our algorithm requires 26 ms on average to extract minutiae from one fingerprint on a single GPU. Compared with other neural network-based algorithms, our algorithm runs approximately 10 times faster and does not lose substantial accuracy. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				JUL	2020	103								107273	10.1016/j.patcog.2020.107273													
J								Study of Hellinger Distance as a splitting metric for Random Forests in balanced and imbalanced classification datasets	EXPERT SYSTEMS WITH APPLICATIONS										Hellinger Distance; Imbalanced problems; Random Forests	STATISTICAL COMPARISONS; DECISION TREES; CLASSIFIERS; SMOTE	Hellinger Distance (HD) is a splitting metric that has been shown to have an excellent performance for imbalanced classification problems for methods based on Bagging of trees, while also showing good performance for balanced problems. Given that Random Forests (RF) use Bagging as one of two fundamental techniques to create diversity in the ensemble, it could be expected that HD is also effective for this ensemble method. The main aim of this article is to carry out an extensive investigation on important aspects about the use of HD in RF, including handling of multi-class problems, hyper-parameter optimization, metrics comparison, probability estimation, and metrics combination. In particular, HD is compared to other commonly used splitting metrics (Gini and Gain Ratio) in several contexts: balanced/imbalanced and two-class/multi-class. Two aspects related to classification problems are assessed: classification itself and probability estimation. HD is defined for two-class problems, but there are several ways in which it can be extended to deal with multi-class and this article studies the performance of the available options. Finally, even though HD can be used as an alternative to other splitting metrics, there is no reason to limit RF to use just one of them. Therefore, the final study of this article is to determine whether selecting the splitting metric using cross-validation on the training data can improve results further. Results show HD to be a robust measure for RF, with some weakness for balanced multi-class datasets (especially for probability estimation). Combination of metrics is able to result in a more robust performance. However, experiments of HD with text datasets show Gini to be more suitable than HD for this kind of problems. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 1	2020	149								113264	10.1016/j.eswa.2020.113264													
J								Convolution on neural networks for high-frequency trend prediction of cryptocurrency exchange rates using technical indicators	EXPERT SYSTEMS WITH APPLICATIONS										Cryptocurrencies; Neural network; Finance; Technical analysis; Deep learning	TIME-SERIES; STOCK; BITCOIN; MODEL; DIVERSIFICATION; ALGORITHMS; LIQUIDITY; SENTIMENT; DIRECTION; PRICES	This study explores the suitability of neural networks with a convolutional component as an alternative to traditional multilayer perceptrons in the domain of trend classification of cryptocurrency exchange rates using technical analysis in high frequencies. The experimental work compares the performance of four different network architectures-convolutional neural network, hybrid CNN-LSTM network, multilayer perceptron and radial basis function neural network- to predict whether six popular cryptocurrencies Bitcoin, Dash, Ether, Litecoin, Monero and Ripple- will increase their value vs. USD in the next minute. The results, based on 18 technical indicators derived from the exchange rates at a one-minute resolution over one year, suggest that all series were predictable to a certain extent using the technical indicators. Convolutional LSTM neural networks outperformed all the rest significantly, while CNN neural networks were also able to provide good results specially in the Bitcoin, Ether and Litecoin cryptocurrencies. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 1	2020	149								113250	10.1016/j.eswa.2020.113250													
J								A Bayesian learning model for design-phase service mashup popularity prediction	EXPERT SYSTEMS WITH APPLICATIONS										Popularity prediction; Bayesian learning; Service mashup		Using web services as building blocks to develop software applications, i.e., service mashups, not only reuses software development efforts to minimize development cost, but also leverages user groups and marketing efforts of those services to attract users and improve profits. This has significantly encouraged the development of a large number of service mashups in various domains. However, using existing services, even popular ones, does not guarantee the success of a mashup. In fact, a large portion of existing mashups fail to attract a good number of users, making the mashup development effort less effective. Design-phase popularity prediction can help avoid unpromising mashup developments by providing early-on insight into the potential popularity of a mashup. In this paper, we investigate the factors that can affect the popularity of a mashup through a comprehensive analysis on one of the largest mashup repository (i.e., ProgrammableWeb). We further propose a novel Bayesian approach that offers early-on insight to developers into the potential popularity of a mashup using design-phase features only. Besides identifying those relevant features, the Bayesian learning model can provide a confidence level for each prediction. This provides useful guidance to developers for successful mashup development. Experimental results demonstrate that the proposed approach achieves high prediction accuracy and outperforms competitive models. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 1	2020	149								113231	10.1016/j.eswa.2020.113231													
J								Artificial electric field algorithm for engineering optimization problems	EXPERT SYSTEMS WITH APPLICATIONS										AEFA Algorithm; Constrained optimization; Engineering design problem; Soft computing	PARTICLE SWARM OPTIMIZATION; WATER CYCLE ALGORITHM; CONSTRAINED OPTIMIZATION; DIFFERENTIAL EVOLUTION; EFFICIENT ALGORITHM; GA ALGORITHM; BEE COLONY; KRILL HERD; INTELLIGENCE	Nature-inspired optimization algorithms have attracted significant attention from researchers during the past decades due to their applicability to solving the challenging optimization problems, efficiently. Many intelligent systems require an excellent constrained optimization scheme to act as an artificially intelligent system. Artificial electric field algorithm (AEFA) is an intelligently designed artificial system that deals with the purpose of function optimization. AEFA works on the principle of Coulombs' law of electrostatic force and Newtons' law of motion. The present article extends the AEFA algorithm for constrained optimization problems by introducing the new velocity and position bound strategies. These bounds lead the particle to interact with each other within the domain of the problem, and they are allowed to learn from the problem space individually. They also help to make a better balance between exploration and exploitation by controlling the position update of the particles. The challenging IEEE CEC 2017 constrained benchmark set of 28 problems, and five multidimensional non-linear structural design optimization problems are solved using AEFA-C, which tests the effectiveness and the efficiency of the proposed scheme. The comparative study of AEFA-C is performed with nine state-of-art algorithms, including some IEEE CEC 2017 competitors. The comparative study, statistical analysis, and the findings suggest that the proposed AEFA-C is an efficient constrained optimizer. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 1	2020	149								113308	10.1016/j.eswa.2020.113308													
J								An Automatic Nucleus Segmentation and CNN Model based Classification Method of White Blood Cell	EXPERT SYSTEMS WITH APPLICATIONS										White blood cell; Nucleus segmentation; CNN; Convolutional layer; Classification metrics		White blood cells (WBCs) play a remarkable role in the human immune system. To diagnose blood-related diseases, pathologists need to consider the characteristics of WBC. The characteristics of WBC can be defined based on the morphological properties of WBC nucleus. Therefore, nucleus segmentation plays a vital role to classify the WBC image and it is an important part of the medical diagnosis system. In this study, color space conversion and k-means algorithm based new WBC nucleus segmentation method is proposed. Then we localize the WBC based on the location of segmented nucleus to separate them from the entire blood smear image. To classify the localized WBC image, we propose a new convolutional neural network (CNN) model by combining the concept of fusing the features of first and last convolutional layers, and propagating the input image to the convolutional layer. We also use a dropout layer for preventing the model from overfitting problem. We show the effectiveness of our proposed nucleus segmentation method by evaluating with seven quality metrics and comparing with other methods on four public databases. We achieve average accuracy of 98.61% and more than 97% on each public database. We also evaluate our proposed CNN model by using nine classification metrics and achieve an overall accuracy of 96% on BCCD test database. To validate the generalization capability of our proposed CNN model, we show the training and testing accuracy and loss curves for random test set of BCCD database. Further, we compare the performance of our proposed CNN model with four state-of-the-art CNN models (biomedical image classifier) by measuring the value of evaluation metrics. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 1	2020	149								113211	10.1016/j.eswa.2020.113211													
J								Enhanced deep learning algorithm development to detect pain intensity from facial expression images	EXPERT SYSTEMS WITH APPLICATIONS										Facial expression; Pain detection; Deep neural networks; Expert systems in healthcare; Machine learning		Automated detection of pain intensity from facial expressions, especially from face images that show a patient's health, remains a significant challenge in the medical diagnostics and health informatics area. Expert systems that prudently analyse facial expression images, utilising an automated machine learning algorithm, can be a promising approach for pain intensity analysis in health domain. Deep neural networks and emerging machine learning techniques have made significant progress in both the feature identification, mapping and the modelling of pain intensity from facial images, with great potential to aid health practitioners in the diagnosis of certain medical conditions. Consequently, there has been significant research within the pain recognition and management area that aim to adopt facial expression datasets into deep learning algorithms to detect the pain intensity in binary classes, and also to identify pain and non-pain faces. However, the volume of research in identifying pain intensity levels in multi-classes remains rather limited. This paper reports on a new enhanced deep neural network framework designed for the effective detection of pain intensity, in four-level thresholds using a facial expression image. To explore the robustness of the proposed algorithms, the UNBC-McMaster Shoulder Pain Archive Database, comprised of human facial images, was first balanced, then used for the training and testing of the classification model, coupled with the fine-tuned VGG-Face pre-trainer as a feature extraction tool. To reduce the dimensionality of the classification model input data and extract most relevant features, Principal Component Analysis was applied, improving its computational efficiency. The pre-screened features, used as model inputs, are then transferred to produce a new enhanced joint hybrid CNN-BiLSTM (EJH-CNN-BiLSTM) deep learning algorithm comprised of convolutional neural networks, that were then linked to the joint bidirectional LSTM, for multi-classification of pain. The resulting EJH-CNN-BiLSTM classification model, tested to estimate four different levels of pain, revealed a good degree of accuracy in terms of different performance evaluation techniques. The results indicated that the enhanced EJH-CNN-BiLSTM classification algorithm was explored as a potential tool for the detection of pain intensity in multi-classes from facial expression images, and therefore, can be adopted as an artificial intelligence tool in the medical diagnostics for automatic pain detection and subsequent pain management of patients. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 1	2020	149								113305	10.1016/j.eswa.2020.113305													
J								A Choquet integral based fuzzy logic approach to solve uncertain multi-criteria decision making problem	EXPERT SYSTEMS WITH APPLICATIONS										Fuzzy number; Fuzzy measures; Choquet integrals; Uncertain; Decision making	TOPSIS	Nowadays, the fuzzy measures and fuzzy integrals have been successfully implemented to solve a variety of uncertain multi-criteria decision-making problems. However, with the growing complexity of the decision-making environment and the diversity of linguistic information in the decision-making process, defining the appropriate and reasonable measures and integrals in the fuzzy logic applications becomes increasingly challenging. As the commonly used interval-valued Sugeno probability space is capable of representing the linguistic information in a more accurate way, in this work, we combine the Choquet integrals with interval-valued Sugeno probability space to develop a new interval-valued function to solve the uncertain multi-criteria decision-making problems. This work first defines the interval-valued Sugeno probability measure based on s-.rules and proposes the Choquet integrals in the interval-valued Sugeno probability space, and then a relevant solution approach is developed to solve the uncertain multi-criteria decision-making problems. The refrigerator components end-of-life strategy determination problem is used as a case study to illustrate the application. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 1	2020	149								113303	10.1016/j.eswa.2020.113303													
J								A novel tourism recommender system in the context of social commerce	EXPERT SYSTEMS WITH APPLICATIONS										Social commerce; Tourism recommender system; Trust; Similarity; Community; Reputation; Social relationships	OF-THE-ART; COLD-START; TRUST; NETWORK; ALLEVIATE; INTENTION	Web 2.0 and its services, such as social networks, have significantly influenced various businesses, including e-commerce. As a result, we face a new generation of e-commerce called Social Commerce. On the other hand, in the tourism industry, a variety of services and products are provided. The dramatic rise in the number of options in travel packages, hotels, tourist attractions, etc. put users in a difficult situation to find what they need. For a reason, tourism recommender systems have been considered by researchers and businesses as a solution. Since tourist attractions are often the reason for travelling, this research proposes a social-hybrid recommender system in the context of social commerce that recommends tourist attractions. The purpose of the research is presenting a personalized list of tourist attractions for each tourist based on the similarity of users' desires and interests, trust, reputation, relationships, and social communities. Compared with the traditional methods, collaborative filtering, content-based, and hybrid, the advantage of the proposed method is the use of various factors and the inclusion of trust factors in recommendation resources, (such as outlier detection in user ratings), and employing social relationships among individuals. The experimental results show the superiority of the proposed method over other common methods. The proposed method can also be used to recommend other products and services in the tourism industry and other social commerce. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 1	2020	149								113301	10.1016/j.eswa.2020.113301													
J								A multimodal particle swarm optimization-based approach for image segmentation	EXPERT SYSTEMS WITH APPLICATIONS										Color image segmentation; Clustering; Particle Swarm Optimisation; Multimodal optimisation	HISTOGRAM; ENTROPY	Color image segmentation is a fundamental challenge in the field of image analysis and pattern recognition. In this paper, a novel automated pixel clustering and color image segmentation algorithm is presented. The proposed method operates in three successive stages. In the first stage, a three-dimensional histogram of pixel colors based on the RGB model is smoothened using a Gaussian filter. This process helps to eliminate unreliable and non-dominating peaks that are too close to one another in the histogram. In the next stage, the peaks representing different clusters in the histogram are identified using a multimodal particle swarm optimization algorithm. Finally, pixels are assigned to the most appropriate cluster based on Euclidean distance. Determining the number of clusters to be used is often a manual process left for a user and represents a challenge for various segmentation algorithms. The proposed method is designed to determine an appropriate number of clusters, in addition to the actual peaks, automatically. Experiments confirm that the proposed approach yields desirable results, demonstrating that it can find an appropriate set of clusters for a set of well-known benchmark images. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 1	2020	149								113233	10.1016/j.eswa.2020.113233													
J								A hybrid safe semi-supervised learning method	EXPERT SYSTEMS WITH APPLICATIONS										Semi-supervised learning; Risk degree; Graph quality; Laplacian regularized least squares	CLASSIFIERS	Within the past few years, Safe Semi-Supervised Learning (S3L) has become a hot topic in the machine learning field and many related S3L methods have been proposed to safely exploit the unlabeled information. However, these methods only considered the risk from a single level, such as the instance or model level. They can not reduce the adverse effects of both the risky unlabeled instances and inappropriate learning models. Therefore, it is important to investigate a novel effective S3L method. In this paper, we present a hybrid S3L method which can inherit the merits of both the instance-level and model-level approaches. In our algorithm, multiple Graph-based SSL (GSSL) classifiers are firstly trained and used to predict the unlabeled instances. The risk degrees of the unlabeled instances and the qualities of the constructed graphs are then estimated through the predictions of multiple GSSL classifiers. Finally, we build two regularization terms to constrain the predictions of the unlabeled instances and adaptively select the graphs with high qualities. These regularization terms aim at reducing the negative effect of both the risky unlabeled instances and inappropriate learning models with low-quality graphs. Experimental results on different real-world datasets verify the effectiveness of our algorithm by comparisons to the state-of-the-art Supervised Learning (SL), SSL and S3L methods. In conclusion, our algorithm can not only enrich the research of S3L, but enlarge the practical scope of SSL in the expert and intelligent systems to a certain extent. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 1	2020	149								113295	10.1016/j.eswa.2020.113295													
J								Fuzzy hidden Markov-switching portfolio selection with capital gain tax	EXPERT SYSTEMS WITH APPLICATIONS										Fuzzy sets; Regime switching; Capital gain tax; Numerical integral simulation; Particle swarm optimization	OPTIMIZATION MODEL; ASSET ALLOCATION; RISK; EQUILIBRIUM	A fuzzy portfolio selection model is considered with a view to incorporating ambiguity about model and data structure. The model features the uncertainty about the exit time of each risky asset within a pre-specified investment horizon and also the presence of transaction costs. However, departing from the traditional paradigm where the transaction costs are often assumed to be unrelated to holding periods, we introduce the capital gain tax of which the realized tax rate is decreasing with respect to the holding periods with a view to encouraging the long-term investment. Meanwhile, the regime switching property of the market state is introduced to fuzzy portfolio selection, where fuzzy random variables are employed to model uncertain returns of risky assets in a Markov-regime switching market. An adjusted L - R fuzzy number is introduced and some of its mathematical properties are studied. In addition, a bi-objective mean-variance model is formulated, and a time varying numerical integral-based particle swarm optimization algorithm (TVNIPSO) is designed to obtain the efficient frontier of the portfolio in the sense of Pareto dominance. Finally, some numerical experiments are provided to validate the effectiveness of the model and the TVNIPSO. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 1	2020	149								113304	10.1016/j.eswa.2020.113304													
J								Blind watermarking for color images using EMMQ based on QDFT	EXPERT SYSTEMS WITH APPLICATIONS										Blind image watermarking; Quaternion discrete Fourier transform; MPSAM; Extreme pixel adjustment; Particle swarm optimization	QUATERNION FOURIER-TRANSFORM; INTERBLOCK PREDICTION; ROBUST; SCHEME; DWT	In this study, we developed a novel scheme for the blind watermarking of color images. The proposed scheme incorporates extreme pixel adjustment (EPA), multi-bit partly sign-altered mean modulation (MPSAM), mixed modulation (MM), and particle swarm optimization (PSO) within a scheme based on crisscross inter-block quaternion discrete Fourier transform (QDFT). Accordingly, the proposed scheme employing EPA, MPSAM, MM, and QDFT is referred to as EMMQ. The image is separated into non-overlapping 8 x8 pixel blocks, whereupon MPSAM is used to map multiple bits within a single block using multiple coefficients in one of the four transformed components of QDFT. The use of MM to embed the watermark allows for superior image quality and strong resistance to image processing attacks. Our use of PSO also makes it possible to optimize the EMMQ parameters, thereby enabling outstanding robustness without compromising imperceptibility. Experiment results demonstrate the efficacy of the proposed scheme in resisting a variety of image-processing attacks, with performance superior to that of existing watermarking schemes in terms of imperceptibility and robustness for a given payload capacity. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 1	2020	149								113225	10.1016/j.eswa.2020.113225													
J								A weight perturbation-based regularisation technique for convolutional neural networks and the application in medical imaging	EXPERT SYSTEMS WITH APPLICATIONS										Convolutional Neural Network; Regularisation; Generalisation; Weight perturbation	NOISE	A convolutional neural network has the capacity to learn multiple representation levels and abstraction in order to provide a better understanding of image data. In addition, a good multi-level representation of data typically results in a better generalisation capability. This fact emphasises the importance of concentrating on the regularity information of training data in order to improve generalisation. However, the training data contain erroneous information owing to noise and outliers. In this paper, we propose a new regularisation approach for convolutional neural networks with better generalisation properties. Specifically, the weights of the convolution layers are perturbed by additive noise in each learning iteration. The approach provides a better model for prediction, as shown by the experimental results on a number of medical benchmark data sets. Furthermore, the effectiveness and accuracy of the proposed convolutional neural network are demonstrated by comparing with several recent perturbation techniques. (C) 2020 Published by Elsevier Ltd.																	0957-4174	1873-6793				JUL 1	2020	149								113196	10.1016/j.eswa.2020.113196													
J								Chimp optimization algorithm	EXPERT SYSTEMS WITH APPLICATIONS										Chimp; Mathematical model; Metaheuristic; Optimization	PERCEPTRON NEURAL-NETWORK; CLASSIFICATION; EVOLUTIONARY; TRAINER	This paper proposes a novel metaheuristic algorithm called Chimp Optimization Algorithm (ChOA) inspired by the individual intelligence and sexual motivation of chimps in their group hunting, which is different from the other social predators. ChOA is designed to further alleviate the two problems of slow convergence speed and trapping in local optima in solving high-dimensional problems. In this paper, a mathematical model of diverse intelligence and sexual motivation of chimps is proposed. In this regard, four types of chimps entitled attacker, barrier, chaser, and driver are employed for simulating the diverse intelligence. Moreover, four main steps of hunting, i.e. driving, chasing, blocking, and attacking, are implemented. The proposed ChOA algorithm is evaluated in 3 main phases. First, a set of 30 mathematical benchmark functions is utilized to investigate various characteristics of ChOA. Secondly, ChOA was tested by 13 high-dimensional test problems. Finally, 10 real-world optimization problems were used to evaluate the performance of ChOA. The results are compared to several newly proposed meta-heuristic algorithms in terms of convergence speed, the probability of getting stuck in local minimums, and exploration, exploitation. Also, statistical tests were employed to investigate the significance of the results. The results indicate that the ChOA outperforms the other benchmark optimization algorithms. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 1	2020	149								113338	10.1016/j.eswa.2020.113338													
J								Anomaly pattern detection for streaming data	EXPERT SYSTEMS WITH APPLICATIONS										Anomaly pattern detection; Control charts; Hypothesis testing; Outlier detection; Streaming data	OUTLIER	Outlier detection aims to find a data sample that is different from most other data samples. While outlier detection is performed at an individual instance level, anomaly pattern detection on a data stream means detecting a time point where a pattern to generate data is unusual and significantly different from normal behavior. Beyond predicting the outlierness of individual data samples in a data stream, it can be very useful to detect the occurrence of anomalous patterns in real time. In this paper, we propose a method for anomaly pattern detection in a data stream based on binary classification for outliers and statistical tests on a data stream of binary labels of normal or an outlier. In the first step, by applying the clustering-based outlier detection method, we transform a data stream into a stream of binary values where 0 stands for the prediction as normal data and 1 for outlier prediction. In the second step, anomaly pattern detection is performed on a stream of binary values by two approaches: testing the equality of parameters in the binomial distributions of a reference window and a detection window, and using control charts for the fraction defective. The proposed method obtained the average true positive detection rate of 94% in simulated experiments using real and artificial data. The experimental results also show that anomaly pattern occurrence can be detected reliably even when outlier detection performance is relatively low. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 1	2020	149								113252	10.1016/j.eswa.2020.113252													
J								Anomaly explanation with random forests	EXPERT SYSTEMS WITH APPLICATIONS										Anomaly detection; Anomaly explanation; Classification rules; Feature selection; Random forests	OUTLIER DETECTION; CLASSIFIER	Anomaly detection has become an important topic in many domains with many different solutions proposed until now. Despite that, there are only a few anomaly detection methods trying to explain how the sample differs from the rest. This work contributes to filling this gap because knowing why a sample is considered anomalous is critical in many application domains. The proposed solution uses a specific type of random forests to extract rules explaining the difference, which are then filtered and presented to the user as a set of classification rules sharing the same consequent, or as the equivalent rule with an antecedent in a disjunctive normal form. The quality of that solution is documented by comparison with the state of the art algorithms on 34 real-world datasets. (C) 2020 Published by Elsevier Ltd.																	0957-4174	1873-6793				JUL 1	2020	149								113187	10.1016/j.eswa.2020.113187													
J								A multi-objective genetic algorithm for text feature selection using the relative discriminative criterion	EXPERT SYSTEMS WITH APPLICATIONS										Text classification; Feature selection; Multi-objective optimization; Relative Discriminative Criterion; Relevancy; Redundancy	MUTUAL INFORMATION; CLASSIFICATION; OPTIMIZATION; RELEVANCE; SCHEME	With exponentially increasing the number of digital documents, text classification has become a major task in data science applications. Selecting discriminative features highly relevant to class labels while having low levels of redundancy is essential to improve the performance of text classification methods. In this paper, we propose a novel multi-objective algorithm for text feature selection, called MultiObjective Relative Discriminative Criterion (MORDC), which balances minimal redundant features against those maximally relevant to the target class. The proposed method employs a multi-objective evolutionary framework to search through the solution space. The first objective function measures the relevance of the text features to the target class, whereas the second one evaluates the correlation between the features. None of these objectives use learning to evaluate the goodness of the selected features; thus, the proposed method can be classified as a multivariate filter method. In order to assess the effectiveness of the proposed method, several experiments are performed on three real-world datasets. Comparisons with state-of-the-art feature selection methods show that in most cases MORDC results in better classification performance. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 1	2020	149								113276	10.1016/j.eswa.2020.113276													
J								Diversity-preserving quantum particle swarm optimization for the multidimensional knapsack problem	EXPERT SYSTEMS WITH APPLICATIONS										Binary optimization; Multidimensional knapsack problem; Population-based metaheuristics; Quantum particle swarm optimization; Diversity-preserving population updating strategy	LOCAL SEARCH; TABU SEARCH; ALGORITHM; HYBRID	Quantum particle swarm optimization is a population-based metaheuristic that becomes popular in recent years in the field of binary optimization. In this paper, we investigate a novel quantum particle swarm optimization algorithm, which integrates a distanced-based diversity-preserving strategy for population management and a local optimization method based on variable neighborhood descent for solution improvement. We evaluate the proposed method on the classic NP-hard 0-1 multidimensional knapsack problem. We present extensive computational results on the 270 benchmark instances commonly used in the literature to show the competitiveness of the proposed algorithm compared to several state-of-the-art algorithms. The ideas of using the diversity-preserving strategy and the probabilistic application of a local optimization procedure are of general interest and can be used to reinforce other quantum particle swarm algorithms. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 1	2020	149								113310	10.1016/j.eswa.2020.113310													
J								Solving complex problems using model transformations: from set constraint modeling to SAT instance solving	EXPERT SYSTEMS WITH APPLICATIONS										Model transformations; Constraint programming; Set constraints; SAT encoding; Combinatorial problem	MINIZINC	On the one hand, solvers for the propositional satisfiability problem (SAT) can deal with huge instances composed of millions of variables and clauses. On the other hand, Constraint Satisfaction Problems (CSP) can model problems as constraints over a set of variables with non-empty domains. They require combinatorial search methods as well as heuristics to be solved in a reasonable time. In this article, we present a technique that benefits from both expressive CSP modeling and efficient SAT solving. We model problems as CSP set constraints. Then, a propagation algorithm reduces the domains of variables by removing values that cannot participate in any valid assignment. The reduced CSP set constraints are transformed into a set of suitable SAT instances. They may be simplified by a preprocessing method before applying a standard SAT solver for computing their solutions. The practical usefulness of this technique is illustrated with two well-known problems: a) the Social Golfer, and b) the Sports Tournament Scheduling. We obtained competitive results either compared with ad hoc solvers or with hand-written SAT instances. Compared with direct SAT modeling, the proposed technique offers higher expressiveness, is less error-prone, and is relatively simpler to apply. The automatically generated propositional satisfiability instances are rather small in terms of clauses and variables. Hence, applying the constraint propagation phase, even huge instances of our problems can be tackled and efficiently solved. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 1	2020	149								113243	10.1016/j.eswa.2020.113243													
J								Memetic niching-based evolutionary algorithms for solving nonlinear equation system	EXPERT SYSTEMS WITH APPLICATIONS										Nonlinear equation system; Niching techniques; Numerical methods; Evolutionary algorithms	OPTIMIZATION ALGORITHM; MODEL	In numerical computation, finding multiple roots of nonlinear equation systems (NESs) in a single run is a fundamental and difficult problem. Recently, evolutionary algorithms (EAs) have been applied to solve NESs. However, due to the diversity preservation mechanism that EAs use, the accuracy of the roots may be reduced. To remedy this drawback, we propose a generic framework of memetic niching-based EA, referred to as MENI-EA. The main features of the framework are: i) the numerical method for a NES is integrated into an EA to obtain highly accurate roots; ii) the niching technique is employed to improve the diversity of the population; iii) different roots of the NESs are located simultaneously in a singe run; and iv) different numerical methods and different niching techniques can be used in the framework. To evaluate the performance of our approach, thirty NESs were chosen from the literature as the test suite. Experimental results show that the proposed approach is capable of yielding promising performance for different NESs in both the root ratio and success rate. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 1	2020	149								113261	10.1016/j.eswa.2020.113261													
J								A new approach for instance selection: Algorithms, evaluation, and comparisons	EXPERT SYSTEMS WITH APPLICATIONS										Big data; Data mining; Instance selection; Global density function; Time complexity	BIG DATA	Several approaches for instance selection have been put forward as a primary step to increase the efficiency and accuracy of algorithms applied to mine big data. The instance selection task scales indeed big data down by removing irrelevant, redundant, and unreliable data, which, in turn, reduces the computational resources necessary for completing the mining task. The local density-based approaches are recently acknowledged as feasible approaches in terms of reduction rate, effectiveness, and computation time metrics. However, these approaches endure low classification accuracy results compared with other approaches. In this manuscript, we propose a new layered and operational approach to address these limitations as well as advance the state-of-the-art by balancing among classification accuracy, reduction rate, and time complexity. We commence by designing a new algorithm (called GDIS) that selects most relevant instances using a global density and relevance functions. This enable us to consider a global view overall a data set to get a better classification accuracy results than current density-based approaches. We design another novel algorithm (called EGDIS), which maintains the effectiveness results of the GDIS algorithm while improving reduction rate results. Moreover, we compare our algorithms against three state-of-the-art algorithms to validate their performance. We develop a Java toolkit called ISTK on the top of the GDIS and EGDIS algorithms, the density-based approaches, and the state-of-the-art algorithms. We also develop a suitable user interface and its management and validation capabilities to ease-of-use and visualize results and data sets. We evaluate and test the performance of our algorithms in terms of four metrics (reduction rate, classification accuracy, effectiveness, and computation time) using twenty-four standard data sets and conduct an intensive set of experiments. The experimental results proved that the GDIS algorithm outperforms the density-based approaches in terms of classification accuracy and effectiveness, the EGDIS algorithm outperforms the density-based approaches in terms of reduction rate and effectiveness, and the GDIS and EGDIS algorithms outperform the state-of-the-art algorithms in terms of achieving a good results in both the effectiveness and computation time metrics. We finally test the scalability and compute experimentally the polynomial-time complexity of our algorithms. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 1	2020	149								113297	10.1016/j.eswa.2020.113297													
J								Resolving data sparsity and cold start problem in collaborative filtering recommender system using Linked Open Data	EXPERT SYSTEMS WITH APPLICATIONS										Collaborative filtering; Matrix factorization; Linked open data, recommender system; Data sparsity	MATRIX FACTORIZATION; MODEL; GRAPH; TIME; FRAMEWORK; ENTITIES; SUPPORT; LINKING; SEARCH	The web contains a huge volume of data, and it's populating every moment to the point that human beings cannot deal with the vast amount of data manually or via traditional tools. Hence an advanced tool is required to filter such massive data and mine the valuable information. Recommender systems are among the most excellent tools for such a purpose in which collaborative filtering is widely used. Collaborative filtering (CF) has been extensively utilized to offer personalized recommendations in electronic business and social network websites. In that, matrix factorization is an efficient technique; however, it depends on past transactions of the users. Hence, there will be a data sparsity problem. Another issue with the collaborative filtering method is the cold start issue, which is due to the deficient information about new entities. A novel method is proposed to overcome the data sparsity and the cold start problem in CF. For cold start issue, Recommender System with Linked Open Data (RS-LOD) model is designed and for data sparsity problem, Matrix Factorization model with Linked Open Data is developed (MF-LOD). A LOD knowledge base "DBpedia" is used to find enough information about new entities for a cold start issue, and an improvement is made on the matrix factorization model to handle data sparsity. Experiments were done on Netflix and MovieLens datasets show that our proposed techniques are superior to other existing methods, which mean recommendation accuracy is improved. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 1	2020	149								113248	10.1016/j.eswa.2020.113248													
J								Cascaded deep learning-based efficient approach for license plate detection and recognition	EXPERT SYSTEMS WITH APPLICATIONS										License plate detection and recognition; Deep segmentation; CNN; Arabic number classification		Automatic license plate (ALP) detection and recognition is an important task for both traffic surveillance and parking management systems, as well as being crucial to maintaining the flow of modern civic life. Various ALP detection and recognition methods have been proposed to date. These methods generally use various image processing and machine learning techniques. In this paper, a cascaded deep learning approach is proposed in order to construct an efficient ALP detection and recognition system for the vehicles of northern Iraq. The license plates in northern Iraq contain three regions, namely a plate number, a city region, and a country region. The proposed method initially employs several preprocessing techniques such as Gaussian filtering and adaptive image contrast enhancement to make the input images more suited to further processing. Then, a deep semantic segmentation network is used in order to determine the three license plate regions of the input image. Segmentation is then carried out via deep encoder-decoder network architecture. The determined license plate regions are fed into two separate convolutional neural network (CNN) models for both Arabic number recognition and the city determination. For Arabic number recognition, an end-to-end CNN model was constructed and trained, whilst for the city recognition, a pretrained CNN model was further fine-tuned. A new license plate dataset was also constructed and used in the experimental works of the study. The performance of the proposed method was evaluated both in terms of detection and recognition. For detection, recall, precision and F-measure scores were used, and for recognition, classification accuracy was used. The obtained results showed the proposed method to be efficient in both license plate detection and recognition. The calculated recall, precision and F-measure scores were 92.10%, 94.43%, and 91.01%, respectively. Moreover, the classification accuracies for Arabic numbers and city labels were shown to be 99.37% and 92.26%, respectively. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 1	2020	149								113280	10.1016/j.eswa.2020.113280													
J								The Self-Organizing Restricted Boltzmann Machine for Deep Representation with the Application on Classification Problems	EXPERT SYSTEMS WITH APPLICATIONS										Deep learning; Self-organizing restricted Boltzmann machines; Separability-correlation measure; MNIST; Moore-set; Wisconsin breast cancer dataset	NEURAL-NETWORKS; DIMENSIONALITY	Recently, deep learning is proliferating in the field of representation learning. A deep belief network (DBN) consists of a deep network architecture that can generate multiple features of input patterns, using restricted Boltzmann machines (RBMs) as a building block of DBN. A deep learning model can achieve extremely high accuracy in many applications that depend on the model structure. However, specifying various parameters of deep network architecture like the number of hidden layers and neurons is a difficult task even for expert designers. Besides, the number of hidden layers and neurons is typically set manually, while this method is costly in terms of time and computational cost, especially in big data. In this paper, we introduce an approach to determine the number of hidden layers and neurons of the deep network automatically during the learning process. To this end, the input vector is transformed from the feature space with a low dimension into the new feature space with a high dimension in a hidden layer of RBM. In the following, new features are ranked according to their discrimination power between classes in the new space, using the Separability-correlation measure for feature importance ranking algorithm. The algorithm uses the mean of weights as a threshold, so the neurons whose weights exceed the threshold are retained, and the others are removed in the hidden layer. The number of retained neurons is presented as a reasonable number of neurons. The number of layers is also determined in the deep model, using the validation data. The proposed approach acts as a regularization method since the neurons whose weights are lower than the threshold are removed; thus, RBM learns to copy input merely approximate. It also prevents over-fitting with a suitable number of hidden layers and neurons. Eventually, DBN can determine its structure according to the input data and is the self-organizing model. The experimental results on benchmark datasets confirm the proposed method. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 1	2020	149								113286	10.1016/j.eswa.2020.113286													
J								Classification of human hand movements based on EMG signals using nonlinear dimensionality reduction and data fusion techniques	EXPERT SYSTEMS WITH APPLICATIONS										Electromyography; Machine learning; Principal component analysis; Diffusion maps; Data fusion; Graph alignment	MYOELECTRIC PATTERN-RECOGNITION; DIFFUSION	Surface electromyography (EMG) is non-invasive signal acquisition technique that plays a central role in many application, including clinical diagnostics, control for prosthetic devices and for human-machine interactions. The processing typically begins with a feature extraction step, which may be followed by the application of a dimensionality reduction technique. The obtained reduced features are input for a machine learning classifier. The constructed machine learning model may then classify new recorded movements. The features extracted for EMG signals usually capture information both from the time and from the frequency domain. Short time Fourier transform (STFT) is commonly used for signal processing and in particular for EMG processing since it captures the temporal and the frequency characteristics of the data. Since the number of calculated STFT features is large, a common approach in signal processing and machine learning applications is to apply a linear or a nonlinear dimensionality reduction technique for simplifying the feature space. Another aspect that arises in medical applications in general and in EMG based hand classification in particular, is the large variability between subjects. Due to this variability, many studies focus on single subject classification. This requires acquiring a large training set for each tested participant which is not practical in real life application. The objectives of this study were first to compare between the performances of a nonlinear dimensionality technique to a standard linear dimensionality method when applied for single subject EMG based hand movement classification, and to examined their performances in case of limited amount of training data samples. The second objective was to propose an algorithm for multi-subjects classification that utilized a data alignment step for overcoming the large variability between subjects. The data set included EMG signals from 5 subjects who perform 6 different hand movements. STFT was calculated for feature extraction, principal component analysis (PCA) and diffusion maps (DM) were compared for dimension reductions. An affine transformation for aligning between the reduced feature spaces of two subjects, was investigated. K-nearest neighbors (KNN) was used for single and multi-subject classification. The results of this study clearly show that the DM outperformed the PCA in case of limited training data. In addition, the multi-subject classification approach, which utilizes dimension reduction methods along with an alignment algorithm enable robust classification of a new subject based on another subjects' data sets. The proposed framework is general and can be adopted for many EMG classification task. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 1	2020	149								113281	10.1016/j.eswa.2020.113281													
J								Integrating complex event processing and machine learning: An intelligent architecture for detecting IoT security attacks	EXPERT SYSTEMS WITH APPLICATIONS										Complex event processing; Machine learning; Software architecture; Intelligent decision making; Internet of Things; Security attack	DECISION-MAKING; INTERNET; THINGS; SYSTEMS; MODEL	The Internet of Things (IoT) is growing globally at a fast pace: people now find themselves surrounded by a variety of IoT devices such as smartphones and wearables in their everyday lives. Additionally, smart environments, such as smart healthcare systems, smart industries and smart cities, benefit from sensors and actuators interconnected through the IoT. However, the increase in IoT devices has brought with it the challenge of promptly detecting and combating the cybersecurity attacks and threats that target them, including malware, privacy breaches and denial of service attacks, among others. To tackle this challenge, this paper proposes an intelligent architecture that integrates Complex Event Processing (CEP) technology and the Machine Learning (ML) paradigm in order to detect different types of IoT security attacks in real time. In particular, such an architecture is capable of easily managing event patterns whose conditions depend on values obtained by ML algorithms. Additionally, a model-driven graphical tool for security attack pattern definition and automatic code generation is provided, hiding all the complexity derived from implementation details from domain experts. The proposed architecture has been applied in the case of a healthcare IoT network to validate its ability to detect attacks made by malicious devices. The results obtained demonstrate that this architecture satisfactorily fulfils its objectives. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 1	2020	149								113251	10.1016/j.eswa.2020.113251													
J								A new hierarchical multi group particle swarm optimization with different task allocations inspired by holonic multi agent systems	EXPERT SYSTEMS WITH APPLICATIONS										Particle swarm optimization; Hierarchical multi group structure; Holonic organization; Multi agent systems; Task allocation; Exploration/Exploitation	ADAPTIVE INERTIA WEIGHT; ARTIFICIAL BEE COLONY; EXPERT-SYSTEM; GLOBAL OPTIMIZATION; GENETIC ALGORITHM; HYBRID; STRATEGY; EVOLUTION; TOPOLOGY; SELECTION	Nowadays expert systems have been used in different fields. They must be able to operate as quickly and efficiently as possible. So, they need optimization mechanism in their different parts and optimization is a critical part of almost all expert systems. Because of difficulties in real world problems, traditional optimization techniques commonly cannot solve them. Therefore, stochastic algorithms are used to do the optimization in expert systems. Particle swarm optimization (PSO) is one the most famous stochastic optimization algorithms. But this algorithm has some difficulties like losing diversity, premature convergence, trapping in local optimums and imbalance between exploration and exploitation. To overcome these drawbacks, inspired by holonic organization in multi agent systems, a new hierarchical multi group structure for PSO is presented in this paper. Considering the particles in PSO as simple agents, PSO is a kind of multi agent system. Existence of different facilities and organizations in multi agent systems and their great impact on performance encouraged us to use them. So, inspired by holonic multi agent systems, a new structure for PSO is presented. This work has been done for the first time in the literature. Meanwhile, to promote exploration and exploitation ability of proposed structure and create a suitable balance between them, different tasks are assigned to different groups of this structure. So, a holonic PSO with different task allocations (HPSO-DTA) is created. It provides the opportunity to employ all aspects for empowering PSO including parameter settings, neighborhood topologies and learning strategies to enhance the ability of it unlike other versions of PSO that use only one of these aspects to improve their solutions. This structure provides a lot of advantages for PSO. It is a new topological structure that improves the performance of PSO. It provides several leaders with efficient information to guide the particles in the search space. Also, it helps to control suitable information flow between groups and particles in order to preserve diversity and prevent from trapping in local optimums. Meanwhile, with assigning different tasks to different groups of proposed structure, an appropriate balance between exploration and exploitation is created to enhance the performance of the algorithm. In each group, based on its assigned task, particles use different parameters settings, different dynamic neighborhood topologies and different learning strategies which are proposed in this paper to enhance the performance of algorithm. A set of thirty four benchmark functions are used to evaluate the performance of proposed structure. Proposed algorithm is compared with a set of well-known PSO algorithms that their efficiency have been proved. Experimental results and comparative analysis demonstrate good performance of HPSO-DTA compared to other algorithms. Its solution accuracy, convergence speed and robustness is completely appropriate especially in more complicated benchmarks. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 1	2020	149								113292	10.1016/j.eswa.2020.113292													
J								Efficient algorithms for discrete resource allocation problems under degressively proportional constraints	EXPERT SYSTEMS WITH APPLICATIONS										Resource allocation; Degressive proportionality; Branch and bound; Metaheuristic; Parallel computing	PARLIAMENT; SEATS	The problem of a fair distribution is considered in relation to many areas and phenomena. The most deeply rooted in the theory of justice are proportional divisions. However, they may be perceived as unfair for common ventures, where strong participants should not dominate the weaker ones. The European Parliament composition and the cost sharing problem of a common infrastructure development are examples. In this paper, we propose an expert system that is based on a mathematical model describing discussed issues as the discrete resource allocation problem under degressively proportional constraints. This approach involves advantages of degressive proportionality to prevent mentioned domination and a proportional division generally perceived as fair to determine an unambiguous allocation. The decision making process is carried out by solving the formulated optimization problem using our highly scalable parallel branch and bound algorithm and the computationally efficient metaheuristic. The experiments prove that our approach can be successfully applied for the considered cases studies. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 1	2020	149								113293	10.1016/j.eswa.2020.113293													
J								NN-SSTA: A deep neural network approach for statistical static timing analysis	EXPERT SYSTEMS WITH APPLICATIONS										Statistical timing analysis; Statistical operation; Deep neural network; Deep learning; Critical path; Delay	APPROXIMATION; COMPUTATION; DESIGN; MODEL; FLOW	Discrete statistical static timing analysis (SSTA) performs the timing analysis by using statistical maximum and convolution operations. The maximum is basically a non-linear operator and it is not a simple task to capture the skewness introduced by it. On the other hand, the convolution has a potential to "blow-up" the number of discrete samples as we going deep inside the timing graph and hence, results in exponential timing complexity. Therefore, in this paper we present novel deep neural network based operations which can accurately approximate the signal arrival-time's distributions with linear-time complexity. The various deep neural network (DNN) architectures have been used to implement both the maximum and the convolution operations using proper training dataset. Simulation results on various benchmark circuits (ISCAS 85, ISCAS 89, and ITC 99) show that the proposed method estimate the mean and standard deviation (STD) of critical path delay distribution with an average error of 0.75% and 2.56% as compared to Monte Carlo (MC), respectively. Our SSTA speeds up the traditional discrete approach by a factor of 20.7x on average. Furthermore, the PDF obtained from our method matches the ones obtained from MC with a reasonable error. Furthermore, we have proposed multi-wise maximum operations to reduce the arrival-time computational complexity at multi-inputs gates. Comparing to MC, the proposed method shows 0.97% and 2.58% average error in mean and STD respectively and the speeding up factor reaches 24.4x on average for all benchmarks. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 1	2020	149								113309	10.1016/j.eswa.2020.113309													
J								Driver behavior detection and classification using deep convolutional neural networks	EXPERT SYSTEMS WITH APPLICATIONS										Driver behavior; Recurrence plot; Convolutional neural networks; Deep learning	DROWSINESS; SYSTEM; RECOGNITION; FRAMEWORK; DISTANCE; SUPPORT; FUSION	Driver behavior monitoring system as Intelligent Transportation Systems (ITS) have been widely exploited to reduce the traffic accidents risk. Most previous methods for monitoring the driver behavior are rely on computer vision techniques. Such methods suffer from violation of privacy and the possibility of spoofing. This paper presents a novel yet efficient deep learning method for analyzing the driver behavior. We have used the driving signals, including acceleration, gravity, throttle, speed, and Revolutions Per Minute (RPM) to recognize five types of driving styles, including normal, aggressive, distracted, drowsy, and drunk driving. To take the advantages of successful deep neural networks on images, we learn a 2D Convolutional Neural Network (CNN) on images constructed from driving signals based on recurrence plot technique. Experimental results confirm that the proposed method can efficiently detect the driver behavior. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 1	2020	149								113240	10.1016/j.eswa.2020.113240													
J								A novel dice similarity measure for IFSs and its applications in pattern and face recognition	EXPERT SYSTEMS WITH APPLICATIONS										Intuitionistic fuzzy sets (IFS); Similarity measures; Pattern recognition; Facerecognition	INTUITIONISTIC FUZZY-SETS; EXPERT-SYSTEM; VAGUE SETS	In the theory of Intuitionistic fuzzy set (IFS), similarity measure is an effective instrument to measure similarity between IFSs. In this paper, we mention limitations of various existing similarity measures and propose a novel dice similarity measure for IFSs. Proposed dice similarity measure is based on inner product and overcomes limitations of existing similarity measures. In order to show the suitability and applicability of proposed dice similarity measure, we implement it on various classification problems of pattern recognition and medical diagnosis. Experimental results show that this does not only overcome limitations of existing similarity measures but also outperforms in pattern recognition and medical diagnosis problems. In this paper, we also propose an algorithm for face recognition problem using proposed dice similarity measure. This algorithm is demonstrated by an example and performance is compared with few existing methods for face recognition problems. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 1	2020	149								113245	10.1016/j.eswa.2020.113245													
J								Automatic lecture video skimming using shot categorization and contrast based features	EXPERT SYSTEMS WITH APPLICATIONS										Video skimming; Capsule preparation; Histogram features; Contrast	BACKGROUND SUBTRACTION; SEGMENTATION; IMAGE	Video skimming is one of the recently, getting popular technique for preparing preview for long watching video sequences. Most of the video skimming techniques developed in the literature uses manual intervention of users to prepare the review. Mostly the literature reported video skimming for sports and movie industries. In sports the portion of video where audience claps are used and in movie important contents are manually selected for preparing the preview. However in literature rarely any work reported for skimming of lecture video sequences. Lecture videos are generally, recorded indoor, low illuminated, noisy environment condition and contents of the scene rarely changes much. Hence designing an automatic skimming scheme is quite difficult task. In this article, we put forward an intelligent expert video skimming technique for lecture video sequences, where human intervention is not required. In the proposed scheme, initially the lecture video is segmented into a number of shots. We proposed the use of radiometric correlation technique for lecture video segmentation or finding the shot transitions. After getting the shot transitions in a video, the shots are recognized. The fuzzy K-nearest neighborhood technique is proposed to recognize the shots in a video. The shots are recognized into three categories: title slides, written texts/displayed slides and talking heads/writing hands. Three contrast based features: one existing i.e., average sharpness (AS) and two newly proposed: relative height (RH) and edge potential (EP) are used to find the contents of a frame. The frames with different contrast values are categorized to prepare the video skimming or the capsule. The media recreation is achieved by selecting a set of frames around these selected content frames. The effectiveness of the proposed scheme is demonstrated in this paper using five test sequences, including three NPTEL and two non NPTEL. It is also observed that the capsule prepared by the proposed scheme, provides a better preview of the actual sequence. The performance of the proposed scheme is tested by comparing it against three state-of-the-art techniques. The evaluation of the proposed scheme is carried out by using three evaluation measures. It is also observed that the proposed scheme is found to be better than that of the existing schemes. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 1	2020	149								113341	10.1016/j.eswa.2020.113341													
J								An interval-stochastic programming based approach for a fully uncertain multi-objective and multi-mode resource investment project scheduling problem with an application to ERP project implementation	EXPERT SYSTEMS WITH APPLICATIONS										Resource investment project scheduling; Interval programming; Chance-constrained programming; Human resource allocation; ERP project management; Risk attitude	GENETIC ALGORITHM; HEURISTICS; NETWORKS; SEARCH; RCPSP; COSTS	Most of the real-life project scheduling cases may involve different types of uncertainties simultaneously such as randomness, fuzziness and dynamism. Based on this motivation, the present paper proposes a novel interval programming and chance constrained optimization based hybrid solution approach for a fully uncertain, multi-objective and multi-mode resource investment project scheduling problem (MRIPSP). The classical discrete-time binary integer programming formulation of the problem is extended by incorporating both the interval-valued and interval-stochastic project parameters as well as variables. In addition to the uncertain project parameters/inputs, the completion times of the activities which represent the project schedule and the availabilities of the renewable project resources are also stated as uncertain project variables and represented by interval numbers. Then, the proposed interval-stochastic multi-mode resource investment project scheduling (IS-MRIPSP) model is converted into its crisp equivalent form by using the proposed approach. The proposed approach is also able to consider different types of project scheduling risks and produces more reliable and risk-free solutions according to the project manager's attitude toward risks. Furthermore, in addition to the classical makespan objective, effective and efficient utilization of the renewable project resources, i.e., human resources, is also targeted. The efficiency and reliability factors of the human resources are also taken into consideration. In order to generate balanced project schedules which tradeoffbetween the project time and total human resource costs, compromise programming approach is adapted. Finally, in order to test the validity and practicality of the proposed approach, a real-life application is presented for an enterprise resource planning (ERP) implementation project scheduling problem of an international industrial software company. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 1	2020	149								113189	10.1016/j.eswa.2020.113189													
J								Motor imagery EEG recognition based on conditional optimization empirical mode decomposition and multi-scale convolutional neural network	EXPERT SYSTEMS WITH APPLICATIONS										Empirical mode decomposition; Convolutional neural network; Motor imagery EEG; Feature extraction; Intelligent wheelchair	FEATURE-EXTRACTION; CLASSIFICATION; BCI; SIGNALS; SYSTEM	Electroencephalogram (EEG) signals classification plays a crucial role in brain computer interfaces (BCIs) system. However, the inherent complex properties of EEG signals make it challenging to get them analyzed and modeled. In this paper, a novel method based on conditional empirical mode decomposition (CEMD) and one-dimensional multi-scale convolutional neural network (1DMSCNN) is proposed to recognize motor imagery (MI) EEG signals. In the CEMD algorithm, the correlation coefficient between the original EEG signal and each intrinsic modal component (IMF) is used as the first condition to select IMFs, and the relative energy occupancy rates between the IMFs are the second condition. The CEMD algorithm is applied to remove the noise of EEG signals. Then, an EEG signals combination method is proposed to encode event-related synchronization/de-synchronization (ERS/ERD) information between the channels. Finally, a model called 1DMSCNN is built to classify the processed EEG signals. The proposed method is applied to the dataset collected in our laboratory and BCI competition IV dataset 2b. The results indicate that the proposed method can achieve higher accuracy for EEG signals classification, compared with other state-of-the-art works. In addition, the proposed algorithm is applied to the online recognition of EEG signals, a BCI system that directly interacts with brain and wheelchair is designed and implemented. This system can directly command wheelchair to turn left and right through EEG signals. The online experimental results indicate that the designed intelligent wheelchair system is a feasible BCI application. It verifies the proposed algorithm can be used in expert and intelligent systems. Our method can provide a stimulus to the development of human-robot interaction. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 1	2020	149								113285	10.1016/j.eswa.2020.113285													
J								Classification of brain MRI using hyper column technique with convolutional neural network and feature selection method	EXPERT SYSTEMS WITH APPLICATIONS										Biomedical signal processing; Decision support system; Brain MRI; Hypercolumn technique; Feature selection	TUMOR CLASSIFICATION; SVM	A proper and certain brain tumor MRI classification has a significant role in current clinical diagnosis, decision making as well as managing the treatment programs. In clinical practice, the examination is performed visually by the specialists, this is a labor-intensive and error-prone process. Therefore, the computer-based systems are in demand so as to carry out objectively this process. In the traditional machine learning approaches, the low-level and high-level handcrafted features used to describe the brain tumor MRI are extracted and classified to overcome the mentioned drawbacks. Considering the recent advances in deep learning, we propose a novel convolutional neural network (CNN) model that is combined with the hypercolumn technique, pretrained AlexNet and VGG-16 networks, recursive feature elimination (RFE), and support vector machine (SVM) in this study. One of the great advantages of the proposed model is that with the help of the hypercolumn technique, it can keep the local discriminative features, which are extracted from the layers located at the different levels of the deep architectures. In addition, the proposed model exploits the generalization abilities of both AlexNet and VGG-16 networks by fusing the deep features achieved from the last fully-connected layers of the networks. Furthermore, the discriminative capacity of the proposed model is enhanced using RFE and thus the most effective deep features are revealed. As a result, the proposed model yielded an accuracy of 96.77% without using any handcrafted feature engine. A fully automated consistent and effective diagnostic model is ensured for the brain tumor MRI classification. Consequently, the proposed model can contribute to realizing a more objective evaluation in the clinics, supporting the decision-making process of the experts, and reducing misdiagnosis rates. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 1	2020	149								113274	10.1016/j.eswa.2020.113274													
J								Optimization of coverage mission for lightweight unmanned aerial vehicles applied in crop data acquisition	EXPERT SYSTEMS WITH APPLICATIONS										Coverage; Crop; Heuristic; Positioning; Unmanned aerial vehicle; ROS	CALIBRATION	The crop data acquisition with unmanned aerial vehicles is a popularized alternative to manage the agricultural processes, due to data emerging from portable sensors for image-gathering. Nevertheless, most unmanned aerial vehicles for data acquisition excess the cost of hundreds of dollars, making them inappropriate for small agricultural producers. In this paper, we proposed to achieve crop data acquisition using a Lightweight Unmanned Aerial Vehicle (LUAV), available at a reasonable cost. However, a LUAV has less flight time and robustness than the professional vehicles. To overcome the limitations, we designed a LUAV agent with the goal of optimizing coverage paths using a heuristic strategy in known areas. The path to follow can be selected from three algorithms, Wavefront, Dijkstra or Spiral, which are compared to define an option for the crop under study. A second goal is to improve the LUAV robustness, which was resolved from planning by selecting the start of the coverage mission in order to the flight lines cross the direction of the wind. We complemented the robustness of outdoors positioning using a Kalman Filter extension to specify movements during missions. Finally, using an AR Drone 2.0 quadcopter, we developed a prototype of the LUAV agent to obtain the mosaic of a grass crop. The results respect to optimized coverage mission showed that the Spiral algorithm with a Backtracking technique and avoiding areas of little interest, got the balanced score between revisits, turns, coverage percent and traveled distance. About the LUAV robustness in the presence of wind, the results stated an error of less than 2 m, considered acceptable for image-acquisition purposes. The developed work is simple but effective, and makes evident the viability for that any LUAV type can support the precision agriculture processes in favorable costs. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 1	2020	149								113227	10.1016/j.eswa.2020.113227													
J								An improved firefly algorithm for global continuous optimization problems	EXPERT SYSTEMS WITH APPLICATIONS										Adaptive switch; Logarithmic spiral; Firefly algorithm; Global continuous optimization	SIDEWAYS VISION; FLIGHT PATHS	Global continuous optimization is populated by its implementation in many real-world applications. Such optimization problems are often solved by nature-inspired and meta-heuristic algorithms, including the firefly algorithm (FA), which offers fast exploration and exploitation. To further strengthen FA's search for global optimum, a Levy-flight FA (LF-FA) has been developed through sampling from a Levy distribution instead of the traditional uniform one. However, due to its poor exploitation in local areas, the LF-FA does not guarantee fast convergence. To address this problem, this paper provides an adaptive logarithmic spiral-Levy FA (AD-IFA) that strengthens the LF-FA's local exploitation and accelerates its convergence. Our AD-IFA is integrated with logarithmic-spiral guidance to its fireflies' paths, and adaptive switching between exploration and exploitation modes during the search process. Experimental results show that the AD-IFA presented in this paper consistently outperforms the standard FA and LF-FA for 29 test functions and 6 real cases of global optimization problems in terms of both computation speed and derived optimum. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 1	2020	149								113340	10.1016/j.eswa.2020.113340													
J								Alleviating the data sparsity problem of recommender systems by clustering nodes in bipartite networks	EXPERT SYSTEMS WITH APPLICATIONS										Recommender system; Sparsity; Bipartite network; Clustering nodes; Collaborative filtering	ACCURACY; MODEL	Recommender systems help users to find information that fits their preferences in an overloaded search space. Collaborative filtering systems suffer from increasingly severe data sparsity problem because more and more products are sold in commercial websites, which largely constrains the performance of recommendation algorithms. User clustering has already been applied to recommendation on sparse data in the literature, but in a completely different way. In most existing works, user clustering is directly used to identify the similar users of the target user to whom we want to make recommendation. More specifically, the users who are clustered in the same group of the target user are considered as similar users. However, in this paper we use user clustering to reconstruct the user-item bipartite network such that the network density is significantly improved. The recommendation made on this dense network thus can achieve much higher accuracy than on the original sparse network. The experimental results on three benchmark data sets demonstrate that, when facing the problem of data sparsity, our proposed recommendation algorithm based on node clustering achieves a significant improvement in accuracy and coverage of recommendation. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 1	2020	149								113346	10.1016/j.eswa.2020.113346													
J								Novel trajectory privacy-preserving method based on clustering using differential privacy	EXPERT SYSTEMS WITH APPLICATIONS										Trajectory data; Cluster analysis; Privacy protection; Differential privacy		With the development of location-aware technology, a large amount of location data of users is collected by the trajectory database. If these trajectory data are directly used for data mining without being processed, it will pose a threat to the user's personal privacy. At the moment, differential privacy is favored by experts and scholars because of its strict mathematical rigor, but how to apply differential privacy technology to trajectory clustering analysis is a difficult problem. To solve the problems in which existing trajectory privacy-preserving models have poor data availability or difficulty to resist complex privacy attacks, we devise novel trajectory privacy-preserving method based on clustering using differential privacy. More specifically, Laplacian noise is added to the count of trajectory location in the cluster to resist the continuous query attack. Then, radius-constrained Laplacian noise is added to the trajectory location data in the cluster to avoid too much noise affecting the clustering effect. According to the noise location data and the count of noise location, the noise clustering center in the cluster is obtained. Finally, it is considered that the attacker can associate the user trajectory with other information to form secret reasoning attack, and secret reasoning attack model is proposed. And we use the differential privacy technology to give corresponding resistance. Experimental results using the open data show that the proposed algorithm can not only effectively protect the private information of the trajectory data, but also ensure the data availability in cluster analysis. And compared with other algorithms, our algorithm has good effect on some evaluation indicators. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				JUL 1	2020	149								113241	10.1016/j.eswa.2020.113241													
J								Urban flow prediction from spatiotemporal data using machine learning: A survey	INFORMATION FUSION										Urban flow prediction; Spatiotemporal data mining; Data fusion; Deep learning; Urban computing	MOVING AVERAGE; UNCERTAINTY; ALGORITHM; FRAMEWORK	Urban spatiotemporal flow prediction is of great importance to traffic management, land use, public safety. This prediction task is affected by several complex and dynamic factors, such as patterns of human activities, weather, events, and holidays. Datasets evaluated the flow come from various sources in different domains, e.g. mobile phone data, taxi trajectories data, metro/bus swiping data, bike-sharing data. To summarize these methodologies of urban flow prediction, in this paper, we first introduced four main factors affecting urban flow. Second, in order to further analyze urban flow, we partitioned the preparation process of multi-source spatiotemporal data related with urban flow into three groups. Third, we chose the spatiotemporal dynamic data as a case study for the urban flow prediction task. Fourth, we analyzed and compared some representative flow prediction methods in detail, classifying them into five categories: statistics-based, traditional machine learning-based, deep learning-based, reinforcement learning-based, and transfer learning-based methods. Finally, we showed open challenges of urban flow prediction and discussed many recent research works on urban flow prediction. This paper will facilitate researchers to find suitable methods and public datasets for addressing urban spatiotemporal flow forecast problems.																	1566-2535	1872-6305				JUL	2020	59						1	12		10.1016/j.inffus.2020.01.002													
J								Reaching a minimum adjustment consensus in social network group decision-making	INFORMATION FUSION										Social network group decision-making; Consensus reaching process; Weight allocation; Minimum adjustment; Optimal feedback mechanism; Incomplete linguistic preference relations	PREFERENCE RELATIONS; INDIVIDUAL CONSISTENCY; SUPPORT-SYSTEM; COST; MODELS; MECHANISM; TRUST; SIMILARITY; WEIGHTS	In this paper, we propose a minimum adjustment consensus framework for the social network group decision-making (SN-GDM) with incomplete linguistic preference relations (ILPRs). The extant studies ignore the influence of network structure on the decision-makers' (DMs') weights, and set a fixed parameter to adjust DM's preferences that may lead to the inefficiency of reaching a consensus. To solve these issues, we first propose a weight allocation method with the structural hole theory by analyzing the tie strength and topology structure of DM's social networks. After obtaining DMs' weights, the consistency/consensus indexes at three levels are constructed and used to identify the inconsistent DMs. Then, a novel minimum adjustment consensus model (MACM) for ILPRs is proposed to obtain the optimal adjustment parameters, which are used to recommend customized adjustments in the feedback mechanism. The existence of optimal solutions and the convergence of the proposed consensus models under certain conditions are also proved. Finally, the validity of the proposed method is verified by an application example. Different from the extant MACMs, we optimized the adjustment parameters just for inconsistent DMs instead of all DMs' adjusted preference values. With less number of consensus rounds and lower costs, we also improved the classical feedback mechanism and established its connection with the current MACMs.																	1566-2535	1872-6305				JUL	2020	59						30	43		10.1016/j.inffus.2020.01.004													
J								Overview and comparative study of dimensionality reduction techniques for high dimensional data	INFORMATION FUSION										Dimensionality reduction; Features; High dimensional data; Linear techniques; Nonlinear techniques	PRINCIPAL COMPONENT ANALYSIS; LOCALITY PRESERVING PROJECTIONS; SELF-ORGANIZING MAPS; LINEAR DISCRIMINANT-ANALYSIS; LEARNING VECTOR QUANTIZATION; LATENT SEMANTIC ANALYSIS; TEXT MINING TECHNIQUES; FEATURE-EXTRACTION; MIXTURE MODEL; PURSUIT	The recent developments in the modern data collection tools, techniques, and storage capabilities are leading towards huge volume of data. The dimensions of data indicate the number of features that have been measured for each observation. It has become a challenging task to analyze high dimensional data. Different dimensionality reduction techniques are available in literature to eliminate irrelevant and redundant features. Selection of an appropriate dimension reduction technique can help to enhance the processing speed and reduce the time and effort required to extract valuable information. This paper presents the state-of-the art dimensionality reduction techniques and their suitability for different types of data and application areas. Furthermore, the issues of dimensionality reduction techniques have been highlighted that can affect the accuracy and relevance of results.																	1566-2535	1872-6305				JUL	2020	59						44	58		10.1016/j.inffus.2020.01.005													
J								An overview on spectral and spatial information fusion for hyperspectral image classification: Current trends and challenges	INFORMATION FUSION										Hyperspectral image; Feature fusion; Decision fusion; Classification	JOINT COLLABORATIVE REPRESENTATION; SUPPORT VECTOR MACHINES; LOCAL BINARY PATTERNS; FEATURE-EXTRACTION; PRESERVING PROJECTION; FRAMEWORK; AUGMENTATION; CLASSIFIERS; TRANSFORM; SUBSPACE	Hyperspectral images (HSIs) have a cube form containing spatial information in two dimensions and rich spectral information in the third one. The high volume of spectral bands allows discrimination between various materials with high details. Moreover, by utilizing the spatial features of image such as shape, texture and geometrical structures, the land cover discrimination will be improved. So, fusion of spectral and spatial information can significantly improve the HSI classification. In this work, the spectral-spatial information fusion methods are categorized into three main groups. The first group contains segmentation based methods where objects or super-pixels are used instead of pixels for classification or the obtained segmentation map is used for relaxation of the pixel-wise classification map. The second group consists of feature fusion methods which are divided into six sub-groups: features stacking, joint spectral-spatial feature extraction, kernel based classifiers, representation based classifiers, 3D spectral-spatial feature extraction and deep learning based classifiers. The third fusion methods are decision fusion based approaches where complementary information of several classifiers are contributed for achieving the final classification map. A review of different methods in each category, is presented. Moreover, the advantages and difficulties/disadvantages of each group are discussed. The performance of various fusion methods are assessed in terms of classification accuracy and running time using experiments on three popular hyperspectral images. The results show that the feature fusion methods although are time consuming but can provide superior classification accuracy compared to other methods. Study of this work can be very useful for all researchers interested in HSI feature extraction, fusion and classification.																	1566-2535	1872-6305				JUL	2020	59						59	83		10.1016/j.inffus.2020.01.007													
J								Large-Scale decision-making: Characterization, taxonomy, challenges and future directions from an Artificial Intelligence and applications perspective	INFORMATION FUSION										Large-scale decision making; Group decision making; Consensus reaching processes; Behaviour management; Subgroup clustering; Artificial Intelligence; Preference modelling	PERSONALIZED INDIVIDUAL SEMANTICS; LINGUISTIC REPRESENTATION MODEL; CONSENSUS REACHING PROCESS; NONCOOPERATIVE BEHAVIORS; TERM SETS; PREFERENCE; INFORMATION; AGGREGATION; SYSTEMS; MAKERS	The last decade witnessed tremendous developments in social media and e-democracy technologies. A fundamental aspect in these paradigms is that the number of decision makers allowed to partake in a decision making event drastically increases. As a result Large Scale Decision Making (LSDM) has established itself as an emerging and rapidly developing research field, attracting comprehensive studies in the last decade. LSDM events are a complex class of decision making problems, in which multiple and highly diverse stakeholders are involved and the provided alternatives are assessed considering multiple criteria/attributes. Since some of the extant LSDM research was extended from group decision making scenarios, there is no established definition for a LSDM problem as of yet. We firstly propose a clear definition and characterization of LSDM events as a basis for characterizing this emerging family of decision frameworks. Secondly, a classification of LSDM literature is provided. Effectively solving an LSDM problem is usually a complex and challenging process, in which reaching a high consensus or accounting for the agreement or conflict relationships between participants becomes critical. Accordingly, we present a taxonomy and an overview of LSDM models, predicated on their key elements, i.e. the procedures and specific steps followed by the existing models: consensus measurement, subgroup clustering, behavior management, and consensus building mechanisms. Finally, we provide a discussion in which we identify research challenges and propose future research directions under a triple perspective: key LSDM methodologies, AI and data fusion for LSDM, and innovative applications. The potential rise of AI-based LSDM is particularly highlighted in the discussion provided.																	1566-2535	1872-6305				JUL	2020	59						84	102		10.1016/j.inffus.2020.01.006													
J								Feature-level fusion approaches based on multimodal EEG data for depression recognition	INFORMATION FUSION										Depression recognition; EEG; Multimodal; Audio stimulus; Fusion	BODY SENSOR NETWORKS; FEATURE-SELECTION; CLASSIFICATION; ASYMMETRY; THETA; FREQUENCY; DIAGNOSIS; ENTROPY; ALPHA; STATE	This study aimed to construct a novel multimodal model by fusing different electroencephalogram (EEG) data sources, which were under neutral, negative and positive audio stimulation, to discriminate between depressed patients and normal controls. The EEG data of different modalities were fused using a feature-level fusion technique to construct a depression recognition model. The EEG signals of 86 depressed patients and 92 normal controls were recorded simultaneously while receiving different audio stimuli. Then, from the EEG signals of each modality, linear and nonlinear features were extracted and selected to obtain features of each modality. In addition, a linear combination technique was used to fuse the EEG features of different modalities to build a global feature vector and find several powerful features. Furthermore, genetic algorithms were used to perform feature weighting to improve the overall performance of the recognition framework. The classification accuracy of each classifier, namely the k-nearest neighbor (KNN), decision tree (DT), and support vector machine (SVM), was compared, and the results were encouraging. The highest classification accuracy of 86.98% was obtained by the KNN classifier in the fusion of positive and negative audio stimuli, demonstrating that the fusion modality could achieve higher depression recognition accuracy rate compared with the individual modality schemes. This study may provide an additional tool for identifying depression patients.																	1566-2535	1872-6305				JUL	2020	59						127	138		10.1016/j.inffus.2020.01.008													
J								Contextual deep learning-based audio-visual switching for speech enhancement in real-world environments	INFORMATION FUSION										Context-aware learning; Multi-modal speech enhancement; Wiener filtering; Audio-visual; Deep learning	RECOGNITION; INFORMATION; LIPS	Human speech processing is inherently multi-modal, where visual cues (e.g. lip movements) can help better understand speech in noise. Our recent work [1] has shown that lip-reading driven, audio-visual (AV) speech enhancement can significantly outperform benchmark audio-only approaches at low signal-to-noise ratios (SNRs). However, consistent with our cognitive hypothesis, visual cues were found to be relatively less effective for speech enhancement at high SNRs, or low levels of background noise, where audio-only (A-only) cues worked adequately. Therefore, a more cognitively-inspired, context-aware AV approach is required, that contextually utilises both visual and noisy audio features, and thus more effectively accounts for different noisy conditions. In this paper, we introduce a novel context-aware AV speech enhancement framework that contextually exploits AV cues with respect to different operating conditions, in order to estimate clean audio, without requiring any prior SNR estimation. In particular, an AV switching module is developed by integrating a convolutional neural network (CNN) and long-short-term memory (LSTM) network, that learns to contextually switch between visualonly (V-only), A-only and both AV cues at low, high and moderate SNR levels, respectively. For testing, the estimated clean audio features are utilised using an innovative, enhanced visually-derived Wiener filter (EVWF) for noisy speech filtering. The context-aware AV speech enhancement framework is evaluated in dynamic real-world scenarios (including cafe, street, bus, and pedestrians) at different SNR levels (ranging from low to high SNRs), using benchmark Grid and ChiME3 corpora. For objective testing, perceptual evaluation of speech quality (PESQ) is used to evaluate the quality of the restored speech. For subjective testing, the standard mean-opinion-score (MOS) method is used. Comparative experimental results show the superior performance of our proposed context-aware AV approach, over A-only, V-only, spectral subtraction (SS), and log-minimum mean square error (LMMSE) based speech enhancement methods, at both low and high SNRs. The preliminary findings demonstrate the capability of our novel approach to deal with spectro-temporal variations in real-world noisy environments, by contextually exploiting the complementary strengths of audio and visual cues. In conclusion, our contextual deep learning-driven AV framework is posited as a benchmark resource for the multi-modal speech processing and machine learning communities.																	1566-2535	1872-6305				JUL	2020	59						163	170		10.1016/j.inffus.2019.08.008													
J								Sequence labeling to detect stuttering events in read speech	COMPUTER SPEECH AND LANGUAGE										Stuttering event detection; Speech disorder; CRF; BLSTM	CHILDREN; PERCEPTIONS; CLINICIAN	Stuttering is a speech disorder that, if treated during childhood, may be prevented from persisting into adolescence. A clinician must first determine the severity of stuttering, assessing a child during a conversational or reading task, recording each instance of disfluency, either in real time, or after transcribing the recorded session and analysing the transcript. The current study evaluates the ability of two machine learning approaches, namely conditional random fields (CRF) and bi-directional long-short-term memory (BLSTM), to detect stuttering events in transcriptions of stuttering speech. The two approaches are compared for their performance both on ideal hand-transcribed data and also on the output of automatic speech recognition (ASR). We also study the effect of data augmentation to improve performance. A corpus of 35 speakers' read speech (13K words) was supplemented with a corpus of 63 speakers' spontaneous speech (11K words) and an artificially-generated corpus (50K words). Experimental results show that, without feature engineering, BLSTM classifiers outperform CRF classifiers by 33.6%. However, adding features to support the CRF classifier yields performance improvements of 45% and 18% over the CRF baseline and BLSTM results, respectively. Moreover, adding more data to train the CRF and BLSTM classifiers consistently improves the results. (C) 2019 Elsevier Ltd. All rights reserved.																	0885-2308	1095-8363				JUL	2020	62								101052	10.1016/j.csl.2019.101052													
J								Sequential neural networks for noetic end-to-end response selection	COMPUTER SPEECH AND LANGUAGE										DSTC7; Response selection; ESIM; BERT; End-to-end; Sequential matching approaches		The noetic end-to-end response selection challenge as one track in the 7th Dialog System Technology Challenges (DSTC7) aims to push the state of the art of utterance classification for real world goal-oriented dialog systems, for which participants need to select the correct next utterances from a set of candidates for the multi-turn context. This paper presents our systems that are ranked top 1 on both datasets under this challenge, one focused and small (Advising) and the other more diverse and large (Ubuntu). Previous state-of-the-art models use hierarchy-based (utterance-level and token-level) neural networks to explicitly model the interactions among different turns' utterances for context modeling. In this paper, we investigate a sequential matching model based only on chain sequence for multi-turn response selection. Our results demonstrate that the potentials of sequential matching approaches have not yet been fully exploited in the past for multi-turn response selection. In addition to ranking top 1 in the challenge, the proposed model outperforms all previous models, including state-of-the-art hierarchy-based models, on two large-scale public multi-turn response selection benchmark datasets. (C) 2020 Elsevier Ltd. All rights reserved.																	0885-2308	1095-8363				JUL	2020	62								101072	10.1016/j.csl.2020.101072													
J								Overview of the seventh Dialog System Technology Challenge: DSTC7	COMPUTER SPEECH AND LANGUAGE										Dialog System Technology Challenge; end-to-end dialog systems; Sentence Selection; Natural Language Generation; Audio Visual Scene-Aware Dialog		This paper provides detailed information about the seventh Dialog System Technology Challenge (DSTC7) and its three tracks aimed to explore the problem of building robust and accurate end-to-end dialog systems. In more detail, DSTC7 focuses on developing and exploring end-to-end technologies for the following three pragmatic challenges: (1) sentence selection for multiple domains, (2) generation of informational responses grounded in external knowledge, and (3) audio visual scene-aware dialog to allow conversations with users about objects and events around them. This paper summarizes the overall setup and results of DSTC7, including detailed descriptions of the different tracks, provided datasets and annotations, overview of the submitted systems and their final results. For Track 1, LSTM-based models performed best across both datasets, allowing teams to effectively handle task variants where no correct answer was present or when multiple paraphrases were included. For Track 2, RNN-based architectures augmented to incorporate facts by using two types of encoders: a dialog encoder and a fact encoder plus using attention mechanisms and a pointer-generator approach provided the best results. Finally, for Track 3, the best model used Hierarchical Attention mechanisms to combine the text and vision information obtaining a 22% better result than the baseline LSTM system for the human rating score. More than 220 participants were registered and about 40 teams participated in the final challenge. 32 scientific papers reporting the systems submitted to DSTC7, and 3 general technical papers for dialog technologies, were presented during the one-day wrap-up workshop at AAAI-19. During the workshop, we reviewed the state-of-the-art systems, shared novel approaches to the DSTC7 tasks, and discussed the future directions for the challenge (DSTC8). (C) 2020 Elsevier Ltd. All rights reserved.																	0885-2308	1095-8363				JUL	2020	62								101068	10.1016/j.csl.2020.101068													
J								Low-resource text classification using domain-adversarial learning	COMPUTER SPEECH AND LANGUAGE										NLP; Low-resource; Deep learning; Domain-adversarial		Deep learning techniques have recently shown to be successful in many natural language processing tasks forming state-of-the-art systems. They require, however, a large amount of annotated data which is often missing. This paper explores the use of domain-adversarial learning as a regularizer to avoid overfitting when training domain invariant features for deep, complex neural networks in low-resource and zero-resource settings in new target domains or languages. In case of new languages, we show that monolingual word vectors can be directly used for training without prealignment. Their projection into a common space can be learnt ad-hoc at training time reaching the final performance of pretrained multilingual word vectors. (C) 2019 Elsevier Ltd. All rights reserved.																	0885-2308	1095-8363				JUL	2020	62								101056	10.1016/j.csl.2019.101056													
J								Towards the first Maithili part of speech tagger: Resource creation and system development	COMPUTER SPEECH AND LANGUAGE										Maithili language processing; Part of speech tagging; Corpus annotation; Maithili POS		Part of speech (POS) tagging for the Indian language Maithili is not an explored territory. There have been substantial efforts at developing POS taggers in several Indian languages including Hindi, Bengali, Tamil, Telugu, Kannada, Punjabi and Marathi; but we did not find any openly available POS tagger and tagged corpus in Maithili. However, Maithili is one of the official languages of India with around 50 million native speakers. Development of Maithili natural language processing (NLP) tools and resources is extremely important as the language is currently being used in education and official contexts in certain states in India. In this paper, we present our effort on the development of a Maithili POS tagger. As we did not find any open training data, we started the development by annotation of a POS tagged corpus. We defined a POS tagset and manually annotated a Maithili corpus containing 52,190 words. We used the corpus to train a conditional random fields (CRF) classifier. We ran experiments using various feature sets and achieved an accuracy of 82.67%. Then we collected large raw corpora containing Wikipedia dump and other Maithili web resources to train neural word embedding. The word2vec CBOW model was trained and the generated word vectors were utilized during CRF training. With this inclusion, the accuracy of the system increased to 85.88%. (C) 2019 Elsevier Ltd. All rights reserved.																	0885-2308	1095-8363				JUL	2020	62								101054	10.1016/j.csl.2019.101054													
J								Classification of aspirated and unaspirated sounds in speech using excitation and signal level information	COMPUTER SPEECH AND LANGUAGE										Aspiration; Excitation source signal; Glottal pulse features; Hidden markov model; Linear prediction residual; Random forest; Support vector machine; Unaspiration	STOPS	In this work, consonant aspiration and unaspiration phenomena are studied. It is known that, pronunciation of aspiration and unaspiration is characterized by the 'puff of air' released at the place of constriction in the vocal tract also known as burst. Here, properties of the vowel immediately after the burst are studied for characterization of the burst. Excitation source signal estimated from speech as low pass filtered linear prediction residual signal is used for the task. The signal characteristics of parameters such as glottal pulse, duration of open, closed & return phases; slope of open, & return phases; duration of burst; ratio of highest and lowest frame wise energies of signal and voice onset point are explored as features to characterize aspiration and unaspiration. Three datasets namely TIMIT, IIIT Hyderabad Marathi and IIIT Hyderabad Hindi (IIIT-H Indic Speech Databases) are used to verify the proposed approach. Random forest, support vector machine and deep feed forward neural networks (DFFNNs) are used as classifiers to test the effectiveness of the features used for the task. Optimal features are selected for the classification using correlation based feature selection (CFS). From the results, it is observed that the proposed features are efficient in classifying the aspirated and unaspirated consonants. Performance of the proposed features in recognition of aspirated and unaspirated phoneme is also evaluated. IIIT Hyderabad Marathi is considered for the analysis. It is observed that the performance of recognition of aspirated and unaspirated sounds using proposed features is improved in comparison with the MFCCs based phoneme recognition system. (C) 2020 Elsevier Ltd. All rights reserved.																	0885-2308	1095-8363				JUL	2020	62								101057	10.1016/j.csl.2019.101057													
J								Generating diverse conversation responses by creating and ranking multiple candidates	COMPUTER SPEECH AND LANGUAGE										End-to-end; Conversation response generation; Variational autoencoders; Diversity		This paper introduces our systems built for Track 2 of Dialog System Technology Challenge 7 (DSTC7). This challenge track aimed to evaluate the response generation methods using fully data-driven conversation models in a knowledge-grounded setting, where textual facts were provided as the knowledge for each context-response pair. The sequence-to-sequence models have achieved impressive results in machine translation and have also been widely used for end-to-end generative conversation modelling. However, they tended to output dull and repeated responses in previous studies. Our work aims to promote the diversity of end-to-end conversation response generation by adopting a two-stage pipeline. 1) Create multiple responses for an input context together with its textual facts. At this stage, two different models are designed, i.e., a variational generative (VariGen) model and a retrieval-based (Retrieval) model. 2) Rank and return the most relevant response by training a topic coherence discrimination (TCD) model for calculating ranking scores. In our experiments, we demonstrated the effectiveness of the response ranking strategy and the external textual knowledge for generating better responses. According to the official evaluation results, our Retrieval and VariGen systems ranked first and second respectively among all participant systems on Entropy metrics which measured the objective diversity of generated responses. Besides, the VariGen system ranked second on NIST and METEOR metrics which measured the objective quality of generated responses. (C) 2020 Elsevier Ltd. All rights reserved.																	0885-2308	1095-8363				JUL	2020	62								101071	10.1016/j.csl.2020.101071													
J								Noetic end-to-end response selection with supervised neural network based classifiers and unsupervised similarity models	COMPUTER SPEECH AND LANGUAGE										DSTC7; Dialogue systems; Sentence selection; Neural networks; Unsupervised similarity models; Ensemble models		This paper describes a solution for the Noetic End-to-End Response Selection challenge - one of the tasks of the 7th Dialog System Technology Challenge. The goal of the task is to select the most appropriate continuation of a dialogue from a given set of responses. We approach this problem by building an ensemble of supervised neural network based classifiers and unsupervised similarity models. The dialogue continuation is selected according to a score that aggregates the rankings of candidate responses determined by the models in the ensemble. (C) 2020 The Author(s). Published by Elsevier Ltd.																	0885-2308	1095-8363				JUL	2020	62								101074	10.1016/j.csl.2020.101074													
J								Context and knowledge aware conversational model and system combination for grounded response generation	COMPUTER SPEECH AND LANGUAGE										DSTC; Dialogue system; Conversational AI; Sentence generation; Grounding knowledge		End-to-end neural-based dialogue systems can potentially generate tailored and coherent responses for user inputs. However, most of existing systems produce universal and non-informative responses, and they have not gone beyond chitchat yet. To tackle these problems, 7th Dialog System Technology Challenges (DSTC7-Track2) was developed to focus on building a dialogue system that produces informational responses that are grounded on external knowledge. In this study, we propose a Memory-augmented Hierarchical Recurrent Encoder-Decoder, called MHRED, that grounded on both multi-turn dialogue context and external knowledge. Furthermore, we apply a combination of multiple dialogue systems. Our final system is an ensemble that combines three modules: a generation-based module, a retrieval-based module, and a reranking module. First, responses are generated by MHRED, and retrieved from a pre-defined database focusing on informativeness. Next, the reranking module sorts these candidates using several hand-crafted features, and finally it selects a response with the highest score. Therefore, this system can return diverse and meaningful responses from various perspectives. Experimental results show that our proposed MHRED outperforms strong baseline models and combining multiple dialogue systems significantly improves the automatic evaluation and human evaluations. (C) 2020 The Author(s). Published by Elsevier Ltd.																	0885-2308	1095-8363				JUL	2020	62								101070	10.1016/j.csl.2020.101070													
J								Pneumatic Quasi-Passive Actuation for Soft Assistive Lower Limbs Exoskeleton	FRONTIERS IN NEUROROBOTICS										soft exoskeleton; exosuit; robotic wearable device; quasi-passive actuation; legged locomotion; gait assistance	ENERGY EFFICIENCY; DESIGN; ORTHOSIS; WALKING; SYSTEM; FLUID	There is a growing international interest in developing soft wearable robotic devices to improve mobility and daily life autonomy as well as for rehabilitation purposes. Usability, comfort and acceptance of such devices will affect their uptakes in mainstream daily life. The XoSoft EU project developed a modular soft lower-limb exoskeleton to assist people with low mobility impairments. This paper presents the bio-inspired design of a soft, modular exoskeleton for lower limb assistance based on pneumatic quasi-passive actuation. The design of a modular reconfigurable prototype and its performance are presented. This actuation centers on an active mechanical element to modulate the assistance generated by a traditional passive component, in this case an elastic belt. This study assesses the feasibility of this type of assistive device by evaluating the energetic outcomes on a healthy subject during a walking task. Human-exoskeleton interaction in relation to task-based biological power assistance and kinematics variations of the gait are evaluated. The resultant assistance, in terms of overall power ratio (?) between the exoskeleton and the assisted joint, was 26.6% for hip actuation, 9.3% for the knee and 12.6% for the ankle. The released maximum power supplied on each articulation, was 113.6% for the hip, 93.2% for the knee, and 150.8% for the ankle.																	1662-5218					JUN 30	2020	14								31	10.3389/fnbot.2020.00031													
J								LoCAR - Low-Cost Autonomous Robot for Object Detection with Voice Command and MobileNets	APPLIED ARTIFICIAL INTELLIGENCE											SEGMENTATION; EXTRACTION	This work details the design, construction, implementation and testing of a standalone robot, based on a convolutional neural network, which receives a voice command, searches and recognizes the target through its camera and moves to the object or person properly recognized. The success rate for the recognition stage has reached 82% in the median for objects tested, 100% for chairs, bottles and people. The processing was performed on a Raspberry Pi 3 B board integrated with an Arduino UNO to control the actuators.																	0883-9514	1087-6545				SEP 18	2020	34	11					816	831		10.1080/08839514.2020.1782004		JUN 2020											
J								Invariant Features-Based Fuzzy Inference System for Animal Detection and Recognition Using Thermal Images	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Fuzzy logic; Thermal images; Skeleton and shape features; Zernike moments		Human-Animal Conflict (HAC) is one of the primary threats to the continued survival of animal species and it has also impacted the lives of humans drastically. In this paper, we propose an efficient animal detection and recognition system with invariant features and fuzzy logic using thermal images. The proposed system exploits various features like Zernike, shape, texture and skeleton path. Cumulatively, these features are invariant to rotation, scaling, translation, illumination, and partly posture. The proposed model is robust to several challenging image conditions like low contrast/illumination, haze/blur, occlusion, camouflage, background clutter, and poses variation. The model is tested on our thermal animal dataset that has 1862 images and 12 different animal species. Experimental results validate the significance of thermal images for animal-based applications. Besides, the proposed fuzzy system has achieved an average accuracy of 97% which is equivalent to the accuracy produced by domain experts in identifying the animals from our thermal dataset.																	1562-2479	2199-3211				SEP	2020	22	6			SI		1868	1879		10.1007/s40815-020-00907-9		JUN 2020											
J								A Novel Preference Measure for Multi-Granularity Probabilistic Linguistic Term Sets and its Applications in Large-Scale Group Decision-Making	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Preference degrees; Probabilistic linguistic term sets; Min-conflict model; Multi-attribute group decision-making; Three-way primary gradings	REPRESENTATION MODEL; NUMERICAL SCALE; CONSENSUS MODEL; 3-WAY DECISIONS; ROUGH SET; INFORMATION; OPERATORS; WEIGHTS	Comparing probabilistic linguistic term sets (PLTSs) is quite essential in solving PLTS-expressed multi-attribute group decision-making problems (PLTS-MAGDM). Researchers have designed various comparison measures to obtain the rank of PLTSs. However, most of the existing PLTS comparison measures need additional tedious adjustments before conducting a specific computation. Besides, these measures do not adequately consider the effects of the semantics of the basic linguistic term set and the probabilistic distributions. This paper proposes a new preference degree for g-granularity probabilistic term sets (g-GPLTSs) to overcome the two shortcomings simultaneously by integrating the effect from basic linguistic terms and probabilistic distributions without any adjustment. Moreover, the g-GPLTS preference degree also shows the extended adaptability for comparing PLTSs with unbalanced semantics. Based on the newly proposed preference degree, we construct a useful min-conflict model to solve PLTS-MAGDM with a large number of experts expressing the three-way primary grading. Finally, an illustrative example concerning software supplier selections, followed by the comparative analysis, is presented to verify the feasibility and effectiveness of the proposed method.																	1562-2479	2199-3211				OCT	2020	22	7					2350	2368		10.1007/s40815-020-00887-w		JUN 2020											
J								Video super-resolution using hybrid support vector regression-Actor Critic Neural Network model	EVOLUTIONARY INTELLIGENCE										Video super-resolution; SCA; Support vector regression; Video enhancement; Fractional theory	IMAGE SUPERRESOLUTION; OPTICAL-FLOW	The aim of video super-resolution (SR) is to produce a high-resolution (HR) video frame from numerous successive low-resolution (LR) frames. Even though there are a large number of techniques employed for the video SR, all these existing techniques face a hectic challenge at various conditions. Thus, this paper proposes an effective video resolution strategy using the hybrid Support vector regression-Actor Critic Neural Network (SVR-ACNN) model for video enhancement. The SR images formed using the individual SVR model and ACNN are integrated using the weighted average concept. The ACNN is tuned optimally by the proposed Fractional-based Sine Cosine algorithm (F-SCA), which is responsible for the global optimal convergence. The experimentation of the proposed method utilizes three videos taken from the Cambridge-driving Labeled Video Database (CamVid), and the results are analyzed for three scaling factors. The results prove that the proposed model offers a better SR image with a better PSNR, SSIM, and SDME of 33.6447 dB, 0.9398, and 45.2779, respectively.																	1864-5909	1864-5917															10.1007/s12065-020-00435-3		JUN 2020											
J								Adaption of 5G networks in industrial sector with improved bandwidth utilization	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										5G network; Industrial SDA; Bandwidth utilization; Data rate; QoS; MIMO; BUS; DMS; MDR	INTERNET; ARCHITECTURE	The newly arrived Software Defined Network (SDN) has been adapted to industry standards. By defining Software Defined Air Interface (SDA) would support achieving higher data rate and utilization of bandwidth. The growing use of internet in industrial sector has claimed higher bandwidth conditions. To provide sufficient bandwidth support for the industrial networks, the fifth generation network has been emerged. The industrial sector utilizes the internet communication for variety of purpose where the bandwidth of the connections needs higher values. They involve in accessing different resources located in different geographic location where the latency of communication should be less and they need to transfer huge amount of data which requires higher bandwidth conditions. However, the increased growth of internet usage has introduced huge challenge for the service providers in handling higher data rate solutions. However different opinions are generated toward 5G network design, still there is no blue print has been arrived. Towards the growth of 5G networks and for the improved performance of data rate maintenance, a novel software defined air interface has been proposed in this paper. The presence of macro cells and small cells supports the higher data rate. Each small cell has been fabricated with number of MIMO which is located in different locations of the network in dense. The software defined interface is capable of selecting optimal macro station or small cell for betterment of data transmission. The interface designed monitor the network conditions and estimates bandwidth utilization support (BUS) and data rate maintenance support (DMS) for different small cells located in different geographic location. The SDA designed is responsible for the selection of small cell, MIMO to maintain the data rate. The proposed method improves the performance of bandwidth utilization and data rate maintenance.																	1868-5137	1868-5145															10.1007/s12652-020-02252-z		JUN 2020											
J								Gradient flow-based deep residual networks for enhancing visibility of scenery images degraded by foggy weather conditions	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Gradient flow; Deep residual networks; Dehazed images; Foggy image; Transmission map		In the recent years, the vehicles are incorporated with camera-based modern driver support systems for facilitating the drivers to confirm their safety under different conditions of driving. However, lower contrast and faded scene visibility is considered as the main issue faced by the driver assistance system while driving in foggy weather conditions. At this juncture, deep neural network methods are considered to be potent in solving the limitations of manually designing haze-related features. In this paper, gradient flow-based deep residual network is utilized for improving the scenery images which are degraded through foggy weather conditions. This proposed scheme uses an undetermined complex function for mathematically modeling the fog in an image, which can be subsequently approximated by the deep residual network into the corresponding mathematical model associated with the fog. This proposed scheme uses two predominant steps that correspond to the determination of transmission map related to the haze image input and removal of foggy haze using residual network based on the estimation of the ratio between transmission map and foggy image. It is considered to be phenomenal in realizing generalization and robustness with minimal input for different unidentified image data. The experimental investigation of the proposed scheme is conducted using NYU2 depth dataset for the purpose of training the utilized residual deep networks. The experimental results proved that the proposed scheme is predominant over the benchmarked fog removal approaches in terms of evaluation metrics such as natural image quality evaluator aspect, blind/reference less image spatial quality evaluator, spatial-spectral entropy-based quality, full-reference metric peak signal to noise ratio, no-reference metric, feature similarity and structural similarity.																	1868-5137	1868-5145															10.1007/s12652-020-02225-2		JUN 2020											
J								Developing a multi-level intrusion detection system using hybrid-DBN	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cyber attack; Deep belief network; Deep neural network; Industrial control system; Intrusion detection system	DEEP BELIEF NETWORKS; CLASSIFICATION; ALGORITHM; OPTIMIZATION; RECOGNITION; ATTACKS	In this study, a hybrid deep belief network (DBN) cyber intrusion detection system was proposed to provide a secure network by controlling network traffic in Industrial control systems (ICS). The disadvantages of DBN have been analyzed and improved to create attack detectors in network traffic. The output is combined with Softmax Regression for effective intrusion detection and classification detection. Training and testing of the hybrid DBN model were carried out with the actual and original data set generated by ICS. DBNs are a much-preferred approach for detecting malicious attacks in network traffic. In instances where there is a lot of data, it is important to select the most appropriate structure for the DBN model. Therefore, in the model the hidden layers are updated by contrastive divergence (CD), and the output layer is combined with the Softmax classifier. The proposed model architecture has proved successful in many limitations, such as the complexity and size of training data. The proposed hybrid DBN model provided 99.72% accuracy in intrusion detection and classification. These results showed that the model achieved better performance than the existing intrusion detection system (IDS). It also provided approximately 5% more accuracy improvements with the hybrid model than with older DBN-based systems.																	1868-5137	1868-5145															10.1007/s12652-020-02271-w		JUN 2020											
J								Discrete social spider algorithm for the traveling salesman problem	ARTIFICIAL INTELLIGENCE REVIEW										Discrete problems; Optimization; Social spider; Traveling salesman problem	SWARM OPTIMIZATION ALGORITHM; SEARCH ALGORITHM; SELECTION; BEHAVIOR; SOLVE	Heuristic algorithms are often used to find solutions to real complex world problems. These algorithms can provide solutions close to the global optimum at an acceptable time for optimization problems. Social Spider Algorithm (SSA) is one of the newly proposed heuristic algorithms and based on the behavior of the spider. Firstly it has been proposed to solve the continuous optimization problems. In this paper, SSA is rearranged to solve discrete optimization problems. Discrete Social Spider Algorithm (DSSA) is developed by adding explorer spiders and novice spiders in discrete search space. Thus, DSSA's exploration and exploitation capabilities are increased. The performance of the proposed DSSA is investigated on traveling salesman benchmark problems. The Traveling Salesman Problem (TSP) is one of the standard test problems used in the performance analysis of discrete optimization algorithms. DSSA has been tested on a low, middle, and large-scale thirty-eight TSP benchmark datasets. Also, DSSA is compared to eighteen well-known algorithms in the literature. Experimental results show that the performance of proposed DSSA is especially good for low and middle-scale TSP datasets. DSSA can be used as an alternative discrete algorithm for discrete optimization tasks.																	0269-2821	1573-7462															10.1007/s10462-020-09869-8		JUN 2020											
J								Multi-fault Condition Monitoring of Slurry Pump with Principle Component Analysis and Sequential Hypothesis Test	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										PCA; SPRT; multi-fault; condition monitoring; fault diagnosis	SENSOR; FIBER	A new method about the multi-fault condition monitoring of slurry pump based on principal component analysis (PCA) and sequential probability ratio test (SPRT) is proposed. The method identifies the condition of the slurry pump by analyzing the vibration signal. The experimental model is established using the normal impeller and the faulty impellers where the collected vibration signals were preprocessed using wavelet packet transform (WPT). The characteristic parameters of the vibration signals are extracted by time domain signal analysis and the dimension of data was reduced by PCA. The principal components with the largest contribution rate are chosen as the inputted signal to SPRT to assess the proposed algorithm. The new methodology is reasonable and practical for the multi-fault diagnosis of slurry pump.																	0218-0014	1793-6381				JUN 30	2020	34	7							2059019	10.1142/S0218001420590193													
J								Facial Expression Recognition Method Based on Improved VGG Convolutional Neural Network	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Convolutional neural network; facial expression recognition; VGG-Net; Alex-Net		Because the shallow neural network has limited ability to represent complex functions with limited samples and calculation units, its generalization ability will be limited when it comes to complex classification problems. The essence of deep learning is to learn a nonlinear network structure, to represent input data distributed representation and demonstrate a powerful ability to learn deeper features of data from a small set of samples. In order to realize the accurate classification of expression images under normal conditions, this paper proposes an expression recognition model of improved Visual Geometry Group (VGG) deep convolutional neural network (CNN). Based on the VGG-19, the model optimizes network structure and network parameters. Most expression databases are unable to train the entire network from the start due to lack of sufficient data. This paper uses migration learning techniques to overcome the shortage of image training samples. Shallow CNN, Alex-Net and improved VGG-19 deep CNN are used to train and analyze the facial expression data on the Extended Cohn-Kanade expression database, and compare the experimental results obtained. The experimental results indicate that the improved VGG-19 network model can achieve 96% accuracy in facial expression recognition, which is obviously superior to the results of other network models.																	0218-0014	1793-6381				JUN 30	2020	34	7							2056003	10.1142/S0218001420560030													
J								Hybrid Mixture Model Based on a Hybrid Optimization for Spectrum Sensing to Improve the Performance of MIMO-OFDM Systems	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Hybrid optimization; spectrum sensing; MIMO; hybrid mixture model; cognitive radio	REINFORCEMENT; NETWORKS; CHANNELS	Cognitive radio (CR) is the trending domain in addressing the inadequate bands for communication, and spectrum sensing is the hectic challenge need to be addressed extensively. In the conventional CRs, the communication is restricted to the secondary users (SUs) in the allocated bands causing the underutilization of the available band. Thus, with the aim to afford higher throughput and spectrum efficiency, this paper introduces the hybrid mixture model for spectrum sensing in the multiple-input-multiple-output (MIMO) systems and the effectiveness is evaluated based on the evaluation parameters, such as detection probability and probability of false alarm. The signal received through the orthogonal frequency-division multiplexing (OFDM) antenna is employed for analyzing the spectral availability for which the energy and Eigen statistics of the signal is generated, which forms the input to the Hybrid mixture model. The developed Hybrid mixture model is the integration of the Gaussian Mixture Model (GMM) and Whale Elephant-Herd Optimization (WEHO). The GMM is subjected to the optimal timing using the WEHO, which is the modification of the standard Whale Optimization Algorithm (WOA) with the Elephant-Herd Optimization (EHO). The analysis reveals that the proposed spectrum sensing model acquired the maximal detection probability and minimal false alarm probability of 99.9% and 46.4%, respectively. The proposed hybrid mixture model derives the spectrum availability and ensures the effective communication in CR without any interference.																	0218-0014	1793-6381				JUN 30	2020	34	7							2058008	10.1142/S0218001420580082													
J								Voltage Disturbance Signals Identification Based on ILMD and Neural Network	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Disturbing signal; ILMD; endpoint extension; signal decomposition; BP neural network	VARIATIONAL MODE DECOMPOSITION; EXTREME LEARNING-MACHINE; PREDICTION	In order to identify the disturbance signal in power system and reduce the influence on system security, a voltage disturbance signal classifier based on improved local mean decomposition (ILMD) and BP neural network is proposed. ILMD is used to decompose the disturbance signal in three layers, and the product function (PF) component with amplitude-frequency information of voltage signal is obtained. The signal energy value constructed by PF component is used as the input of BP neural network to identify and classify the voltage disturbance signal. Experiments on four typical voltage disturbance signals show that the signal classifiers based on ILMD and BP neural networks have high accuracy and good working efficiency for the recognition and classification of voltage disturbance signals.																	0218-0014	1793-6381				JUN 30	2020	34	7							2058007	10.1142/S0218001420580070													
J								The Portrait Depiction of the Market Members Based on Data Mining	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										k-Means algorithm; LRFMC model; KNN algorithm; accuracy rate		Aiming at the problem of portrait of members in shopping malls, this paper analyzes the similarities and differences of consumption behaviors between member groups and nonmember groups, and constructs the LRFMC model with k-means algorithm to analyze the value of membership. Second, active states of members are divided according to the consumption time interval, and KNN algorithm model is established to predict member states and used to predict the membership status. Finally, it discusses which types of goods are more suitable for promotional activities and can bring more profits to the shopping mall.																	0218-0014	1793-6381				JUN 30	2020	34	7							2059024	10.1142/S0218001420590247													
J								Sentence Similarity Algorithm Based on Fused Bi-Channel Dependency Matching Feature	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Deep learning; convolutional neural network; matching similarity; matching sequence; dependency		Many tasks of natural language processing such as information retrieval, intelligent question answering, and machine translation require the calculation of sentence similarity. The traditional calculation methods used in the past could not solve semantic understanding problems well. First, the model structure based on Siamese lack of interaction between sentences; second, it has matching problem which contains lacking position information and only using partial matching factor based on the matching model. In this paper, a combination of word and word's dependence is proposed to calculate the sentence similarity. This combination can extract the word features and word's dependency features. To extract more matching features, a bidirectional multi-interaction matching sequence model is proposed by using word2vec and dependency2vec. This model obtains matching features by convolving and pooling the word-granularity (word vector, dependency vector) interaction sequences in two directions. Next, the model aggregates the bi-direction matching features. The paper evaluates the model on two tasks: paraphrase identification and natural language inference. The experimental results show that the combination of word and word's dependence can enhance the ability of extracting matching features between two sentences. The results also show that the model with dependency can achieve higher accuracy than these models without using dependency.																	0218-0014	1793-6381				JUN 30	2020	34	7							2050019	10.1142/S0218001420500196													
J								Prediction of Cleaning Loss of Combine Harvester Based on Neural Network	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Prediction; neural network; combine harvester; influencing factors	SPEED CONTROL; SYSTEM	This paper explores the performance and obtains a reasonable cleaning effect of the cleaning system of combine harvester and studies the relationship between the cleaning effect of the combine harvester cleaning system and its influencing factors. We established a neural network model between the cleaning loss rate and the clean system parameters. First, we tested the results of the cleaning performance of each group under different combinations of conditions, and analyzed the direct or indirect relationship between the cleaning loss rate and the parameters in the experiment under each working condition. Then, according to the experimental data obtained in the experiment, we predict the clearance loss rate for several sets of conditions by this model. The experimental results show that the prediction results of the model can meet the experimental requirements under the condition that the accuracy is not very high.																	0218-0014	1793-6381				JUN 30	2020	34	7							2059021	10.1142/S0218001420590211													
J								Template Calibration Parameter Optimization of Fuzzy Vault Method	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Fuzzy vault; biometric; geometric hashing algorithm; automatic alignment; fingerprint	FINGERPRINT; SYSTEM; CRYPTOSYSTEM	In the concrete implementation of the fuzzy vault algorithm, the geometric hash method is a common technique for automatic calibration of biometric templates. For the fuzzy problem of parameter acquisition, the matching accuracy of fuzzy vault template is affected in the three parameters: the pixel size, hash table and hash table quantization parameters (alpha and beta). The single factor experiment method obtains the optimal range of these three parameters, and the extraction range of the fuzzy point and the selection rule of the base point distance are improved for the fuzzy vault algorithm. Finally, based on the FVC fingerprint database, their matching precision is compared for the algorithm before and after optimization. The experimental results show that the false rejection rate (FRR) of the optimized algorithm is reduced by at least 9.84%, and the false acceptance rate (FAR) is reduced by at least 7.12%, indicating that the optimization scheme improves the matching accuracy of the algorithm. The algorithm has certain robustness and practicability.																	0218-0014	1793-6381				JUN 30	2020	34	7							2059020	10.1142/S021800142059020X													
J								A Contribution Algorithm from LDRI to HDRI	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Camera response curve; high dynamic range image; low dynamic range image; contribution; display	IMAGE QUALITY ASSESSMENT; TONE MAPPING OPERATORS; FUSION	High dynamic range image (HDRI) which is combined with low dynamic range image (LDRI) needs to be mapped to a low dynamic area to display. In the process of mapping, it is impossible to determine the contribution of low dynamic image sequences in the display images, so that it results in a problem that the low dynamic images cannot be accurately selected. In this paper, for the first time, a contribution algorithm from LDRI to HDRI according to the corresponding response curve of the camera is proposed.																	0218-0014	1793-6381				JUN 30	2020	34	7							2059025	10.1142/S0218001420590259													
J								Neural Network Based on Work Piece Recognition and Robot Intelligent Capture in Complex Environments	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Target recognition; robotic intelligent capture; convolutional neural network; migration learning		In today's rapid development of science and technology, the manual work in the factory has been gradually replaced by machines, and the process of industrial intelligence has been further deepened. Workpiece recognition is the use of machine learning, computer vision and other technologies to identify the target workpiece, and the robot intelligent capture is a higher level of operation of the workpiece recognition, which is the key to realize the intelligentization of industrial robots. Due to the complex environmental factors and the diversity of the shape and size of the objects to be grasped, the accuracy and efficiency of the workpiece recognition are not ideal at this stage, and intelligent crawling is even more difficult to talk about. Aiming at the above problems, this paper builds a crawling planning model based on the convolutional neural network and the grasping pose mapping rules. Based on the established crawling planning model, the sampling candidate grabbing algorithm is designed and the migration learning method is adopted. The pretrained convolutional neural network for image recognition on the ImageNet dataset was migrated to the capture detection task of the Carnegie Mellon dataset. Experiments show that the network model proposed in this paper performs well, and its correct crawl rate is as high as 81.27%. This is to achieve a more stable and reliable identification and intelligent crawling work.																	0218-0014	1793-6381				JUN 30	2020	34	7							2059022	10.1142/S0218001420590223													
J								Convolutional Sparse Representation and Local Density Peak Clustering for Medical Image Fusion	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Convolutional sparse representation (CSR); local density peaks clustering (DPC); dictionary learning; multimodality medical image fusion	MULTI-FOCUS	Aiming at the problem of insufficient detail retention in multimodal medical image fusion (MMIF) based on sparse representation (SR), an MMIF method based on density peak clustering and convolution sparse representation (CSR-DPC) is proposed. First, the base layer is obtained based on the registered input image by the averaging filter, and the original image minus the base layer to obtain the detail layer. Second, for retaining the details of the fused image, the detail layer image is fused by CSR to obtain the fused detail layer image, then the base layer image is segmented into several image blocks, and the blocks are clustered by using DPC to obtain some clusters, and each class cluster is trained to obtain a sub-dictionary, and all the sub-dictionaries are fused to obtain an adaptive dictionary. The sparse coefficient is fused through the learned adaptive dictionary, and the fused base layer image is obtained through reconstruction. Finally, fusing the detail layer and the base layer and reconstructing them forms the ultimate fused image. Experiments show that compared to the state-of-the-art two multiscale transformation methods and five SR methods, the proposed method(CSR-DPC) outperforms the other methods in terms of the image details, the visual quality and the objective evaluation index, which can be helpful for clinical diagnosis and adjuvant treatment.																	0218-0014	1793-6381				JUN 30	2020	34	7							2057003	10.1142/S0218001420570037													
J								Vision-Based Intelligent Vehicle Road Recognition and Obstacle Detection Method	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Intelligent vehicle; vanishing point detection; road segmentation; obstacle detection; trajectory tracking		With the development of the world economy and the accelerating process of urbanization, cars have brought great convenience to people's lives and activities, and have become an indispensable means of transportation. Intelligent vehicles have the important significance of reducing traffic accidents, improving transportation capacity and broad market prospects, and can lead the future development of the automotive industry, so they have received extensive attention. In the existing intelligent vehicle system, the laser radar is a well-deserved protagonist because of its excellent speed and precision. It is an indispensable part of achieving high-precision positioning, but to some extent, the price hindering its marketization is a major factor. Compared with lidar sensors, vision sensors have the advantages of fast sampling rate, light weight, low energy consumption and low price. Therefore, many domestic and foreign research institutions have listed them as the focus of research. However, the current vision-based intelligent vehicle environment sensing technology is also susceptible to factors such as illumination, climate and road type, resulting in insufficient accuracy and real-time performance of the algorithm. This paper takes the environment perception of intelligent vehicles as the research object, and conducts in-depth research on the existing problems in road recognition and obstacle detection algorithms, including road image vanishing point detection, road image segmentation problem, road scene based on binocular vision. Three-dimensional reconstruction and obstacle detection technology.																	0218-0014	1793-6381				JUN 30	2020	34	7							2050020	10.1142/S0218001420500202													
J								Construction Cost Prediction Based on Genetic Algorithm and BIM	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										GA model; engineering cost; BIM; aided analysis	OPTIMIZATION; DESIGN	According to the analysis and prediction of engineering cost, a BIM-aided analysis method based on GA network model is proposed. First, we improve the neural network by genetic algorithm; second, according to the engineering feature vector, BIM software is used to train the GA network model; finally, the GA network model reaches a steady state, given prediction of Engineering cost. According to the experimental study of 20 high-rise residential buildings in YJW area, the experimental results show that the proposed GA model combined with BIM auxiliary analysis method can accurately and easily complete the project cost prediction.																	0218-0014	1793-6381				JUN 30	2020	34	7							2059026	10.1142/S0218001420590260													
J								Design of Outdoor Fire Intelligent Alarm System Based on Image Recognition	INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE										Image recognition; outdoor fire; fire detection; alarm system		Fire is one of the most common serious disasters in human society. It is a kind of burning phenomenon that is out of control in time and space. When a fire occurs, how to detect the fire quickly and remove it in the budding state has become the key content of fire control work. Outdoor fire is very common in our daily life, and once it occurs without effective and timely control, it will cause huge losses. Therefore, it is particularly important to study an intelligent alarm system for outdoor fire. Generally, fire detection technology can be divided into sensor fire detection technology and image fire detection technology. Sensor fire detection technology is low cost and easy to design, but its application field is limited. Under the interference of many factors outside, misjudgement and missed judgement will occur. Image fire detection technology can achieve certain detection function through manual design of features and classifiers, but there are still defects in the application in the actual diversified environment. With the development of neural network technology in recent years, it has made great breakthroughs in the field of image recognition. Its judgment type is obtained through a large number of data training algorithms. Because of its automatic feature extraction and classification characteristics, it can effectively adapt to the external environment. Therefore, this paper proposes an end-to-end two-stream neural network model to detect fires, uses fire video on the network to train the algorithm, and then uses the fire database to test. Compared with the existing fire detection algorithms, it is found that the proposed method has good practicability and versatility, and provides a good reference for the development of fire detection technology.																	0218-0014	1793-6381				JUN 30	2020	34	7							2050018	10.1142/S0218001420500184													
J								Incrementally updating the high average-utility patterns with pre-large concept	APPLIED INTELLIGENCE										High average-utility itemset; Insertion; Pre-large; Dynamic; AUL-structures	EFFICIENT ALGORITHMS; ITEMSETS; DISCOVERY; DATABASES	High-utility itemset mining (HUIM) is considered as an emerging approach to detect the high-utility patterns from databases. Most existing algorithms of HUIM only consider the itemset utility regardless of the length. This limitation raises the utility as a result of a growing itemset size. High average-utility itemset mining (HAUIM) considers the size of the itemset, thus providing a more balanced scale to measure the average-utility for decision-making. Several algorithms were presented to efficiently mine the set of high average-utility itemsets (HAUIs) but most of them focus on handling static databases. In the past, a fast-updated (FUP)-based algorithm was developed to efficiently handle the incremental problem but it still has to re-scan the database when the itemset in the original database is small but there is a high average-utility upper-bound itemset (HAUUBI) in the newly inserted transactions. In this paper, an efficient framework called PRE-HAUIMI for transaction insertion in dynamic databases is developed, which relies on the average-utility-list (AUL) structures. Moreover, we apply the pre-large concept on HAUIM. A pre-large concept is used to speed up the mining performance, which can ensure that if the total utility in the newly inserted transaction is within the safety bound, the small itemsets in the original database could not be the large ones after the database is updated. This, in turn, reduces the recurring database scans and obtains the correct HAUIs. Experiments demonstrate that the PRE-HAUIMI outperforms the state-of-the-art batch mode HAUI-Miner, and the state-of-the-art incremental IHAUPM and FUP-based algorithms in terms of runtime, memory, number of assessed patterns and scalability.																	0924-669X	1573-7497				NOV	2020	50	11					3788	3807		10.1007/s10489-020-01743-y		JUN 2020											
J								A novel multi-classifier based on a density-dependent quantized binary tree LSSVM and the logistic global whale optimization algorithm	APPLIED INTELLIGENCE										Multi-class classification; Least squares support vector machine; Whale optimization algorithm; Binary tree	SUPPORT VECTOR MACHINE; BEE COLONY ALGORITHM; GENETIC ALGORITHM; FAULT-DIAGNOSIS; NEURAL-NETWORK; SPARSE LSSVM; SVM; PREDICTION; SELECTION; STRATEGY	The least squares support vector machine (LSSVM) is a useful binary classifier, but its performance is limited due to the lack of sparseness. The density-dependent quantized LSSVM (DSM) with quantized input data can increase the sparseness to effectively accomplish binary classification. However, the DSM cannot be directly used in multi-classification applications for most practical data-classification problems. We propose a novel multi-classifier based on a density-dependent quantized binary tree LSSVM (DBSM) and the logistic global whale optimization algorithm (LWA) to improve multi-classification accuracy and computational efficiency. The DBSM consists of multiple DSM classifiers, which hierarchically divide data according to a modified binary tree architecture. The tree architecture is constructed quickly and correctly with the quantized data instead of the original input data. An appropriate initial population of DBSM parameters is generated by using a logistic map and an improved opposition-based learning strategy. Then, the DBSM parameters are optimized by the whale optimization algorithm integrated with the gbest-guided artificial bee colony algorithm. According to the experimental results, the DBSM solves multi-classification problems faster than the one-versus-one based support vector machine (OVO-SVM) and the one-versus-all based LSSVM without sacrificing accuracy. The LWA precisely finds the optimal DBSM parameters without a heavy computational burden, in contrast to recent optimization algorithms. The proposed classifier achieves a 3.39% higher accuracy and consumes 52.83% less time than the genetic algorithm-based OVO-SVM. These results prove that the LWA-DBSM can complete multi-class classification tasks precisely and quickly.																	0924-669X	1573-7497				NOV	2020	50	11					3808	3821		10.1007/s10489-020-01736-x		JUN 2020											
J								Clustering based imputation algorithm using unsupervised neural network for enhancing the quality of healthcare data	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Missing values; Imputation; Unsupervised; Neural network; Classifiers	MISSING DATA; PREDICTION; VALUES; TREE	Historical and real-time healthcare data sets are valuable sources of information for predictive data analytics. However, most of the historical healthcare data sets are overloaded with challenges. One of the most frequently faced challenge is the problem of missing values, occurring because of the inaccuracies in data transmission or data entry processes. An appropriate technique for handling missing values is required to generate good quality data sets for achieving better prediction results. Removing the records with missing values, known as marginalization, poses an easy way out to this challenge. But, this will lessen the data volume of the historical data set and disturb the class balance of the data set. An alternative to marginalization is replacing missing values with plausible values, known as imputation. This paper proposes a missing value imputation technique, CLUSTIMP, using an unsupervised neural network Adaptive Resonance Theory 2 (ART2). The efficiency of the proposed imputation method is evaluated on the incomplete Mammographic mass data set and Hepatocellular Carcinoma data set (HCC) from the UCI repository considering Root Mean Squared Error (RMSE) rate and classification accuracy as the evaluation metrics. The proposed CLUSTIMP imputation algorithm outperforms existing state-of-the-art imputation methods by reducing classifiers error rates between 2 and 11%.																	1868-5137	1868-5145															10.1007/s12652-020-02250-1		JUN 2020											
J								Wheel hub customization with an interactive artificial immune algorithm	JOURNAL OF INTELLIGENT MANUFACTURING										Personalized customization; Interactive artificial immune algorithm; Wheel hub; Clustering algorithm; Customer fatigue	GENETIC ALGORITHM; PRODUCT CONFIGURATION; CLUSTERING-ALGORITHM; QFD APPROACH; DESIGN; POPULATION; FRAMEWORK	With the transformation from traditional manufacturing to intelligent manufacturing, customer-oriented personalized customization has gradually become the main mode of production. Interactive algorithms determine the pros and cons of the solution via customers which can make customers better participants in the customization process. However, if the population size is expanded and the number of evolutionary iterations is too high, frequent interactions are likely to cause customer fatigue. This paper proposes an adaptive interactive artificial immune algorithm based on improved hierarchical clustering. This algorithm uses the improved hierarchical clustering algorithm to optimize generation of the initial antibodies and applies the affinity calculation method based on customer intention, adaptive crossover and mutation operators, and a multisolution reservation method based on hybrid selection strategy to the artificial immune algorithm. Via empirical research on the customized operational data of wheel hubs, the proposed method effectively solves the problem of customer fatigue, significantly improves the convergence speed of the algorithm and reduces the time cost.																	0956-5515	1572-8145															10.1007/s10845-020-01613-x		JUN 2020											
J								A fuzzy compromise approach for solving multi-objective stratified sampling design	NEURAL COMPUTING & APPLICATIONS										Fuzzy compromise allocation; Multivariate stratified sampling; Multi-objective programming	MULTIVARIATE; ALLOCATION	In this paper, we established a framework for finding out the optimal allocation in the multivariate stratified sample using the fuzzy compromise method. The problem of multivariate stratified sample is formulated as an all integer nonlinear programming problem, and a solution is obtained by using the criterion of "Minimizing the sum of the squares of coefficient of variation for different characteristics." There is also a quantitative illustration being carried out to explain the statistical nature of the approaches and solved through the LINGO program. We have also studied different techniques for comparison.																	0941-0643	1433-3058															10.1007/s00521-020-05152-7		JUN 2020											
J								Volcano eruption algorithm for solving optimization problems	NEURAL COMPUTING & APPLICATIONS										Optimization; Meta-heuristics; Constrained optimization; Volcano eruption algorithm (VEA); Bi-level optimization		Meta-heuristic algorithms have been proposed to solve several optimization problems in different research areas due to their unique attractive features. Traditionally, heuristic approaches are designed separately for discrete and continuous problems. This paper leverages the meta-heuristic algorithm for solving NP-hard problems in both continuous and discrete optimization fields, such as nonlinear and multi-level programming problems through extensive simulations of volcano eruption process. In particular, a new optimization solution named volcano eruption algorithm is proposed in this paper, which is inspired from the nature of volcano eruption. The feasibility and efficiency of the algorithm are evaluated using numerical results obtained through several test problems reported in the state-of-the-art literature. Based on the solutions and number of required iterations, we observed that the proposed meta-heuristic algorithm performs remarkably well to solve NP-hard problem. Furthermore, the proposed algorithm is applied to solve some large-size benchmarking LP and Internet of vehicles problems efficiently.																	0941-0643	1433-3058															10.1007/s00521-020-05124-x		JUN 2020											
J								Bipartite consensus of double-integrator multi-agent systems with nonuniform communication time delays	NEURAL COMPUTING & APPLICATIONS										Bipartite consensus; Double-integrator multi-agent systems; Communication time delays; Antagonistic network	NETWORKS; AGENTS; FLOCKING	In this paper, the bipartite consensus problem is addressed for a class of double-integrator multi-agent systems with antagonistic interactions. The cases with and without communication time delays are considered. In particular, if the communication time delays are not taken into account, the bipartite consensus of the studied multi-agent systems with directed signed graph can be achieved by the proposed distributed controller. If the nonuniform communication time delays are considered, the bipartite consensus of the considered multi-agent systems with undirected signed graph can be achieved if the time delays are less than a derived upper bound. Moreover, we propose an algorithm to solve the so-called grouping problem. Finally, some numerical examples are provided to illustrate the correctness of the results.																	0941-0643	1433-3058															10.1007/s00521-020-05072-6		JUN 2020											
J								Solving visual object ambiguities when pointing: an unsupervised learning approach	NEURAL COMPUTING & APPLICATIONS										Pointing gestures; Pointing intention; Object ambiguities; Grow-when-required networks; Human-robot interaction		Whenever we are addressing a specific object or refer to a certain spatial location, we are using referential or deictic gestures usually accompanied by some verbal description. Particularly, pointing gestures are necessary to dissolve ambiguities in a scene and they are of crucial importance when verbal communication may fail due to environmental conditions or when two persons simply do not speak the same language. With the currently increasing advances of humanoid robots and their future integration in domestic domains, the development of gesture interfaces complementing human-robot interaction scenarios is of substantial interest. The implementation of an intuitive gesture scenario is still challenging because both the pointing intention and the corresponding object have to be correctly recognized in real time. The demand increases when considering pointing gestures in a cluttered environment, as is the case in households. Also, humans perform pointing in many different ways and those variations have to be captured. Research in this field often proposes a set of geometrical computations which do not scale well with the number of gestures and objects and use specific markers or a predefined set of pointing directions. In this paper, we propose an unsupervised learning approach to model the distribution of pointing gestures using a growing-when-required (GWR) network. We introduce an interaction scenario with a humanoid robot and define the so-called ambiguity classes. Our implementation for the hand and object detection is independent of any markers or skeleton models; thus, it can be easily reproduced. Our evaluation comparing a baseline computer vision approach with our GWR model shows that the pointing-object association is well learned even in cases of ambiguities resulting from close object proximity.																	0941-0643	1433-3058															10.1007/s00521-020-05109-w		JUN 2020											
J								Effect of hand grip actions on object recognition process: a machine learning-based approach for improved motor rehabilitation	NEURAL COMPUTING & APPLICATIONS										Brain-computer interfaces; Machine learning; EEG; Hand action; Neural network	CLASSIFICATION; EPILEPSY	Brain-computer interface (BCI) is the current trend in technology expansion as it provides an easy interface between human brain and machine. The demand for BCI-based applications is growing tremendously, and efforts are in progress to deploy BCI devices for real-world applications. One of the widely known applications of BCI technology is rehabilitation in which BCI devices can provide various types of assistance to specially abled persons. In this paper, the effect of hand actions on objects is analyzed for motor-related mental task. The proposed approach analyzes electroencephalogram (EEG)-based brain activity which was captured for images shown with different gripping actions on objects. The EEG recordings are first pre-processed, followed by extraction of epochs and frequency bands using discrete wavelet transform; afterward, feature extraction followed by training and classification steps is performed for classifying the grip action into congruent (correct) and incongruent (incorrect) grip categories. The proposed work makes use of average power and relative wavelet energy as discriminating features which are then fed to train an artificial neural network for automatically classifying the incoming EEG patterns into correct or incorrect object hand grips. The performance evaluation of the proposed system is done on real EEG dataset obtained from 14 subjects. Experimental results have shown an accuracy of 75%. Also, to evaluate the effectiveness of our work, a comparison of our work with other state-of-the-art works reported by different authors is presented at the end. The results show the effectiveness of proposed approach and suggest further that the system can be used to analyze and train subjects having motor-related disabilities for perceiving correct or incorrect hand grips on objects.																	0941-0643	1433-3058															10.1007/s00521-020-05125-w		JUN 2020											
J								Possibility mean, variance and standard deviation of single-valued neutrosophic numbers and its applications to multi-attribute decision-making problems	SOFT COMPUTING										Possibility mean; Possibility variance; Possibility standard deviation; Single-valued neutrosophic numbers; Multi-attribute decision making	INTUITIONISTIC FUZZY-SETS	Single-valued neutrosophic numbers (SVN-numbers) are a special kind of neutrosophic set on the real number set. The concept of a SVN-number is important for quantifying an ill-known quantity and ranking of SVN-number is a very difficult situation in decision-making problems. The main aim of this paper is to present a new ranking methodology of SVN-numbers for solving multi-attribute decision-making problems. Therefore, we firstly define the possibility mean, variance and standard deviation of single-valued neutrosophic numbers. Using the ratio of possibility mean and standard deviation, we have developed the proposed ranking approach and applied to MADM problems. Finally, a numerical example is examined to show the applicability and embodiment of the proposed method.																	1432-7643	1433-7479															10.1007/s00500-020-05112-2		JUN 2020											
J								An ECC-based access control scheme with lightweight decryption and conditional authentication for data sharing in vehicular networks	SOFT COMPUTING										Elliptic curve cryptography; Key-policy attribute-based encryption (KP-ABE); Authentication; Implicit certificate; Vehicular network	ENCRYPTION SCHEME; CONTROL SYSTEM; CLOUD; SERVICES	Nowadays, more and more data are stored on cloud for sharing in vehicular networks, but the increasing number of cloud data security incidents makes how to guarantee confidentiality of sharing data one of the main concerns. Attribute-based encryption is considered as a suitable method to solve this issue. However, the requirements of lightweight and privacy make it difficult to apply the existing attribute-based encryption schemes directly in vehicular networks. In this paper, we put forward an access control scheme with lightweight decryption and conditional authentication for secure data sharing in vehicular networks. In this scheme, we extend elliptic-curve cryptography-based key-policy attribute-based encryption scheme with token-based decryption for lightweight access control. Moreover, we integrate Elliptic Curve Qu-Vanstone implicit certificate with ELGamal encryption algorithm to achieve both mutual authentication and conditional privacy protection. The performance analysis shows that the proposed scheme requires less time for both encryption and decryption on the user side. The security analysis shows that the proposed scheme can provide conditional anonymity.																	1432-7643	1433-7479															10.1007/s00500-020-05117-x		JUN 2020											
J								Neural optimal self-constrained computing in smart grid considering fine-tuning island constraints with visual information	SOFT COMPUTING										Visual information; Smart grid; Feature extraction; Neural optimization; Self-constrained computing; Communication network		With the expansion of the power grid and the acceleration of power information construction, the information communication network covers all aspects and collects accurate information in time to provide continuous and reliable operation for users. The modern power system will gradually enter the era of interconnected power grids. It is more environmentally friendly and efficient than traditional power systems, management is more information and lean, and its operation is safer and more stable. As the infrastructure for carrying smart grid and future energy information interaction, the power communication network has higher and higher requirements for reliability. The coupling between the communication network and the power grid is more and more closely related. The real-time acquisition and the reliable transmission of control information such as the power system require the support of the power communication network. This paper uses machine learning algorithms to learn effective features or patterns from these data and apply them to new data. In this paper, we present a neural optimal self-constrained computing model based on fine-tuning island constraints with visual information. The experimental results show that the proposed algorithm has higher processing efficiency and accuracy.																	1432-7643	1433-7479															10.1007/s00500-020-05128-8		JUN 2020											
J								Another View on Intuitionistic Fuzzy Preference Relation-Based Aggregation Operators and Their Applications	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Fuzzy set; Fuzzy preference relation; Upward intuitionistic fuzzy preference relation	PYTHAGOREAN MEMBERSHIP GRADES; ATTRIBUTE DECISION-MAKING; CORRELATION-COEFFICIENTS; INFORMATION; NUMBERS; SETS	Multi-attribute group decision making (MAGDM) can be considered as the process of ranking alternatives or selecting an optimal alternative by many decision makers based on multiple criteria. By comparing any two alternatives pairwise, preference relations provide efficient ways to represent the preference degrees of decision makers. The aim of this paper is to introduce the notions of upward intuitionistic fuzzy preference relations from fuzzy information system and to study some properties of these relations. Further the series of upward intuitionistic fuzzy preference aggregation operators including the upward intuitionistic fuzzy preference weighted averaging operator, upward intuitionistic fuzzy preference ordered weighted averaging operator and upward intuitionistic fuzzy preference hybrid averaging operator and their related results are also investigated. We developed a MAGDM method based on the proposed operators under the fuzzy environment and illustrated with a numerical example to study the applicability of the new approach on a candidate selection decision-making problem.																	1562-2479	2199-3211				SEP	2020	22	6			SI		1786	1800		10.1007/s40815-020-00882-1		JUN 2020											
J								Sequence in Hybridization of Statistical and Intelligent Models in Time Series Forecasting	NEURAL PROCESSING LETTERS										Series hybrid models; Sequential modeling order; Multilayer perceptrons (MLPs); Autoregressive integrated moving average (ARIMA); Support vector machines (SVMs); Time series forecasting	HYBRID APPROACH; COMBINATION; SVR	With the importance of forecasting with a high degree of accuracy, the increasing attention has been evolved to combine individual models, especially statistical and intelligent ones. The main aim of such that hybrid models is to extract unique modeling strengths in linear and nonlinear pattern recognition, respectively. Therefore, different hybridization methods are proposed in recent literature for time series forecasting. One of the most widely-used combination strategies applied for numerous forecasting problems to yield more accurate results is the series hybrid strategy. In this hybridization methodology, components of a time series are separated and then modeled sequentially by choosing appropriate single models. However, the most accurate series hybrid model developed by determining the proper arrangement of single models. Thus, one of the critical issues in constructing series hybrid models is how to choose the appropriate sequence of individual models in a sequential modeling procedure. Although it is critically affecting on obtaining more accurate forecasting results, it has not been appropriately discussed in the literature of time series forecasting. Thus, in this paper, the performance of two possible sequence modeling procedures, including linear-nonlinear and nonlinear-linear, are evaluated. For this purpose, autoregressive integrated moving average (ARIMA), support vector machines (SVM), and multilayer perceptrons (MLP) models are chosen due to the popularity of these approaches for developing statistical/intelligent series hybrid models. Five well-known real data sets, e.g., Wolf's Sunspot, Canadian Lynx, British pound/US dollar exchange rate, Nikkei 225 stock price, and the Colorado wind speed, are considered to distinguish better sequences. In this way, the main objective of this paper is to response this unanswered question in the literature that which sequence of single models can lead to obtain much better accuracy in constructing bi-component series hybrid models. Empirical results indicate that choosing the nonlinear intelligent model as first component in sequential modeling procedure can lead to yield more accurate results. Both SVM-ARIMA and MLP-ARIMA models can improve the performance of the ARIMA-SVM and ARIMA-MLP, respectively. Therefore, it can be concluded that the nonlinear-linear series hybrid models may produce more accurate results than linear-nonlinear hybrid models for time series forecasting.																	1370-4621	1573-773X															10.1007/s11063-020-10294-9		JUN 2020											
J								An Enhanced Robot Massage System in Smart Homes Using Force Sensing and a Dynamic Movement Primitive	FRONTIERS IN NEUROROBOTICS										hybrid force; position; teaching by demonstration; dynamic motion primitive; dynamic time warping; gaussian mixture regression		With requirements to improve life quality, smart homes, and healthcare have gradually become a future lifestyle. In particular, service robots with human behavioral sensing for private or personal use in the home have attracted a lot of research attention thanks to their advantages in relieving high labor costs and the fatigue of human assistance. In this paper, a novel force-sensing- and robotic learning algorithm-based teaching interface for robot massaging has been proposed. For the teaching purposes, a human operator physically holds the end-effector of the robot to perform the demonstration. At this stage, the end position data are outputted and sent to be segmented via the Finite Difference (FD) method. A Dynamic Movement Primitive (DMP) is utilized to model and generalize the human-like movements. In order to learn from multiple demonstrations, Dynamic Time Warping (DTW) is used for the preprocessing of the data recorded on the robot platform, and a Gaussian Mixture Model (GMM) is employed for the evaluation of DMP to generate multiple patterns after the completion of the teaching process. After that, a Gaussian Mixture Regression (GMR) algorithm is applied to generate a synthesized trajectory to minimize position errors. Then a hybrid position/force controller is integrated to track the desired trajectory in the task space while considering the safety of human-robot interaction. The validation of our proposed method has been performed and proved by conducting massage tasks on a KUKA LBR iiwa robot platform.																	1662-5218					JUN 29	2020	14								30	10.3389/fnbot.2020.00030													
J								Intelligence in cyberspace: the road to cyber singularity	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Intelligence; technological singularity; cybersecurity; cyber singularity; artificial intelligence	SELF-ORGANIZATION; REPLICATION; EVOLUTION; BEHAVIOR; NETWORK; SYSTEMS	Intelligence has been defined in many ways like logic, awareness, reasoning, critical thinking, etc. Many researchers insist on the possibility of a Technological Singularity shortly, which may see machines gaining intelligence similar to, or greater than humans. While many researchers believe that Technological Singularity is at an arm's length, many counter-question the possibility of the same due to the lack of concrete evidence. Recently Cybersecurity has manoeuvred its way through technology to become one of the most rapidly advancing fields. Artificial Intelligence introduced to Cybersecurity has seen a tremendous increase in the number of systems that are capable of performing tasks faster and better than humans. This has led us to believe that there is intelligence in cyberspace along with the possibility of Cyber Singularity. We emphasise the intelligence of systems using a set of characteristics that insist on how sophisticated the systems have become over time that might lead to Cyber Singularity. We map these characteristics to the characteristics of living species with the hope of locating intelligence in the biomedical domain and further, try to identify systems displaying such characteristics in cyberspace. Keeping in mind the concept of technological singularity proposed before, we also perform an extensive survey of the past research works related to the field and also, use the concepts of set theory to reinforce the possibility of Cyber Singularity in the coming years.																	0952-813X	1362-3079															10.1080/0952813X.2020.1784296		JUN 2020											
J								Adaptive chaotic satin bowerbird optimisation algorithm for numerical function optimisation	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Satin Bowerbird Optimisation; meta-heuristic algorithm; adaptive operator selection; chaotic map	DIFFERENTIAL EVOLUTION ALGORITHM; PARTICLE SWARM OPTIMIZATION; SEARCH OPTIMIZATION; DESIGN OPTIMIZATION; KRILL HERD; MIGRATION; FRAMES	The Satin Bowerbird Optimisation (SBO) was inspired by the Satin Bowerbirds living in Australia's rainforests and other mesic habitats. Like other meta-heuristic algorithms, the main problem faced by the SBO is that it has been empirically demonstrated to become easily trapped into local optimal solutions, creating low precision and slow convergence speeds. To overcome these deficiencies, we propose herein the Adaptive Chaotic Satin Bowerbird Optimisation algorithm (AC-SBO). Within the AC-SBO algorithm, a chaotic map is introduced to modify the search process, with which to enhance global convergence speeds, and to obtain better performance. We introduced the chaos theory into the SBO optimisation process, in order to replace the main parameter's greatest step size (alpha), which assists in controlling the balance between both exploration and exploitation. The search accuracy and performance of the AC-SBO algorithm were verified on ten classical benchmark functions. In addition, in the experimental CEC2014 results showed that for almost all functions, the AC-SBO technique proved superior to the other comparative algorithms optimisations. The Wilcoxon rank-sum statistical test was performed in order to judge the significance of the results, and further demonstrated the improved performance of the proposed AC-SBO algorithm.																	0952-813X	1362-3079															10.1080/0952813X.2020.1785018		JUN 2020											
J								A machine learning approach to automatic detection of irregularity in skin lesion border using dermoscopic images	PEERJ COMPUTER SCIENCE										Machine learning; Dermoscopy; Skin lesion; Melanoma; Segmentation	PIGMENTED LESIONS; MALIGNANT-MELANOMA; 7-POINT CHECKLIST; SEGMENTATION; CLASSIFICATION; CANCER; DIAGNOSIS; BENIGN; SHAPE	Skin lesion border irregularity is considered an important clinical feature for the early diagnosis of melanoma, representing the B feature in the ABCD rule. In this article we propose an automated approach for skin lesion border irregularity detection. The approach involves extracting the skin lesion from the image, detecting the skin lesion border, measuring the border irregularity, training a Convolutional Neural Network and Gaussian naive Bayes ensemble, to the automatic detection of border irregularity, which results in an objective decision on whether the skin lesion border is considered regular or irregular. The approach achieves outstanding results, obtaining an accuracy, sensitivity, specificity, and F-score of 93.6%, 100%, 92.5% and 96.1%, respectively.																	2376-5992					JUN 29	2020									e268	10.7717/peerj-cs.268													
J								An expert study on hierarchy comparison methods applied to biological taxonomies curation	PEERJ COMPUTER SCIENCE										Hierarchy comparison; Biological taxonomy; Information visualization; Expert study; Comparison; Hierarchy visualization methods; Quantitative and qualitative evaluation	VISUALIZATION	Comparison of hierarchies aims at identifying differences and similarities between two or more hierarchical structures. In the biological taxonomy domain, comparison is indispensable for the reconciliation of alternative versions of a taxonomic classification. Biological taxonomies are knowledge structures that may include large amounts of nodes (taxa), which are typically maintained manually. We present the results of a user study with taxonomy experts that evaluates four well-known methods for the comparison of two hierarchies, namely, edge drawing, matrix representation, animation and agglomeration. Each of these methods is evaluated with respect to seven typical biological taxonomy curation tasks. To this end, we designed an interactive software environment through which expert taxonomists performed exercises representative of the considered tasks. We evaluated participants' effectiveness and level of satisfaction from both quantitative and qualitative perspectives. Overall quantitative results evidence that participants were less effective with agglomeration whereas they were more satisfied with edge drawing. Qualitative findings reveal a greater preference among participants for the edge drawing method. In addition, from the qualitative analysis, we obtained insights that contribute to explain the differences between the methods and provide directions for future research.																	2376-5992					JUN 29	2020									e277	10.7717/peerj-cs.277													
J								From machine ethics to computational ethics	AI & SOCIETY										Ethics of AI; Machine ethics; Robot ethics; Computational ethics; Autonomous intelligent systems; Artificial intelligence	CONSCIOUSNESS; ROBOTICS; INTELLIGENCE; RATIONALITY; BEHAVIOR; FUTURE; RIGHTS; LAWS	Research into the ethics of artificial intelligence is often categorized into two subareas-robot ethics and machine ethics. Many of the definitions and classifications of the subject matter of these subfields, as found in the literature, are conflated, which I seek to rectify. In this essay, I infer that using the term 'machine ethics' is too broad and glosses over issues that the term computational ethics best describes. I show that the subject of inquiry of computational ethics is of great value and indeed is an important frontier in developing ethical artificial intelligence systems (AIS). I also show that computational is a distinct, often neglected field in the ethics of AI. In contrast to much of the literature, I argue that the appellation 'machine ethics' does not sufficiently capture the entire project of embedding ethics into AI/S, and hence the need for computational ethics. This essay is unique for two reasons; first, it offers a philosophical analysis of the subject of computational ethics that is not found in the literature. Second, it offers a finely grained analysis that shows the thematic distinction among robot ethics, machine ethics and computational ethics.																	0951-5666	1435-5655															10.1007/s00146-020-01010-1		JUN 2020											
J								Speaker recognition based on characteristic spectrograms and an improved self-organizing feature map neural network	COMPLEX & INTELLIGENT SYSTEMS										Speaker recognition; Characteristic spectrogram; Adaptive clustering; Neural network; Deep learning; Edge intelligence		To obtain a speaker's pronunciation characteristics, a method is proposed based on an idea from bionics, which uses spectrogram statistics to achieve a characteristic spectrogram to give a stable representation of the speaker's pronunciation from a linear superposition of short-time spectrograms. To deal with the issue of slow network training and recognition speed for speaker recognition systems on resource-constrained devices, based on a traditional SOM neural network, an adaptive clustering self-organizing feature map SOM (AC-SOM) algorithm is proposed. This algorithm automatically adjusts the number of neurons in the competition layer based on the number of speakers to be recognized until the number of clusters matches the number of speakers. A 100-speaker database of characteristic spectrogram samples was built and applied to the proposed AC-SOM model, yielding a maximum training time of only 304 s, with a maximum sample recognition time of less than 28 ms. Comparing to other approaches, the proposed method offers greatly improved training and recognition speed without sacrificing too much recognition accuracy. The promising results suggest that the proposed method satisfies real-time data processing and execution requirements for edge intelligence systems better than other speaker recognition methods.																	2199-4536	2198-6053															10.1007/s40747-020-00172-1		JUN 2020											
J								Modified Vogel's approximation method for transportation problem under uncertain environment	COMPLEX & INTELLIGENT SYSTEMS										Fuzzy set; Interval type 2 fuzzy set; Transportation problem; Vogel's approximation method	TYPE-2 FUZZY-SETS; DECISION-MAKING; ACCURACY FUNCTION; TIME	The fuzzy transportation problem is a very popular, well-known optimization problem in the area of fuzzy set and system. In most of the cases, researchers use type 1 fuzzy set as the cost of the transportation problem. Type 1 fuzzy number is unable to handle the uncertainty due to the description of human perception. Interval type 2 fuzzy set is an extended version of type 1 fuzzy set which can handle this ambiguity. In this paper, the interval type 2 fuzzy set is used in a fuzzy transportation problem to represent the transportation cost, demand, and supply. We define this transportation problem as interval type 2 fuzzy transportation problems. The utility of this type of fuzzy set as costs in transportation problem and its application in different real-world scenarios are described in this paper. Here, we have modified the classical Vogel's approximation method for solved this fuzzy transportation problem. To the best of our information, there exists no algorithm based on Vogel's approximation method in the literature for fuzzy transportation problem with interval type 2 fuzzy set as transportation cost, demand, and supply. We have used two Numerical examples to describe the efficiency of the proposed algorithm.																	2199-4536	2198-6053															10.1007/s40747-020-00153-4		JUN 2020											
J								ANFIS-Based Accurate Estimation of the Confinement Effect for Concrete-Filled Steel Tubular (CFST)	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Concrete-filled; ANFIS; CFST column; Short and long; D; tratio; ANN; Load capacity; OTR; SD; MV	BEAM-COLUMNS; SUSTAINED LOAD; STRENGTH; BEHAVIOR; PERFORMANCE; ALGORITHM; DESIGN	This research is mainly focused on the accurate estimation of the confinement effect for the concrete-filled steel tubular (CFST) that makes it possible to evaluate the interaction between various parameters that affect the confinement effect. To do that, the CFST is analyzed with concrete and steel properties using ANFIS method. With respect to the shape of the CFST, both the circular and rectangle shapes are considered. Only then, theD/tratio is increased and reduced the hoop stress, self-stress in the steel tube. To analyze theD/tratio, the confinement effect and axial load capacity is determined. After that, the concrete strength is also analyzed according to their statistical measures like output target ratio (OTR), precision, efficiency, mean value (MV), mean square error (MSE), standard deviation (SD), etc. The proposed method is implemented in MATLAB platform and compared with the Artificial Neural Network (ANN) method. The proposed ANFIS method achieved a good prediction of the confinement effect and axial load capacity of the CFST.																	1562-2479	2199-3211				SEP	2020	22	6			SI		1760	1771		10.1007/s40815-020-00902-0		JUN 2020											
J								Coordinated Control of Dual-Motor Using the Interval Type-2 Fuzzy Logic in Autonomous Steering System of AGV	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Trajectory tracking; Model predictive control (MPC); Dual motor; Coordinated control; Type-2 fuzzy controller Hardware-in-Loop (HiL)	TRACKING	To achieve better trajectory tracking of the autonomous ground vehicle (AGV), a dual-motor autonomous steering structure and control method are proposed. The model predictive control (MPC) algorithm is utilized to calculate the optimal front wheel angle in real time based on the vehicle information, and dual-motor steering system follows the target angle to achieve trajectory tracking. In this process, to improve the stability of the angle motor, a coordinated steering controller of the angle motor and torque motor is proposed, and the type-2 fuzzy logic is designed based on the target front wheel angle and vehicle speed to optimize coordination coefficient. The front wheel angle step input and double-lane change trajectory tracking conditions are simulated in the carsim and simulink joint simulation platform, the results show that dual-motor steering system has good trajectory tracking ability, compared with no coordinated control, the current and velocity of angle motor are more stable under the coordinated steering control, which greatly reduces the steering load of the angle motor under different working conditions. The final hardware-in-loop test is put forward to verify the effectiveness of the MPC controller and coordinated steering controller.																	1562-2479	2199-3211															10.1007/s40815-020-00886-x		JUN 2020											
J								In-store customer traffic and path monitoring in small-scale supermarket using UWB-based localization and SSD-based detection	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Image processing; Indoor localization; Retail analytics; Single shot multibox detection; Smart supermarket; Ultra-wideband	INTERNET; SYSTEM	Nowadays, retailers are embracing the Internet of Things as the latest technology to drive superior customer experience. Leverage data sources from sensors, beacons and mobile devices to identify and analyze in-store customer shopping behavior. With this motivation, this study implemented an in-store customer traffic and path monitoring system for supermarket using image processing and object detection. the system utilized the ultra-wideband indoor positioning technique to monitor the customer shopping path and the single shot multibox detection technique to monitor the real-time customer traffic. The customer monitoring system was implemented and evaluated in an actual small-scale supermarket. Results showed that the detection model prediction score and the traffic counting both obtained an accuracy score of 99%. In addition, the localization system achieved the minimum error difference of 9.73% for x coordinate and 3.86% for y coordinate between pre-determined positions and the actual anchor position readings. Furthermore, the system successfully generated the most frequent path and the total customer traffic of the day. In the future, this work can aid retail owners make better choices, run businesses more efficiently, and deliver improved customer service.																	1868-5137	1868-5145															10.1007/s12652-020-02236-z		JUN 2020											
J								TriDroid: a triage and classification framework for fast detection of mobile threats in android markets	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Android security; App triage; Malware detection; Data mining; Machine learning	MALWARE DETECTION	The Android platform is highly targeted by malware developers, which aim to infect the maximum number of mobile devices by uploading their malicious applications to different app markets. In order to keep a healthy Android ecosystem, app-markets check the maliciousness of newly submitted apps. These markets need to (a) correctly detect malicious app, and (b) speed up the detection process of the most likely dangerous applications among an overwhelming flow of submitted apps, to quickly mitigate their potential damages. To address these challenges, we propose TriDroid, a market-scale triage and classification system for Android apps. TriDroid prioritizes apps analysis according to their risk likelihood. To this end, we categorize the submitted apps as: botnet, general malware, and benign. TriDroid starts by performing a (1) Triage process, which applies a fast coarse-grained and less-accurate analysis on a continuous stream of the submitted apps to identify their corresponding queue in a three-class priority queuing system. Then, (2) the Classification process extracts fine-grained static features from the apps in the priority queue, and applies three-class machine learning classifiers to confirm with high accuracy the classification decisions of the triage process. In addition to the priority queuing model, we also propose a multi-server queuing model where the classification of each app category is run on a different server. Experiments on a dataset with more than 24K malicious and 3K benign applications show that the priority model offers a trade-off between waiting time and processing overhead, as it requires only one server compared to the multi-server model. Also it successfully prioritizes malicious apps analysis, which allows a short waiting time for dangerous applications compared to the FIFO policy.																	1868-5137	1868-5145															10.1007/s12652-020-02243-0		JUN 2020											
J								Inception-SSD: An improved single shot detector for vehicle detection	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Vehicle detection; Deep learning; Computer vision; Deep neural network; SSD		Vehicle detection plays an effective and important role in traffic safety, which has attracted extensive attention from both academic and industry. Deep learning has made significant breakthroughs in vehicle detection application. The Single Shot Detector (SSD) algorithm, which is one of the object detection algorithms, is used to detect vehicles. However, its main challenge is that the computing complexity and low accuracy. In this paper, an improved vehicle detection algorithm based on SSD is proposed to improve accuracy, especially for small vehicles detection. We add an Inception block to the extra layer in the SSD before the prediction to improve its performance. Then we use a new method that is more suitable for vehicle detection to set the scales and aspect ratios of the default bounding boxes, which benefits position regression and maintains the fast speed. The validity of our algorithm is verified on KITTI and UVD datasets. Compared with SSD, our algorithm achieves a higher mean average precision (mAP), while maintaining a fast speed.																	1868-5137	1868-5145															10.1007/s12652-020-02085-w		JUN 2020											
J								Computation of minimal unsatisfiable subformulas for SAT-based digital circuit error diagnosis	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Minimal unsatisfiable subsets; Minimal correction subsets; SAT solving; Conjunction normal form; GPU; Boolean satisfiability problem	ALGORITHM; DESIGN	The explanation of infeasibilities formed in Minimal Unsatisfiable Subformulas (MUSes) is a core task in the analysis of over-constrained Boolean formulas. A wide range of applications necessitate MUS detection including knowledge-based validation, software design, verification and error diagnosis in digital VLSI circuits. Consequently, various enhanced algorithms for determining MUS have been utilized for solving Maximum Satisfiability algorithms and Conjunction Normal Form (CNF) redundancies. Three enhancements are proposed in this paper. The first is a CPU-GPU algorithm for computation of Minimal Correction Subsets (MCSes) optimized for NVIDIA General Purpose Graphics Processing Unit paradigm. The proposed algorithm of generating all MCSes from the encoded CNF instance was developed using our parallel SSGPU solver and implemented using CUDA. The second enhancement is an algorithm for MUS computation based on auto-reduction of the enhanced MCSes for faster MUS detection. The third improved algorithm is for computing MUS directly without using MCSes. The two proposed algorithms outperform techniques as they could locate and explain design faults in digital VLSI circuits in earlier stages of IC design flow. The second proposed routine of MUS extraction was performed by avoiding a non-critical step in calling the SAT solver during MUS computation, leading to improving the performance of the MUS extraction algorithm. The third proposed mechanism for direct extraction of MUS was optimized by reducing the required SAT-solver calls using a classification of clauses in the input formula. Comparative analysis of the proposed algorithm against the Compute All Minimal Unsatisfiable Subsets (CAMUS) algorithm determined 1.54 x faster detection of MUS using ISCAS'85, ISCAS'89 and synthetic benchmarks. Also, the third algorithm for direct MUS computation delivers 17.05 x faster than shrinking algorithm used in MUST (minimal unsatisfiable subset) tool using ISCAS'85 and synthetic benchmarks. Moreover, it was observed that the CPU-GPU algorithm for MCSes computation based on the SSGPU solver delivered 1.92 x faster than its conventional counterpart, based on CUD@SAT equivalent on GPU using ISCAS'85, ISCAS'89 and synthetic Benchmarks.																	1868-5137	1868-5145															10.1007/s12652-020-02247-w		JUN 2020											
J								A multi-period maximal coverage model for locating simultaneous ground and air emergency medical services facilities	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Emergency medical services; Ground ambulance stations; Air ambulance stations; Transfer point location; Demand pattern change	DYNAMIC-MODEL; AMBULANCE; ALLOCATION; RELOCATION; REDEPLOYMENT; OPTIMIZATION; STATIONS; BASE	In many cases where emergency medical services are required, ground and air ambulances cooperate in a shared activity to transfer the injured from the accident scene to treatment centers. The cooperation becomes necessary, particularly where the air ambulance can not land at the accident scene, and it sometimes seems essential to make use of transfer points during the cooperation. Given that the demand pattern might change during time, it does not seem logical to formulate the problem statically. Hence, in this paper, we have presented a multi-period maximal coverage location model, which simultaneously locates the transfer points, ground ambulance stations, air ambulance stations, and allocates ambulances to them. Also, the available ambulances are moved among the established stations if required subject to demand changes during the planning horizon. Coverage is provided based on the times it takes the ambulance to arrive at the accident scene and to transfer from the accident scene to the nearest treatment center. In this research, four methods of providing service to the injured and transferring them to treatment centers have been considered. In the presented model, the inaccessibility of ambulances has been considered in light of the notion of backup coverage. Given the complexity of the problem and the impossibility to solve it optimally at large-scale problems, a heuristic algorithm with the greedy approach has been presented for solving it. The obtained computational results demonstrate the efficiency of the proposed algorithm in solving different problems as compared to CPLEX.																	1868-5137	1868-5145															10.1007/s12652-020-02230-5		JUN 2020											
J								Modelling the map reduce based optimal gradient boosted tree classification algorithm for diabetes mellitus diagnosis system	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Big data; Classification; Clustering; Diabetes mellitus; Hadoop; Gradient boosting	HEALTH-CARE; BIG DATA	In recent days, the term big data become popular and refers to data heterogeneity and massive quantity which gets updated and multiplied in every fraction of second. This paper discusses the application of big data and its impact on the medical domain. It is noted that the usage of big data models and methods is seamlessly using in the management of exponential data growth in the healthcare domain. Presently, it is complex to visualize how machine learning and big data will have an impact on the medical field. At the same time, there is a significant increment in the number of persons suffers from diabetes mellitus (DM) in various healing centers. This study develops a new map reduce based optimal data classifier (MRODC) technique to diagnose DM efficiently. The presented MRODC model involves different stages of the Hadoop ecosystem, data acquisition, and classification based on gradient boosting tree (GBT). To further improvising the classifier results of the GBT, an improved k-means clustering approach is integrated into it. The traditional K-means clustering involves a random generation of seed value, which greatly affects the cluster's outcome. In improved K-means clustering, a new mechanism is introduced, which sets the seed value based on the minimal clustering error (CE). A detailed experimentation takes place on the benchmark PIMA Indians Diabetes dataset under several aspects. The obtained simulation outcome depicted that the presented MRODC model produces consistently better results over the compared methods with a supreme precision of 99.23, recall of 97.48, accuracy of 97.79, F-score of 98.34, and kappa value of 95.02 .																	1868-5137	1868-5145															10.1007/s12652-020-02242-1		JUN 2020											
J								Power flow control and power quality analysis in power distribution system using UPQC based cascaded multi-level inverter with predictive phase dispersion modulation method	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Multi-level inverter; Sag; Swell; UPQC; Predictive phase dispersion modulation	PERFORMANCE; IMPROVEMENT; DESIGN; ELIMINATION	Power quality is associated with the ability of utilities to provide electrical power without interruption. One of the major concerns in the electrical industry today is the issue of power quality over major loads. This work presents a Cascaded H-Bridge Multi-Level Inverter (CHBMLI) based unified power quality conditioner (UPQC) for the compensation of voltage sag/swell in the source side and current harmonics in the load side due to the non-linear loads. A predictive control based algorithm called as predictive phase dispersion modulation (PPDM) is used in the proposed system for the control of series and shunt active power filter of the UPQC. The shunt and series active power filter is connected back to back with a common DC link capacitor. The objectives of the proposed algorithm is to maintain a constant DC link voltage and to control the switching pulses of the CHBMLI according to the variations of the current in the load side and also the voltage unbalances in the source side. The performance of the UPQC is significantly improved with the proposed CHBMLI based UPQC than the conventional Voltage Source Inverters (VSI) based UPQC. The algorithm used in this work reduces the computational burden than the conventional model predictive controllers. The proposed CHBMLI based UPQC with PPDM technique is validated through simulation studies using MATLAB and a hardware prototype also realized and the results are investigated.																	1868-5137	1868-5145															10.1007/s12652-020-02253-y		JUN 2020											
J								An innovative interval type-2 fuzzy approach for multi-scenario multi-project cash flow evaluation considering TODIM and critical chain with an application to energy sector	NEURAL COMPUTING & APPLICATIONS										Multi-project; Cash flow assessment; Critical chain project management; TODIM method; Trapezoidal interval type-2 fuzzy sets (TIT2FSs); Energy sector	CRITERIA DECISION-ANALYSIS; CONSTRUCTION PROJECTS; LINEAR ASSIGNMENT; LOGIC SYSTEMS; MANAGEMENT; OPTIMIZATION; MODEL; SETS; UNCERTAINTY; PERFORMANCE	Project management has been proven to be an effective tool to manage sophisticated activities. Various techniques of project management have played an essential role for successful project implementation in different areas. Managing projects, especially in a multi-project environment, involves a complex situation because of its distinguishing feature in which a number of projects are being executed simultaneously, that is, they are followed in parallel. Therefore, applying human resources will be more effective and more idle time will be eliminated as well. More so, it can enable people to share their lessons learned from one project to another. With respect to handling a number of projects at the same time by most firms, it is sophisticated for contractors to cope with financial issues of projects, which involve different project cash inflows and outflows. Thus, taking an accurate cash flow prediction into account for projects has been changed into a crucial matter for firms, and lack of this consideration may result in the failure of projects as well. Moreover, there is a desperate need for uncertainties to be addressed thoroughly regarding their vital role for suitable project management. With these in mind, an innovative approach is proposed in this paper to anticipate the cash flow of project by considering type-2 fuzzy extension of both critical chain project management (CCM) for project scheduling and TODIM (an acronym in Portuguese for interactive Multi-Criteria Decision Making) method for selecting the best scenario in a multi-project environment. Hence, type-2 fuzzy numbers are utilized in order to state uncertainties. Eventually, a real-world project in a petro-refinery firm is applied to indicate the capability of the presented approach.																	0941-0643	1433-3058															10.1007/s00521-020-05095-z		JUN 2020											
J								Solution of asymmetric discrete competitive facility location problems using ranking of candidate locations	SOFT COMPUTING										Asymmetric facility location; Binary choice rule; Combinatorial optimization; Random search; Population-based heuristic algorithms	GENETIC ALGORITHM; OPERATOR; MODELS	We address a discrete competitive facility location problem with an asymmetric objective function and a binary customer choice rule. Both an integer linear programming formulation and a heuristic optimization algorithm based on ranking of candidate locations are designed to solve the problem. The proposed population-based heuristic algorithm is specially adapted for the discrete facility location problems by using their features such as geographical distances and the maximal possible utility of candidate locations, which can be evaluated in advance. The performance of the proposed algorithm was experimentally investigated by solving different instances of the model with real data of municipalities in Spain.																	1432-7643	1433-7479															10.1007/s00500-020-05106-0		JUN 2020											
J								Moore-Penrose generalized inverse mixture design applied in low-density dispersive liquid-liquid microextraction	JOURNAL OF CHEMOMETRICS										blood serum; linoleic acid; miniaturized technique; mixture design; ruminants	MODEL; DERIVATIZATION; MILK	The modification in the nutritional composition of the ruminant diet causes significant alterations in the fatty acids (FAs) structure supplemented because of the action of rumen microorganisms. The modification in the FAs structure alters the role that these play in the ruminant metabolism. The Folch method is the most often used to determine fatty acids in these animals' tissues and presents certain disadvantages such as the great volume of solvent and low mass transfer from the analyte to the extracting phase. Thus, we tested the low-density dispersive liquid-liquid microextraction (LD-DLLME) as an alternative method to determine these substances. In this paper, a simples-augmented mixture design was used. The Scheffe's polynomial was applied in that design, and Moore-Penrose generalized matrix inverse was used because of the possibility of concurrently determining value estimates of coefficients of the parameters that represent cubic terms. The application of the modeling allowed the chemical interpretation of the LD-DLLME best extraction condition for linoleic acid in ruminant serum samples.																	0886-9383	1099-128X														e3275	10.1002/cem.3275		JUN 2020											
J								Propositionalization and embeddings: two sides of the same coin	MACHINE LEARNING										Inductive logic programming; Relational learning; Propositionalization; Embeddings; Knowledge graphs	RELATIONAL DATA; EVIDENCE CONTRARY; STATISTICAL VIEW; DISCOVERY	Data preprocessing is an important component of machine learning pipelines, which requires ample time and resources. An integral part of preprocessing is data transformation into the format required by a given learning algorithm. This paper outlines some of the modern data processing techniques used in relational learning that enable data fusion from different input data types and formats into a single table data representation, focusing on the propositionalization and embedding data transformation approaches. While both approaches aim at transforming data into tabular data format, they use different terminology and task definitions, are perceived to address different goals, and are used in different contexts. This paper contributes a unifying framework that allows for improved understanding of these two data transformation techniques by presenting their unified definitions, and by explaining the similarities and differences between the two approaches as variants of a unified complex data transformation task. In addition to the unifying framework, the novelty of this paper is a unifying methodology combining propositionalization and embeddings, which benefits from the advantages of both in solving complex data transformation and learning tasks. We present two efficient implementations of the unifying methodology: an instance-based PropDRM approach, and a feature-based PropStar approach to data transformation and learning, together with their empirical evaluation on several relational problems. The results show that the new algorithms can outperform existing relational learners and can solve much larger problems.																	0885-6125	1573-0565				JUL	2020	109	7			SI		1465	1507		10.1007/s10994-020-05890-8		JUN 2020											
J								CappedL(1)-norm distance metric-based fast robust twin extreme learning machine	APPLIED INTELLIGENCE										CappedL(1)-norm; Robustness; Pattern classification; Outliers	REGRESSION; CLASSIFICATION	In this paper, we propose a new 0.00,0.00,1.00 fast robust twin extreme learning machine (FRTELM) based on the least squares sense and cappedL(1)-norm distance metric. FRTELM first replaced the inequality constraints in TELM with equality constraints, and then introduced the cappedL(1)-norm distance metric to replace theL(2)-norm distance metric in TELM. FRTELM not only retains the advantages of TELM, but also overcomes the shortcomings of TELM exaggeration of outliers based on squaredL(2)-norm distance metrics. This improvement improves the robustness and learning efficiency of TELM in solving outlier problems. An efficient optimization algorithm is exploited to solve the nonconvex and nonsmooth challenging problem. In theory, we analyze and discuss the complexity of the algorithm in detail, and prove the convergence and local optimality of the algorithm. Extensive experiments conducted across multiple datasets demonstrates that the proposed method is competitive with state-of-the-art methods in terms of robustness and feasibility.																	0924-669X	1573-7497				NOV	2020	50	11					3775	3787		10.1007/s10489-020-01757-6		JUN 2020											
J								A bibliometric analysis on deep learning during 2007-2019	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Deep learning; Machine learning; Bibliometric analysis; Hot topic; Development trend	NEURAL-NETWORKS; CLASSIFICATION; ALGORITHM	As an emerging and applicable method, deep learning (DL) has attracted much attention in recent years. With the development of DL and the massive of publications and researches in this direction, a comprehensive analysis of DL is necessary. In this paper, from the perspective of bibliometrics, a comprehensive analysis of publications of DL is deployed from 2007 to 2019 (the first publication with keywords "deep learning" and "machine learning" was published in 2007). By preprocessing, 5722 publications are exported from Web of Science and they are imported into the professional science mapping tools: VOS viewer and Cite Space. Firstly, the publication structures are analyzed based on annual publications, and the publication of the most productive countries/regions, institutions and authors. Secondly, by the use of VOS viewer, the co-citation networks of countries/regions, institutions, authors and papers are depicted. The citation structure of them and the most influential of them are further analyzed. Thirdly, the cooperation networks of countries/regions, institutions and authors are illustrated by VOS viewer. Time-line review and citation burst detection of keywords are exported from Cite Space to detect the hotspots and research trend. Finally, some conclusions of this paper are given. This paper provides a preliminary knowledge of DL for researchers who are interested in this area, and also makes a conclusive and comprehensive analysis of DL for these who want to do further research on this area.																	1868-8071	1868-808X				DEC	2020	11	12					2807	2826		10.1007/s13042-020-01152-0		JUN 2020											
J								Cluster-based zero-shot learning for multivariate data	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Zero-shot learning; Clustering; Machine learning; Multivariate data		Supervised learning requires a sufficient training dataset which includes all labels. However, there are cases that some class is not in the training data. Zero-shot learning (ZSL) is the task of predicting class that is not in the training data (unseen class). The existing ZSL method is done for image data. However, the zero-shot problem should happen to every data type. Hence, considering ZSL for other data types is required. In this paper, we propose the cluster-based ZSL method, which is a baseline method for multivariate binary classification problems. The proposed method is based on the assumption that if data is far from training data, the data is considered as unseen class. In training, clustering is done for training data. In prediction, the data is determined belonging to a cluster or not. If data does not belong to a cluster, the data is predicted as unseen class. The proposed method is evaluated and demonstrated using the KEEL datasets.																	1868-5137	1868-5145															10.1007/s12652-020-02268-5		JUN 2020											
J								Divide et impera: How disentangling common and distinctive variability in multiset data analysis can aid industrial process troubleshooting and understanding	JOURNAL OF CHEMOMETRICS										canonical correlation analysis (CCA); common components; distinctive components; permutation testing; singular value decomposition (SVD)	COMPONENT ANALYSIS; MULTIVARIATE; SYNCHRONIZATION; REGRESSION; ROTATION; MODELS	The possibility of addressing the problem of process troubleshooting and understanding by modelling common and distinctive sources of variation (factorsorcomponents) underlying two sets of measurements was explored in a real-world industrial case study. The used strategy includes a novel approach to systematically detect the number of common and distinctive components. An extension of this strategy for the analysis of a larger number of data blocks, which allows the comparison of data from multiple processing units, is also discussed.																	0886-9383	1099-128X														e3266	10.1002/cem.3266		JUN 2020											
J								The effects of spatial auditory and visual cues on mixed reality remote collaboration	JOURNAL ON MULTIMODAL USER INTERFACES										Mixed reality; Augmented reality; Virtual reality; Remote collaboration; Spatial audio; Hand gesture	COMMUNICATION CUES; DISPLAY; GESTURE; MOBILE; AUDIO	Collaborative Mixed Reality (MR) technologies enable remote people to work together by sharing communication cues intrinsic to face-to-face conversations, such as eye gaze and hand gestures. While the role of visual cues has been investigated in many collaborative MR systems, the use of spatial auditory cues remains underexplored. In this paper, we present an MR remote collaboration system that shares both spatial auditory and visual cues between collaborators to help them complete a search task. Through two user studies in a large office, we found that compared to non-spatialized audio, the spatialized remote expert's voice and auditory beacons enabled local workers to find small occluded objects with significantly stronger spatial perception. We also found that while the spatial auditory cues could indicate the spatial layout and a general direction to search for the target object, visual head frustum and hand gestures intuitively demonstrated the remote expert's movements and the position of the target. Integrating visual cues (especially the head frustum) with the spatial auditory cues significantly improved the local worker's task performance, social presence, and spatial perception of the environment.																	1783-7677	1783-8738				DEC	2020	14	4			SI		337	352		10.1007/s12193-020-00331-1		JUN 2020											
J								Arabic handwriting recognition system using convolutional neural network	NEURAL COMPUTING & APPLICATIONS										Convolutional neural network; Arabic character recognition; Hijja Dataset; Machine learning		Automatic handwriting recognition is an important component for many applications in various fields. It is a challenging problem that has received a lot of attention in the past three decades. Research has focused on the recognition of Latin languages' handwriting. Fewer studies have been done for the Arabic language. In this paper, we present a new dataset of Arabic letters written exclusively by children aged 7-12 which we call Hijja. Our dataset contains 47,434 characters written by 591 participants. In addition, we propose an automatic handwriting recognition model based on convolutional neural networks (CNN). We train our model on Hijja, as well as the Arabic Handwritten Character Dataset (AHCD) dataset. Results show that our model's performance is promising, achieving accuracies of 97% and 88% on the AHCD dataset and the Hijja dataset, respectively, outperforming other models in the literature.																	0941-0643	1433-3058															10.1007/s00521-020-05070-8		JUN 2020											
J								High-dimensional cluster boundary detection using directed Markov tree	PATTERN ANALYSIS AND APPLICATIONS										Cluster boundary; High-dimensional space; Directed Markov tree; Fine-grained	SEGMENTATION	Hypersurface of an inscribed geometry decides the distribution of an embedded cluster, in which its boundary points approximately fit this surface. To detect these points, capturing the implicit features of a local space is used to distinguish whether the data is an inner or outer feature. However, this approximation on the boundary is coarse-grained and may be ineffective in a high-dimensional space due to unbalanced feature distribution. In this paper, we introduce a directed Markov tree in high-dimensional cluster boundary detection. The key idea is to project each one-dimensional subspace of a local high-dimensional feature space into a layer of a directed Markov tree, covering absorptive and reflective walls. We then derive a fine-grained detection coefficient against on the Markov process of knight's tour over each layer of the tree. In this fine-grained view, the local feature space centered with a cluster boundary point has lower estimate on the tour cost than the internal data of the cluster. Based on this observation, we propose a knight algorithm to detect the boundary points of a high-dimensional feature space. Experiments on gene expression and video retrieval datasets demonstrate that the proposed algorithm can achieve a higher F-measure score than the other boundary detection baselines.																	1433-7541	1433-755X															10.1007/s10044-020-00897-2		JUN 2020											
J								An analysis of technological frameworks for data streams	PROGRESS IN ARTIFICIAL INTELLIGENCE										Data streaming; Big data; Big data frameworks; Technological frameworks; Data stream engines		Real-time data analysis is becoming increasingly important in Big Data environments for addressing data stream issues. To this end, several technological frameworks have been developed, both open-source and proprietary, for the analysis of streaming data. This paper analyzes some open-source technological frameworks available for data streams, detailing their main characteristics. The objective is to facilitate decisions on which framework to use, meeting the needs of data mining methods for data streams. In this sense, there are important factors affecting the choice about which framework is most suitable for this purpose. Some of these factors are the existence of data mining libraries, the available documentation, the maturity of the platform, fault tolerance and processing guarantees, among others. Another decisive factor when choosing a data stream framework is its performance. For this reason, two comparisons have been made: a performance and latency comparison between Spark Streaming, Spark Structured Streaming, Storm, Flink and Samza following the Yahoo Streaming Benchmark methodology, and a comparison between Spark Streaming and Flink with a clustering algorithm for data streaming called streaming K-means.																	2192-6352	2192-6360				SEP	2020	9	3					239	261		10.1007/s13748-020-00210-6		JUN 2020											
J								Three-way clustering around latent variables approach with constraints on the configurations to facilitate interpretation	JOURNAL OF CHEMOMETRICS										CLV3W; consumer segmentation; metabolomics; nonnegativity; Parafac	SIMULTANEOUS COMPONENT; SPECTROMETRY DATA; DATA MATRICES; LOCAL OPTIMA; MODELS; HETEROGENEITY; STRATEGY; SUBJECT	The set-up of comprehensive studies in life sciences involving a longitudinal dimension-as appears in time-scale metabolomics-calls for the use of dimension reduction techniques for three-way data structures (e.g., samples by variables by time points). For this purpose, a clustering around latent variables for three-way data approach,CLV3W, has been proposed.CLV3Waims at both partitioning the variables into nonoverlapping clusters and estimating within each cluster a rank-one Parafac model consisting of a latent component (resp. a weighting system) associated with the first mode (resp. third mode) and a vector of loadings reflecting the degree of closeness of each variable of the second mode to its cluster. In this paper, two constrainedCLV3Wmodels are discussed. First, a nonnegativity constraint is defined implying that clusters are composed of positively correlated variables. Second, it is proposed to constrain the weighting system to be the same for all clusters. These two constraints aim at providing more parsimonious models with configurations that are easier to interpret. The appropriateness of both constraints is evaluated in a simulation study and illustrated on two case studies pertaining to sensory evaluation and metabolomics data. Regarding the first case study,CLV3Wyields the identification of two consumer segments together with one common emotional pleasantness dimension associated with coffee aromas.CLV3Wanalysis of human preterm breast milk metabolomics data provided three clusters of lipid species that are responsible for specific functions (i.e., milk fat globules membrane-constituents, fatty acid oxidation-products, lipid mediators as eicosanoids and endocannabinoids).																	0886-9383	1099-128X														e3269	10.1002/cem.3269		JUN 2020											
J								Cotangent similarity measure of single-valued neutrosophic interval sets with confidence level for risk-grade evaluation of prostate cancer	SOFT COMPUTING										Single-valued neutrosophic interval set; Cotangent similarity measure; Confidence level; Prostate cancer; Risk-grade evaluation	INTUITIONISTIC FUZZY-SETS; MEDICAL DIAGNOSIS; DECISION-MAKING	The indeterminacy/inconsistency information of physicians' confident degrees regarding their judgments is not considered in existing risk-grade evaluation methods of prostate cancer (PC). To overcome the insufficiency, based on the single-valued neutrosophic interval sets (SvNISs) expressing the hybrid information of both the uncertain judgment given by an interval number and the confident degree regarding the uncertain judgment expressed by a single-valued neutrosophic number, this original study contributes a cotangent similarity measure of SvNISs with confidence level, and a novel risk-grade evaluation method of PC by using the confidence level-based cotangent similarity measure. Then, 16 PC actual clinical cases are used to demonstrate the applicability and effectiveness of the developed risk-grade evaluation method in SvNIS setting. Finally, the comparison analysis with other existing evaluation methods and the sensitivity analysis of confidence levels show that the proposed risk-grade evaluation method of PC is reasonable and effective.																	1432-7643	1433-7479															10.1007/s00500-020-05089-y		JUN 2020											
J								Group sparse additive machine with average top-k loss	NEUROCOMPUTING										Average top-k loss; Additive models; Generalization error; Data dependent hypothesis space; Robustness	COEFFICIENT; REGRESSION; SELECTION; RISK; CLASSIFICATION; CONSISTENCY; CLASSIFIERS; RATES	Sparse additive models have shown competitive performance for high-dimensional variable selection and prediction due to their representation flexibility and interpretability. Despite their theoretical properties have been studied extensively, few works have addressed the robustness for the sparse additive models. In this paper, we employ the robust average top-k (AT(k)) loss as classification error measure and propose a new sparse algorithm, named AT(k) group sparse additive machine (AT(k)-GSAM). Besides the robust concern, the AT(k)-GSAM has well adaptivity by integrating the data dependent hypothesis space and group sparse regularizer together. Generalization error bound is established by the concentration estimate with empirical covering numbers. In particular, our error analysis shows that AT(k)-GSAM can achieve the learning rate O(n(-1/2)) under appropriate conditions. We further analyze the robustness of AT(k)-GSAM via a sample-weighted procedure interpretation, and the theoretical guarantees on grouped variable selection. Experimental evaluations on both simulated and benchmark datasets validate the effectiveness and robustness of the new algorithm. (C) 2020 Published by Elsevier B.V.																	0925-2312	1872-8286				JUN 28	2020	395						1	14		10.1016/j.neucom.2020.01.104													
J								A variational Bayesian approach for robust identification of linear parameter varying systems using mixture laplace distributions	NEUROCOMPUTING										Linear parameter-varying systems; Robust identification; Laplace distribution; Variational inference	NONLINEAR PROCESS IDENTIFICATION; MODEL LPV APPROACH; MAXIMUM-LIKELIHOOD	The robust identification problem of the linear parameter varying (LPV) systems with output data corrupted by outliers is considered in this paper. The local identification approach is used, and the LPV model is obtained by interpolating the local models with an exponential weighting function. In order to handle outliers that could occur in industrial processes, the corresponding probabilistic model is established with the process noise assumed to be mixture Laplace distributed, then the formulas to iteratively update the unknown model parameters and noise-free output are derived under the Variational Bayesian (VB) framework, which approximates the required posteriors and could avoid high dimensional integrals. One numerical example and a practical chemical process are employed to verify the efficacy of the developed algorithm. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 28	2020	395						15	23		10.1016/j.neucom.2020.01.088													
J								Sampled-time containment control of high-order continuous-time MASs under heterogenuous time-varying delays and switching topologies: A scrambling matrix approach	NEUROCOMPUTING										Multi-agent systems; Distributed containment control; Synchronous communications; Time-varying delays; High-order continuous-time dynamics; Switching topologies; Scrambling matrices	2ND-ORDER MULTIAGENT SYSTEMS; SUFFICIENT CONDITIONS; ASYNCHRONOUS CONSENSUS; DYNAMIC LEADERS; AGENTS	This paper studies the containment control problem for high-order continuous-time multi-agent systems (MASs) under switching topologies and time-varying delays. We propose a distributed sampled-time containment protocol in which the restriction of choosing weights of the communication links from a finite set is eliminated. Under such a protocol, the states of the followers will asymptotically converge to the convex hull spanned by those of the leaders. Based on the properties of scrambling matrices, necessary and sufficient containment criteria are obtained for the network of agents with arbitrary bounded communication delays and switching interaction topologies. The proposed condition does not need the communication graph has a united spanning tree at each time instant and only requires that the union of communication graphs has a united spanning tree over a bounded period of time. In addition, the appropriate feedback gains are computed via given mathematical relations. Some numerical examples are given to demonstrate the effectiveness of the new protocols. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 28	2020	395						24	38		10.1016/j.neucom.2020.02.031													
J								More general results of aperiodically intermittent synchronization for stochastic Markovian switching complex networks with multi-links and time-varying coupling structure	NEUROCOMPUTING										Synchronization; Halanay-type inequality; Multi-links; Time-varying coupling; Aperiodically intermittent control; Chua's circuits	CHUAS CIRCUIT; STABILITY; SYSTEMS; DELAYS	The paper is concerned with the pth moment exponential synchronization for stochastic Markovian switching complex networks with multi-links via aperiodically intermittent control. Different from the current literature on multi-links systems, the networks we consider have time-varying topology structure. New differential inequalities are established in which the Markovian switching and multiple time-varying delays are taken into account and the constraint on parameter that determines exponential convergence rate is reduced. Based on Lyapunov method, graph theory and differential inequality techniques, some sufficient conditions are obtained to achieve exponential synchronization. As a result, the developed synchronization criteria have more accurate exponential convergence rate than that in most existing literature which focuses on delayed systems with intermittent control. Finally, an application to modified Chua's circuits and corresponding numerical simulations are presented to demonstrate the effectiveness of the proposed theoretical results. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 28	2020	395						39	55		10.1016/j.neucom.2020.02.026													
J								Adaptive dynamic programming based robust control of nonlinear systems with unmatched uncertainties	NEUROCOMPUTING										Adaptive dynamic programming; Robust control; Neural network; Adaptive control; Uncertain systems	STABILIZATION; GAMES	This paper proposes a new approach to address robust control design for nonlinear continuous-time systems with unmatched uncertainties. First, we transform the robust control problem into an equivalent optimal control problem, which allows to simplify the control design. A critic neural network (NN) is then adopted to reformulate the derived Hamilton-Jacobi-Bellman (HJB) equation based on the optimal control methodology. Then, a novel adaptation algorithm is used to online directly estimate the unknown NN weights, so as to achieve the guaranteed convergence. The control system stability and the convergence of the derived control action to the optimal solution can be rigorously proved. Finally, two simulation examples are provided to illustrate the validity and efficacy of the proposed method. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 28	2020	395						56	65		10.1016/j.neucom.2020.02.025													
J								ReMemNN: A novel memory neural network for powerful interaction in aspect-based sentiment analysis	NEUROCOMPUTING										Aspect-based sentiment analysis; Natural language processing; Attention mechanism; Deep learning	CLASSIFICATION	Deep neural networks have been employed to analyze the sentiment of text sequences and achieved significant effect. However, these models still face the issues of weakness of pre-trained word embeddings and weak interaction between the specific aspect and the context in attention mechanism. The pre-trained word embeddings lack the specific semantic information from the context. The weak interaction results in poor attention weights and produces limited aspect dependent sentiment representation in aspect-based sentiment analysis (ABSA). In this paper, we propose a novel end-to-end memory neural network, termed Recurrent Memory Neural Network (ReMemNN), to mitigate the above-mentioned problems. In ReMemNN, to tackle weakness of pre-trained word embeddings, a specially module named embedding adjustment learning module is designed to transfer the pre-trained word embeddings into adjustment word embeddings. To tackle weak interaction in attention mechanism, a multielement attention mechanism is designed to generate powerful attention weights and more precise aspect dependent sentiment representation. Besides, an explicit memory module is designed to store these different representations and generate hidden states and representations. Extensive experimental results on all datasets show that ReMemNN outperforms typical baselines and achieve the state-of-the-art performance. Besides, these experimental results also demonstrate that ReMemNN is language-independent and dataset type-independent. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 28	2020	395						66	77		10.1016/j.neucom.2020.02.018													
J								Distributed bipartite tracking consensus of nonlinear multi-agent systems with quantized communication	NEUROCOMPUTING										Bipartite tracking consensus; Multi-agent systems; Quantization; Adaptive control	LEADER-FOLLOWING CONSENSUS; TIME-DELAY SYSTEMS; PERFORMANCE; SYNCHRONIZATION; FLOCKING; NETWORKS; AGENTS	This paper mainly tends to address distributed bipartite tracking consensus problem for nonlinear multi-agent systems subject to logarithmic quantization. Based on appropriate quantized criterion, a distributed protocol with static coupling gain is developed. By employing Lyapunov technique and other mathematical analysis, the results show that distributed bipartite tracking consensus of multi-agent systems can be ensured with quantized relative state measurements, if some conditions are met. Additionally, compared with distributed protocol, the fully distributed protocol with dynamic coupling gain is further designed and analyzed via adaptive control, which is independent of any global information. Finally, simulations are exploited to support our theoretical analysis. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 28	2020	395						78	85		10.1016/j.neucom.2020.02.017													
J								Siamese denoising autoencoders for joints trajectories reconstruction and robust gait recognition	NEUROCOMPUTING										Gait recognition; Siamese denoising autoencoder; Joints trajectories reconstruction; Autoencoder with LSTM; Skeleton-based gait recognition	FRAMEWORK; DATABASE; IMAGE	Dynamics of body skeletons convey significant information for human gait recognition. However, it is inevitable that missing points, overlapping error, or confusion of left and right error will frequently occur during the process of skeleton estimation. Existing skeleton-based methods have difficulty in achieving satisfactory performance in gait recognition since they treat the noisy data and the normal data equally to the recognition process. In this paper, we propose a novel skeleton-based model called Siamese Denoising Autoencoder networks (Siamese DAE), which can automatically learn to remove position noise, recover missing skeleton points and correct outliers in joint trajectories. More precisely, we construct an encoder that compresses the characteristics of input trajectories into a latent space and a decoder that attempts to reconstruct more accurate skeleton trajectories from the latent feature. The corrected joint trajectories not only lead to higher discriminative power but also stronger generalization capability. Moreover, we design a Siamese structure to reduce intra-class variations and increase inter-class variations of the encoded features. Experiments demonstrate that our method enhances the robustness against inaccurate skeleton estimation and achieves substantial improvements over mainstream skeleton-based methods for gait recognition. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 28	2020	395						86	94		10.1016/j.neucom.2020.01.098													
J								Feature selection with missing labels based on label compression and local feature correlation	NEUROCOMPUTING										Multi-label learning; Incomplete labels; Feature selection; Label compression; Local feature correlation	MULTILABEL CLASSIFICATION; RELEVANCE	Feature selection can efficiently alleviate the issue of curse of dimensionality, especially for multi-label data with multiple features to embody diverse semantics. Although many supervised feature selection methods have been proposed, they generally assume the labels of training data are complete, whilst we only have data with incomplete labels in many real applications. Some methods try to select features with missing labels of training data, they still can not handle feature selection with a large and sparse label space. In addition, these approaches focus on global feature correlations, but some feature correlations are local and shared by a subset of data. In this paper, we introduce an approach called Feature Selection with missing labels based on Label Compression and Local feature Correlation (FSLCLC for short). FSLCLC adopts the low-rank matrix factorization on the sparse sample-label association matrix to compress labels and recover the missing labels in the compressed label space. In addition, it utilizes sparsity regularization and local feature correlation induced manifold regularizations to select the discriminative features. To solve the joint optimization objective for label compression, recovering missing labels and feature selection, we develop an iterative algorithm with guaranteed convergence. Experimental results on benchmark datasets show that the proposed FSLCLC outperforms the state-of-the-art multi-label feature selection algorithms. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 28	2020	395						95	106		10.1016/j.neucom.2019.12.059													
J								Robust formation tracking and collision avoidance for uncertain nonlinear multi-agent systems subjected to heterogeneous communication delays	NEUROCOMPUTING										Robust formation tracking; Collision avoidance; Heterogeneous communication delays; Neural networks	NETWORKED MOBILE ROBOTS; DISTRIBUTED FORMATION; COOPERATIVE TRACKING; SLIDING MODE; CONSENSUS	In this paper, formation control problem for multiple uncertain nonlinear second-order agents in the presence of heterogeneous communication delays is addressed. A continuous repulsive vector is incorporated into agents' velocity to ensure collision avoidance, whose time derivative is approximated via a finite-time robust integral of the sign of the error (RISE)-like observer. Using adaptive neural control scheme, unknown model dynamics, external disturbances, RISE estimation error, and the time-delays among agents are robustly addressed. The sufficient conditions on the stability of the overall system and collision avoidance are derived using Lyapunov-Krasovskii functionals and algebraic graph theory, and it is proven that the formation tracking error converges to a small neighborhood of zero. Numerical simulation results are also given to illustrate the effectiveness of our proposed methods. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 28	2020	395						107	116		10.1016/j.neucom.2020.02.032													
J								Relative coordinates constraint for face alignment	NEUROCOMPUTING										Face alignment; Relative coordinates constraint; CNN; Loss function design		We present a practical approach to improve the precision of face alignment for a single image. Recently, face alignment is deemed as a regression problem, and convolutional neural networks (CNNs) or recurrent neural networks (RNNs) are utilized to predict the coordinates of facial landmarks. However, most existing methods only adopt Euclidean loss as the optimization target for each landmark, and neglect the correlations between them, which we think may be inappropriate. To address this issue, in this paper, we introduce a novel Relative Coordinates Constraint (RCC) loss function for face alignment, which considers the relative coordinates between any pairs of landmarks as a new supervision signal. More importantly, we prove that the proposed RCC loss function is trainable and can be easily incorporated in existing CNNs optimization procedure. With the joint supervision of Euclidean loss and RCC loss, we train a robust and light CNNs framework for face alignment. Extensive experimental results on several datasets show that the precision of face alignment improved significantly by the proposed RCC loss and quantitative results are comparable to state-of-the-art methods (mean error 5.39 on 300-W and 6.99 on AFLW). In addition, the proposed framework is also an efficient solution (300 FPS on CPU). We share the implementation code of our proposed methods at https://github.com/nianfudong/RCC-loss. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 28	2020	395						119	127		10.1016/j.neucom.2017.12.071													
J								A fast face detection method via convolutional neural network	NEUROCOMPUTING										Fast face detection; Convolutional neural network; Discriminative complete feature maps	FEATURES; CASCADE	Current face or object detection methods via convolutional neural network (such as OverFeat, R-CNN and DenseNet) explicitly extract multi-scale features based on an image pyramid. However, such a strategy increases the computational burden for face detection. In this paper, we propose a fast face detection method based on discriminative complete features (DCFs) extracted by an elaborately designed convolutional neural network, where face detection is directly performed on the complete feature maps. DCFs have shown the ability of scale invariance, which is beneficial for face detection with high speed and promising performance. Therefore, extracting multi-scale features on an image pyramid employed in the conventional methods is not required in the proposed method, which can greatly improve its efficiency for face detection. Experimental results on several popular face detection datasets show the efficiency and the effectiveness of the proposed method for face detection. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 28	2020	395						128	137		10.1016/j.neucom.2018.02.110													
J								Pose guided structured region ensemble network for cascaded hand pose estimation	NEUROCOMPUTING										Hand pose estimation; Convolutional neural network; Human computer interaction; Depth images		Hand pose estimation from single depth images is an essential topic in computer vision and human computer interaction. Despite recent advancements in this area promoted by convolutional neural networks, accurate hand pose estimation is still a challenging problem. In this paper we propose a novel approach named as pose guided structured region ensemble network (Pose-REN) to boost the performance of hand pose estimation. Under the guidance of an initially estimated pose, the proposed method extracts regions from the feature maps of convolutional neural network and generates more optimal and representative features for hand pose estimation. The extracted feature regions are then integrated hierarchically according to the topology of hand joints by tree-structured fully connections to regress the refined hand pose. The final hand pose is obtained by an iterative cascaded method. Comprehensive experiments on public hand pose datasets demonstrate that our proposed method outperforms state-of-the-art algorithms. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 28	2020	395						138	149		10.1016/j.neucom.2018.06.097													
J								Attribute hierarchy based multi-task learning for fine-grained image classification	NEUROCOMPUTING										Attribute hierarchy; Multiple granularities; Multi-task learning; Fine-grained image classification	SALIENCY	Fine-grained image classification aims to distinguish subcategories belonging to the same basic-level category, such as 200 subcategories belonging to bird. It is a challenging problem in computer vision and multimedia field due to: attribute similarity (e.g. color and texture) among different subcategories and attribute variance (e.g. pose and viewpoint) in the same subcategory. Attribute similarity causes the difficulty to classify different subcategories even for human, while attribute variance causes the learned feature representations chaotic and confused. Naturally, classification can benefit from a hierarchy of subcategories: since going to a coarser granularity leverages high-level semantic features, while going to a finer granularity leverages discriminative and subtle features. Therefore, we propose an attribute hierarchy based multi-task learning (AHMTL) approach, and its main novelties are: (1) Attribute hierarchy: We reassign all images to multi-granularity subcategories automatically, namely coarse-grained, fine-grained and ultra-fine-grained subcategories. Similar fine-grained subcategories are reassigned to the same coarse-grained subcategory according to their attribute similarity, which pays more attention to the high-level semantic feature representations. Simultaneously, the same fine-grained subcategory but with different attributes are divided into different ultra-fine-grained subcategories, which can obtain more discriminative and subtle feature representations for attribute variance. (2) Multi-task learning: A multi-task learning framework is designed to effectively learn robust feature representations by jointly optimizing coarse-grained, fine-grained and ultra-fine-grained image classification tasks. These three level tasks learn coarse to fine feature representations, meaning high-level semantic to subtle features, which can regularize and boost each other to prevent overfitting, and the mutual promotion of them ensures feature representations more discriminative. Compared with more than 10 state-of-the-art methods on two widely-used CUB-200-2011 and Cars-196 datasets, our AHMTL approach achieves the best classification performance. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 28	2020	395						150	159		10.1016/j.neucom.2018.02.109													
J								Discriminative multimodal embedding for event classification	NEUROCOMPUTING										Social media; Event classification; Multimodal embedding	IMAGE ANNOTATION	Most of existing multimodal event classification methods fuse the traditional hand-crafted features with some manually defined weights, which may be not suitable to the event classification task with large amounts of photos. Besides, the feature extraction and event classification model are always performed separately, which cannot capture the most useful features to describe the semantic concepts of complex events. To deal with these issues, we propose a novel discriminative multimodal embedding (DME) model for event classification in user generated photos by jointly learning the representation together with the classifier in a unified framework. In the proposed DME model, we can effectively resolve the multimodal, intra-class variation and inter-class confusion challenges by using the contrastive constraints on the multimodal event data. Extensive experimental results on two collected datasets demonstrate the effectiveness of the proposed DME model for event classification. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 28	2020	395						160	169		10.1016/j.neucom.2017.11.078													
J								Cascade region proposal and global context for deep object detection	NEUROCOMPUTING										Object detection; Cascade region proposal; Global context		Deep region-based object detector consists of a region proposal step and a deep object recognition step. In this paper, we make significant improvements on both of the two steps. For region proposal we propose a novel cascade structure which can effectively improve RPN proposal quality without incurring heavy extra computational cost. For object recognition we re-implement global context modeling with a few modifications and obtain a performance boost (4.2% mAP gain on the ILSVRC 2016 validation set). Besides, we apply the idea of pre-training extensively and show its importance in both steps. Together with common training and testing tricks, we improve Faster R-CNN baseline by a large margin. In particular, we obtain 87.9% mAP on the PASCAL VOC 2012 test set, 65.3% on the ILSVRC 2016 test set and 36.8% on the COCO test-std set. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 28	2020	395						170	177		10.1016/j.neucom.2017.12.070													
J								MCFF-CNN: Multiscale comprehensive feature fusion convolutional neural network for vehicle color recognition based on residual learning	NEUROCOMPUTING										Vehicle color recognition; Convolutional neural network; Residual learning; Video surveillance; Intelligent transportation system		Automatic vehicle color recognition is very important for video surveillance, especially for intelligent transportation system. Currently, some approaches have been proposed. However, it is still very difficult to recognize the vehicle color correctly in the complex traffic scenes with constantly changing illuminations. To solve this problem, we propose a new network structure - Multiscale Comprehensive Feature Fusion Convolutional Neural Network (MCFF-CNN) based on residual learning for color feature extraction. First, we use MCFF-CNN network to extract the deep color features of the vehicles. Then, we employ support vector machine (SVM) classifier to obtain the final color recognition results. Based on the proposed approach, we have built a system for robust vehicle color recognition in practical traffic scenes. Extensive experimental results show our solution is effective. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 28	2020	395						178	187		10.1016/j.neucom.2018.02.111													
J								E(2)BoWs: An end-to-end Bag-of-Words model via deep convolutional neural network for image retrieval	NEUROCOMPUTING										Large-scale image retrieval; Bag-of-Words Model; Deep convolutional neural network		Traditional Bag-of-Words (BoWs) model is commonly generated with many steps, including local feature extraction, codebook generation and feature quantization, etc. Those steps are relatively independent with each other and are hard to be jointly optimized. Moreover, the dependency on hand-crafted local feature makes BoWs model not effective in conveying high-level semantics. These issues largely hinder the performance of BoWs model in large-scale image applications. To conquer these issues, we propose an End-to-End BoWs (E(2)BoWs) model based on Deep Convolutional Neural Network (DCNN). Our model takes an image as input, then identifies and separates semantic objects in it, and finally outputs visual words with high semantic discriminative power. Specifically, our model firstly generates Semantic Feature Maps (SFMs) corresponding to different object categories through convolutional layers, then introduces Bag-of-Words Layers (BoWL) to generate visual words from each individual feature map. We also introduce a novel learning algorithm to reinforce the sparsity of the generated E(2)BoWs model, which further ensures the time and memory efficiency. We evaluate the proposed E(2)BoWs model on several image search datasets including MNIST, SVHN, CIFAR-10, CIFAR-100, MIRFLICKR-25K and NUS-WIDE. Experimental results show that our method achieves promising accuracy and efficiency compared with recent deep learning based retrieval works. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				JUN 28	2020	395						188	198		10.1016/j.neucom.2017.12.069													
J								Security topics related microblogs search based on deep convolutional neural networks	NEUROCOMPUTING										Microblog search; Deep convolutional neural networks; Deep learning; Ranking model; Similarity matching	SOCIAL MEDIA; RETRIEVAL; FRAMEWORK	Social network information search, especially for microblog search, has been one of the research hotspots in the domain of information search. For complexities of microblog data on arbitrary typing and semantic ambiguity, classical approaches cannot be directly adopted. In this paper, we propose a security topics related microblogs search model based on deep convolutional neural networks (DCNN-CSTRS) to search microblogs similar to a specific security topic contents. This method is trained to capture local semantic features of short microblog texts to filter security topics related contents from microblogs. A matching model based on deep convolution neural network is designed to rank the results by matching the extracted local features of queries and documents respectively through non-linear feature transformations of the convolution and pooling. The matching model ranks the pairs of query-document by similarities. Experimental results demonstrate that the proposed approach performs better compared with the state-of-the-art methods. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				JUN 28	2020	395						199	211		10.1016/j.neucom.2018.09.105													
J								Image captioning via semantic element embedding	NEUROCOMPUTING										Image captioning; Element embedding; CNN; LSTM		Image caption approaches that use the global Convolutional Neural Network (CNN) features are not able to represent and describe all the important elements in complex scenes. In this paper, we propose to enrich the semantic representations of images and update the language model by proposing semantic element embedding. For the semantic element discovery, an object detection module is used to predict regions of the image, and a captioning model, Long Short-Term Memory (LSTM), is employed to generate local descriptions for these regions. The predicted descriptions and categories are used to generate the semantic feature, which not only contains detailed information but also shares a word space with descriptions, and thus bridges the modality gap between visual images and semantic captions. We further integrate the CNN feature with the semantic feature into the proposed Element Embedding LSTM (EE-LSTM) model to predict a language description. Experiments on MS COCO datasets demonstrate that the proposed approach outperforms conventional caption methods and is flexible to combine with baseline models to achieve superior performance. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				JUN 28	2020	395						212	221		10.1016/j.neucom.2018.02.112													
J								Fused GRU with semantic-temporal attention for video captioning	NEUROCOMPUTING										Video captioning; GRU; Encoder-decoder; Attention mechanism	SELECTION	The encoder-decoder framework has been widely used for video captioning to achieve promising results, and various attention mechanisms are proposed to further improve the performance. While temporal attention determines where to look, semantic decides the context. However, the combination of semantic and temporal attention has never be exploited for video captioning. To tackle this issue, we propose an end-to-end pipeline named Fused GRU with Semantic-Temporal Attention (STA-FG), which can explicitly incorporate the high-level visual concepts to the generation of semantic-temporal attention for video captioning. The encoder network aims to extract visual features from the videos and predict their semantic concepts, while the decoder network is focusing on efficiently generating coherent sentences using both visual features and semantic concepts. Specifically, the decoder combines both visual and semantic representation, and incorporates a semantic and temporal attention mechanism in a fused GRU network to accurately learn the sentences for video captioning. We experimentally evaluate our approach on the two prevalent datasets MSVD and MSR-VTT, and the results show that our STA-FG achieves the currently best performance on both BLEU and METEOR. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 28	2020	395						222	228		10.1016/j.neucom.2018.06.096													
J								Visual concept conjunction learning with recurrent neural networks	NEUROCOMPUTING										Attribute learning; Concept conjunction; Visual relationship detection; Image retrieval		Learning the conjunction of multiple visual concepts shows practical significance in various real world applications (e.g. multi-attribute image retrieval and visual relationship detection). In this paper, we propose Concept Conjunction Recurrent Neural Network ((CRNN)-R-2) to tackle this problem. With our model, visual concepts involved in a conjunction are mapped into the hidden units and combined in a recurrent way to generate the representation of the concept conjunction, which is then used to compute a concept conjunction classifier as the output. We also present an order invariant version of the proposed method based on attention mechanism to learn the tasks without pre-defined concept order. To tackle concept conjunction learning from multiple semantic domains, we introduce a multiplicative framework to learn the joint representation. Experimental results on multi-attribute image retrieval and visual relationship detection show that our method achieves significantly better performance than other related methods on various datasets. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				JUN 28	2020	395						229	236		10.1016/j.neucom.2017.12.068													
J								Learning fashion compatibility across categories with deep multimodal neural networks	NEUROCOMPUTING										Fashion compatibility; Deep learning; Neural networks; Multimodal	SYNCHRONIZATION	Fashion compatibility is a subjective sense of human for relationships between fashion items, which is essential for fashion recommendation. Recently, it increasingly attracts more and more attentions and has become a very hot research topic. Learning fashion compatibility is a challenging task, since it needs to consider plenty of factors about fashion items, such as color, texture, style and functionality. Unlike low-level visual compatibility (e.g., color, texture), high-level semantic compatibility (e.g., style, functionality) cannot be handled purely based on fashion images. In this paper, we propose a novel multimodal framework to learn fashion compatibility, which simultaneously integrates both semantic and visual embeddings into a unified deep learning model. For semantic embeddings, a multilayered Long Short-Term Memory (LSTM) is employed for discriminative semantic representation learning, while a deep Convolutional Neural Network (CNN) is used for visual embeddings. A fusion module is then constructed to combine semantic and visual information of fashion items, which equivalently transforms semantic and visual spaces into a latent feature space. Furthermore, a new triplet ranking loss with compatible weights is introduced to measure fine-grained relationships between fashion items, which is more consistent with human feelings on fashion compatibility in reality. Extensive experiments conducted on Amazon fashion dataset demonstrate the effectiveness of the proposed method for learning fashion compatibility, which outperforms the state-of-the-art approaches. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 28	2020	395						237	246		10.1016/j.neucom.2018.06.098													
J								Style-adaptive photo aesthetic rating via convolutional neural networks and multi-task learning	NEUROCOMPUTING										Convolutional neural networks; Deep learning; Multi-task learning; Photo quality assessment; Support vector machines		Photo aesthetic rating aims at automatically and precisely evaluating the pictorial aesthetic score. Recently, great progresses has been achieved in this area due to the fabulous ability of convolutional neural networks (CNNs). Existing deep approaches try to train CNNs from a large set of photos with score annotations. However, the considerable cost in collecting score annotations limits the generalization of these approaches to other types of media. To combat this limitation, we propose a novel photo aesthetic rating architecture. Our method comprises three modules: a CNN based feature extractor, a style classifier, and a group of style-specific aesthetic prediction models. First, we propose to train a CNN in the standard binary aesthetic classification task and use it to extract aesthetic-aware features, because binary labels are easy to access. Afterwards, we use a support vector machine (SVM) to formulate the style classifier. Besides, we explored a multi-task learning (MTL) approach to jointly learn the style-specific rating models, which will further improve the generalization ability. Additionally, to address the imbalance in the distribution of styles and ratings, we adopted the strategy of data augmentation and selective sampling. Finally, we estimate the rating of an input photo by combining the estimates of the style-specific rating models according to the outputs of the style classifier. Experiments conducted on the AVA database show that the proposed method is considerably comparative to state-of-the-art approaches. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUN 28	2020	395						247	254		10.1016/j.neucom.2018.06.099													
J								Multi-source domain adaptation for image classification	MACHINE VISION AND APPLICATIONS										Transfer learning; Domain adaptation; Multi-source learning; Maximum mean discrepancy; Sample reweighting	KERNEL; REGULARIZATION; FRAMEWORK	In recent years, domain adaptation and transfer learning are known as promising techniques with admirable performance to deal with problems with distribution difference between the training (source domain) and test (target domain) data. In this paper, a novel unsupervised multi-source transductive transfer learning approach, referred to as multi-source domain adaptation for image classification (MDA), is proposed, to transfer knowledge across the selected samples of multiple-source domains and samples of target domain into a shared low-dimensional subspace with maximum decision regions. MDA extends maximum mean discrepancy criteria across multiple-source domains to find an optimal projection subspace and constructs embedded condensed domain-invariant clusters. Furthermore, MDA minimizes empirical risk and maximizes the rate of consistency between manifold and prediction function via learning an optimal classification. Extensive evaluations on two types of visual benchmark datasets under different difficulties illustrate that MDA significantly outperforms other baseline and state-of-the-art methods in both multiple- and single-source tasks. Our source code is available at https://github.com/jtahmores/MDA.																	0932-8092	1432-1769				JUN 27	2020	31	6							44	10.1007/s00138-020-01093-2													
J								A Nonlocal Laplacian-Based Model for Bituminous Surfacing Crack Recovery and its MPI Implementation	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Gamma-convergence; Viscosity solutions; Nonlocal second-order operators; Message passing interface (MPI) parallelization	TOTAL VARIATION MINIMIZATION; SOBOLEV SPACES; TOPOLOGICAL GRADIENT; IMAGE DECOMPOSITION; PARAMETER SELECTION; SEGMENTATION; RESTORATION; PATTERNS	This paper is devoted to the challenging problem of fine structure detection with applications to bituminous surfacing crack recovery. Drogoul (SIAM J Imag Sci 7(4):2700-2731, 2014) shows that such structures can be suitablymodeled by a sequence of smooth functions whose Hessian matrices blow up in the perpendicular direction to the crack, while their gradient is null. This observation serves as the basis of the introduced model that also handles the natural dense and highly oscillatory texture exhibited by the images: We propose weighting vertical bar partial derivative(2)u/partial derivative x(1)(2)vertical bar(2) +vertical bar partial derivative(2)u/partial derivative x(2)(2)vertical bar(2) , u denoting the reconstructed image, by a variable that annihilates great expansion of this quantity, making then a connection with the elliptic approximation of the Blake-Zisserman functional. Extending then the ideas developed in the case of first-order nonlocal regularization to higher-order derivatives, we derive and analyze a nonlocal version of the model, and provide several theoretical results among which there are a Gamma-convergence result as well as a detailed algorithmic approach and an MPI implementation based on a natural domain decomposition approach.																	0924-9907	1573-7683				JUL	2020	62	6-7			SI		1007	1033		10.1007/s10851-020-00968-3		JUN 2020											
J								Hairpin completions and reductions: semilinearity properties	NATURAL COMPUTING										DNA hairpin formation; Hairpin completions; Hairpin reductions; Semilinearity property	COMPUTATION; STRINGS	This paper is part of the investigation of some operations on words and languages with motivations coming from DNA biochemistry, namely three variants of hairpin completion and three variants of hairpin reduction. Since not all the hairpin completions or reductions of semilinear languages remain semilinear, we study sufficient conditions for semilinear languages to preserve their semilinearity property after applying the non-iterated hairpin completion or hairpin reduction. A similar approach is then applied to the iterated variants of these operations. Along these lines, we define the hairpin reduction root of a language and show that the hairpin reduction root of a semilinear language is not necessarily semilinear except the universal language. A few open problems are finally discussed.																	1567-7818	1572-9796															10.1007/s11047-020-09797-0		JUN 2020											
J								Consistent Discriminant Correlation Analysis	NEURAL PROCESSING LETTERS										Canonical correlation analysis; Dimensionality reduction; Discriminant information; Consistency information	CANONICAL CORRELATION-ANALYSIS; MULTIVIEW	Multi-view dimensionality reduction is an importan subject in multi-view learning. Canonical correlation analysis and its various improved forms can effectively solve this problem. But most of these algorithms do not fully consider the discriminant information and view consistency information contained in the data itself simultaneously. To solve this problem, a new multi-view dimensionality reduction algorithm, consistent discriminant correlation analysis, is proposed in this paper. The algorithm integrates the class information and the consistency information between views into the dimension reduction process. By maximizing the within-class correlations and the consistency between views, and minimizing the between-class correlations simultaneously, it extracts the low-dimensional features that are more efficient to classification. Furthermore, a kernel consistent discriminant correlation analysis is proposed. The experimental results on several data sets demonstrate the effectiveness of the proposed methods.																	1370-4621	1573-773X				AUG	2020	52	1			SI		891	904		10.1007/s11063-020-10285-w		JUN 2020											
J								Deep kernel learning in extreme learning machines	PATTERN ANALYSIS AND APPLICATIONS										Extreme learning machines; Deep kernel machines; Arc-cosine kernel; Deep kernel extreme learning machines	NETWORKS; MULTIPLE; CLASSIFICATION; APPROXIMATION	Emergence of extreme learning machine as a breakneck learning algorithm has marked its prominence in solitary hidden layer feed-forward networks. Kernel-based extreme learning machine (KELM) reflected its efficiency in diverse applications where feature mapping functions of hidden nodes are concealed from users. The conventional KELM algorithms involve only solitary layer of kernels, thereby emulating shallow learning architectures for its feature transformation. Trend in migrating shallow-based learning models into deep learning architectures opens up a new outlook for machine learning domains. This paper attempts to bestow deep kernel learning approach in a conventional shallow architecture. The emerging arc-cosine kernels possess the potential to mimic the prevailing deep layered frameworks to a greater extent. Unlike other kernels such as linear, polynomial and Gaussian, arc-cosine kernels have a recursive nature by itself and have the potential to express multilayer computation in learning models. This paper explores the possibility of building a new deep kernel machine with extreme learning machine and multilayer arc-cosine kernels. This framework outperforms conventional KELM and deep support vector machine in terms of training time and accuracy.																	1433-7541	1433-755X															10.1007/s10044-020-00891-8		JUN 2020											
J								Simultaneous identification of points and circles: structure from motion system in industry scenes	PATTERN ANALYSIS AND APPLICATIONS										SFM; Low-texture; Circles construction		To address the issue of dynamic 3D circular features recognition in robot arm grasping, we propose a feature-based and incremental simultaneous points and circles structure from motion system. First, we represent the 3D target scene with sparse point cloud from multiple observations. The fundamental points construction pipeline combines the benefits of feature-based mapping and probabilistic depth estimation, which reduce the computational cost of generating practical 3D structures. Second, accurate 3D circles are extracted from the keyframes and are optimized in the backend. The circles construction pipeline attempts to find potential circles in the produced sparse point cloud and apply an adjusted level set method to do a novel 3D circle optimization process, which can work on keyframes smoothly. The integrated system is compared with other real-time construction systems and outperforms in industry scenes with more stable land-marks. Meanwhile the experimental results illustrate the ability of capturing circular features in target scenes.																	1433-7541	1433-755X															10.1007/s10044-020-00889-2		JUN 2020											
J								Designing an interval type-2 fuzzy disturbance observer for a class of nonlinear systems based on modified particle swarm optimization	APPLIED INTELLIGENCE										Fuzzy disturbance observer; Interval type-2 fuzzy system; Modified particle swarm optimization; Ball and beam system	NEURAL-NETWORK; LOGIC SYSTEMS; ALGORITHM; COLONY; REDUCTION	This paper presents a new interval type-2 fuzzy disturbance observer design for a class of nonlinear systems using modified particle swarm optimization. The design procedure has two main parts, including the selection of the initial structure of the type-2 fuzzy disturbance observer, and the optimization of the observer parameters using a modified particle swarm optimization algorithm. The modified particle swarm optimization algorithm has a better performance in terms of the accuracy and convergence rate compared with the standard particle swarm optimization and many other evolutionary algorithms. In this algorithm, the upper and lower bounds of the search space are defined for the parameters of each particle based on their values, and weaker particles are substituted with new particles. To accentuate the outstanding performance of the modified particle swarm optimization for the considered task, its performance is compared with five famous meta-heuristic optimization algorithms. In addition, utilizing interval type-2 fuzzy systems in the proposed observer provides more robustness compared with type-1 fuzzy systems. The effectiveness of the proposed fuzzy disturbance observer is shown through computer simulation and experimental results for the ball and beam system, while the system is subjected to sinusoid and square disturbances, and a comparison is drawn to indicate the superiority of the proposed fuzzy disturbance observer over the other observers.																	0924-669X	1573-7497				NOV	2020	50	11					3731	3747		10.1007/s10489-020-01774-5		JUN 2020											
J								A multi-objective algorithm for multi-label filter feature selection problem	APPLIED INTELLIGENCE										Feature selection; Multi-objective optimization; Multi-label; PSO	PARTICLE SWARM OPTIMIZATION; FEATURE SUBSET-SELECTION; DIFFERENTIAL EVOLUTION; GENETIC ALGORITHM; MUTUAL INFORMATION; HYBRID APPROACH; CLASSIFICATION; PSO; MUTATION; SCORE	Feature selection is an important data preprocessing method before classification. Multi-objective optimization algorithms have been proved an effective way to solve feature selection problems. However, there are few studies on multi-objective optimization feature selection methods for multi-label data. In this paper, a multi-objective multi-label filter feature selection algorithm based on two particle swarms (MOMFS) is proposed. We use mutual information to measure the relevance between features and label sets, and the redundancy between features, which are taken as two objectives. In order to avoid Particle Swarm Optimization (PSO) from falling into the local optimum and obtaining a false Pareto front, we employ two swarms to optimize the two objectives separately and propose an improved hybrid topology based on particle's fitness value. Furthermore, an archive maintenance strategy is introduced to maintain the distribution of archive. In order to study the effectiveness of the proposed algorithm, we select five multi-label evaluation criteria and perform experiments on seven multi-label data sets. MOMFS is compared with classic single-objective multi-label feature selection algorithms, multi-objective filter and wrapper feature selection algorithms. The experimental results show that MOMFS can effectively reduce the multi-label data dimension and perform better than other approaches on five evaluation criteria.																	0924-669X	1573-7497				NOV	2020	50	11					3748	3774		10.1007/s10489-020-01785-2		JUN 2020											
J								Deep learning techniques for skin lesion analysis and melanoma cancer detection: a survey of state-of-the-art	ARTIFICIAL INTELLIGENCE REVIEW										Skin cancer; Skin lesion images; Machine learning; Deep learning; Survey; Pre-processing; Segmentation; Classification	DERMOSCOPY IMAGES; BORDER DETECTION; SEGMENTATION; CLASSIFICATION; DIAGNOSIS; FUSION; NEVUS	Analysis of skin lesion images via visual inspection and manual examination to diagnose skin cancer has always been cumbersome. This manual examination of skin lesions in order to detect melanoma can be time-consuming and tedious. With the advancement in technology and rapid increase in computational resources, various machine learning techniques and deep learning models have emerged for the analysis of medical images most especially the skin lesion images. The results of these models have been impressive, however analysis of skin lesion images with these techniques still experiences some challenges due to the unique and complex features of the skin lesion images. This work presents a comprehensive survey of techniques that have been used for detecting skin cancer from skin lesion images. The paper is aimed to provide an up-to-date survey that will assist investigators in developing efficient models that automatically and accurately detects melanoma from skin lesion images. The paper is presented in five folds: First, we identify the challenges in detecting melanoma from skin lesions. Second, we discuss the pre-processing and segmentation techniques of skin lesion images. Third, we make comparative analysis of the state-of-the-arts. Fourth we discuss classification techniques for classifying skin lesions into different classes of skin cancer. We finally explore and analyse the performance of the state-of-the-arts methods employed in popular skin lesion image analysis competitions and challenges of ISIC 2018 and 2019. Application of ensemble deep learning models on well pre-processed and segmented images results in better classification performance of the skin lesion images.																	0269-2821	1573-7462															10.1007/s10462-020-09865-y		JUN 2020											
J								Iterative residual tuning for system identification and sim-to-real robot learning	AUTONOMOUS ROBOTS										Sim-to-real transfer; System identification; Simulation; Physics prediction		Robots are increasingly learning complex skills in simulation, increasing the need for realistic simulation environments. Existing techniques for approximating real-world physics with a simulation require extensive observation data and/or thousands of simulation samples. This paper presents iterative residual tuning (IRT), a deep learning system identification technique that modifies a simulator's parameters to better match reality using minimal real-world observations. IRT learns to estimate the parameter difference between two parameterized models, allowing repeated iterations to converge on the true parameters similarly to gradient descent. In this paper, we develop and analyze IRT in depth, including its similarities and differences with gradient descent. Our IRT implementation, TuneNet, is pre-trained via supervised learning over an auto-generated simulated dataset. We show that TuneNet can perform rapid, efficient system identification even when the true parameter values lie well outside those in the network's training data, and can also learn real-world parameter values from visual data. We apply TuneNet to a sim-to-real task transfer experiment, allowing a robot to perform a dynamic manipulation task with a new object after a single observation.																	0929-5593	1573-7527				SEP	2020	44	7			SI		1167	1182		10.1007/s10514-020-09925-w		JUN 2020											
J								An algorithmic approach for finding the fuzzy constrained shortest paths in a fuzzy graph	COMPLEX & INTELLIGENT SYSTEMS										Fuzzy graph; SPP; CSPP; Fuzzy shortest path problem; Online cab booking system	ROBUST	Shortest path problem (SPP) is a fundamental and well-known combinatorial optimization problem in the area of graph theory. In real-life scenarios, the arc weighs in a shortest path of a network/graph have the several parameters which are very hard to define exactly (i.e., capacity, cost, demand, traffic frequency, time, etc.). We can incorporate the fuzziness into a graph to handle this type of uncertain situation. In this manuscript, we propose the idea of constrained SPP (CSPP) in fuzzy environment. CSPP has an useful real-life application in online cab booking system. The main motivation of this study is to determine a path with minimal cost where traveling time within two locations does not more than predetermined time. We can not predicate the exact time and cost of the path due to uncertain traffic scenarios and another unexpected reasons; still, the geometrical distance between the locations is fixed. Here, we use trapezoidal fuzzy number to describe the edge weight of a fuzzy network/graph for CSPP. We define this CSPP as fuzzy CSPP (FCSPP). The utility of FCSPP is described in several real-life scenarios. We propose a mathematical formulation for the FCSPP and an algorithm is proposed for solving the FCSPP. We describe an application of our proposed algorithm on an online cab booking system.																	2199-4536	2198-6053															10.1007/s40747-020-00143-6		JUN 2020											
J								Online planning for relative optimal and safe paths for USVs using a dual sampling domain reduction-based RRT* method	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Unmanned surface vehicle; Online path planning; Obstacle avoidance; Sampling space reduction; Rapidly-exploring random tree; Collision detection	UNMANNED SURFACE VEHICLE; ALGORITHM	A heuristic Dual sampling domain Reduction-based Optimal Rapidly-exploring Random Tree scheme is proposed by guiding the planning procedure of the optimal rapidly-exploring random tree (RRT*) method through learning environmental knowledge. The scheme aims to plan low fuel expenditure, easy-execution, and low collision probability paths online for an unmanned surface vehicle (USV) under constraints. First, an elliptic sampling domain, which is subject to an elliptic equation and the shortest obstacle avoidance path estimation, is created to plan short paths. Second, by the consideration of the USV motion states, obstacles and external interferences of the current, the near sampling domains of tree nodes are reduced to exclude high-cost sampling domains. Path feasibility is ensured by explicitly handling motion constraints. Third, a safe distance-based collision detection (CD) scheme and a velocity-based bounding box of USV are proposed to decrease the path collision probability. Additionally, a layered USV online path planning framework is built in accordance with the model predictive control method, and the path smoothing scheme is applied via the Dubins curve under the curvature constraint. Results demonstrate that the proposed dual sampling domain reduction method outperforms traditional reduction schemes in terms of improving the execution efficiency of RRT*. Meanwhile, the proposed CD method is more reliable than the conventional one.																	1868-8071	1868-808X				DEC	2020	11	12					2665	2687		10.1007/s13042-020-01144-0		JUN 2020											
J								Multiresolution analysis relying on Beta wavelet transform and multi-mother wavelet network for a novel 3D mesh alignment and deformation technique	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										3D high mesh deformation; Multiresolution analysis; Trust region spherical parameterization; Multi library wavelet neural network architecture		In this paper, we propose a new 3D high mesh deformation technique to extract intuitive and interpretable deformation and alignment components. Our framework is based on a fast Beta wavelet transform for a multi-resolution analysis relying on multi-library wavelet neural network architecture. The main drawback of 3D high mesh deformation is the large number of triangles necessary to characterize a smooth surface; the majority of these techniques impose a very high computational cost. Our approach is based on the idea of combining the decomposition technique of multi-resolution analysis by Beta wavelet transform for each level of deformation process and a multi-mother wavelet network structure to construct an effective 3D alignment algorithm. We use, in our experiment, only the approximation coefficients at a chosen decomposition level to reduce the complexity of the mesh and to facilitate the alignment until reaching the target mesh, for the purpose of improving various executions and obtaining an optimal solution while reducing the error between the original and the reconstructed object to create a well-formed object. Then, to enhance the performance of wavelet networks, a novel learning algorithm based on multi-mother wavelet neural network architecture using trust region spherical is employed as an approximation tool for feature alignment between the source and the target models. This network architecture ensures the use of several mother wavelets to solve the problem of high mesh deformation utilizing the best wavelet mother that well models the object. Extensive experimental results demonstrate that the progressive deformation processes aim at avoiding the weaknesses of traditional approaches such as the slowness and the difficulty of finding an exact reconstruction.																	1868-8071	1868-808X				DEC	2020	11	12					2703	2717		10.1007/s13042-020-01146-y		JUN 2020											
J								Interval-valued intuitionistic fuzzy parameterized interval-valued intuitionistic fuzzy soft sets and their application in decision-making	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Soft sets; Interval-valued intuitionistic fuzzy sets; d-sets; Soft decision-making	MEDIAN FILTER; SALT; OPERATORS	Although some statistical tools, such as mean and median, used for modelling a problem containing parameters or alternatives with multiple intuitionistic fuzzy values because these values are obtained in a specific period, decrease uncertainty, they lead to data loss. However, interval-valued intuitionistic fuzzy values can overcome such a concern. For this reason, the present study proposes the concept of interval-valued intuitionistic fuzzy parameterized interval-valued intuitionistic fuzzy soft sets (d-sets) and presents several of its basic properties. Moreover, by usingd-sets, we suggest a new soft decision-making method and apply it to a problem concerning the eligibility of candidates for two vacant positions in an online job advertisement. Since it is the first method proposed in relation to this structure (d-sets), it is impossible to compare this method with another in this sense. To deal with this difficulty, we introduce four new concepts, i.e. mean reduction, mean bireduction, mean bireduction-reduction, and mean reduction-bireduction. By using these concepts, we apply four state-of-the-art soft decision-making methods to the problem. We then compare the ranking performances of the proposed method with those of the four methods. Besides, we apply five methods to a real problem concerning performance-based value assignment to some filters used in image denoising and compare the ranking performances of these methods. Finally, we discussd-sets and the proposed method for further research.																	1868-5137	1868-5145															10.1007/s12652-020-02227-0		JUN 2020											
J								An energy saving medium access control protocol for wireless sensor networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cluster head; Schedule; Energy consumption; Control slot; Data slot	MAC PROTOCOL; CLOCK SYNCHRONIZATION; DUTY-CYCLE; EFFICIENT; DELAY; SCHEME; LIFETIME	Medium access control (MAC) protocols for wireless sensor networks (WSNs), consider the utilization of batteries of individual sensor nodes to extend the overall network lifetime. In this paper, an energy saving medium access control (ES-MAC) protocol is proposed. ES-MAC allows a cluster head to wake for a short duration at the beginning of each slot in a session/frame. If the cluster head doesn't hear anything from the node assigned for the slot, then it goes into sleep state (radio is turned OFF) till the beginning of the next slot. But if the cluster head receives any data from the corresponding node, it continues to listen till the end of that slot. Unlike some existing event-driven MAC protocols namely BMA-MAC, BEE-MAC, and LDC-MAC, the proposed protocol does not include any contention period/control period within a frame. This saves the energy which would be consumed otherwise, in sending and receiving control packets during this period. The simulation results show that ES-MAC reduces the energy consumption compared to BMA-MAC, BEE-MAC, and LDC-MAC.																	1868-5137	1868-5145															10.1007/s12652-020-02214-5		JUN 2020											
J								A partial key pre-distribution based en-route filtering scheme for wireless sensor networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Combinatorial design; En-route filtering; Data authentication; Wireless sensor networks (WSNs)	INJECTED FALSE DATA; SECURITY	Compromised sensor nodes can be used to inject false reports (bogus reports) in wireless ssensor networks (WSNs). This can cause the sink to take wrong decisions. En-route filtering is a method to detect and filter false reports from WSNs. Most of the existing en-route filtering schemes use probabilistic approaches to filter false reports from the network, where filtering of false reports is based on a fixed probability. Thus false reports can travel multiple hops before being dropped. In this article we seek to overcome limitations of the existing schemes and reduce the overall key storage overhead in the cluster heads. In this article we propose a combinatorial design based partial en-route filtering scheme (CD-PEFS) which filters the fabricated reports deterministically. CD-PEFS reduces the energy requirements in the network by early detection and elimination of the false reports. Adoption of combinatorial design based keys get rid of shared key discovery phase from the network. This considerably reduces the communication overhead in the network. We carried out a detailed analysis of CD-PEFS against an increasing number of compromised sensor nodes in the network. We found that our scheme performs better than existing schemes in terms of filtering efficiency while maintaining low key storage overhead in the network. Further the performance of CD-PEFS is at par with existing schemes in terms of other protocol overheads.																	1868-5137	1868-5145															10.1007/s12652-020-02216-3		JUN 2020											
J								Adaptive supervised multi-resolution approach based modeling of performance improvement in satellite image classification	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Hyper spectral image; Root mean square; Correlation coefficient; Spectral error; Adaptive supervised multi-resolution	KERNEL SPARSE REPRESENTATION; SUBSPACE	Satellite image classification is a significant piece of utilizations in different fields, for example, horticulture, nature observing, and disaster management. This work is expected to improve the spatial and spectral information of satellite images by using higher request bits of knowledge in a blend with power shade l inundation utilizing Adaptive Supervised Multi-Resolution demonstrating classification approach. The proposed Adaptive Supervised Multi-Resolution based strategy that naturally orders the various regions from spatiotransient remote detecting pictures. Initially, a kernel representation has been planned by the structure of multispectral and temporal remote detecting information. Also, the Adaptive Supervised Multi-Resolution system with tweaked parameters has been proposed for preparing region tests and learning spatiotemporal discriminative representations. The following parameters are used to assess the execution of the proposed Adaptive Supervised Multi-Resolution: sensitivity, specificity, accuracy, and false classification ratio. To reduce the dimensionality of multi-band satellite images, the autonomous part assessment is used, which uses the higher-organize experiences of the data independently. The exhibition of the proposed technique was approved through simulation utilizing the MATLAB programming. Compared with ordinary satellite image classifier with a proposed Adaptive Supervised Multi-Resolution strategy based classifier execution by accomplishing the result of 97.695% in accuracy, 94.815% in sensitivity, and 97.75% in specificity.																	1868-5137	1868-5145															10.1007/s12652-020-02251-0		JUN 2020											
J								PAFF: predictive analytics on forest fire using compressed sensing based localized Ad Hoc wireless sensor networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Mobile node; Random trajectory path; Compressed sensing; Semi-supervised classification; Forest fire	TRACKING; SCHEME	Early detection of a forest fire can save our flora and fauna. Ad Hoc Wireless Sensor Networks (WSN) plays an important role in detecting forest fire. This article proposes a model for early detection of forest fire through predictive analytics. In this approach, the forest area is divided into different zones. Status of a zone, i.e., High Active (HA), Medium Active (MA), and Low Active (LA), is predicted by applying the semi-supervised classification technique. Each zone has static sensors, mobile sensors, and an Initiator node. Initiator nodes of LA and MA zone transfer their mobile nodes (MN) to the nearer HA zone for the quick prediction of forest fire by using the Random trajectory generation (RTG) technique. This technique generates the intermediate points between LA/MA to HA zone to create the movement path of MN. Compressed sensing based Gradient descent (GD) localization technique is used to track the movement of MN by the anchor nodes. This technique reduces the energy consumption of MN that causes an increase in network lifetime. The analysis of the localization error of MN during its traveling towards the HA zone increases the accuracy of its path detection. Thus the increase of sensor nodes in the HA zone results in transferring a huge amount of data from HA zone to base station for quick prediction of a forest fire.																	1868-5137	1868-5145															10.1007/s12652-020-02238-x		JUN 2020											
J								Parameterized Model Checking on the TSO Weak Memory Model	JOURNAL OF AUTOMATED REASONING										Parameterized model checking; MCMT; SMT; Weak memory; Partial order reduction		We present an extended version of the model checking modulo theories framework for verifying parameterized systems under the TSO weak memory model. Our extension relies on three main ingredients: (1) an axiomatic theory of the TSO memory model based on relations over (read, write) events, (2) a TSO-specific backward reachability algorithm and (3) an SMT solver for reasoning about TSO formulas. One of the main originality of our work is a partial order reduction technique that exploits specificities of the TSO memory model. We have implemented this framework in a new version of the Cubicle model checker called Cubicle-W. Our experiments show that Cubicle-W is expressive and efficient enough to automatically prove safety of concurrent algorithms, for an arbitrary number of processes, ranging from mutual exclusion to synchronization barriers translated from actual x86-TSO implementations.																	0168-7433	1573-0670				OCT	2020	64	7			SI		1307	1330		10.1007/s10817-020-09565-w		JUN 2020											
J								Skill transfer support model based on deep learning	JOURNAL OF INTELLIGENT MANUFACTURING										Deep learning; Convolutional neural network; Faster region-based convolutional neural network; Human-machine interaction; Skill transfer	HUMAN-ROBOT INTERACTION; RECOGNITION; SYSTEM; CLASSIFICATION; TECHNOLOGIES	The paradigm shift toward Industry 4.0 is not solely completed by enabling smart machines in a factory but also by facilitating human capability. Refinement of work processes and introduction of new training approaches are necessary to support efficient human skill development. This study proposes a new skill transfer support model in a manufacturing scenario. The proposed model develops two types of deep learning as the backbone: a convolutional neural network (CNN) for action recognition and a faster region-based CNN (R-CNN) for object detection. A case study using toy assembly is conducted utilizing two cameras with different angles to evaluate the performance of the proposed model. The accuracy for CNN and faster R-CNN for the target job reached 94.5% and 99%, respectively. A junior operator can be guided by the proposed model given that flexible assembly tasks have been constructed on the basis of a skill representation. In terms of theoretical contribution, this study integrated two deep learning models that can simultaneously recognize the action and detect the object. The present study facilitates skill transfer in manufacturing systems by adapting or learning new skills for junior operators.																	0956-5515	1572-8145															10.1007/s10845-020-01606-w		JUN 2020											
J								Connecting Knowledge to Data Through Transformations in KnowID: System Description	KUNSTLICHE INTELLIGENZ										Ontology-mediated data access; Data management; Conceptual modeling		Intelligent information systems deploy applied ontologies or logic-based conceptual data models for effective and efficient data management and to assist with decision-making. A core deliberation in the design of such systems, is how to link the knowledge to the data. We recently designed a novel knowledge-to-data architecture (KnowID) which aims to solve this critical step through a set of transformation rules rather than a mapping layer, which operate between models represented in EER notation and an enhanced relational model called the ARM. This system description zooms in on the novel tool for the core component of the transformation from the Artificial Intelligence-oriented modelling to the relational database-oriented data management. It provides an overview of the requirements, design, and implementation of the modular transformations module that straightforwardly permits extension with other components of the modular KnowID architecture.																	0933-1875	1610-1987				SEP	2020	34	3			SI		373	379		10.1007/s13218-020-00675-6		JUN 2020											
J								Application of artificial neural network model based on GIS in geological hazard zoning	NEURAL COMPUTING & APPLICATIONS										Geological hazard risk zoning; Geographic information system (GIS); Artificial neural network; Danger level probability	LANDSLIDE HAZARD; SEISMIC HAZARD; DISASTERS; BIOSPHERE; ZONATION; ENERGY; RISK	Under specific terrain and climatic conditions, it is extremely easy to cause various types of geological hazards, and the occurrence of geological hazards will affect people's production and life, with the consequence that the economic losses are extremely great, and even severely endanger human life. However, the existing geological hazard danger zoning is slightly insufficient in accuracy and operating efficiency, and the effect in practical application needs to be further improved. In view of the above problems, this paper proposes a study on the application of GIS-based artificial neural network models in the geological disaster risk zoning. This article first expounds the related concepts of geological hazards zoning and gives the principles to be followed. Then, the calculation of evaluation factors and risk level probabilities are proposed, and a hierarchical model is constructed using the analytic hierarchy process. The weight of each evaluation factor is multiplied with the information to obtain the weighted information. Calculate the probability based on the topographic features, stratigraphic lithology, and terrain slope. Combine the artificial neural network with BP neural network to predict the results. Through the simulation experiments on the geological data of the low-mountain and hilly areas in Wanli District, Nanchang, the results show that the proportion of high-probability-prone areas and medium-probability-prone areas predicted by this paper is as high as 91.87%, and the evaluation results are consistent with the actual disaster occurrence better. The area under the ROC curve was 87.3%, which also verified the effectiveness of the method in this paper.																	0941-0643	1433-3058															10.1007/s00521-020-04987-4		JUN 2020											
J								Performance evaluation of low resolution visual tracking for unmanned aerial vehicles	NEURAL COMPUTING & APPLICATIONS										UAV tracking; Low resolution dataset; Fusion algorithm; Image enhancement		Several datasets for unmanned aerial vehicle (UAV) visual tracking research have been released in recent years. Despite their usefulness, whether they are sufficient for understanding the strengths and weakness of different resolution videos tracking remains questionable. Tracking in low resolution videos is a critical problem in UAV tracking. To address this issue, we construct a group of low resolution tracking datasets and study the performance of different trackers on these datasets. We find that some trackers suffered more performance degradation than others, which brings to light a previously unexplored aspect of the tracking methods. The relative rank of these trackers based on their tracking results on the datasets may change in the presence of low resolution. Based on these findings, we develop a multiple feature tracking framework which takes advantage of image enhancement scheme to improve image quality. In addition, we utilize the forward and backward tracking to evaluate multiple feature tracking results. Experimental results demonstrate that our tracker is competitive in performance to state-of-the-art methods in different resolutions scenarios. We believe our studies can provide a solid baseline when conducting experiments for low resolution UAV tracking research.																	0941-0643	1433-3058															10.1007/s00521-020-05067-3		JUN 2020											
J								The uncertainty measures for covering rough set models	SOFT COMPUTING										Covering rough set; Uncertainty measure; Rough entropy; Granularity	INFORMATION-THEORETIC MEASURES; FEATURE-SELECTION; NEIGHBORHOOD OPERATORS; GRANULARITY MEASURES; ENTROPY; GRANULATION; REDUCTION	Uncertainty measures are important tools for analyzing various data. However, there are relatively few studies on the uncertainty measures for covering rough set models. In this paper, from the viewpoint of the lower and upper approximations, we propose new uncertainty measures, the lower rough entropy and the upper rough entropy, for covering rough set models. Then, we define the concepts of the joint entropy and the conditional entropy in the covering rough set models. Some important properties of these measures are obtained, and their relationships are investigated. Furthermore, we provide a certain characterization of reducible element of a covering by means of the proposed measures, and apply the proposed rough entropy to evaluate the significance of covering granules of a covering. Finally, we apply these rough entropies to measure a dual degree between covering lower and upper approximations. The theoretical analysis and examples show that the proposed uncertainty measures for covering rough set models are reasonable and useful.																	1432-7643	1433-7479				AUG	2020	24	16					11909	11929		10.1007/s00500-020-05098-x		JUN 2020											
J								Expected utility operators and coinsurance problem	SOFT COMPUTING										Expected utility operators; Coinsurance	POSSIBILISTIC RISK-AVERSION; MEAN-VALUE; INSURANCE	The expected utility operators introduced in a previous paper offer a framework for a general risk aversion theory, in which risk is modeled by a fuzzy numberA. In this paper, we formulate a coinsurance problem in the possibilistic setting defined by an expected utility operatorT. Some properties of the optimal savingT-coinsurance rate are proved, and an approximate calculation formula of this is established with respect to the Arrow-Pratt index of the utility function of the policyholder, as well as the expected value and the variance of a fuzzy numberA. Various formulas of the optimalT-coinsurance rate are deduced for a few expected utility operators in case of a triangular fuzzy number and of some HARA- and CRRA-type utility functions.																	1432-7643	1433-7479															10.1007/s00500-020-05100-6		JUN 2020											
J								An integrated probabilistic linguistic projection method forMCGDMbased onELECTRE IIIand the weighted convex median voting rule	EXPERT SYSTEMS										Bayesian best-worst method; multi-criteria group decision-making; probabilistic linguistic term sets; projection; the weighted convex median voting rule	GROUP DECISION-MAKING; TERM SETS; SELECTION; CRITERIA; SIMILARITY	In the multi-criteria group decision-making (MCGDM) problems with great uncertainty, making full use of participants' evaluation information could help improve the accuracy and reliability of decision results. Probabilistic linguistic term set (PLTS) is an effective tool to represent qualitative data and can fully express the hesitation and preference of decision makers. Therefore, this paper aims to propose an MCGDM method based on PLTSs. In the proposed method, the projection of PLTSs is explored to measure the distance and angle differences between two objects, and Bayesian best-worst method (Bayesian BWM) is used to determine the aggregated final weights of criteria. Besides, the elimination and choice translating reality III (ELECTRE III) method combined with distillation algorithm deals with the projection of PLTSs to obtain the alternatives' ranking of each decision maker. Then, the weighted convex median voting rule is developed to integrate the rankings results regarding all decision makers, which can solve the conflict of ranking results among experts and ensure that the comprehensive ranking results are reasonable and practical. Finally, a case study of health-care waste management is designed and comparative analyses are implemented to show the effectiveness and advantages of the proposed method.																	0266-4720	1468-0394														e12593	10.1111/exsy.12593		JUN 2020											
J								Grade Level of Lignite Coal datas in the different areas with Decison Tree, Random Forest, and Discriminant Analysis Methods	APPLIED ARTIFICIAL INTELLIGENCE											GROSS CALORIFIC VALUE; NEURAL-NETWORKS; CLASSIFICATION; PREDICTION; REGRESSION; ANN	Lignite is one of the most important energy sources. An important problem in the economic and technical evaluation of lignite reserves is to measure lignite quality. The quality of lignite depends on some parameters such as moisture, ash, sulfur, and calorific values. The assessment of the parameters has a critical importance. The lignite data obtained from Kalburcayi area of the Sivas-Kangal Basin (SKKB) and the dataset in the Turkey Lignite Inventory (TLI) were used in this article. In addition to the average values given in TLI, another set (SKKB), which beyond the inventory, has been employed. By this way, comparable data were created for performing the modeling and classification work. To make lignite quality classification, a study was performed in five steps. In the first step, the calorific values have been used for verification by the k-means method. The coal lignite data are seperated into two groups, low and high quality. In the second step, wavelet families have been applied to the properties of moisture, ash, and sulfur regulated in the first step. The applied wavelet families such as haar, daubechies, symlet, biorspline, and reversebiorspline were used and the approximate coefficients produced by wavelet families have been obtained. In the third step, the features obtained in the second step have been given to random forest, discriminant analysis, and decision tree classifiers as input. In the next step, the quality classification performances have been compared for lignite coal data derived from SKKB and TLI. While the highest quality classification performance of lignite coals in the SKKB area has been found as 93.75%, the highest quality classification performance for lignite coals obtained from TLI has been found about 100%. In the final step, the success rates provided in this study have been compared with the conventional applications in literature. The results showed that the success rates of classification recorded by the proposed method better performs than the studies used for the comparison. Because this study addresses a hybrid work, more transparent and flexible classification structures can be provided. Making an effective and reliable classification between high and low lignite calorifics can provide some possibilities for decision-makers.																	0883-9514	1087-6545				SEP 18	2020	34	11					755	776		10.1080/08839514.2020.1783849		JUN 2020											
J								Flexible Spiking CPGs for Online Manipulation During Hexapod Walking	FRONTIERS IN NEUROROBOTICS										CPG (central pattern generator); spiking neural network (SNN); hexapod robot; biological neuron; neuromorphic engineering	CENTRAL PATTERN GENERATORS; NETWORKS; MODEL; MOVEMENTS; INSECTS; NEURONS; SYSTEM; LEG	Neural signals for locomotion are influenced both by the neural network architecture and sensory inputs coordinating and adapting the gait to the environment. Adaptation relies on the ability to change amplitude, frequency, and phase of the signals within the sensorimotor loop in response to external stimuli. However, in order to experiment with closed-loop control, we first need a better understanding of the dynamics of the system and how adaptation works. Based on insights from biology, we developed a spiking neural network capable of continuously changing amplitude, frequency, and phase online. The resulting network is deployed on a hexapod robot in order to observe the walking behavior. The morphology and parameters of the network results in a tripod gait, demonstrating that a design without afferent feedback is sufficient to maintain a stable gait. This is comparable to results from biology showing that deafferented samples exhibit a tripod-like gait and adds to the evidence for a meaningful role of network topology in locomotion. Further, this work enables research into the role of sensory feedback and high-level control signals in the adaptation of gait types. A better understanding of the neural control of locomotion relates back to biology where it can provide evidence for theories that are currently not testable on live insects.																	1662-5218					JUN 26	2020	14								41	10.3389/fnbot.2020.00041													
J								Nonlinear observability of unicycle multi-robot teams subject to nonuniform environmental disturbances	AUTONOMOUS ROBOTS										Observability; Nonlinear observability; Estimation; Graph theoretic methods; Rigidity; Networked robotics	UNDERWATER NAVIGATION; RANGE MEASUREMENTS; LOCALIZATION; RIGIDITY	In this work, we consider the problem of localizing a team of robots, without access to direct pose measurements, under the influence of nonuniform environmental disturbances and measurement bias. Specifically, we are interested in the conditions under which teams remain range-only localizable when the environmental disturbances vary from robot to robot. We approach this problem through nonlinear observability and graph theory. After analyzing the system's observability properties, we present theorems that identify thestructuralconditions under which the system maintains local weak observability. We demonstrate thatrigidstructures are important not only in defining multi-robot interactions, but also in characterizing the influence of nonuniform disturbances. We also give several example systems to cement intuition on the derived conditions. An observability-based planner is then presented that guides a subset of robots toward trajectories that are highly observable through finite-horizon optimization on robot headings. Simulations are then presented, along with an extended Kalman filter for state estimation, and a comparison to previous methods, to corroborate and demonstrate the results derived.																	0929-5593	1573-7527				SEP	2020	44	7			SI		1149	1166		10.1007/s10514-020-09923-y		JUN 2020											
J								Optimized machine learning based collaborative filtering (OMLCF) recommendation system in e-commerce	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Collaborative filtering (CF); Support vector machine (SVM); Improved ant colony optimization (IACO); E-commerce; Recommender system (RS)	ANT COLONY	A recommender system (RS) is a subcategory of an information filtering system that attempts the prediction of the score or the importance given to an item by a user. RS has garnered the attention of the business community and individuals towards itself owing to its significance in the e-commerce field. One of the most common methods of the RS used for the generation of recommendations is the CF technique (collaborative filtering). But, CF-based RS yields untrustworthy similarity information and yields a recommendation quality that is not satisfactory. Support vector machine (SVM) helps in enhancing issues in the CF technique. The parameter of the SVM algorithm minimizes the system's accuracy, and therefore in classifier improved ant colony optimization (IACO) is brought-in for parameter optimization. In the newly introduced system, RS will be carried out in two stages which include (1) SVM classifier for classifying the entities into positive and negative feedback. The best value achieved indicates the optimized values of the parameters of SVM employing the IACO algorithm, which are given in the form of an input to the classifier to carry out pair-wise classification, (2) then, we construct SVM-IACO based collaborative filtering algorithm. The collaborative filtering recommendation's execution is only done on the entities' positive-feedback. The actual content used for recommendation is highly reduced owing to the classification much earlier; therefore the collaborative filtering improves the efficiency in comparison with the classical one. Tests on Taobao data (an Alibaba owned Chinese online shopping website) revealed that the algorithm yields a superior recommendation accuracy thereby commanding a particular predominant place in the e-commerce field.																	1868-5137	1868-5145															10.1007/s12652-020-02234-1		JUN 2020											
J								Performance-aware energy-efficient parallel job scheduling in HPC grid using nature-inspired hybrid meta-heuristics	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Grid; High-performance computing; Meta-heuristic; Parallel job; Scheduling	PARTICLE SWARM OPTIMIZATION; GENETIC ALGORITHM; ALLOCATION; SYSTEMS	High-Performance Computing (HPC) systems offer massive computation strength to execute large-scale applications. However, the availability of thousands of CPU cores in the HPC Systems has also triggered a significant increase in the associated energy consumption translating to higher energy expenses of system providers and carbon emissions in the environment. Therefore efficient job schedulers, which can trade-off between user-desired performance and conflicting energy-efficiency objectives simultaneously, are the need of the hour and must nowadays. Job scheduling in HPC systems is a known NP-Hard problem for which meta-heuristics may provide a near-to-optimal solution. Cuckoo search (CS) is a well-known robust swarm-intelligence based meta-heuristic, which has been applied extensively in many optimization problems due to the strong searching efficiency and requirement of very few tuning parameters. However, it suffers from the likelihood of trapping in the local minima and lack of solution diversity towards the end of the algorithm. These drawbacks could result in unacceptable results when the CS algorithm applies to the parallel job scheduling problem. To overcome these limitations and improve the searching efficiency of the traditional CS, we have proposed a multi-objective hybrid scheduling algorithm called MOHCSFA to optimally schedule the batch of parallel jobs in HPC Grid. The proposed MOHCSFA policy combines the solution search mechanisms of both Cuckoo Search (CS) and Firefly algorithm (FA) during each generation. Our proposed policy is further integrated with efficient resource allocation (ERA) heuristic to improve job scheduler performance by effectively using multi-site resource allocation. The experiments are conducted on the GridSim simulator and the benchmarking of the proposed algorithm is done using real data-sets extracted from two supercomputing workload logs. The simulation results showed that the proposed MOHCSFA policy outperforms many heuristics and meta-heuristic scheduling policies for different test cases for both performance and energy-efficiency objectives. Specifically, in the case of Unilu-Gaia workloads, the MOHCSFA obtained 5.87-24.05%, 3.46-28.50%, and 7.06-26.76% performance improvement for the makespan, energy consumption and avg. flowtime, respectively over other tested scheduling policies. The statistical tests validated the stability and robustness of the proposed policy over other scheduling policies.																	1868-5137	1868-5145															10.1007/s12652-020-02255-w		JUN 2020											
J								Using machine learning methods to detect physical conditions with postural balance	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Machine learning; Postural balance; Center of pressure; Physical conditions	ONE-LEG STANCE; STANDING BALANCE; HEALTHY-YOUNG; STABILITY; PEOPLE; FALLS; AGE; CLASSIFICATION; HYPERTENSION; PRESSURE	Previous researches investigated the association between diseases and the postural balance (PB), such as Parkinson's disease, multiple sclerosis, and Leprosy, etc. However, there is limited study exploring whether the PB can predict a person's physical condition. Therefore, the aim of this study was to build a physical conditions detection system via a simple machine learning classifier-logistic regression (LR) with PB characterized by the center of pressure (COP) measured by a force plate. We converted COP to total excursion distance (TOTEX), TOTEX of anterior-posterior distance (TOEXAP) and TOTEX of medial-lateral distance (TOTEXML) as major features in the LR model along with gender, age, and body mass index (BMI). We conducted a perspective study to collect 67 patients' records. Using those 67 records, we built 6 independent LG models based on gender, age, BMI, and collaborated with and without PB measurements to examine the effectiveness of using PB in the model to predict a person's physical condition. We compared those 6 LR models' performances based on the Area Under the Receiver Operating Characteristics (AUC), confusion matrix including accuracy, sensitivity, and specificity rate. The performance comparison results showed the predictive models with PB measurements were better than those of without PB (average AUC: 0.81 vs. 0.72). Therefore, the proposed physical conditions detection system can better discriminate healthy and unhealthy person with PB measurements in the LR classifier.																	1868-5137	1868-5145															10.1007/s12652-020-02261-y		JUN 2020											
J								Deep multitask learning for pervasive BMI estimation and identity recognition in smart beds	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Smart beds; Pressure-sensing mattress; Subject identification; BMI estimation; Multitask learning	BODY-MASS INDEX; MONITOR; OBESITY; RISK	Smart devices in the Internet of Things (IoT) paradigm provide a variety of unobtrusive and pervasive means for continuous monitoring of bio-metrics and health information. Furthermore, automated personalization and authentication through such smart systems can enable better user experience and security. In this paper, simultaneous estimation and monitoring of body mass index (BMI) and user identity recognition through a unified machine learning framework using smart beds is explored. To this end, we utilize pressure data collected from textile-based sensor arrays integrated onto a mattress to estimate the BMI values of subjects and classify their identities in different positions by using a deep multitask neural network. First, we filter and extract 14 features from the data and subsequently employ deep neural networks for BMI estimation and subject identification on two different public datasets. Finally, we demonstrate that our proposed solution outperforms prior works and several machine learning benchmarks by a considerable margin, while also estimating users' BMI in a 10-fold cross-validation scheme.																	1868-5137	1868-5145															10.1007/s12652-020-02210-9		JUN 2020											
J								CID: a novel clustering-based database intrusion detection algorithm	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Intrusion; Intrusion detection; Database; Anomaly detection; Outlier detection; Density-based clustering	USER	At the same time with the increase in the data volume, attacks against the database are also rising, therefore information security and confidentiality became a critical challenge. One promised solution against malicious attacks is theintrusion detectionsystem. In this paper, anomaly detection concept is used to propose a method for distinguishing between normal and abnormal activities. For this purpose, a new density-based clustering intrusion detection (CID) method is proposed which clusters queries based on a similarity measure and labels them as normal or intrusion. The experiments are conducted on two standard datasets including TPC-C and TPC-E. The results show proposed model outperforms state-of-the-art algorithms as baselines in terms of FN, FP, Precision, Recall and F-score measures.																	1868-5137	1868-5145															10.1007/s12652-020-02231-4		JUN 2020											
J								Intelligent mining algorithm for complex medical data based on deep learning	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Deep learning; Complex attributes; Medical data; Convolutional neural network; Association rules	DIAGNOSIS	In order to address the problems of low precision, long time-consuming and low recall rate in mining complex attribute medical data in medical information, an intelligent mining algorithm for complex attribute medical data based on deep learning is proposed. Discretized medical data with complex attributes and converted it into a data type suitable for deep learning research, the convolutional neural network is used to analyze the association mapping relationship between complex attribute medical data sets and extract association rules of data. According to the degree of association between complex attribute medical data sets in multi-dimensional subspace to realize the effective mining of complex attribute medical data. The results show that the proposed algorithm takes less time and can extract association rules accurately, the data priority control efficiency is higher, the data mining accuracy is better, and the data mining recall rate is much higher than other methods, which verifies the feasibility of the proposed algorithm.																	1868-5137	1868-5145															10.1007/s12652-020-02239-w		JUN 2020											
J								Composite feature vector based cardiac arrhythmia classification using convolutional neural networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cardiac arrhythmias; Electrocardiogram; Dual tree complex wavelet transform; Temporal features; Convolutional neural networks; MIT-BIH database; Accuracy; Sensitivity		Electrocardiogram analysis for the classification of several cardiac arrhythmias has gained a significant research importance in the medical field. Towards such objective, this paper proposed a novel approach based on the fusion of multiple features extracted from a signal through various methods and Convolution Neural Networks. The multiple features are precisely consisting of morphological features, temporal features, and statistical features. Every electrocardiogram signal is initially pre-processed to remove the base line and then processed for segmentation through a simple strategy. Further, for every heart beat segment, three different set of features are extracted. Among them, morphological features are obtained through Dual Tree Complex Wavelet Transform and remaining features are extracted through statistical measures. Further, Principal Component Analysis is applied over the morphological feature set to reduce the dimensionality. Finally, a composite and final feature vector is formulated and then fed to Convolutional Neural Networks classifier to predict the label for a given input heartbeat. Simulation experiments conducted through the MIT-BIH benchmark database exhibited that the proposed system achieves better classification accuracy and on an average, it of 98%. Compared with state-of-art methods, the improvement is approximately 5%.																	1868-5137	1868-5145															10.1007/s12652-020-02259-6		JUN 2020											
J								Improved binary gray wolf optimizer and SVM for intrusion detection system in wireless sensor networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Intrusion detection system; Binary grey wolf optimizer; Wireless sensor networks; Intrusions; NSL-KDD	FEATURE-SELECTION	Intrusion in wireless sensor networks (WSNs) aims to degrade or even eliminating the capability of these networks to provide its functions. In this paper, an enhanced intrusion detection system (IDS) is proposed by using the modified binary grey wolf optimizer with support vector machine (GWOSVM-IDS). The GWOSVM-IDS used 3 wolves, 5 wolves and 7 wolves to find the best number of wolves. The proposed method aims to increase intrusion detection accuracy and detection rate and reduce processing time in the WSN environment through decrease false alarms rates, and the number of features resulted from the IDSs in the WSN environment. Indeed, the NSL KDD'99 dataset is used to demonstrate the performance of the proposed method and compare it with other existing methods. The proposed methods are evaluated in terms of accuracy, the number of features, execution time, false alarm rate, and detection rate. The results showed that the proposed GWOSVM-IDS with seven wolves overwhelms the other proposed and comparative algorithms.																	1868-5137	1868-5145															10.1007/s12652-020-02228-z		JUN 2020											
J								Automated Hand-drawn sketches retrieval and recognition using regularized Particle Swarm Optimization based deep convolutional neural network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Fuzzy logic; Free hand-drawn sketch; Deep convolutional neural network; Regularized particle swarm optimization; Data model and features	FRAMEWORK	One of the most popular and rising research area of image processing is free hand-drawn sketch recognition and its retrieval. Enlarger number of methods is introduced to retrieve the sketch images but it made few complexity issues and their performance often degraded. So, in this paper, we proposed an effective method of Regularized Particle Swarm Optimization Based Deep Convolutional Neural Network (RPSO-DCNN) algorithm to retrieve the performance of free hand-drawn sketches. In feature extraction, the Regularized Particle Swarm Optimization (RPSO) model that aim is to produce an optimal evolutionary deep learning result. Therefore, the free hand-drawn sketch image classification and its retrieval are performed by Support Vector Machine and Levenshtein distance-based fuzzy k-nearest neighbour (L-FkNN) algorithms. Hence, this work can bring in communication between human and computer. Experimentally, the simulation work of the proposed RPSO-DCNN model is implemented in the running software of MATLAB. The sketch images are chosen from the TU-Berlin dataset, Sketch dataset, SHREC13 dataset, Flickr dataset and Sketchy dataset. Aiming is to facilitate the performance of the proposed RPSO-DCNN model with various kinds of state of art methods such as H-CNN, Fuzzy, CNN, MARQS and TCVD. The experimental result demonstrates that, the proposed RPSO-DCNN accomplish the optimal accuracy with different state-of-art methods.																	1868-5137	1868-5145															10.1007/s12652-020-02248-9		JUN 2020											
J								KGAnet: a knowledge graph attention network for enhancing natural language inference	NEURAL COMPUTING & APPLICATIONS										Natural language processing; Natural language inference; External knowledge		Natural language inference (NLI) is the basic task of many applications such as question answering and paraphrase recognition. Existing methods have solved the key issue of how the NLI model can benefit from external knowledge. Inspired by this, we attempt to further explore the following two problems: (1) how to make better use of external knowledge when the total amount of such knowledge is constant and (2) how to bring external knowledge to the NLI model more conveniently in the application scenario. In this paper, we propose a novel joint training framework that consists of a modified graph attention network, called the knowledge graph attention network, and an NLI model. We demonstrate that the proposed method outperforms the existing method which introduces external knowledge, and we improve the performance of multiple NLI models without additional external knowledge.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14963	14973		10.1007/s00521-020-04851-5		JUN 2020											
J								Automated cell division classification in early mouse and human embryos using convolutional neural networks	NEURAL COMPUTING & APPLICATIONS										Cell division detection; Convolutional neural network; Embryology; In vitro fertilization; Machine learning; Time-lapse microscopy	ARTIFICIAL-INTELLIGENCE	During in vitro fertilization (IVF), the timing of cell divisions in early human embryos is a key predictor of embryo viability. Recent developments in time-lapse microscopy (TLM) have allowed us to observe cell divisions in much greater detail than previously possible. However, it is a time-consuming process that relies on a highly trained staff and subjective observations. We describe an automated method based on a convolutional neural network to detect and classify cell divisions from original (unprocessed) TLM images. Here, we used two embryo TLM image datasets to evaluate our method: a public dataset with mouse embryos up to the 4-cell stage and a private dataset with human embryos up to the 8-cell stage. Compared to embryologists' annotations, our results were almost 100% accurate for the mouse embryo images and accurate within five frames in 93.9% of cell stage transitions for the human embryos. Our approach can be used to improve the consistency and quality of the existing annotations or as part of a platform for fully automated embryo assessment. The code is available at http://github.com/JonasEMalmsten/CellDivision..																	0941-0643	1433-3058															10.1007/s00521-020-05127-8		JUN 2020											
J								Three-value cutting tensors of intuitionistic fuzzy tensors	SOFT COMPUTING										Intuitionistic fuzzy tensor; Three-value cutting tensor; Three-value decomposition theorem; Evaluating engineering projects	DECOMPOSITION; REPRESENTATION; DIMENSIONALITY; ALGORITHMS	In this paper, we first introduce the definition of three-value cutting tensors of intuitionistic fuzzy tensors. Secondly, we discuss some fundamental properties by the definition of the three-value cutting tensors and relationships between elements and discuss the application of three-value cutting tensors in evaluating engineering projects. Finally, we investigate decomposition of intuitionistic fuzzy tensors by three-value cutting tensors under max-min compositional operations. Our numerical examples show the feasibility of the presented decomposition methods.																	1432-7643	1433-7479															10.1007/s00500-020-05125-x		JUN 2020											
J								A particle swarm optimization-based feature selection for unsupervised transfer learning	SOFT COMPUTING										Domain adaptation; Classification; Particle swarm optimization; Transfer learning; Unsupervised discriminant analysis; Feature selection	DOMAIN ADAPTATION; KERNEL; FRAMEWORK	Transfer learning (TL) method has captured an attractive presence because it facilitates the learning ability in the target domain by acquiring knowledge from well-established source domains. To gain strong knowledge from the source domain, it is important to narrow down the distribution difference between the source and the target domains. For this purpose, it is necessary to consider the objectives such as preserving the discriminative information, preserving the original similarity of the source and the target domain data, maximizing the variance of the target domain, and preserving marginal and conditional distribution at the same time. Furthermore, some existing TL methods use only original feature data, so there is a threat of degenerated feature transformation. To overcome all these limitations, in this paper, a novel feature selection-based transfer learning approach using particle swarm optimization (PSO) for unsupervised transfer learning (FSUTL-PSO) is implemented. In FSUTL-PSO, we incorporate all such objectives into one fitness function and select common good features from the source and target domains based on the fitness function for eliminating the threat of degenerated features. Extensive experiments have been done on all possible tasks of Office+Caltech and PIE Face datasets and our proposed method FSUTL-PSO has shown significant improvement over the existing transfer or non-transfer learning methods.																	1432-7643	1433-7479															10.1007/s00500-020-05105-1		JUN 2020											
J								Application of infrared microscopy and alternating least squares to the forensic analysis of automotive paint chips	JOURNAL OF CHEMOMETRICS										chemometrics; cross sectioning; forensic automotive paint analysis; microtome; multivariate curve resolution; spectral library matching; transmission infrared imaging microscopy		To collect infrared (IR) absorbance spectra from an automotive paint chip with an IR imaging microscope, it is a common practice to cast the paint chip in epoxy and then cross section it using a microtome to reveal the individual layers of paint. Ideally, the epoxy should present little or no spectral interference. However, the epoxy can infiltrate individual layers of the paint chip as it cures contaminating the IR spectra of the layers and impairing the accuracy of a search of each of these layers against an automotive paint library. In this study, we have demonstrated that automotive paint chips can be successfully cross sectioned without the use of embedding media. Sample preparation is easier, and more importantly, interfering peaks in the spectra due to the epoxy are eliminated. To demonstrate the advantages of this approach for sample preparation, IR image maps of four automotive paint chips that were not cast in epoxy prior to cross sectioning were collected. After each IR image was unfolded using an oblique transit to traverse the image, the spectra of the individual paint layers comprising the line map were reconstructed by alternating least squares. Comparing each recovered IR spectrum against a spectral library, we show that high quality spectral matches were obtained for spectra from the same line/model of the vehicle from which the paint sample originated. When the same paint chips were cast in epoxy prior to cross sectioning, high quality spectral matches could not always be obtained.																	0886-9383	1099-128X														e3277	10.1002/cem.3277		JUN 2020											
J								On data lake architectures and metadata management	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Data lakes; Data lake architectures; Metadata management; Metadata modeling	BIG DATA	Over the past two decades, we have witnessed an exponential increase of data production in the world. So-called big data generally come from transactional systems, and even more so from the Internet of Things and social media. They are mainly characterized by volume, velocity, variety and veracity issues. Big data-related issues strongly challenge traditional data management and analysis systems. The concept of data lake was introduced to address them. A data lake is a large, raw data repository that stores and manages all company data bearing any format. However, the data lake concept remains ambiguous or fuzzy for many researchers and practitioners, who often confuse it with the Hadoop technology. Thus, we provide in this paper a comprehensive state of the art of the different approaches to data lake design. We particularly focus on data lake architectures and metadata management, which are key issues in successful data lakes. We also discuss the pros and cons of data lakes and their design alternatives.																	0925-9902	1573-7675															10.1007/s10844-020-00608-7		JUN 2020											
J								A multi-task and multi-scale convolutional neural network for automatic recognition of woven fabric pattern	JOURNAL OF INTELLIGENT MANUFACTURING										Weave pattern recognition; Texture analysis; Computer vision; Multi-task learning; Convolutional neural network	WEAVE PATTERNS; CLASSIFICATION	The recognition of woven fabric pattern is a crucial task for mass manufacturing and quality control in the textile industry. Traditional methods based on image processing have some limitations on accuracy and stability. In this paper, an automatic method is proposed to jointly realize yarn location and weave pattern recognition. First, a new big fabric dataset is established by a portable wireless device. The dataset contains wide kinds of fabrics and detailed fabric structure parameters. Then, a novel multi-task and multi-scale convolutional neural network (MTMSnet) is proposed to predict the location maps of yarns and floats. By adopting the multi-task structure, the MTMSnet can better learn the related features between yarns and floats. Finally, the weave pattern and basic weave repeat are recognized by combining the yarn and float location maps. Extensive experimental results on various kinds of fabrics indicate that the proposed method achieves high accuracy and quality in weave pattern recognition.																	0956-5515	1572-8145															10.1007/s10845-020-01607-9		JUN 2020											
J								Interpretation of chemical data from glass analysis for forensic purposes	JOURNAL OF CHEMOMETRICS										calibrated likelihood ratios; elemental analysis; glass evidence interpretation	LA-ICP-MS; ATOMIC EMISSION-SPECTROMETRY; PLASMA-MASS SPECTROMETRY; X-RAY-FLUORESCENCE; REFRACTIVE-INDEX; ELEMENTAL ANALYSIS; FLOAT GLASS; DISCRIMINATION; VARIANCE; SHEET	The aims of evaluating forensic evidence are to provide a transparent, coherent, and unbiased opinion of the value of the evidence to fact-finders. Measurements from glass evidence in a hit-and-run, for example, can help decide if a particular vehicle was involved in the accident. The evaluation involves the comparison of the physical, optical, and chemical properties of the glass recovered from the broken window with glass fragments suspected of originating from the window. A standard method (ASTM E2927-16e1) describes a consensus-based approach to sampling, sample preparation, quantitative analysis and "match" criterion for comparison of chemical properties. The result is a binary decision of either finding a difference in the elemental composition (exclusion) or a failure to exclude, based on elemental composition. This study demonstrates the utility of likelihood ratio (LR) calculations using novel datasets of glass samples of known manufacturing history. The LRs calculated from comparing glass manufactured at three different plants over relatively short periods (over 2-6 weeks) range from very low values (LR similar to 10(-3)) when the glass are manufactured at different plants or manufactured weeks-months apart in the same plant to very high values (LR similar to 10(3)) when the glass samples are manufactured on the same day. Although the glass samples being compared may not originate from the same broken window source, they do exhibit chemical similarity within these lower and upper bounds and the LRs presented here, for the first time, closely correlate chemical relatedness to manufacturing history, specifically the time interval between production. The work presented here supports the use of the match criteria recommended within ASTM E2927-16e1 and provides a data-driven path forward to expand on the interpretation of glass using LRs.																	0886-9383	1099-128X														e3267	10.1002/cem.3267		JUN 2020											
J								Exploring Stiffness Modulation in Prosthetic Hands and Its Perceived Function in Manipulation and Social Interaction	FRONTIERS IN NEUROROBOTICS										prosthetics; impedance control; soft robotics; human-robot social interaction; task adaptability	IMPEDANCE CONTROL; LIMB; ELBOW; COCONTRACTION; COACTIVATION; SYNERGIES; MOTION; GRASP; EMG	To physically interact with a rich variety of environments and to match situation-dependent requirements, humans adapt both the force and stiffness of their limbs. Reflecting this behavior in prostheses may promote a more natural and intuitive control and, consequently, improve prostheses acceptance in everyday life. This pilot study proposes a method to control a prosthetic robot hand and its impedance, and explores the utility of variable stiffness when performing activities of daily living and physical social interactions. The proposed method is capable of a simultaneous and proportional decoding of position and stiffness intentions from two surface electro-myographic sensors placed over a pair of antagonistic muscles. The feasibility of our approach is validated and compared to existing control modalities in a preliminary study involving one prosthesis user. The algorithm is implemented in a soft under-actuated prosthetic hand (SoftHand Pro). Then, we evaluate the usability of the proposed approach while executing a variety of tasks. Among these tasks, the user interacts with other 12 able-bodied subjects, whose experiences were also assessed. Several statistically significant aspects from the System Usability Scale indicate user's preference of variable stiffness control over low or high constant stiffness due to its reactivity and adaptability. Feedback reported by able-bodied subjects reveal a general tendency to favor soft interaction, i.e., low stiffness, which is perceived more human-like and comfortable. These combined results suggest the use of variable stiffness as a viable compromise between firm control and safe interaction which is worth investigating further.																	1662-5218					JUN 25	2020	14								33	10.3389/fnbot.2020.00033													
J								Interactive Natural Language Grounding via Referring Expression Comprehension and Scene Graph Parsing	FRONTIERS IN NEUROROBOTICS										interactive natural language grounding; referring expression comprehension; scene graph; visual and textual semantics; human-robot interaction		Natural language provides an intuitive and effective interaction interface between human beings and robots. Currently, multiple approaches are presented to address natural language visual grounding for human-robot interaction. However, most of the existing approaches handle the ambiguity of natural language queries and achieve target objects grounding via dialogue systems, which make the interactions cumbersome and time-consuming. In contrast, we address interactive natural language grounding without auxiliary information. Specifically, we first propose a referring expression comprehension network to ground natural referring expressions. The referring expression comprehension network excavates the visual semantics via a visual semantic-aware network, and exploits the rich linguistic contexts in expressions by a language attention network. Furthermore, we combine the referring expression comprehension network with scene graph parsing to achieve unrestricted and complicated natural language grounding. Finally, we validate the performance of the referring expression comprehension network on three public datasets, and we also evaluate the effectiveness of the interactive natural language grounding architecture by conducting extensive natural language query groundings in different household scenarios.																	1662-5218					JUN 25	2020	14								43	10.3389/fnbot.2020.00043													
J								An edge streaming data processing framework for autonomous driving	CONNECTION SCIENCE										Edge computing; streaming data processing; spark streaming; autonomous driving		In recent years, with the rapid development of sensing technology and the Internet of Things (IoT), sensors play increasingly important roles in traffic control, medical monitoring, industrial production and etc. They generated high volume of data in a streaming way that often need to be processed in real time. Therefore, streaming data computing technology plays an indispensable role in the real-time processing of sensor data in high throughput but low latency. However, there are two problems in deploying streaming data process ability in cloud computing data centre. Firstly, massive sensor nodes simultaneously upload data to the remote cloud computing data centre, which requires a large number of bandwidth resources supports. The existing network infrastructure cannot provide enough bandwidth at a reasonable price. Secondly, due to the geographical distribution characteristics of the cloud computing data centre, there will inevitably be large transmission delay during the process of data transmission. Such end-to-end delay is intolerable to mobile applications especially for those latency sensitive tasks. In view of the above problems, this paper proposes an autonomous driving oriented edge streaming data processing framework, which migrates the computing and storage capability from the remote cloud data centre to the edge data centre. It focuses on the change of vehicle flow in a specific geographical area, and uses the computing power sunk to edge node to process the massive streaming data generated by autonomous vehicles nearby. The proposed framework is implemented on top of Spark Streaming, which builds up a gray model based traffic flow monitor, a traffic prediction orientated prediction layer and a fuzzy control based Batch Interval dynamic adjustment layer for Spark Streaming. It could forecast the variation of sensors data arrive rate, make streaming Batch Interval adjustment in advance and implement real-time streaming process by edge. Therefore, it can realise the monitor and prediction of the data flow changes of the autonomous driving vehicle sensor data in geographical coverage of edge computing node area, meanwhile minimise the end-to-end latency but satisfy the application throughput requirements. The experiments show that it can predict short-term traffic with no more than 4% relative error in a whole day. By making batch consuming rate close to data generating rate, it can maintain system stability well even when arrival data rate changes rapidly. The Batch Interval can be converged to a suitable value in two minutes when data arrival rate is doubled. Compared with vanilla version Spark Streaming, where there has serious task accumulation and introduces large delay, it can reduce 35% latency by squeezing Batch Interval when data arrival rate is low; it also can significantly improve system throughput by only at most 25% Batch Interval increase when data arrival rate is high.																	0954-0091	1360-0494															10.1080/09540091.2020.1782840		JUN 2020											
J								Monitoring of air pollution to establish optimal less polluted path by utilizing wireless sensor network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Wireless sensor network; Carbon monoxide; Air pollution; Multi-path routing; Adaptive neuro-fuzzy inference system; Krill herd optimization and MapReduce	QUALITY; SYSTEM	An efficient air pollution monitoring (APM) scheme is proposed to establish an optimal less polluted path using WSN (wireless sensor network), in which the sensor node (SN) senses the temperature and CO gas (carbon monoxide) concentration's existent in the air. Initially, the sensed information from the SNs is preprocessed. During preprocessing, the value that is missed in the sensed information is imputed. Next, Hadoop's distributed file system (HDFS) MapReduce (MR) is implemented on the preprocessed data and subsequently, the resulting data is saved in the cloud server. The resulting data is analyzed using Improved-Adaptive Neuro-Fuzzy Inference System (I-ANFIS) Algorithm for checking air pollutions severities and its location is then presented in the Google Map. After that, the multi-path routing is established through the less polluted area. Lastly, the optimal path is chosen with the assistance of KHOA (Krill Herd Optimization Algorithm). The outcomes are evaluated by contrasting the proposed and prevailing techniques.																	1868-5137	1868-5145															10.1007/s12652-020-02232-3		JUN 2020											
J								A distributed key authentication and OKM-ANFIS scheme based breast cancer prediction system in the IoT environment	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Distributed key-based advanced encryption standard (DK-AES); Optimized K-means based adaptive neuro-fuzzy inference system (OKM-ANFIS); Seven sensors; Genetic algorithm (GA)	2-FACTOR AUTHENTICATION	The Internet of Things (IoT) has significantly upgraded in medical and health care. This technology aids the patients as well as doctors for envisaging an assortment of diseases precisely and diagnoses these diseases as per the outcomes. However, the prevailing research methodologies encompass the issue of poor diagnostic accuracy in addition to safe data transfer betwixt IoT and cloud storage. This paper proposed a distributed key authentication in addition to OKM-ANFIS cantered breast cancer (BC) prediction system on the IoT environment to trounce such disadvantages also, the research used GA for the prediction of multi models. Initially, the authentication is performed by means of the patient. Then, the sensed values are attained as of the ' sensors that are placed inside the bra. Later, the DK-AES algorithm uploads the attained data safely to the hospital public cloud server (CS). Subsequently, the hospital management (HM) system downloads the data securely. The HM-system envisages BC in '2' phases: (1) pre-processing and (2) prediction. Utilizing removal redundancy, replacement of missing attributes, along with normalization, the data is pre-processed. Subsequently, the OKM-ANFIS classification algorithm predicts the disease. If any critical concerns arise, an alert text is sent by the HM to the patient's mobile. In an experimental assessment, the proposed work renders better outcomes than the prevailing methods.																	1868-5137	1868-5145															10.1007/s12652-020-02249-8		JUN 2020											
J								Water contaminants detection using sensor placement approach in smart water networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Water distributed system; Water quality monitoring; Sensor placement; Optimization; Evolutionary algorithm	DRINKING-WATER; QUALITY	Incidents of water pollution or contamination have occurred repeatedly in recent years, causing significant disasters and negative health impacts. Water quality sensors need to be installed in the water distribution system (WDS) to allow real-time water contamination detection to reduce the risk of water contamination. Deploying sensors in WDS is essential to monitor and detect any pollution incident at the appropriate time. However, it is impossible to place sensors on all nodes of the network due to the relatively large structure of WDS and the high cost of water quality sensors. For that, it is necessary to reduce the cost of deployment and guarantee the reliability of the sensing, such as detection time and coverage of the whole water network. In this paper, a dynamic approach of sensor placement that uses an evolutionary algorithm (EA) is proposed and implemented. The proposed method generates a multiple set of water contamination scenarios in several locations selected randomly in the WDS. Each contamination scenario spreads in the water networks for several hours, and then the proposed approach simulates the various effect of each contamination scenario on the water networks. On the other hand, the multiple objectives of the sensor placement optimization problem, which aim to find the optimal locations of the deployed sensors, have been formulated. The sensor placement optimization solver, which uses the EA, is operated to find the optimal sensor placements. The effectiveness of the proposed method has been evaluated using two different case studies on the example of water networks: Battle of the water sensor network (BWSN) and another real case study from Madrid (Spain). The results have shown the capability of the proposed method to adapt the location of the sensors based on the numbers and the locations of contaminant sources. Moreover, the results also have demonstrated the ability of the proposed approach for maximising the coverage of deployed sensors and reducing the time to detect all the water contaminants using a few numbers of water quality sensors.																	1868-5137	1868-5145															10.1007/s12652-020-02262-x		JUN 2020											
J								An improved range based localization using Whale Optimization Algorithm in underwater wireless sensor network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Underwater wireless sensor networks; Range-based localization; Improved range based localization; Whale Optimization Algorithm; Localization error		The localization of underwater sensors is the most crucial task in underwater wireless sensor networks (UWSNs). The sensors which are situated under the water sense information from the earth and detected information is transmitted to the observing station. Although the monitoring station receives the sensed information, the data is meaningless without knowing the exact position of the sensors. So, in UWSN, localization is the major issue to be resolved. The Range-Based Whale Optimization Algorithm (WOA) is introduced in this paper to discover this issue. To evaluate the valued location of the sensors, localization error is further reduced. The localization error is reduced by applying the WOA in this work. Reproduction results exhibit that presentation measurements of the planned methodology beats that of the current work as far as limitation mistake and restriction inclusion. Besides, the proposed scheme achieves better localization coverage, delivery ratio, delay and energy consumption than the existing algorithms.																	1868-5137	1868-5145															10.1007/s12652-020-02263-w		JUN 2020											
J								Brain tissue segmentation for medical decision support systems	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Fuzzy C-means; Exposure threshold; Tissue segmentation; Intensity inhomogeneity	C-MEANS ALGORITHM; HISTOGRAM EQUALIZATION; IMAGE	Automatic brain tissue segmentation on clinically acquired magnetic resonance image is a very challenging task due to the presence of intensity inhomogeneity, noise, and the complex anatomical structure of interest. Due to the existence of noise in clinical magnetic resonance brain images, various segmentation techniques suffer from low segmentation accuracy. Thus, to overcome the ambiguity caused by the above special effects, an enhanced fuzzy relaxation approach called fuzzy relaxation-based modified fuzzy c-means clustering algorithm is presented. In the proposed method, exposure-based sub-image fuzzy brightness adaptation algorithm is implemented for the enhancement of brain tissues, and it is followed by a modified fuzzy c-means clustering algorithm to segment the enhanced brain magnetic resonance image into white matter, gray matter and cerebrospinal fluid tissues. The proposed method is compared with other existing methods in terms of quantitative measures such as peak signal to noise ratio, discrete entropy, contrast improvement index, sensitivity, specificity, accuracy, jaccard similarity, and dice similarity coefficient. Experimental results demonstrate that the proposed method achieves a good trade-off between intensity inhomogeneity and noise. The proposed method conforms its success on brain tissue segmentation and provides extensive support to radiologists and clinical centers.																	1868-5137	1868-5145															10.1007/s12652-020-02257-8		JUN 2020											
J								Hybrid collective intelligence in a human-AI society	AI & SOCIETY										Artificial intelligence; Hybrid intelligence; Collective intelligence; Human intelligence; Human-AI collaboration; Human-AI society	BIG DATA; DESIGN; FUTURE; ROBOTS	Within current debates about the future impact of Artificial Intelligence (AI) on human society, roughly three different perspectives can be recognised: (1) thetechnology-centric perspective, claiming that AI will soon outperform humankind in all areas, and that the primary threat for humankind is superintelligence; (2) thehuman-centric perspective, claiming that humans will always remain superior to AI when it comes to social and societal aspects, and that the main threat of AI is that humankind's social nature is overlooked in technological designs; and (3)the collective intelligence-centric perspective, claiming that true intelligence lies in the collective of intelligent agents, both human and artificial, and that the main threat for humankind is that technological designs create problems at the collective, systemic level that are hard to oversee and control. The current paper offers the following contributions: (a) a clear description for each of the three perspectives, along with their history and background; (b) an analysis and interpretation of current applications of AI in human society according to each of the three perspectives, thereby disentangling miscommunication in the debate concerning threats of AI; and (c) a new integrated and comprehensive research design framework that addresses all aspects of the above three perspectives, and includes principles that support developers to reflect and anticipate upon potential effects of AI in society.																	0951-5666	1435-5655															10.1007/s00146-020-01005-y		JUN 2020											
J								Solving artificial ant problem using two artificial bee colony programming versions	APPLIED INTELLIGENCE										Robotic path planning; Artificial ant problem; Evolutionary computation based automatic programming; Artificial bee colony programming; Shrinking artificial bee colony programming; Dynamically decreasing population size	SYMBOLIC REGRESSION; ELASTIC-MODULUS; ALGORITHMS	Artificialant problem is considered as a sub-problem of robotic path planning. In this study, it is solved using two different methods: artificial bee colony programming and a new version of it called shrinking artificial bee colony programming. The former is a novel evolutionary computation based automatic programming method based on artificial bee colony algorithm and it was previously applied to this problem by the researchers. However, in this study, more comprehensive analyses and comparison study are provided. The shrinking artificial bee colony programming was developed in this study and its basic idea is to reduce the number of food sources, periodically, instead of a constant number used in the artificial bee colony programming. First, some parameter tuning studies were carried out for the shrinking artificial bee colony programming. Then, performances of the artificial bee colony programming, shrinking artificial bee colony programming and some other evolutionary computation based automatic programming methods were compared on Santa Fe and Los Altos Hills trails. Simulation results and the comparison study show that both of the algorithms can be used to solve the artificial ant problem effectively. Furthermore, the periodically decreasing population size property added to the artificial bee colony programming improves the performance of the algorithm on the artificial ant problem. While the proposed approach shows one of the superior performances among the considered methods, the results of the artificial bee colony programming are competitive to the methods in the comparison study.																	0924-669X	1573-7497				NOV	2020	50	11					3695	3717		10.1007/s10489-020-01741-0		JUN 2020											
J								A new parameter reduction algorithm for interval-valued fuzzy soft sets based on Pearson's product moment coefficient	APPLIED INTELLIGENCE										Soft set; Pearson's product moment coefficient; Interval-valued fuzzy soft set; Parameter reduction		Interval-valued fuzzy soft set theory is a newly emerging mathematical tool for dealing with uncertain problems. Parameter reduction abandons redundant parameters meanwhile holds the powerful ability to support decision making Ma et al. (2014) expressed four diverse parameter reduction approaches which are appropriate for the different scenarios. Optimal choice considered parameter reduction approach is not effective to support the additive parameters. The other three methods provide the support for the newly additive parameters but possess very low rate of success. And the four methods are computationally complicated. In this paper, we propose Pearson's product moment coefficient based parameter reduction algorithm for an interval-valued fuzzy soft sets. By comparison with four algorithms, this approach not only is carried out before getting the scores, give attention to the newly additive parameters, and has much higher probability to find parameter reduction, but also is not more computationally complicated. Therefore, this algorithm is the most efficient to support extension and combination of multiple evaluation systems based on interval-valued fuzzy soft set in the down-to-earth applications environment. The superiority and effectiveness of the proposed approach is demonstrated by this means of a suitable practical application case of on-line reservation for accommodation and twenty synthetic generated datasets.																	0924-669X	1573-7497				NOV	2020	50	11					3718	3730		10.1007/s10489-020-01708-1		JUN 2020											
J								Survey on evaluation methods for dialogue systems	ARTIFICIAL INTELLIGENCE REVIEW										Dialogue systems; Evaluation metrics; Discourse model; Conversational AI; Chatbots	COMPUTER; OPTIMIZATION; STRATEGIES; QUALITY; MODEL	In this paper, we survey the methods and concepts developed for the evaluation of dialogue systems. Evaluation, in and of itself, is a crucial part during the development process. Often, dialogue systems are evaluated by means of human evaluations and questionnaires. However, this tends to be very cost- and time-intensive. Thus, much work has been put into finding methods which allow a reduction in involvement of human labour. In this survey, we present the main concepts and methods. For this, we differentiate between the various classes of dialogue systems (task-oriented, conversational, and question-answering dialogue systems). We cover each class by introducing the main technologies developed for the dialogue systems and then present the evaluation methods regarding that class.																	0269-2821	1573-7462															10.1007/s10462-020-09866-x		JUN 2020											
J								Spatially-dependent Bayesian semantic perception under model and localization uncertainty	AUTONOMOUS ROBOTS										Semantic perception; Object classification and pose estimation; SLAM	OBJECT; CLASSIFICATION	Semantic perception can provide autonomous robots operating under uncertainty with more efficient representation of their environment and better ability for correct loop closures than only geometric features. However, accurate inference of semantics requires measurement models that correctly capture properties of semantic detections such as viewpoint dependence, spatial correlations, and intra- and inter-class variations. Such models should also gracefully handle open-set conditions which may be encountered, keeping track of the resultant model uncertainty. We propose a method for robust visual classification of an object of interest observed from multiple views in the presence of significant localization uncertainty and classifier noise, and possible dataset shift. We use a viewpoint dependent measurement model to capture viewpoint dependence and spatial correlations in classifier scores, showing how to use it in the presence of localization uncertainty. Assuming a Bayesian classifier providing a measure of uncertainty, we show how its outputs can be fused in the context of the above model, allowing robust classification under model uncertainty when novel scenes are encountered. We present statistical evaluation of our method both in synthetic simulation, and in a 3D environment where rendered images are fed into a Deep Neural Network classifier. We compare to baseline methods in scenarios of varying difficulty showing improved robustness of our method to localization uncertainty and dataset shift. Finally, we validate our contribution w.r.t. localization uncertainty on a dataset of real-world images.																	0929-5593	1573-7527				JUL	2020	44	6					1091	1119		10.1007/s10514-020-09921-0		JUN 2020											
J								Exploration of the applicability of probabilistic inference for learning control in underactuated autonomous underwater vehicles	AUTONOMOUS ROBOTS										PILCO; LOS; Underwater vehicle; Path tracking; Reinforcement learning	TRACKING; ROBUST	Underwater vehicles are employed in the exploration of dynamic environments where tuning of a specific controller for each task would be time-consuming and unreliable as the controller depends on calculated mathematical coefficients in idealised conditions. For such a case, learning task from experience can be a useful alternative. This paper explores the capability of probabilistic inference learning to control autonomous underwater vehicles that can be used for different tasks without re-programming the controller. Probabilistic inference learning uses a Gaussian process model of the real vehicle to learn the correct policy with a small number of real field experiments. The use of probabilistic reinforcement learning looks for a simple implementation of controllers without the burden of coefficients calculation, controller tuning or system identification. A series of computational simulations were employed to test the applicability of model-based reinforcement learning in underwater vehicles. Three simulation scenarios were evaluated: waypoint tracking, depth control and 3D path tracking control. The 3D path tracking is done by coupling together a line-of-sight law with probabilistic inference for learning control. As a comparison study LOS-PILCO algorithm can perform better than a robust LOS-PID. The results show that probabilistic model-based reinforcement learning can be a deployable solution to motion control of underactuated AUVs as it can generate capable policies with minimum quantity of episodes.																	0929-5593	1573-7527				JUL	2020	44	6					1121	1134		10.1007/s10514-020-09922-z		JUN 2020											
J								Robust Position/Force Control of Constrained Flexible Joint Robots with Constraint Uncertainties	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Constrained flexible joint robots; Position and force control; Robust control; Constraint uncertainties	IMPEDANCE CONTROL; FORCE CONTROL; TRACKING CONTROL; MANIPULATORS; MOTION	A novel robust control method for simultaneous position/force control of constrained flexible joint robots is proposed. The facts that the uncertainties make the usual control task unsolvable and that the equations of the controlled system are differential-algebraic make the problem dealt with considerably demanding. In order to overcome the unsolvability problem due to the constraint uncertainties the position control task is redefined in a practical way such that only a suitable subgroup of the link positions are driven to their desired trajectories. To determine the elements of the subgroup a simple algorithm of practical relevance is proposed. Under certain smoothness conditions to the contact surfaces, it is demonstrated that the position control problem can dynamically be isolated from the force control. Thus, it becomes possible to handle the position and force control tasks separately. The most significant advantage of the separation of the position and force control tasks is that it makes possible to adapt the position control methods known from free robots. Each joint is used in either position control or force control. The proposed position controller has a cascaded structure: First, trajectories for joint positions that drive the link positions to their desired values are calculated. Then, the joint torques that drive the joint positions to their calculated values are determined. A further significant benefit of the separation of the position and force control tasks arises in the force control such that the transformed equations are linear and any linear robust control approach can be used for the force control. The whole controller requires the measurement of the link and joint positions, the link and joint velocities and the contact forces, and allows modeling uncertainties in the equations of both the robot dynamics and the contact surfaces. The proposed control method is also confirmed by simulations.																	0921-0296	1573-0409															10.1007/s10846-020-01220-1		JUN 2020											
J								A New Parcel Delivery System with Drones and a Public Train	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Unmanned aerial vehicles; Drones; Parcel delivery; Generalized travelling salesman problem	TRAVELING SALESMAN PROBLEM; UNMANNED AERIAL VEHICLE; BASE; CITY; UAVS; OPTIMIZATION; DEPLOYMENT; STATION; MODEL	In this paper, we propose a new parcel delivery system consisting of a public train and drones. The train, an already existing mobile platform in our neighbourhood, follows its normal predefined route and timetable to transport passengers. In the meanwhile, some parcels to be delivered to some customers and a delivery drone are stored on its roof. The drone can launch from the train, deliver the parcel to a customer, and return to the train. It can also travel with the train and replace its battery on the roof via an automatic battery swap system. As the parcel delivery system cannot manage the movement of the train, the route and the timetable of the train need to be accounted carefully to schedule the deliveries. We formulate an optimization problem to minimize the total delivery time of a given set of parcels, and we propose two algorithms. Though the exact algorithm gives the optimal schedule, its computational complexity is exponential to the number of parcels. To make the proposed system possible to be implemented in practice, a suboptimal algorithm is developed, which is more efficient than the exact algorithm and can achieve close performance with the exact algorithm. Additionally, we propose a simple but effective strategy to deal with the uncertainty associated with the train's timetable. Moreover, the proposed algorithm for the single drone case is modified to deal with the multiple drone case. The effectiveness of the proposed algorithms is verified via computer simulations and comparison with existing methods. The results show that the presented approach is more cost-efficient than the Truck only scheme and the Truck plus Drone scheme. Moreover, the parcel delivery time can be reduced and the delivery area can be enlarged compared to the scheme using drones only.																	0921-0296	1573-0409															10.1007/s10846-020-01223-y		JUN 2020											
J								Towards Higher-order OWL	KUNSTLICHE INTELLIGENZ										Description logics; OWL; higher-order logic		We summarize our ongoing endeavour towards proposing a suitable higher-order description logic that could serve as the semantic foundation for higher-order OWL, similarly toSROIQ serving as the semantic foundation of regular OWL.																	0933-1875	1610-1987				SEP	2020	34	3			SI		417	421		10.1007/s13218-020-00665-8		JUN 2020											
J								Prediction of electromagnetic field patterns of optical waveguide using neural network	NEURAL COMPUTING & APPLICATIONS										Deep learning; Electomagnetics; Field patterns	POLARIZATION ROTATOR; INVERSE DESIGN; SILICON; COMPACT; OPTIMIZATION; PHOTONICS; INDEX	Physical fields represent quantities that vary in space and/or time axes. Understanding the distribution of a field pattern is a key element in scientific discoveries and technological developments. In this article, by picking up the electromagnetic field of an optical waveguide as an example, we demonstrate how field patterns can be uncovered using artificial neural networks. The cross section plane of the optical waveguide is discretized into a set of tiny pixels, and the field values are obtained at these pixels. Deep learning model is created by assuming the field values as outputs, and the geometrical dimensions of the waveguide as inputs. The correlation between the field values in the adjacent pixels is established by mean of feedback using a recurrent neural network. The trained deep learning model enables field pattern prediction for the entire (and usual) parameter space for applications in the field of photonics.																	0941-0643	1433-3058															10.1007/s00521-020-05061-9		JUN 2020											
J								Recognition of motion patterns using accelerometers for ataxic gait assessment	NEURAL COMPUTING & APPLICATIONS										Multidimensional signal analysis; Computational intelligence; Machine learning; Accelerometers; Ataxic gait; Motion classification	CLASSIFICATION; REFLEXES; FEATURES; SENSORS	The recognition of motion patterns belongs to very important research areas related to neurology, rehabilitation, and robotics. It is based on modern sensor technologies and general mathematical methods, multidimensional signal processing, and machine learning. The present paper is devoted to the detection of features associated with accelerometric data acquired by 31 time-synchronized sensors located at different parts of the body. Experimental data sets were acquired from 25 individuals diagnosed as healthy controls and ataxic patients. The proposed method includes the application of the discrete Fourier transform for the estimation of the mean power in selected frequency bands and the use of these features for data segments classification. The study includes a comparison of results obtained from signals recorded at different positions. Evaluations are based on classification accuracy and cross-validation errors estimated by support vector machine, Bayesian, nearest neighbours (k-NN], and neural network (NN) methods. Results show that highest accuracies of 77.1%, 78.9%, 89.9%, 98.0%, and 98.5% were achieved by NN method for signals acquired from the sensors on the feet, legs, uplegs, shoulders, and head/spine, respectively, recorded in 201 signal segments. The entire study is based on observations in the clinical environment and suggests the importance of augmented reality to decisions and diagnosis in neurology.																	0941-0643	1433-3058															10.1007/s00521-020-05103-2		JUN 2020											
J								Group decision on the evaluation of outsourcing for information systems employing interval-valued hesitant fuzzy modeling	NEURAL COMPUTING & APPLICATIONS										Information systems outsourcing; Evaluation selection problems; Group decision analysis; Interval-valued hesitant fuzzy sets; Interval-valued hesitant fuzzy-utility degree	GREEN SUPPLIER SELECTION; PREFERENCE RELATIONS; RISK-ASSESSMENT; MAKING METHOD; SETS; AGGREGATION; OPERATORS; VIKOR; AHP	Organizations design procedures for information systems (ISs) management by outsourcing operations that could incorporate applications, assets, and other resources related to the IS-activities. This paper proposes a novel group decision-making approach in light of cooperative choice investigations under interval-valued hesitant fuzzy (IVHF)-uncertainty to evaluate IS-outsourcing candidates. The IVHFSs could help the IS-decision makers (DMs) by appointing interval-valued membership degrees for IS-outsourcing' exercises among the conflict factors by regarding a set to the margin of errors. For a reason, preferences judgments of IS-experts are communicated by linguistic terms, which they are changed over to IVHF-elements. In the proposed model, the weights of evaluation factors or criteria and experts are figured in light of IVHF-preference evaluation technique and IVHF-utility degree strategy. Then, criteria and IS-DMs' weights are given in the method of proposed assessing and ranking approach to decrease the errors. Furthermore, the judgments are amassed in the last step of the presented model to prevent the loss of information. At last, the proposed IVHF-group decision model is implemented to a case study to demonstrate its suitability along with comparative analysis from different aspects.																	0941-0643	1433-3058															10.1007/s00521-020-05059-3		JUN 2020											
J								Substep active deep learning framework for image classification	PATTERN ANALYSIS AND APPLICATIONS										Convolutional neural network; Active learning; Substep; Image classification	ALGORITHM; NETWORKS	In image classification, the acquisition of images labels is often expensive and time-consuming. To reduce this labeling cost, active learning is introduced into this field. Although some active learning algorithms have been proposed, they are all single-sampling strategies or combined with multiple-sampling strategies simultaneously (i.e., correlation, uncertainty and label-based measure), without considering the relationship between substep sampling strategies. To this end, we designed a new active learning scheme called substep active deep learning (SADL) for image classification. In SADL, samples were selected by correlation strategy and then determined by the uncertainty and label-based measurement. Finally, it is fed to CNN model training. Experiments were performed with three data sets (i.e., MNIST, Fashion-MNIST and CIFAR-10) to compare against state-of-the-art active learning algorithms, and it can be verified that our substep active deep learning is rational and effective.																	1433-7541	1433-755X															10.1007/s10044-020-00894-5		JUN 2020											
J								Forecasting Tourist Arrivals via Random Forest and Long Short-term Memory	COGNITIVE COMPUTATION										Tourist arrivals forecasting; Search query index; Random forest; Long short-term memory; Differential evolution algorithm	BIG DATA; DIFFERENTIAL EVOLUTION; NEURAL-NETWORKS; DEMAND; LSTM; REGRESSION; ALGORITHM; SELECTION; MODEL; INDEX	In recent years, deep learning has been attracting substantial attention due to its outstanding forecasting performance. However, the application of deep learning methods in solving the problem of forecasting tourist arrivals has been few. For the efficient allocation of tourism resources, tourist arrivals must be accurately predicted for government and tourism enterprises. In this study, a new hybrid deep learning approach is developed for tourist arrival forecasting. Random forest is used to reduce the dimensionality of the search query index data for selecting a small subset of informative features that contain the information that is most related to the tourist arrivals. Differential evolution algorithm is designed for choosing the lag lengths of each search query index and historical tourist arrival data for reconstructing the forecasting input. Long short-term memory (LSTM) is used for modeling the nonlinear relationship between tourist arrivals and search query index data. Two comparative examples, namely, Beijing City and Jiuzhaigou Valley, are applied for verification of the forecasting accuracy of the proposed deep learning method. The results indicate that the proposed deep learning method outperforms some time series and machine learning methods.																	1866-9956	1866-9964															10.1007/s12559-020-09747-z		JUN 2020											
J								Toward classifying small lung nodules with hyperparameter optimization of convolutional neural networks	COMPUTATIONAL INTELLIGENCE										computer-aided diagnosis; convolutional neural network; early diagnosis; hyperparameter optimization; lung cancer	FALSE-POSITIVE REDUCTION; PULMONARY NODULES; DIAGNOSIS; CLASSIFICATION; FEATURES; DATABASE; IMAGES	Among all cancer-related deaths, lung cancer leads all indicators, accounting for approximately 20% of all types. Patients diagnosed in the early stages have a 1-year survival rate of 81% to 85%, while in an advanced stage have 15% to 19% chances of survival. The primary manifestation of this cancer is through pulmonary nodule on computed tomography images. In the early stages, it is a complex task even for experienced specialists and presents some challenges to classify these nodules in benign or malignant. So, to assist specialists, computer-aided diagnosis systems have been used to improve the accuracy in the diagnosis. In this article, we explored and compared the use of random search, simulating annealing, and Tree-of-Parzen-estimators algorithms of hyperparameter tuning to find the best architecture of a convolutional neural network to classify small pulmonary nodules in benign or malignant with a diameter of 5 to 10 mm. Our best model used result was the model using the simulating annealing algorithm and yielded an area under the receiver operating characteristic curve of 0.95, the sensitivity of 82%, the specificity of 94%, and accuracy of 88% using a balanced data set of nodules. Therefore, our model is capable of classifying early lung nodules, where the patients have bigger chances of survival.																	0824-7935	1467-8640															10.1111/coin.12350		JUN 2020											
J								A general rule for uniqueness in self-modeling curve resolution methods	JOURNAL OF CHEMOMETRICS										chemical; physical constraints; unique solution; duality concept; self-modeling curve resolution (SMCR)	RANK-DEFICIENCY; DUALITY; CONSTRAINT	Self-modeling curve resolution (SMCR) techniques are widely applied for resolving chemical data to the pure-component spectra and composition profiles. In most circumstances, there is a range of mathematical solutions to the curve resolution problem. The mathematical solutions generated by SMCR obey the applied constraints coming from a priori physicochemical information about the system under investigation. However, several studies demonstrate that a unique solution can be obtained by implementing some constraints such as trilinearity, equality, zero concentration region, correspondence, local-rank, and non-negativity under data-based uniqueness (DBU) condition. In this research, a general rule for uniqueness (GRU) is proposed to unify all the different information that lead to a unique solution in one framework. Moreover, GRU can be a guide for developing new constraints in SMCR to get more accurate solutions. The authors are delighted to dedicate this manuscript to Professor Paul J. Gemperline in recognition of his significant contributions to the field of chemometrics. We believe that the chemometrics society's success in addressing its mission owes a great deal to his vision, passion for learning and teaching, and extensive scientific efforts over the years. We honor his friendship and generous supports.																	0886-9383	1099-128X				JUL	2020	34	7			SI				e3268	10.1002/cem.3268		JUN 2020											
J								SDM3d: shape decomposition of multiple geometric priors for 3D pose estimation	NEURAL COMPUTING & APPLICATIONS										3D pose estimation; Sparse representation model; Shape decomposition model; Multiple geometric learning	SPARSE REPRESENTATION	Recovering the 3D human pose from a single image with 2D joints is a challenging task in computer vision applications. The sparse representation (SR) model has been successfully adopted in 3D pose estimation approaches. However, since existing available training 3D data are often collected in a constrained environment (i.e., indoor) with limited diversity of subjects and actions, most SR-based approaches would have a lower generalization to real-world scenarios that may contain more complex cases. To alleviate this issue, this paper proposesSDM3d, a novel shape decomposition using multiple geometric priors for 3D pose estimation.SDM3dmakes a new attempt by separating a 3D pose into the global structure and body deformations that are encoded explicitly via different priors constraints. Furthermore, a joint learning strategy is designed to learn two over-complete dictionaries from training data to capture more geometric priors information. We have evaluatedSDM3don four well-recognized benchmarks, i.e., Human3.6M, HumanEva-I, CMU MoCap, and MPII. The experiment results show the effectiveness ofSDM3d.																	0941-0643	1433-3058															10.1007/s00521-020-05086-0		JUN 2020											
J								Aesthetical criterion in art and science	NEURAL COMPUTING & APPLICATIONS										Creativity; imagination; Self-organized systems; Originality; Personality; Neuroaesthetics	COGNITIVE NEUROSCIENCE; MODELS; COMPLEXITY; TIME	In the paper, the authors elaborate a recent research concerning the originality of artworks in terms of self-organization in the complex systems physics. It has been demonstrated that the originality issue, so conceived leads to a criterion of applied aesthetics that is not restricted to the fine arts domain, but covers also physics, biology, cosmology and other fields construed in the complex systems terms. It is a truth criterion related to the traditional personality conception, transcending gnoseological dualism of subjective and objective reality that is a characteristic of the modern humanities. The signal processing model developed in accordance with the criterion concerns neuroaesthetics indicating an opportunity of its computational foundation. Fechner's law reproduced by statistical properties of the model, that relates the outer phychophysical scaling of a stimulus to the inner psychophysical sensation scale, is a confirmation of its adequacy. The impact to psychology, education and other humanities is briefly indicated.																	0941-0643	1433-3058															10.1007/s00521-020-05065-5		JUN 2020											
J								Optimal Input Variables Disposition of Artificial Neural Networks Models for Enhancing Time Series Forecasting Accuracy	APPLIED ARTIFICIAL INTELLIGENCE											PREDICTION	Artificial Neural Networks (ANNs) models play an increasingly significant role in accurate time series prediction tools. However, an accurate time series forecasting using ANN requires an optimal model. Hence, great forecasting methods have been developed from optimized ANN models. Most of them focus more on input variables selection and preprocessing, topologies selection, optimum configuration and its associated parameters regardless of their input variables disposition. This paper provides an investigation of the effects of input variables disposition on ANNs models on training and forecasting performances. After investigation, a new ANNs optimization approach is proposed, consisting of finding optimal input variables disposition from the possible combinations. Therefore, a modified Back-Propagation neural networks training algorithm is presented in this paper. This proposed approach is applied to optimize the feed-forward and recurrent neural networks architectures; both built using traditional techniques, and pursuing to forecast the wind speed. Furthermore, the proposed approach is tested in a collaborative optimization method with single-objective optimization technique. Thus, Genetic Algorithm Back-Propagation neural networks aim to improve the forecasting accuracy relative to traditional methods was proposed. The experiment results demonstrate the requirement to take into consideration the input variables disposition to build a more optimal ANN model. They reveal that each proposed model is superior to its old considered model in terms of forecasting accuracy and thus show that the proposed optimization approach can be useful for time series forecasting accuracy improvement.																	0883-9514	1087-6545				SEP 18	2020	34	11					792	815		10.1080/08839514.2020.1782003		JUN 2020											
J								The material difference in human cognition	ADAPTIVE BEHAVIOR										Materiality; writing; stone tools; cognitive evolution; Material Engagement Theory	WILD CHIMPANZEES; BRAIN; GENUS; NUMBERS; METAPLASTICITY; RECOGNITION; CHILDREN; LANGUAGE; THINKING; SYSTEMS	Humans leverage material forms for unique cognitive purposes: We recruit and incorporate them into our cognitive system, exploit them to accumulate and distribute cognitive effort, and use them to recreate phenotypic change in new individuals and generations. These purposes are exemplified by writing, a relatively recent tool that has become highly adept at eliciting specific psychological and behavioral responses in its users, capability it achieved by changing in ways that facilitated, accumulated, and distributed incremental behavioral and psychological change between individuals and generations. Writing is described here as a self-organizing system whose design features reflect points of maximal usefulness that emerged under sustained collective use of the tool. Such self-organization may hold insights applicable to human cognitive evolution and tool use more generally. Accordingly, this article examines the emergence of the ability to leverage material forms for cognitive purposes, using the tool-using behaviors and lithic technologies of ancestral species and contemporary non-human primates as proxies for matters like collective use, generational sustainment, and the non-teleological emergence of design features.																	1059-7123	1741-2633														1059712320930738	10.1177/1059712320930738		JUN 2020											
J								Scalable and explainable legal prediction	ARTIFICIAL INTELLIGENCE AND LAW										Artificial intelligence and law; Machine learning; Human language technology; Explainable prediction	ACT	Legal decision-support systems have the potential to improve access to justice, administrative efficiency, and judicial consistency, but broad adoption of such systems is contingent on development of technologies with low knowledge-engineering, validation, and maintenance costs. This paper describes two approaches to an important form of legal decision support-explainable outcome prediction-that obviate both annotation of an entire decision corpus and manual processing of new cases. The first approach, which uses an attention network for prediction and attention weights to highlight salient case text, was shown to be capable of predicting decisions, but attention-weight-based text highlighting did not demonstrably improve human decision speed or accuracy in an evaluation with 61 human subjects. The second approach, termed semi-supervised case annotation for legal explanations, exploits structural and semantic regularities in case corpora to identify textual patterns that have both predictable relationships to case decisions and explanatory value.																	0924-8463	1572-8382															10.1007/s10506-020-09273-1		JUN 2020											
J								Populating legal ontologies using semantic role labeling	ARTIFICIAL INTELLIGENCE AND LAW										Classification; Information extraction; Ontology; Normative reasoning; Semantic role labeling; Artificial intelligence; Law	INFORMATION; CLASSIFICATION; EXTRACTION	This article seeks to address the problem of the 'resource consumption bottleneck' of creating legal semantic technologies manually. It describes a semantic role labeling based information extraction system to extract definitions and norms from legislation and represent them as structured norms in legal ontologies. The output is intended to help make laws more accessible, understandable, and searchable in a legal document management system.																	0924-8463	1572-8382															10.1007/s10506-020-09271-3		JUN 2020											
J								Online trajectory planning and control of a MAV payload system in dynamic environments	AUTONOMOUS ROBOTS										Micro aerial vehicle; Collision avoidance; Trajectory optimization; Optimal control; MAV-payload system; MPC; Motion Planning in dynamic environments	MODEL-PREDICTIVE CONTROL; OBSTACLE AVOIDANCE; QUADROTOR; STABILITY	Micro Aerial Vehicles (MAVs) can be used for aerial transportation in remote and urban spaces where portability can be exploited to reach previously inaccessible and inhospitable spaces. Current approaches for path planning of MAV swung payload system either compute conservative minimal-swing trajectories or pre-generate agile collision-free trajectories. However, these approaches have failed to address the prospect of online re-planning in uncertain and dynamic environments, which is a prerequisite for real-world deployability. This paper describes an online method for agile and closed-loop local trajectory planning and control that relies on Non-Linear Model Predictive Control and that addresses the mentioned limitations of contemporary approaches. We integrate the controller in a full system framework, and demonstrate the algorithm's effectiveness in simulation and in experimental studies. Results show the scalability and adaptability of our method to various dynamic setups with repeatable performance over several complex tasks that include flying through a narrow opening and avoiding moving humans.																	0929-5593	1573-7527				JUL	2020	44	6					1065	1089		10.1007/s10514-020-09919-8		JUN 2020											
J								Inferring 3D Shapes from Image Collections Using Adversarial Networks	INTERNATIONAL JOURNAL OF COMPUTER VISION										3D generative models; Unsupervised learning; Differentiable rendering; Adversarial networks		We investigate the problem of learning a probabilistic distribution over three-dimensional shapes given two-dimensional views of multiple objects taken from unknown viewpoints. Our approach calledprojective generative adversarial network(PrGAN) trains a deep generative model of 3D shapes whose projections (or renderings) matches the distribution of the provided 2D views. The addition of adifferentiable projection moduleallows us to infer the underlying 3D shape distribution without access to any explicit 3D or viewpoint annotation during the learning phase. We show that our approach produces 3D shapes of comparable quality to GANs trained directly on 3D data. Experiments also show that the disentangled representation of 2D shapes into geometry and viewpoint leads to a good generative model of 2D shapes. The key advantage of our model is that it estimates 3D shape, viewpoint, and generates novel views from an input image in a completely unsupervised manner. We further investigate how the generative models can be improved if additional information such as depth, viewpoint or part segmentations is available at training time. To this end, we present new differentiable projection operators that can be used to learn better 3D generative models. Our experiments show thatPrGANcan successfully leverage extra visual cues to create more diverse and accurate shapes.																	0920-5691	1573-1405				NOV	2020	128	10-11			SI		2651	2664		10.1007/s11263-020-01335-w		JUN 2020											
J								Global Path Planning and Path-Following for Wheeled Mobile Robot Using a Novel Control Structure Based on a Vision Sensor	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Path planning; Visual servoing; Soft computing; Image processing; Collision-free	PROBABILITY; NAVIGATION; ALGORITHM	This paper presents a novel design for the kinematic control structure of the wheeled mobile robot (WMR) path planning and path-following. The proposed system is focused on the implementation of practical real-time model-free algorithms based on visual servoing. The mainframe of this study is to implement a novel kinematic control structure based on visual sevoing and hybrid algorithms in real-time mobile robot applications. First, the structure of the proposed algorithm based on the visual information extracted from an overhead camera has been addressed. Then, the classification process of robot position and orientation, target, and obstacles has been addressed. Second, the path planning algorithms' initial parameters and obstacles-free path coordinates have been determined by visual information extracted from images in real time. In this step, the interval type-2 fuzzy inference (IT2FIS) algorithm and various algorithms used in path planning have been compared and their performances have been analyzed. The third stage handled the path-following process using a novel control structure for keeping up the robot on the generated path. In this step, the proposed approach is compared with fuzzy Type-1/Type-2 and fuzzy-PID control algorithms, and their results have been analyzed statistically. The proposed system has been successfully implemented on several maps. The experimental results show that the developed design is valid in generating collision-free paths efficiently and consistently and able to guide the robot to follow the path in real time.																	1562-2479	2199-3211				SEP	2020	22	6			SI		1880	1891		10.1007/s40815-020-00888-9		JUN 2020											
J								Incremental hashing with sample selection using dominant sets	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Image retrieval; Incremental hashing; Semi-supervised hashing; Concept drift; Dominant sets		In the world of big data, large amounts of images are available in social media, corporate and even personal collections. A collection may grow quickly as new images are generated at high rates. The new images may cause changes in the distribution of existing classes or the emergence of new classes, resulting in the collection being dynamic and having concept drift. For efficient image retrieval from an image collection using a query, a hash table consisting of a set of hash functions is needed to transform images into binaryhash codeswhich are used as the basis to find similar images to the query. If the image collection is dynamic, the hash table built at one time step may not work well at the next due to changes in the collection as a result of new images being added. Therefore, the hash table needs to be rebuilt or updated at successive time steps. Incremental hashing (ICH) is the first effective method to deal with the concept drift problem in image retrieval from dynamic collections. In ICH, a new hash table is learned based on newly emerging images only which represent data distribution of the current data environment. The new hash table is used to generate hash codes for all images including old and new ones. Due to the dynamic nature, new images of one class may not be similar to old images of the same class. In order to learn new hash table that preserves within-class similarity in both old and new images,incremental hashing with sample selection using dominant sets(ICHDS) is proposed in this paper, which selects representative samples from each class for training the new hash table. Experimental results show that ICHDS yields better retrieval performance than existing dynamic and static hashing methods.																	1868-8071	1868-808X				DEC	2020	11	12					2689	2702		10.1007/s13042-020-01145-z		JUN 2020											
J								A faster tensor robust PCA via tensor factorization	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS											RANK; RECOVERY; MODELS	Many kinds of real-world multi-way signal, like color images, videos, etc., are represented in tensor form and may often be corrupted by outliers. To recover an unknown signal tensor corrupted by outliers, tensor robust principal component analysis (TRPCA) serves as a robust tensorial modification of the fundamental PCA. Recently, a successful TRPCA model based on the tubal nuclear norm (TNN) (Lu et al. in IEEE Trans Pattern Anal Mach Intell 42:925-938, 2019) has attracted much attention thanks to its superiority in many applications. However, TNN is computationally expensive due to the requirement of full singular value decompositions, seriously limiting its scalability to large tensors. To address this issue, we propose a new TRPCA model which adopts a factorization strategy. Algorithmically, an algorithm based on the non-convex augmented Lagrangian method is developed with convergence guarantee. Theoretically, we rigorously establish the sub-optimality of the proposed algorithm. We also extend the proposed model to the robust tensor completion problem. Both the effectiveness and efficiency of the proposed algorithm is demonstrated through extensive experiments on both synthetic and real data sets.																	1868-8071	1868-808X				DEC	2020	11	12					2771	2791		10.1007/s13042-020-01150-2		JUN 2020											
J								Benchmarking algorithms for food localization and semantic segmentation	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Benchmarking; Convolutional neural network; Food localization; Food segmentation	CLASSIFICATION; RECOGNITION; RETRIEVAL; FEATURES	The problem of food segmentation is quite challenging since food is characterized by intrinsic high intra-class variability. Also, segmentation of food images taken in-the-wild may be characterized by acquisition artifacts, and that could be problematic for the segmentation algorithms. A proper evaluating of segmentation algorithms is of paramount importance for the design and improvement of food analysis systems that can work in less-than-ideal real scenarios. In this paper, we evaluate the performance of different deep learning-based segmentation algorithms in the context of food. Due to the lack of large-scale food segmentation datasets, we initially create a new dataset composed of 5000 images of 50 diverse food categories. The images are accurately annotated with pixel-wise annotations. In order to test the algorithms under different conditions, the dataset is augmented with the same images but rendered under different acquisition distortions that comprise illuminant change, JPEG compression, Gaussian noise, and Gaussian blur. The final dataset is composed of 120,000 images. Using standard benchmark measures, we conducted extensive experiments to evaluate ten state-of-the-art segmentation algorithms on two tasks: food localization and semantic food segmentation.																	1868-8071	1868-808X				DEC	2020	11	12					2827	2847		10.1007/s13042-020-01153-z		JUN 2020											
J								A new framework of multi-objective evolutionary algorithms for feature selection and multi-label classification of video data	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Multi-label classification; Multi-objective optimization; Evolutionary; Machine learning; Feature selection	GENETIC ALGORITHM; OPTIMIZATION	There are few studies in the literature to address the multi-objective multi-label feature selection for the classification of video data using evolutionary algorithms. Selecting the most appropriate subset of features is a significant problem while maintaining/improving the accuracy of the prediction results. This study proposes a framework of parallel multi-objective Non-dominated Sorting Genetic Algorithms (NSGA-II) for exploring a Pareto set of non-dominated solutions. The subsets of non-dominated features are extracted and validated by multi-label classification techniques, Binary Relevance (BR), Classifier Chains (CC), Pruned Sets (PS), and Random k-Labelset (RAkEL). Base classifiers such as Support Vector Machines (SVM), J48-Decision Tree (J48), and Logistic Regression (LR) are performed in the classification phase of the algorithms. Comprehensive experiments are carried out with local feature descriptors extracted from two multi-label data sets, the well-known MIR-Flickr dataset and a Wireless Multimedia Sensor (WMS) dataset that we have generated from our video recordings. The prediction accuracy levels are improved by 6.36% and 25.7% for the MIR-Flickr and WMS datasets respectively while the number of features is significantly reduced. The results verify that the algorithms presented in this new framework outperform the state-of-the-art algorithms.																	1868-8071	1868-808X															10.1007/s13042-020-01156-w		JUN 2020											
J								Fuzzy unordered rule induction algorithm based classification for reliable communication using wearable computing devices in healthcare	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Healthcare monitoring; Wearable computing devices (WCD); Reliability; Fuzzy logic	WIRELESS SENSOR NETWORKS; BODY AREA NETWORKS; ANOMALY DETECTION; OUTLIER DETECTION	Wireless sensor nodes that can be strategically located across the human body to create a network for various types of healthcare applications such as a network is known as wearable computing devices (WCDs). Robust treatment is provided to the patient in this network, to maintain stable patient status. The coordination in the effective route needs to be improved. Patient care should be maintained in WCD and communicated more in a reliable manner. In general data from such a network is vulnerable to attacks/misbehavior. Hence it is warranted to detect and introduce methods for sustaining the reliability of the network. Data classification methods by selecting classification algorithms, fuzzy unordered rule induction algorithm (FURIA) have been introduced in this research as a possible solution to address the problem. An attempt has also been made to detect faulty measurements while collecting data from the WCD and the data has been securely transmitted through fuzzy logic technique. The main objective of this research is to introduce alarming techniques when the patient goes critical. The proposed FURIA based classification and linear regression algorithm outperforms existing methods.																	1868-5137	1868-5145															10.1007/s12652-020-02219-0		JUN 2020											
J								k-coveragem-connected node placement using shuffled frog leaping: Nelder-Mead algorithm in WSN	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Target based WSN; Connectivity; Coverage; Shuffled frog leaping algorithm; Nelder-Mead		In wireless sensor network (WSN) target-based coverage plays a vital role in forwarding the information from source to destination via multiple sensors by covering the given targets in location specific wise. The coverage of targets and connectivity among the sensors are the two most addressable issues that are to be considered for effective data transmission. In this paper, a mathematical model called Nelder-Mead method is imposed with shuffled frog leaping algorithm for improvised local search to deploy the sensors to cover the given targets without violating the constraints in coverage as well as connectivity. The proposed algorithm is evaluated with standard performance metrics and compared with the existing algorithms and shows the significance of the proposed algorithm.																	1868-5137	1868-5145															10.1007/s12652-020-02223-4		JUN 2020											
J								Design of ANFIS controller for intelligent energy management in smart grid applications	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Adaptive neuro fuzzy inference system (ANFIS); Energy management system (EMS); Maximum power point tracking (MPPT); Photo voltaic (PV); Solid oxide fuel cell (SOFC); Smart grid (SG)	MODEL; SYSTEM	Processing of information in minimum time duration with maximum accuracy is the prime factor for efficient functioning of any network. A smart grid (SG) is a network structure that supplies electricity using digital communication technology by the application of computer intelligence for the purpose of control and automation of various components like smart meters, smart appliances and renewable energy resources connected to it. A hybrid power system (HPS) is one which has multiple power generating sources like photo voltaic (PV) system, Wind turbine, fuel cell etc. interconnected to supply electric power for varying demand requirements with/without energy storage backup which is the key component of a SG. This paper focuses on the integration and control automation of renewable energy sources viz. PV system, solid oxide fuel cell (SOFC) with nickel-metal-hydride (Ni-MH) battery together with a variable load present in a SG. The proposed energy management system (EMS) used in the designed HPS focuses on the use of PV which is 100% clean in nature with no toxic emissions on power generation. Here, PV system with maximum power point tracking (MPPT) is used as the major supply contributor in the HPS to meet with variable load demands. If there is deficit of power supply from PV, the power from the Ni-MH battery/SOFC is utilized to meet with the varying load demands. On the other hand, if there is excess supply from PV system, the excess energy will be stored in the Ni-MH battery. For efficient supply demand balance, the EMS makes use of various control strategies namely proportional-integral (PI) and adaptive neuro fuzzy inference system (ANFIS).																	1868-5137	1868-5145															10.1007/s12652-020-02180-y		JUN 2020											
J								CGSPN : cascading gated self-attention and phrase-attention network for sentence modeling	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Sentence modeling; Gated self-attention; CNN; Phrase-attention mechanism; NLP	CLASSIFICATION	Sentence modeling is a critical issue for the feature generation of some natural language processing (NLP) tasks. Recently, most works generated the sentence representation by sentence modeling based on Convolutional Neural Network (CNN), Long Short-Term Memory network (LSTM) and some attention mechanisms. However, these models have two limitations: (1) they only present sentences for one individual task by fine-tuning network parameters, and (2) sentence modeling only considers the concatenation of words and ignores the function of phrases. In this paper, we propose a Cascading Gated Self-attention and Phrase-attention Network (CGSPN) that generates the sentence embedding by considering contextual words and key phrases in a sentence. Specifically, we first present a word-interaction gating self-attention mechanism to identify some important words and build the relationship between words. Then, we cascade a phrase-attention structure by abstracting the semantic of phrases to generate the sentence representation. Experiments on different NLP tasks show that the proposed CGSPN model achieves the highest accuracy among most sentence encoding methods. It improves the latest best result by 1.76% on the Stanford Sentiment Treebank (SST), and shows the best test accuracy on different sentence classification data sets. In the Natural Language Inference (NLI) task, the performance of CGSPN without phrase-attention is better than CGSPN model itself and it obtains competitive performance against state-of-the-art baselines, which show the different applicability of the proposed model. In other NLP tasks, we also compare our model with popular methods to explore our direction.																	0925-9902	1573-7675															10.1007/s10844-020-00610-z		JUN 2020											
J								Trajectory clustering method based on spatial-temporal properties for mobile social networks	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Trajectory clustering; Spatial-temporal properties; Spatial distances; Semantic distances		As an important issue in the trajectory mining task, the trajectory clustering technique has attracted lots of the attention in the field of data mining. Trajectory clustering technique identifies the similar trajectories (or trajectory segments) and classifies them into the several clusters which can reveal the potential movement behaviors of nodes. At present, most of the existing trajectory clustering methods focus on some spatial properties of trajectories (such as geographic locations, movement directions), while the spatial-temporal properties (especially the combination of spatial distances and semantic distances) are ignored, and thus some vital information regarding the movement behaviors of nodes is probably lost in the trajectory clustering results. In this paper, we propose a Joint Spatial-Temporal Trajectory Clustering Method (JSTTCM), where some spatial-temporal properties of the trajectories are exploited to cluster the trajectory segments. Finally, the number of clusters and the silhouette coefficient are observed through simulations, and the results show that JSTTCM can cluster the trajectory segments appropriately.																	0925-9902	1573-7675															10.1007/s10844-020-00607-8		JUN 2020											
J								Online monitoring and control of a cyber-physical manufacturing process under uncertainty	JOURNAL OF INTELLIGENT MANUFACTURING										Cyber-manufacturing; Cyber-physical; Monitoring; Control; Uncertainty; Bayesian network	REAL-TIME PROCESS; BAYESIAN NETWORKS; PRECISION; MODEL; RELIABILITY; PERFORMANCE; PREDICTION; QUALITY; SYSTEM; SENSOR	Recent technological advancements in computing, sensing and communication have led to the development of cyber-physical manufacturing processes, where a computing subsystem monitors the manufacturing process performance in real-time by analyzing sensor data and implements the necessary control to improve the product quality. This paper develops a predictive control framework where control actions are implemented after predicting the state of the manufacturing process or product quality at a future time using process models. In a cyber-physical manufacturing process, the product quality predictions may be affected by uncertainty sources from the computing subsystem (resource and communication uncertainty), manufacturing process (input uncertainty, process variability and modeling errors), and sensors (measurement uncertainty). In addition, due to the continuous interactions between the computing subsystem and the manufacturing process, these uncertainty sources may aggregate and compound over time. In some cases, some process parameters needed for model predictions may not be precisely known and may need to be derived from real time sensor data. This paper develops a dynamic Bayesian network approach, which enables the aggregation of multiple uncertainty sources, parameter estimation and robust prediction for online control. As the number of process parameters increase, their estimation using sensor data in real-time can be computationally expensive. To facilitate real-time analysis, variance-based global sensitivity analysis is used for dimension reduction. The proposed methodology of online monitoring and control under uncertainty, and dimension reduction, are illustrated for a cyber-physical turning process.																	0956-5515	1572-8145															10.1007/s10845-020-01609-7		JUN 2020											
J								Trajectory Planning of Autonomous Vehicles Based on Parameterized Control Optimization in Dynamic on-Road Environments	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Autonomous vehicles; Trajectory planning; Control parameterization; Moving objects	TRACKING CONTROL; GENERATION; AVOIDANCE	This paper presents a trajectory planning framework to deal with the highly dynamic environments for on-road driving. The trajectory optimization problem with parameterized curvature control was formulated to reach the goal state with the vehicle model and its dynamic constraints considered. This in contrast to existing curve fitting techniques guarantees the dynamic feasibility of the planned trajectory. With generation of multiple trajectory candidates along the Frenet frame, the vehicle is reactive to other road users or obstacles encountered. Additionally, to deal with more complex driving scenarios, its seamless interaction with an upper behavior planning layer was considered by having longitudinal motion planning responsive to the desired goal state. The trajectory evaluation and selection methodologies, along with the low-level tracking control, were also developed under this framework. The potential of the proposed trajectory planning framework was demonstrated under different dynamic driving scenarios such as lane-changing or merging with surrounding vehicles with its computation efficiency proven in real-time simulations.																	0921-0296	1573-0409															10.1007/s10846-020-01215-y		JUN 2020											
J								Dual-mode power reduction technique for real-time image and video processing board	JOURNAL OF REAL-TIME IMAGE PROCESSING										Dual-mode logic (DML); Self-controllable voltage level (SVL); Full adder; Ripple carry adder (RCA); Power leakage; Real-time image and video processing boards	TECHNOLOGY; CIRCUIT	In real-time image and video processing boards, power, speed, and area are the most often used measures for determining the performance of motion imagery applications. Due to technological advancement, power consumption has gained major attention in real-time image processing ability compared to speed. The increase in on-chip temperature due to larger power consumption has resulted in reduced operating life of chip and battery-driven devices. In this work, a new logic family has been introduced i.e., dual-mode logic (DML), which provides flexibility between the optimization of energy and delay (E-D optimization). This gate can be switched between two modes of operation that is a static mode (CMOS-like mode), which provides low power consumption and dynamic mode, which provides high speed. Recently, power leakage has become a dominant problem due to continuous data transfer among a large number of connected devices. Thus, to reduce power leakage, a self-controllable voltage level (SVL) power reduction technique is used along with DML logic. In the SVL technique, a maximum dc voltage is provided to the active load circuit on-demand or decrease the dc supplied to the load circuit in the standby mode. Integrating DML with the SVL technique reduces power consumption as well as leakage power. A 4-bit RCA, 8-bit RCA, and 16-bit RCA are used for verifying the proposed method and comparison of performance parameters is done with a conventional circuit. Complete circuit implementation and simulation are carried out in TANNER EDA version 13 tools with operating voltage of 1 V. The proposed system is further applied to real-time image, and we obtain the finest resolution level with minimum power consumption.																	1861-8200	1861-8219															10.1007/s11554-020-00992-x		JUN 2020											
J								Mining evolutions of complex spatial objects using a single-attributed Directed Acyclic Graph	KNOWLEDGE AND INFORMATION SYSTEMS										Graph mining; Spatiotemporal data; Attributed DAG; Weighted path; Environmental monitoring	SEQUENTIAL PATTERNS; FREQUENT PATTERNS; ALGORITHM; SETS	Directed acyclic graphs (DAGs) are used in many domains ranging from computer science to bioinformatics, including industry and geoscience. They enable to model complex evolutions where spatial objects (e.g., soil erosion) may move, (dis)appear, merge or split. We study a new graph-based representation, called attributed DAG (a-DAG). It enables to capture interactions between objects as well as information on objects (e.g., characteristics or events). In this paper, we focus on pattern mining in such data. Our patterns, called weighted paths, offer a good trade-off between expressiveness and complexity. Frequency and compactness constraints are used to filter out uninteresting patterns. These constraints lead to an exact condensed representation (without loss of information) in the single-graph setting. A depth-first search strategy and an optimized data structure are proposed to achieve the efficiency of weighted path discovery. It does a progressive extension of patterns based on database projections. Relevance, scalability and genericity are illustrated by means of qualitative and quantitative results when mining various real and synthetic datasets. In particular, we show how such an approach can be used to monitor soil erosion using remote sensing and geographical information system (GIS) data.																	0219-1377	0219-3116				OCT	2020	62	10					3931	3971		10.1007/s10115-020-01478-9		JUN 2020											
J								PKM3: an optimal Markov model for predicting future navigation sequences of the web surfers	PATTERN ANALYSIS AND APPLICATIONS										Web; All-Kth modified; Markov model; Error; Pruned; State; Path; Accuracy; Navigation; Prediction		Predicting the browsing behavior of the user on the web has gained significant importance, as it improves the productivity of the website owners and also raises the interest of web users. The Markov model has been used immensely for user's web navigation prediction. To enhance the coverage and accuracy of the Markov model, higher order Markov models are integrated with lower order models. However, this integration results in large state-space complexity. To reduce the state-space complexity, this paper proposes a novel technique, namely Pruned all-Kth modified Markov model (PKM3). PKM3 eliminates the irrelevant states from a higher order model, which have a negligible contribution toward prediction. The proposed model is evaluated on four standard weblogs: BMS, MSWEB, CTI and MSNBC. PKM3 performance was optimal for the website in which pages were closely placed and share high interlinking. This pruning-based optimal model achieves a significant reduction in state-space complexity while maintaining comparable accuracy.																	1433-7541	1433-755X															10.1007/s10044-020-00892-7		JUN 2020											
J								Learning latent hash codes with discriminative structure preserving for cross-modal retrieval	PATTERN ANALYSIS AND APPLICATIONS										Cross-modal retrieval; Hashing; Structure preserving	INFORMATION; EXTRACTION; SPACE	Due to the low storage cost and computational efficiency, hashing approaches have drawn considerable interest and gained great success in multimodal retrieval. However, most existing works study the local geometric structure in the original space, which suffers from intra- and inter-modality ambiguity, resulting in low discriminative hash codes. To address this issue, we propose a novel cross-modal hashing approach by taking inter- and intra-modality structure preserving into consideration, dubbed discriminative structure preserving hashing (DSPH). Specifically, DSPH explores the intra- and inter-modality in the latent structure of the constructed common space. In addition, the local geometric consistency is improved by a supervised shrinking scheme. DSPH learns the hash codes and latent features based on factorization coding scheme. The objective function includes common latent subspace learning and inter- & intra-modality structure embedding. We devise an alternative optimization scheme, where the hash codes are solved by a bitwise scheme, and the large quantization error can be avoided. Owing to the merit of DSPH, more discriminative hash codes can be generated. The extensive experimental results on several widely used databases demonstrate that the proposed algorithm outperforms several state-of-art cross-media retrieval methods.																	1433-7541	1433-755X															10.1007/s10044-020-00893-6		JUN 2020											
J								TSASC: tree-seed algorithm with sine-cosine enhancement for continuous optimization problems	SOFT COMPUTING										Continuous optimization problem; Tree-seed algorithm (TSA); Sine-cosine algorithm (SCA); Swarm intelligence	PARTICLE SWARM; IDENTIFICATION; SEARCH	Tree-seed algorithm (TSA) establishes a novel approach to solve continuous optimization problems, which is applied in many fields because of its simplicity and strength in finding optimal solutions. However, due to somewhat imbalance of its ability between exploration and exploitation in different search phases, the exploratory capability of TSA is relatively weak in optimizing multimodal and high-dimensional objective functions. To make some improvements, we propose a hybrid heuristic tree-seed algorithm named TSASC by integrating two features from sine-cosine algorithm. The proposed algorithm is then tested in comparison with TSA and other relevant algorithms through 30 benchmark functions from IEEE CEC 2014 and 3 constrained real engineering optimization problems. The results prove its enhanced balance between exploration and exploitation in both finding better global optimal solutions and effectively avoiding falling into local optimum, which shows that it has promising advantages in solving continuous optimization problems in engineering practices.																	1432-7643	1433-7479															10.1007/s00500-020-05099-w		JUN 2020											
J								Using fuzzy logics to determine optimal oversampling factor for voxelizing 3D surfaces in radiation therapy	SOFT COMPUTING										Voxelization; Fuzzy logics; Radiation therapy; Dose-volume histogram	QUALITY-ASSURANCE; ASSOCIATION; PARAMETERS; CANCER	Voxelizing three-dimensional surfaces into binary image volumes is a frequently performed operation in medical applications. In radiation therapy (RT), dose-volume histograms (DVHs) calculated within such surfaces are used to assess the quality of an RT treatment plan in both clinical and research settings. To calculate a DVH, the 3D surfaces need to be voxelized into binary volumes. The voxelization parameters may considerably influence the output DVH. An effective way to improve the quality of the voxelized volume (i.e., increasing similarity between that and the original structure) is to apply oversampling to increase the resolution of the output binary volume. However, increasing the oversampling factor raises computational and storage demand. This paper introduces a fuzzy inference system that determines an optimal oversampling factor based on relative structure size and complexity, finding the balance between voxelization accuracy and computation time. The proposed algorithm was used to automatically calculate oversampling factor in four RT studies: two phantoms and two real patients. The results show that the method is able to find the optimal oversampling factor in most cases, and the calculated DVHs show good match to those calculated using manual overall oversampling of two. The algorithm can potentially be adopted by RT treatment planning systems based on the open-source implementation to maintain high DVH quality, enabling the planning system to find the optimal treatment plan faster and more reliably.																	1432-7643	1433-7479															10.1007/s00500-020-05126-w		JUN 2020											
J								A hybrid of the simplicial partition-based Bayesian global search with the local descent	SOFT COMPUTING										Global optimization; Stochastic models; Bayesian optimization; Local descent	OPTIMIZATION; ALGORITHM	We propose a global optimization algorithm hybridizing a version of Bayesian global search with local minimization. The implementation of Bayesian algorithm is based on the simplician partition of the feasible region. Our implementation is free from the typical computational complexity of the standard implementations of Bayesian algorithms. The local minimization counterpart improves the efficiency of search in the indicated potential basins of global minimum. The performance of the proposed algorithm is illustrated by the results of a numerical experiment.																	1432-7643	1433-7479															10.1007/s00500-020-05095-0		JUN 2020											
J								Using search-based techniques for testing executable software models specified through graph transformations	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Model testing; Graph transformation specification; Specification testing; Design by contract; Coverage criteria	GENERATION; CHECKING; CHECKERS	Design by contract is a software development methodology that uses contracts for defining interfaces among interacting components of a software system. Graph transformation system is used to specify the behavioral aspects of software components by defining the pre- and post-conditions of methods as contracts. In this paper, we focus on testing executable software models specified by a graph transformation system. A set of model-specific coverage criteria and a cost-aware search-based test generation approach are introduced. To evaluate the effectiveness of the proposed coverage criteria and the test generation approach, a type of mutation analysis is presented at the model level. Furthermore, a couple of fault-detection methods are used to assess the quality of the generated tests in the model-level mutation analysis. The proposed approach is implemented in GROOVE, a toolset for model checking graph transformation systems. The empirical results based on some well-known case studies demonstrate the efficiency and scalability of each proposed coverage criterion and testing approach. The comparison of the proposed test generation approach with state-of-the-art techniques indicates a significant improvement in terms of fault-detection capability and testing costs.																	1868-8071	1868-808X				DEC	2020	11	12					2743	2770		10.1007/s13042-020-01149-9		JUN 2020											
J								Constructive Decision via Redundancy-Free Proof-Search	JOURNAL OF AUTOMATED REASONING										Constructive decidability; Relevance logic; Sequent calculi; Contraction rule; Redundancy-free search; Almost full relations; Mechanization in Coq	ENTAILMENT; THEOREM	We give a constructive account of Kripke-Curry's method which was used to establish the decidability of implicational relevance logic (R-->). To sustain our approach, we mechanize this method in axiom-free Coq, abstracting away from the specific features of R--> to keep only the essential ingredients of the technique. In particular we show how to replace Kripke/Dickson's lemma by a constructive form of Ramsey's theorem based on the notion of almost full relation. We also explain how to replace Konig's lemma with an inductive form of Brouwer's Fan theorem. We instantiate our abstract proof to get a constructive decision procedure for R--> and discuss potential applications to other logical decidability problems.																	0168-7433	1573-0670				OCT	2020	64	7			SI		1197	1219		10.1007/s10817-020-09555-y		JUN 2020											
J								Root identification in minirhizotron imagery with multiple instance learning	MACHINE VISION AND APPLICATIONS										Image segmentation; Multiple instance learning; Minirhizotron image; Image processing; Plant roots; Imprecise labels	ARCHITECTURE; YIELD	In this paper, multiple instance learning (MIL) algorithms to automatically perform root detection and segmentation in minirhizotron imagery using only image-level labels are proposed. Root and soil characteristics vary from location to location, and thus, supervised machine learning approaches that are trained with local data provide the best ability to identify and segment roots in minirhizotron imagery. However, labeling roots for training data (or otherwise) is an extremely tedious and time-consuming task. This paper aims to address this problem by labeling data at the image level (rather than the individual root or root pixel level) and train algorithms to perform individual root pixel level segmentation using MIL strategies. Three MIL methods (multiple instance adaptive cosine coherence estimator, multiple instance support vector machine, multiple instance learning with randomized trees) were applied to root detection and compared to non-MIL approaches. The results show that MIL methods improve root segmentation in challenging minirhizotron imagery and reduce the labeling burden. In our results, multiple instance support vector machine outperformed other methods. The multiple instance adaptive cosine coherence estimator algorithm was a close second with an added advantage that it learned an interpretable root signature which identified the traits used to distinguish roots from soil and did not require parameter selection.																	0932-8092	1432-1769				JUN 24	2020	31	6							43	10.1007/s00138-020-01088-z													
J								Analysing rumours spreading considering self-purification mechanism	CONNECTION SCIENCE										Media management; self-purification mechanism; rumours spreading; threshold value	MODEL	Rumours spreading in nowadays highly connected society is an interesting and important topic in studying human behaviour and communication networks dynamics. As rumours propagation or evolution with respect to time is highly submitted to networks users' dynamics, rumours propagation in a targeted community may inherit self-purification mechanism due to human ability to judge and clarify allegations and assumptions not based on truth. In this article, we analysed rumours spreading through social media taking into account self-purification mechanism using epidemic control approach. Rumours spreading threshold value was obtained. Results of numerical simulation validated theoretical analysis and suggested controlling social media influence on potential rumours purifiers and disseminators.																	0954-0091	1360-0494															10.1080/09540091.2020.1783640		JUN 2020											
J								Research on commercial logistics inventory forecasting system based on neural network	NEURAL COMPUTING & APPLICATIONS										Neural network; Commercial logistics; Warehouse; Inventory; Forecast		Logistics cost control is an important means to increase commercial logistics profits and control corporate capital risks. This paper uses BP neural network to build a prediction model to study the analysis of inventory demand and explores the complex relationship between inventory demand and each influencing factor by training the data of inventory demand influencing factors to obtain effective measures for inventory management and control. Moreover, this article chooses the way that BA optimizes the BP neural network to build a predictive model and uses the actual data to conduct a model test to verify the validity of the model. In addition, this article performs performance analysis of the prediction model of this study by setting up experiments. Through comparative experimental research, we can see that the method proposed in this paper has certain effects, can be applied to the actual forecast of logistics inventory, and can provide theoretical references for subsequent related research.																	0941-0643	1433-3058															10.1007/s00521-020-05090-4		JUN 2020											
J								NaNOD: A natural neighbour-based outlier detection algorithm	NEURAL COMPUTING & APPLICATIONS										Outlier detection; Natural neighbour; Kernel density estimation; Adaptive kernel width		Outlier detection is an essential task in data mining applications which include, military surveillance, tax fraud detection, telecommunication, etc. In recent years, outlier detection received significant attention compared to other problem of discoveries. The focus on this has resulted in the growth of several outlier detection algorithms, mostly concerning the strategy based on distance or density. However, each strategy has intrinsic weaknesses. The distance-based techniques have the problem of local density, while the density-based method is recognized as having an issue of a low-density pattern. Also, most of the existing outlier detection algorithms have a parameter selection problem, which leads to poor detection results. In this article, we present an unsupervised density-based outlier detection algorithm to deal with these shortcomings. The proposed algorithm uses a Natural Neighbour (NaN) concept, to obtain a parameter called Natural Value (NV) adaptively, and a Weighted Kernel Density Estimation (WKDE) method to estimate the density at the location of an object. Besides, our proposed algorithm employed two different categories of nearest neighbours, k Nearest Neighbours (kNN), and Reverse Nearest Neighbours (RNN), which make our system flexible in modelling different data patterns. A Gaussian kernel function is adopted to achieve smoothness in the measure. Further, we use an adaptive kernel width concept to enhance the discrimination power between normal and outlier samples. The formal analysis and extensive experiments carried out on both artificial and real datasets demonstrate that this technique can achieve better outlier detection performance.																	0941-0643	1433-3058															10.1007/s00521-020-05068-2		JUN 2020											
J								Optimizing deep learning and neural network to explore enterprise technology innovation model	NEURAL COMPUTING & APPLICATIONS										Deep learning; Neural network; Enterprise technology innovation; Model simulation		The technological innovation capabilities of enterprises have a greater impact on regional economic development, and the evaluation of technological innovation capabilities of enterprises has a certain guiding role in the formulation of regional economic development plans. From the current situation, it can be seen that the technological innovation model of enterprises is affected by various factors. Based on deep learning and neural network technology, this research takes a high-tech park as an example to comprehensively evaluate the innovation ability of the high-tech park. The use of predicted values in some evaluation indicators can better reflect the innovation ability. At the same time, this research uses the historical GDP data of the park to predict future values and uses the predicted values as indicators of innovation ability. Finally, the idea of combined forecasting is used to comprehensively evaluate the technological innovation capability of enterprises. The research results show that the method proposed in this paper has certain predictive power, can provide guidance for regional economic development, and can provide theoretical references for subsequent related research.																	0941-0643	1433-3058															10.1007/s00521-020-05106-z		JUN 2020											
J								Anomaly detection in dynamic attributed networks	NEURAL COMPUTING & APPLICATIONS										Dynamic attributed graphs; Anomaly detection; Latent network; Cascade data		Graph anomaly detection plays a central role in many emerging network applications, ranging from cloud intrusion detection to online payment fraud detection. It has been studied under the contexts of dynamic graphs and attributed graphs separately. In many practical applications, graphs with dynamic attributes provide crucial information for such detections, yet there exist few studies on discovering anomalies in dynamic attributed graphs. Therein, we present a novel framework for defining and detecting anomalies in dynamic attributed graphs, where communities and outliers are mined in succession. Specifically, we propose a community detection model that combines dynamic graph clustering with learning of latent network structures into a unified optimization problem. The latent network is inferred from cascade data, and our model can sufficiently incorporate time-dependent attribute information for identifying dynamic changes. We then rank all the graph nodes according to their deviance from both their closest community centers and historical behaviors. Experiments on synthetic and real-world datasets illustrate the effectiveness of our model.																	0941-0643	1433-3058															10.1007/s00521-020-05091-3		JUN 2020											
J								Improving coalition structure search with an imperfect algorithm: analysis and evaluation results	ARTIFICIAL INTELLIGENCE REVIEW										Coalition structure generation; Dynamic programming; Coalition formation; Imperfect algorithm	STRUCTURE GENERATION; TASK ALLOCATION	Optimal Coalition Structure Generation (CSG) is a significant research problem in multi-agent systems that remains difficult to solve. This problem has many important applications in transportation, eCommerce, distributed sensor networks and others. The CSG problem is NP-complete and finding the optimal result for n agents needs to check O(n(n)) possible partitions. The ODP-IP algorithm (Michalak et al. in Artif Intell 230:14-50, 2016) achieves the current lowest worst-case time complexity of O(3(n)). In the light of its high computational time complexity, we devise an Imperfect Dynamic Programming (ImDP) algorithm for the CSG problem with runtime O(n2(n)) given n agents. Imperfect algorithm means that there are some contrived inputs for which the algorithm fails to give the optimal result. We benchmarked ImDP against ODP-IP and proved its efficiency. Experimental results confirmed that ImDP algorithm performance is better for several data distributions, and for some it improves dramatically ODP-IP. For example, given 27 agents, with ImDP for agent-based uniform distribution time gain is 91% (i.e. 49 min).																	0269-2821	1573-7462															10.1007/s10462-020-09850-5		JUN 2020											
J								Large-scale network motif analysis using compression	DATA MINING AND KNOWLEDGE DISCOVERY										Motifs; Network analysis; Minimum description length	ALGORITHM	We introduce a new method for findingnetwork motifs. Subgraphs are motifs when their frequency in the data is high compared to the expected frequency under anull model. To compute this expectation, a full or approximate count of the occurrences of a motif is normally repeated on as many as 1000 random graphs sampled from the null model; a prohibitively expensive step. We use ideas from the minimum description length literature to define a new measure of motif relevance. With our method, samples from the null model are not required. Instead we compute the probability of the data under the null model and compare this to the probability under a specially designed alternative model. With this new relevance test, we can search for motifs by random sampling, rather than requiring an accurate count of all instances of a motif. This allows motif analysis to scale to networks with billions of links.																	1384-5810	1573-756X				SEP	2020	34	5			SI		1421	1453		10.1007/s10618-020-00691-y		JUN 2020											
J								Group Decision-Making Based on Set Theory and Weighted Geometric Operator with Interval Rough Multiplicative Reciprocal Matrix	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Interval rough multiplicative reciprocal matrix; Consistency; Uniform approximation matrix; Group decision-making	FUZZY PREFERENCE-RELATION; INFORMATION; FRAMEWORK; RANKING	Interval rough numbers play an important role in dealing with complex fuzzy relationships. In this paper, a group decision-making (GDM) model based on interval rough multiplicative reciprocal (IRMR) matrix is proposed. Firstly, the inconsistency, satisfactory consistency and complete consistency of the IRMR matrix are defined from the perspective of set theory. Secondly, an improved method for the inconsistent IRMR matrix is introduced to address the inconsistent preference matrix in GDM. We define the uniform approximation matrix of the IRMR matrix, prove its existence, and provide a new calculation method for the sorting vector of IRMR matrix. Finally, the multiplicative reciprocal matrix obtained with a weighted geometric operator assembly is still the IRMR matrix. A GDM algorithm of the IRMR matrix is presented. The proposed algorithm is demonstrated using an illustrative example, and its feasibility and effectiveness are verified through comparison with other existing methods.																	1562-2479	2199-3211				SEP	2020	22	6			SI		1815	1831		10.1007/s40815-020-00900-2		JUN 2020											
J								Fuzzy Quantized Control of Nonstrict Feedback Nonlinear Systems with Actuator Faults	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Adaptive tracking control; Nonstrict feedback nonlinear systems; Quantized inputs; Actuator faults	TOLERANT CONTROL; CONTROL SCHEME; STATE-FEEDBACK; INPUT; DESIGN	This paper studies the adaptive tracking control problem for a class of nonstrict feedback nonlinear systems with quantized inputs and actuator faults. Compared with existing works on quantized control problem, the asymmetric hysteresis quantizer is considered in the actuator failure problem in this paper. To resolve the control challenge caused by the quantization effect, a nonlinear decomposition of the quantizer is proposed and applied to the controller design in the last step of the backstepping-based control approach. Combining with fuzzy logic systems, the unknown nonlinear functions contained in the researched nonlinear system is disposed without knowing some restrictive conditions of uncertainties. The developed controller guarantees that the system output converges to a small neighborhood of the desired reference signal and all the signals of the closed-loop system are bounded. Finally, simulation results are depicted to illustrate the efficiency of the proposed control algorithm.																	1562-2479	2199-3211				SEP	2020	22	6			SI		1922	1936		10.1007/s40815-020-00891-0		JUN 2020											
J								Improved Type2-NPCM Fuzzy Clustering Algorithm Based on Adaptive Particle Swarm Optimization for Takagi-Sugeno Fuzzy Modeling Identification	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Fuzzy clustering; Takagi-Sugeno fuzzy modeling; Improved adaptive particle swarm optimization (IAPSO); NPCM algorithm; Type2-NPCM clustering algorithm; Type2-NPCM-IAPSO algorithm	SETS	In this paper, an improved Type2-NPCM clustering algorithm based on improved adaptive particle swarm optimization called Type2-NPCM-IAPSO is proposed. First, a new clustering algorithm called Type2-NPCM is proposed. The Type2-NPCM algorithm can solve the problems encountered by the algorithms FCM, G-K, PCM and NPCM (sensitivity to noise or aberrant points and local minimal sensitivity), etc. Second, we combined our Type2-NPCM algorithm with the improved adaptive particle swarm optimization IAPSO algorithm to ensure proper convergence to a local minimum of the objective function. The effectiveness of the proposed Type2-NPCM-IAPSO algorithm was tested on the electro-hydraulic system, convection system and other nonlinear systems described by differential equation.																	1562-2479	2199-3211				SEP	2020	22	6			SI		2011	2024		10.1007/s40815-020-00881-2		JUN 2020											
J								Predictive Trajectory Tracking Control of Autonomous Underwater Vehicles Based on Variable Fuzzy Predictor	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Predictive control; Autonomous underwater vehicle; Sliding data window; Variable fuzzy model	NEURAL-NETWORK; MODEL; OPTIMIZATION; ALGORITHM; SYSTEMS	A variable-fuzzy-predictor-based predictive control approach is presented to solve the dynamic trajectory tracking problem of an autonomous underwater vehicle (AUV) in a three-dimensional underwater environment. To adapt to the characteristics of AUV's motion such as nonlinearity and time-varying dynamics, a predictive controller framework is proposed based on a variable fuzzy predictor whose structure and parameters are both online adjusted. To achieve a precise estimation of AUV's motion, the variable multi-dimensional fuzzy predictor employs a sliding data window (SDW) as the system status observer, and uses the Delaunay triangulation partition method for model construction. The predictive control scheme takes advantages of the constraints-tolerance of predictive control, the uncertainty immunity of fuzzy logic calculation, and the adaptability of variable fuzzy model. The comparative simulation of the AUV trajectory tracking control is conducted in the scenario of underwater operation of an offshore platform, and the result demonstrates the feasibility and effectiveness of the proposed control strategy in respect of accuracy and stability.																	1562-2479	2199-3211															10.1007/s40815-020-00898-7		JUN 2020											
J								Multivariate morphological reconstruction based fuzzy clustering with a weighting multi-channel guided image filter for color image segmentation	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Color image segmentation; Fuzzy clustering; Multi-channel guided filter; Multivariate morphological reconstruction	C-MEANS; ALGORITHM; INFORMATION; FCM	The fuzzy c-means clustering with guided image filter (GF) is a useful method for image segmentation. The single-channel GF can be efficiently applied to the gray-scale guidance image, but for the color guidance image, due to the high run-time overhead on the calculation of the inverse of the covariance matrix, it is a hard work to perform the multi-channel GF. To address this issue, we propose a novel weighting multi-channel guided image filter (WMGF) method. In this method, each channel of the color guidance image is utilized to guide the filtering for the input image independently and a novel weight is defined for each channel according to the variance of the image pixels in a local window, which greatly eliminates the mutual influence between different channels and brings about a low run-time overhead. In addition, based on the WMGF method, we present a new fuzzy c-means clustering algorithm (FCMWMGF) for the color image segmentation, in which the WMGF is performed on the membership matrix in each iteration of the fuzzy c-means clustering. To further enhance the different noise-immunity and edge preservation, the multivariate morphological reconstruction (MMR) method is introduced into the proposed fuzzy clustering method (MMR_ FCMWMGF) to obtain higher segmentation precision. Experiments on color images with Salt & Pepper and Gaussian noises demonstrate the superiority of the proposed methods.																	1868-8071	1868-808X				DEC	2020	11	12					2793	2806		10.1007/s13042-020-01151-1		JUN 2020											
J								A hybrid method of recurrent neural network and graph neural network for next-period prescription prediction	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Medical prediction; Recurrent neural network; Graph neural network; Next-period prescription prediction		Electronic health records (EHRs) have been widely used to help physicians to make decisions by predicting medical events such as diseases, prescriptions, outcomes, and so on. How to represent patient longitudinal medical data is the key to making these predictions. Recurrent neural network (RNN) is a popular model for patient longitudinal medical data representation from the view of patient status sequences, but it cannot represent complex interactions among different types of medical information, i.e., temporal medical event graphs, which can be represented by graph neural network (GNN). In this paper, we propose a hybrid method of RNN and GNN, called RGNN, for next-period prescription prediction from two views, where RNN is used to represent patient status sequences, and GNN is used to represent temporal medical event graphs. Experiments conducted on the public MIMIC-III ICU data show that the proposed method is effective for next-period prescription prediction, and RNN and GNN are mutually complementary.																	1868-8071	1868-808X				DEC	2020	11	12					2849	2856		10.1007/s13042-020-01155-x		JUN 2020											
J								Cervical cell classification based on the CART feature selection algorithm	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Feature extraction; CART feature selection; PSO-SVM; Cervical cell classification	DECISION TREE; CANCER; SEGMENTATION; SYSTEM	In recent years, conventional artificial method leads to low efficiency in the classification of cervical cell, which requires professional completion. Therefore, the classification process is increasingly dependent on artificial intelligence. The traditional image classification method needs to extract a large number of features. Redundant features cause the recognition speed to be slow, and influence the recognition effect. To address these problems and obtain the highest recognition accuracy with the least number of features, this paper proposes a machine learning method based on feature selection algorithm for cervical cell classification. Firstly, we introduced classification and regression trees (CART) for cell feature selection, which reduces the dimension of input feature attributes. Subsequently, particle swarm optimization (PSO) was used to optimize the hyperparameters of support vector machine (SVM) in this paper, making the SVM model better for classification. Finally, the Herlev dataset was introduced to verify the classification performance. The experimental results show that the proposed algorithm can extract accurate and effective features and obtain high classification accuracy, thus verifying the effectiveness of the proposed algorithm. Moreover, the network structure of the proposed algorithm is relatively simple with a low computation cost, which makes it feasible of further extension to the classification application of other cancer cells.																	1868-5137	1868-5145															10.1007/s12652-020-02256-9		JUN 2020											
J								A multimodal biometric authentication scheme based on feature fusion for improving security in cloud environment	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Authentication; Cloud computing; Cryptography; Hashing; Multimodal biometric system; Symmetric key encryption	RECOGNITION	In recent days, due to the advent of advanced technologies such as cloud computing, accessing data can be done anywhere at any time. Meanwhile, ensuring the data security is highly significant. Authentication plays a major role in preserving security via different access control mechanisms. As a recent trend, the biological information of the individual user is considered as verification scheme for the authentication process. Traits such as fingerprint, iris, ear or palm print are widely used to develop the authentication systems from its patterns. But, to increase the complexity of the user authentication and to ensure high security, more than a trait is combined together. In this paper, a multimodal authentication system is proposed by fusing the feature points of fingerprint, iris and palm print traits. Each trait has undergone the following procedures of image processing techniques such as pre-processing, normalization and feature extraction. From the extracted features, a unique secret key is generated by fusing the traits in two stages. False Acceptance Rate (FAR) and False Rejection Rate (FRR) metrics are used to measure the robustness of the system. This performance of the model is evaluated using three standard symmetric cryptographic algorithms such as AES, DES and Blowfish. This proposed model provides better security and access control over data in cloud environment.																	1868-5137	1868-5145															10.1007/s12652-020-02184-8		JUN 2020											
J								Wireless backhaul network's capacity optimization using time series forecasting approach	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING											TRAFFIC PREDICTION; NEURAL-NETWORKS; INTERNET; TECHNOLOGIES; 4G	The extensive technological participation in our daily life and business world has transformed the technology model predominantly from last decade and it has created many directions for further research and development. In previous generations of technology, the major development was hardware side but now it is shifting to the smart and intelligent solutions based on AI. The new challenges have provoked and complexity is increasing for resource orchestration because of the nonstop increasing network and its applications. In cellular architecture, the backhaul network (intermediate network) is used to connect the radio and core network. The key technology is the point to point fixed links that is used in the wireless backhaul network. More than 60% radio base station are connected using this technology. Currently, the static approach is used for planning and optimization of the network which is now becoming very difficult as the network is increasing continuously and becoming more complex. The dynamic resource allocation method is proposed for the future capacity forecasting system which is founded on the factual employment of point to point links. Capacity is a crucial factor in wireless backhaul network so best capacity optimization can lead to a good frequency reuse and optimal use of other network resources. The development is based on three different models namely (1) Autoregressive (AR), (2) Seasonal Autoregressive Integrated Moving Average (SARIMA) and (3) Multi-Layer Perceptron (MLP) neural network. Root Means Squared Error (RMSE) and Means Absolute Percentage Error (MAPE) are performance criterion that are used to evaluate the models. When we compare the models' performance, the MLP results are most credible but it takes more time to converge than AR and SARIMA. By using the proposed estimation method, the static optimization will positively move to the dynamic (forecasted) optimization and the distribution of capacity utilization will be right skewed. Hence, the proposed system is efficient and has the ability to optimize the network according to the actual network's capacity utilization. It will assist the network planner to perform more efficiently, resource distribution will be more balanced and the wastage of resources can be reduced.																	1868-5137	1868-5145															10.1007/s12652-020-02209-2		JUN 2020											
