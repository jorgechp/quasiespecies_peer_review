PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	RP	EM	RI	OI	FU	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	D2	EA	PG	WC	SC	GA	UT	PM	OA	HC	HP	DA
J								Robust and high-security fingerprint recognition system using optical coherence tomography	NEUROCOMPUTING										Optical cohernce tomography(OCT); Subsurface fingerprint; Fingerprint reconstruction; Fingerprint recognition	ACQUISITION	Traditional fingerprint recognition systems are vulnerable to attacks, such as the use of artificial fingerprints, and poor performance will be achieved if the captured surface fingerprints are of low-quality. Developing high-security and robust fingerprint recognition systems is of increasing concern in modern society. The introduction of optical coherence tomography (OCT) for fingerprint imaging opens up a new research domain for fingerprint recognition due to its ability to capture the depth information of skin layers. This paper proposes a fingerprint recognition system based on OCT. The research first establishes a database with normal, worn-out, artificial and degraded fingerprints imaged by our custom-built, inhouse OCT device. Then, we propose to reconstruct three layers of subsurface fingerprints by considering diverse skin layer information. For each of the subsurface fingerprints divided according to the physical structure of the fingertip, we propose a simple yet effective projection-based reconstruction method. Finally, a pixel-level fusion strategy based on the local image quality is proposed to the fuse the three levels of subsurface fingerprints for robust fingerprint recognition. In our experiments, we show that as much diverse fingerprint information can be retained as possible by the proposed subsurface fingerprint reconstruction method. We also demonstrate the effectiveness and efficiency of the proposed method by comparing it with existing state-of-the-art internal fingerprint reconstruction approaches. The robustness and high anti-spoofing ability of the proposed system is verified by comparing the matching performance evaluated on the established OCT-based database and another database with the same fingerprints imaged by a commercial optical sensor. The best EER and FMR100 evaluated on OCT-based fingerprints are 0.42% and 0.36%, respectively. The best EER and FMR100 evaluated on traditional 2D surface fingerprints are 8.05% and 18.18%, respectively, which shows the vast potential of the proposed system in current automated fingerprint recognition systems (AFRSs). (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 18	2020	402						14	28		10.1016/j.neucom.2020.03.102													
J								Image super-resolution via multi-view information fusion networks	NEUROCOMPUTING										Super-resolution; Multi-view information fusion; Lightweight network		In recent years, convolution neural networks (CNN) have achieved substantial advantages for single image super resolution (SISR). However, as the depth and width of the networks increase, the model parameters and computation become more complicated. To solve these problems, we propose a lightweight network structure for super-resolution which adopts the multi-view information fusion strategy. The proposed network consists of a feature extraction block, N channel-fusion enhancement blocks (CFEBs) and an upscale block. In order to reduce model parameters and computation complexity, each CFEB adopts group convolution operation and channel-fusion strategy to fuse image information from different channels. In addition, we propose a global-relation attention block to enable interactions among groups and an enhancement block to better extract and fuse multi-scale feature information. Experimental results on public datasets demonstrate that both the proposed model and its lightweight version achieve superior performance than their state-of-the-art couterparts. Moreover, the proposed lightweight model sometimes even has better performance than some state-of-the-art heavy models. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 18	2020	402						29	37		10.1016/j.neucom.2020.03.073													
J								MAMNet: Multi-path adaptive modulation network for image super-resolution	NEUROCOMPUTING										Single image super-resolution; Feature modulation; Deep learning		In recent years, single image super-resolution (SR) methods based on deep convolutional neural networks (CNNs) have made significant progress. However, due to the non-adaptive nature of the convolution operation, they cannot adapt to various characteristics of images, which limits their representational capability and, consequently, results in unnecessarily large model sizes. To address this issue, we propose a novel multi-path adaptive modulation network (MAMNet). Specifically, we propose a multi-path adaptive modulation block (MAMB), which is a lightweight yet effective residual block that adaptively modulates residual feature responses by fully exploiting their information via three paths. The three paths model three types of information suitable for SR: 1) channel-specific information (CSI) using global variance pooling, 2) inter-channel dependencies (ICD) based on the CSI, 3) and channel-specific spatial dependencies (CSD) via depth-wise convolution. We demonstrate that the proposed MAMB is effective and parameter-efficient for image SR than other feature modulation methods. In addition, experimental results show that our MAMNet outperforms most of the state-of-the-art methods with a relatively small number of parameters. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 18	2020	402						38	49		10.1016/j.neucom.2020.03.069													
J								Reinforcement Learning-Based Control for Nonlinear Discrete-Time Systems with Unknown Control Directions and Control Constraints	NEUROCOMPUTING										Neural networks; Reinforcement learning; Non-affine nonlinear systems; Output feedback; Unknown control directions	OUTPUT-FEEDBACK CONTROL; NN CONTROL; ADAPTIVE-CONTROL; DESIGN	In this work, output-feedback control problems for a class of discrete-time non-affine nonlinear systems with unknown control directions and input constraints are considered by using reinforcement learning (RL) method. Two neural networks (NNs) implement the control: 1) a critic NN that estimates a nonquadratic strategic utility function (SUF) and 2) an action NN that generates optimized control input and minimizes the SUF. The implicit function theorem is applied to obtain the optimal control law since the control is appeared in a non-affine form. For the first time, the discrete Nussbaum gain is introduced to overcome the difficulty that the control directions are unknown and a non-quadratic SUF is used to deal with the control constraints in the RL-based control. The theoretical derivation of the uniformly ultimately boundedness of the NN weights and the closed-loop output tracking error is given. And two numerical examples have been supplied to valid the proposed method. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 18	2020	402						50	65		10.1016/j.neucom.2020.03.061													
J								A novel biologically-inspired target detection method based on saliency analysis for synthetic aperture radar (SAR) imagery	NEUROCOMPUTING										Biological vision system; Saliency detection; Focus of Attention (FOA); Target detection; Synthetic aperture radar (SAR)	OBJECT DETECTION; MODEL	Saliency Object Detection (SOD) models driven by the biologically-inspired Focus of Attention (FOA) mechanism can result in highly accurate saliency maps. However, their application in high-resolution Synthetic Aperture Radar (SAR) images entails a number of intractable problems due to complex backgrounds. In this paper, we propose a novel hierarchical self-diffusion saliency (HSDS) method for detecting vehicle targets in large scale SAR images. To reduce the influence of cluttered returns on saliency analysis, we learn a weight vector from the training set to capture optimal initial saliency of the superpixels during saliency diffusion. By accounting for the multiple sizes of background objects, the saliency analysis is implemented in multi-scale space, and a saliency fusion strategy employed to integrate the multi-scale saliency maps. Simulation experiments demonstrate that our proposed method can produce a more accurate and stable detection performance, with decreased false alarms, compared to benchmark approaches. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				AUG 18	2020	402						66	79		10.1016/j.neucom.2019.12.009													
J								Multiple fragment-level interactive networks for answer selection	NEUROCOMPUTING										Answer selection; Question answering; Fragment-level interaction; Attention		Answer selection in question answering (QA) denotes a task which selects the most appropriate one from candidate answers for a given question. Previous researches on answer selection usually conduct it by isolated word-level interaction between questions and answers. In these methods, the abundant contextual information is hardly captured, which affects the choice of the correct answer. To overcome this problem, we propose to exploit a Multiple Fragment-level Interactive Network (MFIN) for this task. The MFIN can extend the search space from word-level to fragment-level, which is conducive to obtaining more contextual information. In MFIN, we apply the multiple fragment-level attention mechanism to select key fragment pairs and achieve multiple fragment-level interaction. Meanwhile, we utilize the recurrent representation encoding to integrate multiple interactive information to reduce noise. The experimental results demonstrate that our proposed model is efficient compared to the existing methods on the WikiQA and SemEval-2016 CQA datasets. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 18	2020	402						80	88		10.1016/j.neucom.2020.03.089													
J								Multiple-Instance ranking based deep hashing for multi-Label image retrieval	NEUROCOMPUTING										Deep hashing; Multi-label image retrieval; Multiple-instance ranking	QUANTIZATION	Hashing methods have been widely applied to approximate nearest neighbor search in large-scale image retrieval due to its fast search speed and efficient storage space. In practice, most images are with multiple category-aware objects, i.e., multi-label images. This paper focuses on hash code learning for multi-label image retrieval. Most existing hashing methods directly extract one patch such as a downsized crop from each image as a training example, which ignores the multi-label characteristic of images and leads to suboptimal representations for multi-label images. Some researches have proved that each multi-label image follows a multi-instance assumption, where each image is represented as a bag of category-aware proposals (instances). However, existing multiple-instance learning methods use predefined statistical functions with limited learning capability to construct bag features, and they are designed for classification or pairwise-similarity preserving. Thus, directly applying existing multiple-instance learning methods into deep hashing framework still leads to suboptimal hash codes for retrieval. In this paper, we pose hashing learning for multi-label image retrieval as a problem of multiple-instance ranking learning. To solve this problem, we present an end-to-end deep hashing framework, referred to as Deep Multiple-Instance Ranking based Hashing (DMIRH). In DMIRH, we design a category-aware bag feature construction module, which jointly assigns the learned instances into categories and aggregates the selected instance features into a bag feature representation that can capture the multi-label information of each image. In addition, we propose a novel learning objective, which consists of an Inner Product distance based quantization loss to control the hash quality and a listwise ranking loss to preserve the ranking relationships. Experimental results on public benchmarks show the superiority of DMIRH over several state-of-the-art hashing methods. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 18	2020	402						89	99		10.1016/j.neucom.2020.03.077													
J								A non-linear view transformations model for cross-view gait recognition	NEUROCOMPUTING										Cross-view gait recognition; View transformations; Spatiotemporal features	PERSON IDENTIFICATION; MOTION; FRAMEWORK; FEATURES	Gait has emerged as an important biometric feature which is capable of identifying individuals at distance without requiring any interaction with the system. Various factors such as clothing, shoes, and walking surface can affect the performance of gait recognition. However, cross-view gait recognition is particularly challenging as the appearance of individual's walk drastically changes with the change in the viewpoint. In this paper, we present a novel view-invariant gait representation for cross-view gait recognition using the spatiotemporal motion characteristics of human walk. The proposed technique trains a deep fully connected neural network to transform the gait descriptors from multiple viewpoints to a single canonical view. It learns a single model for all the videos captured from different viewpoints and finds a shared high-level virtual path to project them on a single canonical view. The proposed deep neural network is learned only once using the spatiotemporal gait representation and applied to testing gait sequences to construct their view-invariant gait descriptors which are used for cross-view gait recognition. The experimental evaluation is carried out on two large benchmark cross-view gait datasets, CASIA-B and OU-ISIR large population, and the results are compared with current state-of-the-art methods. The results show that the proposed algorithm outperforms the state-of-the-art methods in cross-view gait recognition. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 18	2020	402						100	111		10.1016/j.neucom.2020.03.101													
J								Imbalanced learning algorithm based intelligent abnormal electricity consumption detection	NEUROCOMPUTING										Abnormal electricity detection; Multi-class imbalance learning; SMOTE; K-means clustering; KELM	NONTECHNICAL LOSS DETECTION; MACHINE; REGRESSION; TREES	Abnormal electricity consumption (AEC) caused huge economic losses to power supply enterprises in the past years, and also posed severe threats to the safety of peoples' daily live. An accurate AEC detection is crucial to reducing the non-technical losses (NTLs) suffered by power supply enterprises and the State Grid. Comparing with the huge amount of electricity data flow, AEC data are relative few, that makes the AEC detection a typical imbalanced learning problem. To address this issue, two effective AEC detection algorithms from the perspective of data balancing and data weighting, respectively, are studied in this paper: (i) the K-means clustering and synthetic minority oversampling (K-means SMOTE) technique combining with the artificial neural network (ANN) trained by kernel extreme learning machine (KELM), and (ii) the deep weighted ELM (DWELM), that builds on an improved multiclass AdaBoost imbalanced learning algorithm (AdaBoost-ID) and an enhanced deep representation network based ELM (EH-DrELM). Experiments on the electricity consumption data of State Grid Zhejiang Electric Power Corporation are presented to show the effectiveness of the proposed algorithms. Comparisons to many state-of-the-art methods are provided for the superiority demonstration. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 18	2020	402						112	123		10.1016/j.neucom.2020.03.085													
J								Learning refined attribute-aligned network with attribute selection for person re-identification	NEUROCOMPUTING										Person re-identification; Attribute-aligned; Attribute selection; Policy gradient		Effective person re-identification (Re-ID) is often required in real applications. While most exiting approaches either assume the detected pedestrian bounding box well-aligned or utilize limited human structural information (pose, attention, segmentation) to calibrate the misalignment. However, the value of utilizing attributes for pedestrian alignment is still under explored. Furthermore, the hierarchy of attributes in previous works has been largely ignored, appearance feature and attribute feature are often fused in a rigid way. This directly limits the discriminatory and robustness of feature representation. In this paper, we propose a Refined Attribute-aligned Network (RAN), which consists of a coarse-alignment and a fine-alignment module. First, the pre-trained part and attribute predictor are used to generate body parts and candidate attributes. Then the body parts are used for coarse alignment and the attributes are selected by an agent. The agent is optimized with policy gradient algorithm, which can maximize the accumulative reward to increase the probability of proper attribute selection. Finally, for the fine-alignment, the attribute maps and body part features are aggregated by a bilinear-pooling layer to support accurate Re-ID. Extensive experimental results based on multiple datasets including CUHK03, DukeMTMC and Market-1501 demonstrate the superiority of our method over state-of-the-art methods. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 18	2020	402						124	133		10.1016/j.neucom.2020.03.057													
J								Averaged Bi-LSTM networks for RUL prognostics with non-life-cycle labeled dataset	NEUROCOMPUTING										Neural network; Remaining useful life; Model averaging method; LSTM network; Bi-LSTM	NEURAL-NETWORKS; PREDICTION; MACHINE; CELLS	LSTM network is an effective RNN model to predict the system RUL for its superior performance in sequential data processing. Usually, networks trained by life-cycle labeled dataset would possess similar RUL predicting accuracies, because the network training algorithm could ensure the network optimality for the whole training dataset. However, for networks trained by non-life-cycle labeled samples, the network uncertainty caused by different training conditions could lead to degradation prediction uncertainty for some local points. Further, the RUL predicting results that are computed by these uncertain local points may shows relatively large differences. Therefore, in order to obtain an accurate RUL prediction with networks trained by non-life-cycle labeled samples, our paper proposes a novel network model averaging method to deal with the network uncertainty. What is more, to learn the temporal correlation information of training samples sufficiently, we adopt the Bi-LSTM network to illustrate the application of the proposed network model averaging method. Finally, degradation values of Graphite/LiCoO2 battery are used to verify the effectiveness of the proposed method. The results show that the proposed method could improve the RUL prediction accuracy and reduce the prediction error. (C) 2020 Published by Elsevier B.V.																	0925-2312	1872-8286				AUG 18	2020	402						134	147		10.1016/j.neucom.2020.03.041													
J								An overview of recent multi-view clustering	NEUROCOMPUTING										Machine learning; Unsupervised learning; Multi-view clustering; Graph-based clustering; Space learning	FEATURE-SELECTION; SUBSPACE; ALGORITHM	With the widespread deployment of sensors and the Internet-of-Things, multi-view data has become more common and publicly available. Compared to traditional data that describes objects from single perspective, multi-view data is semantically richer, more useful, however more complex. Since traditional clustering algorithms cannot handle such data, multi-view clustering has become a research hotspot. In this paper, we review some of the latest multi-view clustering algorithms, which are reasonably divided into three categories. To evaluate their performance, we perform extensive experiments on seven real-world data sets. Three mainstream metrics are used, including clustering accuracy, normalized mutual information and purity. Based on the experimental results and a large number of literature reading, we also discuss existing problems in current multi-view clustering and point out possible research directions in the future. This research provides some insights for researchers in related fields and may further promote the development of multi-view clustering algorithms. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 18	2020	402						148	161		10.1016/j.neucom.2020.02.104													
J								Neuroadaptive finite-time output feedback control for PMSM stochastic nonlinear systems with iron losses via dynamic surface technique	NEUROCOMPUTING										Adaptive neural network control; Finite-time technology; State observer; PMSM Stochastic nonlinear systems; Iron losses	ADAPTIVE-CONTROL; STABILIZATION; STABILITY; TRACKING; DESIGN	In this paper, an observer-based adaptive neural network finite-time dynamic surface control method is proposed for the position tracking control of PMSM stochastic nonlinear systems with iron losses. First, the finite-time technology is used to realize the fast and effective tracking of the desired signal and make the system have better robust performance. Then, the adaptive neural network (NN) technology and state observer are applied to approximating the uncertain nonlinear functions and estimating the immeasurable states, respectively. And, the dynamic surface control (DSC) technology is used to resolve the "explosion of complexity" problem. In addition, the influence of iron losses and stochastic disturbances in the system is considered, and a quartic stochastic Lyapunov function is established to analyze the stability of the system. Finally, the simulation results show the effectiveness of the proposed method. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 18	2020	402						162	170		10.1016/j.neucom.2020.02.063													
J								Portfolio trading system of digital currencies: A deep reinforcement learning with multidimensional attention gating mechanism	NEUROCOMPUTING										Portfolio; Deep-reinforcement learning; Reinforcement learning; Attention gating mechanism		As a hot topic in the financial engineering, the portfolio optimization aims to increase investors' wealth. In this paper, a portfolio management system based on deep-reinforcement learning is proposed. In contrast to inflexible traditional methods, the proposed system achieves a better trading strategy through Reinforcement learning. The reward signal of Reinforcement learning is updated by action weights from Deep learning networks. Low price, high price and close price constitute the inputs, but the importance of these three features is quite different. Traditional methods and the classical CNN can't deal with these three features separately, but in our method, a designed depth convolution is proposed to deal with these three features separately. In a virtual currency market, the price rise only occurs in a flash. Traditional methods and CNN networks can't accurately judge the critical time. In order to solve this problem, a three-dimensional attention gating network is proposed and it gives higher weights on rising moments and assets. Under different market conditions, the proposed system achieves more substantial returns and greatly improves the Sharpe ratios. The short-term risk index of the proposed system is lower than those of the traditional algorithms. Simulation results show that the traditional algorithms (including Best, CRP, PAMR, CWMR and CNN) are unable to perform as well as our approach. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 18	2020	402						171	182		10.1016/j.neucom.2020.04.004													
J								Adaptive neuro-fuzzy PID controller based on twin delayed deep deterministic policy gradient algorithm	NEUROCOMPUTING										Twin delayed deep deterministic policy gradient algorithm; Reinforcement learning; Fuzzy PID controller; Cart-pole system		This paper presents an adaptive neuro-fuzzy PID controller based on twin delayed deep deterministic policy gradient (TD3) algorithm for nonlinear systems. In this approach, the observation of the environment is embedded with information of a multiple input single output (MISO) fuzzy inference system (FIS) and have a specially defined fuzzy PID controller in neural network (NN) formation acting as the actor in the TD3 algorithm, which achieves automatic tuning of gains of fuzzy PID controller. From the control perspective, the controller combines the merits of both FIS and PID controller and utilizes reinforcement learning algorithm for optimizing parameters. From the reinforcement learning point of view, embedding the prior knowledge into the fuzzy PID controller incorporated in the actor network helps reduce the learning difficulty in the training process. The proposed method was tested on the cart-pole system in simulation environment with comparison of a linear PID controller, which demonstrates the robustness and generalization of the proposed approach. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 18	2020	402						183	194		10.1016/j.neucom.2020.03.063													
J								Subject-based dipole selection for decoding motor imagery tasks	NEUROCOMPUTING										MI-tasks decoding; EEG source imaging; Dipole selection; Common spatial patterns; Continuous wavelet transform	BRAIN-COMPUTER INTERFACE; COMMON SPATIAL-PATTERN; BOUNDARY-ELEMENT METHOD; SOURCE LOCALIZATION; EEG CLASSIFICATION; DENSITY; SIGNAL	In the BCI rehabilitation system, the decoding of motor imagery tasks (MI-tasks) with dipoles in the source domain has gradually become a new research focus. For complex multiclass MI-tasks, the number of activated dipoles is large, and the activation area, activation time and intensity are also different for different subjects. The means by which to identify fewer subject-based dipoles is very important. There exist two main methods of dipole selection: one method is based on the physiological functional partition theory, and the other method is based on human experience. However, the number of dipoles that are selected by the two methods is still large and contains information redundancy, and the selected dipoles are the same in both number and position for different subjects, which is not necessarily ideal for distinguishing different MI-tasks. In this paper, the data-driven method is used to preliminarily select fully activated dipoles with large amplitudes; the obtained dipoles are refined by using continuous wavelet transform (CWT) to best reflect the differences among the multiclass MI-tasks, thereby yielding a subject-based dipole selection method, which is named PRDS. PRDS is further used to decode multiclass MI-tasks in which some representative dipoles are found, and their wavelet coefficient power is calculated and input to one-vs.-one common spatial pattern (OVO-CSP) for feature extraction, and the features are classified by the support vector machine. We denote this decoding method as D-CWTCSP, which enhances the spatial resolution and also makes full use of the time-frequency-spatial domain information. Experiments are carried out using a public dataset with nine subjects and four classes of MI-tasks, and the proposed D-CWTCSP is compared with the relevant methods in sensor space and brain-source space in terms of the decoding accuracy, standard deviation, recall rate and kappa value. The experimental results show that D-CWTCSP reaches an average decoding accuracy of 82.66% among the nine subjects, which generates 8-20% improvement over other methods, thus reflecting its great superiority in decoding accuracy. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 18	2020	402						195	208		10.1016/j.neucom.2020.03.055													
J								Improved deep embedding learning based on stochastic symmetric triplet loss and local sampling	NEUROCOMPUTING										Deep learning; Representation learning; Metric learning; Loss function		Designing more powerful feature representations has motivated the development of deep metric learning algorithms over the last few years. The idea is to transform data into a representation space where some prior similarity relationships between examples are preserved, e.g., distances between similar examples being smaller than those between dissimilar examples. While such approaches have produced some impressive results, they often suffer from difficulties in training. In this paper, we introduce an improved triplet-based loss for deep metric learning. Our method aims to minimize distances between similar examples, while maximizing distances between those that are dissimilar under a stochastic selection rule. Additionally, we propose a simple sampling strategy, which focuses on maintaining locally the similarity relationships of examples in their neighborhoods. This technique aims to reduce the local overlap between different classes in different parts of the embedded space. Experimental results on three standard benchmark data sets confirm that our method provides more accurate and faster training than other state-of-the-art methods. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 18	2020	402						209	219		10.1016/j.neucom.2020.04.062													
J								Multi-granularity generative adversarial nets with reconstructive sampling for image inpainting	NEUROCOMPUTING										Image inpainting; Generative adversarial networks; Reconstructive sampling; Unstable training; Gradient vanishing	QUALITY ASSESSMENT	Image inpainting based on deep neural network has drawn more attention with the development of deep learning, then Generative adversarial nets(GANs) which is a combination of multiple deep networks has been applied to image inpainting and achieved top-level performance. However, GANs-based inpainting method always suffers from unstable training and vanishing gradient. In this paper, we present an image inpainting method based on GANs with reconstructive sampling and multi-granularity generative adversarial strategy. The key idea is to solve unstable training and vanishing gradient by proposed reconstructive sampling, in which we sample from reconstructive distribution than low-dimension noise. It has been proved that reconstructive sampling is effective to avoid unstable training and gradient vanish. Then, multi-granularity generative adversarial strategy, which is decomposed into two steps, is adopted to make inpainted image more continuous and realistic in texture structure and vision, respectively. Extensive experiments show that ours brings substantial improvements over other state-of-the-art image inpainting algorithms in distortion and perception evaluation. Besides, comparisons on stability with baselines show that our method gains better stability during image inpainting. (C) 2020 Published by Elsevier B.V.																	0925-2312	1872-8286				AUG 18	2020	402						220	234		10.1016/j.neucom.2020.04.011													
J								AFPNet: A 3D fully convolutional neural network with atrous-convolution feature pyramid for brain tumor segmentation via MRI images	NEUROCOMPUTING										Brain tumor segmentation; 3D fully convolutional neural network; Atrous-convolution feature pyramid; 3D fully connected Conditional Random; Field	MODEL	Traditional deep convolutional neural networks for fully automatic brain tumor segmentation have two problems: spatial information loss caused by both the repeated pooling/striding and weak ability of multi-scale lesion processing. To overcome the first problem, we use a 3D atrous-convolution with a single stride to replace pooling/striding and build the backbone for feature learning. For the second problem, a 3D atrous-convolution feature pyramid is designed and added to the end of the backbone. By integrating with contextual features, this structure improves the discriminating ability of the overall model to segment tumors with various sizes. Finally, a 3D fully connected Conditional Random Field is constructed as a post-processing step for the network's output to obtain structural segmentation of both the appearance and spatial consistency. Abundant ablation experiments carried on Magnetic Resonance Imaging datasets demonstrate that lossless feature computation and multi-scale information fusion driven by our method are feasible to address the above problems. Compared with the state-of-the-art methods on the public benchmarks, our method achieves competitive performance and can be efficiently implemented into the clinical medical application. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 18	2020	402						235	244		10.1016/j.neucom.2020.03.097													
J								Point clouds learning with attention-based graph convolution networks	NEUROCOMPUTING										Point clouds; Attention mechanism; Graph network; Semantic segmentation	EDUCATION	Point clouds, as a kind of 3D objects representation, are the most primitive outputs obtained by 3D sensors. Unlike 2D images, point clouds are disordered and unstructured. Hence the classification techniques such as the convolution neural network are not applicable to point cloud analysis directly. To solve this problem, we propose a novel network to extract point clouds feature, named attention-based graph convolutional network (AGCN). Taking the learning process as a message propagation between adjacent points, we specifically introduce attention mechanism to construct a point attention layer for analyzing the relationship between local points feature. The object classification is implemented by stacking multiple layers of point attention layer. In addition, the proposed network is extended to an attention-based encoder-decoder structure for segmentation tasks. We also introduce an additional global graph structure network to compensate for the relative location information of the individual points in the graph structure network. Experimental results show that our network has lower computational complexity and faster convergence speed. Compared with existing methods, the proposed network can achieve comparable performance in classification and segmentation tasks. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 18	2020	402						245	255		10.1016/j.neucom.2020.03.086													
J								Joint Deep Recommendation Model Exploiting Reviews and Metadata Information	NEUROCOMPUTING										Convolutional Neural Network; Recommender Systems; Metadata; Rating Prediction; Deep Learning; Reviews	NEURAL-NETWORKS	User-generated product reviews contain a lot of valuable information including users' opinions on products and product features that is not fully exploited by the current recommendation models. Similarly, the metadata information related to the products, about the reviews and about the users who have written the reviews has rarely been exploited for recommender systems. These heterogeneous information sources have the potential to alleviate the cold start and sparsity problems and improve the quality of recommendations. In this paper, we present a joint deep recommendation model UDRM) that consists of two parallel neural networks, learning lower-order feature interactions of users and items separately and higher-order feature interactions jointly using a shared last layer. Each of the networks is further composed of two sub-networks. One of the sub-networks focus on exploiting product reviews (of user/item) and the other sub-network learns user preferences/items properties leveraging metadata information along with the ratings. The learned latent features in each network are concatenated, thus producing the user and item latent feature vectors. We combine the two networks by introducing a shared layer on the top, which is a dense fully connected layer used to learn higher level latent features obtained from the two networks and produces final ratings. Extensive experiments on real-world datasets demonstrate that JDRM significantly outperforms state of the art recommendation models. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 18	2020	402						256	265		10.1016/j.neucom.2020.03.075													
J								On the design and analysis of structured-ANN for online PID-tuning to bulk resumption process in ore mining system	NEUROCOMPUTING										Bulk resumption process; Computational aspects; Bio-inspired adaptive method; Neurocomputing; Structured artificial neural networks	NEURAL-NETWORK; FUZZY; CONTROLLER	The tuning of proportional-integral-derivative (PID) controllers has been extensively used in the industry, this adjustment is performed by means of conventional methods, such as the Ziegler-Nichols and trial-and-error that often fails to address inherent complexity of industrial processes. To overcome these problems of PID tuning, a neurocomputing adaptive novel methodology is presented in this paper. The proposed methodology is based on a structured artificial neural network (S-ANN) and a propagation matrix of PID actions that was developed to support the tuning. The problem formulation and S-ANN-based solution are tightly connected, i.e., the nodes and layers of S-ANN topology are ruled by the order of the propagation matrix. For given operational conditions of the plant and disturbance in its parameters, the performance of the trained network is evaluated for tasks within the scope of learning on a bulk resumption process in ore mining system, focusing the control system output accuracy and convergence of the S-ANN algorithm. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 18	2020	402						266	282		10.1016/j.neucom.2020.03.074													
J								An Autuencoder-based Data Augmentation Strategy for Generalization Improvement of DCNNs	NEUROCOMPUTING										Data augmentation; Autoencoder; Deep neural network; Data distribution; Generalization performance	NEURAL-NETWORKS; MULTIPLE; MACHINE; MODEL	Inspired by the phenomenon that the decoding weights of a well-trained autoencoder contain the information of the training samples, we proposed a data augmentation method by utilizing the decoding weights. Given a batch of training data, the autoencoder is trained and the decoding weights are activated; the decoding weights are then combined with the raw samples to generate augmented samples. Furthermore, we probe its working mechanism in three ways: (i) we prove that the decoding weights and the raw samples are of linear relationship under the transformation of a certain invertible function; (ii) the proposed method can sample from a larger range in both feature dimensions and label dimension, which can be interpreted as a broader distribution vicinity compared with those by other approaches; (iii) the model trained with our data augmentation approach has better representation capability, which is reflected by the higher Fisher's criteria value in deep feature space. We conduct extensive experiments on image and tabular dataset with multiple network architectures. The proposed method provides significant generalization performance improvement compared with the baseline and better or comparable performance compared with the other state-of-the-art data augmentation approaches. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 18	2020	402						283	297		10.1016/j.neucom.2020.03.062													
J								Semi-global leader-following output consensus for heterogeneous fractional-order multi-agent systems with input saturation via observer-based protocol	NEUROCOMPUTING										Heterogeneous fractional-order multi-agent systems; Semi-global output consensus; Observer-based consensus protocol; Input saturation; External disturbances	STABILITY; FEEDBACK	In this paper, semi-global leader-following output consensus problem for heterogeneous fractional-order multi-agent systems with input saturation and external disturbances is investigated. The case where the dynamics are integer-order can be viewed as a special case of fractional-order systems. First, a novel observer-based consensus algorithm is designed to solve the semi-global output consensus problem. It is worth noting that the consensus is semi-global since the set of initial states is bounded. Then, the low gain feedback technique is utilized based on the solution of a parametric algebraic Riccati equation. Moreover, sufficient conditions of output consensus for heterogeneous fractional-order multi-agent systems are derived by using algebraic graph theory, matrix theory and the stability theory of fractional-order systems. It can be proven that the output signals of followers can reach synchronization with the leader's. Finally, numerical examples are provided to illustrate the effectiveness of our conclusions. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 18	2020	402						298	306		10.1016/j.neucom.2020.03.028													
J								Relevance feedback based online learning model for resource bottleneck prediction in cloud servers	NEUROCOMPUTING										Resource bottleneck identification; Relevance feedback; Cloud; Architectural evolution; Online learning	QUANTITATIVE-ANALYSIS; FEATURE-SELECTION; ANOMALY DETECTION; NETWORK; MEMORY; ELM	Cloud servers are highly prone to resource bottleneck failures. In this work, we propose an ensemble learning model to build LSTM-based multiclass classifier for resource bottleneck identification. The workload at cloud servers is highly dynamic in nature. To support continuous online learning of resource bottleneck identification models, we propose relevance feedback based online learning of proposed ensemble model. Here we propose to analyse, catastrophe forgetting and incremental architectural evolution as two fundamental challenges associated with online adaptation of LSTM-based multiclass classifier models. To avoid catastrophic forgetting, we propose a combination of distillation loss and the standard crossentropy loss. For architectural evolution, we propose and analyse three different alternatives to update the architecture of the bottleneck identification model on the fly. We evaluate the proposed approaches on a real world dataset collected in an industrial case study and on a dataset collected in a virtual environment setup using Docker containers. The experimental results show that the proposed approaches outperform existing state-of-the-art methods for bottleneck identification. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 18	2020	402						307	322		10.1016/j.neucom.2020.04.080													
J								Parallel Multi-Environment Shaping Algorithm for Complex Multi-step Task	NEUROCOMPUTING										Reinforcement Learning; Multi-step Task; Parallel Multiple Environments; Adaptive Reward Shaping		Because of the sparse reward and the sequence of the complex multi-step task, there is a big challenge in reinforcement learning, that is, the agent needs to implement several consecutive sequential steps to complete the whole task without the intermediate reward. Reward Shaping and Curriculum Learning algorithms are always used to solve this challenge, but Reward Shaping is prone to sub-optimal policies and Curriculum Learning easily suffers from the catastrophic forgetting problem. In this paper, we propose a novel algorithm called Parallel Multi-Environment Shaping (PMES), where several sub-environments are built based on human knowledge to make the agent aware of the importance of intermediate steps, each of which corresponds to a key intermediate step. Specifically, the learning agent is trained under these parallel multiple environments including the original environment and several sub-environments by synchronous advantage actor-critic algorithm. And the PMES algorithm has the mechanism of adaptive reward shaping to adjust the reward function. In this way, PMES algorithm effectively incorporates human experience by multiple different environments rather than only shaping the reward function, which combines the benefits of Reward Shaping and Curriculum Learning algorithms while avoiding their drawbacks. Extensive experiments on the mini-game 'Build Marines' of StarCraft II environment show that our proposed algorithm is more effective than Reward Shaping, Curriculum Learning, and PLAID algorithms, which is almost close to the level of human Grandmaster. And compared with the existing work, it takes less time and computing resources to reach a good result. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 18	2020	402						323	335		10.1016/j.neucom.2020.04.070													
J								End-to-end 3D object model retrieval by projecting the point cloud onto a unique discriminating 2D view	NEUROCOMPUTING										3D vision; Object retrieval; Object pose; Point cloud; Energy-based machine learning		Identification of rigid-body objects from point clouds is a well-studied topic and one of the most important utilities of 3D computer vision in robotics, industrial automation, unmanned systems and autonomous vehicle applications. However, simultaneous retrieval of both identity and pose of objects is still an open problem because of the challenges attributed to the vast search space of all possible poses of an object. Further, the precision of pose retrieval is inherently hard to improve because the resolution of the pose database is limited. This paper presents a point cloud retrieval method that finds the object ID and pose, simultaneously. Unlike the traditional learning-based methods that retrieve the point cloud by matching high dimensional feature vectors, we solve the point cloud retrieval problem using an end-to-end point cloud convolution neural network (CNN) model that linearly projects the 3D point cloud onto a unique discriminating 2D view using an energy-based loss function. Based on the proposed mechanism of the projection network, the rigid-body pose is obtained using the reduced QR factorization. As a result, the object ID retrieval problem is simplified because the within-class differences caused by various poses of the object are removed. The projection results exhibit strong discriminating features based on shapes while the precision of pose retrieval is improved thanks to the generalization ability of the network model. Comprehensive experiments show that the proposed method yields considerably more robust and accurate object ID retrieval, and at the same time significantly improves the precision of pose retrieval compared to the descriptor-based methods as well as two commonly used learning-based methods. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 18	2020	402						336	345		10.1016/j.neucom.2020.04.021													
J								Automatic obstacle avoidance of quadrotor UAV via CNN-based learning	NEUROCOMPUTING										Obstacle avoidance; Unmanned aerial vehicle; Convolutional neural network; Collision probability		In this paper, a CNN-based learning scheme is proposed to enable a quadrotor unmanned aerial vehicle (UAV) to avoid obstacles automatically in unknown and unstructured environments. In order to reduce the decision delay and to improve the robustness for the UAV, a two-stage end-to-end obstacle avoidance architecture is designed, where a forward-facing monocular camera is used only. In the first stage, a convolutional neural network (CNN)-based model is adopted as the prediction mechanism. Utilizing three effective operations, namely depthwise convolution, group convolution and channel split, the model predicts the steering angle and the collision probability simultaneously. In the second stage, the control mechanism maps the steering angle to an instruction that changes the yaw angle of the UAV. Consequently, when the UAV encounters an obstacle, it can avoid collision by steering automatically. Meanwhile, the collision probability is mapped as a forward speed to maintain the flight or stop going forward. The presented automatic obstacle avoidance scheme of quadrotor UAV is verified by several indoor/outdoor tests, where the feasibility and efficacy have been demonstrated clearly. The novelties of the method lie in its low sensor requirement, light-weight network structure, strong learning ability and environmental adaptability. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 18	2020	402						346	358		10.1016/j.neucom.2020.04.020													
J								Facial Image Synthesis and Super-Resolution With Stacked Generative Adversarial Network	NEUROCOMPUTING										Generative Adversarial Network; Face hallucination; Super Resolution; Image synthesis		Image synthesis and super-resolution (SR) have always been a hot spot for computer vision and image processing research. Since the development of Deep Learning, especially after the Deep Convolutional Generative Adversarial Network (DC-GAN) methods, facial image synthesis and SR problem had been solved in many circumstances. But most of the existing works were focused on natural-looking of the synthesized result rather than keeping facial information of the original image. Our paper presented an end-to-end method of getting high-resolution photo-realistic facial images from low-resolution (LR) in-the-wild images without losing the facial identity details. The pipeline used a flexible stacked GAN structure for the SR process with different target image resolutions on different upscaling factors. To avoid getting blur or nonsensical image output and realize the flexibility, "U-Net" architecture and upsampling layers with residual learning blocks were stacked. The stacked network structure makes applying different loss functions in different parts of the network possible, which helps to solve the two problems of keeping identical facial details of the LR input image and generating high-quality output images simultaneously. By using 3 different loss functions in different positions of the stacked network separately, through experimental comparison, we found the best stacked residual block parameters which could get the best output image quality. Experimental results also explicated that the network had a good SR ability compare to state of the art methods in different resolution and upscaling factor. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 18	2020	402						359	365		10.1016/j.neucom.2020.03.107													
J								Selective residual learning for Visual Question Answering	NEUROCOMPUTING										Visual Question Answering; Residual learning; Multimodal learning		Visual Question Answering (VQA) aims to reason an answer, given a textual question and image pair. VQA methods are required to learn the relationship between image region features. These methods have the limitation of inefficient learning that can produce a performance drop. It is because current intra-relationship methods are trying to learn all the intra-relationships, regardless of their importance. In this paper, a novel self-attention based VQA module named Selective Residual learning (SelRes) is proposed. SelRes processes the residual learning selectively in self-attention networks. It measures the importance of the input vectors by the attention map and limits residual learning, except in the selected regions which related to the correct answer. Selective masking is also proposed, which can ensure that the selection in SelRes is preserved in the multi-stack structure of the VQA network. Our full model achieves new state-of-the-art performances on both from-scratch and fine-tuning models. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 18	2020	402						366	374		10.1016/j.neucom.2020.03.098													
J								Semantic-spatial fusion network for human parsing	NEUROCOMPUTING										SSFNet; Semantic modulation model; Resolution-aware model; Human parsing	SEGMENTATION; MODELS	Recently, many methods have united low-level and high-level features to generate the desired accurate high-resolution prediction for human parsing. Nevertheless, there exists a semantic-spatial gap between low-level and high-level features in some methods, i.e., high-level features represent more semantics and less spatial details, while low-level ones have less semantics and more spatial details. In this paper, we propose a Semantic-Spatial Fusion Network (SSFNet) for human parsing to shrink the gap, which generates the accurate high-resolution prediction by aggregating multi-resolution features. SSFNet includes two models, a semantic modulation model and a resolution-aware model. The semantic modulation model guides spatial details with semantics and then effectively facilitates the feature fusion, narrowing the gap. The resolution-aware model sufficiently boosts the feature fusion and obtains multi-receptive-fields, which generates reliable and fine-grained high-resolution features for each branch, in bottom-up and top-down processes. Extensive experiments on three public datasets, PASCAL-Person-Part, LIP and PPSS, show that SSFNet achieves significant improvements over state-of-the-art methods. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 18	2020	402						375	383		10.1016/j.neucom.2020.03.096													
J								Adaptive visual servoing with an uncalibrated camera using extreme learning machine and Q-leaning	NEUROCOMPUTING										Image-based visual servoing (IBVS); Extreme learning machine (ELM); Reinforcement learning; Q-Learning; Robot	SLIDING MODE CONTROL; ELECTRONIC THROTTLE; MOBILE ROBOTS; TRACKING	In this paper, a novel image-based visual servoing (IBVS) method using Extreme Learning Machine (ELM) and Q-learning is proposed to solve the problems of complex modeling and selection of the servo gain. First, the pseudoinverse of the interaction matrix is approached by ELM which avoids the singularity of the interaction matrix effectively and is robust to interferences such as feature noises and camera calibration errors. Second, a reinforcement learning method, Q-learning, is adopted to adaptively adjust the servo gain in order to improve the convergence speed and stability. Compared with other methods, ELM has better generalization performance, faster operation speed and a unique optimal solution. Also, Q-learning has self-learning ability without experience in advance. The effectiveness of the proposed method is validated by simulations and experiment on a 6-DOF robot with eye-in-hand configuration. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 18	2020	402						384	394		10.1016/j.neucom.2020.03.049													
J								New error function designs for finite-time ZNN models with application to dynamic matrix inversion	NEUROCOMPUTING										Zeroing neural network (ZNN); Activation function; Finite-time convergence; Dynamic matrix inversion; Error Functions	RECURRENT NEURAL-NETWORK; VARYING SYLVESTER EQUATION; SYSTEMS; ROBUST; SEGMENTATION; CONVERGENCE	The zeroing neural network (ZNN), as a special kind of recurrent neural network (RNN), is often utilized to solve dynamic matrix inversion problems in the many fields recently. In this work, two finite-time ZNN (termed as ZNN-A and ZNN-B) models with the sign-bi-power (SBP) activation function are proposed by designing two novel error functions. The theoretical analysis shows the superior stability and finite-time convergence properties of the ZNN-A and ZNN-B models. Furthermore, three simulative examples show the effectiveness of the proposed ZNN-A and ZNN-B models for finding dynamic matrix inversion and the correctness of the corresponding theorems. To reveal the superior performance of the proposed ZNN-A and ZNN-B models with SBP activation function, the standard ZNN model with the linear activation function is comparatively applied in experiments. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 18	2020	402						395	408		10.1016/j.neucom.2020.02.121													
J								Survey on visual rhythms: A spatio-temporal representation for video sequences	NEUROCOMPUTING										Visual rhythm; temporal slice; spatio-temporal information; video analysis	REAL-TIME; RECOGNITION; TRACKING; SEGMENTATION; RETRIEVAL; BEHAVIOR	Strategies for representing spatial and temporal information from video sequences in images are very convenient due to the demand for efficient tools for storing, browsing, retrieving, indexing and transmitting multimedia content. Visual rhythm, also known as temporal slice, summarizes relevant information present in a video through a space-time representation by means of one or a few images. In this work, we provide a comprehensive review of the main methods and techniques for constructing visual rhythms, classify the approaches into different categories, and identify new trends. Additionally, we discuss some limitations, challenges and applications related to the visual rhythm representation, as well as outline some directions for future research. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 18	2020	402						409	422		10.1016/j.neucom.2020.04.035													
J								Salient instance segmentation via subitizing and clustering	NEUROCOMPUTING										Saliency detection; Instance segmentation; Subitizing; Multitask networks	OBJECT DETECTION; VISUAL-ATTENTION; MODEL	The goal of salient region detection is to identify the regions of an image that attract the most attention. Many methods have achieved state-of-the-art performance levels on this task. Recently, salient instance segmentation has become an even more challenging task than traditional salient region detection; however, few of the existing methods have concentrated on this underexplored problem. Unlike the existing methods, which usually employ object proposals to roughly count and locate object instances, our method applies salient objects subitizing to predict an accurate number of instances for salient instance segmentation. In this paper, we propose a multitask densely connected neural network (MDNN) to segment salient instances in an image. In contrast to existing approaches, our framework is proposal-free and category-independent. The MDNN contains two parallel branches: the first is a densely connected subitizing network (DSN) used for subitizing prediction; the second is a densely connected fully convolutional network (DFCN) used for salient region detection. The MDNN generates both saliency maps and salient object subitizing. Then, an adaptive deep feature-based spectral clustering operation segments the salient regions into instances based on the subitizing and saliency maps. The experimental results on salient instance segmentation datasets demonstrate the competitive performance of our framework. Its AP reaches 57.32%, which surpasses the state-of-the-art methods by about 5%. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 18	2020	402						423	436		10.1016/j.neucom.2020.04.022													
J								Using Taylor Expansion and Convolutional Sparse Representation for Image Fusion	NEUROCOMPUTING										Image decomposition; Sparse representation; Image fusion; Taylor expansion; Convolutional sparse representation	MULTI-FOCUS IMAGE; VISIBLE IMAGES; TRANSFORM; FRAMEWORK; DECOMPOSITION; ALGORITHMS; EQUATIONS	Image decomposition and sparse representation (SR) based methods have achieved enormous successes in multi-source image fusion. However, there exists the performance degradation caused by the following two aspects: (i) limitation of image descriptions for decomposition based methods; (ii) limited ability in detail preservation resulted by divided overlap patches for SR based methods. In order to address such deficiencies, a novel method based on Taylor expansion and convolutional sparse representation (TE-CSR) is proposed for image fusion. Firstly, the Taylor expansion theory, to the best of our knowledge, is for the first time introduced to decompose each source image into many intrinsic components including one deviation component and several energy components. Secondly, the convolutional sparse representation with gradient penalties (CSRGP) model is built to fuse these deviation components, and the average rule is employed for combining the energy components. Finally, we utilize the inverse Taylor expansion to reconstruct the fused image. This proposed method is to suppress the gap of image descriptions in existing decomposition based algorithms. In addition, the new method can improve the limited ability to preserve details caused by the sparse patch coding with SR based approaches. Extensive experimental results are provided to demonstrate the effectiveness of the TE-CSR method. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 18	2020	402						437	455		10.1016/j.neucom.2020.04.002													
J								Socio-political stability, voter's emotional expectations, and information management	AI & SOCIETY										Society; Politician; Emotion; Expectation; Desire; Phobia	TECHNOLOGIES; SYSTEM	The dependence of socio-political stability on the emotional expectations of voters is investigated. For this, a model of a socio-political system consisting of a society of voters and a democratically elected politician is considered. The neuropsychological model of the voter takes into account his emotional expectations. The social stability is guaranteed by the expectations of positive emotions of all voters. Socio-political stability means both the social stability and the re-election of politician. One type of voter is a Progressist who seeks to fulfil his desires. It is shown that for the socio-political stability of the Progressist society, is enough a supportive environment. Another type of voter-a Phobic works in a fearful environment. It is shown that for the socio-political stability of the Phobic's society, regular impacts that support their phobias are sufficient. As an example, emotional expectations, socio-political stability and information management in Eastern Europe are considered. The management of phobias in a weak economy is investigated. It is shown that in the absence of regular impacts supporting phobias a Phobic turns into a Progressist. Then the socio-political system becomes unstable with a weak economy and growth limits. The classification of the main social threats and phobias is given. The socio-political consequences of creating phobias, including corporate psychopaths and toxic leaders, are examined. Ways are proposed to resolve the contradiction between increased consumption and growth limits without the use of phobias, using high humanitarian technologies.																	0951-5666	1435-5655															10.1007/s00146-020-01017-8		AUG 2020											
J								A soft-margin convex polyhedron classifier for nonlinear task with noise tolerance	APPLIED INTELLIGENCE										Piecewise linear classifier; Convex polyhedron classifier; Soft margin; Noise tolerance; Support vector machine	PIECEWISE-LINEAR CLASSIFIERS; SUPPORT VECTOR MACHINES; MULTICONLITRON; ALGORITHM	As a special form of piecewise linear classifier, the convex polyhedron classifier is simple to implement and achieves rapid response in real-time classification. However, it usually performs badly in the case of high noise where severe boundary intrusion exists. Inspired by the scheme of soft margin in support vector machine, in this paper we propose a soft-margin convex polyhedron classifier for nonlinear classification task. The base (linear) classifier is first generalized to its soft-margin version through kernelization process and slack variables. In each local region, the soft-margin base classifier learns a decision hyperplane with noise tolerance. Then, a series of learned hyperplanes are structurally integrated into a convex polyhedron classifier, which is essentially a convex polyhedron that encloses one class and excludes the other class outside. Experimental results on fifteen benchmark datasets show the proposed soft-margin convex polyhedron classifier is comparable to linear support vector machine and four piecewise linear classifiers, but does not perform as well as the support vector machine with radial basis function kernel in general. When random noises are added to datasets, the soft-margin convex polyhedron classifier achieves similar or better accuracies with the well-known classifiers used for comparison, implying its promising ability of noise tolerance.																	0924-669X	1573-7497															10.1007/s10489-020-01854-6		AUG 2020											
J								Crowd counting method based on the self-attention residual network	APPLIED INTELLIGENCE										Crowd counting; Multiscale convolutional module; Deformable convolution; Self-attention residual module; Density map	MULTIPLE; HUMANS	Estimating the crowd density in surveillance videos is a hot issue in the field of computer vision and has become the basis of data processing and analysis of public transport services, commercial passenger flow analysis, public security protection and other industries. However, in terms of practical applications, due to the problems of pedestrian occlusion and scale changes, existing methods are inadequate with regard to the acquisition of the human head, which affects the accuracy of counting. To solve this problem, a crowd counting method based on a self-attention residual network is proposed. First, a multiscale convolution module composed of dilated convolution and deformation convolution is used. To avoid losing image resolution, some of the sampling positions are shifted to the occluded crowd by shifting the sampling points, which solves the problem of crowd occlusion. Then, a self-attention residual module is designed to score and classify the feature map, which allows all pixels in the feature map to be classified. The corresponding weight is generated, and the population scale is determined by the weight, which solves the problem of crowd scale changes. The algorithm is applied in ShanghaiTech and the UCF_CC_50 and WorldExpo'10 datasets are tested. The experimental results show that the mean absolute error (MAE) and mean square error (MSE) of this algorithm are significantly reduced compared with those of a comparative algorithm.																	0924-669X	1573-7497															10.1007/s10489-020-01842-w		AUG 2020											
J								Block-sparse CNN: towards a fast and memory-efficient framework for convolutional neural networks	APPLIED INTELLIGENCE										Block-sparse kernel; Convolutional neural network; Sparse matrix multiplication		In this paper, we propose a block-sparse convolutional neural network (BSCNN) architecture that converts a dense convolution kernel into a sparse one. Traditional convolutional neural networks (CNNs) face the problem that an increase in the number of network parameters will lead to more model and floating-point computations, and a higher risk of network overfitting. The block-sparse convolution uses sparse factor pairs to randomize a sparse convolution kernel, which can introduce mixed information and thereby enabling the extraction of more diverse features. In the meantime, a SUMMA-based parallel computing method is adopted to achieve a lightweight storage and a fast calculation of the convolution kernel. Experimental results show that, compared with current sparse networks, the proposed framework achieves better prediction accuracy than the classical backbone networks in terms of faster floating-point operation and less storage space requirements.																	0924-669X	1573-7497															10.1007/s10489-020-01815-z		AUG 2020											
J								An Additive Approximation to Multiplicative Noise	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Multiplicative noise; Additive approximation; Pre-marginalisation	INVERSE PROBLEMS; SPECKLE; BOUNDARY; ERRORS; MODEL; TOMOGRAPHY; REDUCTION	Multiplicative noise models are often used instead of additive noise models in cases in which the noise variance depends on the state. Furthermore, when Poisson distributions with relatively small counts are approximated with normal distributions, multiplicative noise approximations are straightforward to implement. There are a number of limitations in the existing approaches to deal with multiplicative errors, such as positivity of the multiplicative noise term. The focus in this paper is on large dimensional (inverse) problems for which sampling-type approaches have too high computational complexity. In this paper, we propose an alternative approach utilising the Bayesian framework to carry out approximative marginalisation over the multiplicative error by embedding the statistics in an additive error term. The Bayesian framework allows the statistics of the resulting additive error term to be found based on the statistics of the other unknowns. As an example, we consider a deconvolution problem on random fields with different statistics of the multiplicative noise. Furthermore, the approach allows for correlated multiplicative noise. We show that the proposed approach provides feasible error estimates in the sense that the posterior models support the actual image.																	0924-9907	1573-7683				NOV	2020	62	9					1227	1237		10.1007/s10851-020-00984-3		AUG 2020											
J								The likelihood-based optimization ordering model for multiple criteria group decision making with Pythagorean fuzzy uncertainty	NEURAL COMPUTING & APPLICATIONS										Likelihood measure; Scalar function order relation; Likelihood-based optimization ordering model; Multiple criteria group decision making (MCGDM); Pythagorean fuzzy (PF) set	MEMBERSHIP GRADES; SETS; DISTANCE; NUMBERS	The purpose of this paper is to propose a useful likelihood measure for determining scalar function order relations and developing a novel likelihood-based optimization ordering model for solving multiple criteria group decision making (MCGDM) problems based on Pythagorean fuzzy (PF) sets. This paper scrutinizes PF order relations based on scalar functions to compare sophisticated uncertain information and establish a precedence order. By way of scalar function order relations, this paper utilizes scalar functions that are associated with Pythagorean membership grades and admissible upper approximations to present a novel likelihood measure in PF contexts. With the aid of useful concepts, such as levels of agreement and disagreement and comprehensive performance values, this paper originates a PF likelihood-based optimization ordering model to acquire the optimal group consensus solution for addressing MCGDM problems. Practical applications and several comparative studies are performed to reveal the practicality and strong points of the proposed methodology in tackling real-world MCGDM issues within uncertain environments of PF sets. This paper finds that the new scalar function-based likelihood measure is more flexible and beneficial than the current probability distribution approach. Furthermore, an easy-to-use algorithmic procedure can realize the proposed methodology to efficaciously process sophisticated PF information and improve the understandability of a decision model via a likelihood comparison approach. The originality and main contributions of this work are fourfold: (1) A PF likelihood measure is introduced as a basis for scalar function order relations; (2) the PF likelihood-based optimization ordering model is established for consensus ranking; (3) a predominant procedure is constructed for addressing PF information; and (4) the likelihood-based decision models are enriched under complex uncertainty.																	0941-0643	1433-3058															10.1007/s00521-020-05278-8		AUG 2020											
J								Memristive self-learning logic circuit with application to encoder and decoder	NEURAL COMPUTING & APPLICATIONS										Memristor; Logic circuit; Self-learning; Boolean logic; Encoder	DESIGN; OPERATIONS; METHODOLOGY	Different logic circuits based on memristors have been extensively investigated. However, most of these circuits require accurate initialization. A self-learning logic circuit based on mermristors that can achieve various logic gates without initialization is proposed in this paper. Three functional blocks, including a sum block, a learning block, and a compare block, are elaborately designed in the proposed logic circuit. Programmable switches in the sum and compare blocks enable the circuit to perform various logic gates, such as Boolean, IMPLY, and random logical combinations. In these various logical operations, the learning block can automatically obtain different memristance states. The aforementioned logic operations can easily be extended to multi-fan-in logic and logical cascade operations. Circuit designs of an encoder and decoder are considered as application examples. Finally, PSpice simulation results of the logic circuits and extended applications are provided. Simulation results indicate that the proposed circuit can effectively perform different logic operations and exhibits excellent robustness to circuit device variations.																	0941-0643	1433-3058															10.1007/s00521-020-05281-z		AUG 2020											
J								Sales forecasting in rapid market changes using a minimum description length neural network	NEURAL COMPUTING & APPLICATIONS										Sales forecasting; Neural network; Minimum description length; Decision support system	EXTREME LEARNING-MACHINE; FASHION; MODEL	In this study, we address a real sales forecasting problem of a multinational fashion retailer. The fashion retailer has been operating retail stores and warehouses in a major Asian city for decades. The volume of daily sales of each stock keeping unit in this city needs to be determined to develop an effective and efficient inventory and logistics system for the retailer. Based on complicated real sales data sets, we use a minimum description length neural network (MDL-NN) which searches for the optimal model size to predict this value. In order to address the problem of intermittent sales data and zero actual sales, we propose a revised mean absolute percentage error (RMAPE) measurement for performance evaluation. Our experimental results show that for most test data sets, the evaluation results based on RMAPE are consistent with those based on two other established measurements, the symmetric mean average percentage error and mean absolute scaled error. Specifically, on all three performance indicators, the MDL-NN method achieves a smaller error than almost all other commonly used sales forecasting methods on almost all test data sets. Finally, we examine the gap between the performance indicators between the forecasting result values in decimal format, which are directly generated by the forecasting methods, and the forecasting result values in integer format, which are rounded off from the decimal numbers. Our findings indicate that this gap is not trivial and should not be ignored.																	0941-0643	1433-3058															10.1007/s00521-020-05294-8		AUG 2020											
J								Improving aspect-level sentiment analysis with aspect extraction	NEURAL COMPUTING & APPLICATIONS										ALSA; AE; Knowledge transfer		Aspect-based sentiment analysis (ABSA), a popular research area in NLP, has two distinct parts-aspect extraction (AE) and labelling the aspects with sentiment polarity (ALSA). Although distinct, these two tasks are highly correlated. The work primarily hypothesizes that transferring knowledge from a pre-trained AE model can benefit the performance of ALSA models. Based on this hypothesis, word embeddings are obtained during AE and, subsequently, feed that to the ALSA model. Empirically, this work shows that the added information significantly improves the performance of three different baseline ALSA models on two distinct domains. This improvement also translates well across domains between AE and ALSA tasks.																	0941-0643	1433-3058															10.1007/s00521-020-05287-7		AUG 2020											
J								Multi-Variate vocal data analysis for Detection of Parkinson disease using Deep Learning	NEURAL COMPUTING & APPLICATIONS										Parkinson; Disease detection; Acoustic data; Machine learning; Deep learning		Machine learning (ML) and Deep learning (DL) methods are differently implemented with various decision-making abilities. Particularly, the usage of ML and DL techniques in disease detection is inevitable in the near future. This work uses the ability of acoustic-based DL techniques for detecting Parkinson disease symptoms. This disease can be identified by many DL techniques such as deep knowledge creation networks and recurrent networks. The proposed Deep Multi-Variate Vocal Data Analysis (DMVDA) System has been designed and implemented using Acoustic Deep Neural Network (ADNN), Acoustic Deep Recurrent Neural Network (ADRNN), and Acoustic Deep Convolutional Neural Network (ADCNN). Further, DMVDA has been specially developed with absolute multi-variate speech attribute processing algorithm for effective value creation. In order to improve the benefits of this speech-processing algorithm, the DMVDA has acoustic data sampling procedures. The DL techniques introduced in this work helps to identify Parkinson symptoms by analyzing the heterogeneous dataset. The integration of these techniques produces nominal 3% increase in the performance than the existing techniques.																	0941-0643	1433-3058															10.1007/s00521-020-05233-7		AUG 2020											
J								Identifying epidemic spreading dynamics of COVID-19 by pseudocoevolutionary simulated annealing optimizers	NEURAL COMPUTING & APPLICATIONS										COVID-19; Epidemic spreading; Evolutionary computation; Complex network; Prediction	PARAMETER-ESTIMATION; MATHEMATICAL-THEORY; MODELS; NETWORKS; PREDICTION; ALGORITHM; SYSTEMS	At the end of 2019, a new coronavirus (COVID-19) epidemic has triggered global public health concern. Here, a model integrating the daily intercity migration network, which constructed from real-world migration records and the Susceptible-Exposed-Infected-Removed model, is utilized to predict the epidemic spreading of the COVID-19 in more than 300 cities in China. However, the model has more than 1800 unknown parameters, which is a challenging task to estimate all unknown parameters from historical data within a reasonable computation time. In this article, we proposed a pseudocoevolutionary simulated annealing (SA) algorithm for identifying these unknown parameters. The large volume of unknown parameters of this model is optimized through three procedures co-adapted SA-based optimization processes, respectively. Our results confirm that the proposed method is both efficient and robust. Then, we use the identified model to predict the trends of the epidemic spreading of the COVID-19 in these cities. We find that the number of infections in most cities in China has reached their peak from February 29, 2020, to March 15, 2020. For most cities outside Hubei province, the total number of infected individuals would be less than 100, while for most cities in Hubei province (exclude Wuhan), the total number of infected individuals would be less than 3000.																	0941-0643	1433-3058															10.1007/s00521-020-05285-9		AUG 2020											
J								Confidential information protection method of commercial information physical system based on edge computing	NEURAL COMPUTING & APPLICATIONS										Edge computing; Data security; Business information physical system; Identity authentication; Confidential protection	AUTHENTICATION; RECOGNITION; INTERNET	With the rapid integration and wide application of the enterprise Internet of Things, big data and 5G-class large networks, traditional enterprise cloud computing systems cannot timely process various mass information data generated by connecting with network edge electronic devices. There are obvious technical disadvantages. In order to effectively solve this complex problem, edge mobile computing came into being. The purpose of this article is to study the protection methods of commercial confidential information, using the security relationship between data information in edge technology computing and the technical characteristics of data privacy information protection. This paper proposes a theoretical system and technical architecture centered on the use of data security technology. The three key technologies of access control, identity authentication and information privacy security protection are researched on the privacy protection processing methods of commercial information security physical systems. The experimental data show that the recognition errors mainly occur in identifying non-anomalous data as abnormal data. The analysis and identification of abnormal information data are basically accurate, and it can quickly complete various processing tasks to eliminate abnormal information data to meet the requirements. The experimental data show that the abnormal data can be used to monitor the security of the commercial information physical system, and it is also a good security protection method for commercial confidential information. It has guiding significance for the confidential information protection of commercial information physical systems. In the next few years, more than 50% of major data applications will need to be analyzed, processed and data stored at the edge of the network. Cloud computing technologies at the edge are widely used.																	0941-0643	1433-3058															10.1007/s00521-020-05272-0		AUG 2020											
J								Termite inspired algorithm for traffic engineering in hybrid software defined networks	PEERJ COMPUTER SCIENCE										Multi Commodity Flow (MCF); Software Defined Networking (SDN); Termite-inspired; Traffic Engineering; Hybrid SDN	TIME	In the era of Internet of Things and 5G networks, handling real time network traffic with the required Quality of Services and optimal utilization of network resources is a challenging task. Traffic Engineering provides mechanisms to guide network traffic to improve utilization of network resources and meet requirements of the network Quality of Service (QoS). Traditional networks use IP based and Multi-Protocol Label Switching (MPLS) based Traffic Engineering mechanisms. Software Defined Networking (SDN) have characteristics useful for solving traffic scheduling and management. Currently the traditional networks are not going to be replaced fully by SDN enabled resources and hence traffic engineering solutions for Hybrid IP/SDN setups have to be explored. In this paper we propose a new Termite Inspired Optimization algorithm for dynamic path allocation and better utilization of network links using hybrid SDN setup. The proposed bioinspired algorithm based on Termite behaviour implemented in the SDN Controller supports elastic bandwidth demands from applications, by avoiding congestion, handling traffic priority and link availability. Testing in both simulated and physical test bed demonstrate the performance of the algorithm with the support of SDN. In cases of link failures, the algorithm in the SDN Controller performs failure recovery gracefully. The algorithm also performs very well in congestion avoidance. The SDN based algorithm can be implemented in the existing traditional WAN as a hybrid setup and is a less complex, better alternative to the traditional MPLS Traffic Engineering setup.																	2376-5992					AUG 17	2020									e283	10.7717/peerj-cs.283													
J								Database limitations for studying the human gut microbiome	PEERJ COMPUTER SCIENCE										Human Microbiome; Database; Gut microbiome; Functional diversity; KEGG; Uniprot; EggNOG	CARBOHYDRATE-ACTIVE ENZYMES; GENE; KEGG	Background. In the last twenty years, new methodologies have made possible the gathering of large amounts of data concerning the genetic information and metabolic functions associated to the human gut microbiome. In spite of that, processing all this data available might not be the simplest of tasks, which could result in an excess of information awaiting proper annotation. This assessment intended on evaluating how well respected databases could describe a mock human gut microbiome. Methods. In this work, we critically evaluate the output of the cross-reference between the Uniprot Knowledge Base (Uniprot KB) and the Kyoto Encyclopedia of Genes and Genomes Orthologs (KEGG Orthologs) or the evolutionary genealogy of genes: Non-supervised Orthologous groups (EggNOG) databases regarding a list of species that were previously found in the human gut microbiome. Results. From a list which contemplates 131 species and 52 genera, 53 species and 40 genera had corresponding entries for KEGG Database and 82 species and 47 genera had corresponding entries for EggNOG Database. Moreover, we present the KEGG Orthologs (KOs) and EggNOG Orthologs (NOGs) entries associated to the search as their distribution over species and genera and lists of functions that appeared in many species or genera, the "core'' functions of the human gut microbiome. We also present the relative abundance of KOs and NOGs throughout phyla and genera. Lastly, we expose a variance found between searches with different arguments on the database entries. Inferring functionality based on cross-referencing UniProt and KEGG or EggNOG can be lackluster due to the low number of annotated species in Uniprot and due to the lower number of functions affiliated to the majority of these species. Additionally, the EggNOG database showed greater performance for a cross-search with Uniprot about a mock human gut microbiome. Notwithstanding, efforts targeting cultivation, single-cell sequencing or the reconstruction of high-quality metagenome-assembled genomes (MAG) and their annotation are needed to allow the use of these databases for inferring functionality in human gut microbiome studies.																	2376-5992					AUG 17	2020									e289	10.7717/peerj-cs.289													
J								Modular automatic design of collective behaviors for robots endowed with local communication capabilities	PEERJ COMPUTER SCIENCE										Swarm robotics; Communication; Automatic design; Swarm; Robotics	ALGORITHM; EVOLUTION; SWARM; COOPERATION; STRATEGIES; EMERGENCE; SELECTION; SYSTEM	We investigate the automatic design of communication in swarm robotics through two studies. We first introduce Gianduja an automatic design method that generates collective behaviors for robot swarms in which individuals can locally exchange a message whose semantics is not a priori fixed. It is the automatic design process that, on a per-mission basis, defines the conditions under which the message is sent and the effect that it has on the receiving peers. Then, we extend Gianduja to Gianduja2 and Gianduja3, which target robots that can exchange multiple distinct messages. Also in this case, the semantics of the messages is automatically defined on a per-mission basis by the design process. Gianduja and its variants are based on Chocolate, which does not provide any support for local communication. In the article, we compare Gianduja and its variants with a standard neuro-evolutionary approach. We consider a total of six different swarm robotics missions. We present results based on simulation and tests performed with 20 e-puck robots. Results show that, typically, Gianduja and its variants are able to associate a meaningful semantics to messages.																	2376-5992					AUG 17	2020									e291	10.7717/peerj-cs.291													
J								RGB-D camera calibration and trajectory estimation for indoor mapping	AUTONOMOUS ROBOTS										RGB-D; Computer vision; 3D mapping; Camera calibration	MODEL	In this paper, we present a system for estimating the trajectory of a moving RGB-D camera with applications to building maps of large indoor environments. Unlike the current most researches, we propose a 'feature model' based RGB-D visual odometry system for a computationally-constrained mobile platform, where the 'feature model' is persistent and dynamically updated from new observations using a Kalman filter. In this paper, we firstly propose a mixture of Gaussians model for the depth random noise estimation, which is used to describe the spatial uncertainty of the feature point cloud. Besides, we also introduce a general depth calibration method to remove systematic errors in the depth readings of the RGB-D camera. We provide comprehensive theoretical and experimental analysis to demonstrate that our model based iterative-closest-point (ICP) algorithm can achieve much higher localization accuracy compared to the conventional ICP. The visual odometry runs at frequencies of 30 Hz or higher, on VGA images, in a single thread on a desktop CPU with no GPU acceleration required. Finally, we examine the problem of place recognition from RGB-D images, in order to form a pose-graph SLAM approach to refining the trajectory and closing loops. We evaluate the effectiveness of the system on using publicly available datasets with ground-truth data. The entire system is available for free and open-source online.																	0929-5593	1573-7527				NOV	2020	44	8					1485	1503		10.1007/s10514-020-09941-w		AUG 2020											
J								Approaching the cold-start problem using community detection based alternating least square factorization in recommendation systems	EVOLUTIONARY INTELLIGENCE										Collaborative filtering; Cold-start; Louvain's community detection algorithm; Alternating least square; Recommender systems	INFORMATION; ADDRESS	In e-commerce, the opinion of users about products and the reviews are identified using recommender systems. Collaborative filtering techniques are popularly used techniques for giving recommendations to the users. One of the common challenges in the collaborative filtering technique for giving recommendations is cold start problem, which occurs due to insufficient information about new items and new users. This paper proposes a hybrid approach entitled LA-ALS to address the cold start problem to provide effective recommendations. The LA-ALS approach makes use of the benefits of both Louvain's algorithm and alternating least square algorithm. The Louvain's algorithm is used to analyze the relationship between users and alternating least square algorithm is used to predict recommendations. Experiments are carried out by using real-world datasets such as Movielens and Facebook databases. The effectiveness of the LA-ALS approach is shown with two parameters namely mean absolute error and root mean square error. The results showed that LA-ALS approach generated better recommendations when compared with the existing techniques such as k-nearest neighbors and singular value decomposition.																	1864-5909	1864-5917															10.1007/s12065-020-00464-y		AUG 2020											
J								Fuzzy entropies for class-specific and classification-based attribute reducts in three-way probabilistic rough set models	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Three-way decision; Fuzzy entropy; Class-specific attribute reduct; Classification-based attribute reduct; Probabilistic rough set model	DECISION; UNCERTAINTY; FUZZINESS; BIREDUCTS; APPROXIMATION; ACQUISITION; NEGATION	There exist two formulations of the theory of rough sets, consisting of the conceptual formulations and the computational formulations. Class-specific and classification-based attribute reducts are two crucial notions in three-way probabilistic rough set models. In terms of conceptual formulations, the two types of attribute reducts can be defined by considering probabilistic positive or negative region preservations of a decision class and a decision classification, respectively. However, in three-way probabilistic rough set models, there are few studies on the computational formulations of the two types of attribute reducts due to the non-monotonicity of probabilistic positive and negative regions. In this paper, we examine the computational formulations of the two types of attribute reducts in three-way probabilistic rough set models based on fuzzy entropies. We construct monotonic measures based on fuzzy entropies, from which we can obtain the computational formulations of the two types of attribute reducts. On this basis, we develop algorithms for finding the two types of attribute reducts based on addition-deletion method or deletion method. Finally, the experimental results verify the monotonicity of the proposed measures with respect to the set inclusion of attributes and show that class-specific attribute reducts provide a more effective way of attribute reduction with respect to a particular decision class compared with classification-based attribute reducts.																	1868-8071	1868-808X															10.1007/s13042-020-01179-3		AUG 2020											
J								ADET: anomaly detection in time series with linear time	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Time series; Anomaly detection; Interval table; Extend interval table; Linear time	OUTLIER DETECTION	Time series data is ubiquitous in financial, biomedical, and other areas. Anomaly detection in time series has been widely researched in these areas. However, most existing algorithms suffer from "curse of dimension" and may lose some information in the process of feature extraction. In this paper, we propose two new data structures named interval table (ITable) and extend interval table (EITable) for time series representation to capture more original information. We also proposed ADET: a novelAnomalyDetection algorithm based onEITable, which only needs linear time to detect meaningful anomalies. Extensive experiments on eleven data sets of UCR Repository, MIT-BIH datasets, and the BIDMC database show that ADET has overall good performance in terms of AUC-ROC and outperforms other algorithms in time complexity.																	1868-8071	1868-808X															10.1007/s13042-020-01171-x		AUG 2020											
J								A Novel Local Motion Planner: Navibug	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Path planning; Collision avoidance; Mobile robotics		In this study, a new local path planning scheme is presented. It is a reactive path planning method, but it can also provide solutions to local minimums using partial or total map information. The proposed method, referred to as Navibug, processes the field scan data at the local level and can perform global planning by processing map information (not mandatory). The algorithm operates by switching between defined main actions: global path planning, path traversing and passage determination strategy. Kinematic and physical maneuvering constraints are majorly neglected in similar methods evaluated in the geometric planner category. However, the collision avoidance routine, defined under the Navibug scheme, raises the maneuverability capability by forcing the instant decisions to be on a curvilinear trajectory. Such an implementation outperforms the feasibility of the conventional geometric-reactive planners. Global map information is not one of the mandatory inputs of the algorithm, but if the map is known, the path tracking strategy operates in coordination with the dynamic path planning mode to improve the total performance of the main scheme.																	0921-0296	1573-0409															10.1007/s10846-020-01239-4		AUG 2020											
J								A deep attention-based ensemble network for real-time face hallucination	JOURNAL OF REAL-TIME IMAGE PROCESSING										Face hallucination; Attention mechanism; Residual learning; Ensemble model		Face hallucination (FH) aims to reconstruct high-resolution faces from low-resolution face inputs, making it significant to other face-related tasks. Different from general super resolution issue, it often requires facial priors other than general extracted features thus leading to fusion of more than one kind of feature. The existing CNN-based FH methods often fuse different features indiscriminately which may introduce noises. Also the latent relations among different features which may be useful are taken into less consideration. To address the above issues, we propose an end-to-end deep ensemble network which aggregates three extraction sub-nets in attention-based manner. In our ensemble strategy, both relations among different features and inter-dependencies among different channels are dug out through the exploitation of spatial attention and channel attention. And for the diversity of extracted features, we aggregate three different sub-nets, which are the basic sub-net for basic features, the auto-encoder sub-net for facial shape priors and the dense residual attention sub-net for fine-grained texture features. Conducted ablation studies and experimental results show that our method achieves effectiveness not only in PSNR (Peak Signal to Noise Ratio) and SSIM (Structural Similarity Index) metrics but more importantly in clearer details within both key facial areas and whole range. Also results show that our method achieves real-time hallucinating faces by generating one image in 0.0237s.																	1861-8200	1861-8219															10.1007/s11554-020-01009-3		AUG 2020											
J								Ensembles of extremely randomized predictive clustering trees for predicting structured outputs	MACHINE LEARNING										Multi-target regression; Multi-label classification; Hierarchical multi-label classification; Structured output prediction; Feature ranking; Ensembles; Extremely randomized trees; Predictive clustering trees	MULTITARGET REGRESSION; CLASSIFICATION; ALGORITHMS; INDUCTION	We address the task of learning ensembles of predictive models for structured output prediction (SOP). We focus on three SOP tasks: multi-target regression (MTR), multi-label classification (MLC) and hierarchical multi-label classification (HMC). In contrast to standard classification and regression, where the output is a single (discrete or continuous) variable, in SOP the output is a data structure-a tuple of continuous variables MTR, a tuple of binary variables MLC or a tuple of binary variables with hierarchical dependencies (HMC). SOP is gaining increasing interest in the research community due to its applicability in a variety of practically relevant domains. In this context, we consider theExtra-Treeensemble learning method-the overall top performer in the DREAM4 and DREAM5 challenges for gene network reconstruction. We extend this method for SOP tasks and call the extensionExtra-PCTsensembles. As base predictive models we propose using predictive clustering trees (PCTs)-a generalization of decision trees for predicting structured outputs. We conduct a comprehensive experimental evaluation of the proposed method on a collection of 41 benchmark datasets: 21 for MTR, 10 for MLC and 10 for HMC. We first investigate the influence of the size of the ensemble and the size of the feature subset considered at each node. We then compare the performance ofExtra-PCTsto other ensemble methods (random forests and bagging), as well as to single PCTs. The experimental evaluation reveals that theExtra-PCTsachieve optimal performance in terms of predictive power and computational cost, with 50 base predictive models across the three tasks. The recommended values for feature subset sizes vary across the tasks, and also depend on whether the dataset contains only binary and/or sparse attributes. TheExtra-PCTsgive better predictive performance than a single tree (the differences are typically statistically significant). Moreover, theExtra-PCTsare the best performing ensemble method (except for the MLC task, where performances are similar to those of random forests), andExtra-PCTscan be used to learn good feature rankings for all of the tasks considered here.																	0885-6125	1573-0565															10.1007/s10994-020-05894-4		AUG 2020											
J								An Effective Approach for Noise Robust and Rotation Invariant Handwritten Character Recognition Using Zernike Moments Features and Optimal Similarity Measure	APPLIED ARTIFICIAL INTELLIGENCE											GENERIC ORTHOGONAL MOMENTS; JACOBI-FOURIER MOMENTS	Zernike moments (ZMs) are very effective orthogonal rotation invariant moments. Conventionally, the magnitudes of ZMs are used as feature descriptors and the Euclidean distance is used as a classifier. Recently, a few classifiers based on ZM magnitude and phase have been developed which are reported to be very effective in pattern matching problems. One such a recently developed similarity measure, known as optimal similarity measure, is known to provide very good performance over the ZM magnitude-based Euclidean distance measure in pattern recognition problems, especially under noisy conditions. In this paper, we investigate the conventional magnitude-based similarity measure and the new similarity measures including the optimal similarity measure and compare their performance on segmented handwritten characters and numerals. It is observed that the performance of optimal similarity measure is far better than all other similarity measures. Its performance is very much better than other similarity measures even under very high noisy condition. However, it is slow owing to the optimization of the process involved in its computation. Therefore, we also propose a fast algorithm for its computation and reduce its time complexity. Detailed experimental results are provided to support the above observations.																	0883-9514	1087-6545				NOV 9	2020	34	13					1011	1037		10.1080/08839514.2020.1796370		AUG 2020											
J								Forecasting mortality rates using hybrid Lee-Carter model, artificial neural network and random forest	COMPLEX & INTELLIGENT SYSTEMS										Mortality rates; Lee-Carter (LC) model; Autoregressive integrated moving average (ARIMA); Artificial neural network (ANN); Random forest (RF)		Inaccurate prediction would cause the insurance company encounter catastrophic losses and may lead to overpriced premiums where low-earning consumers cannot afford to insure themselves. The ability to forecast mortality rates accurately can allow the insurance company to take preventive measures to introduce new policies with reasonable prices. In this paper, several Lee-Carter (LC) based models are used to forecast the mortality rates in a case study of the Malaysian population. The LC-ARIMA model and also a combination of the LC model with two machine learning (ML) methods, namely the random forest (RF) and artificial neural network (ANN) methods are utilized on the prediction of mortality rates for males and females in Malaysia, whereby the LC-Random Forest (LC-RF) hybrid model is a new model that is introduced in this paper. Seventeen years of mortality data in Malaysia are selected as the dataset for this research. To analyze how the forecasting models perform for other countries, we have determined the model that has the best fit and produced the best forecasted mortality rates for all the other countries that are studied. This research has showed that LC-ANN and LC-ARIMA are the best model in predicting the mortality rates of males and females in Malaysia, respectively. This study has also found that the LC-ARIMA model is the best performing model in forecasting the mortality rates in countries that have longer life expectancy and a good healthcare system such as Sweden, Ireland, Japan, Hong Kong, Norway, Switzerland and Czechia. In contrast, the LC-ANN model is the best performing model in forecasting the mortality rates in countries that have a less efficiency, less accessibility healthcare system, and bad personal behavior such as Malaysia, Canada and Latvia.																	2199-4536	2198-6053															10.1007/s40747-020-00185-w		AUG 2020											
J								Solving Rolling Shutter 3D Vision Problems using Analogies with Non-rigidity	INTERNATIONAL JOURNAL OF COMPUTER VISION										Rolling shutter; Absolute pose; Structure-from-motion; Non-rigid; Shape-from-template	SHAPE-FROM-TEMPLATE; MOTION	We propose an original approach to absolute pose and structure-from-motion (SfM) which handles rolling shutter (RS) effects. Unlike most existing methods which either augment global shutter projection with velocity parameters or impose continuous time and motion through pose interpolation, we use local differential constraints. These are established by drawing analogies with non-rigid 3D vision techniques, namely shape-from-template and non-rigid SfM (NRSfM). The proposed idea is to interpret the images of a rigid surface acquired by a moving RS camera as those of a virtually deformed surface taken by a GS camera. These virtually deformed surfaces are first recovered by relaxing the RS constraint using SfT or NRSfM. Then we upgrade the virtually deformed surface to the actual rigid structure and compute the camera pose and ego-motion by reintroducing the RS constraint. This uses a new 3D-3D registration procedure that minimizes a cost function based on the Euclidean 3D point distance. This is more stable and physically meaningful than the reprojection error or the algebraic distance used in previous work. Experimental results obtained with synthetic and real data show that the proposed methods outperform existing ones in terms of accuracy and stability, even in the known critical configurations.																	0920-5691	1573-1405															10.1007/s11263-020-01368-1		AUG 2020											
J								Label Embedding for Multi-label Classification Via Dependence Maximization	NEURAL PROCESSING LETTERS										Multi-label learning; Label embedding; Low-rank factorization; Hilbert-Schmidt independence criterion; Missing labels	MATRIX COMPLETION; MODEL	Multi-label classification has aroused extensive attention in various fields. With the emergence of high-dimensional label space, academia has devoted to performing label embedding in recent years. Whereas current embedding approaches do not take feature space correlation sufficiently into consideration or require an encoding function while learning embedded space. Besides, few of them can be spread to track the missing labels. In this paper, we propose a Label Embedding method via Dependence Maximization (LEDM), which obtains the latent space on which the label and feature information can be embedded simultaneously. To end this, the low-rank factorization model on the label matrix is applied to exploit label correlations instead of the encoding process. The dependence between feature space and label space is increased by the Hilbert-Schmidt independence criterion to facilitate the predictability. The proposed LEDM can be easily extended the missing labels in learning embedded space at the same time. Comprehensive experimental results on data sets validate the effectiveness of our approach over the state-of-art methods on both complete-label and missing-label cases.																	1370-4621	1573-773X				OCT	2020	52	2			SI		1651	1674		10.1007/s11063-020-10331-7		AUG 2020											
J								Credibility-based fuzziness and incomplete information value in fuzzy programming	EVOLUTIONARY INTELLIGENCE										Credibility measure; Two-stage fuzzy programming; The value of incomplete information	DECOMPOSITION; SIMULATION; MANAGEMENT; NETWORK; MODEL	The research shows that traditional possibility measure has defects in dealing with fuzzy programming, while the credibility measure with self-duality is more proved better. Based on the credibility theory, many researchers study the information value and fuzziness under complete information. However, most of the real-life decision-making problems have incomplete information, and the above research methods cannot solve the problem in this situation. Therefore, based on credibility theory, we investigate the incomplete information value and fuzziness when the fuzzy information is incomplete in the fuzzy programming by employing the two-stage method. To measure the maximum size of paying for incomplete information and the importance of fuzziness, we present two optimal indices which are the expected value of incomplete information and the value of fuzzy solution, and study their theoretical properties and rationality by some numerical examples. The theoretical results obtained in this paper. The results obtained in this paper theoretically guarantees the effectiveness of fuzzy decision making on the incomplete information value in fuzzy systems.																	1864-5909	1864-5917															10.1007/s12065-020-00467-9		AUG 2020											
J								Circus in Motion: a multimodal exergame supporting vestibular therapy for children with autism	JOURNAL ON MULTIMODAL USER INTERFACES										Autism; Exergame; Vestibular system; Children	BALANCE; TOUCH	Exergames are serious games that involve physical exertion and are thought of as a form of exercise by using novel input models. Exergames are promising in improving the vestibular differences of children with autism but often lack of adaptation mechanisms that adjust the difficulty level of the exergame. In this paper, we present the design and development of Circus in Motion, a multimodal exergame supporting children with autism with the practice of non-locomotor movements. We describe how the data from a 3D depth camera enables the tracking of non-locomotor movements allowing children to naturally interact with the exergame . A controlled experiment with 12 children with autism shows Circus in Motion excels traditional vestibular therapies in increasing physical activation and the number of movements repetitions. We show how data from real-time usage of Circus in Motion could be used to feed a fuzzy logic model that can adjust the difficulty level of the exergame according to each childs motor performance. We close discussing open challenges and opportunities of multimodal exergames to support motor therapeutic interventions for children with autism in the long-term.																	1783-7677	1783-8738															10.1007/s12193-020-00345-9		AUG 2020											
J								Learning representations from dendrograms	MACHINE LEARNING										Representation learning; Unsupervised learning; Ensemble method; Feature extraction; Dendrogram	GRAPHS	We propose unsupervised representation learning and feature extraction from dendrograms. The commonly used Minimax distance measures correspond to building a dendrogram with single linkage criterion, with defining specific forms of a level function and a distance function over that. Therefore, we extend this method to arbitrary dendrograms. We develop a generalized framework wherein different distance measures and representations can be inferred from different types of dendrograms, level functions and distance functions. Via an appropriate embedding, we compute a vector-based representation of the inferred distances, in order to enable many numerical machine learning algorithms to employ such distances. Then, to address the model selection problem, we study the aggregation of different dendrogram-based distances respectively in solution space and in representation space in the spirit of deep representations. In the first approach, for example for the clustering problem, we build a graph with positive and negative edge weights according to the consistency of the clustering labels of different objects among different solutions, in the context of ensemble methods. Then, we use an efficient variant of correlation clustering to produce the final clusters. In the second approach, we investigate the combination of different distances and features sequentially in the spirit of multi-layered architectures to obtain the final features. Finally, we demonstrate the effectiveness of our approach via several numerical studies.																	0885-6125	1573-0565				SEP	2020	109	9-10			SI		1779	1802		10.1007/s10994-020-05895-3		AUG 2020											
J								Interpretable clustering: an optimization approach	MACHINE LEARNING										Clustering; Interpretability; Unsupervised learning; Mixed integer optimization	RISK-FACTOR; VALIDATION	State-of-the-art clustering algorithms provide little insight into the rationale for cluster membership, limiting their interpretability. In complex real-world applications, the latter poses a barrier to machine learning adoption when experts are asked to provide detailed explanations of their algorithms' recommendations. We present a new unsupervised learning method that leverages Mixed Integer Optimization techniques to generate interpretable tree-based clustering models. Utilizing a flexible optimization-driven framework, our algorithm approximates the globally optimal solution leading to high quality partitions of the feature space. We propose a novel method which can optimize for various clustering internal validation metrics and naturally determines the optimal number of clusters. It successfully addresses the challenge of mixed numerical and categorical data and achieves comparable or superior performance to other clustering methods on both synthetic and real-world datasets while offering significantly higher interpretability.																	0885-6125	1573-0565															10.1007/s10994-020-05896-2		AUG 2020											
J								Causal Reasoning and Meno's Paradox	AI & SOCIETY										Causal epistemology; Causal reasoning; Automation question; Meno's Paradox	VIRTUE	Causal reasoning is an aspect of learning, reasoning, and decision-making that involves the cognitive ability to discover relationships between causal relata, learn and understand these causal relationships, and make use of this causal knowledge in prediction, explanation, decision-making, and reasoning in terms of counterfactuals. Can we fully automate causal reasoning? One might feel inclined, on the basis of certain groundbreaking advances in causal epistemology, to reply in the affirmative. The aim of this paper is to demonstrate that one still has good skeptical grounds for resisting any conclusions in favour of the automation of causal reasoning. If by causal reasoning is meant the entirety of the process through which we discover causal relationships and make use of this knowledge in prediction, explanation, decision-making, and reasoning in terms of counterfactuals, then one relies besides on tacit knowledge, as might be constituted by or derived from the epistemic faculty virtues and abilities of the causal reasoner, the value systems and character traits of the causal reasoner, the implicit knowledge base available to the causal reasoner, and the habits that sustain our causal reasoning practices. While certain aspects of causal reasoning may be axiomatized and formalized and algorithms may be implemented to approximate causal reasoning, one has to remain skeptical about whether causal reasoning may be fully automated. This demonstration will involve an engagement with Meno's Paradox.																	0951-5666	1435-5655															10.1007/s00146-020-01037-4		AUG 2020											
J								Positioning push-pull boundary in a hesitant fuzzy environment	EXPERT SYSTEMS										analytic network process; apparel supply chain; customer order decoupling point; hesitant fuzzy sets; VIKOR	ORDER DECOUPLING POINT; DECISION-MAKING APPROACH; SUPPLY CHAIN; PROGRAMMING METHOD; MCDM APPROACH; VIKOR METHOD; TO-ORDER; SELECTION; INFORMATION; PERFORMANCE	Nowadays, fierce competition enforces supply chain planners to develop market-oriented production strategies. Customer order decoupling point (CODP) could increase the supply chain efficiency and responsiveness simultaneously. The right position of CODP in production industries will result in a pattern for trade-off between responsiveness and operational efficiency. The purpose of this paper is to address the positioning problem of a push-pull boundary in a fuzzy hesitant environment. To this end, a hybrid multi-criteria decision-making methodology of analytic network process (ANP) and VIKOR is proposed in a hesitant judgement environment to determine the position of CODP in a supply chain. Finally, CODP positioning in the apparel supply chain, as an industry-based example, is analysed to show the applicability of the proposed method.																	0266-4720	1468-0394														e12616	10.1111/exsy.12616		AUG 2020											
J								Designing a context-aware model for RPL load balancing of low power and lossy networks in the internet of things	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Load balancing; Context-aware; Routing protocol; Internet of things; Flabellum algorithm	ROUTING PROTOCOL	The IPv6 routing protocol (RPL) for low power and lossy Networks (LLNs) was accepted as the standard routing protocol for the IoT by IETF in March 2012. Since then, it has been used for different IoT applications. Although the RPL deals considerably with IoT network requirements, there are still some open-ended problems to solve, for it was not initially designed for IoT applications. This paper addresses the RPL problems including load imbalance, which causes congestion in some nodes, significantly reduces the network performance, and decreases node energy and network lifetime. This paper proposes the automata-ant colony based multiple recursive RPL (AMRRPL), which is a modified version of the RPL for IoT networks, and uses a balancing model to avoid congestion. As a result, it will reduce network energy consumption, prolong the network lifetime, and reduce packet loss. The AMRRPL is evaluated in three steps. First, a multi-hop return objective function is presented based on the ant colony and computes the rank according to node context. The second step develops a new parent selection mechanism dynamically selected by stochastic automata and dynamic metrics for an optimal parent. General evaluation results show that this algorithm can make better decisions with regard to the optimal parent instead of making decisions simply based on the parent's rank. The third step resolve bottlenecks and swarm problems by managing the moving nodes through the heuristic flabellum algorithm inspired by physical and biological behaviour of flabella in the sea. Finally, the proposed algorithm performance is evaluated through the Cooja simulator. The proposed algorithm shows significant improvements in packet delivery and network lifetime, energy and convergence.																	1868-5137	1868-5145															10.1007/s12652-020-02382-4		AUG 2020											
J								Resampling-based noise correction for crowdsourcing	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Crowdsourcing learning; multiple noisy labels; integrated labels; noise correction	STATISTICAL COMPARISONS; LABEL NOISE; QUALITY; CLASSIFIERS; MODEL	Crowdsourcing services provide an economic and efficient means of acquiring multiple noisy labels for each training instance in supervised learning. Ground truth inference methods, also known as consensus methods, are then used to obtain the integrated labels of training instances. Although consensus methods are effective, there still exists a level of noise in the set of integrated labels. Therefore, it is necessary to handle noise in the integrated labels to improve label and model quality. In this paper, we propose a resampling-based noise correction method (simply RNC). Different from previous label noise correction methods for crowdsourcing, RNC first employs a filter to obtain a clean set and a noisy set and then repeatedly resamples the clean and noisy sets several times according to a certain proportion. Finally, multiple classifiers built on the resampled data sets are used to re-label the training data. Experimental results based on 18 simulated data sets and five real-world data sets demonstrate that RNC rarely degrades the label and model quality compared to other three state-of-the-art noise correction methods and, in many cases, improves quality dramatically.																	0952-813X	1362-3079															10.1080/0952813X.2020.1806519		AUG 2020											
J								Social Group Optimization-Assisted Kapur's Entropy and Morphological Segmentation for Automated Detection of COVID-19 Infection from Computed Tomography Images	COGNITIVE COMPUTATION										COVID-19 infection; CT scan image; Fused feature vector; KNN classifier; Segmentation and detection accuracy	CT FINDINGS; CHEST CT; CLASSIFICATION; PNEUMONIA; DATASET; MODEL	The coronavirus disease (COVID-19) caused by a novel coronavirus, SARS-CoV-2, has been declared a global pandemic. Due to its infection rate and severity, it has emerged as one of the major global threats of the current generation. To support the current combat against the disease, this research aims to propose a machine learning-based pipeline to detect COVID-19 infection using lung computed tomography scan images (CTI). This implemented pipeline consists of a number of sub-procedures ranging from segmenting the COVID-19 infection to classifying the segmented regions. The initial part of the pipeline implements the segmentation of the COVID-19-affected CTI using social group optimization-based Kapur's entropy thresholding, followed by k-means clustering and morphology-based segmentation. The next part of the pipeline implements feature extraction, selection, and fusion to classify the infection. Principle component analysis-based serial fusion technique is used in fusing the features and the fused feature vector is then employed to train, test, and validate four different classifiers namely Random Forest, K-Nearest Neighbors (KNN), Support Vector Machine with Radial Basis Function, and Decision Tree. Experimental results using benchmark datasets show a high accuracy (> 91%) for the morphology-based segmentation task; for the classification task, the KNN offers the highest accuracy among the compared classifiers (> 87%). However, this should be noted that this method still awaits clinical validation, and therefore should not be used to clinically diagnose ongoing COVID-19 infection.																	1866-9956	1866-9964				SEP	2020	12	5					1011	1023		10.1007/s12559-020-09751-3		AUG 2020											
J								A Revised Picture Fuzzy Linguistic Aggregation Operator and Its Application to Group Decision-Making	COGNITIVE COMPUTATION										Picture fuzzy linguistic set; Limitation; Novel operational law; Aggregation	SETS	The complexity of decision environments poses challenges for individuals engaged in decision-making proceedings. Picture fuzzy linguistic sets (PFLSs) are an effective tool for depicting the inherent subjective nature of human cognition. Aiming for the promotion of a general theory on PFLSs, we conduct a critical research to identify some limitations of a recent publication in Cognitive Computation [2018, 10(2), 242-259]. This published article is a meaningful and interesting study that initiates the PFLS as well as introduces the corresponding operations and the Archimedean picture fuzzy linguistic weighted arithmetic averaging operator. Unfortunately, we have carefully analyzed these operations and found that they may not be appropriate to all situations within picture fuzzy linguistic environment, which would result in the violation of human cognition. To eliminate this limitation, we explore and suggest novel operational laws and an aggregation operator for PFLSs from a modified version, so as to support further study on picture fuzzy linguistic group decision-making. Furthermore, a comparative and illustrative example is presented to validate the advantages of our modified operations., the research outcome will be of significant benefit to promoting the development of picture fuzzy linguistic decision-making theory. Accordingly, our contribution to improving the current research on PFLSs makes their application in solving realistic problems feasible and practical.																	1866-9956	1866-9964				SEP	2020	12	5					1070	1082		10.1007/s12559-020-09728-2		AUG 2020											
J								Distributed Rendezvous of Heterogeneous Robots with Motion-Based Power Level Estimation	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Networked system; Sensor networks; Power level estimation; Heterogeneous agent; Distributed rendezvous; Network connectivity; Obstacle; Power sharing	CONSENSUS TRACKING CONTROL; MULTIAGENT SYSTEMS; CONTROL STRATEGIES; COLLISION; LOCALIZATION; ALGORITHM	This article introduces distributed rendezvous algorithms to make all heterogeneous robots (agents) rendezvous at a designated agent in environments with many obstacles. The proposed control laws are developed considering heterogeneous agents having distinct power level, sensing range, or communication range. It is assumed that each agent is initially distributed in a cluttered environment. An agent cannot utilize global positioning systems (GPS) to localize itself. Each agent can only sense neighboring agents and moves based on proximity interaction. This article presents distributed rendezvous algorithms so that heterogeneous agents rendezvous at a designated agent while maintaining (directed) network connectivity. The speed of each agent is set proportional to the remaining power level of the agent. This article then presents a method of estimating the power of each agent by measuring the movement of the agent. Then, we present a method of sharing power if an agent is lack of power. Under the proposed approach, we achieve distributed rendezvous as well as power recharging. It is proved that multiple heterogeneous agents rendezvous at the designated agent while maintaining (directed) network connectivity. Utilizing MATLAB simulations, the performance of the proposed distributed controls is verified in an environment with many obstacles.																	0921-0296	1573-0409															10.1007/s10846-020-01243-8		AUG 2020											
J								Attributed network representation learning via improved graph attention with robust negative sampling	APPLIED INTELLIGENCE										Graph attention; Robust negative sampling; Weighted neighborhood attributes; Triplet loss	CONVOLUTIONAL NEURAL-NETWORKS	Attributed network representation learning is to embed graphs in low dimensional vector space such that the embedded vectors follow the differences and similarities of the source graphs. To capture structural features and node attributes of attributed network, we propose a novel graph auto-encoder method which is stacked encoder-decoder layers based on graph attention with robust negative sampling. Here, minimize the negative log-likelihood, triplet distance, and weighted neighborhood attributes are proposed as the loss function. To alleviate the over-fitting on reconstruct graph structural features or node attributes, a trade off algorithm between reconstruction loss of node attributes and reconstruction loss of structural features is proposed. Furthermore, to alleviate the impact of random sampling, we propose additional constraints on negative sampling based on node degree. Experimental results on several benchmark datasets for transductive and inductive learning tasks show that the proposed model is competitive against well-known methods in node classification and link prediction.																	0924-669X	1573-7497															10.1007/s10489-020-01825-x		AUG 2020											
J								TSCMF: Temporal and social collective matrix factorization model for recommender systems	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Social recommender system; Preference dynamics; Temporal dynamics; Collective matrix factorization	PREFERENCE DYNAMICS; TIME	In real-world recommender systems, user preferences are dynamic and typically change over time. Capturing the temporal dynamics of user preferences is essential to design an efficient personalized recommender system and has recently attracted significant attention. In this paper, we consider user preferences change individually over time. Moreover, based on the intuition that social influence can affect the users' preferences in a recommender system, we propose a Temporal and Social Collective Matrix Factorization model called TSCMF for recommendation. We jointly factorize the users' rating information and social trust information in a collective matrix factorization framework by introducing a joint objective function. We model user dynamics into this framework by learning a transition matrix of user preferences between two successive time periods for each individual user. We present an efficient optimization algorithm based on stochastic gradient descent for solving the objective function. The experiments on a real-world dataset illustrate that the proposed model outperforms the competitive methods. Moreover, the complexity analysis demonstrates that the proposed model can be scaled up to large datasets.																	0925-9902	1573-7675															10.1007/s10844-020-00613-w		AUG 2020											
J								Modeling pseudo-second-order kinetics of orange peel-paracetamol adsorption process using artificial neural network	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Artificial neural network; Pseudo second order kinetics; Orange peel activated carbon; Paracetamol; Adsorption	BAGASSE FLY-ASH; ACTIVATED CARBON; AQUEOUS-SOLUTIONS; WASTE-WATER; METHYL-ORANGE; BISPHENOL-A; TEXTILE DYE; REMOVAL; DEGRADATION; IBUPROFEN	In this work, an artificial neural network (ANN) was developed to model the Pseudo- Second Order (PSO) kinetics of orange peel-paracetamol adsorption process. The orange peel used for the adsorption process was prepared, activated, and characterized using Scanning Electron Microscopy (SEM) and Fourier Transform Infrared Spectroscopy (FTIR) techniques respectively. Batch adsorption experiment was carried out to obtain the concentrations of paracetamol (PCM) adsorbed on orange peel activated carbon (OPAC) a different operating conditions which include contact time (0-330 minutes), initial PCM concentration (10-50 mg/L), and temperature (30-50 degrees C). Then, the experimental data was used to compute PSO kinetics of the orange peel - paracetamol adsorption process. To predict the PSO kinetics, different ANN structures were investigated. The optimal ANN structure which uses 18 hidden neurons, hyperbolic tangent sigmoid transfer function (tansig) at the input layer, linear transfer function (purelin) at the output layer, and Levenberg Marquardt as its backpropagation algorithm demonstrated the optimal prediction ability. Specifically, the optimal ANN model gave Mean Average Error (MAE), Mean Square Error (MSE), Root Mean Square Error (RMSE), and R-2 values of 0.0515, 0.0064, 0.0798 and 1.0000 respectively when compared with the experimental data. The results obtained showed that ANN can be used to effectively model PSO kinetics of orange peel-paracetamol adsorption process.																	0169-7439	1873-3239				AUG 15	2020	203								104053	10.1016/j.chemolab.2020.104053													
J								Prediction of human phosphorylated proteins by extracting multi-perspective discriminative features from the evolutionary profile and physicochemical properties through LFDA	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Post-translational modifications; Phosphorylated proteins; Evolutionary information; Physiochemical properties; Support vector machine; Local fisher discrimination analysis	POSTTRANSLATIONAL MODIFICATION SITES; SUPPORT VECTOR MACHINES; AMINO-ACID-COMPOSITION; COVARIANCE TRANSFORMATION; MEMBRANE-PROTEINS; AUTO COVARIANCE; WEB SERVER; IDENTIFICATION; INFORMATION; BINDING	Protein phosphorylation is an emerging post-translational modification, which critically involved in the introcellular process of the human body by controlling diverse functions ranging from cell growth to metabolism. The existing experimental methods for identifying phosphorylated proteins are overpriced and resource-intensive; thus, it is necessary to develop a fast and accurate computational method to address the problem. Here we report a novel predictor HPhosPPred, a phosphorylated protein prediction method that is incorporating highly discriminative evolutionary and physicochemical information conserved in protein primary motifs, namely pseudo-position specific scoring matrix, the auto-covariance transformation of the position-specific scoring matrix and normalized moreau-broto auto-correlation. Further, to boost up the generalization capability of HPhosPPred, we used local fisher discriminant analysis as a dominant feature selection strategy for eliminating redundant and noise patterns from the extracted features. Finally, the optimized features feed to support vector machine with radial basis function kernel to predict phosphorylated proteins. As evident from the results, the proposed method achieved promising performance with an accuracy of 80.68%, sensitivity of 84.63%, specificity of 73.67%, and Matthew's correlation coefficient of 0.581 using rigorous leave-one-out-cross-validation test and 10-fold cross-validation test. The empirical outcomes demonstrate that the developed model outperformed the existing stateof-the-art methods. Furthermore, our analysis reveals that the proposed tool can help detect unseen phosphorylated proteins in particular and proteomics research in general. The source code and dataset are publicly available at https://github.com/saeed344/HPhosPPred.																	0169-7439	1873-3239				AUG 15	2020	203								104066	10.1016/j.chemolab.2020.104066													
J								Online tuning of predictor weights for relevant data selection in just-in-time-learning	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Adaptive learning; Concept drift; CUSUM; EWMA; Optimization; SPC	SOFT-SENSOR DEVELOPMENT; QUALITY PREDICTION; VARIABLE SELECTION; MOVING WINDOW; REGRESSION; PLS; CALIBRATION; MODEL	Just-in-Time-Learning (JITL) is one of the most frequently used adaptive methods in data-based soft sensor design for chemical processes. While JITL is an effective method to combat concept drifts, samples selected via similarity metric in a Euclidean space consisting of a large number of equally-weighted predictors may diminish the performance of the JITL, due to curse of dimensionality and the varying degrees of nonlinearity between the predictor and response variables. Algorithms involving offline tuning of predictor weights were developed to tackle this issue, but changes in process conditions may depreciate a currently used set of weights for measuring similarity. In the current study, an adaptive method is developed for adjusting predictor weights in the Euclidean space used in measuring similarity between samples, named JITL via Online Weighted Euclidean Distance (JITL-OWED). JITL-OWED mainly consists of four steps: i) Relevant data is selected from an online weighted Euclidean space, and multiple models using different weights in their similarity measures are simultaneously constructed. ii) Control charts are used to detect changes in the prediction accuracy of the multiple models constructed online, hence triggering new subset searches. iii) A small number of search steps is used in subset selection, akin to early stopping in gradient search, with the "modest" aim of moving towards a local minimum for feasible online implementation. Exponentially Weighted Moving Average (EWMA) filtering is employed to stabilize the results from feature searches. iv) Final predictions are obtained via stacking multiple models. Employing JITL-OWED on one synthetic and four publicly available real datasets shows that online predictor weighting may indeed improve the prediction accuracy of the traditional JITL up to 80%, and JITL-OWED is both easy to implement and tune.																	0169-7439	1873-3239				AUG 15	2020	203								104043	10.1016/j.chemolab.2020.104043													
J								A chemometric approach to the evaluation of the ageing ability of red wines	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Ageing ability; Ageing potential; Phenolic compounds; Tannins; Anthocyanins; Chemometrics	PHENOLIC-COMPOUNDS; METHYL CELLULOSE; OXYGEN EXPOSURE; COLOR; GRAPE; COPIGMENTATION; ANTHOCYANINS; ASTRINGENCY; TANNINS; MULTIBLOCK	One of the most important quality attributes of red wines is its ability to withstand and ageing process. The multidimensionality of the ageing ability concept, which includes a combination of colour, taste, mouthfeel, and aroma was reported in previous studies. Phenolic compounds are largely or partially involved in most of these wine attributes and have been proposed as potential candidates to evaluate the ageing ability of red wines. The phenolic and colour properties of a large number of wines were measured during a barrel and bottle ageing process of 24 months. To our understanding, a wine that needs the longest time to reach optimal phenolic quality is considered a wine with higher ability to age (concentration assumption). Moreover, a wine that is able to maintain its optimal phenolic quality for longer will also be a wine with high ageing ability (stability assumption). Based on the formulated assumptions a scoring system was used to identify those wines with theoretically high ability to age. As expected, these wines contained initial high levels of tannins, total phenols and anthocyanins, including high polymeric pigment presence and enhanced colour properties. Interestingly, high ageing ability wines showed a smaller change in the colour properties over time which might be indicating slower pigment formation rates. In addition, a chemometric attempt was undertaken to explore an ageing index based on initial phenolic content. The evolution of the index over time using a multi-block approach showed stability for the index values, in line with the short term stability of tannin and total phenol levels. Finally, promising results were obtained as two thirds of the wines were correctly classified when a validation task between the ageing index and the score values was attempted.																	0169-7439	1873-3239				AUG 15	2020	203								104067	10.1016/j.chemolab.2020.104067													
J								Clustering algorithm for mixed datasets using density peaks and Self-Organizing Generative Adversarial Networks	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Density peaks clustering; Mixed data; Generative adversarial networks; Adaptive cost function		This paper presents a new Density-Peaks and Self-Organizing Generative Adversarial Networks (DP-SO-GAN) for clustering mixed datasets. Many clustering methods depend on the assumption that datasets contain either categorical or numerical attributes. Nevertheless, in real-time, most of the applications include mixed categorical and numerical attributes. In medicine, the clustering of cardiovascular disease is an essential task. The clustering of such data attributes is a vital and challenging issue. First, we transform mixed data attributes such as categorical attributes using a one-hot encoding technique and numerical attributes using normalization techniques. The converted characteristics are input to a Self-Organizing Generative Adversarial Networks (SO-GAN) to learn the feature map. Second, we train two kernel networks, such as the generator and discriminator, and each one holds a trivial amount of convolution kernels. Last, we propose an enhanced density peaks clustering algorithm and computing similarity measure between the data objects in the feature representation. The clustering accuracy for the cardiovascular disease dataset results in 88.32% with a standard deviation of 0.1 and is relatively higher than that of other existing algorithms. The training time for hand-written digits datasets over 300 epochs is 3148.26 s. Experiment results obtained on a set of five datasets demonstrate the merits of the proposed method, especially in terms of the stability and efficiency of network training. The computational complexity of the proposed method in terms of floating-point operations is reduced by around 18% as compared with the classical generative adversarial networks.																	0169-7439	1873-3239				AUG 15	2020	203								104070	10.1016/j.chemolab.2020.104070													
J								Cross-product penalized component analysis (X-CAN)	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Sparsity; Principal component analysis; Data interpretation; Sparse principal component analysis; Group-wise principal component analysis	PRINCIPAL COMPONENTS; ALGORITHMS; SELECTION; MODELS; PCA	Matrix factorization methods are extensively employed to understand complex data. In this paper, we introduce the cross-product penalized component analysis (X-CAN), a matrix factorization based on the optimization of a loss function that allows a trade-off between variance maximization and structural preservation, with a focus on highlighting differences between groups of observations and/or variables. The approach is based on previous developments, notably (i) the Sparse Principal Component Analysis (SPCA) framework based on the LASSO, (ii) extensions of SPCA to constrain both modes of the factorization, like co-clustering or the Penalized Matrix Decomposition (PMD), and (iii) the Group-wise Principal Component Analysis (GPCA) method. The result is a flexible modeling approach that can be used for data exploration in a large variety of problems. We demonstrate its use with applications from different disciplines.																	0169-7439	1873-3239				AUG 15	2020	203								104038	10.1016/j.chemolab.2020.104038													
J								Key performance index estimation based on ensemble locally weighted partial least squares and its application on industrial nonlinear processes	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Locally weighted partial least squares; Soft sensors; Ensemble learning; Quality prediction; Catalytic reforming process	JUST-IN-TIME; ADAPTIVE SOFT-SENSOR; CLASSIFICATION; MACHINE; DESIGN; MODEL; PLS	Recent decades have witnessed a trend that soft sensing, instead of hard sensing, has been extensively applied to estimate the key performance indices under the circumstances that practical measurements are hardly to be achieved at a reasonable cost. However, due to the existence of nonlinearities and time-varying characteristics in the practical industrial processes, the conventional soft sensor models probably suffer from severe performance degradations when the original designed models are mismatched. Although many novel methodologies have been employed to alleviate this problem, each of them merely focuses on certain aspect of model features, a comprehensive framework combining these features is needed. Therefore, this study proposes an online predictive methodology based on an integration of ensemble learning based on a novel adaptive locally weighted partial least squares. Specifically, sub-models established on the respective dataset are generated by moving window model, time difference model and just-in-time learning model for the sake of different properties in processes. The effectiveness of the proposed model is validated on the practical nonlinear processes represented by a benchmark simulation model No.1 (BSM1), in wastewater treatment plants (WWTP), and a real industrial catalytic reforming process.																	0169-7439	1873-3239				AUG 15	2020	203								104031	10.1016/j.chemolab.2020.104031													
J								Kurtosis-based projection pursuit analysis to extract information from sensory attributes of cachaca	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Sensory analysis; Expert tasters; Cachaca; Projection pursuit; Variable selection; Chemometrics	PREDICTION; QUALITY	Kurtosis-based projection pursuit (kPPA) coupled with a variable selection was employed to evaluate twenty cachaca samples concerning the sensory attributes. Cachacas aged in wood barrels from Minas Gerais state, and artisanal flavored cachacas from Morretes, a historical city in the Parana state were evaluated. The selection of sensory attributes by simple variable selection improved the pattern recognition by searching for combinations of the sensory attributes that have a minimum kurtosis value when kPPA is applied. The variable selection highlights the sensory attributes related to the taste (bitter, sweet, and astringency), and related to the smell (caramel, alcoholic, and wood). This approach was able to show that flavoring can help with cachaca costs reduction related to the wood barrels and produce some sensory attributes with notes similar to those aging in wood barrels. A kurtosis-based projection pursuit is a powerful tool for extract information from the sensory analysis.																	0169-7439	1873-3239				AUG 15	2020	203								104075	10.1016/j.chemolab.2020.104075													
J								Inverse sum indeg status index of graphs and its applications to octane isomers and benzenoid hydrocarbons	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Distance; Status of a vertex; Status connectivity index; Eccentric connectivity index; Molecular graph; Inverse sum indeg index	1ST	The status or transmission of a vertex u in a graph G is defined by sigma(u) = Sigma(v is an element of V)d(u, v), where V is the set of vertices of the graph G, and d(u, v) stands for the distance between u and v in G. In this paper we have defined inverse sum indeg status (ISIS) index of a graph G as ISIS(G) = Sigma(uv is an element of E) sigma(u)sigma(v)/sigma(u)+sigma(v), where E is the set of edges of G. The ISIS index is investigated as a predictor of various physicochemical properties of octane isomers and boiling points of benzenoid hydrocarbons. We further compute this novel index of some specific graphs and also obtain some bounds of the proposed index.																	0169-7439	1873-3239				AUG 15	2020	203								104059	10.1016/j.chemolab.2020.104059													
J								Fast reconstruction of Raman spectra based on global weighted linear regression	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Raman spectra; Multi-channel measurements; Global assignment weighted; Linear regression function; Polynomial regression	SPECTROSCOPY	Raman spectroscopy has shown great potential in biomedical applications. However, slow data acquisition of Raman spectra has seriously hindered the expansion of its application. In this paper, we have completed the reconstruction of Raman spectra with mull-channel measurements based global weighted linear regression. This algorithm establishes a linear regression function by optimizing the training samples and making a global assignment weighted to the optimizing samples. Simultaneously, the normalization and polynomial regression are introduced in order to improve the accuracy of reconstructed spectra. It has evaluated the Raman spectra of several materials. According to the root mean square error, the fitness between reconstructed and original spectra is excellent. This algorithm can be used in quickly testing for potential sample component in a substance, where the sample component to be tested is known and provides a theoretical support for the application of Raman imaging technology in fast dynamic systems.																	0169-7439	1873-3239				AUG 15	2020	203								104073	10.1016/j.chemolab.2020.104073													
J								How cliff shaped my philosophy on faculty development and mentorship: A remembrance	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS																													0169-7439	1873-3239				AUG 15	2020	203								104094	10.1016/j.chemolab.2020.104094													
J								Hard modeling of titration curve for absolute analysis	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Absolute analysis; Hard modeling; Titration curve	TRILINEAR DECOMPOSITION ALGORITHM; ACID-BASE; RESOLUTION; CALIBRATION; 2ND-ORDER	We provide a solution for absolute analysis that has been an unrealized goal for decades. We performed hard modeling of a titration curve to generate a calculation formula to directly estimate the stoichiometric point directly. We show that the method error is negligible when the formation constant of a titration reaction is moderately large. Two examples are given to show how to apply the developed theory in practical quantitative analysis.																	0169-7439	1873-3239				AUG 15	2020	203								104069	10.1016/j.chemolab.2020.104069													
J								ML-RBF: Predict protein subcellular locations in a multi-label system using evolutionary features	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Multi-label classification; PSSM; ML-kNN; Rank-SVM; ML-RBF	AMINO-ACID-COMPOSITION; INTELLIGENT COMPUTATIONAL MODEL; SEQUENCE-DERIVED FEATURES; LOW-SIMILARITY SEQUENCES; COUPLED RECEPTOR CLASSES; OUTER-MEMBRANE PROTEINS; S-NITROSYLATION SITES; DNA-BINDING PROTEINS; GENERAL-FORM; ENSEMBLE CLASSIFIER	Machine learning assisted sub-cellular protein localization has been an emerging research area from the last two decades since it provides fast, low cost and more precise tagging of different sub-cellular proteins. However, most of the research is focused on single-plex proteins which always resides on a particular location in all the cells at all times. Later studies reveal that many proteins can reside at more than one location and are therefore called multiplex. These proteins were previously noted mistakenly as moved away from their location due to some cellular malfunction caused by some disease. The dynamic behavior of these proteins depicts their importance in cellular functionality and thus they are more worthy of being studied by the researchers. The current study proposes a novel model for the classification of mull-label proteins using evolutionary feature extraction via Position Specific Scoring Matrix. Two benchmark mull-label datasets (of bacteria and viruses) are employed to draw realistic comparisons. The study utilizes three state-of-the-art classifiers to draw a working comparison and the results are discussed rigorously utilizing various statistical performance metrics specifically proposed for mull-label classification. The proposed model yielded 93% and 94% average precisions for the two datasets respectively. It demonstrates its reliability and capability for utilization in further similar studies.																	0169-7439	1873-3239				AUG 15	2020	203								104055	10.1016/j.chemolab.2020.104055													
J								Ensemble just-in-time model based on Gaussian process dynamical models for nonlinear and dynamic processes	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Process control; Soft sensor; Just-in-time; Gaussian process dynamical models; Ensemble learning	SOFT SENSOR DEVELOPMENT; PARTIAL LEAST-SQUARES; REGRESSION; FRAMEWORK; MACHINE	Process data have a number of characteristics, such as noise, nonlinearity, process dynamics, and autocorrelation. Ideally, adaptive soft sensors would be used to solve model degradation problems resulting from changes in process characteristics. However, it is necessary to optimize the hyperparameters for each model and, depending on the state of the process, the optimal hyperparameters will change. In this study, we focus on a Gaussian process dynamical model (GPDM), a dimension-reduction method that considers all of the data characteristics. We combine a just-in-time (JIT) model and ensemble learning, and then predict y-values with multiple JIT models that have different sets of hyperparameters. Each JIT model is constructed using latent variables obtained by the GPDM. The weights of the JIT models are determined based on Bayes' theorem in consideration of their predictive ability. Analysis of two industrial datasets confirms that the proposed model is more accurate than existing approaches.																	0169-7439	1873-3239				AUG 15	2020	203								104061	10.1016/j.chemolab.2020.104061													
J								Parametric optimization and MCR-ALS kinetic modeling of electro oxidation process for the treatment of textile wastewater	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										MCR-ALS; RSM; Electro oxidation; Kinetic-modeling; Spectrophotometric data; Kinetic parameters	MULTIVARIATE CURVE RESOLUTION; ELECTROCHEMICAL OXIDATION; RANK AUGMENTATION; ORGANIC-COMPOUNDS; DEGRADATION; EFFLUENTS; REMOVAL; REACTOR	Kinetic modeling using multivariate curve resolution-alternating least squares (MCR-ALS) was performed successfully for the electro oxidation (EO) treatment of textile wastewater. The present study represents the monitoring of the % degradation and kinetics of the EO reaction during the treatment of textile effluent. Optimum condition was found to bet = 130 min, i = 1.41 A, pH = 5.41 and Retention time = 143 min. At this optimum condition, the degradation suggested by response surface methodology (RSM) under central composite design (CCD) was 86%. A good correlation was observed between the predicted and experimental % degradation at the optimum value of process parameters. Spectrophotometric data of EO process during the treatment of textile effluent was analyzed during MCR-ALS. ALS optimization was performed by applying a series of constraints. The initial and final concentration of the pollutants along with the kinetic parameters were successfully resolved. The performance of kinetic modeling on predicting EO treatment was evaluated by a lack of fit in % experimental (1.215) and explained variance (99.98%).																	0169-7439	1873-3239				AUG 15	2020	203								104027	10.1016/j.chemolab.2020.104027													
J								Prediction of piRNAs and their function based on discriminative intelligent model using hybrid features into Chou's PseKNC	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Piwi-interacting RNAs; Neural network application; Cancer discovery; PseKNC; PCA	AMINO-ACID-COMPOSITION; SMALL RNAS; PHYSICOCHEMICAL PROPERTIES; ENSEMBLE CLASSIFIER; PSEUDO COMPONENTS; WEB SERVER; PROTEIN; DNA; PSEAAC; PIWI	Piwi interacting RNA (piRNA) is a recognized group of small non-coding RNA molecules. The piRNA molecules are associated with multiple tumors type diagnosis and drug development. It is also linked to regulate gene expression, suppressing transposon and maintains genome integrity. Due to a vital role of piRNAs in biology, the identification of piRNAs and their function has become an important area of research in computational biology. This paper proposes a robust two-layer predictor called "piRNA (2L)-PseKNC" to improve prediction of piRNAs and their function. The proposed predictor employing hybrid pseudo-K-tuple nucleotide composition (PseKNC) for sequence formulation, unsupervised principal component analysis (PCA) algorithm for discriminant feature selection and deep neural network (DNN) as a classifier. The proposed predictor was designed based on two layers approach. The first layer predicts either the encoded sequence belongs to piRNA or non-piRNA sequence and the second layer predict the selected piRNA sequence is functional piRNA or non-functional piRNA sequence. The overall accuracies of the proposed model using 5-fold cross validation test were 94.73% at first layer and 85.21% at second layer, surpassing the existing predictors with accuracies improvement 7.59% and 2.81% at the first layer and at the second layer respectively.																	0169-7439	1873-3239				AUG 15	2020	203								104056	10.1016/j.chemolab.2020.104056													
J								Soft classification scheme with pre-cluster-based regression for identification of same-base alloys using laser-induced breakdown spectroscopy	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Laser-induced breakdown spectroscopy (LIBS); Pre-cluster-based regression (PCBR); Multivariate analysis; Same-base alloy classification; Soft information	QUANTITATIVE-ANALYSIS; ELEMENTAL ANALYSIS; SELF-ABSORPTION; LIBS; NORMALIZATION; SURFACE; SAMPLES; MARS	In this study, a novel soft classification scheme is proposed for metal scrap identification with laser-induced breakdown spectroscopy (LIBS) measurements. LIBS provides unique spectra for different metals that can be utilized for classifying metal scraps in real time. Despite its potential, LIBS-based metal classification is not yet fully implemented in practice due to the large shot-to-shot variation and non-linear relationships between spectral intensities and elemental concentrations. Particularly for recycling metal alloys of the same base, learning all candidate types is infeasible, and conventional classification approaches exhibit limited performance in classifying unknown samples of untrained types due to the variability and non-linearity in LIBS measurements. To overcome the limitations of LIBS-based metal scrap classification, the proposed scheme employs pre-cluster-based regression (PCBR) analysis. PCBR takes advantage of the joint relationships between the elemental concentration variations of the pre-clusters (p-clusters), which are pre-determined by prior information of the probability distributions. The variance of the regression of individual p-clusters can be significantly reduced compared to global regression by jointly taking into account common relationships between the elemental concentrations within a particular p-cluster. By combining the layered regression results with their estimated statistics, soft multi-label classification and extraction of the likelihood values of trained classes is possible even for samples of untrained types. For performance evaluation, a list of reference alloys from the National Institute of Standard and Technology (NIST) and Brammer databases was divided into finite sets of p-clusters based on the relationships of elemental concentrations, in particular, four p-clusters for Cu-based alloy tests with Cu and Zn concentrations. Then, PCBR were trained with LIBS-captured spectra of 35 certified reference materials for all and four individual p-clusters. The partial least squares (PLS) and random forest (RF) regression methods were employed, and the root mean square error (RMSE) of the estimation and soft classification measures was investigated. The evaluation results of same-base alloy regression revealed that the proposed PCBR reduced the RMSE of the major element concentration estimation compared to conventional regression schemes. In addition, the accuracy of the soft classification of same-base alloys by PCBR for untrained types was tangibly improved compared to that of prior approaches, such as PLS discriminant analysis and soft independent modeling of class analogy (SIMCA).																	0169-7439	1873-3239				AUG 15	2020	203								104072	10.1016/j.chemolab.2020.104072													
J								Superiority of neuro fuzzy simulation versus common methods for Detection of Abnormal Pressure Zones in a southern Iranian oil field	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										D-exponent; Neuro fuzzy; Rate of penetration; Weight on bit; Rotary speed	PORE PRESSURE; EXPONENT METHOD; PREDICTION; CONTROLLER; VELOCITY; SYSTEM	Due to the importance of formation pore pressure detection, different methods have been established for its estimation. One of these methods is the d-exponent method which is used widely in petroleum industry due to its rapidity and cheapness. In this study we firstly estimate formation pore pressure using the d-exp method by the traditional formula based procedure, then an adaptive network has been applied based on fuzzy interface system (ANFIS) to predict the d-exponent amounts and formation pore pressure in an Iranian oil field. Finally the reliability of resulted network values has been checked by comparing to the traditional methods values. In this network the variables include N (rpm), Average ROP and Bit diameter as the chosen inputs.																	0169-7439	1873-3239				AUG 15	2020	203								104039	10.1016/j.chemolab.2020.104039													
J								Adaptive Bottom-Up Space Exploration in model population analysis: An agile variable selection algorithm for PLS models	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										ABUSE; Binary matrix sampling; Partial least squares regression; Variable selection; VISSA	NEAR-INFRARED SPECTROSCOPY; MULTIVARIATE; ELIMINATION; REGRESSION; ENSEMBLE; SUBSET	All variable selection algorithms for partial least squares (PLS) regression models based on model population analysis, including variable iterative space shrinkage approach (VISSA), take an iterative top-down space shrinkage approach. The time efficiency of this unidirectional VISSA is not promising as it drains much of the valuable time while evaluating sub-models of irrelevant size while shrinking variable space in a step-wise manner. In this work, two variants of Adaptive Bottom-Up Space Exploration (ABUSE) approach have been proposed. Both variants of ABUSE, based on VISSA framework, adopts a low weight (e.g. 0.005) of variable selection to formulate short length sub-model populations. When average fitness of sub-model population stops improving in reweighted sampling, the weight of variables appearing in "best-fit" sub-model is increased to 0.5, while maintaining others (but not expunged like VISSA) at the same low selection frequency for next round of iteration. The first algorithmic variant enforces the weight vector manipulation once in a binary matrix sampling, while the other enforces this weight manipulation in every cycle of weighted binary matrix sampling. The proposed methods offered better fitness, outcome stability and algorithmic efficiency particularly for large benchmark NIR data sets. Choice of variable weight is critical as a higher weight, though still better than VISSA, the algorithm tends to project more variables with a deterioration of model fitness.																	0169-7439	1873-3239				AUG 15	2020	203								104057	10.1016/j.chemolab.2020.104057													
J								A "big-data" algorithm for KNN-PLS	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										KNN-PLSDA; PLSDA; parSketch; Big-data; Local-PLS	PARTIAL LEAST-SQUARES; NEAR-INFRARED SPECTROSCOPY; LOCALLY WEIGHTED REGRESSION; QUANTITATIVE-ANALYSIS; CALIBRATION; PREDICTION	A well known issue regarding PLS lies in the difficulty to apprehend nonlinearities. As a solution, an extension of the method, "KNN-PLS", was developed. However, this solution is based on a neighbourhood selection algorithm whose execution time is highly dependent on the size of the database, leading to prohibitive response times. This article proposes, as an alternative, a new method designed to process large data volumes: "parSketch-PLS". This method combines a "big-data domain" neighbour selection method, called "parSketch", and the PLS method. Essentially, this paper presents a feasibility study, regarding the adaptation of big-data principles for spectral datasets, in non-linear contexts. The parSketch method has not been studied in the context of chemometrics and considering the specific properties of spectral data. This method is based on the approximation of sample neighbourhoods, based on spectral distances. It is then necessary to investigate the relevance of these neighbourhoods for PLS models and predictions. This article compares PLS and KNN-PLS methods with the parSketch-PLS method. In this context, PLS allows to process large volumes of data quickly but performs poorly in prediction while the KNN-PLS method returns accurate predictions, yet with much higher computational time. This paper shows that the proposed pairing offers a good operational trade-off between prediction performances and computational cost. In addition a comprehensive study of the input parameters of parSketch-PLS is conducted. The objective is to understand the influence of these parameters on the prediction performances. This article proposes a framework to interpret the neighbourhoods returned by comparing their relative sizes with the evolution of performances and the input parameters of parSketch.																	0169-7439	1873-3239				AUG 15	2020	203								104076	10.1016/j.chemolab.2020.104076													
J								hubViz: A novel tool for hub-centric visualization	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Bayesian modeling; Visualization; Network modeling	DEPENDENT DIABETES-MELLITUS; DISEASE RISK-FACTORS; MODEL; DIAGNOSIS; INFERENCE	Visualization algorithms have been widely used for intuitive interrogation of genomic data and popularly used tools include MDS, t-SNE, and UMAP. However, these algorithms are not tuned for the visualization of binary data and none of them consider the hubness of observations for the visualization. In order to address these limitations, here we propose hubViz, a novel tool for hub-centric visualization of binary data. We evaluated the performance of hubViz with its application to the gene expression data measured in multiple brain regions of rats exposed to cocaine, the single-cell RNA-seq data of peripheral blood mononuclear cells treated with interferon beta, and the literature mining data to investigate relationships among diseases. We further evaluated the performance of hubViz using simulation studies. We showed that hubViz provides effective visual inspection by locating the hub in the center and the contrasting elements in the opposite sides around the center. We believe that hubViz and its software can be powerful tools that can improve visualizations of various genomic data. The hubViz is implemented as an R package hubviz, which is publicly available at https://dongjunchung.github.io/hubviz/.																	0169-7439	1873-3239				AUG 15	2020	203								104071	10.1016/j.chemolab.2020.104071													
J								Residual spaces in latent variables model inversion and their impact in the design space for given quality characteristics	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Partial least squares; Process analytical technology; Quality by design; Linear application; Null space; Model inversion	ORTHOGONAL SIGNAL CORRECTION; PARTIAL LEAST-SQUARES; PREPROCESSING METHODS; REGRESSION; FRAMEWORK	The paper contains a discussion about the null spaces associated to linear prediction models for the particular case of Partial Least Squares regression models. The discussion separately considers the two existing null spaces: the one in the input space related to the projection onto the latent space and the null space, coming from the projection space, corresponding to the mapping of the scores onto the predicted responses. The paper also explores the impact of such null spaces in the definition of the design space around some feasible solutions obtained by inverting the prediction model, via several cases with simulated and real data from the literature. The case-studies serve to illustrate the discussion and the need of considering points in the two null spaces, rather than just take into account the null space within the latent space. They also serve to show how to address the use of the resulting vectors in the design space to maintain the desired quality by modifying the tunable (maneuverable) process variables to compensate for variations due to some other feature variables not so easy to control.																	0169-7439	1873-3239				AUG 15	2020	203								104040	10.1016/j.chemolab.2020.104040													
J								Simple chemometrics estimation of absorptivity and concentration of silver nanoparticles	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Silver nanoparticles; Absorptivity; Concentration; Chemometrics; Kinetic	CARBON DOTS; GOLD NANOPARTICLES; KINETICS; GROWTH; SIZES; PROBE; ASSAY; IONS; HG2+	A new method was introduced for determination of absorptivity and subsequent determination of concentration and size of silver nanoparticles (AgNPs). Without using any complicated instrument, based on a kinetic reaction of AgNPs, a ratio curve is extracted and chemometrically fitted to obtain the absorptivity of the AgNPs. Afterwards, it can be possible to estimate size of the AgNPs based on the reported relations. The method can be simply used for any NPs provided that it can absorb in UV-Vis region of the electromagnetic spectrum and after employing in a kinetic reaction. The agreement between the calculated diameter of the AgNPs based on the absorptivity derived from the method and the transmission electron microscopy (TEM) imaging were excellent. It can be claimed that this is the most direct method to determine NPs absorptivity and concentration.																	0169-7439	1873-3239				AUG 15	2020	203								103983	10.1016/j.chemolab.2020.103983													
J								Estimating variances and kinetic parameters from spectra across multiple datasets using KIPET	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Kinetic parameter estimation; Differential algebraic equations; Spectroscopic data; Pharmaceutical processes; Chemical processes; Chemometrics; Multiset data; Variance estimation	CURVE RESOLUTION; RATE CONSTANTS; ALGORITHM; PARAFAC2	Multivariate spectroscopic data is increasingly abundant in the chemical and pharmaceutical industries. However, it is often challenging to estimate reaction kinetics directly from it. Recent advances in obtaining kinetic parameter estimates from spectroscopic data based on large-scale nonlinear programming (NLP), maximum likelihood principles, and discretization on finite elements lead to increased speed and efficiency (Chen a al., 2016). These new techniques have great potential for widespread use in parameter estimation. However they are currently limited due to their applicability to relatively small problem sizes. In this work, we extend the opensource package for estimating reaction kinetics directly from spectra or concentration data, KIPET, for use with multiple experimental datasets, or multisets (Schenk a al., 2020). Through a detailed initialization scheme and by taking advantage of large-scale nonlinear programming techniques and problem structure, we are able to solve large problems obtained from multiple experiments, simultaneously. The enhanced KIPET package can solve problems wherein multiple experiments contain different reactants and kinetic models, different dataset sizes with shared or unshared individual species' spectra, and can obtain confidence intervals quickly based on the NLP sensitivities. In addition, we propose a new variance estimation technique based on maximum likelihood derivations for unknown covariances from two sample populations. This new variance estimation technique is compared to the previously proposed iterative-heuristics-based algorithm of Chen a al. (2016) for distinguishing between variances of the noise in model variables and in the spectral measurements. We demonstrate the new techniques on a variety of example problems, with sample code, to show the utility of the approach and its ease of use. We also include the curve-fitting problem to cases where we have concentration data given directly, and are required to estimate kinetic parameters across multiple experimental datasets.																	0169-7439	1873-3239				AUG 15	2020	203								104012	10.1016/j.chemolab.2020.104012													
J								Rayleigh scattering correction for fluorescence spectroscopy analysis	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Fluorescence spectroscopy; Rayleigh scattering; Missing data recovery (MDR); Principal component analysis (PCA); Parallel factor analysis (PARAFAC)	PARAFAC; DECOMPOSITION; CALIBRATION; ALGORITHM	Rayleigh scattering signals, not conforming bilinear and trilinear structure of spectral excitation-emission matrices (EEMs), significantly increases the difficulty of spectral resolution. To eliminate or reduce the interference of Rayleigh scattering, we propose missing data recovery (MDR) coupled with principal component analysis (PCA) or parallel factor analysis (PARAFAC) as a novel strategy for Rayleigh scattering correction and corresponding EEM decomposition. MDR treats the scattering data as missing by weighting them as zeros to remove Rayleigh scattering signals thoroughly. Then, sample signals are dramatically recovery in the scattering missing region during rapid iterative process of PCA or PARAFAC to repair bilinearity and trilinearity of EEMs. For significant Rayleigh scattering leading to severe signal loss, profile constraint on both of excitation and emission spectra following fluorescence spectral laws is further proposed for MDR-PCA and MDR-PARAFAC. It is so as to avoid mathematical reasonable but chemical meaningless solutions. The results reveal MDR-PCA and MDR-PARAFAC enable robust Rayleigh scattering correction and EEM decomposition both for simulated and practical data sets. Without the need of any specific priori knowledge and pretreatment such as wavelength selection, it therefore suggests great potential of the proposed method to be a generalized strategy for robust Rayleigh scattering correction and spectral resolution of EEMs.																	0169-7439	1873-3239				AUG 15	2020	203								104028	10.1016/j.chemolab.2020.104028													
J								New duality based generalized rank annihilation algorithm for determining analyte concentration with realistically estimated error level for practical data sets	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Rank annihilation factor analysis (RAFA); Generalized rank annihilation method (GRAM); Second order advantage; Self-modeling curve resolution (SMCR); Duality principle; Estimation with error interval	STANDARD ERRORS; RESOLUTION; EIGENVALUES; UNIQUENESS; REDUCTION; EQUALITY	A new duality based algorithm is developed to achieve the "second-order advantage" and determination of the standard component in the presence of uncalibrated components. For this purpose, the duality principle, as a powerful mathematical approach, is used to define the interference(s)' subspace. Afterward, the concentration ratio, lambda, i.e., the amount of the standard component (analyte) in the test sample to its amount in the standard has been calculated systematically. The provided estimations of lambda yield reliable results for the standard component with adequate data-driven error intervals. Additionally, the geometrical visualizations beside algebraic formulas are used to show the quantitative analysis of this study. By illustrating the procedure in graphical plots, the details of the proposed method can be explained understandably. This contribution can also be considered as another attempt for the rehabilitation of the generalized rank annihilation concept.																	0169-7439	1873-3239				AUG 15	2020	203								104058	10.1016/j.chemolab.2020.104058													
J								An automated Residual Exemplar Local Binary Pattern and iterative ReliefF based COVID-19 detection method using chest X-ray image	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Residual Exemplar LBP; Covid-19; Iterative ReliefF; Classification; Machine learning	DECISION TREE CLASSIFIER	Coronavirus is normally transmitted from animal to person, but nowadays it is transmitted from person to person by changing its form. Covid-19 appeared as a very dangerous virus and unfortunately caused a worldwide pandemic disease. Radiology doctors use X-ray or CT images for the diagnosis of Covid-19. It has become crucial to help diagnose such images using image processing methods. Therefore, a novel intelligent computer vision method to automatically detect the Covid-19 virus was proposed. The proposed automatic Covid-19 detection method consists of preprocessing, feature extraction, and feature selection stages. Image resizing and grayscale conversion are used in the preprocessing phase. The proposed feature generation method is called Residual Exemplar Local Binary Pattern (ResExLBP). In the feature selection phase, a novel iterative ReliefF (IRF) based feature selection is used. Decision tree (DT), linear discriminant (LD), support vector machine (SVM), k nearest neighborhood (kNN), and subspace discriminant (SD) methods are chosen as classifiers in the classification phase. Leave one out cross-validation (LOOCV), 10-fold cross-validation, and holdout validation are used for training and testing. In this work, SVM classifier achieved 100.0% classification accuracy by using 10-fold cross-validation. This result clearly has shown that the perfect classification rate by using X-ray image for Covid-19 detection. The proposed ResExLBP and IRF based method is also cognitive, lightweight, and highly accurate.																	0169-7439	1873-3239				AUG 15	2020	203								104054	10.1016/j.chemolab.2020.104054													
J								Deep Ranking Analysis by Power Eigenvectors (DRAPE): A polypharmacology case study	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Multi-criteria decision making; MCDM; Power-weakness ratio; PWR; DRAPE; Polypharmacology	MULTICRITERIA DECISION-MAKING; ORDER	Multi-criteria decision making processes comprehend several ranking methods able to handle multiple and often conflicting criteria and sorting objects according to a definition of the optimality direction for each criterion. In the field of polypharmacology it can be useful for virtual screening or prioritization purposes to rank molecules distinguishing the most multi-target ones, i.e. molecules able to interact with different targets, from the selective or inactive ones. In this work, the Deep Ranking Analysis by Power Eigenvectors approach is applied to a small set of molecules characterized by their half maximal binding concentrations for seven different nuclear receptors. The existence of a correspondence between the DRAPE ranking and a manually grouping of molecules based on their multi-target behaviour was verified. Moreover, a comparison between DRAPE rankings and those obtained from five traditional methods was carried out in order to highlight the main similarities and the differences of the approaches.																	0169-7439	1873-3239				AUG 15	2020	203								104001	10.1016/j.chemolab.2020.104001													
J								A novel bound constrained optimization method for three-way chemical data analysis with application to fluorescence excitation-emission matrix spectroscopy	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Multi-way data analysis; Bound constrained optimization; Fluorescence spectroscopy; Data processing	TRILINEAR DECOMPOSITION ALGORITHM; DISSOLVED ORGANIC-MATTER; PARALLEL FACTOR-ANALYSIS; CALIBRATION; PERFORMANCE; MODEL	There is a great deal of interests in multivariate approaches which may offer a necessary, and often sufficient, data analysis technique in many fields such as analytical chemistry, biology and environmental chemistry. However, few of these multivariate approaches have paid attention to the nonnegativity constraint in the decomposition process. In this paper, a novel bound constrained optimization method was proposed for three-way chemical data analysis. In this method, nonnegative matrix factorization was introduced to replace the traditional trilinear decomposition to provide the constraints on the nonnegative boundary on excitation-emission matrix spectra data. And the least-squares problem was transformed into a bound constrained optimization problem which can be solved by projected gradient methods. The alternating least squares were applied during each optimization iteration to obtain the individual components. Analysis of simulated three-way arrays indicated that the proposed method has a better performance than parallel factor analysis and alternating trilinear decomposition methods in nonnegativity. Experiments of real excitation-emission matrix spectra data also show that the proposed method is robust with the background interferences in practical applications.																	0169-7439	1873-3239				AUG 15	2020	203								104036	10.1016/j.chemolab.2020.104036													
J								Stacked locality preserving autoencoder for feature extraction and its application for industrial process data modeling	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Deep learning; Output prediction; Soft sensor; Stacked autoencoder (SAE); Stacked locality preserving autoencoder (S-LPAE)	SOFT; REGRESSION; NETWORKS; SENSORS	Deep learning has recently caught much attention in the industrial processes, particularly for soft sensor applications. However, most traditional deep learning networks cannot extract local features for data modeling. To overcome this problem, a novel stacked locality preserving autoencoder (S-LPAE) is proposed in this paper. First, the neighborhood topological structure is built for the historical samples and the weights between the neighbor samples are calculated. Then, locality preserving autoencoder (LPAE) is designed to minimize both the reconstruction error and the additional local preserving constraint of the training dataset, with which the potential features can better preserve the local data structure. After that, multiple LPAE modules are sequentially stacked to construct the S-LPAE network to obtain deep locality-preserving features. Finally, the extracted features are directly used for the output prediction of soft sensor. To validate the performance of the proposed algorithm, it is applied to an industrial hydrocracking process to predict the 90% boiling point of aviation kerosene and the 50% boiling point of diesel.																	0169-7439	1873-3239				AUG 15	2020	203								104086	10.1016/j.chemolab.2020.104086													
J								Peak detection and random forests classification software for gas chromatography/differential mobility spectrometry (GC/DMS) data	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS												Gas Chromatography/Differential Mobility Spectrometry (GC/DMS) is an effective tool to discern volatile chemicals. The process of correlating GC/DMS data outputs to chemical identifies requires time and effort from trained chemists due to lack of commercially available software and the lack of appropriate libraries. This paper describes the coupling of computer vision techniques to develop models for peak detection and can align chemical signatures across datasets. The result is an automatically generated peak table that provides integrated peak areas for the inputted samples. The software was tested against a simulated dataset, whereby the number of detected features highly correlated to the number of actual features (r(2) = 0.95). This software has also been developed to include random forests, a discriminant analysis technique that generates prediction models for application to unknown samples with different chemical signatures. In an example dataset described herein, the model achieves 3% classification error with 12 trees and 0% classification error with 48 trees. The number of trees can be optimized based on the computational resources available. We expect the public release of this software can provide other GC/DMS researchers with a tool for automated featured extraction and discriminant analysis capabilities.																	0169-7439	1873-3239				AUG 15	2020	203								104085	10.1016/j.chemolab.2020.104085													
J								Soft sensor model for dynamic processes based on multichannel convolutional neural network	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Soft sensors; Convolutional neural network (CNN); Multichannel CNN (MCNN); Quality prediction	REGRESSION; FRAMEWORK	Soft sensors have been extensively used to predict the difficult-to-measure key quality variables. The robust soft sensors should be able to sufficiently extract the local dynamic and nonlinear features of process data for accurate prediction. Convolutional neural network (CNN) has shown powerful performance in local feature representation that is suitable for soft sensor modeling. However, the process variables that have a distant topological structure usually cannot be covered within the same convolution kernel when applying CNN to process data, which results in the fact that local correlations of those distant process variables are not captured. Therefore, a new multichannel CNN (MCNN) is proposed for various local dynamic feature representation. As a key step, a multichannel 3-D tensor is augmented for each sample as the input to the MCNN model. For the 3-D tensor, each channel has specific local correlations of certain variables, while the variables have different neighborhood relationships for different channels, which refer the various local correlations of different combination variables. Combining with the time axis of each channel, the various local dynamic correlations of different variable combinations can be learnt using MCNN regardless of their distance. The feasibility and effectiveness of MCNN-based soft sensor are demonstrated on the industrial debutanizer column and hydrocracking process.																	0169-7439	1873-3239				AUG 15	2020	203								104050	10.1016/j.chemolab.2020.104050													
J								Noise reduction in the spectral domain of hyperspectral images using denoising autoencoder methods	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Denoising autoencoder; Noise reduction; Hyperspectral image; Wavelet transform; Spectra simulation; Empirical mode decomposition	WAVELET TRANSFORMS; NEURAL-NETWORKS; QUALITY	Denoising of spectra has been a great challenge in hyperspectral image analysis. Near-infrared hyperspectral images of milk powder, rice flour and soybean flour were acquired and denoising in the spectral domain were studied. Noise free spectra and noises were simulated based on sample pixel-wise spectra. The noisy spectra with signal to noise ratio (SNR) around 45 dB (similar to real pixel-wise spectra) were simulated. The simulated noisy spectra were preprocessed by traditional methods as moving average smoothing (MAS), Savitzky-Golay smoothing (SGS), wavelet transform (WT) and empirical mode decomposition (EMD). The basic denoising autoencoder (DAE-1) and the stacked DAE (DAE-2) were studied for denoising. The noisy spectra with SNR around 35 dB and 55 dB were further simulated to explore the effectiveness of DAE based methods. DAE-1 and DAE-2 performed better than the other methods, with higher SNR, lower mean squared error (MSE) and mean absolute error (MAE). The developed DAE methods were applied to real-world pixel-wise spectra with good performances. The overall results proved the feasibility of using DAE based methods for noise reduction in the spectral domain of hyperspectral images, and the DAE based methods have great potential to be extended to spectral denoising of other vibrational spectroscopy techniques.																	0169-7439	1873-3239				AUG 15	2020	203								104063	10.1016/j.chemolab.2020.104063													
J								PLS-DA - A MATLAB GUI tool for hard and soft approaches to partial least squares discriminant analysis	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Soft and hard partial least squares discriminant analysis (PLS-DA); MATLAB tool		An open source MATLAB tool implementing the standard Partial least squares discriminant analysis (PLS-DA) method as well as its recent developments, known as multiclass PLS-DA in hard and soft versions, is presented. The main functionality of the tool and usage scenarios are described using a real-world example. A brief introduction to the underlying implemented methods and algorithms is provided.																	0169-7439	1873-3239				AUG 15	2020	203								104064	10.1016/j.chemolab.2020.104064													
J								Assessing a swarm-GAP based solution for the task allocation problem in dynamic scenarios	EXPERT SYSTEMS WITH APPLICATIONS										Unmanned aerial vehicles; Task allocation; Swarm intelligence; Replication; Dynamic	DIVISION-OF-LABOR; SYSTEM; ALGORITHMS	Swarm-GAP is a heuristic that combines a swarm intelligence strategy with the generalized assignment problem (GAP) method. This approach is especially appropriate when there are agents engaged in a collaborative task, but in general, heuristics have drawbacks to optimize resource allocation. A previous work proposed the usage of three swarm-GAP variants to solve the task allocation problem among agents representing a group of Unmanned Aerial Vehicles (UAVs) aiming at the optimization of their resources usage applied in the context of static environments. However, there is a lack of empirical assessment of these algorithms in dynamic scenarios, i.e., with some attributes changing along the system execution. Such changes represent important features of real-world application scenarios, such as in military operations in which a number of non-expected events may happen, e.g., loss of members of the UAV-team or onboard sensor failure. Therefore, the contributions of this work are the performance evaluation of the original algorithms in dynamic context, and the extension of these algorithms to properly address more realistic dynamic scenarios. Considering changes in some attributes of the environment, a trade-off in terms of the quality in the mission performance and the overhead in the communication among the UAVs is explored. The empirical assessment of the original algorithms and the proposed extensions were performed by conducting independent replications in a scenario where the number of agents (UAVs) changes at runtime and adaptations occur autonomously to maintain the mission execution. The acquired results provide evidence that the proposed solution is capable of dealing with dynamic scenarios, covering the gap left by other works in the literature, and enriching the realism of applications in autonomous intelligent systems. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				AUG 15	2020	152								113437	10.1016/j.eswa.2020.113437													
J								A new preference disaggregation TOPSIS approach applied to sort corporate bonds based on financial statements and expert's assessment	EXPERT SYSTEMS WITH APPLICATIONS										TOPSIS; Sorting; Credit risk expert; Corporate bonds; Preference disaggregation; MCDM	INTEGRATED FRAMEWORK; DECISION-SUPPORT; CREDIT RISK; MULTICRITERIA APPROACH; UTILITY-FUNCTIONS; RANK REVERSAL; ELECTRE; MODEL; PREDICTION; PERFORMANCE	This paper presents a new version of the TOPSIS method for sorting problems. The proposed method, called Preference Disaggregation on Technique for Order of Preference by Similarity to Ideal Solution - Sort (PDTOPSIS-Sort), is based on nonlinear programming for inferring parameters and uses expert's holistic evaluations. The existing TOPSIS-Sort method demands a significant number of parameters from the expert, including the definition of boundary profiles and weights. The proposed method contributes to the literature by relieving the demand for cognitive effort observed in prior methods. Instead of providing boundary profiles for the limit between every two consecutive classes, the expert provides decision examples. In addition, the specification of weights is not required. A numerical validation of PDTOPSIS-Sort was undertaken based on results previously obtained from the literature for TOPSIS-Sort, which has been presented in detail as supplementary material. In addition, the first analysis of Brazilian corporate bonds supported by an MCDM/A model is presented. To do so, data were collected from the financial statements published by the issuers of these bonds. In total, the method proposed classified 50 debentures and the results were consistent with the preferences of the decision-maker, an investment-banking expert. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				AUG 15	2020	152								113369	10.1016/j.eswa.2020.113369													
J								Enhancing random projection with independent and cumulative additive noise for privacy-preserving data stream mining	EXPERT SYSTEMS WITH APPLICATIONS										Data stream mining; Privacy-preserving data mining; Privacy-preserving data publishing; Data perturbation; Maximum A Posteriori attack	DATA PERTURBATION; CLASSIFICATION; RECONSTRUCTION	The sensitive nature of many data streams necessitates data mining techniques that are privacy-preserving. This paper proposes two data perturbation methods for privacy-preserving stream mining based on a combination of random projection, random translation, and two alternative forms of additive noise: noise generated independently for each record and noise that accumulates over the lifetime of a stream. Variations of the known input-output Maximum A Posteriori (MAP) attack that can account for the combinations of perturbation techniques are proposed as a means of evaluating the privacy guarantees of the proposed perturbation methods. The capabilities of the proposed methods to resist privacy-breaching recovery attacks and retain accuracy in models trained on perturbed data are experimentally evaluated. Experimentation revealed that the cumulative noise injection scheme outperformed other schemes by achieving a superior trade-off between privacy and classification. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				AUG 15	2020	152								113380	10.1016/j.eswa.2020.113380													
J								Marine Predators Algorithm: A nature-inspired metaheuristic	EXPERT SYSTEMS WITH APPLICATIONS										Marine Predators Algorithm; Metaheuristic; Stochastic optimization; Global optimization; Evolutionary computation; Swarm intelligence	CONSTRAINED OPTIMIZATION PROBLEMS; PARTICLE SWARM OPTIMIZATION; DIFFERENTIAL EVOLUTION; GLOBAL OPTIMIZATION; GENETIC ALGORITHM; ENGINEERING OPTIMIZATION; CELLULAR-AUTOMATA; SEARCH; LEVY; SIMULATION	This paper presents a nature-inspired metaheuristic called Marine Predators Algorithm (MPA) and its application in engineering. The main inspiration of MPA is the widespread foraging strategy namely Levy and Brownian movements in ocean predators along with optimal encounter rate policy in biological interaction between predator and prey. MPA follows the rules that naturally govern in optimal foraging strategy and encounters rate policy between predator and prey in marine ecosystems. This paper evaluates the MPA's performance on twenty-nine test functions, test suite of CEC-BC-2017, randomly generated landscape, three engineering benchmarks, and two real-world engineering design problems in the areas of ventilation and building energy performance. MPA is compared with three classes of existing optimization methods, including (1) GA and PSO as the most well-studied metaheuristics, (2) GSA, CS and SSA as almost recently developed algorithms and (3) CMA-ES, SHADE and LSHADE-cnEpSin as high performance optimizers and winners of IEEE CEC competition. Among all methods, MPA gained the second rank and demonstrated very competitive results compared to LSHADE-cnEpSin as the best performing method and one of the winners of CEC 2017 competition. The statistical post hoc analysis revealed that MPA can be nominated as a high-performance optimizer and is a significantly superior algorithm than GA, PSO, GSA, CS, SSA and CMA-ES while its performance is statistically similar to SHADE and LSHADE-cnEpSin. The source code is publicly available at: https: //github.com/afshinfaramarzi/Marine-Predators-Algorithm, http: //built-envi.com/portfolio/marinepredators-algorithm/, https://www.mathworks.com/matlabcentral/fileexchange/74578-marine-predatorsalgorithm-mpa, and http://www.alimirjalili.com/MPA.html. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				AUG 15	2020	152								113377	10.1016/j.eswa.2020.113377													
J								Interpreting RFID tracking data for simultaneously moving objects: An offline sampling-based approach	EXPERT SYSTEMS WITH APPLICATIONS										RFID tracking; MH sampling; Multiple targets; Integrity constraints; Offline approach		We consider the scenario of multiple RFID-tagged objects that simultaneously move across an indoor space where several RFID antennas are placed. We assume that a logical partition of the indoor space into a set of locations is given, along with a set of hard and weak integrity constraints describing both the valid movements of the objects and the capacity of the locations. In this setting, we address the problem of matching the collected readings to the trajectories (namely, the sequences of locations) followed by the target objects. We model this problem as estimating a probability distribution function over the possible matchings of the readings to the locations. The core of our approach is a novel Metropolis Hastings sampler that is guided by the integrity constraints to distinguish between likely and unlikely ways of interpreting the readings. The challenges of integrating the constraints into the sampler are discussed, and a thorough experimental analysis, where the proposed approach is compared with the state of the art, is provided. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				AUG 15	2020	152								113368	10.1016/j.eswa.2020.113368													
J								Deep submodular network: An application to multi-document summarization	EXPERT SYSTEMS WITH APPLICATIONS										Deep submodular network; Deep learning; Submodularity; Multi-document summarization; Extractive document summarization	NEURAL-NETWORKS	Employing deep learning makes it possible to learn high-level features from raw data, resulting in more precise models. On the other hand, submodularity makes the solution scalable and provides the means to guarantee a lower bound for its performance. In this paper, a deep submodular network (DSN) is introduced, which is a deep network meeting submodularity characteristics. DSN lets modular and submodular features to participate in constructing a tailored model that fits the best with a problem. Various properties of DSN are examined and its learning method is presented. By proving that cost function used for learning process is a convex function, it is concluded that minimization can be done in polynomial time and also, by choosing a suitable learning rate and performing enough iterations, a lower empirical error can be ensured. Finally, in order to demonstrate the applicability of DSN for real-world problems, automatic multi-document summarization is considered and a summarizer called DSNSum is introduced. Then, the performance of DSNSum is compared with the state-of-the-art summarizers based on DUC 2004 and CNN/DailyMail corpora. The experimental results show that the performance of the proposed summarizer is comparable with the state-of-the-art methods. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				AUG 15	2020	152								113392	10.1016/j.eswa.2020.113392													
J								A novel weighted TPR-TNR measure to assess performance of the classifiers	EXPERT SYSTEMS WITH APPLICATIONS										Classification; Classifiers evaluation; Assess classifiers performance; Performance measures	CLASSIFICATION ALGORITHMS; CLASS IMBALANCE	Assessing performance of different classifiers and selecting the best one is one of the most important tasks in classification problem. The assessment of classifiers becomes more complex when dataset is imbalanced because most of the frequently used performance metrics can be misleading. Many real world classification problems such as fraud detection, churn prediction, medical diagnosis, and cyber-security suffer from the problem of imbalanced datasets. Therefore, in all such classification tasks it is very important to select the best classifier very carefully. In this study we propose new weighted TPR-TNR measure to assess performance of the classifiers. The proposed measure takes into consideration imbalance ratio of the dataset and assign different weights to the TPR and TNR to assess classifiers performance. We have used five different datasets to assess performance of twelve different classifiers using weighted TPR-TNR measure and compared it with the existing measures. The experimental results show that the weighted TPR-TNR measure is more suitable to assess performance of the classifiers when dataset is imbalanced. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				AUG 15	2020	152								113391	10.1016/j.eswa.2020.113391													
J								Extracting actionable knowledge from social networks with node attributes	EXPERT SYSTEMS WITH APPLICATIONS										Social network; Node attribute; Actionable knowledge discovery; Action extraction; Random walk	ACTION-RULES	Actionable Knowledge Discovery has attracted much interest lately. It is almost a new paradigm shift toward mining more usable and more applicable knowledge in each specific domain. An action is a new tool in this research area that suggests some changes to the user to gain a profit in his/her domain. Currently, most of action mining methods rely on simple data which describes each object independently. Since social data has more complex structure due to the relationships between individuals, a major problem is that such structural information is not taken into account in the action mining process. This leads to miss some useful knowledge and profitable actions. Consequently, more effective methods are needed for mining actions. The main focus of this work is to extract cost-effective actions from social networks in which nodes have attributes. The actions suggest optimal changes in nodes' attributes that are likely to result in changing labels of users to more desired one when they are applied. We develop an action mining method based on Random Walks that naturally combines the information from the network structure with nodes attributes. We formulate action mining as an optimization problem where the goal is to learn a function that varies the values of nodes' attributes which in turn affect edges' weights in the network so that the labels of intended individuals are likely to take the desired label while minimizing the cost of incurring the changes. Experiments confirm that the proposed approach outperforms the current state-of-the-art in action mining. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				AUG 15	2020	152								113382	10.1016/j.eswa.2020.113382													
J								Word2vec-based latent semantic analysis (W2V-LSA) for topic modeling: A study on blockchain technology trend analysis	EXPERT SYSTEMS WITH APPLICATIONS										Trend analysis; Topic modeling; Word2vec; Probabilistic latent semantic analysis; Blockchain	TEXT; CLASSIFICATION	Blockchain has become one of the core technologies in Industry 4.0. To help decision-makers establish action plans based on blockchain, it is an urgent task to analyze trends in blockchain technology. However, most of existing studies on blockchain trend analysis are based on effort demanding full-text investigation or traditional bibliometric methods whose study scope is limited to a frequency-based statistical analysis. Therefore, in this paper, we propose a new topic modeling method called Word2vec-based Latent Semantic Analysis (W2V-LSA), which is based on Word2vec and Spherical k-means clustering to better capture and represent the context of a corpus. We then used W2V-LSA to perform an annual trend analysis of blockchain research by country and time for 231 abstracts of blockchain-related papers published over the past five years. The performance of the proposed algorithm was compared to Probabilistic LSA, one of the common topic modeling techniques. The experimental results confirmed the usefulness of W2V-LSA in terms of the accuracy and diversity of topics by quantitative and qualitative evaluation. The proposed method can be a competitive alternative for better topic modeling to provide direction for future research in technology trend analysis and it is applicable to various expert systems related to text mining. (C) 2020 The Authors. Published by Elsevier Ltd.																	0957-4174	1873-6793				AUG 15	2020	152								113401	10.1016/j.eswa.2020.113401													
J								An aggregative learning gravitational search algorithm with self-adaptive gravitational constants	EXPERT SYSTEMS WITH APPLICATIONS										Gravitational search algorithm; Gravitational constant; Elite individuals; Exploration and exploitation; Aggregative learning; Neural network learning	PARTICLE SWARM OPTIMIZATION; FUZZY-LOGIC; NEURAL-NETWORKS; SCALE-FREE; ADAPTATION; DESIGN; CHAOS; GSA	The gravitational search algorithm (GSA) is a meta-heuristic algorithm based on the theory of Newtonian gravity. This algorithm uses the gravitational forces among individuals to move their positions in order to find a solution to optimization problems. Many studies indicate that the GSA is an effective algorithm, but in some cases, it still suffers from low search performance and premature convergence. To alleviate these issues of the GSA, an aggregative learning GSA called the ALGSA is proposed with a self-adaptive gravitational constant in which each individual possesses its own gravitational constant to improve the search performance. The proposed algorithm is compared with some existing variants of the GSA on the IEEE CEC2017 benchmark test functions to validate its search performance. Moreover, the ALGSA is also tested on neural network optimization to further verify its effectiveness. Finally, the time complexity of the ALGSA is analyzed to clarify its search performance. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				AUG 15	2020	152								113396	10.1016/j.eswa.2020.113396													
J								PQ-RRT*: An improved path planning algorithm for mobile robots	EXPERT SYSTEMS WITH APPLICATIONS										Path planning; Sampling-based algorithms; Rapidly-exploring random tree (RRT); Optimal path planning		During the last decade, sampling-based algorithms for path planning have gained considerable attention. The RRT*, a variant of RRT (rapidly-exploring random trees), is of particular concern to researchers due to its asymptotic optimality. However, the limits of the slow convergence rate of RRT* makes it inefficient for applications. For the purposes of overcoming these limitations, this paper proposes a novel algorithm, PQ-RRT*, which combines the strengths of P-RRT* (potential functions based RRT*) and Quick-RRT*. PQ-RRT* guarantees a fast convergence to an optimal solution and generates a better initial solution. The asymptotic optimality and fast convergence of the proposed algorithm are proved in this paper. Comparisons of PQ-RRT* with P-RRT* and Quick-RRT* in four benchmarks verify the effectiveness of the proposed algorithm. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				AUG 15	2020	152								113425	10.1016/j.eswa.2020.113425													
J								A modified particle swarm optimization using adaptive strategy	EXPERT SYSTEMS WITH APPLICATIONS										Particle swarm optimization; Chaos; Stochastic learning; Mainstream learning; Adaptive strategy	NUMERICAL FUNCTION OPTIMIZATION; ALGORITHM; SEARCH; TIME; SYSTEMS	In expert systems, complex optimization problems are usually nonlinear, nonconvex, multimodal and discontinuous. As an efficient and simple optimization algorithm, particle swarm optimization(PSO) has been widely applied to solve various real optimization problems in expert systems. However, avoiding premature convergence and balancing the global exploration and local exploitation capabilities of the PSO remains an open issue. To overcome these drawbacks and strengthen the ability of PSO in solving complex optimization problems, a modified PSO using adaptive strategy called MPSO is proposed. In MPSO, in order to well balance the global exploration and local exploitation capabilities of the PSO, a chaos-based non-linear inertia weight is proposed. Meanwhile, to avoid the premature convergence, stochastic and mainstream learning strategies are adopted. Finally, an adaptive position updating strategy and terminal replacement mechanism are employed to enhance PSO's ability to solve complex optimization problems in expert systems. 30 complex CEC2017 benchmark functions are utilized to verify the promising performance of MPSO, experimental results and statistical analysis indicate that MPSO has competitive performance compared with 16 state-of-the-art algorithms. The source code of MPSO is provided at https://github.com/lhustl/MPSO. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				AUG 15	2020	152								113353	10.1016/j.eswa.2020.113353													
J								Structural optimization of fuzzy rule-based models: Towards efficient complexity management	EXPERT SYSTEMS WITH APPLICATIONS										Fuzzy rule-based systems; Particle swarm optimization; Least square error; Fuzzy c-means clustering; Complexity management	IDENTIFICATION METHOD; GENETIC ALGORITHMS; INFERENCE SYSTEMS; INTERPRETABILITY; CLASSIFICATION; DESIGN	The primary aim of this study is concerned with the structural optimization of data-driven fuzzy rule-based systems (FRBS), with the intent of their complexity management. This is accomplished in two ways: the first one involves a structuralization of the antecedents and the second one deals with a structuralization of the consequents of the fuzzy rules. More specifically, this study contributes to the complexity management of fuzzy models by focusing on (i) the efficient arrangement (reduction) of the input spaces over which the antecedents of rules are formed and (ii) allocating the orders of local polynomial functions across the consequents of the rules. The originality of the study comes with the flexibility of FRBS that is endowed by admitting variability of input spaces standing in the antecedents of different rules as well as the variability of orders of polynomials (local functions) forming the consequents of the rules. Particle swarm optimization (PSO) is guided by the root mean squared error (RMSE) accuracy criterion to realize the efficient arrangement of input spaces and an allocation of the orders of the individual polynomials. In this optimization process, the Fuzzy C-Means (FCM) algorithm is employed to create fuzzy sets in the antecedents of the rules, while the standard Least Square Error (LSE) criterion is minimized to determine the coefficients of the polynomials in the consequents. The performance of the proposed model is quantified using some numeric data, including both synthetic and machine learning datasets. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				AUG 15	2020	152								113362	10.1016/j.eswa.2020.113362													
J								A novel approach to define the local region of dynamic selection techniques in imbalanced credit scoring problems	EXPERT SYSTEMS WITH APPLICATIONS										Credit scoring; Imbalanced learning; Dynamic Selection Classification	CLASSIFICATION ALGORITHMS; ENSEMBLE CLASSIFICATION; CLASSIFIERS; SMOTE; PERFORMANCE	Lenders, such as banks and credit card companies, use credit scoring models to evaluate the potential risk posed by lending money to customers, and therefore to mitigate losses due to bad credit. The profitability of the banks thus highly depends on the models used to decide on the customer's loans. State-of-the-art credit scoring models are based on machine learning and statistical methods. One of the major problems of this field is that lenders often deal with imbalanced datasets that usually contain many paid loans but very few not paid ones (called defaults). Recently, dynamic selection methods combined with ensemble methods and preprocessing techniques have been evaluated to improve classification models in imbalanced datasets presenting advantages over the static machine learning methods. In a dynamic selection technique, samples in the neighborhood of each query sample are used to compute the local competence of each base classifier. Then, the technique selects only competent classifiers to predict the query sample. In this paper, we evaluate the suitability of dynamic selection techniques for credit scoring problem, and we present Reduced Minority k-Nearest Neighbors (RMkNN), an approach that enhances state of the art in defining the local region of dynamic selection techniques for imbalanced credit scoring datasets. This proposed technique has a superior prediction performance in imbalanced credit scoring datasets compared to state of the art. Furthermore, RMkNN does not need any preprocessing or sampling method to generate the dynamic selection dataset (called DSEL). Additionally, we observe an equivalence between dynamic selection and static selection classification. We conduct a comprehensive evaluation of the proposed technique against state-of-the-art competitors on six real-world public datasets and one private one. Experiments show that RMkNN improves the classification performance of the evaluated datasets regarding AUC, balanced accuracy, H-measure, G-mean, F-measure, and Recall. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				AUG 15	2020	152								113351	10.1016/j.eswa.2020.113351													
J								An efficient henry gas solubility optimization for feature selection	EXPERT SYSTEMS WITH APPLICATIONS										Classification; Dimensionality reduction; Feature selection (FS); Henry gas solubility optimization (HGSO); Pattern recognition	PARTICLE SWARM OPTIMIZATION; DIFFERENTIAL EVOLUTION; ALGORITHM; COLONY; CLASSIFICATION	In classification, regression, and other data mining applications, feature selection (FS) is an important preprocess step which helps avoid advert effect of noisy, misleading, and inconsistent features on the model performance. Formulating it into a global combinatorial optimization problem, researchers have employed metaheuristic algorithms for selecting the prominent features to simplify and enhance the quality of the high-dimensional datasets, in order to devise efficient knowledge extraction systems. However, when employed on datasets with extensively large feature-size, these methods often suffer from local optimality problem due to considerably large solution space. In this study, we propose a novel approach to dimensionality reduction by using Henry gas solubility optimization (HGSO) algorithm for selecting significant features, to enhance the classification accuracy. By employing several datasets with wide range of feature size, from small to massive, the proposed method is evaluated against well-known metaheuristic algorithms including grasshopper optimization algorithm (GOA), whale optimization algorithm (WOA), dragonfly algorithm (DA), grey wolf optimizer (GWO), salp swarm algorithm (SSA), and others from recent relevant literature. We used k-nearest neighbor (k-NN) and support vector machine (SVM) as expert systems to evaluate the selected feature-set. Wilcoxon's ranksum non-parametric statistical test was carried out at 5% significance level to judge whether the results of the proposed algorithms differ from those of the other compared algorithms in a statistically significant way. Overall, the empirical analysis suggests that the proposed approach is significantly effective on low, as well as, considerably high dimensional datasets, by producing 100% accuracy on classification problems with more than 11,000 features. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				AUG 15	2020	152								113364	10.1016/j.eswa.2020.113364													
J								Privacy-preserving human action recognition as a remote cloud service using RGB-D sensors and deep CNN	EXPERT SYSTEMS WITH APPLICATIONS										Privacy-preserving; Expert system; Deep learning; Human action recognition; Multimedia security; Cloud computing	VIDEO; PARADOX	Cloud-based expert systems are highly emerging nowadays. However, the data owners and cloud service providers are not in the same trusted domain in practice. For the sake of data privacy, sensitive data usually has to be encrypted before outsourcing which makes effective cloud utilization a challenging task. Taking this concern into account, we propose a novel cloud-based approach to securely recognize human activities. A few schemes exist in the literature for secure recognition. However, they suffer from the problem of constrained data and are vulnerable to re-identification attack, where advanced deep learning models are used to predict an object's identity. We address these problems by considering color and depth data, and securing them using position based superpixel transformation. The proposed transformation is designed by actively involving additional noise while resizing the underlying image. Due to this, a higher degree of obfuscation is achieved. Further, in spite of securing the complete video, we secure only four images, that is, one motion history image and three depth motion maps which are highly saving the data overhead. The recognition is performed using a four stream deep Convolutional Neural Network (CNN), where each stream is based on pre-trained MobileNet architecture. Experimental results show that the proposed approach is the best suitable candidate in "security-recognition accuracy (%)" trade-off relation among other image obfuscation as well as state-of-the-art schemes. Moreover, a number of security tests and analyses demonstrate robustness of the proposed approach. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				AUG 15	2020	152								113349	10.1016/j.eswa.2020.113349													
J								A weighted information-gain measure for ordinal classification trees	EXPERT SYSTEMS WITH APPLICATIONS										Information-gain; Decision trees; Classification tree; Weighted entropy; C4.5; Ordinal classification	DECISION TREES; ENTROPY; MONOTONICITY; ALGORITHMS	This paper proposes an ordinal decision-tree model, which applies a new weighted information-gain ratio (WIGR) measure for selecting the classifying attributes in the tree. The proposed measure utilizes a weighted entropy function that is defined proportionally to the value deviation of different classes and thus reflects the consequences of the magnitude of potential classification errors. The WIGR can be used to select the classifying attributes in decision trees in a manner that reduces risks. The proposed ordinal decision tree is found effective for classification problems in which the class variable exhibits some form of ordinal ordering, and where dependencies between the attributes and the class value can be nonmonotonic. In a series of experiments based on publicly-known datasets, it is shown that the proposed ordinal decision tree outperforms its non-ordinal counterparts that utilize traditional entropy measures. The proposed model can be used as a part of an expert system for ordinal classification applications, such as health-state monitoring, portfolio investments classification and performance evaluation of service systems. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				AUG 15	2020	152								113375	10.1016/j.eswa.2020.113375													
J								A comparative evaluation of aggregation methods for machine learning over vertically partitioned data	EXPERT SYSTEMS WITH APPLICATIONS										Vertical data partitioning; Distributed machine learning; Classification; Predictions aggregation; Attribute-partitioned data	CLASSIFICATION; SVM	It is increasingly common applications where data are naturally generated in a distributed fashion, especially after the emergence of technologies like the Internet of Things (IoT). In sensor networks, in collaborative health or genomic projects, in credit risk analysis, among other domains, distinct features are collected from multiple sources, including the use of social media and mobile applications, and due to privacy concerns or communication costs, may not be shared among sites. This scenario of vertical data partitioning poses challenges to traditional machine learning (ML) approaches, as classical algorithms are designed to learn from the complete set of features. A common strategy is to combine predictions from local models trained at each site into a global model, and for this purpose, several aggregation methods have been proposed. In this work we tackle a gap within the related literature, performing a comparative evaluation of elementary and meta-learning-based aggregation methods to reveal their strengths and weakness for 46 datasets with varied characteristics. We show that no method outperforms its counterparts in all domains, emphasizing the need for experimental comparison to ensure a good choice in the domain of interest. Moreover, our experiments provide the first insights into the relations between datasets' properties and aggregators' performance. We show that for low class imbalance and a good instance-to-feature ratio, almost all aggregation methods tend to perform well. The silhouette coefficient (reflecting class separability) and class imbalance coefficient are the most influential properties on aggregators' performance, thus we recommend their analysis in the first step of the methodological design. We found that arithmetic-based methods are not suitable for datasets with poor class separability and a large number of classes, whereas meta-learning approaches are less sensitive for datasets with silhouette coefficient close to 0. Our analyses were summarized as classification and regression trees, which have the impact to serve as practical tools for future research. Taken together, our findings give rise to interesting applications in the domain of intelligent systems, especially regarding their potential to reduce the burden of vast experimental comparisons when training ML models with feature-partitioned data. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				AUG 15	2020	152								113406	10.1016/j.eswa.2020.113406													
J								Adaptive sampling using self-paced learning for imbalanced cancer data pre-diagnosis	EXPERT SYSTEMS WITH APPLICATIONS										Imbalanced classification; Adaptive sampling; Cancer pre-diagnosis; Elastic-net regularization	SUPPORT VECTOR MACHINES; FEATURE-SELECTION; DATA CLASSIFICATION; IDENTIFICATION; CLASSIFIERS; PREDICTION; SYSTEM	The early diagnosis of cancer diseases is an indispensable part in the cancer research. It urges people to develop many new machine learning approaches to assist the diseases identification based on the gene expression data. However, the race occurrence of malignant tumors creates a challenge due to the potential over-fitting risk in the current model training. Typically, people use various sampling methods (e.g., random oversampling and undersampling) to address this challenge to provide a balanced data distribution. However, these methods might discard potentially useful samples. In this paper, we proposed an imbalanced sampling approach via self-paced learning (ISPL) to effectively select high-quality samples to improve the robustness. The experimental results showed that our proposed ISPL method increased the classification accuracy by approximately 16% compared with the average performance obtained by other sampling methods. In addition, the new method successfully selected some important genes for further investigation. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				AUG 15	2020	152								113334	10.1016/j.eswa.2020.113334													
J								Bioacoustic signal classification in continuous recordings: Syllable-segmentation vs sliding-window	EXPERT SYSTEMS WITH APPLICATIONS										Bioacoustic signal classification; Bioacoustic signal segmentation; 1D convolutional neural network	ACOUSTIC CLASSIFICATION; RECOGNITION; FEATURES	Frog population has been experiencing rapid decreases worldwide, which is regarded as one of the most critical threats to the global biodiversity. Therefore, large volumes of frog recordings have been collected for assessing this decline. Building an automatic frog species classification system is becoming ever more important. The traditional system for classifying frog species consists of four steps: (1) bioacoustic signal preprocessing, (2) segmentation, (3) feature extraction, (4) classification. Each prior step has a direct impact on the subsequent step. Consequently, the final classification performance is highly affected by the initial three steps. However, the performance of bioacoustic signal segmentation is highly dependent on the background noise of those environmental recordings. In this study, we propose an end-to-end approach for acoustic classification of frog species in continuous recordings. First, a sliding window is used to segment the audio signal into frames. Then, 1D-Convolution Neural Network and long short-term memory (CNN-LSTM) network is used to learn a representation from the raw audio signal, where three Convolutional layers and one LSTM layer are used to capture the signal's pattern. Experimental results in classifying 23 Australian frog species demonstrate the effectiveness of our proposed CNN-LSTM based method. Compared to the syllable-segmentation based frog species classification system, our proposed CNN-LSTM based approach is more robust in frog species classification under various noisy conditions. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				AUG 15	2020	152								113390	10.1016/j.eswa.2020.113390													
J								A co-evolutionary genetic algorithm for the two-machine flow shop group scheduling problem with job-related blocking and transportation times	EXPERT SYSTEMS WITH APPLICATIONS										Flow shop group scheduling; Job-related blocking time; Transportation time; Co-evolutionary genetic algorithm	SEQUENCE-DEPENDENT SETUP; HYBRID ALGORITHM	This study investigates a new two-machine flow shop group scheduling problem with job-related blocking and transportation times, which is derived from the realistic pipe-making process of steel pipe products in the modern steel manufacturing industry. In contrast to the traditional blocking constraint, the attributes of jobs, not the quantity of jobs in the buffer area, are used to determine the need for a blocking feature. The objective is to minimize the makespan. We present a mixed integer linear programming model and prove that the problem is strongly NP-hard. As the problem is a joint decision of two sub-problems, namely group scheduling and job scheduling within each group, a co-evolutionary genetic algorithm (CGA) is proposed to solve it. In the proposed CGA, the two sub-problems are synergistically evolved via a co-evolutionary framework. A block-mining-based artificial chromosome construction strategy is designed to speed up the convergence process. Computational experiments based on actual production data are carried out. The results indicate that the proposed CGA is effective for the considered problem. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				AUG 15	2020	152								113360	10.1016/j.eswa.2020.113360													
J								FluidsNet: End-to-end learning for Lagrangian fluid simulation	EXPERT SYSTEMS WITH APPLICATIONS										deep learning; Lagrangian fluid simulation; physical-based animation	SYSTEM	Over the past few decades, fluid simulation has emerged as an important tool in computer animation. However, traditional physical-based fluid simulation systems are time consuming and requires large computational resources to generate large-scale fluid flows. Intelligent systems provide us a new method of data-driven to accelerate simulations. Previous intelligent system manually crafted feature vectors by using a context-based integral method and resulted in high computational and memory requirements. Unlike the existing techniques, we do not use any manually crafted feature, instead directly operate on Lagrangian fluid simulation data and use machine learned features. This paper presents a novel end-to-end deep learning neural network that can automatically generate a model for fluid animation based-on Lagrangian fluid simulation data. This approach synthesizes velocity fields with irregular Lagrangian data structure using neural network. Every fluid particle is treated independently and identically. We use symmetric functions to capture space structure and interactions among particles and design different network structures to learn various hierarchical features of fluid. We test this method using several data sets and applications in various scenes with different sizes. Our experiments show that the model is able to infer velocity field with realistic details such as splashes. In addition, compared with exiting simulation system, this method shows significant speed-ups, especially on large scene simulations. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				AUG 15	2020	152								113410	10.1016/j.eswa.2020.113410													
J								An improved quantum particle swarm optimization algorithm for environmental economic dispatch	EXPERT SYSTEMS WITH APPLICATIONS										Environmental economic dispatch; Carbon emission reduction; Quantum particle swarm optimization; Differential evolution operator; Crossover operator; Adaptive control	COAL-FIRED POWER; GENETIC ALGORITHM; CARBON EMISSIONS; LOAD DISPATCH; PERFORMANCE; PLANTS; ALLOCATION; CAPTURE; OPTIONS; STORAGE	Consumption of traditional fossil energy has promoted rapid economic development and caused effects such as climate warming and environmental degradation. In order to solve the problem of environmental economic dispatch (EED), this paper proposes a DE-CQPSO (Differential Evolution-Crossover Quantum Particle Swarm Optimization) algorithm based on the fast convergence of differential evolution algorithms and the particle diversity of crossover operators of genetic algorithms. In order to obtain better optimization results, a parameter adaptive control method is used to update the crossover probability. And the problem of multi-objective optimization is solved by introducing a penalty factor. The experimental results show that: the evaluation index and convergence speed of the DE-CQPSO algorithm are better than QPSO (Quantum Particle Swarm Optimization) and other algorithms, whether it is single-objective optimization of fuel cost and emissions or multi-objective optimization considering both optimization objectives. A good compromise value is verified, which verifies the effectiveness and robustness of the DE-CQPSO algorithm in solving environmental economic dispatch problems. The study provides a new research direction for solving environmental economic dispatch problems. At the same time, it provides a reference for the reasonable output of the unit to a certain extent. (C) 2020 Elsevier Ltd. All rights reserved.																	0957-4174	1873-6793				AUG 15	2020	152								113370	10.1016/j.eswa.2020.113370													
J								A novel image compression technology based on vector quantisation and linear regression prediction	CONNECTION SCIENCE										Vector quantisation (VQ); pixel prediction; linear regression; compression ratio; image quality	ALGORITHM; MODELS	In the information age, a digital image is an important media for people's daily interactions. Looking to maintain the quality of the restored image, how to maximise the compression of images has become a challenging topic. Vector quantisation (VQ) compression is an easy-operating image compression method that can compress images to 1/16th of the original size. Based on VQ compresses, a novel image compression method is proposed in this paper. The proposed scheme compresses the image depending on the result of linear regression prediction which can significantly increase the compression ratio.																	0954-0091	1360-0494															10.1080/09540091.2020.1806206		AUG 2020											
J								Improving the one-against-all binary approach for multiclass classification using balancing techniques	APPLIED INTELLIGENCE										Multiclass classification; Supervised learning; Imbalanced learning; Large margin classifiers	SMOTE	One-against-one and one-against-all are common approaches to break down multiclass classification problems into binary classification problems and build a multiclass classifier. The former approach often yields better multiclass classifiers than the latter due to its structure. The one-against-all approach strengthens or sometimes creates linear inseparability and class imbalance in the binary classifiers during the training phase. In this sense, balancing techniques can be applied to handle the binary imbalance problem and motivate the use of the computationally simpler approach. The one-against-all approach with balancing techniques proposed in this work reaches better accuracy values than the pure one-against-all approach for 7 out of 8 datasets and shows a considerable increase in the weighted recall value for 4 out of 8 datasets. Besides, the accuracy values of the one-against-all approach with balancing techniques are considerably closer to the ones found by the one-against-one approach with less computational efforts.																	0924-669X	1573-7497															10.1007/s10489-020-01805-1		AUG 2020											
J								Research on plantar pressure dynamic distribution characteristics of samba step movements based on biomechanics	CONNECTION SCIENCE										Biology; samba dance; feature study	MUSCLE-ACTIVITY; GAIT; FOOT; METAANALYSIS; PEOPLE; ACCELERATION; FOREFOOT; WALKING	Samba dance is a factional part of sports dance. It forms a sports dance system with modern dance. According to the method of sports biomechanics experiment, the dynamic parameters and changing rules of fork foot pressure dynamic distribution characteristics are summarised, which can provide theoretical reference for further study of the basic law of samba dance. From the point of view of sports biomechanics, this paper analyses the rotation of athletes, and combines the knowledge of biomechanics with the rotation characteristics of samba dance. Studies have shown that in the comparison of general parameters, the different indicators of the foot pressure have a certain regularity in different action stages. There were no significant differences between the test group and the control group in terms of walking speed, relative speed, step size, relative step size, foot contact area time, and duration.																	0954-0091	1360-0494															10.1080/09540091.2020.1806205		AUG 2020											
J								The identification of influential nodes based on structure similarity	CONNECTION SCIENCE										Complex networks; influential nodes; PageRank; K-L divergence; SI model	COMPLEX NETWORKS; SMART CARD; SPREADERS; CENTRALITY	The identification of influential nodes in complex networks is an open issue. To address it, many centrality measures have been proposed, among which the most representative iteration algorithm is the PageRank algorithm. However, it ignores the correlation between nodes and assumes that the jumping probability from a node to its adjacent nodes is the same. To make up it, we proposed a method to improve the PageRank based on the structural similarity of nodes calculated by Kullback-Leibler divergence. The Susceptible-infected (SI) model was used in six real networks, and the results of comparison experiments demonstrate the effectiveness of the proposed method.																	0954-0091	1360-0494															10.1080/09540091.2020.1806203		AUG 2020											
J								M-pSC: a manifoldp-spectral clustering algorithm	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										p-Laplacian matrix; Clustering; Manifold distance; Affinity measure		Sincep-spectral clustering has good performance in many practical problems, it has attracted great attention. The Cheeger cut criterion is used inp-spectral clustering to do graph partition. However, due to the improper affinity measure and outliers, the originalp-spectral clustering algorithm is not effective in dealing with manifold data. To solve this problem, we propose a manifoldp-spectral clustering (M-pSC) using path-based affinity measure. First, we design a path-based affinity function to describe the complex structures of manifold data. This affinity function obeys the clustering assumption that the data pairs within the manifold structure share high affinities, and the data pairs between different manifold structures share low affinities. This will help us construct a good affinity matrix, which carry more category information of the points. Then we propose a M-pSC algorithm using the path-based affinity function. In the Cheeger cut criterion, thep-Laplacian matrix are constructed based on the manifold affinity function, and the final clustering results are obtained by using the eigenvectors of graphp-Laplacian. At last, the proposed algorithm is tested on several public data sets and the experiments show that our algorithm is adaptive to different manifold data. Compared with other popular clustering algorithms, our algorithm has good clustering quality and robustness.																	1868-8071	1868-808X															10.1007/s13042-020-01187-3		AUG 2020											
J								A novel randomised particle swarm optimizer	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Randomized algorithms; Evolutionary computation; Particle swarm optimization; Gaussian white noise; Acceleration coefficients	ALGORITHM; PARAMETERS; STABILITY	The particle swarm optimization (PSO) algorithm is a popular evolutionary computation approach that has received an ever-increasing interest in the past decade owing to its wide application potential. Despite the many variants of the PSO algorithm with improved search ability by means of both the convergence rate and the population diversity, the local optima problem remains a major obstacle that hinders the global optima from being found. In this paper, a novel randomized particle swarm optimizer (RPSO) is proposed where the Gaussian white noise with adjustable intensity is utilized to randomly perturb the acceleration coefficients in order for the problem space to be explored more thoroughly. With this new strategy, the RPSO algorithm not only maintains the population diversity but also enhances the possibility of escaping the local optima trap. Experimental results demonstrate that the proposed RPSO algorithm outperforms some existing popular variants of PSO algorithms on a series of widely used optimization benchmark functions.																	1868-8071	1868-808X															10.1007/s13042-020-01186-4		AUG 2020											
J								Chinese medical relation extraction based on multi-hop self-attention mechanism	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Chinese medical literature; Multi-hop self-attention mechanism; Relation extraction; Natural language processing (NLP)	INFORMATION	The medical literature is the most important way to demonstrate academic achievements and academic exchanges. Massive medical literature has become a huge treasure trove of knowledge. It is necessary to automatically extract implicit medical knowledge from the medical literature. Medical relation extraction aims to automatically extract medical relations from the medical text for various medical researches. However, there are a few kinds of research in Chinese medical literature. Currently, the popular methods are based on neural networks, which focus on semantic information on one aspect of the sentence. However, complex semantic information in the sentence determines the relation between entities, the semantic information cannot be represented by one sentence vector. In this paper, we propose an attention-based model to extract the multi-aspect semantic information for the Chinese medical relation extraction by multi-hop attention mechanism. The model could generate multiple weight vectors for the sentence through each attention step, therefore, we can generate the different semantic representation of a sentence, respectively. Our model is evaluated by using Chinese medical literature from China National Knowledge Infrastructure (CNKI). It achieves an F1 score of 93.19% for therapeutic relation tasks and 73.47% for causal relation tasks.																	1868-8071	1868-808X															10.1007/s13042-020-01176-6		AUG 2020											
J								An exploratory teaching program in big data analysis for undergraduate students	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Big data analysis; Data science; Data visualization; Exploratory data analytics; Jupyter Notebook; Reproducible education program		Many of the world's biggest discoveries and decisions in science, technology, business, medicine, politics, and society as a whole, are now being made on the basis of analyzing massive datasets. In this paper, exploratory teaching program is proposed. It provides a broad and practical introduction to big data analysis. This exploratory teaching program was designed and given in Department of Computer Engineering at Kocaeli University in the spring semester of 2018-2019. To assess the educational program's impact on the learning process and to evaluate the acceptance and satisfaction level of students, they answered a questionnaire after finishing the program. According to students' feedback, the exploratory teaching program is useful for learning how to analyze large datasets and identify patterns that will improve any company's and organization decision-making process.																	1868-5137	1868-5145				OCT	2020	11	10			SI		4285	4304		10.1007/s12652-020-02447-4		AUG 2020											
J								Novel feature selection approaches for improving the performance of sentiment classification	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Feature selection; Sentiment classification; Social media; Support vector machines; Online reviews	BLOGS	Text based social media has become one of important communication tools between customers and enterprises. In social media, users can easily express their opinions and evaluation regarding products or services. These online user experiences, especially negative evaluations indeed affect other consumers' behaviors. Consequently, to effectively identify customers' sentiments and avoid these negative comments to bring a great damage to enterprisers has become one of critical issues. In recent years, machine learning algorithms were viewed as one of effective solutions for sentiment classification. But, when the amount of the online reviews arises, the dimensionality of text data rises remarkably. The performances of machine learning methods have been degraded due to the dimensionality problem. But, conventional feature selection methods tend to select attributes from the majority sentiments, which usually cannot improve classification performance. Therefore, this study attempt to present two feature selection methods called modified categorical proportional difference (MCPD) approach that improves conventional CPD method, and balance category feature (BCF) strategy that equally selects attributes from both positive and negative examples, to improve sentiment classification performances. Finally, several real sentiment cases of text reviews will be provided to demonstrate the effectiveness of our proposed methods. Results showed that the combination of proposed BCF strategy and MCPD method can not only remarkably reduce feature space, but also improve the sentiment classification performance.																	1868-5137	1868-5145															10.1007/s12652-020-02468-z		AUG 2020											
J								An intelligent botnet blocking approach in software defined networks using honeypots	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Software defined networking; Honeypot; Botnet detection; Intelligent blocking; Network security; Cyber deception	CHALLENGES; ATTACKS	Using a massive number of coordinated and distributed machines, botnets have become one of the most sophisticated cyber threats. However, software defined networking leads to more effective mitigation approaches by providing a flexible and dynamic way to control the network. Existing botnet detection approaches fail to detect unknown botnet threats and are time consuming. Facing these shortcomings motivates us to employ honeypots as a competent solution. We propose a novel blocking approach that uses honeypots to detect and efficiently prevent botnet propagation in software defined networks. This approach identifies the relationship among botnet members and intelligently blocks them. We also design and implement a deception system based on our blocking approach with two goals: reducing the botnet infection rate and wasting the adversary's time. Experimental results, which are based on a real malware, show that our proposed system compared with current blocking approaches can reduce the infection rate up to 25% and increase the adversary's wasted time by a factor of four. Our system also provides a satisfactory detection performance.																	1868-5137	1868-5145															10.1007/s12652-020-02461-6		AUG 2020											
J								A hybrid fuzzy brain-storm optimization algorithm for the classification of brain tumor MRI images	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Brain tumor; MRI; Segmentation; Fuzzy brain-storm optimization; BRATS		Brain tumor is the most severe nervous system disorder and causes significant damage to health and leads to death. Glioma was a primary intracranial tumor with the most elevated disease and death rate. One of the most widely used medical imaging techniques for brain tumors is magnetic resonance imaging (MRI), which has turned out the principle diagnosis system for the treatment and analysis of glioma. The brain tumor segmentation and classification process was a complicated task to perform. Several problems could be more effectively and efficiently solved by the swarm intelligence technique. In this paper, the fuzzy brain-storm optimization algorithm for medical image segmentation and classification was proposed, a combination of fuzzy and brain-storm optimization techniques. Brain-storm optimization concentrates on the cluster centers and provides them the highest priority; it might fall in local optima like any other swarm algorithm. The fuzzy perform several iterations to present an optimal network structure, and the brain-storm optimization seems promising and outperforms the other techniques with better results in this analysis. The BRATs 2018 dataset was used, and the proposed FBSO was efficient, robust and mainly reduced the segmentation duration of the optimization algorithm with the accuracy of 93.85%, precision of 94.77%, the sensitivity of 95.77%, and F1 score of 95.42%.																	1868-5137	1868-5145															10.1007/s12652-020-02470-5		AUG 2020											
J								A time-aware service recommendation based on implicit trust relationships and enhanced user similarities	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Implicit trust; Indirect trusted users; Kendall correlation coefficient; Jaccard correlation coefficient; merged ratings; Time-aware recommendation	MATRIX FACTORIZATION; DATA SPARSITY; SYSTEMS	In recent years, the ever-increasing number of cloud services has led to the service targeting issue. It becomes challenging to provide users with services that fit their needs. Recommender systems address the service targeting problem by helping users to easily retrieve services matching their tastes. Collaborative filtering-based methods are spreadly used recommendation approaches since numerous methods are based on them. However, most of them assume the stationarity of user profiles over time, which is an unrealistic consideration. Indeed, in practice, users' tastes are dynamic and therefore show a significant variability over time. Time-aware recommender systems effectively integrate the variability of users' needs to improve the recommendation quality. However, the recommendation credibility is another crucial requirement that is not met by those time-aware recommender systems that moreover badly deal with the data sparsity issue. To address the service targeting problem while meeting requirements on the recommendation reliability and the users' tastes variability, we propose in this paper a realistic temporal service recommendation approach based on implicit trust relationships inference. Our method integrates the time feature into the recommendation process in order to consider the instantaneity of users' needs. Our proposal is based on a trusted network that infers implicit trust relationships to ensure the recommendation credibility even in case of data sparsity. Experiments are conducted on real-world service invocations datasets. Our proposal is then compared to existing methods and displays valuable performances in terms of mean absolute error, root mean square error, and normalized discounted cumulative.																	1868-5137	1868-5145															10.1007/s12652-020-02462-5		AUG 2020											
J								An Isabelle/HOL Formalisation of the SPARC Instruction Set Architecture and the TSO Memory Model	JOURNAL OF AUTOMATED REASONING										Instruction set architecture; Form verification; Isabelle/HOL; Weak memory model; TSO		The SPARC instruction set architecture (ISA) has been used in various processors in workstations, embedded systems, and in mission-critical industries such as aviation and space engineering. Hence, it is important to provide formal frameworks that facilitate the verification of hardware and software that run on or interface with these processors. In this work, we give the first formal model formulti-coreSPARC ISA and Total Store Ordering (TSO) memory model in Isabelle/HOL. We present two levels of modelling for the ISA: The low-level ISA model, which isexecutable, covers many features specific to SPARC processors, such as delayed-write for control registers, windowed general registers, and more complex memory access. We have tested our model extensively against a LEON3 simulation board, the test covers both single-step executions and sequential execution of programs. We also prove some important properties for our formal model, including a non-interference property for the LEON3 processor. The high-level ISA model is an abstraction of the low-level model and it provides an interface for memory operations in multi-core processors. On top of the high-level ISA model, we formalise two TSO memory models: one is an adaptation of the axiomatic SPARC TSO model (Sindhu et al. in Formal specification of memory models, Springer, Boston, 1992; SPARC in The SPARC architecture manual version 8, 1992.), the other is a new operational TSO model which is suitable for verifying execution results. We prove that the operational model is sound and complete with respect to the axiomatic model. Finally, we give verification examples with two case studies drawn from the SPARCv9 manual.																	0168-7433	1573-0670															10.1007/s10817-020-09579-4		AUG 2020											
J								An adaptive large neighborhood search heuristic for the planar storage location assignment problem: application to stowage planning for Roll-on Roll-off ships	JOURNAL OF HEURISTICS										Maritime transportation; Packing; Stowage; Roll-on; Roll-off; RoRo	BERTH ALLOCATION; YARD; TRANSSHIPMENT; WAREHOUSE; ALGORITHM; PACKING	This paper considers a generalized version of the planar storage location problem arising in the stowage planning for Roll-on/Roll-off ships. A ship is set to sail along a predefined voyage where given cargoes are to be transported between different port pairs along the voyage. We aim at determining the optimal stowage plan for the vehicles stored on a deck of the ship so that the time spent moving vehicles to enable loading or unloading of other vehicles (shifting), is minimized. We propose a novel mixed integer programming model for the problem, considering both the stowage and shifting aspect of the problem. An adaptive large neighborhood search (ALNS) heuristic with several new destroy and repair operators is developed. We further show how the shifting cost can be effectively evaluated using Dijkstra's algorithm by transforming the stowage plan into a network graph. The computational results show that the ALNS heuristic provides high quality solutions to realistic test instances.																	1381-1231	1572-9397				DEC	2020	26	6					885	912		10.1007/s10732-020-09451-z		AUG 2020											
J								Intelligent computing and simulation in seismic mitigation efficiency analysis for the variable friction coefficient RFPS structure system	NEURAL COMPUTING & APPLICATIONS										Seismic isolation structure; Inter-story displacement; The theorem of kinematics energy; The nonlinear differential equation		According to Lagrange's equation, the governing equation for a base isolated structural system was presented, with a variable friction coefficient RFPS (the rolling friction pendulum system) bearing on arbitrary curves. Its solution was presented with software prepared from MATLAB language. Results show that the variable friction coefficient RFPS might result in complication of structural dynamic responses. It is noticed that seismic mitigation efficiency for the variable friction coefficient is close to that for the high friction coefficient, but with notable disadvantages to the lower friction coefficient. As RFPS slipper transits from the low- to the high-friction area, the structural system could exhibit noticeable whipping effect, which will amplify story accelerations. It is suggested that lower friction coefficient FPS bearings, instead of variable friction coefficient RFPS bearings, should be applied.																	0941-0643	1433-3058															10.1007/s00521-020-05290-y		AUG 2020											
J								Deep Plot-Aware Generalized Matrix Factorization for Collaborative Filtering	NEURAL PROCESSING LETTERS										Plot representation; Generalized matrix factorization; Information fusion; Convolutional neural networks		Fusing auxiliary information into ratings has shown promising performance for many recommendation tasks, such as age, sex, vocation of users or actors, director, genre, reviews of movies. However, all above auxiliary information is still sparse and not informative. For movie recommendations, besides the above information, there exists richer information in plot texts, exerting huge impacts on improving the recommendation accuracy. In this paper, we explore effective fusion of movie ratings and plot texts, we propose a deep plot-aware generalized matrix factorization for collaborative filtering model, which effectively combines both ratings and plot texts to implement a generalized collaborative filtering. To verify our proposal, we conduct extensive experiments on two popular datasets, and the results perform better than other state-of-the-art approaches in common recommendation tasks.																	1370-4621	1573-773X															10.1007/s11063-020-10333-5		AUG 2020											
J								A simple numerical scheme for generation of weighting factors for multiobjective optimisation	SOFT COMPUTING										Weighting factors; Scalarisation; Many-objective optimisation; Multiobjective meta-heuristics	NONDOMINATED SORTING APPROACH; ALGORITHM; MOEA/D	Optimisers for multi- or many-objective optimisation problems can be categorised as scalarisation and meta-heuristic approaches. Many of the approaches from both groups require to use a set of weighting vectors, which are expected to be as evenly distributed as possible. The current practice employs the normal-boundary intersection (NBI) method which has one disadvantage in that the number of sampling points must be the number ofk-combination. This work proposes a numerical scheme called clustering-based hyperplane sampling (CBHS) to deal with such a weak point. The method is based on random sampling on a hyperplane and clustering. The classical NBI method and some of its extended versions are used to examine the performance of the proposed algorithm. The comparative results reveal that CBHS is the best performer with using longer computing time. Moreover, its real advantage is the capability of generating a set of weighting vectors with any sample size.																	1432-7643	1433-7479															10.1007/s00500-020-05249-0		AUG 2020											
J								Generalized trapezoidal hesitant fuzzy numbers and their applications to multi criteria decision-making problems	SOFT COMPUTING										Fuzzy sets; Hesitant fuzzy sets; Generalized trapezoidal hesitant fuzzy (GTHF) numbers; Score; Standard deviation degree; Deviation degree; Aggregation operators; Multi-criteria decision-making	EINSTEIN AGGREGATION OPERATORS; INFORMATION AGGREGATION; GEOMETRIC OPERATORS; SETS	Generalized hesitant trapezoidal fuzzy number whose membership degrees are expressed by several possible trapezoidal fuzzy numbers, is more adequate or sufficient to solve real-life decision problem than real numbers. Therefore, in this paper, to model the some multi-criteria decision-making (MCDM) problems, we define concept of generalized trapezoidal hesitant fuzzy (GTHF) number, whose membership degrees of an element to a given set are expressed by several different generalized trapezoidal fuzzy numbers in the set of real numbersR. Then, we introduce some basic operational laws of GTHF-numbers and some properties of them. Also, we propose a decision-making method to solve the MCDM problems in which criteria values take the form of GTHF information. To use in proposed decision-making method, we first give definitions of some concepts such as score, standard deviation degree, deviation degree of GTHF-numbers. We second develop some GTHF aggregation operators called the GTHF-number weighted geometric operator, GTHF-number weighted arithmetic operator, GTHF-number weighted geometric operator, GTHF-number weighted arithmetic operator. Finally, we give a numerical example for proposed MCDM to validate the reasonable and applicable of the proposed method.																	1432-7643	1433-7479															10.1007/s00500-020-05201-2		AUG 2020											
J								Development of smart controller for demand side management in smart grid using reactive power optimization	SOFT COMPUTING										Smart controller; Reactive power control; DSM; EHO-FF algorithm; Smart grid	ENERGY MANAGEMENT; MULTIAGENT SYSTEMS; FRAMEWORK; ALGORITHM; GENERATION	Reactive power optimization is one of the major problems of concern in smart grid (SG) environment. Although several techniques have been proposed for reactive power optimization, demand side management (DSM) plays a vital role in smart grid networks. The main idea of this work is to address reactive power optimization by developing a smart controller for DSM by effective monitoring of real power loss in the smart grid network. The proposed smart controller is developed by formulating the DSM as an optimization problem and obtaining its solution by applying elephant herd optimization-firefly (EHO-FF) evolutionary algorithm. Further, the proposed smart controller for DSM aims to meet the power demand and limit the power flow in transmission network by adding distributed generation (DGs) units at optimal locations. The proposed work aims to improve the energy efficiency and voltage profile in the power grid network when operating under different load scenarios. The benchmark IEEE 30 bus system consisting of 6 generating units, 41 transmission lines with total load of 283.4 MW and 126.2 MVAR is used as the test system in this work. The test system is subjected to varying load pattern for 24 h in a typical day. The performance of the proposed smart controller is evaluated on the bench mark IEEE 30 bus system through program code developed in MATLAB environment. The simulation results have proved that the proposed smart controller for DSM minimizes the power loss and improves the voltage profile significantly by incorporating DG units in optimal locations. The effectiveness of the EHO-FF algorithm is analyzed by comparing the results obtained with PSO and bAT algorithm.																	1432-7643	1433-7479															10.1007/s00500-020-05246-3		AUG 2020											
J								Tyre pattern image retrieval - current status and challenges	CONNECTION SCIENCE										Tyre pattern retrieval; tyre pattern data sets; low-level features; high-level semantic features; future research direction	FEATURES	Tyre pattern image retrieval (TPIR) is an important tool in the investigation of criminal activities and traffic accidents. Although content-based image retrieval (CBIR) has been developed for decades with abundant results, the study on TPIR which started in the 1990s has not made much progress. The lack of large standard test datasets is a crucial shortcoming which limits the research in this field. Information presented in this paper is a result of the authors' literature research on recent academic publications and practical field investigation in the public security and transportation sectors. The state-of-the-art technologies in the field of TPIR are surveyed in detail from two aspects of tyre patterns - their low-level spatial features and high-level semantic features. Existing algorithms are examined and their pros and cons are compared and verified through experimental results. This paper also surveys the available tyre pattern datasets used in all available literature. Finally, with the considerations on technology trends in image retrieval and application requirements in TPIR, the future research directions in this field are laid out.																	0954-0091	1360-0494															10.1080/09540091.2020.1806207		AUG 2020											
J								Improving covariance-regularized discriminant analysis for EHR-based predictive analytics of diseases	APPLIED INTELLIGENCE										Linear discriminant analysis; De-sparsified graphical lasso; Electronic health records; High dimension low sample size	SURVEILLANCE; DEPRESSION; SELECTION; RATES	Linear Discriminant Analysis (LDA) is a well-known technique for feature extraction and dimension reduction. The performance of classical LDA however, significantly degrades on the High Dimension Low Sample Size (HDLSS) data for theill-posed inverse problem. Existing approaches for HDLSS data classification typically assume the data in question are with Gaussian distribution and deal the HDLSS classification problem with regularization. However, these assumptions are too strict to hold in many emerging real-life applications, such as enabling personalized predictive analysis using Electronic Health Records (EHRs) data collected from an extremely limited number of patients who have been diagnosed with or without the target disease for prediction. In this paper, we revised the problem of predictive analysis of disease using personal EHR data and LDA classifier. To fill the gap, in this paper, we first studied an analytical model that understands the accuracy of LDA for classifying data with arbitrary distribution. The model gives a theoretical upper bound of LDA error rate that is controlled by two factors: (1) thestatistical convergence rateof (inverse) covariance matrix estimators and (2) the divergence of the training/testing datasets to fitted distributions. To this end, we could lower the error rate by balancing the two factors for better classification performance. Hereby, we further proposed a novel LDA classifierDe-Sparsethat leveragesDe-sparsified Graphical Lassoto improve the estimation of LDA, which outperforms state-of-the-art LDA approaches developed for HDLSS data. Such advances and effectiveness are further demonstrated by both theoretical analysis and extensive experiments on EHR datasets https://www.overleaf.com/project/5d2728c718f6ff3b2bcf5991.																	0924-669X	1573-7497															10.1007/s10489-020-01810-4		AUG 2020											
J								PM(2.5)estimation using multiple linear regression approach over industrial and non-industrial stations of India	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Aerosol optical depth; Respirable suspended particulate matter; Multiple linear regression	AEROSOL OPTICAL DEPTH; ESTIMATING PM2.5 CONCENTRATION; PARTICULATE MATTER PM2.5; URBAN AIR-POLLUTION; GROUND-LEVEL PM2.5; UNITED-STATES; METEOROLOGICAL VARIABLES; CALIBRATION APPROACH; PM10; SENSITIVITY	PM2.5(particulate matter size less than 2.5 mu m, also called Respirable suspended particulate matter (RSPM)) is causing devastating effects on various living entities and is deleterious more than any other pollutants. As ambient air pollution is a scourge to India, in the present research work, PM(2.5)is considered and the current study aims to estimate surface level PM(2.5)concentrations using satellite-derived aerosol optical depth (AOD) along with meteorological data obtained from reanalysis and in-situ measurements over two different cities of India, namely: Agra, a non-industrial site for a study period of 2011-2015 and Rourkela, a highly industrialized location for 2009-2013, respectively. From the average daily variation of PM2.5, the pollution levels are critical and exceeding the threshold values defined by the pollution control board for most of the days at both the sites. Satellite-observed AOD values were also found to be very high over Agra (average AOD 0.76-0.8) and Rourkela (average AOD 0.4-0.46) during the study period. The annual exceedance factor (AEF) values over Agra and Rourkela were found to be always > 1.5 which indicates the above critical state of pollution. Traditional simple linear regression method (Model I), multiple linear regression (Model II (a-e)), log-linear regression (Model III) and conditional based MLR (Model IV and Model V) methods are applied to estimate the PM(2.5)concentrations over Taj for Agra region for a study period of 2011-2015 and Sonaparbat for Rourkela region for a study period of 2009-2013. The models obtained over Taj and Sonaparbat are applied to Rambagh (2011-2015) and Rourkela (2009-2013) sites for validation. The coefficient of determination (R) between observed and estimated values are found to be statistically significant for model II (e) during training and validation at both the sites and model performance is adequate. The Model II (e) can thus be used as a unified explanatory model for the estimation of PM(2.5)over these two monitoring stations.																	1868-5137	1868-5145															10.1007/s12652-020-02457-2		AUG 2020											
J								Developing a hand sizing system for a hand exoskeleton device based on the Kansei Engineering method	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Hand anthropometry; Dorsal aspect; Self-organizing map; Clustering; Sizing system; Hand exoskeleton assistive devices	DIMENSIONS	This study aimed to identify the customers' requirements for a hand rehabilitation exoskeleton device using the Kansei Engineering approach and propose improvements. The Kansei Engineering approach was used to extract the design attributes for a hand rehabilitation exoskeleton device by conducting an in-depth interview with 3 physical therapists. The Kano questionnaire survey investigated 25 participants with hand rehabilitation experience. The Kano classification results found three attractive factors and corresponded it to the design attributes of the evaluation grid method results that were suitable (size), flexible (controllability), and light (weight). The principal component analysis results indicated three components can be considered as the improvement categories; functions, structure-texture, and appearance of the hand exoskeleton device. The new functional and structural designs were proposed in a previous study. This study collects 110 male participants' hand anthropometric data to establish a hand sizing system as a design improvement (suitable attribute). The 3D and manual measurements were obtained for 63 hand dimensions with an open palm posture. The 14 dorsal length dimensions with three bent finger postures were also manually measured. The results showed that the MAD Precision (mean absolute difference) of 3D measurement is smaller than that of manual measurement, indicating better reproducibility of 3D scanning. The MAD(Accuracy) of all dimensions are within the 1 mm of the referenced ISO 20685 criteria. Significant differences were found in the 14 dorsal hand length, showing that databases should explicitly state the aspect (dorsal or palmar) where dimensions have been of measurement. The 77 hand dimensions were taken into consideration to develop a hand sizing system using the clustering results. The two-level self-organizing map method was performed and 3 handshape types and 8 sizes were extracted. The coverage rate of the sizing system was over 85%. These results provide useful information for designing and manufacturing for hand-related products.																	1868-5137	1868-5145															10.1007/s12652-020-02354-8		AUG 2020											
J								A novel access control mechanism for secure cloud communication using SAML based token creation	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Access control; Cloud computing; Security assertion markup language; Cloud service provider	ATTRIBUTE-BASED ENCRYPTION; MANAGEMENT; IDENTITY	Cloud computing has prominent branches of research with emerging technologies. The research related to security has increased with unusual disputes. Cloud service providers, as well as the users, have similar problems in concern with security issues. This paper has implemented to surmount protection-connected problems in deep related to data transfer problems in connection with efficiency in regard to time and cost. A hybrid access control mechanism (HACM) has been implemented, combining User-ID and User-Profile verification. Furthermore, the mechanism emphasizes the provision of safety measures in various circumstances. In this mechanism, SAML based token creation is employed to enhance safety measures. The architecture of this hybrid approach is offered to deal with User-ID and User-Profile managerial issues in the INTER/INTRA cloud framework. Indeed, it is verified with the implementation that this approach effectively improves safety measures in regard to time and cost.																	1868-5137	1868-5145															10.1007/s12652-020-02427-8		AUG 2020											
J								Ontologies and Data Management: A Brief Survey	KUNSTLICHE INTELLIGENZ											TEMPORAL DESCRIPTION LOGIC; EXPRESSIVE DESCRIPTION LOGICS; REGULAR PATH QUERIES; DL-LITE FAMILY; KNOWLEDGE REPRESENTATION; CONJUNCTIVE QUERIES; OWL-DL; UNDECIDABLE FRAGMENTS; POSSIBILISTIC LOGIC; MODULE EXTRACTION	Information systems have to deal with an increasing amount of data that is heterogeneous, unstructured, or incomplete. In order to align and complete data, systems may rely on taxonomies and background knowledge that are provided in the form of an ontology. This survey gives an overview of research work on the use of ontologies for accessing incomplete and/or heterogeneous data.																	0933-1875	1610-1987				SEP	2020	34	3			SI		329	353		10.1007/s13218-020-00686-3		AUG 2020											
J								Passive concept drift handling via variations of learning vector quantization	NEURAL COMPUTING & APPLICATIONS										Stream classification; Concept drift; Robust Soft Learning Vector Quantization; Generalized Learning Vector Quantization		Concept drift is a change of the underlying data distribution which occurs especially with streaming data. Besides other challenges in the field of streaming data classification, concept drift has to be addressed to obtain reliable predictions. Robust Soft Learning Vector Quantization as well as Generalized Learning Vector Quantization has already shown good performance in traditional settings and is modified in this work to handle streaming data. Further, momentum-based stochastic gradient descent techniques are applied to tackle concept drift passively due to increased learning capabilities. The proposed work is tested against common benchmark algorithms and streaming data in the field and achieved promising results.																	0941-0643	1433-3058															10.1007/s00521-020-05242-6		AUG 2020											
J								Boosting attention fusion generative adversarial network for image denoising	NEURAL COMPUTING & APPLICATIONS										Boosting; Image denoising; Generative adversarial network; Attention	MIXED NOISE REMOVAL; K-SVD; REDUCTION; ALGORITHM	Boosting has received considerable attention to improve the overall performance of model in multiple tasks by cascading many steerable sub-modules. In this paper, a boosting attention fusion generative adversarial network (BAF-GAN) was proposed, which allows boosting idea and attention mechanism modeling for high-quality image denoising. Specifically, several boosting module groups (BMGs) with group skip connection were employed to form denoiser. Each BMG contains some boosting attention fusion blocks (BAFBs). Each BAFB consists of parallel spatial attention unit and channel attention unit interleaved connection. Moreover, the multi-dimensional inner skip connection within BAFB can carry abundant informative features. Besides, spatial and channel attention mechanisms were also embedded in the discriminator to enhance its ability of discriminating various dimensional information. Meanwhile, a new loss function was given to assist the training process of the model. BAF-GAN can be applied to remove image noise, e.g., Gaussian noise and mixed noise. Comprehensive experiment results demonstrate that the BAF-GAN has the state-of-the-art performance.																	0941-0643	1433-3058															10.1007/s00521-020-05284-w		AUG 2020											
J								Research on trend analysis method of multi-series economic data based on correlation enhancement of deep learning	NEURAL COMPUTING & APPLICATIONS										Association rules; Correlation coefficient; Deep learning; Machine learning; Time-series analysis; Financial engineering	PRICE; MODEL	The analysis on economic data based on time series takes an important position in the field of analysis on time-series data and is also an important task of the field of big data and artificial intelligence. Traditional time-series analysis method is of relatively weak competence in dealing with multi-series analysis. In this research, based on the problem associated with the analysis on time-series economic data, efficient handling method and model are put forward in the face of multi-series analysis task. Also, combined with the association rules, trend correlation and self-trend correlation among multiple series, a trend and correlation deep neural network model (TC-DNM) is established and then tested and verified by using three kinds of economic datasets with representativeness based on the trend analysis task handed by multi-series analysis. The results show that the model proposed in this research is effective than a number of baseline models, can be employed to achieve precision-recall balance and also possesses strong reusability. The two correlation models and joint models in this paper are of peculiarity and innovativeness.																	0941-0643	1433-3058															10.1007/s00521-020-05263-1		AUG 2020											
J								Motion control of multiple humanoids using a hybridized prim's algorithm-fuzzy controller	SOFT COMPUTING										Humanoid NAO; Prim's algorithm; Fuzzy controller; Motion planning; Simulation; V-REP	MOBILE ROBOT; GENETIC ALGORITHM; NEURAL-NETWORK; NAVIGATION; PATH; OPTIMIZATION	Prim's algorithm has demonstrated a very effective and selective method of solving the minimum spanning tree optimization problems. It is a greedy algorithm that starts from an empty spanning tree and reaches its goal by picking the minimum weight edges which alternately optimizes the path in less possible time. In this paper, the capability of prim's algorithm in designing the behavioural controller of a humanoid robot has been shown. Here, a new hybrid PA-Fuzzy motion planning approach has been proposed that uses the concept of minimizing the distance between the robot and obstacles as well as robot and target. An optimal turning angle is generated by the hybrid controller that helps to avoid the obstacles present in the arena to create a collision-free path. The results obtained from hybrid PA-Fuzzy motion planning procedure show the capability of the controller in achieving the optimal paths in different environments with both static and dynamic obstacles. The results observed from simulation and experimental arenas are found to be in satisfactory agreement with each other producing minimal error limits. The developed hybrid technique is compared with some existing methodologies, and significant improvement is found in relation to path length and computational time.																	1432-7643	1433-7479															10.1007/s00500-020-05212-z		AUG 2020											
J								Novel operators for quantum evolutionary algorithm in solving timetabling problem	EVOLUTIONARY INTELLIGENCE										Optimization; Quantum evolutionary algorithms; Diversity preservation; Reinitialization; Structured population	LOCAL SEARCH ALGORITHMS; GENETIC ALGORITHM; FITNESS LANDSCAPE; POPULATION-SIZE; DIVERSITY; OPTIMIZATION; REAL; HEURISTICS; GENERATION; ANATOMY	Timetabling is a well-known combinatorial optimization problem which belongs to the class of NP hard problems. Quantum Evolutionary Algorithms (QEA) are highly suitable for the class of combinatorial optimization problems, but since proposed, they have not been used in solving this problem. In this paper we develop new operators for QEA to improve its performance in solving timetabling problem. The first operator we develop is a reinitialization operator which checks the population and when it is converged, reinitializes it to maintain the diversity. The other operator is called the Diversity Preserving operator which monitors the q-individuals, and if it finds more than one q-individuals searching around the same local optimum, reinitializes some of them to make sure different q-individuals are exploiting different regions in the search space. In this paper we also study the population size and the population structure of QEA in solving timetabling problem. In order to test the proposed algorithm, we perform experiments and compare the proposed algorithm to the existing algorithms on some well-known benchmark functions.																	1864-5909	1864-5917															10.1007/s12065-020-00438-0		AUG 2020											
J								Natural Projection as Partial Model Checking	JOURNAL OF AUTOMATED REASONING										Natural projection; Partial evaluation; Formal verification; Model checking	SUPERVISORY CONTROL; LOGIC	Verifying the correctness of a system as a whole requires establishing that it satisfies a global specification. When it does not, it would be helpful to determine which modules are incorrect. As a consequence, specification decomposition is a relevant problem from both a theoretical and practical point of view. Until now, specification decomposition has been independently addressed by the control theory and verification communities throughnatural projectionandpartial model checking, respectively. We prove that natural projection reduces to partial model checking and, when cast in a common setting, the two are equivalent. Apart from their foundational interest, our results build a bridge whereby the control theory community can reuse algorithms and results developed by the verification community. Furthermore, we extend the notions of natural projection and partial model checking from finite-state to symbolic transition systems and we show that the equivalence still holds. Symbolic transition systems are more expressive than traditional finite-state transition systems, as they can model large systems, whose behavior depends on the data handled, and not only on the control flow. Finally, we present an algorithm for the partial model checking of both kinds of systems that can be used as an alternative to natural projection.																	0168-7433	1573-0670				OCT	2020	64	7			SI		1445	1481		10.1007/s10817-020-09568-7		AUG 2020											
J								Identification of cutting tool wear condition in turning using self-organizing map trained with imbalanced data	JOURNAL OF INTELLIGENT MANUFACTURING										Tool wear monitoring; Self-organizing map neural network; Imbalanced data; Unsupervised learning; Vibration measurement	SUPPORT VECTOR MACHINE; CLASSIFICATION; ALGORITHM; FEATURES; SYSTEM; SIGNAL; SVM	One of the most important parameters in machining process is tool wear. Thus, monitoring the wear of cutting tools is essential to ensure product quality, increase productivity, reduce environmental impact and avoid catastrophic damages. As wear is related to the vibrations of the process, the vibration signal is commonly used to monitor the process non-intrusively. Traditional wear monitoring techniques present a number of problems such as: the difficulty of identifying vibration features sensitive to wear evolution, the specialist requirement for supervising the model training and an endless series of tests to work with balanced data. To overcome these difficulties, this paper aims to propose a new approach in the application of unsupervised artificial intelligence technique with imbalanced data to identify the cutting tool wear condition during the turning process. The methodology will allow industrial applications since no supervision is required in the model training when machining condition is changed. From vibration signals collected during each tool pass, a self-organizing map model was used to identify the ideal moment of tool change. The classifier used was compared to benchmark supervised methods (weighted k-nearest neighbor and support vector machine). Imbalanced data sets were used to simulate the industrial reality. Tool tests were performed under different wear conditions and changing the cutting parameters. The results showed that it is possible to predict the cutting tool wear condition with a self-organizing map neural for imbalanced data, using only the vibration signal with up to 92% accuracy.																	0956-5515	1572-8145															10.1007/s10845-020-01564-3		AUG 2020											
J								Fisher-regularized supervised and semi-supervised extreme learning machine	KNOWLEDGE AND INFORMATION SYSTEMS										Extreme learning machine; Semi-supervised learning; Within-class scatter; Fisher regularization; Manifold regularization	FEEDFORWARD NETWORKS; ALGORITHM; MANIFOLD	The structural information of data contains useful prior knowledge and thus is important for designing classifiers. Extreme learning machine (ELM) has been a potential technique in handling classification problems. However, it only simply considers the prior class-based structural information and ignores the prior knowledge from statistics and geometry of data. In this paper, to capture more structural information of the data, we first propose a Fisher-regularized extreme learning machine (called Fisher-ELM) by applying Fisher regularization into the ELM learning framework, the main goals of which is to build an optimal hyperplane such that the output weight and within-class scatter are minimized simultaneously. The proposed Fisher-ELM reflects both the global characteristics and local properties of samples. Intuitively, the Fisher-ELM can approximatively fulfill the Fisher criterion and can obtain good statistical separability. Then, we exploit graph structural formulation to obtain semi-supervised Fisher-ELM version (called Lap-FisherELM) by introducing manifold regularization that characterizes the geometric information of the marginal distribution embedded in unlabeled samples. An efficient successive overrelaxation algorithm is used to solve the proposed Fisher-ELM and Lap-FisherELM, which converges linearly to a solution, and can process very large datasets that need not reside in memory. The proposed Fisher-ELM and Lap-FisherELM do not need to deal with the extra matrix and burden the computations related to the variable switching, which makes them more suitable for relatively large-scale problems. Experiments on several datasets verify the effectiveness of the proposed methods.																	0219-1377	0219-3116				OCT	2020	62	10					3995	4027		10.1007/s10115-020-01484-x		AUG 2020											
J								A fuzzy-AHP-based approach to select software architecture based on quality attributes (FASSA)	KNOWLEDGE AND INFORMATION SYSTEMS										Architectural styles; Software architecture; Multi-objective decision method; Fuzzy logic; The fuzzy hierarchical analysis process		The software system design phase has recently received increasing attention due to continuous growth in both the size and complexity of software systems. As a key concept of this phase, software architecture plays an important role in the software extension cycle to the extent that the success of a software project is often determined by the degree of its design efficiency. In addition, software architecture evaluation is a fundamental step toward its subsequent validation. This paper is an attempt to propose an innovative method, based on fuzzy logic, to evaluate software architecture that addresses the inherent problems of existing methods found in the literature. The method can be used for complete design or even reconstruction of the architecture. Given the multi-faceted nature of the problem of evaluation and selection of an optimal architecture, we have employed a multi-objective decision technique, namely fuzzy hierarchical analysis process, which solves the problems associated with uncertainties and inaccuracies by incorporating fuzzy logic.																	0219-1377	0219-3116															10.1007/s10115-020-01496-7		AUG 2020											
J								Multimodal machine translation through visuals and speech	MACHINE TRANSLATION										Natural language processing; Machine translation; Multimodal machine translation; Image-guided translation; Speech language translation	RECOGNITION; DATASETS	Multimodal machine translation involves drawing information from more than one modality, based on the assumption that the additional modalities will contain useful alternative views of the input data. The most prominent tasks in this area are spoken language translation, image-guided translation, and video-guided translation, which exploit audio and visual modalities, respectively. These tasks are distinguished from their monolingual counterparts of speech recognition, image captioning, and video captioning by the requirement of models to generate outputs in a different language. This survey reviews the major data resources for these tasks, the evaluation campaigns concentrated around them, the state of the art in end-to-end and pipeline approaches, and also the challenges in performance evaluation. The paper concludes with a discussion of directions for future research in these areas: the need for more expansive and challenging datasets, for targeted evaluations of model performance, and for multimodality in both the input and output space.																	0922-6567	1573-0573				SEP	2020	34	2-3					97	147		10.1007/s10590-020-09250-0		AUG 2020											
J								Building Thinking Machines by Solving Animal Cognition Tasks	MINDS AND MACHINES										Turing test; AI; Comparative cognition; Deep reinforcement learning	INTELLIGENCE; OBJECTS	In 'Computing Machinery and Intelligence', Turing, sceptical of the question 'Can machines think?', quickly replaces it with an experimentally verifiable test: the imitation game. I suggest that for such a move to be successful the test needs to berelevant, expansive, solvable by exemplars, unpredictable, and lead toactionableresearch. The Imitation Game is only partially successful in this regard and its reliance on language, whilst insightful for partially solving the problem, has put AI progress on the wrong foot, prescribing a top-down approach for building thinking machines. I argue that to fix shortcomings with modern AI systems a nonverbal operationalisation is required. This is provided by the recent Animal-AI Testbed, which translates animal cognition tests for AI and provides a bottom-up research pathway for building thinking machines that create predictive models of their environment from sensory input.																	0924-6495	1572-8641															10.1007/s11023-020-09535-6		AUG 2020											
J								Spiking Neural Networks: Background, Recent Development and the NeuCube Architecture	NEURAL PROCESSING LETTERS										Artificial neural networks; Spiking neural networks; Spike encoding; Spike-timing dependent plasticity; Spatio-temporal brain data; NeuCube	MODEL; INFORMATION; IMPULSES; NEURONS; STDP	This paper reviews recent developments in the still-off-the-mainstream information and data processing area of spiking neural networks (SNN)-the third generation of artificial neural networks. We provide background information about the functioning of biological neurons, discussing the most important and commonly used mathematical neural models. Most relevant information processing techniques, learning algorithms, and applications of spiking neurons are described and discussed, focusing on feasibility and biological plausibility of the methods. Specifically, we describe in detail the functioning and organization of the latest version of a 3D spatio-temporal SNN-based data machine framework called NeuCube, as well as it's SNN-related submodules. All described submodules are accompanied with formal algorithmic formulations. The architecture is highly relevant for the analysis and interpretation of various types of spatio-temporal brain data (STBD), like EEG, NIRS, fMRI, but we highlight some of the recent both STBD- and non-STBD-based applications. Finally, we summarise and discuss some open research problems that can be addressed in the future. These include, but are not limited to: application in the area of EEG-based BCI through transfer learning; application in the area of affective computing through the extension of the NeuCube framework which would allow for a biologically plausible SNN-based integration of central and peripheral nervous system measures. Matlab implementation of the NeuCube's SNN-related module is available for research and teaching purposes.																	1370-4621	1573-773X				OCT	2020	52	2			SI		1675	1701		10.1007/s11063-020-10322-8		AUG 2020											
J								Fixed versus variable time window warehousing strategies in real time	PROGRESS IN ARTIFICIAL INTELLIGENCE										Time window; Fixed time window; Variable time window; Online order batching; Warehousing	ORDER BATCHING PROBLEM; NEIGHBORHOOD SEARCH; MILP FORMULATIONS; MULTIPLE PICKERS; PICKING; ASSIGNMENT	Warehousing includes many different regular activities such as receiving, batching, picking, packaging, and shipping goods. Several authors indicate that the picking operation might consume up to 55% of the total operational costs. In this paper, we deal with a subtask arising within the picking task in a warehouse, when the picking policy follows the order batching strategy (i.e., orders are grouped into batches before being collected) and orders are received online. Particularly, once the batches have been compiled it is necessary to determine the moment in the time when the picker starts collecting each batch. The waiting time of the picker before starting to collect the next available batch is usually known as time window. In this paper, we compare the performance of two different time window strategies: Fixed Time Window and Variable Time Window. Since those strategies cannot be tested in isolation, we have considered: two different batching algorithms (First Come First Served and a Greedy algorithm based on weight); one routing algorithm (S-Shape); and a greedy selection algorithm for choosing the next batch to collect based on the weight.																	2192-6352	2192-6360															10.1007/s13748-020-00215-1		AUG 2020											
J								Development of intuitionistic fuzzy special embedded convolutional neural network for mammography enhancement	COMPUTATIONAL INTELLIGENCE										convolutional neural network; fuzzy set; hesitant score; hyperbolic regularization; intuitionistic fuzzy special set; mammograms	IMAGE; FRAMEWORK	This article proposes a novel mammogram enhancement approach using adaptive intuitionistic fuzzy special set (IFSS) with deep convolutional neural network (called MECNNIFS) for visual interpretation of mammography lesions, lumps, and abnormal cells in low-dose X-ray images. The proposed MECNNIFS scheme utilizes the membership grade modification by IFSS on low-dose X-ray images (mammography). The suggested model attempts to increase the underexposed and abnormal structural regions such as breast lesions, lumps, and nodules on the mammogram. The proposed algorithm initially separates mammograms using convolutional neural networks (CNNs) into foreground and background areas and then fuzzifies the image by intuitionistic fuzzy set theory. Low-level features of a mammogram of the adjacent part are integrated with CNN in pixel classification during the separation task stage to improve the performance. Hyperbolic regularization and hesitant score have been applied on fuzzy plane to quantify the uncertainty and fuzziness in spatial domain for the proposed contrast enhancement. Finally, an enhanced mammogram is acquired through the process of defuzzification. The results show better quality and performance for improvement of contrast and visual quality in mammograms compared with other state-of-the-art methods.																	0824-7935	1467-8640															10.1111/coin.12391		AUG 2020											
J								Extracting road maps from high-resolution satellite imagery using refined DSE-LinkNet	CONNECTION SCIENCE										CNNs; road map extraction; satellite imagery; binary segmentation; multiscale and multilevel features; deep learning		Road detection and extraction have gained momentum in recent past years with crucial applications such as urban planning, autonomous driving, automated map update, providing aid to rescue missions, etc. The current methodologies generate the disconnected road segments, cause boundary loss, and also they are incapable of handling the imbalanced class distribution problems. In this paper, we propose a fully convolutional architecture, named as refined DSE-LinkNet, to extract the connected and precise road maps. We use a pre-trained encoder by combining the layers of the two very efficient and light-weight CNN models: DenseNet and SE-Net that makes the proposed model more expressive with faster convergence. We introduce a new module, Fusion block, in our architecture that enhances its precise localisation as well as classification ability by capturing multilevel as well as multiscale features. To address the imbalanced class distribution problem, a new aggregate loss function is proposed by integrating binary cross-entropy, Jaccard coefficient, and Lovasz sigmoid loss functions. The experiments are performed on a publicly available dataset, DeepGlobe Road Extraction Challenge 2018, to show its efficacy over the D-LinkNet, winner of DeepGlobe Challenge 2018, by achieving IoU of 0.69 with lesser number of parameters and better computational complexity.																	0954-0091	1360-0494															10.1080/09540091.2020.1807466		AUG 2020											
J								Visual interpretation of regression error	EXPERT SYSTEMS										black box model; error; explainability; performance; regression; transparency		Several sophisticated machine learning tools (e.g., ensembles or deep networks) have shown outstanding performance in different regression forecasting tasks. In many real world application domains the numeric predictions of the models drive important and costly decisions. Nevertheless, decision makers frequently require more than a black box model to be able to "trust" the predictions up to the point that they base their decisions on them. In this context, understanding these black boxes has become one of the hot topics in Machine Learning research. This paper proposes a series of visualization tools that explain the relationship between the expected predictive performance of black box regression models and the values of the input variables of any given test case. This type of information thus allows end-users to correctly assess the risks associated with the use of a model, by showing how concrete values of the predictors may affect the performance of the model. Our illustrations with different real world data sets and learning algorithms provide insights on the type of usage and information these tools bring to both the data analyst and the end-user. Furthermore, a thorough evaluation of the proposed tools is performed to showcase the reliability of this approach.																	0266-4720	1468-0394														e12621	10.1111/exsy.12621		AUG 2020											
J								End-to-end analysis modeling of vibrational spectroscopy based on deep learning approach	JOURNAL OF CHEMOMETRICS										deep learning; end-to-end model; preprocessing; residual module	CONVOLUTIONAL NEURAL-NETWORKS; INFRARED-SPECTROSCOPY; MULTIVARIATE-ANALYSIS; RAMAN-SPECTRA	The characteristics of the spectral data are essential for the qualitative analysis of substances. Traditional classification models often need to preprocess the data. However, misuse of preprocessing may change the characteristic information carried by the original data which result in poor model performance. This paper proposes an end-to-end deep learning method that combines residual modules to learn features from raw data to improve model performance, which called ResidualSpectra. ResidualSpectra model is compared with three convolutional neural network (CNN) models on the original data. The 15 preprocessing approaches are used to evaluate the preprocessing impact by testing five open-access mid-infrared, near-infrared, and Raman spectra datasets (fruits, meats, olive_oils, Tablets_Nir, Tablets_Raman). In most cases, the ResidualSpectra model performs better than the other three CNN models on five datasets and obtains better results in original data than in preprocessed data. The model is compared with linear discriminant analysis (LDA), nonlinear artificial neural network (ANN), support vector machines (SVM) for original and preprocessed data. The results show that the ResidualSpectra method provides improved results over traditional classification methods in most scenarios.																	0886-9383	1099-128X														e3291	10.1002/cem.3291		AUG 2020											
J								Global asymptotic stability for discrete-time Cohen-Grossberg neural networks with delays by combining graph theoretic approach with Homeomorphism concept	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Discrete-time Cohen-Grossberg Neural Networks; global asymptotic stability; graph theoretic approach; homeomorphism concept	EXPONENTIAL STABILITY; PERIODIC-SOLUTIONS; EXISTENCE; DYNAMICS; MODELS; FINITE	In this paper, global asymptotic stability for a class of discrete-time Cohen-Grossberg Neural Networks with finite and infinite delays is investigated. By combining graph theoretic approach with Homeomorphism concept as well as Lyapunov functional method, two new sufficient conditions ensuring the global asymptotic stability of equilibrium point for above neural networks are established. Combining graph theoretic approach with Homeomorphism concept studies the equilibrium point of neural networks is a novel approach.																	0952-813X	1362-3079															10.1080/0952813X.2020.1801854		AUG 2020											
J								Feature selection schema based on game theory and biology migration algorithm for regression problems	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Feature selection; Nash equilibrium; Multi-objective optimization; Biology migration algorithm; Game theory	ARTIFICIAL BEE COLONY; FEATURE-EXTRACTION; OPTIMIZATION; CLASSIFICATION; PREDICTION; MANAGEMENT; SYSTEM	Many real-world datasets nowadays are of regression type, while only a few dimensionality reduction methods have been developed for regression problems. On the other hand, most existing regression methods are based on the computation of the covariance matrix, rendering them inefficient in the reduction process. Therefore, a BMA-based multi-objective feature selection method, GBMA, is introduced by incorporating the Nash equilibrium approach. GBMA is intended to maximize model accuracy and minimize the number of features through a less complex procedure. The proposed method is composed of four steps. The first step involves defining three players, each of which is trying to improve its objective function (i.e., model error, number of features, and precision adjustment). The second step includes clustering features based on the correlation therebetween and detecting the most appropriate ordering of features to enhance cluster efficiency. The third step comprises extracting a new feature from each cluster based on various weighting methods (i.e., moderate, strict, and hybrid). Finally, the fourth step encompasses updating players based on stochastic search operators. The proposed GBMA strategy explores the search space and finds optimal solutions in an acceptable amount of time without examining every possible solution. The experimental results and statistical tests based on ten well-known datasets from the UCI repository proved the high performance of GBMA in selecting features for solving regression problems.																	1868-8071	1868-808X															10.1007/s13042-020-01174-8		AUG 2020											
J								Do robots dream of escaping? Narrativity and ethics in Alex Garland'sEx-Machinaand Luke Scott'sMorgan	AI & SOCIETY										Posthuman focalization; A-focalization; X-focalization; Posthuman ethics; Ex-Machina; Morgan	ARTIFICIAL-INTELLIGENCE; CONSCIOUSNESS	Ex-Machina(2014) andMorgan(2016), two recent science-fiction films that deal with the creation of humanoids, also explored the relationship between artificial intelligence (AI), spatiality and the lingering question mark regarding artificial consciousness. In both narratives, the creators of the humanoids have tried to mimic human consciousness as closely as possible, which has resulted in the imprisonment of the humanoids due to proprietary concerns inEx-Machinaand due to the violent behavior of the humanoid inMorgan. This article addresses the dilemma of whether or not the humanoids in both films possess high levels of artificial consciousness and its possible consequences regarding focalization, a narrative term that presupposes subjectivity, as well as offer two new categories of posthuman focalization-X-focalizationandA-focalization. The issue of captivity also has far-reaching ethical implications when considering the underlying assumption of artificial consciousness-if humanoids are indeed endowed with a subjective inner life, then they are entitled to be treated as moral agents, equivalent to humans rather than animals.																	0951-5666	1435-5655															10.1007/s00146-020-01031-w		AUG 2020											
J								Role of ruler or intruder? Patient's right to autonomy in the age of innovation and technologies	AI & SOCIETY										Digital medicine; Innovations in healthcare; Right to autonomy; Informed consent; Advance will; Advance directives	ADVANCE DIRECTIVES; ULYSSES CONTRACTS; DECISION-MAKING; HEALTH-CARE; CONSENT; SELF; INFORMATION	Rapid advancement of technologies continues to revolutionize healthcare foundations and outlook. Technological progress in medicine are not only continuing to improve quality of individual life but also generally improving quality of healthcare services. As a matter of fact, the most significant change in healthcare systems was the shift from standardized, patronizing and rigid physician-patient relationship to more patient-focused, personalized and participatory practice. With this shift came increased attention to the assurance of patient's right to autonomy. Therefore, this article aims to discuss principal problematic aspects of patient's right to autonomy hereby the patient's role in the context of technologies and innovation. It is argued, that one of the effective ways to ensure patient's right to autonomy is implementation of legal instruments, such as informed consent, advance directives and Ulysses contracts. However, this article also proposes, that with a potential of new technologies and artificial intelligence, these legal instruments need to be reconsidered and transformed in more efficient and eligible model, presenting information in more individualized, appealing and convenient manner.																	0951-5666	1435-5655															10.1007/s00146-020-01034-7		AUG 2020											
J								Fruit fly optimization algorithm based on a novel fluctuation model and its application in band selection for hyperspectral image	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Fruit fly optimization algorithm; Subsection strategy; Fluctuation model; Band selection	PARTICLE SWARM OPTIMIZATION; FEATURE-EXTRACTION; SPARSE	Spectral band selection is an important operation in the field of hyperspectral remote sensing. However, most of the techniques cannot satisfy the needs of efficiency and accuracy at the same time. In this paper, we present a novel spectral band selection method, fruit fly optimization algorithm (FOA). As yet, FOA has not been used to solve the problem of band selection in hyperspectral image. Through the study of the algorithm, we know that the advantages of FOA are its simple structure and fewer parameters to be adjusted, but the algorithm itself also has some drawbacks. Thus, we first analyze the shortcomings of the traditional FOA, and the corresponding proofs are given by mathematical method. Then, we separate the whole optimization process into two sub-processes, each of which plays a different role. According to the change of the current iteration information and historical optimum value, a fluctuation model is designed in sub-pro1, and its validity is analyzed and validated theoretically and experimentally. In sub-pro2, a control factor is defined to guide the change rate of the step size. These two sub-processes have their own emphasis, and they cooperate with each other, taking into account the global and local optimization capabilities of the algorithm. The test results on 26 benchmark functions also prove that the proposed algorithm is superior to various state-of-art comparison algorithms. Finally, we introduce the proposed algorithm into the band selection of hyperspectral remote sensing, the gratifying results indicate that the proposed algorithm has great potential in hyperspectral remote sensing field.																	1868-5137	1868-5145															10.1007/s12652-020-02226-1		AUG 2020											
J								A new algorithm based CSP framework for RFID network planning	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Radio frequency identification; Network planning; Constraint satisfaction problem	LOCALIZATION; TECHNOLOGY	The huge growth of industrial society requires the deployment of radio frequency identification networks on a large scale. This necessitates the installation of a large number of radio frequency identification components (readers, tags, middleware and others). As a consequence, the cost and complexity of networks are increasing due to the large number of readers to be installed. Finding the optimal number, placement and parameters of readers to provide a high quality of service for radio frequency identification systems is a critical problem. A good planning affords a basic need for radio frequency identification networks, such as coverage, load balance and interference between readers. This problem is famous in the literature as a radio frequency identification network planning problem. All the proposed approaches in the literature have been based on meta-heuristics. In this paper, we design a new algorithm, called the RNP-CSP algorithm based on the constraint satisfaction problem framework to solve the radio frequency identification network planning problem. The performance evaluation shows that the RNP-CSP algorithm is more efficient than (PSO)-O-2, GPSO and VNPSO-RNP.																	1868-5137	1868-5145															10.1007/s12652-020-02446-5		AUG 2020											
J								PDE Evolutions for M-Smoothers in One, Two, and Three Dimensions	JOURNAL OF MATHEMATICAL IMAGING AND VISION										M-smoother; Partial differential equation; Mode filter; Mean curvature motion; Shock filter; Backward parabolic operator; Anisotropy; Finite difference method; Operator splitting; Shape analysis	DIFFERENTIAL-EQUATIONS; IMAGE-ENHANCEMENT; ROBUST ESTIMATION; EDGE-DETECTION; SCALE-SPACE	Local M-smoothers are interesting and important signal and image processing techniques with many connections to other methods. In our paper, we derive a family of partial differential equations (PDEs) that result in one, two, and three dimensions as limiting processes from M-smoothers which are based on local order-pmeans within a ball the radius of which tends to zero. The orderpmay take any nonzero value>-1 , allowing also negative values. In contrast to results from the literature, we show in the space-continuous case that mode filtering does not arise forp -> 0 , but forp ->-1 . Extending our filter class top-values smaller than-1 allows to include, e.g. the classical image sharpening flow of Gabor. The PDEs we derive in 1D, 2D, and 3D show large structural similarities. Since our PDE class is highly anisotropic and may contain backward parabolic operators, designing adequate numerical methods is difficult. We present an L-infinity-stable explicit finite difference scheme that satisfies a discrete maximum-minimum principle, offers excellent rotation invariance, and employs a splitting into four fractional steps to allow larger time step sizes. Although it approximates parabolic PDEs, it consequently benefits from stabilisation concepts from the numerics of hyperbolic PDEs. Our 2D experiments show that the PDEs forp<1 pare of specific interest: Their backward parabolic term creates favourable sharpening properties, while they appear to maintain the strong shape simplification properties of mean curvature motion.																	0924-9907	1573-7683															10.1007/s10851-020-00986-1		AUG 2020											
J								Deceptive Appearances: the Turing Test, Response-Dependence, and Intelligence as an Emotional Concept	MINDS AND MACHINES										Artificial intelligence (AI); Imitation game; Machine intelligence; Response-dependence; The Turing Test		The Turing Test is routinely understood as a behaviourist test for machine intelligence. Diane Proudfoot (Rethinking Turing's Test,Journal of Philosophy, 2013) has argued for an alternative interpretation. According to Proudfoot, Turing's claim that intelligence is what he calls 'an emotional concept' indicates that he conceived of intelligence in response-dependence terms. As she puts it: 'Turing's criterion for "thinking" is horizontal ellipsis : x is intelligent (or thinks) if in the actual world, in an unrestricted computer-imitates-human game, x appears intelligent to an average interrogator'. The role of the famous test is thus to provide the conditions in which to examine the average interrogator's responses. I shall argue that Proudfoot's analysis falls short. The philosophical literature contains two main models of response-dependence, what I shall call thetransparencymodel and thereference-fixingmodel. Proudfoot resists the thought that Turing might have endorsed one of these models to the exclusion of the other. But the details of her own analysis indicate that she is,in fact, committed to the claim that Turing's account of intelligence is grounded in a transparency model, rather than a reference-fixing one. By contrast, I shall argue that while Turing did indeed conceive of intelligence in response-dependence terms, his account is grounded in a reference-fixing model, rather than a transparency one. This is fortunate (for Turing), because, as an account of intelligence, the transparency model is arguably problematic in a way that the reference-fixing model isn't.																	0924-6495	1572-8641															10.1007/s11023-020-09533-8		AUG 2020											
J								A new pose accuracy compensation method for parallel manipulators based on hybrid artificial neural network	NEURAL COMPUTING & APPLICATIONS										Parallel manipulator; Pose accuracy; Accuracy compensation; Kinematic calibration	KINEMATIC CALIBRATION; MODEL	The pose accuracy of parallel manipulators is one of the most important performance indices in advanced industrial applications. The modeling and estimation of geometrical parameter errors are no longer an issue because a large number of kinematic calibration techniques have been used to compensate pose accuracy of parallel manipulators in recent years. The modeling and identification of non-geometrical parameter errors are an extremely complicated technical procedure. A hybrid artificial neural network which involves a BP neural network, a RBF neural network and a control module is presented and used for compensating pose error caused by non-geometrical parameter errors of parallel manipulators. The control module serves as a linear mapping network which combines the outputs of the BP neural network and the RBF neural network to obtain the final pose accuracy compensation results. The pose accuracy compensation methods of the hybrid artificial neural network are established and discussed. Simulations and experiments were performed on a parallel manipulator. The feasibility and validity of the proposed pose accuracy compensation method based on the hybrid artificial neural network are verified through pose accuracy improvement and enhancement.																	0941-0643	1433-3058															10.1007/s00521-020-05288-6		AUG 2020											
J								Basic algebras and L-algebras	SOFT COMPUTING										Basic algebras; L-algebras; MV-algebras; Orthomodular lattices; Effect algebras		In this paper, we study the relation between L-algebras and basic algebras. In particular, we construct a lattice-ordered effect algebra which improves an example of Chajda et al. (Algebra Univ 60(1), 63-90, 2009).																	1432-7643	1433-7479				OCT	2020	24	19					14327	14332		10.1007/s00500-020-05231-w		AUG 2020											
J								Design and performance analysis of adaptive neuro-fuzzy controller for speed control of permanent magnet synchronous motor drive	SOFT COMPUTING										Adaptive neuro-fuzzy; PID control; Sliding mode control; Robust control; Switching algorithm	SLIDING-MODE CONTROL; DISTURBANCE COMPENSATION; PREDICTIVE CONTROL; SYSTEMS	This article has been focused on the design of the artificial neural network with fuzzy inference system (ANFIS) for the speed control of permanent magnet synchronous motor (PMSM). PMSM is widely used in industrial applications such as robotic manipulators and machine tools due to the high efficiency, high torque to weight ratio and smaller size. One of the efficient control strategies of PMSM is based on ANFIS. ANFIS is very popular technique to deal with uncertainties. System dynamics in such cases can be compared with combining the proportional-integral-derivative (PID) with the Sliding Mode Controller (SMC). Simulations have been performed in MATLAB to validate the performance of the proposed model, and comparisons are made with ANFIS, SMC-PID and PID controllers compared to other controllers reported in the benchmark of the proposed controller's efficiency. The proposed adaptive neuro-fuzzy-dependent results indicate good transient efficiency. Robustness against the robustness of adaptive neuro-fuzzy-based PID and SMC-PID controllers is satisfactory in terms of easy settling time, zero peaks overflow and zero steady state error. The simulation results have been implemented in MATLAB 2019b, and experimental results are implemented in BD63030.																	1432-7643	1433-7479															10.1007/s00500-020-05236-5		AUG 2020											
J								Handwriting Biometrics: Applications and Future Trends in e-Security and e-Health	COGNITIVE COMPUTATION										Online handwriting; Biometrics; e-Security; e-Health; Privacy	RAPID HUMAN MOVEMENTS; KINEMATIC THEORY; SIGNATURE VERIFICATION; WRITER IDENTIFICATION; RECOGNITION; CHILDREN; ONLINE; PEN; SEGMENTATION; PROFICIENT	Online handwritten analysis presents many applications in e-security, signature biometrics being the most popular but not the only one. Handwriting analysis also has an important set of applications in e-health. Both kinds of applications (e-security and e-health) have some unsolved questions and relations among them that should be addressed in the next years. We summarize the state of the art and applications based on handwriting signals. Later on, we focus on the main achievements and challenges that should be addressed by the scientific community, providing a guide for future research. Among all the points discussed in this article, we remark the importance of considering security, health, and metadata from a joint perspective. This is especially critical due to the risks inherent when using these behavioral signals.																	1866-9956	1866-9964				SEP	2020	12	5					940	953		10.1007/s12559-020-09755-z		AUG 2020											
J								A Camera Model for Line-Scan Cameras with Telecentric Lenses	INTERNATIONAL JOURNAL OF COMPUTER VISION										Line-scan cameras; Telecentric lenses; Camera models; Camera model degeneracies; Camera calibration	CALIBRATION; GEOMETRY	We propose a camera model for line-scan cameras with telecentric lenses. The camera model assumes a linear relative motion with constant velocity between the camera and the object. It allows to model lens distortions, while supporting arbitrary positions of the line sensor with respect to the optical axis. We comprehensively examine the degeneracies of the camera model and propose methods to handle them. Furthermore, we examine the relation of the proposed camera model to affine cameras. In addition, we propose an algorithm to calibrate telecentric line-scan cameras using a planar calibration object. We perform an extensive evaluation of the proposed camera model that establishes the validity and accuracy of the proposed model. We also show that even for lenses with very small lens distortions, the distortions are statistically highly significant. Therefore, they cannot be omitted in real-world applications.																	0920-5691	1573-1405															10.1007/s11263-020-01358-3		AUG 2020											
J								Application of response surface methodology for optimization of ultrasound-assisted solid-liquid extraction of phenolic compounds fromCenostigma macrophyllum	JOURNAL OF CHEMOMETRICS										Box-Behnken design; Cenostigma macrophyllum; phenolic compounds; response surface methodology; ultrasound-assisted solid-liquid extraction	SOLVENT-EXTRACTION; MACERATION; LEAVES	This study determines the optimal conditions for the ultrasound-assisted solid-liquid extraction (UA-SLE) of the total phenolic compounds (TPC) ofCenostigma macrophyllumleaves. The experiments were carried out according to three-level, four-variable Box-Behnken design combined with the response surface methodology (RSM). Extraction (sonication) time, liquid-solid ratio, methanol concentration, and ultrasonic power were investigated as independent variables to obtain the optimal extraction conditions for TPC. Data were fitted to a second-order polynomial model, and the model fitness was evaluated by the analysis of variance, which indicated the model (P< 0.0001), presenting precision and reliability with the experiments performed (R-2= 0.9947, coefficient of variance [CV] = 4.29, SD = 1.67). Individual and interactive effects of the independent variables on TPC extraction were interpreted using the proposed mathematical model. The results showed that for the maximum TPC extraction, the optimal conditions included 32 min extraction time, 29.73 mL center dot g(-1)liquid-solid ratio, 47.72% methanol concentration, and 145.71 W ultrasonic power. Under these optimal conditions, the predicted values were in agreement with the experimental values, thus validating the RSM model. When the optimal conditions were compared with those of conventional extraction techniques, the results showed that UA-SLE presented significantly better results for TPC extraction (71.2% to 138.3% more efficient) and for antioxidant activity (115.0% to 179.2% more efficient). High-performance liquid chromatography analysis of the extracts revealed that UA-SLE did not interfere with the extract chemical composition, thus confirming that it was more effective than conventional techniques.																	0886-9383	1099-128X														e3290	10.1002/cem.3290		AUG 2020											
J								Mitigating congestion in wireless sensor networks through clustering and queue assistance: a survey	JOURNAL OF INTELLIGENT MANUFACTURING										Wireless sensor network; Congestion control; Quality of service; Cross layer network; Traffic control; Network resource; Clustering techniques; Queue management	ROUTING ALGORITHMS; CONTROL SCHEME; MANAGEMENT; PROTOCOL; COMMUNICATION; LIGHTWEIGHT; IMPROVEMENT; AVOIDANCE; DESIGN; MODEL	A network of randomly deployed sensor nodes which shares limited resources like bandwidth, buffer, queue, and battery powered nodes is known as wireless sensor network. Such network must have energy, to avoid the chances of congestion because congested network degrades the performance of network. Congestion may occur due to several reasons like data packet collision, transmission channel contention and buffer overflow. A congestion control protocol must acquire the functionalities that can increase the lifetime and efficiency of network which are major responsibilities of wireless sensor network. In this paper traffic oriented, resource oriented and a hybrid approach with some additional functionalities of controlling congestion are discussed in a wide manner. The hybrid approach is best as per this survey as it integrates various factors of wireless sensor networks to control and mitigate the situation. A comprehensive analysis is also done on these factors to justify the nature of different approaches.																	0956-5515	1572-8145															10.1007/s10845-020-01640-8		AUG 2020											
J								Evaluation of 3D LiDAR Sensor Setup for Heterogeneous Robot Team	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										LiDAR configurations for multi agent robot systems; Heterogeneous robot team; LiDAR sensor setup; Collaborative mapping; Localization for multi-robots; 3D laser sensor positioning; Map merging from different sources; Performance metrics for point clouds; Occupancy metrics for point clouds; ICP; OctoMap; ROS	UAV-LIDAR	We encounter a world where the role of robots has increased in society. Day by day, we can observe that problems related to autonomous vehicles are being solved. The focused problem that we have recently confronted is based on collaborative mapping with multi-robots since collaboratively generated map provides more information about the environment than with a single robot. Moreover, the mapping framework for robot team can be more efficient if 3D laser sensors are placed to cover wider area. However, laser sensors are commonly positioned intuitively and depended mostly on the designer's choice. To overcome designer-independent laser sensor setup for efficient collaborative mapping, a subjective evaluation of laser sensor placement is developed as another key aspect. This paper proposed a collaborative mapping framework for heterogeneous robot teams to make full use of collected information from different viewpoints and explore large areas more efficiently. It's comprised of two modules, localization of multi-robots and collaborative map construction. In addition, to avoid arbitrary setup of laser sensors in existing methods whose performance is heavily affected by designers' experience, a metric evaluation system was defined to determine the optimal placements of laser sensors and achieve maximum sweep area. The proposed methods were verified on both simulation and experiments which were carried out on a heterogeneous robot team (including an unmanned aerial vehicle and a ground vehicle). According to the metric evaluation, best placements were chosen for the laser sensors, and presented collaborative mapping technique was shown to work more efficiently compared with existing methods.																	0921-0296	1573-0409				NOV	2020	100	2					689	709		10.1007/s10846-020-01207-y		AUG 2020											
J								Adaptive multi-branch correlation filters for robust visual tracking	NEURAL COMPUTING & APPLICATIONS										Visual tracking; Correlation filter; Multi-branch; Appearance changes; Background suppression	OBJECT TRACKING	In recent years, deep convolutional features have been applied to discriminative correlation filters-based methods, which have achieved impressive performance in tracking. Most of them utilize hierarchical features from a certain layer. However, this is not always sufficient to learn target appearance changes and to suppress the background interference in complicated interfering factors (e.g., deformation, fast motion, low resolution, and rotations). In this paper, we propose an adaptive multi-branch correlation filter tracking method, by constructing multi-branch models and using an adaptive selection strategy to improve the accuracy and robustness of visual tracking. Specially, the multi-branch models are introduced to tolerate temporal changes of the object, which can serve different circumstances. In addition, the adaptive selection strategy incorporates both foreground and background information to learn background suppression. To further improve the tracking performance, we propose a measurement method to handle tracking failures from unreliable samples. Extensive experiments on OTB-2013, OTB-2015, and VOT-2016 datasets show that the proposed tracker has comparable performance compared to state-of-the-art tracking methods. Especially, on the OTB-2015, our method significantly improves the baseline with a gain of 5.5% in overlap precision.																	0941-0643	1433-3058															10.1007/s00521-020-05126-9		AUG 2020											
J								An efficient stacking model with label selection for multi-label classification	APPLIED INTELLIGENCE										Multi-label classification; Label correlation; Label specific features	NEURAL-NETWORKS; DECISION TREES; LARGE-SCALE; PREDICTION	Binary relevance (BR) is one of the most popular frameworks in multi-label learning. It constructs a group of binary classifiers, one for each label. BR is a simple and intuitive way to deal with multi-label problem, but fails to utilize label correlations. To deal with this problem, dependent binary relevance (DBR) and other works employ stacking learning paradigm for BR, in which all labels are viewed as additional features. Those works may be suboptimal as each label has its own most related label subset. In this paper, a novel two-layer stacking based approach, which is named a Stacking Model with Label Selection (SMLS), is induced to exploit proper label correlations for improving the performance of DBR. At the first layer, we construct several binary classifiers in the way of BR. At the second layer, we find the specific label subset through label selection for each labels , and expand them into feature space. The final binary classifiers are constructed based on their corresponding augmented feature space. Comprehensive experiments are conducted on a collection of benchmark data sets. Comparison results with the state-of-the-art approaches validate the competitive performance of our proposed approach. Comparison results with DBR shows that our approach is not only more time efficient but also more robust.																	0924-669X	1573-7497															10.1007/s10489-020-01807-z		AUG 2020											
J								JDF-DE: a differential evolution with Jrand number decreasing mechanism and feedback guide technique for global numerical optimization	APPLIED INTELLIGENCE										Differential evolution; Crossover strategy; Jrand number decreasing mechanism; Feedback guide technique; Global numerical optimization problems	ALGORITHM; MUTATION; PARAMETERS; CROSSOVER	Differential Evolution (DE) is a readily comprehensible and highly powerful intelligent optimized method for numerical optimization. The performance of DE significantly depends on its parameters and strategies generating both mutation vector and trial vector. To further enhance its exhibition, we propose a new DE variant called JDF-DE based on JADE by introducing the improved parameter approach with weight and crossover strategy with Jrand number decreasing mechanism and feedback guide technique. The new way for updating parameter mu(CR)and mu(F)brings fitness value to generate more reasonable parameters with the fixed orientation during evolution. Meanwhile, Levy distribution is used to complete the adaptive distribution of CR when the population has a high clustering intensity so that solutions escape from the local optimal value. Jrand number decreasing mechanism is embedded to crossover operation to strengthen population diversity instead the number of Jrand equals 1 in primary DE algorithm. Feedback guide method is utilized to determine the step size for Jrand number to advance the ability that JDF-DE searches the optimum value. In order to investigate performance of JDF-DE, In order to analyze performance of JDF-DE, 29 benchmark functions from CEC2017 on real parameter optimization are employed to verify the validity of JDF-DE for solving complex high-dimensional problems. The experimental results show that JDF-DE is better than, or at least comparable with several state-of-the-art DE variants including DE variants JADE, SinDE, TSDE, AGDE, and EFADE and non-DE variants TSA, SHO, GWO, MVO, SCA, and GSA in the global numerical optimization problems.																	0924-669X	1573-7497															10.1007/s10489-020-01795-0		AUG 2020											
J								Detection of COVID-19 using CXR and CT images using Transfer Learning and Haralick features	APPLIED INTELLIGENCE										Chest X-Ray; Computed tomography; COVID-19; Haralick feature; Transfer learning; Viral pneumonia		Recognition of COVID-19 is a challenging task which consistently requires taking a gander at clinical images of patients. In this paper, the transfer learning technique has been applied to clinical images of different types of pulmonary diseases, including COVID-19. It is found that COVID-19 is very much similar to pneumonia lung disease. Further findings are made to identify the type of pneumonia similar to COVID-19. Transfer Learning makes it possible for us to find out that viral pneumonia is same as COVID-19. This shows the knowledge gained by model trained for detecting viral pneumonia can be transferred for identifying COVID-19. Transfer Learning shows significant difference in results when compared with the outcome from conventional classifications. It is obvious that we need not create separate model for classifying COVID-19 as done by conventional classifications. This makes the herculean work easier by using existing model for determining COVID-19. Second, it is difficult to detect the abnormal features from images due to the noise impedance from lesions and tissues. For this reason, texture feature extraction is accomplished using Haralick features which focus only on the area of interest to detect COVID-19 using statistical analyses. Hence, there is a need to propose a model to predict the COVID-19 cases at the earliest possible to control the spread of disease. We propose a transfer learning model to quicken the prediction process and assist the medical professionals. The proposed model outperforms the other existing models. This makes the time-consuming process easier and faster for radiologists and this reduces the spread of virus and save lives.																	0924-669X	1573-7497															10.1007/s10489-020-01831-z		AUG 2020											
J								Feature selection for multi-label classification by maximizing full-dimensional conditional mutual information	APPLIED INTELLIGENCE										Feature selection; Classification; Conditional mutual information; Relevance maximization; Redundance minimization; Hilbert-Schmidt independence criterion (HSIC)	ALGORITHMS; DEPENDENCE	Conditional mutual information (CMI) maximization is a promising criterion for feature selection in a computationally efficient stepwise way, but it is hard to be applied comprehensively because of imprecise probability calculation and heavy computational load. Many dimension-reduced CMI-based and mutual information (MI)-based methods have been reported to achieve state-of-art performances in terms of classification. However, model deviations are introduced into the CMI and MI formulations in these methods during dimension reduction. In this paper, we start with the full-dimensional CMI to deal with the feature selection problem, so as to retain full inter-feature and feature-label mutual information when selecting new features. The cost function is approximated and simplified from a mathematical perspective to overcome the difficulties for maximizing the original full-dimensional CMI. A relationship is established between the proposed feature selection criterion and the one based on Hilbert-Schmidt independence, which explains qualitatively how the new criterion succeeds to achieve relevance maximization and redundance minimization simultaneously. Experiments on real-world datasets demonstrate the predominance of the proposed method over the existing ones.																	0924-669X	1573-7497															10.1007/s10489-020-01822-0		AUG 2020											
J								Multiple delay-dependent noise-to-state stability for a class of uncertain switched random nonlinear systems with intermittent sensor and actuator faults	APPLIED INTELLIGENCE										Random systems; Switched systems; Intermittent faults (IFs); Noise-to-state stability (NSS); Multiple delay-dependent stability	TOLERANT CONTROL; TRACKING CONTROL; LINEAR-SYSTEMS; TIME-DELAY; STABILIZATION; DESIGN	This paper focuses on multiple delay-dependent noise-to-state stability (NSS) for a class of switched random nonlinear systems against uncertainty terms and intermittent sensor and actuator faults. External disturbances, nonlinear functions, as well as measurement noise, are also taken into account. This is the first attempt to achieve dynamic output feedback controller design for uncertain switched random nonlinear systems subject to intermittent sensor and actuator faults. First, a controller is established to perform passive fault-tolerant control (FTC). Random systems are more common than Ito stochastic systems. Thus, compared with the previous works, the proposed controller has a wider application scope and is more feasible. Next, an augmented closed-loop system is exhibited to realize NSS. Moreover, a piecewise Lyapunov function is utilized with less conservatism than common Lyapunov function. The delay dependent stability conditions are gathered via linear matrix inequalities (LMIs) and controller matrices are earned. At last, the novelty and validity of the approach suggested in this paper are demonstrated through two simulation examples.																	0924-669X	1573-7497															10.1007/s10489-020-01753-w		AUG 2020											
J								Decision-making method based on new entropy and refined single-valued neutrosophic sets and its application in typhoon disaster assessment	APPLIED INTELLIGENCE										Multi-attribute decision-making; Refined single-valued neutrosophic sets (RSVNSs); Evaluation based on distance from average solution (EDAS) method; Neutrosophic entropy; Typhoon disaster assessment	INTUITIONISTIC FUZZY-SETS; SIMILARITY MEASURE; AGGREGATION OPERATORS; EDAS METHOD; NUMBERS; TOPSIS; INFORMATION; ALGORITHMS; SELECTION; RANKING	This study proposes a multi-attribute decision-making method for the decision-making problems with attributes and sub-attribute where the attribute weight is unknown, based on information entropy and the evaluation based on distance from average solution (EDAS) method under a refined single-valued neutrosophic set environment. First, the new distance measure, similarity measure, and neutrosophic entropy based on refined single-valued neutrosophic sets are defined. Further, the relationship between them is discussed and the attribute weights are determined based on the new neutrosophic entropy. Then, the EDAS method is used to rank and select the best alternative. Finally, two illustrative examples of typhoon disaster assessment (typhoon disaster assessment with multi-layer indicators and dynamic assessment of typhoon disaster) are presented to demonstrate the feasibility, effectiveness, and practicality of the proposed method. The advantages of the proposed method are illustrated by sensitive analysis and comparative analysis with other methods.																	0924-669X	1573-7497															10.1007/s10489-020-01706-3		AUG 2020											
J								A robust watermarking approach for security issue of binary documents using fully convolutional networks	INTERNATIONAL JOURNAL ON DOCUMENT ANALYSIS AND RECOGNITION										Document security; Watermarking; Watermarking regions; Hiding patterns; Document analysis	DATA HIDING SCHEME; IMAGE AUTHENTICATION	Motivated by increasing possibility of the tampering of genuine documents during a transmission over digital channels, we focus on developing a watermarking framework for determining whether a received document is genuine or falsified, which is performed by hiding a security feature or secret information within it. To begin with, the input document is transformed into a standard form to minimize geometric distortion. Fully convolutional network (FCN) is utilized to detect document's watermarking regions. Next, we construct hiding patterns used for hiding secret information. Modifying pixel values of these patterns for carrying secret bits depends on the edge and corner features of document content and the connectivity of their neighboring pixels. Lastly, the watermarking process is conducted by either changing the center pixel of the hiding patterns or changing the ratio between the number of edge features and the number of corner features of subregions within the watermarking regions. The experiments are performed on various binary documents, and our approach gives competitive performance compared to state-of-the-art approaches.																	1433-2833	1433-2825				SEP	2020	23	3					219	239		10.1007/s10032-020-00355-z		AUG 2020											
J								Combining Embeddings of Input Data for Text Classification	NEURAL PROCESSING LETTERS										Text classification; Multi-input network; Agglutinative language; Inflected language; Embedding combination	PERCEPTUAL SIMILARITY; NORTH GERMAN	The problem of automatic text classification is an essential part of text analysis. The improvement of text classification can be done at different levels such as a preprocessing step, network implementation, etc. In this paper, we focus on how the combination of different methods of text encoding may affect classification accuracy. To do this, we implemented a multi-input neural network that is able to encode input text using several text encoding techniques such as BERT, neural embedding layer, GloVe, skip-thoughts and ParagraphVector. The text can be represented at different levels of tokenised input text such as the sentence level, word level, byte pair encoding level and character level. Experiments were conducted on seven datasets from different language families: English, German, Swedish and Czech. Some of those languages contain agglutinations and grammatical cases. Two out of seven datasets originated from real commercial scenarios: (1) classifying ingredients into their corresponding classes by means of a corpus provided byNorthfork; and (2) classifying texts according to the English level of their corresponding writers by means of a corpus provided byProvenWord. The developed architecture achieves an improvement with different combinations of text encoding techniques depending on the different characteristics of the datasets. Once the best combination of embeddings at different levels was determined, different architectures of multi-input neural networks were compared. The results obtained with the best embedding combination and best neural network architecture were compared with state-of-the-art approaches. The results obtained with the dataset used in the experiments were better than the state-of-the-art baselines.																	1370-4621	1573-773X															10.1007/s11063-020-10312-w		AUG 2020											
J								Two-dimensional Subclass Discriminant Analysis for face recognition	PATTERN ANALYSIS AND APPLICATIONS										Feature extraction; Discriminant analysis; Pattern recognition; Classification; Eigenvalue decomposition; Subclass discriminant analysis	PRINCIPAL COMPONENT ANALYSIS; REPRESENTATION; EIGENFACES; 2D-LDA; PCA	Dimensionality reduction plays a major role in face recognition. Discriminant analysis (DA) and principal component analysis (PCA) are two of the most important approaches in this field. In particular, subclass discriminant analysis (SDA) is a well-known scheme for feature extraction and dimensionality reduction. It is widely used in many high-dimensional data-driven applications, namely face recognition and image retrieval. It is also found to be applicable under various scenarios. However, it has high cost in time and space given the need for an eigendecomposition involving the scatter matrices, known as the singularity problem. This limitation is caused by the high-dimensional space of data, particularly when dimensions exceed the number of observations. Recent advances widely reported that 2D methods with matrix-based representation perform better than the traditional 1D vector-based ones. In this paper, we propose a novel 2D-SDA algorithm to avoid the "curse of dimensionality" and address the singularity issue. The performance of the proposed algorithm is evaluated for face recognition in terms of recognition performance and computational cost. Experiments are conducted on four benchmark face databases and compared to several competitive 1D and 2D methods based on PCA and DA. Results show that 2DSVD achieves the best recognition performance at low dimensions. In particular, 2D-SDA works significantly better on large-sized data sets where intra-class variation is the most important.																	1433-7541	1433-755X															10.1007/s10044-020-00905-5		AUG 2020											
J								The uncertain two-stage network DEA models	SOFT COMPUTING										Two-stage network system; Data envelopment analysis; Uncertainty theory; Efficiency	DATA ENVELOPMENT ANALYSIS; EFFICIENCY MEASUREMENT	The two-stage network DEA models based on the framework that the efficiency of the whole stage is equal to the product of the efficiencies of two sub-stages can not only turn the 'black box' into the 'glass box' to identify the root causes of the inefficiency of the network system, but also consider the relationship between the two sub-stages within the whole stage. Nowadays, the two-stage network DEA models have been widely applied in the field of economy and management, such as green supply chain and reverse supply chain. Due to the novelty of evaluation indexes, these emerging research objects with network structure, such as green supply chain, involve not only traditional evaluation indexes such as cost and time, but also some novel evaluation indexes such as customer satisfaction and flexibility. However, these new evaluation indexes are difficult to quantify accurately, which will lead to the failure of the traditional two-stage network DEA models. Therefore, this paper attempts to extend the traditional two-stage network DEA models to the uncertain two-stage network DEA models with the application of uncertainty theory. In the new models, inputs, intermediates and outputs are considered to be uncertain variables to deal with the problem of inaccurate data. Finally, a numerical example of the uncertain two-stage network DEA models will be presented for illustration.																	1432-7643	1433-7479															10.1007/s00500-020-05157-3		AUG 2020											
J								Entropy measure and TOPSIS method based on correlation coefficient using complex q-rung orthopair fuzzy information and its application to multi-attribute decision making	SOFT COMPUTING										Complex q-rung orthopair fuzzy sets; TOPSIS method; Entropy measure; Correlation coefficient	SIMILARITY MEASURES; MEAN OPERATORS; VAGUE SETS	Entropy measure (EM) and similarity measure (SM) are important techniques in the environment of fuzzy set (FS) theory to resolve the similarity between two objects. The q-rung orthopair FS (q-ROFS) and complex FS are new extensions of FS theory and have been widely used in various fields. In this article, the EM, Technique for Order of Preference by Similarity to Ideal Solution (TOPSIS) based on the correlation coefficient is investigated. It is very important to study the SM of Cq-ROFS. Then, the established approaches and the existing drawbacks are compared by an example, and it is verified that the explored work can distinguish highly similar but inconsistent Cq-ROFS. Finally, to examine the reliability and feasibility of the new approaches, we illustrate an example using the TOPSIS method based on Cq-ROFS to manage a case related to the selection of firewall productions, and then, a situation concerning the security evaluation of computer systems is given to conduct the comparative analysis between the established TOPSIS method based on Cq-ROFS and previous decision-making methods for validating the advantages of the established work by comparing them with the other existing drawbacks.																	1432-7643	1433-7479															10.1007/s00500-020-05218-7		AUG 2020											
J								Performance-enhanced rough k-means clustering algorithm	SOFT COMPUTING										Roughk-means; Cluster initialization; Weight optimization; Customer segmentation; RFM analysis	CUSTOMER SEGMENTATION; MARKET-SEGMENTATION; FRAMEWORK; FUZZY; MODEL; SYSTEM; USERS	Customer segmentation (CS) is the most critical application in the field of customer relationship management that primarily depends on clustering algorithms. Rough k-means (RKM) clustering algorithm is widely adopted in the literature for achieving CS objective. However, the RKM has certain limitations that prevent its successful application to CS. First, it is sensitive to random initial cluster centers. Second, it uses default values for parameters w(l) and w(u) used in calculating cluster centers. To address these limitations, a new initialization method is proposed in this study. The proposed initialization mitigates the problems associated with the random choice of initial cluster centers to achieve stable clustering results. A weight optimization scheme for w(l) and w(u) is proposed in this study. This scheme helps to estimate suitable weights for w(l) and w(u) by counting the number of data points present in clusters. Extensive experiments were carried out by using several benchmark datasets to assess the performance of these proposed methods in comparison with the existing algorithm. The results reveal that the proposed methods have improved the performance of the RKM algorithm, which is validated by the evaluation metrics, namely convergence speed, clustering accuracy, Davies-Bouldin (DB) index, within/total (W/T) clustering error index and statistical significance t test. Further, the results are compared with other promising clustering algorithms to show its advantage. A CS framework that shows the utility of these proposed methods in the application domain is also proposed. Finally, it is demonstrated through a case study in a retail supermarket.																	1432-7643	1433-7479															10.1007/s00500-020-05247-2		AUG 2020											
J								Neighborhood centroid opposite-based learning Harris Hawks optimization for training neural networks	EVOLUTIONARY INTELLIGENCE										Harris hawks optimization; Neighborhood centroid; Opposite-based learning; NCOHHO; Feed-forward neural network; Metaheuristic	PARTICLE SWARM OPTIMIZATION; ALGORITHM; SEARCH	The Harris Hawks Optimization Algorithm is a new metaheuristic optimization that simulates the process of Harris Hawk hunting prey (rabbit) in nature. The global and local search processes of the algorithm are performed by simulating several stages of cooperative behavior during hunting. To enhance the performance of this algorithm, in this paper we propose a neighborhood centroid opposite-based learning Harris Hawks optimization algorithm (NCOHHO). The mechanism of applying the neighborhood centroid under the premise of using opposite-based learning technology to improve the performance of the algorithm, the neighborhood centroid is used as a reference point for the generation of the opposite particle, while maintaining the diversity of the population and make full use of the swarm search experience to expand the search range of the reverse solution. Enhancing the probability of finding the optimal solution and the improved algorithm is superior to the original Harris Hawks Optimization algorithm in all aspects. We apply NCOHHO to the training of feed-forward neural network (FNN). To confirm that using NCOHHO to train FNN is more effective, five classification datasets are applied to benchmark the performance of the proposed method. Comprehensive comparison and analysis from the three aspects of mean, variance and classification success rate, the experimental results show that the proposed NCOHHO algorithm for optimization FNN has the best comprehensive performance and has more outstanding performance than other metaheuristic algorithms in terms of the performance measures.																	1864-5909	1864-5917															10.1007/s12065-020-00465-x		AUG 2020											
J								A New Fuzzy PID Control System Based on Fuzzy PID Controller and Fuzzy Control Process	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Fuzzy PID controller; Generalized Hukuhara differentiability; Fuzzy differential equations; Fuzzy PID control system		In this paper, we present a fuzzy PID control system as a combination of a fuzzy PID controller and a fuzzy control process, which is represented by a fuzzy control differential equation in linear form. We use the concepts of the generalized Hukuhara differentiability and the fuzzy integral of fuzzy-valued functions to study some qualitative properties for this system in the space of fuzzy numbers. We also study the existence and uniqueness result for solutions of fuzzy PID control differential equations under some suitable conditions. A number of examples are also provided to illustrate the results of the theory.																	1562-2479	2199-3211				OCT	2020	22	7					2163	2187		10.1007/s40815-020-00904-y		AUG 2020											
J								A bipartite matching-based feature selection for multi-label learning	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Multi-label learning; Bipartite graph matching; Hungarian algorithm; Weighted correlation distance	GRAVITATIONAL SEARCH ALGORITHM; OPTIMIZATION	Many real-world data have multiple class labels known as multi-label data, where the labels are correlated with each other, and as such, they are not independent. Since these data are usually high-dimensional, and the current multi-label feature selection methods have not been precise enough, then a new feature selection method is necessary. In this paper, for the first time, we have modeled the problem of multi-label feature selection to a bipartite graph matching process. The proposed method constructs a bipartite graph of features (as the left vertices) and labels (as the right vertices), called Feature-Label Graph (FLG), where each feature is connected to the set of labels, where the weight of the edge between each feature and label is equal to their correlation. Then, the Hungarian algorithm estimates the best matching in FLG. The selected features in each matching are sorted by weighted correlation distance and added to the ranking vector. To select the discriminative features, the proposed method considers both the redundancy of features and the relevancy of each feature to the class labels. The results indicate the superiority of the proposed method against the other methods in classification measures.																	1868-8071	1868-808X															10.1007/s13042-020-01180-w		AUG 2020											
J								An ontology enabled internet of things framework in intelligent agriculture for preventing post-harvest losses	COMPLEX & INTELLIGENT SYSTEMS										Hierarchical model; Ontology; IoT; Intelligent agriculture; Post-harvest loss; Attribute extraction; 3D sensor; Smallest S-vertex; Separation; Spatial image	DEVICES	Constituting the agriculture solid substance manufacture, the post-harvest sector processing schema is direct to preventing reduce the losses in intelligent agriculture. Many processing schemata will be preventing post-harvest losses on the agriculture solid substance manufacture, especially sekai-ichi apple is the regularly used fruit also used to make active in human-related activities of the sensory and control function consisting of an agricultural industry. Sekai-ichi apple is being a definite number of diseases induce, but it is to the highest degree of wastage involving in the Post-Harvest process. Especially sekai-ichi apple count loss is an unsafe many time because it not critically post-harvest. Regardless of consideration, the existing hierarchical model specified post-harvest losses prevention research has deficiencies to precise and quick detection of wastage for ensuring healthy separation of agriculture surroundings. This paper suggests a "Hierarchical Model within Ontology Enabled IoT" for distinguishable healthy separation of sekai-ichi apple by using Boosted Continuous Non-spatial whole Attribute Extraction (BCNAE). Sekai-ichi apple count loss is always safe on critically post-harvest. Proposed Post-Harvest hierarchical model specified post-harvest losses prevention and deficiencies to precise and quick detection of wastage for ensuring healthy separation of agriculture surroundings. In these suggestions, the separation cognitive operation takes the three levels of processing schemes such as lower level, middle level, and higher level. Firstly, the lower level is express agreements with the dynamic functioning for maintaining the definite number of manual induces. This lower level showing an absorption with the activity of manual separation by the human reliability determination. Secondly, the middle level is an express arrangement with the dynamic functioning for reducing the overfitting and accommodate to fitting the right shape deliberation. Middle level is establishing being generalized by concentrating the time-varying features in the occurrence of a change for the worse identification. Finally, the upper level is express for features refining with the help of the function of sekai-ichi apple image segmentation connection. This interpretability process helps to make the proven position of a prominent classification in a particular fruit on the agriculture solid substance. These three processing flow constructs the ontology structure with manually collected sekai-ichi apple images from a 3D sensor. The observational consequences express that the proposed BCNAE framework recognizes a detection performance carrying out with an optimized-separation ratio for time-variant of the separation process.																	2199-4536	2198-6053															10.1007/s40747-020-00183-y		AUG 2020											
J								Improving maintenance performance by developing an IFC BIM/RFID-based computer system	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Facilities management; IFC BIM; RFID; Information management; Maintenance performance	FACILITY MANAGEMENT; BIM; IMPLEMENTATION; DOCUMENTATION; AREAS; IOT	Building maintenance includes actions that aim to ensure the components of a building are in acceptable conditions. In traditional maintenance methods, as-built drawings and maintenance manuals are regarded as essential documents for the operation and maintenance of buildings. In this method, the time and cost of facilities management are increased due to managing a large number of documents. Thus, the necessity of creating a range of fields from the information generation to the development of automation systems is evident in the maintenance services from the technological perspective. This research presents a computerized system that integrates the building information model objects in industry foundation classes and radio-frequency identification to improve building maintenance performance. The computerized system is successfully applied to the building of a soccer stadium via the proposed research methodology using a qualitative and practical approach. The main contribution of this research is to provide an innovative framework for the implementation of the proposed system with a step-by-step approach, and also to evaluate the maintenance methods to help different building maintenance stakeholders decide on the appropriate method. The research indicates that with a slight effort on the implementation of the proposed system, a significant improvement of overall maintenance performance can be achieved compared to the traditional method in terms of time, cost, and functionality. It is also hoped that the proposed system can join with other new technologies to promote the system's value in the field of building maintenance.																	1868-5137	1868-5145															10.1007/s12652-020-02464-3		AUG 2020											
J								SLA-driven resource re-allocation for SQL-like queries in the cloud	KNOWLEDGE AND INFORMATION SYSTEMS										Cloud computing; Databases; Services-level agreement; Statistics collection; Resource re-allocation	OPTIMIZATION	Cloud computing has become a widely used environment for database querying. In this context, the goal of a query optimizer is to satisfy the needs of tenants and maximize the provider's benefit. Resource allocation is an important step toward achieving this goal. Allocation methods are based on analytical formulas and statistics collected from a catalog to estimate the cost of various possible allocations and then choose the best one. However, the allocation initially chosen is not necessarily the optimal one because of the approximate nature of the analytical formulas and the fact that the catalog may not be up to date. To solve this problem, existing work was proposed to collect statistics during the execution of the query and then trigger a re-allocation if suboptimality is detected. However, these proposals consider that queries have the same level of priority. Unlike the existing work, we propose in this paper a method of statistics collector placement and resource re-allocation by taking into account that the cloud is a multi-tenant environment and queries have different services-level agreements. In the experimental section, we show that our method provides a better benefit for the provider compared to state-of-the-art methods.																	0219-1377	0219-3116															10.1007/s10115-020-01501-z		AUG 2020											
J								Next word prediction based on the N-gram model for Kurdish Sorani and Kurmanji	NEURAL COMPUTING & APPLICATIONS										Next word prediction; Kurdish language; N-gram; Corpus		Next word prediction is an input technology that simplifies the process of typing by suggesting the next word to a user to select, as typing in a conversation consumes time. A few previous studies have focused on the Kurdish language, including the use of next word prediction. However, the lack of a Kurdish text corpus presents a challenge. Moreover, the lack of a sufficient number of N-grams for the Kurdish language, for instance, five-grams, is the reason for the rare use of next Kurdish word prediction. Furthermore, the improper display of several Kurdish letters in the RStudio software is another problem. This paper provides a Kurdish corpus, creates five, and presents a unique research work on next word prediction for Kurdish Sorani and Kurmanji. The N-gram model has been used for next word prediction to reduce the amount of time while typing in the Kurdish language. In addition, little work has been conducted on next Kurdish word prediction; thus, the N-gram model is utilized to suggest text accurately. To do so, R programming and RStudio are used to build the application. The model is 96.3% accurate.																	0941-0643	1433-3058															10.1007/s00521-020-05245-3		AUG 2020											
J								Dragonflies segmentation with U-Net based on cascaded ResNeXt cells	NEURAL COMPUTING & APPLICATIONS										U-Net; Neural network; Dragonfly; Residual network	INDICATORS; NETWORKS	In cooperation with biologists, we discuss the problem of animal species protection with the usage of modern technologies, namely mobile phones. In our work, we consider the problem of dragonfly image classification, where the aim is given to a preprocessing-segmentation of a dragonfly body from a background. To solve the task, we improve U-Net architecture by ResNeXt cells firstly. Further, we focus on the reasonability of features in neural networks with cardinality dimension and propose the cascaded way of re-using the features among blocks in particular cardinal dimensions. The reuse of the already trained features leads to composing more robust features and more efficient usage of neural network parameters. We test our cascaded cells together with three various U-Net versions for four different settings of hyperparameters with the conclusion that the system of cascaded features leads to higher accuracy than the other versions with the same number of parameters. Also, the cascaded cells are more robust to overfitting the dataset. The obtained results are confirmed on two additional public datasets.																	0941-0643	1433-3058															10.1007/s00521-020-05274-y		AUG 2020											
J								A noise injection strategy for graph autoencoder training	NEURAL COMPUTING & APPLICATIONS										Graph autoencoder; Noise injection; Training algorithm; Overfitting	NEURAL-NETWORKS	Graph autoencoder can map graph data into a low-dimensional space. It is a powerful graph embedding method applied in graph analytics to lower the computational cost. Researchers have developed different graph autoencoders for addressing different needs. This paper proposes a strategy based on noise injection for graph autoencoder training. This is a general training strategy that can flexibly fit most existing training algorithms. The experimental results verify this general strategy can significantly reduce overfitting and identify the noise rate setting for consistent training performance improvement.																	0941-0643	1433-3058															10.1007/s00521-020-05283-x		AUG 2020											
J								Multi-label learning on principles of reversek-nearestneighbourhood	EXPERT SYSTEMS										classification classification; k nearest neighbourhoodneighborhood; multi-label learning learning; reverse nearest neighbourhoodneighborhood	FEATURE-SELECTION; CLASSIFICATION	In this article, we present a novel neighbourhood based multi-label classifier, Multi-label Learning on principles of Reverse k-Nearest Neighbourhood (ML-RkNN) where we estimate the neighbourhood of the points on the basis of their reversek-nearest neighbourhood (RkNN). Through RkNN, for the same value ofk, we get different number of neighbours for different instances and this happens adaptively according to the neighbourhood configuration of the points. The automatically adaptive neighbourhood helps us in better learning of the local configurations around the points. Our scheme also facilitates implicit handling of the local imbalances prevailing in the datasets by comparing the class distributions of the test points and their reverse nearest neighbours. This implicit and adaptive handling is particularly useful for multi-label label datasets, whose labels are differentially imbalanced. Empirical study is performed on 10 real-world multi-label datasets considering five neighbourhood based multi-label learners. MacroF(1)is used as the evaluating metric. The proposed method has given statistically superior and statistically comparable performances with respect to three and two comparing methods respectively. Additionally, we have explored the use of two different distance metrics, Euclidean and Jaccard in our scheme for nominal datasets.																	0266-4720	1468-0394														e12615	10.1111/exsy.12615		AUG 2020											
J								Unsupervised Domain Adaptation in the Wild via Disentangling Representation Learning	INTERNATIONAL JOURNAL OF COMPUTER VISION										In the wild; Cross-domain; Recognition; Segmentation		Most recently proposed unsupervised domain adaptation algorithms attempt to learn domain invariant features by confusing a domain classifier through adversarial training. In this paper, we argue that this may not be an optimal solution in the real-world setting (a.k.a. in the wild) as the difference in terms of label information between domains has been largely ignored. As labeled instances are not available in the target domain in unsupervised domain adaptation tasks, it is difficult to explicitly capture the label difference between domains. To address this issue, we propose to learn a disentangled latent representation based on implicit autoencoders. In particular, a latent representation is disentangled into a global code and a local code. The global code is capturing category information via an encoder with a prior, and the local code is transferable across domains, which captures the "style" related information via an implicit decoder. Experimental results on digit recognition, object recognition and semantic segmentation demonstrate the effectiveness of our proposed method.																	0920-5691	1573-1405															10.1007/s11263-020-01364-5		AUG 2020											
J								cPEA: a parallel method to perform pathway enrichment analysis using multiple pathways databases	SOFT COMPUTING										Parallel computing; Statistical analysis; Pathway enrichment analysis; Gene expression; SNP	POSTTRANSLATIONAL MODIFICATIONS; COLORECTAL-CANCER; GENOME; MITOSIS	Genes/proteins are essential to activate or inhibit biological pathways both inside or outside the cells in each living organism. The key to understand the functional roles of genes/proteins is the deduction of the relationship between pathways and genes/proteins. To understand the role of genes/proteins in a biological context, we can use pathway enrichment analysis (PEA), an essential method in omics research, to identify the biological role of genes/proteins. A large number of PEA methods and tools are available; nevertheless, only a few can perform PEA exploiting information coming from multiple databases in the same analysis. Many of these databases were initially developed to use their pathway representation format, resulting in a heterogeneous collection of resources that are extremely difficult to combine and use. Soft computing enables approximate solutions for problems challenging to solve precisely, such as merging and integrating structured and unstructured data, or data from different databases. The integration and merging of biological pathways from diverse data sources are challenging due to the different pathway data representations used. The use of parallel preprocessing methods to deal with approximation and imprecision can contribute to integrate heterogeneous pathway data. We implemented an automatic methodology to perform PEA using pathways coming from different databases and a method to compute topological scores to rank enriched pathways. This methodology is available in a software framework called cross-pathway enrichment analysis. The obtained results show good performance in terms of execution times and reduced memory consumption, allowing to improve PEA by using pathways coming from different databases.																	1432-7643	1433-7479															10.1007/s00500-020-05243-6		AUG 2020											
J								Faults diagnosis of a centrifugal pump using multilayer perceptron genetic algorithm back propagation and support vector machine with discrete wavelet transform-based feature extraction	COMPUTATIONAL INTELLIGENCE										Back propagation; centrifugal pump; discrete wavelet transform; genetic algorithm; multilayer perceptron	ARTIFICIAL NEURAL-NETWORKS; FEATURE-SELECTION; CLASSIFICATION	This paper presents a comparative study of two artificial intelligent systems, namely; Multilayer Perceptron (MLP) and support vector machine (SVM), to classify six fault conditions and the normal (nonfaulty) condition of a centrifugal pump. A hybrid training method for MLP is proposed for this work based on the combination of Back Propagation (BP) and Genetic Algorithm (GA). The two training algorithms are tested and compared separately as well. Features are extracted using Discrete Wavelet Transform (DWT), both approximations, details, and two mother wavelets were used to investigate their effectiveness on feature extraction. GA is also used to optimize the number of hidden layers and neurons of MLP. In this study, the feature extraction, GA-based hidden layers, neurons selection, training algorithm, and classification performance, based on the strengths and weaknesses of each method, are discussed. From the results obtained, it is observed that the DWT with both MLP-BP and SVM produces better classification rates and performances.																	0824-7935	1467-8640															10.1111/coin.12390		AUG 2020											
J								Handwritten Meitei Mayek recognition using three-channel convolution neural network of gradients and gray	COMPUTATIONAL INTELLIGENCE										character recognition; convolution neural network; gradient direction; gradient magnitude; gray channel; handwritten Meitei Mayek	CHARACTER-RECOGNITION	The problem of searching a similar pattern is an exciting and challenging research field of pattern recognition. The intelligence of humans for vision to read is a crucial phenomenon for machine simulation and has been carried out for a few decades. Therefore, in this article, a recognition system of handwritten Meitei Mayek (Manipuri script) is introduced using a convolutional neural network. Generally, character recognition is performed using the gray scale of the image of characters. However, we have additionally considered the corresponding gradient direction and gradient magnitude images to create three-channels image for every character so that supplementary information from gradient images can be obtained for efficient recognition. Experiments are conducted on 14 700 sample images collected from various individuals of different age groups and educational backgrounds. A recognition rate of 98.70% is obtained, which is compared with the existing methods, and it is found to be superior performance than other neural network methods on Meitei Mayek.																	0824-7935	1467-8640															10.1111/coin.12392		AUG 2020											
J								CoGCN: Combiningco-attentionwith graph convolutional network for entity linking with knowledge graphs	EXPERT SYSTEMS										co-attention mechanism; entity linking; graph convolutional network; knowledge graphs	BASE	Entity linking is a fundamental task in natural language processing. The task of entity linking with knowledge graphs aims at linking mentions in text to their correct entities in a knowledge graph like DBpedia or YAGO2. Most of existing methods rely on hand-designed features to model the contexts of mentions and entities, which are sparse and hard to calibrate. In this paper, we present a neural model that first combines co-attention mechanism with graph convolutional network for entity linking with knowledge graphs, which extracts features of mentions and entities from their contexts automatically. Specifically, given the context of a mention and one of its candidate entities' context, we introduce the co-attention mechanism to learn the relatedness between the mention context and the candidate entity context, and build the mention representation in consideration of such relatedness. Moreover, we propose a context-aware graph convolutional network for entity representation, which takes both the graph structure of the candidate entity and its relatedness with the mention context into consideration. Experimental results show that our model consistently outperforms the baseline methods on five widely used datasets.																	0266-4720	1468-0394														e12606	10.1111/exsy.12606		AUG 2020											
J								A hybridMCDMmodel combiningDANPwithTODIMto evaluate the information quality of community question answering in a two-dimensional linguistic environment	EXPERT SYSTEMS										two-dimensional uncertain linguistic variables; community question answering; fuzzy multicriteria decision-making; information quality	GROUP DECISION-MAKING; SOURCE CREDIBILITY; TODIM METHOD; SELECTION; DEMATEL; MODEL; REPUTATION; ANP	Evaluating the information quality of cQA (community question-answering) websites helps users select a cQA website with high-quality information and improves the information quality. In this paper, an approach to evaluating the information quality of cQA based on a novel hybrid multicriteria decision-making (MCDM) model is proposed. First, the source, content, expression and usefulness criteria for evaluating the information quality of the cQA are established. Then, considering the ambiguity and inner correlations of the criteria, DANP ([DEMATEL]-based Analytic Network Process) and TODIM (interactive and multiple attribute decision making, in Portuguese) methods are combined in a two-dimensional linguistic environment to process linguistic evaluation information. Lastly, the proposed approach is applied to evaluate the quality of five popular cQA websites. The key criteria are identified, and the evaluation results are derived comprehensively. The key factors consist of reputation, coverage, politeness, usability, helpfulness, clarity, readability and conciseness. The websites, Know almost and Baidu knows, perform better in terms of information quality. The application, along with the sensitivity analysis and comparative analysis, shows the effectiveness of the proposed model. The proposed approach has both practical and research implications. It provides an approach for users to choose websites, and, for operators, to improve the information quality of their website. The evaluation criteria and their relations provide a reference for research on the information quality. The evaluation method considers the user's psychological information to provide a more accurate MCDM approach. The comprehensive aspects of the experiment can be used to verify the other MCDM methods.																	0266-4720	1468-0394														e12619	10.1111/exsy.12619		AUG 2020											
J								Single-shot bidirectional pyramid networks for high-quality object detection	NEUROCOMPUTING										Object detection; Deep learning; Computer vision; Anchor refinement		Recent years have witnessed significant advances in deep learning based object detection. Despite being extensively explored, most existing detectors are designed to detect objects with relatively low-quality prediction of locations, i.e., they are often trained with the threshold of Intersection over Union (IoU) set as 0.5. This can yield low-quality or even noisy detections. Designing high quality object detectors which have a more precise localization (e.g. IoU > 0.5) remains an open challenge. In this paper, we propose a novel single-shot detection framework called Bidirectional Pyramid Networks (BPN) for high-quality object detection. It comprises two novel components: (i) Bidirectional Feature Pyramid structure and Anchor Refinement (AR). The bidirectional feature pyramid structure aims to use semantic-rich deep layer features to enhance the quality of the shallow layer features, and simultaneously use the spatially-rich shallow layer features to enhance the quality of deep layer features, leading to a stronger representation of both small and large objects for high quality detection. Our anchor refinement scheme gradually refines the quality of pre-designed anchors by learning multi-level regressors, giving more precise localization predictions. We performed extensive experiments on both PASCAL VOC and MSCOCO datasets, and achieved the best performance among all single-shot detectors. The performance was especially superior in the regime of high-quality detection. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 11	2020	401						1	9		10.1016/j.neucom.2020.02.116													
J								Autonomous root-cause fault diagnosis using symbolic dynamic based causality analysis	NEUROCOMPUTING										Root-cause fault diagnosis; Causality analysis; Multi-dimensional symbolic dynamic-based normalized direct transfer entropy (SDNDTE); N-joint xD-Markov machine; Immediate and source intermediate variables	RECONSTRUCTION-BASED CONTRIBUTION; INFORMATION-TRANSFER; OSCILLATIONS; PROPAGATION; ENTROPY; SYSTEMS	This paper proposes a general framework for autonomous root-cause fault diagnosis in a complex process. In this framework, as a prerequisite step after conducting fault detection, the potential root-cause candidates are selected using a contribution score-based method (e.g. accumulative rate contribution scores). Then a fully automated procedure is proposed to determine the root-cause(s) of the detected fault amongst potential candidates without a priori knowledge of the base-line model or intervention of an expert. To locate the root-cause variable(s), firstly symbolic dynamic-based normalized transfer entropy (SDNTE) is proposed to generate an initial causal graph of root-cause candidates. Then symbolic dynamic-based normalized direct transfer entropy (SDNDTE) is proposed and utilized for pruning the initial graph (i.e. discard indirect and spurious causal edges). To this aim, explicit definitions of immediate intermediate variables (IIV) and source intermediate variables (SIV) are given and systematic algorithms are developed to find them efficiently. At last, a topological approach is proposed to locate the root-cause variables according to the pruned causal graph. The proposed generalized SDNDTE approach overcomes the computational complexity in the calculation of normalized direct transfer entropy (NDTE) using conventional multi-dimensional kernel probability density functions (PDFs) fitting, which enables real-time root-cause fault diagnosis. To demonstrate the effectiveness and applicability, the proposed autonomous scheme is tested on a numerical example at first, and its computational complexity is analysed and compared to other existing works. Finally, it is validated on the Tennessee Eastman process (TEP) benchmark model. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 11	2020	401						10	27		10.1016/j.neucom.2020.03.007													
J								Higher-order potentials for video object segmentation in bilateral space	NEUROCOMPUTING										Video; Object segmentation; Markov random field; Higher-order; Bilateral space	TRACKING	We propose an effective approach to make segmentation for objects in videos with an initial input of the object masks in a few frames of the source video. In this method, we cast the segmentation task as a Markov Random Field (MRF) labeling problem. Different from the conventional MRF models, our model uses an additional term of higher-order potential to better propagate the global consistency among frames. The higher-order potential presented in this paper is significant for the proposed method because of its capability to keep the long-range consistency during segmentation. In order to make the MRF energy minimized, we also introduce a smart skill that makes the intractable higher-order potential "invisible" during the optimization so that the problem can be solved simply by applying a standard graph cut algorithm. Besides, the entire process is operated in a bilateral space, where the labeling can be inferred efficiently on the vertices that are sampled regularly from the bilateral grid. The results of a comparison of our method with a number of recently developed methods show that it performs favorably against state-of-the-art algorithms on multiple benchmark data sets in view of accuracy and achieves a much faster runtime performance. (C) 2020 Published by Elsevier B.V.																	0925-2312	1872-8286				AUG 11	2020	401						28	35		10.1016/j.neucom.2020.03.020													
J								Siamese Deformable Cross-Correlation Network for Real-Time Visual Tracking	NEUROCOMPUTING										Visual Tracking; Convolutional Neural Networks; Siamese network; Deformable Convolutional Network	OBJECT TRACKING; ROBUST	In recent years, SiamFC-based trackers have received much attention because of their great potentials in balancing tracking accuracy and speed. However, the robustness of most such trackers is greatly affected by the large deformations of targets. We argue that in the cross-correlation operation which is widely used by modern SiamFC-based trackers, the static correlation between the template kernel and the feature maps of test sample is difficult to adapt to the large deformation of the target object. In this paper, we propose a Siamese deformable cross-correlation network (SiamDCN), which introduces the deformable cross-correlation operation into SiamFC in an online self-adaptive way, for robust visual tracking. Compared to the previous SiamFC-based trackers, our SiamDCN is more robust to the large deformations of targets through dynamically and adaptively adjusting the location of correlation calculation for each element of the template kernel in the cross-correlation operation. Moreover, we build a twofold Siamese network, named SiamDCN+, which consists of a SiamDCN branch and a SiamFC branch, for accurate and real-time visual tracking after observing that the features learned in SiamFC are static and discriminative, whereas those in SiamDCN are dynamic and robust, and they complement each other. Extensive experiments on three public benchmarks, 0TB2015, V0T2016, and V0T2017, show that the proposed SiamDCN achieves superior localization accuracy than its baseline tracker SiamFC and the proposed SiamDCN+ achieves competitive performance compared to state-of-the-art real-time trackers, while running beyond 40 FPS. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 11	2020	401						36	47		10.1016/j.neucom.2020.02.080													
J								Cumulative link models for deep ordinal classification	NEUROCOMPUTING										Deep learning; Ordinal regression; Cumulative link models; Kappa index	CONVOLUTIONAL NEURAL-NETWORK; AGE	This paper proposes a deep convolutional neural network model for ordinal regression by considering a family of probabilistic ordinal link functions in the output layer. The link functions are those used for cumulative link models, which are traditional statistical linear models based on projecting each pattern into a 1-dimensional space. A set of ordered thresholds splits this space into the different classes of the problem. In our case, the projections are estimated by a non-linear deep neural network. To further improve the results, we combine these ordinal models with a loss function that takes into account the distance between the categories, based on the weighted Kappa index. Three different link functions are studied in the experimental study, and the results are contrasted with a statistical analysis. The experiments run over two different ordinal classification problems and the statistical tests confirm that these models improve the results of a nominal model and outperform other robust proposals considered in the literature. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 11	2020	401						48	58		10.1016/j.neucom.2020.03.034													
J								Product quantization with dual codebooks for approximate nearest neighbor search	NEUROCOMPUTING										Approximate nearest neighbor search; Product quantization; Vector quantization; Dual codebooks; Sub-database	VECTOR QUANTIZATION; ALGORITHM	Product quantization (PQ) is a powerful technique for approximate nearest neighbor (ANN) search. In this paper, to improve the accuracy of ANN search, we propose a new PQ-based method named product quantization with dual codebooks (DCPQ). Different from traditional PQ-based methods, we analyze quantization errors after learning the first PQ codebook, and then part of training vectors with larger quantization errors are found and selected to relearn a second PQ codebook. When encoding the database offline, all database vectors are firstly quantized using both of dual codebooks in each subspace, and the encoding mode of a database vector is determined after comparing the two quantization errors based on dual codebooks. Moreover, database vectors with the same encoding mode are grouped as a sub-database and can be more efficiently searched. Experimental results demonstrate that our proposed dual codebooks solution can achieve higher accuracy compared with the standard PQ and its variants. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 11	2020	401						59	68		10.1016/j.neucom.2020.03.016													
J								On reachable set estimation of multi-agent systems	NEUROCOMPUTING										Reachable set estimation; Multi-agent systems; Markovian switching topologies	INFINITY CONSENSUS CONTROL; COOPERATIVE CONTROL; LINEAR-SYSTEMS; DELAY; NETWORKS	In this paper, the problem of reachable set estimation of multi-agent systems (MASs) is investigated. For the ease of application, balanced communication topology with a directed spanning tree and non-zero initial condition are considered. Based on the Lyapunov method and orthogonal transformation technique, sufficient conditions are established in terms of linear matrix inequalities to guarantee that the reachable set of MASs is bounded by a compact ellipsoid. Moreover, by employing some useful skills from stochastic analysis, the result is then extended to the problem for MASs under Markovian switching topologies. The effectiveness of the obtained results is illustrated by two numerical examples. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 11	2020	401						69	77		10.1016/j.neucom.2020.03.012													
J								Learning selection channels for image steganalysis in spatial domain	NEUROCOMPUTING										Steganalysis; Selection channel; Deep learning; Convolutional neural network	STEGANOGRAPHY	Advanced steganography modifies the complex regions of digital media to embed secret messages, while steganalysis aims to detect whether the digital media contains secret messages or not. It is well recognized that the content adaptivity which adopted in steganography should also be considered for steganalysis to improve detection accuracy, and thus the embedding locations are weighted (so called selection channel) to build steganalytic detectors. However, the existing selection channels incorporated into steganalysis are all manually designed and keep constant even in the whole training stage of deep learning based steganalysis. Therefore, the handcrafted and fixed selection channels leave much room for improvement in steganalysis. In this paper, we propose to learn the selection channels in an end-to-end manner. Our steganalytic scheme has two parts: selection channel network and steganalysis network. These two networks are trained together. The selection channel network learns and outs the selection channels for the steganalysis network, and the steganalysis network integrated with the learned selection channels predicts the final steganalysis results. Our experiments under various conditions show that the learned selection channels considerably improve the detection accuracy of steganalytic schemes against content-adaptive steganography, and also exhibit high universality and robustness in real-world environments. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 11	2020	401						78	90		10.1016/j.neucom.2020.02.105													
J								New stability criterion of fractional-order impulsive coupled non-autonomous systems on networks	NEUROCOMPUTING										Exponential stability; Impulsive coupled systems; Mittag-Leffler stability; Networks; Lyapunov function	REACTION-DIFFUSION SYSTEMS; MITTAG-LEFFLER STABILITY; NEURAL-NETWORKS; GLOBAL STABILITY; DIFFERENTIAL-EQUATIONS; LYAPUNOV FUNCTIONS; L-2-GAIN ANALYSIS; DELAY SYSTEMS; SYNCHRONIZATION	This paper explores stability of new fractional-order impulsive coupled (FOIC) non-autonomous systems on networks. By utilizing graph theory and structuring individual vertex's Lyapunov function, a systematic method is present to construct global Lyapunov functions of FOIC systems. By Lyapunov method, a new inequality as well as the graph theory, several new stable principles are derived based on the strongly connected property. Besides, several more general criteria are obtained by utilizing these principles to the FOIC systems, including uniform stability, exponential stability and Mittag-Leffler stability for a kind of FOIC systems. At last, a numerical example is provided to show the effectiveness of our findings. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 11	2020	401						91	100		10.1016/j.neucom.2020.03.001													
J								Neural-network-based adaptive output-feedback formation tracking control of USVs under collision avoidance and connectivity maintenance constraints	NEUROCOMPUTING										Output-feedback formation control; Collision avoidance; Connectivity maintenance; Unmanned surface vehicles (USVs); Neural network (NN)	PRESCRIBED PERFORMANCE GUARANTEES; FOLLOWER FORMATION CONTROL; DYNAMIC SURFACE CONTROL; MULTIAGENT SYSTEMS; STATE; VEHICLES; SYNCHRONIZATION; TRANSIENT; SUBJECT; VESSELS	In this paper, we study the output-feedback formation tacking control problem for a group of unmanned surface vehicles (USVs) with modeling uncertainties under communication constraints. We consider a one-to-one communication topology, in which the leading vehicle is assigned a task to track a desired trajectory and each vehicle except for the last follower (tail agent) communicates only with one leader and with one follower. We assume that the information exchange among the vehicles is limited by some given communication radius. Under the limited communication range, connectivity maintenance and collision avoidance between the leader and follower are considered in the formation tracking control design. To compensate for the modeling uncertainties, we employ neural network (NN) approximators to estimate uncertain dynamics. Based on the dynamic surface control technique, backstepping procedure, NN-based observers, tan-type barrier Lyapunov functions, and control Lyapunov synthesis, a decentralized adaptive output-feedback formation tracking controller is presented to achieve the boundedness of the signals in the closed-loop system, while guaranteeing connectivity maintenance and collision avoidance between the leader and follower during whole operation. Simulation results demonstrate the effectiveness of the proposed formation controller. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 11	2020	401						101	112		10.1016/j.neucom.2020.03.033													
J								SBNN: Slimming binarized neural network	NEUROCOMPUTING										Binarized neural network; Network pruning; Knowledge distillation		With the rapid developments of deep neural networks related applications, approaches for accelerating computationally intensive convolutional neural networks, such as network quantization, pruning, knowledge distillation, have attracted ever-increasing attention. Network binarization is an extreme form of network quantization technique, which binarizes the network weights and/or activation values to save computational resources. However, it often introduces noises into the network, and requires larger model size (more parameters) to compensate for the loss of representation capacity. To address the model complexity reduction challenges and further improve the network performance, this paper proposes an approach: slimming binarized neural networks (SBNN), which reduces complexity of binarized networks with acceptable accuracy loss. SBNN prunes the convolutional layers and fully-connected layer in a binarized network. Then it is refined by the proposed SoftSign function, knowledge distillation and fullprecision computation to enhance the network accuracy. The proposed SBNN can be also conveniently applied to a pre-trained binarized network. We demonstrate the effectiveness of our approach through several state-of-the-art binarized models. For AlexNet and ResNet-18 on ILSVRC-2012 dataset, SBNN obtains negligible accuracy loss but even a better accuracy than the pre-pruning model while using only 75% of original filters. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 11	2020	401						113	122		10.1016/j.neucom.2020.03.030													
J								A novel data augmentation scheme for pedestrian detection with attribute preserving GAN	NEUROCOMPUTING										Generative Adversarial Networks; Pedestrian detection; Data augmentation		Recently pedestrian detection has progressed significantly. However, detecting pedestrians of small scale or in heavy occlusions is still notoriously difficult. Besides, the generalization ability of pre-trained detectors across different datasets remains to be improved. Both of these issues can be attributed to insufficient training data coverage. To cope with this, we present an efficient data augmentation scheme by transferring pedestrians from other datasets into the target scene with a novel Attribute Preserving Generative Adversarial Networks (APGAN). The proposed methodology consists of two steps: pedestrian embedding and style transfer. The former step can simulate pedestrian images of various scale and occlusion, in any pose or background, thus greatly promoting the data variation. The latter step aims to make the generated samples more realistic while guarantee the data coverage. To achieve this goal, we propose APGAN, which pursues both good visual quality and attribute preserving after style transfer. With the proposed method, we can make effective sample augmentations to improve the generalization ability of the trained detectors and enhance its robustness to scale change and occlusions. Extensive experiment results validate the effectiveness and advantages of our method. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 11	2020	401						123	132		10.1016/j.neucom.2020.02.094													
J								Cross domain knowledge learning with dual-branch adversarial network for vehicle re-identification	NEUROCOMPUTING										Domain adaptation; Dual-branch adversarial network; Vehicle re-identification	PERSON REIDENTIFICATION; ADAPTATION	The widespread popularization of vehicles has facilitated all people's life during the last decades. However, the emergence of a large number of vehicles poses the critical but challenging problem of vehicle re-identification (reID). Till now, for most vehicle reID algorithms, both the training and testing processes are conducted on the same annotated datasets under supervision. However, even a well-trained model will still cause fateful performance drop due to the severe domain bias between the trained dataset and the real-world scenes. To address this problem, this paper proposes a domain adaptation framework for vehicle reID (DAVR), which narrows the cross-domain bias by fully exploiting the labeled data from the source domain to adapt the target domain. DAVR develops an image-to-image translation network named Dual-branch Adversarial Network (DAN), which promotes the images from the source domain (well-labeled) to learn the style of the target domain (unlabeled). Specially, DAN doesn't need any annotation and can preserve identity information from source domain before and after translation. Furthermore, the generated images are employed to train the vehicle reID model by a proposed attention-based feature learning network. Through the proposed framework, the well-trained reID model has better generalization ability for various scenes in real-world situations. Comprehensive experimental results have demonstrated that our proposed DAVR can achieve excellent performances on benchmark datasets VehiclelD and VeRi-776. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 11	2020	401						133	144		10.1016/j.neucom.2020.02.112													
J								Robust stability of uncertain fractional order singular systems with neutral and time-varying delays	NEUROCOMPUTING										Fractional singular system; Robust stability; Lyapunov approach; Neutral delays; Time-varying delays	RECURSIVE STATE ESTIMATION; COMPLEX NETWORKS; ASYMPTOTICAL STABILITY; NONLINEAR-SYSTEMS; NEURAL-NETWORKS; PARTIAL-NODES; JUMP SYSTEMS; H-INFINITY; DISCRETE; CRITERIA	In this paper, the robust stability of uncertain fractional order singular systems with neutral and time-varying delays is investigated. By applying Lyapunov-Krasovskii functional approach, several sufficient conditions in the form of linear matrix inequality to ensure asymptotical stability and robust stability are derived for the considered systems. The advantage of the employed method in this paper is that one may directly calculate integer-order derivatives of Lyapunov-Krasovskii functional. Several simple examples are given to illustrate the effectiveness of the obtained results. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 11	2020	401						145	152		10.1016/j.neucom.2020.03.015													
J								Decision Tree SVM: An extension of linear SVM for non-linear classification	NEUROCOMPUTING										Linear Support Vector Machine; Decision Tree; Classification; Machine learning		Kernel trick is widely applied to Support Vector Machine (SVM) to deal with linearly inseparable data which is known as kernel SVM. However, kernel SVM always has high computational cost in practice which makes it unsuitable to handle large scale data. Moreover, kernel SVM always brings hyperparameters, e.g. bandwidth in Gaussian kernel. Since the hyper-parameters have a significant influence on the final performance of kernel SVM and are pretty hard to tune especially for large scale data, one may need to put lots of effort into finding good enough parameters, and improper settings of the hyperparameters often make the classification performance even lower than that of linear SVM. Inspired by recent progresses on linear SVM for dealing with large scale data, we propose a well-designed classifier to efficiently handle large scale linearly inseparable data, i.e., Decision Tree SVM (DTSVM). DTSVM has much lower computational cost compared with kernel SVM, and it brings almost no hyper-parameters except a few thresholds which can be fixed in practice. Comprehensive experiments on large scale datasets demonstrate the superiority of the proposed method. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 11	2020	401						153	159		10.1016/j.neucom.2019.10.051													
J								INOR-An Intelligent noise reduction method to defend against adversarial audio examples	NEUROCOMPUTING										Adversarial audio examples; Defense against adversarial audio examples; INOR		Recently, Automatic Speech Recognition(ASR) systems are seriously threatened by adversarial audio examples. The defense against adversarial audio examples has become an urgent issue. Different from adversarial image examples whose target is limited in the finite categories, the target of adversarial audio examples can be any combination of the words in a language. Adversarial audio examples aim to change the semantic of the audio. The semantic is explicitly represented in transcription distance, which affects the adversarial perturbation. This paper analyzes the relationship between semantic difference and adversarial perturbation. Quantization and local smoothing are calibrated to evaluate their performance. We observe that, for adversarial audio examples with different transcription distance levels, the capability of different denoising strategies varies. Therefore, we first introduce the wavelet filter, which denoises the signal in the transformed domain. Then we explore the defense capability of combined filters. Finally, a new intelligent noise reduction method-INOR is proposed to improve the denoising performance of audios under different levels of transcription distance. Experimental results show that INOR is effective in mitigating the adversarial perturbations for adversarial examples with different transcription distance levels. The average CER and WER is reduced by 33% and 55%. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 11	2020	401						160	172		10.1016/j.neucom.2020.02.110													
J								Speech Based Estimation of Parkinson's Disease Using Gaussian Processes and Automatic Relevance Determination	NEUROCOMPUTING										Speech disorder; Parkinson's disease; Machine learning; Gaussian processes; Feature selection	RATING-SCALE; DIAGNOSIS; PROGRESSION; ALGORITHMS	Parkinson's disease is a progressive neurodegenerative disorder often accompanied by impairment in articulation, phonation, prosody and fluency of speech. In fact, speech impairment is one of the earliest Parkinson's disease symptoms, and may be used for early diagnosis. We present an experimental study of identification of Parkinson's disease and assessment of disease progress from speech using Gaussian processes, which is further combined with Automatic Relevance Determination (ARD) for efficient feature selection. Hyperparameters of ARD covariance functions are learned for each individual feature; therefore, can be used for evaluation of their importance. In that way only a small subset of highly relevant acoustic features is selected, leading to models with better performance and lower complexity. The performance of the proposed method was assessed on two datasets: Parkinson's disease detection dataset, which contains a range of biomedical voice measurements obtained from 31 subjects, 23 of them suffering from Parkinson's disease and 8 healthy subjects; and Parkinson's telemonitoring dataset, containing biomedical voice measurements collected from 42 Parkinson's disease patients for estimation of the disease progress. Gaussian process classification with automatic relevance determination is able to successfully discriminate between Parkinson's disease patients and healthy controls with 96.92% accuracy, outperforming Support Vector Machines and decision tree ensembles (random forests, boosted and bagged decision trees). The usability of Gaussian processes is further confirmed in regression task for tracking the progress of the disease. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 11	2020	401						173	181		10.1016/j.neucom.2020.03.058													
J								Multi-type synchronization dynamics of delayed reaction-diffusion recurrent neural networks with discontinuous activations	NEUROCOMPUTING										Recurrent neural network; Reaction-diffusion; Discontinuous activation; Finite-/fixed-time synchronization	TIME-VARYING DELAYS; FINITE-TIME; EXPONENTIAL SYNCHRONIZATION; DISTRIBUTED DELAYS; STABILITY ANALYSIS; COMPLEX NETWORKS; STABILIZATION; CONVERGENCE	In this paper, we are concerned with the finite-/fixed-time synchronization(FFTS) problem of delayed reaction-diffusion recurrent neural networks (RNNs) with discontinuous activations. By designing a novel unified controller, with the help of theory of Filippov regularization, and a generalized finite-/fixed-time convergence theorem, we establish a threshold FFTS dynamics, which is determined by the power parameter, and the upper-bound of the settling time is explicitly estimated as well. The theoretical results herein generalize and improve some existing ones. Moreover, numerical simulations are performed to substantiate the effectiveness of the theoretical analysis. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 11	2020	401						182	192		10.1016/j.neucom.2020.03.040													
J								Quasi-periodic invariant 2-tori in a delayed BAM neural network	NEUROCOMPUTING										BAM neural network; Normal form; Double Hopf bifurcation; Quasi-periodic invariant torus	FUNCTIONAL-DIFFERENTIAL EQUATIONS; HOPF-BIFURCATION ANALYSIS; NORMAL FORMS; STABILITY; MODEL; SYSTEMS; TORI	In this paper, we consider a four-neuron bi-directional associative memory (BAM, for short) neural network with two delays. We choose connection weights and the sum of delays as bifurcation parameters and derive the critical values where a double Hopf bifurcation may occur by analyzing the associated characteristic equation which is a fourth-degree polynomial exponential equation. Meanwhile, we obtain some parameter conditions on the existence of invariant 2-tori of the truncated normal form near the bifurcation point by the center manifold theorem and normal form method. Despite the fact that the higher-degree terms may destroy the invariant 2-tori of the truncated normal form, we prove that the neural network model has quasi-periodic invariant 2-tori for most of the parameter set where the truncated normal form possesses invariant 2-tori in a sufficiently small neighborhood of the bifurcation point. Numerical examples and simulations are given to support the theoretical analysis. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 11	2020	401						193	208		10.1016/j.neucom.2020.03.039													
J								Deep feature importance awareness based no-reference image quality prediction	NEUROCOMPUTING										Deep neural network; No-reference image quality assessment; Feature importance awareness; Squeeze-and-Excitation block	CHALLENGES; STATISTICS; TRACKING	Deep-learning based image quality assessment (IQA) algorithms usually use the transfer learning method that transfers a pre-trained network for classification task to handle IQA task. Although it can overcome the problem of having insufficient IQA databases to some extent, it cannot distinguish between the important and unimportant deep features for the IQA task, which potentially leads to inaccurate prediction performance. In this paper, we propose a no-reference IQA method based on modelling of deep feature importance. A SE-VGG network is developed by using adaptive transfer learning method. It can suppress the features of local parts of salient objects of images that are not important to the IQA task, and emphasize the features of image distortion and salient objects that are important to IQA task. Moreover, the structure of the SE-VGG is investigated to improve the accuracy of the image quality assessment on a small IQA database. Experiments are conducted to evaluate the performance of the proposed method on various databases, including the LIVE, TID2013, CSIQ, LIVE multiply distorted and LIVE challenge. The results show the proposed method significantly outperforms the state-of-the-art methods. In addition, our method demonstrates a strong generalization ability. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 11	2020	401						209	223		10.1016/j.neucom.2020.03.072													
J								Clustering via Adaptive and Locality-constrained Graph Learning and Unsupervised ELM	NEUROCOMPUTING										Graph learning; Clustering; Adaptive regularizations; Extreme learning machine	DIMENSIONALITY REDUCTION; MACHINE; REPRESENTATION	In this paper an effective graph learning method is proposed for clustering based on adaptive graph regularizations. Many graph learning methods focus on optimizing a global constraint on sparsity, low-rankness or weighted pair-wise distances, but they often fail to consider local connectivities. We demonstrate the importance of locality by generalizing the Locality-constrained Linear Coding (LLC) for unsupervised learning. Each data sample is expressed as a representation of its nearest neighbors, which naturally leads to a combination of distance regularized features and a Locally Linear Embedding (LLE) decomposition. The representation enforces a locally sparse connection on the data graph that exhibits high discrimination power and is easy to optimize. To improve the learned graph structure and incorporate cluster information, a rank constraint is further imposed on the Laplacian matrix of the data graph so that the connected components match the class number. The obtained representations are smoothed via manifold regularizations on a predefined graph which serves as a prior for graph learning. Finally, we utilize unsupervised Extreme Learning Machine (US-ELM) to learn a flexible and discriminative data embedding. Extensive evaluations show that the proposed algorithm outperforms graph learning counterpart methods on a wide range of benchmark datasets. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 11	2020	401						224	235		10.1016/j.neucom.2020.03.045													
J								Classification of electromyographic hand gesture signals using machine learning techniques	NEUROCOMPUTING										Convolutional auto-encoder; Convolutional neural networks; Deep learning; EMG signals classification; Machine learning	SURFACE; FINGERS	The electromyogram (EMG) signals from an individual's muscles can reflect the biomechanics of human movement. The accurate classification of individual and combined finger movements using surface EMG signals is able to support many applications such as dexterous prosthetic hand control. The existing research of EMG-based hand gesture classification faces the challenges of inaccurate classification, insufficient generalization ability and weak robustness. To address these problems, this paper proposes a deep learning model that combines convolutional auto-encoder and convolutional neural network (CAE+CNN) to classify an EMG dataset consisting of 10 classes of hand gestures. The proposed method shrinks the inputs into a smaller latent space representation using CAE and the resultant compressed features are served as inputs of CNN, which reduces the redundancy of EMG signals and improves the classification accuracy and training efficiency. Besides, to enhance the robustness and generalization ability for classification, a data processing approach is proposed which combines the windowing method and majority voting of the obtained results from the classifier. In addition, comprehensive comparative study is carried out with 8 widely applied and state-of-the-art classifiers in terms of classification accuracy, robustness subject to noise and statistical analysis (sensitivity, specificity, precision, F1 Score and Matthews correlation coefficient). The results demonstrates that the integration of windowing method, CAE+CNN and majority voting achieves the best performance (99.38% test accuracy for the data without adding noise, which is 3.78% higher than the best classifier used for comparison), strongest robustness (achieved 98.13% test accuracy when Gaussian noise of level le-5 is added to the raw dataset, which is 4.07% higher than the best classifier used for comparison) and statistical properties compared to other classifiers, which shows the potential for healthcare applications such as movement intention detection and dexterous prostheses control. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 11	2020	401						236	248		10.1016/j.neucom.2020.03.009													
J								Evolutionary recurrent neural network for image captioning	NEUROCOMPUTING										Image captioning; Evolutionary algorithm; Multimodal learning		Automatic architecture search is efficient to discover novel neural networks while it is mostly employed for pure vision or natural language tasks. However, cross-modality tasks are highly emphasized on the associative mechanisms between visual and language models rather than merely convolutional neural network (CNN) or recurrent neural network (RNN) with the best performance. In this work, the intermediary associative connection is approximated to the topological inner structure of RNN cell, which is further evolved by an evolutionary algorithm on the proxy of image captioning task. On the MSCOCO dataset, the proposed algorithm, starting from scratch, discovers more than 100 RNN variants with the performances all above 100 on CIDEr and 31 on BLEU4, and the top performance achieves 101.4 and 32.6 accordingly. Additionally, several unknown interesting patterns as well as many existing powerful structures are found in the generated RNNs. The patterns of operation and connection in the generated architecture are analyzed to understand the language modeling of cross-modality compared with general RNNs. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 11	2020	401						249	256		10.1016/j.neucom.2020.03.087													
J								Theoretical and experimental analysis on the generalizability of distribution regression network	NEUROCOMPUTING										Supervised learning; Distribution regression		There is emerging interest in performing regression between distributions. In contrast to prediction on single instances, these machine learning methods can be useful for population-based studies or on problems that are inherently statistical in nature. The recently proposed distribution regression network (DRN) [13] has shown superior performance for the distribution-to-distribution regression task compared to conventional neural networks. However, in Kou et al. [13] and some other works on distribution regression, there is a lack of comprehensive comparative study on both the theoretical basis and generalization abilities of the methods. We derive some mathematical properties of DRN and compare it to conventional neural networks. We also perform comprehensive experiments to study the generalizability of distribution regression models, by studying their performance with limited training data, data sampling noise and varying task difficulty. DRN consistently outperforms conventional neural networks, requiring fewer training data and maintaining strong performance with noise. Furthermore, the theoretical properties of DRN can be used to provide some explanations on the ability of DRN to achieve better generalization performance than conventional neural networks. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 11	2020	401						257	270		10.1016/j.neucom.2020.03.084													
J								A closed-loop BMI system design based on the improved SJIT model and the network of Izhikevich neurons	NEUROCOMPUTING										Brain-machine interface; Model improvement; Model predictive control; Closed-loop system design; Intracortical micro-stimulation	VOLUNTARY ARM MOVEMENTS; CORTICAL CONTROL; BRAIN; STIMULATION; INTERFACE; AMPLITUDE; MOTOR	Brain-machine interface (BMI) is a useful technology which creates a new way for disable people to communicate with the world, but experimenting with human brains is risky. Hence, a precise mathematical model of the information transmission in the process of limb movement is necessary to be established. In this paper, firstly, we improve the classical single-joint information transmission (SJIT) model through introducing several neuron models, and the improved model is closer to the true single-joint movements. Secondly, a closed-loop system with a Wiener filter-based decoder, an auxiliary controller based on model predictive control (MPC) and a network of Izhikevich neurons is formulated based on the improved model, and the used network of Izhikevich neurons is more time efficient than the existing one. Finally, in this closed-loop system, the intra-cortical micro-stimulation (ICMS) technology is introduced to feedback the information from the MPC controller in real time. The auxiliary controller assist the brain to control artificial arm by changing the frequency of stimulation current. In this way, the computational complexity of the optimization problem proposed in this paper is greatly reduced, and the closed-loop BMI system designed in this paper can well track the desired trajectory. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 11	2020	401						271	280		10.1016/j.neucom.2020.03.047													
J								Sliding-Mode-Control-Theory-Based Adaptive General Type-2 Fuzzy Neural Network Control for Power-line Inspection Robots	NEUROCOMPUTING										General type-2 fuzzy system; adaptive fuzzy neural network control; sliding mode learning algorithm; power-line inspection robot	STRATEGY; SYSTEMS	In this paper, adaptive general type-2 fuzzy neural network control for motion balance adjusting of a power-line inspection robot is developed. It is used to enhance the anti-interference ability of the controlled plant. General type-2 fuzzy system is adopted because of its ability to more effectively handle uncertainties which may exist as external disturbances and parameter perturbations. The structure of general type-2 fuzzy system is designed by mimicking the neural network. The adaptive laws can be obtained based on the sliding mode control theory. This provides a kind of dynamic general type-2 fuzzy system whose membership functions and consequent parts are changing adaptively. The proposed controller is used to control an under-actuated non-linear power-line inspection (PLI) robot. Different simulation conditions are considered to test the anti-interference ability of the proposed controller. Simulation results indicate that the proposed method can strengthen the PLI robot's anti-interference ability in a better way as compared to its interval type-2 fuzzy counterpart and PD controller. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 11	2020	401						281	294		10.1016/j.neucom.2020.03.050													
J								Robust visual tracking with channel attention and focal loss	NEUROCOMPUTING										Visual tracking; Channel attention; Focal logistic loss	OBJECT TRACKING	Recently, the tracking community leads a fashion of end-to-end feature representation learning for visual tracking. Previous works treat all feature channels and training samples equally during training. This ignores channel interdependencies and foreground-background data imbalance, thus limiting the tracking performance. To tackle these problems, we introduce channel attention and focal loss into the network design to enhance feature representation learning. Specifically, a Squeeze-and-Excitation (SE) block is coupled to each convolutional layer to generate channel attention. Channel attention reflects the channel-wise importance of each feature channel and is used for feature weighting in online tracking. To alleviate the foreground-background data imbalance, we propose a focal logistic loss by adding a modulating factor to the logistic loss, with two tunable focusing parameters. The focal logistic loss down-weights the loss assigned to easy examples in the background area. Both the SE block and focal logistic loss are computationally lightweight and impose only a slight increase in model complexity. Extensive experiments are performed on three challenging tracking datasets including OTB100, UAV123 and TC128. Experimental results demonstrate that the enhanced tracker achieves significant performance improvement while running at a real-time frame-rate of 66 fps. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				AUG 11	2020	401						295	307		10.1016/j.neucom.2019.10.041													
J								A new perspective for Minimal Learning Machines: A lightweight approach	NEUROCOMPUTING										Minimal Learning Machine; Speedup training procedure; Sparsity; Model regularization	SPARSE; REGRESSION	This paper introduces a new procedure to train Minimal Learning Machines (MLM) for regression tasks. Besides that, we propose a new prediction process in MLM. A well-known drawback concerning the (original) MLM model formulation is the lack of sparseness.The most recent efforts on this problem strongly rely on the selection of reference points before training and prediction steps in MLM, all based on some supposition regarding the data. In the opposite direction, here, we explore another formulation of MLM in which we do not rely on any assumption regarding the data for prior selection. Instead, our proposal, named Lightweight Minimal Learning Machine (LW-MLM), builds a regularized system that imposes sparseness. We thrive in such a sparse criterion, not by selection but instead using a piece of weighted information into the model. We validate the contributions of this paper through four types of experiments to evaluate different aspects of our proposal: the prediction error performance, the goodness-of-fit of estimated vs. measured values, the norm values which are related to the sparsity, and finally, the prediction error in high dimensional settings. Based on the results, we show that LW-MLM is a valid alternative since achieved similar or higher accuracy rates against other variants being all seen as statistically equivalent. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 11	2020	401						308	319		10.1016/j.neucom.2020.03.088													
J								Sarcasm Detection with Sentiment Semantics Enhanced Multi-level Memory Network	NEUROCOMPUTING										Sarcasm detection; Sentiment semantics; Memory network; Natural language processing		Sarcasm detection is a challenging natural language processing task for sentiment analysis. Existing deep learning based sarcasm detection models have not fully considered sentiment semantics, even though sentiment semantics is necessary to improve the performance of sarcasm detection. To deal with the problem, we propose a multi-level memory network using sentiment semantics to capture the features of sarcasm expressions. In our model, we use the first-level memory network to capture sentiment semantics, and use the second-level memory network to capture the contrast between sentiment semantics and the situation in each sentence. Moreover, we use an improved convolutional neural network to improve the memory network in the absence of local information. The experimental results on the Internet Argument Corpus (IAC-V1 and IAC-V2) and Twitter dataset demonstrate the effectiveness of our model. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 11	2020	401						320	326		10.1016/j.neucom.2020.03.081													
J								A deadlock-free physical mapping method on the many-core neural network chip	NEUROCOMPUTING										Mapping; Routing; Many-core neural network chip; Deep learning; Neuromorphic computing		Many-core neural network chip is widely developed and used for both the deep learning and neuromorphic computing applications. Many-core architecture brings high parallelism while makes the model-to-core mapping intractable. In order to decrease the routing time, transmission packets amount and energy consumption, along with deadlock-free performance for inter-core data movement, we formulate an optimization problem for the physical mapping under the routing strategies with point-to-point and multicast paths. The Weighted Communication of Application(WCA) is defined as the objective function and simulated annealing algorithm incorporated with two deadlock-free constraints is designed to solve the mapping problem. Multi-layer perceptron(MLP) and convolutional neural network(CNN) applications are used for evaluation. Experimental results show that the proposed algorithm is quite efficient saving the routing time and power comsumption for inter-core communication, and the routing diversity has been significantly improved, the hotspot paths are greatly reduced after optimization, compare with the baseline of zigzag and neighbor mapping. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 11	2020	401						327	337		10.1016/j.neucom.2020.03.078													
J								Semi-supervised image attribute editing using generative adversarial networks	NEUROCOMPUTING										Image attribute editing; Generative models; Generative adversarial networks; Deep learning; Convolutional neural networks		Image attribute editing is a challenging problem that has been recently studied by many researchers using generative networks. The challenge is in the manipulation of selected attributes of images while preserving the other details. The method to achieve this goal is to find an accurate latent vector representation of an image and a direction corresponding to the attribute. Almost all the works in the literature use labeled datasets in a supervised setting for this purpose. In this study, we introduce an architecture called Cyclic Reverse Generator (CRG), which allows learning the inverse function of the generator accurately via an encoder in an unsupervised setting by utilizing cyclic cost minimization. Attribute editing is then performed using the CRG models for finding desired attribute representations in the latent space. In this work, we use two arbitrary reference images, with and without desired attributes, to compute an attribute direction for editing. We show that the proposed approach performs better in terms of image reconstruction compared to the existing end-to-end generative models both quantitatively and qualitatively. We demonstrate state-of-the-art results on both real images and generated images in CelebA dataset. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 11	2020	401						338	352		10.1016/j.neucom.2020.03.071													
J								Observer-based H-infinity control of two-dimensional delayed networks under the random access protocol	NEUROCOMPUTING										Two-dimensional systems; random access protocol; H-infinity control; observer-based controller; randomly occurring uncertainties	STOCHASTIC COMMUNICATION PROTOCOL; 2-D DISCRETE-SYSTEMS; STATE ESTIMATION; STABILITY ANALYSIS; FAULT-DETECTION; STABILIZATION; H-2	In this paper, the H-infinity control problem is investigated for a time-delayed two-dimensional (2-D) network under the random access protocol (RAP). The control input is transmitted to the actuator via a network with communication constraint, where only one signal obtains the network access at each transmission instant, and the RAP is adopted here to schedule the data transmission which is depicted by an independent and identically distributed sequence. The randomly occurring uncertainty, nonlinearity and unknown time delays are all introduced in the underlying 2-D system. The aim of the addressed problem is to design an observer-based controller so that the considered 2-D system has a predefined H-infinity performance index. Sufficient condition is given to ensure the H-infinity performance of the closed-loop system via the Lyapunov theory and stochastic analysis. Moreover, the observer-based controller is dexterously designed to ensure the achievement of the H-infinity performance, under which the controller gains are designed by solving certain matrix inequalities. Finally, a numerical example is given to demonstrate effectiveness of the proposed approach. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 11	2020	401						353	363		10.1016/j.neucom.2020.03.044													
J								Fixed-time synchronization of fractional order memristive MAM neural networks by sliding mode control	NEUROCOMPUTING										Fixed-time synchronization; Sliding mode control; Fractional order; Multidirectional associative memory neural networks; Memristor	STABILITY	In this paper, we first established the fractional order memristive multidirectional associative memory neural networks (FMMAMNNs) model, and then considered its fixed-time synchronization control problem. On the basis of sliding model control and Lyapunov stability theorem, a fractional order sliding mode controller is constructed. By adding this controller to the response system, the error of the driver-response systems gradually converges to 0 in a fixed time. Compared with the previous researches, this paper considers a more complex model, and the proposed control theories can ensure that the setting time is only related to the model and controller, but not to the initial states of the system. Besides, the control theories are also applicable to the integer order models. Finally, two numerical simulations are given, the results show the validity of the theories. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 11	2020	401						364	376		10.1016/j.neucom.2020.03.043													
J								Social relationship prediction across networks using tri-training BP neural networks	NEUROCOMPUTING										Social networks; Social relationships; Transfer learning; BP neural network; Tri-training	TRUST; TIES	It is well known that the number of users is increasing rapidly in online social networks. People are linked through various types of social relationships. Detecting the type of social relationships is fundamental to improve performance on many applications in social networks. Existing studies mainly focus on predicting social relationships in the same network based on its own abundant information. However, there are few works on predicting social relationships across different networks. In this paper, we study a tri-training model to predict the labels of social relationships across different networks based on BP neural networks. Firstly, we aim to obtain the hidden common characteristics between two different networks. Depending on these hidden common characteristics, unlabeled target samples are assigned pseudo labels by training two BP neural networks. Then we capture the special features for the target network by analyzing the structural characters. Finally, combining with the special and common features, we optimize the third BP neural network by training the pseudo labeled target samples. After training, we can use the third optimized BP neural network to predict the types of social relationships in the target network. We evaluate the model and compare it with some existing methods on six real online social networks. The experimental results indicate that our model can effectively exploit the unlabeled target sample and outperform the existing methods on multiple metrics, and also show that the special features in the target network can help to enhance the learning performance. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 11	2020	401						377	391		10.1016/j.neucom.2020.02.057													
J								The computation power of spiking neural P systems with polarizations adopting sequential mode induced by minimum spike number	NEUROCOMPUTING										Bio-inspired computing; Membrane computing; Spiking neural P system; Universality	EXHAUSTIVE USE; NETWORKS; ALGORITHM; IMAGES; RULES	Spiking neural P systems (SN P systems) refer to a type of distributed parallel neural computation model in the membrane computing framework. In this work, a new type of SN P system is examined, i.e., spiking neural P systems with polarizations (PSN P systems), by taking inspiration from the polarized cell membrane of a neuron. For the PSN P system, the firing condition of rules is the neuron-associated polarization. This work focuses on examining the computation power of the PSN P systems adopting the sequential mode induced by the minimum spike number, where at a computation step, one (resp., all) of neurons that hold the minimum spike number within the neurons which can fire will fire, i.e., the min-sequentiality strategy (resp., min-pseudo-sequentiality strategy). We prove that PSN P systems adopting the min-sequentiality strategy or min-pseudo-sequentiality strategy are Turing universal as the number generating devices. These results indicate the computation power of PSN P systems is robust regarding their strategies of sequentiality. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 11	2020	401						392	404		10.1016/j.neucom.2020.03.095													
J								Balancing computational complexity and generalization ability: A novel design for ELM	NEUROCOMPUTING										Extreme learning machine; Similarity functions; Electronic embedded systems	EXTREME LEARNING-MACHINE; BIG DATA; INTELLIGENCE	Learning paradigms that use random basis functions provide effective tools to deal with large datasets, as they combine efficient training algorithms with remarkable generalization performances. The paper first considers the affinity between the paradigm of learning with similarity functions and the Extreme Learning Machine (ELM) model, and reformulates the mapping scheme of ELMs. A mapping scheme that better balances generalization ability and network size is a novelty point of the proposed approach, and represent a major advantage when targeting implementation on resource-constrained devices. A computationally efficient heuristic supports the training procedure, and suitably applies the theory of learning with similarity functions to the availability of consistent amounts of data. Experimental results on standard datasets confirm the effectiveness of the proposed approach. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 11	2020	401						405	417		10.1016/j.neucom.2020.03.046													
J								The effect of contextual interference on the learning of adapted sailing for people with spinal cord injury	ADAPTIVE BEHAVIOR										Sailing; learning; navigation; spinal cord injury; variability	MOVEMENT COORDINATION; SYNERGETIC THEORY; ACQUISITION; RETENTION; VARIABILITY; REPRESENTATION; ORGANIZATION; PERFORMANCE; PATTERNS; SKILLS	The aim of this study was to determine the effect of contextual interference on learning of adapted sailing for people with spinal cord injury. Seven participants with traumatic spinal injury were selected to undergo learning in an adapted boat equipped with wind-measuring instrument. A learning program, defined by two conditions, (1) blocked practice and (2) random practice, was applied. In blocked schedule, fixed sequence of two maneuvers consist of tacking on a close-hauled course (45 degrees-55 degrees) and gybing on a broad reaching course (135 degrees-145 degrees). In random schedule, the same maneuvers were carried out randomly. Eight 30-min practice sessions were carried out over a period of 2 weeks, in 3 days per week. Velocity Made Good was analyzed in the familiarization test, pre-test, post-test, and retention test. The learning program was effective in both random and blocked practice conditions on all participants. However, there is an evident variability in the results. This study demonstrates the suitability of applying contextual interference to facilitate the learning of adapted sailing. The variability of the results could be related to the heterogeneity of the participants and suggests the need for further research that can provide information about how restricted mobility affects the learning process.																	1059-7123	1741-2633														1059712320946110	10.1177/1059712320946110		AUG 2020											
J								An efficient priority based resource management framework for IoT enabled applications in the cloud	EVOLUTIONARY INTELLIGENCE										IoT; Cloud; Storage; Priority; Sensor nodes	THINGS IOT; INTERNET	Internet of things (IoT) enabled applications are gaining importance in the recent times in many services likehealth care, smart homes, security services etc. The IoT enabled application generates huge volumes of data by continuously monitoring the environment. However, the IoT nodes are supported by cloud computing to meet computational and storage requirements due to limited computation capacity and storage. This paper presents cloud based framework for priority based IoT applications that use resources in an effective manner based on the emergency of the applications by the cloud. The simulation results proved the efficacy of the proposed framework with respect to CPU utilization and memory utilization. It is shown that the memory utilisation is 70% and CPU utilisation is 90% for the proposed framework and it performs better when compared with existing methods.																	1864-5909	1864-5917															10.1007/s12065-020-00468-8		AUG 2020											
J								A New Approach to Fuzzy Output Feedback Controller Design of Continuous-Time Takagi-Sugeno Fuzzy Systems	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Takagi-Sugeno fuzzy model; Output feedback control; Linear matrix inequality; Non-quadratic Lyapunov function	H-INFINITY CONTROL; NONLINEAR-SYSTEMS; STABILITY ANALYSIS; MODEL; STABILIZATION	This paper is concerned with the problem of output feedback control for continuous-time T-S fuzzy systems. The aim is to reduce the conservatism of finding output feedback controller design conditions based on the Lyapunov function which depends on membership functions. Firstly, in order to improve the existing results, a new approach to bound the time derivatives of the membership functions is proposed. Secondly, based on the non-quadratic Lyapunov function and matrix decoupling techniques, the static-output feedback controller and the dynamic output feedback controller are designed to guarantee that the system is asymptotically stable, respectively. Finally, three examples are given to indicate the effectiveness of the approach.																	1562-2479	2199-3211				OCT	2020	22	7					2223	2235		10.1007/s40815-020-00920-y		AUG 2020											
J								Adaptive Fuzzy Dynamic Surface Control for Nonlinear Systems with Time-Varying Input Delay and Sampled Data	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Adaptive control; Backstepping; Time-varying input delay; Dynamic surface control; Sampled-data control	TRACKING CONTROL; STABILIZATION	This paper studies fuzzy adaptive dynamic surface control strategy for nonlinear system with sampled data and time-varying input delay. By utilizing the approximated property of fuzzy logic systems (FLSs), a fuzzy estimator (FE) model is designed to identify the states of the original system, which is mainly used to provide information of estimation states to replace the sampled data of the nonlinear controlled system. In the proposed strategy, with the help of sampled-data activity, an integral term is designed to compensate the problem of time-varying input delay. Moreover, by invoking the dynamic surface control (DSC) technique, the problem of 'explosion of complexity' has been overcame. And the developed control strategy demonstrates that all signals of the controlled system are semi-globally uniformly ultimately bounded (SGUUB). Ultimately, two numerical simulation examples are given to prove the feasibility of the developed control method and theory.																	1562-2479	2199-3211				OCT	2020	22	7					2236	2245		10.1007/s40815-020-00927-5		AUG 2020											
J								Citation Analysis of Fuzzy Set Theory Journals: Bibliometric Insights About Authors and Research Areas	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Fuzzy set theory journals; Citation analysis; Authors; Research areas; Bibliometry	DECISION-MAKING; SYSTEMS; FIELD	Publications on fuzzy set theory and its applications have grown exponentially. The increasing rate of developments in the field is a response to diverse factors, including the need for robust mathematical approaches that model human-like perceptions, values and decision-making processes in complex and dynamic systems. This study presents a citation analysis of 22 narrowly targeted fuzzy set theory journals with a focus on leading authors and research areas. In this paper, bibliometric tools are used for the treatment and analysis of a large amount of data retrieved from the rigorous Web of Science scientific database. The aim of the paper is to offer a general overview of the influence that fuzzy set theory has on academicians and diverse scientific fields. Its objective is to identify connections, trends and opportunities for synergies. The results of over 62,000 published documents, which represent more than 1,300,000 citations in the selected journals, show computer science and engineering as the top citing research fields and authors Xu, Pedrycz and Herrera as the top citing researchers.																	1562-2479	2199-3211															10.1007/s40815-020-00924-8		AUG 2020											
J								Analysis and Control of Blood Glucose Situation for Diabetic Patients Based on Interval Type-2 Fuzzy Sets	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										IT2 FSs; LDS; Fuzzy comprehension evaluation; Blood glucose situation	DYNAMIC-SYSTEMS; LOGIC SYSTEM; MANAGEMENT; OPTIMIZATION; DIAGNOSIS	For diabetes mellitus (DM), the technology of blood glucose monitoring provides detection information for patients and helps sufferers to ameliorate bad states. Management rules and intervention measures, which are consistent with blood glucose situation, help to control blood glucose stability, establish a healthy lifestyle, and prevent the occurrence of DM complications. Formulating effective rules and measures is the key to the blood sugar management. What is more, the analysis of blood glucose situation is beneficial to the formulation of management rules and intervention measures. In this paper, interval type-2 fuzzy sets (IT2 FSs) and fuzzy comprehension evaluation are applied in the analysis of blood sugar situation. Moreover, dynamic fuzzy rules are built to provide the corresponding blood glucose management rules. Linguistic dynamic systems (LDS) are used to describe and analyze the evolution process of blood glucose situation. Finally, the analysis results indicate the method is applicable and valuable for actual management.																	1562-2479	2199-3211															10.1007/s40815-020-00918-6		AUG 2020											
J								Fuzzy Multi-objective Particle Swarm Optimization Solving the Three-Objective Portfolio Optimization Problem	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Fuzzy multi-objective particle swarm optimization; Multi-objective portfolio selection problem; Cardinality constraint		In this paper, the portfolio optimization problem is approached. It is a NP-hard problem that consists in periodically creating an instance of the problem, using the time series of the shares value of a stock exchange. The instance is solved to determine the shares set that maximize the return, minimize the risk and minimize the number of selected shares. As far as we know, only three algorithms of the state-of-the-art of the portfolio selection problem with three objectives have been assessed. In this work, we propose a Fuzzy Multi-objective Particle Swarm Optimization (FOMOPSO) that uses an auto-Tuning Fuzzy Controller. To validate our approach, a series of experiments with the realistic instances and the performance of the proposed algorithm were compared with three state-of-the-art evolutionary algorithms using six commonly used metrics. To support the conclusions, two hypothesis tests were applied. The results show that the Fuzzy Rules auto-configuration contributes to that the proposed algorithm performance clearly outperforms three of the algorithms in comparison, for four of the metrics used.																	1562-2479	2199-3211															10.1007/s40815-020-00928-4		AUG 2020											
J								A hybrid machine learning framework to predict mortality in paralytic ileus patients using electronic health records (EHRs)	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Paralytic ileus; Mortality prediction; Machine learning; MIMIC III database; Statistical feature analysis	CHRONIC INTESTINAL PSEUDOOBSTRUCTION; ACUTE COLONIC PSEUDOOBSTRUCTION; OGILVIES-SYNDROME; SYSTEM	Paralytic ileus (PI) is the pseudo-obstruction of the intestine secondary to intestinal muscle paralysis. Causes of PI include electrolyte imbalances, gastroenteritis (inflammation or infection of the stomach or intestines), overuse of medications, abdominal surgery, etc. Predicting mortality in PI patients hospitalized to ICU is crucial for assessing the severity of illness and adjudicating the value of treatment strategy and resource planning. We have developed a Statistically Robust Machine Learning based Mortality Prediction framework namely SRML-MortalityPredictor that could potentially help intensivists, surgeons, and other medical professionals to carefully plan treatment strategies for critically ill PI patients. We used MIMIC III v1.4, a publicly available ICU database to extract patients data (with age > 18 years old) admitted to the ICU with paralytic ileus (PI) as their primary illness. At phase 1, the SRML-MortalityPredictor framework uses univariate statistical analysis to filter out those risk factors which are not associated with the label of the data. Subsequently, it uses the risk survival statistical methods such as cox-regression and Kaplan-Meier survival analyses. The cox-regression analysis provides the hazard ratio about the potential PI risk factors that later used in conjunction with the Kaplan-Meier analysis to generate a rank order list (highest to lowest risk factors). At phase 2, we used several machine learning classification approaches such as linear discriminant analysis (LDA), Gaussian naive bayes (GNB), decision tree (DT) model, k-nearest neighbor (KNN), and support vector machine (SVM) to find the one with highest predictive power using the rank order features extracted at phase 1. We have evaluated the SRML-MortalityPredictor framework and recorded the accuracy, sensitivity, specificity, receiver operating characteristic (ROC) curves, and area under the curve (AUC) scores for each model. The SRML-MortalityPredictor framework with support vector machine (using RBF kernel) showed better performance and yielded an accuracy score: 81.30% and AUC score: 81.38% while predicting mortality in PI patients. We demonstrated a feasible framework for the mortality risk prediction in PI patients admitted to the ICU. The proposed framework could potentially be helpful for intensivists in clinical decision making. Further research is necessary to incorporate more risk factors associated with PI patients to ensure the adaptability of SRML-MortalityPredictor at the bedside.																	1868-5137	1868-5145															10.1007/s12652-020-02456-3		AUG 2020											
J								Cloud of Things: architecture, applications and challenges	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cloud Computing; Mobile Computing; Cloud of Things; IoT; MCC; CoT issues and challenges; CoT applications	SMART LOGISTICS; MOBILE CLOUD; INTERNET; SYSTEM; INTEGRATION; ISSUES; FOG; IOT	Nowadays, Cloud Computing and Internet of Things (IoT) are amongst the most emerging internet technologies playing a vital role in our daily lives. In the near future, their usage is expected to be increased exponentially that is making them the important technologies of the future internet. Cloud Computing provides access to unlimited capacities and capabilities, such as computation, processing, and storage through a shared group of resources. That can easily be acquired and dynamically assigned to potential IoT based applications. Because, IoT devices and applications are rather deficient in terms of these capabilities. This paper is primarily dedicated to investigating the integration of technologies of Mobile Computing, Cloud Computing and IoT; coined as Cloud of Things (CoT), its characteristics, architectures, potential challenges with possible solutions and applications.																	1868-5137	1868-5145															10.1007/s12652-020-02448-3		AUG 2020											
J								Hybrid ant lion mutated ant colony optimizer technique for Leukemia prediction using microarray gene data	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Microarray data analysis; Ant Colony Optimization (ACO); Ant Lion Optimization (ALO) algorithm; A hybrid ant lion mutated ant colony optimizer technique	CLASSIFICATION; SELECTION; CANCER; DESIGN	The classification of cancers is one of the most vital functions of Microarray data analysis. The classification of the gene expression profile is treated as a NP-Hard problem since it is a very demanding job. Compared to the individual search utilized by conventional algorithms, the population search utilized by Evolutionary Algorithm (EA) is visibly more beneficial. In feasible search areas, EA algorithms also are more likely to detect various optimums instantly. Evolutionary techniques which are inspired by nature perform exceptionally well and are extensively used for Microarray data analysis. Ant Colony Optimization (ACO) is a distinct intelligent optimization algorithm based on iterative optimization which uses ideas like evolution and group. ACO algorithm was developed by studying how ants identify paths while food foraging. Ant Lion Optimization (ALO) algorithm is proposed and employed as muted selection process and the ant lions to hunt process is simulated. A hybrid ant lion mutated ant colony optimizer technique is proposed in this work.																	1868-5137	1868-5145															10.1007/s12652-020-02454-5		AUG 2020											
J								Development of an adaptive neuro fuzzy inference system based vehicular traffic noise prediction model	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Traffic noise; Prediction modelling; Neural networks; Fuzzy inference system; Soft-computing	ENVIRONMENTAL NOISE; HEARING-LOSS; EXPOSURE; NETWORK; POLLUTION; SOUND	Noise pollution has been a global concern among the scientific community as it can cause long term and short-term adverse effects on human health. Vehicular traffic is one of the major causes of noise pollution. In the present work, an efficient methodology to predict the traffic noise level (L(eq)dBA) based upon vehicular traffic volume, percentage of heavy vehicles and average speed of vehicles has been proposed. To predict the noise level, adaptive neuro fuzzy inference system (ANFIS) has been developed and a detailed comparative analysis has been performed with conventional soft-computing techniques such as neural networks (NN), generalized linear model (GLM), random forests (RF), Decision Trees and Support Vector Machine (SVM). Implementation of ANFIS proof-of-concept model on testing data has resulted in higher accuracy for noise level prediction within 0.5 dBA and yielded significantly lower value of root mean square Error as compared to the conventional techniques. The results of current study signify the efficacy of the proposed method in prediction of traffic noise level and validate its suitability in planning mitigation measures for the new and existing roads. In order to analyse the performance of proposed technique, a case study of the highway locations near the city of Patiala in India has been presented.																	1868-5137	1868-5145															10.1007/s12652-020-02431-y		AUG 2020											
J								A comparative study of ANN and ANFIS models for the prediction of cement-based mortar materials compressive strength	NEURAL COMPUTING & APPLICATIONS										Artificial neural networks; Cement; Compressive strength; Metakaolin; Mortar; Artificial intelligence techniques; Adaptive neuro-fuzzy inference system	ARTIFICIAL NEURAL-NETWORK; ULTRASONIC PULSE VELOCITY; FUZZY INFERENCE SYSTEM; STEEL FIBER; SILICA FUME; SHEAR-STRENGTH; FLY-ASH; CONCRETE; METAKAOLIN; BEHAVIOR	Despite the extensive use of mortars materials in constructions over the last decades, there is not yet a reliable and robust method, available in the literature, which can estimate its strength based on its mix parameters. This limitation is due to the highly nonlinear relation between the mortar's compressive strength and the mixed components. In this paper, the application of artificial intelligence techniques toward the prediction of the compressive strength of cement-based mortar materials with or without metakaolin has been investigated. Specifically, surrogate models (such as artificial neural network, ANN and adaptive neuro-fuzzy inference system, ANFIS models) have been developed to the prediction of the compressive strength of mortars trained using experimental data available in the literature. The comparison of the derived results with the experimental findings demonstrates the ability of both ANN and ANFIS models to approximate the compressive strength of mortars in a reliable and robust manner. Although ANFIS was able to obtain higher performance prediction to estimate the compressive strength of mortars compared to ANN model, it was found through the verification process of some other additional data, the ANFIS model has overfitted the data. Therefore, the developed ANN model has been introduced as the best predictive technique for solving problem of the compressive strength of mortars. Furthermore, using the optimum developed model an ambitious attempt to reveal the nature of mortar materials has been made.																	0941-0643	1433-3058															10.1007/s00521-020-05244-4		AUG 2020											
J								Modeling fractional polytropic gas spheres using artificial neural network	NEURAL COMPUTING & APPLICATIONS										Neural network; Stellar structure; Polytropic gas sphere; Fractional Lane-Emden equation; Conformable fractional derivatives	SOLVING DIFFERENTIAL-EQUATIONS; CLASSIFICATION; PREDICTION; APPROXIMATE; DESIGN; STARS	Lane-Emden differential equations describe different physical and astrophysical phenomena that include forms of stellar structure, isothermal gas spheres, gas spherical cloud thermal history, and thermionic currents. This paper presents a computational approach to solve the problems related to fractional Lane-Emden differential equations based on neural networks. Such a solution will help solve the fractional polytropic gas spheres problems which have different applications in physics, astrophysics, engineering, and several real-life issues. We used artificial neural network (ANN) framework in its feed-forward back-propagation learning scheme. The efficiency and accuracy of the presented algorithm are checked by testing it on four fractional Lane-Emden equations and compared with the exact solutions for the polytropic indicesn = 0,1,5 and those of the series expansions for the polytropic indexn = 3. The results we obtained prove that using the ANN method is feasible and accurate and may outperform other methods.																	0941-0643	1433-3058															10.1007/s00521-020-05277-9		AUG 2020											
J								Lean thinking by integrating with discrete event simulation and design of experiments: an emergency department expansion	PEERJ COMPUTER SCIENCE										DES; Lean Healthcare; Design of Experiments; forecasting; expansion; demand	HEALTH-CARE; PHYSICAL-THERAPY; IMPLEMENTATION; METHODOLOGY	Background. Many management tools, such as Discrete Event Simulation (DES) and Lean Healthcare, are efficient to support and assist health care quality. In this sense, the study aims at using Lean Thinking (LT) principles combined with DES to plan a Canadian emergency department (ED) expansion and at meeting the demand that comes from small care centers closed. The project`s purpose is reducing the patients' Length of Stay (LOS) in the ED. Additionally, they must be assisted as soon as possible after the triage process. Furthermore, the study aims at determining the ideal number of beds in the Short Stay Unit (SSU). The patients must not wait more than 180 min to be transferred. Methods. For this purpose, the hospital decision-makers have suggested planning the expansion, and it was carried out by the simulation and modeling method. The emergency department was simulated by the software FlexSim Healthcare (R), and, with the Design of Experiments (DoE), the optimal number of beds, seats, and resources for each shift was determined. Data collection and modeling were executed based on historical data (patients' arrival) and from some databases that are in use by the hospital, from April 1st, 2017 to March 31st, 2018. The experiments were carried out by running 30 replicates for each scenario. Results. The results show that the emergency department cannot meet expected demand in the initial planning scenario. Only 17.2% of the patients were completed treated, and LOS was 2213.7 (average), with a confidence interval of (2131.8-2295.6) min. However, after changing decision variables and applying LT techniques, the treated patients' number increased to 95.7% (approximately 600%). Average LOS decreased to 461.2, with a confidence interval of (453.7-468.7) min, about 79.0%. The time to be attended after the triage decrease from 404.3 min to 20.8 (19.8-21.8) min, around 95.0%, while the time to be transferred from bed to the SSU decreased by 60.0%. Moreover, the ED reduced human resources downtime, according to Lean Thinking principles.																	2376-5992					AUG 10	2020									e284	10.7717/peerj-cs.284													
J								Reconfigurable monitoring for telecommunication networks	PEERJ COMPUTER SCIENCE										Monitoring system; Complex object model; Abstract machine; Telecommunication network	SYSTEM; MODEL; IMPLEMENTATION	This article addresses the monitoring problem of the telecommunication networks. We consider these networks as multilevel dynamic objects. It shows that reconfigurable systems are necessary for their monitoring process in real life. We implement the reconfiguration abilities of the systems through the synthesis of monitoring programs and their execution in the monitoring systems and on the end-user devices. This article presents a new method for the synthesis of monitoring programs and develops a new language to describe the monitoring programs. The programs are translated into binary format and executed by the virtual machines installed on the elements of the networks. We present an example of the program synthesis for real distributed networks monitoring at last.																	2376-5992					AUG 10	2020									e288	10.7717/peerj-cs.288													
J								A methodology for psycho-biological assessment of stress in software engineering	PEERJ COMPUTER SCIENCE										Stress; Software development; Biological markers; Methodology; Psychological assessment; Effects of stress	SALIVARY ALPHA-AMYLASE; HEART-RATE; BLOOD-PRESSURE; WORK STRESS; CORTISOL; RESPONSES; PERFORMANCE; BIOMARKERS	Stress pervades our everyday life to the point of being considered the scourge of the modern industrial world. The effects of stress on knowledge workers causes, in short term, performance fluctuations, decline of concentration, bad sensorimotor coordination, and an increased error rate, while long term exposure to stress leads to issues such as dissatisfaction, resignation, depression and general psychosomatic ailment and disease. Software developers are known to be stressed workers. Stress has been suggested to have detrimental effects on team morale and motivation, communication and cooperation-dependent work, software quality, maintainability, and requirements management. There is a need to effectively assess, monitor, and reduce stress for software developers. While there is substantial psycho-social and medical research on stress and its measurement, we notice that the transfer of these methods and practices to software engineering has not been fully made. For this reason, we engage in an interdisciplinary endeavor between researchers in software engineering and medical and social sciences towards a better understanding of stress effects while developing software. This article offers two main contributions. First, we provide an overview of supported theories of stress and the many ways to assess stress in individuals. Second, we propose a robust methodology to detect and measure stress in controlled experiments that is tailored to software engineering research. We also evaluate the methodology by implementing it on an experiment, which we first pilot and then replicate in its enhanced form, and report on the results with lessons learned. With this work, we hope to stimulate research on stress in software engineering and inspire future research that is backed up by supported theories and employs psychometrically validated measures.																	2376-5992					AUG 10	2020									e286	10.7717/peerj-cs.286													
J								EBL-algebras	SOFT COMPUTING										EBL-algebra; EMV-algebra; BL-algebra; MV-algebra; Ideal	EQUALITY ALGEBRAS	In this paper, we define the notion of EBL-algebras, which are generalizations of BL-algebras and EMV-algebras. The notions of ideals, congruences and filters in EBL-algebras are introduced, and their mutual relationships are investigated. There is a one-to-one correspondence between the set of all ideals in an EBL-algebra and the set of all congruences on an EBL-algebra. Moreover, we give a representation theorem on EBL-algebras. Every proper EBL-algebras under some condition can be embedded into an EBL-algebras with a top element as an ideal.																	1432-7643	1433-7479				OCT	2020	24	19					14333	14343		10.1007/s00500-020-05235-6		AUG 2020											
J								An algorithm for finding approximate Nash equilibria in bimatrix games	SOFT COMPUTING										Nash equilibria (NE); Approximate NE; Fuzzification; Triangular fuzzy number; Possibility distribution	MATRIX GAMES; FUZZY	This paper describes an algorithm for calculating approximately mixed Nash equilibria (NE) in bimatrix games. The algorithm fuzzifies the strategies with normalized possibility distributions. The fuzzification takes advantage of the piecewise linearity of possibility distributions and transforms the NE problem of bimatrix games into a reduced form. The algorithm is guaranteed to find approximate NE in bimatrix games and ensures that the approximate NE is the saddle point of expected payoff functions in the reduced form. The algorithm provides a method of determining how close an approximate NE is to a solution during computation. Numerical results show that the new algorithm is approximately seven-time faster than the Lemke-Howson (LH) algorithm when the game size is 96, and the value of approximation deviation can be as small as 0.1.																	1432-7643	1433-7479															10.1007/s00500-020-05213-y		AUG 2020											
J								Design of variable control charts based on type-2 fuzzy sets with a real case study	SOFT COMPUTING										Control charts; The fuzzy set theory; Type-2 fuzzy sets; x - R; x - S; I - MR	RANGE CONTROL CHARTS; INTERVAL TYPE-2; CONSTRUCTION; SYSTEMS; (X)OVER-BAR; TILDE	Control charts (CCs) are very effective tools to follow process variation and to improve process quality. It is completely critical to increase the sensitiveness and flexibility of CCs to gain a deeper perspective for process control. When constructing conventional CCs, errors may occur due to operator or measuring instruments in performing the measurement. By the way, some data related to CCs can include "uncertainty" or "vagueness" related to process and human evaluations or inspector judgments. We know that classical CCs cannot be able to manage this process. The fuzzy set theory (FST) is one of the most important tools to solve these problems, and the CCs designed based on FST are more usable and preferable for monitoring the process. By the way, one of the extensions of FST named type-2 fuzzy sets that have fuzzy membership degrees is more capable of modeling uncertainty. Therefore, they can be successfully used to the design of the control process in a more flexible and sensitive way. In this paper, the type-2 fuzzy sets have been used to the design of variable control charts to increase the precision and flexibility of them. For this aim, (x) over bar -R , (x) over bar -S and I-MR control charts have been re-designed by using type-2 fuzzy sets. Additionally, these charts have been applied on a real case application from electronic industry, and the obtained results indicate that CCs based on type-2 fuzzy sets can evaluate the process in a more sensitive and flexible way.																	1432-7643	1433-7479															10.1007/s00500-020-05172-4		AUG 2020											
J								AutoSSR: an efficient approach for automatic spontaneous speech recognition model for the Punjabi Language	SOFT COMPUTING										Gaussian mixtures; MFCC; Recognition accuracy; Spontaneous speech; Acoustic model	SYSTEM	In this article, the authors have presented the design and development of automatic spontaneous speech recognition of the Punjabi language. To dimensions up to the natural speech recognizer, the very large vocabulary Punjabi text corpus has been taken from a Punjabi interview's speech corpus, presentations, etc. Afterward, the Punjabi text corpus has been cleaned by using the proposed corpus optimization algorithm. The proposed automatic spontaneous speech model has been trained with 13,218 of Punjabi words and more than 200 min of recorded speech. The research work also confirmed that the 2,073,456 unique in-word Punjabi tri-phoneme combinations present in the dictionary comprise of 131 phonemes. The performance of the proposed model has grown increasingly to 87.10% sentence-level accuracy for 2381 Punjabi trained sentences and word-level accuracy of 94.19% for 13,218 Punjabi words. Simultaneously, the word error rate has been reduced to 5.8% for 13,218 Punjabi words. The performance of the proposed system has also been tested by using other parameters such as overall likelihood per frame and convergence ratio on various iterations for different Gaussian mixtures.																	1432-7643	1433-7479															10.1007/s00500-020-05248-1		AUG 2020											
J								Statistical harmonic summability of sequences of fuzzy numbers	SOFT COMPUTING										Sequences of fuzzy numbers; Statistical convergence; Statistical summability; Tauberian theorems	TAUBERIAN-THEOREMS; CONVERGENCE	In this paper, we extend the concept of statistical harmonic summability from real sequences to fuzzy sequences. We show that if a sequence of fuzzy numbers is bounded and statistically convergent, then it is statistically harmonic summable to same fuzzy number, but not conversely. Our main aim is to establish necessary and sufficient conditions under which statistical convergence follows from statistical harmonic summability.																	1432-7643	1433-7479															10.1007/s00500-020-05151-9		AUG 2020											
J								Unsupervised multi-view representation learning with proximity guided representation and generalized canonical correlation analysis	APPLIED INTELLIGENCE										Unsupervised multi-view representation learning; Proximity guided dynamic routing; Latent specific characteristic; Discrimination representation; Generalized canonical correlation analysis	FEATURES	Multi-view data can collaborate with each other to provide more comprehensive information than single-view data. Although there exist a few unsupervised multi-view representation learning methods taking both the discrepancies and incorporating complementary information from different views into consideration, they always ignore the use of inner-view discriminant information. It remains challenging to learn a meaningful shared representation of multiple views. To overcome this difficulty, this paper proposes a novel unsupervised multi-view representation learning model, MRL. Unlike most state-of-art multi-view representation learning, which only can be used for clustering or classification task, our method explores the proximity guided representation from inner-view and complete the task of multi-label classification and clustering by the discrimination fusion representation simultaneously. MRL consists of three parts. The first part is a deep representation learning for each view and then aims to represent the latent specific discriminant characteristic of each view, the second part builds a proximity guided dynamic routing to preserve its inner features of direction,location and etc. At last, the third part, GCCA-based fusion, exploits the maximum correlations among multiple views based on Generalized Canonical Correlation Analysis (GCCA). To the best of our knowledge, the proposed MRL could be one of the first unsupervised multi-view representation learning models that work in proximity guided dynamic routing and GCCA modes. The proposed model MRL is tested on five multi-view datasets for two different tasks. In the task of multi-label classification, the results show that our model is superior to the state-of-the-art multi-view learning methods in precision, recall, F1 and accuracy. In clustering task, its performance is better than the latest related popular algorithms. And the performance varies w.r.t. the dimensionality ofGis also made to explore the characteristics of MRL.																	0924-669X	1573-7497															10.1007/s10489-020-01821-1		AUG 2020											
J								A context-aware recommender method based on text and opinion mining	EXPERT SYSTEMS										association rules; context-aware recommender systems; opinion mining; text mining; word embedding		A recommender system is an information filtering technology that can be used to recommend items that may be of interest to users. Additionally, there are the context-aware recommender systems that consider contextual information to generate the recommendations. Reviews can provide relevant information that can be used by recommender systems, including contextual and opinion information. In a previous work, we proposed a context-aware recommendation method based on text mining (CARM-TM). The method includes two techniques to extract context from reviews:CIET.5(embed), a technique based on word embeddings; andRulesContext, a technique based on association rules. In this work, we have extended our previous method by includingCEOM, a new technique which extracts context by using aspect-based opinions. We call our extension of CARM-TOM (context-aware recommendation method based on text and opinion mining). To generate recommendations, our method makes use of the CAMF algorithm, a context-aware recommender based on matrix factorization. To evaluate CARM-TOM, we ran an extensive set of experiments in a dataset about restaurants, comparing CARM-TOM against the MF algorithm, an uncontextual recommender system based on matrix factorization; and against a context extraction method proposed in literature. The empirical results strongly indicate that our method is able to improve a context-aware recommender system.																	0266-4720	1468-0394														e12618	10.1111/exsy.12618		AUG 2020											
J								A Creative Computing Approach to Film-story Creation: A Proposed Theoretical Framework	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Creative computing; film-story creation; creativity rules; automatic generation; human-like scriptwriting		The film industry is currently witnessing a severe shortage of good stories and a decline in storytelling art. Meanwhile, creative computing has been employed successfully in the humanities, especially in the fields of art. Seeing the similarities between the process of film-story creation and that of creative computing, we propose a theoretical framework across the two domains, where the exploratory, the combinational and the transformational rules are jointly utilized to generate new ideas and provide potential options in filmstory creation. The framework consists of a film knowledge library, a creative computing system, an evaluation model and an output module. The combination of creative computing and film story creation not only helps to produce novel storylines, shorten the creation cycle, and speed up film industry, but also contributes to the novelty and specificity of interdisciplinary studies.																	1476-8186	1751-8520				OCT	2020	17	5					678	690		10.1007/s11633-020-1238-8		AUG 2020											
J								Rail Detection Based on LSD and the Least Square Curve Fitting	INTERNATIONAL JOURNAL OF AUTOMATION AND COMPUTING										Rail inspection; line segment detector (LSD) algorithm; the least square; curve fitting; foreign object detection		It is necessary to rely on the rail gauge to determine whether the object beside the track will affect train operation safety or not. A convenient and fast method based on line segment detector (LSD) and the least square curve fitting to identify the rail in the image is proposed in this paper. The image in front of the train can be obtained through the camera on-board. After preprocessing, it will be divided equally along the longitudinal axis. Utilizing the characteristics of the LSD algorithm, the edges are approximated into multiple line segments. After screening the terminals of the line segments, it can generate the mathematical model of the rail in the image based on the least square. Experiments show that the algorithm in this paper can fit the rail curve accurately and has good applicability and robustness.																	1476-8186	1751-8520															10.1007/s11633-020-1241-4		AUG 2020											
J								CL-MAX: a clustering-based approximation algorithm for mining maximal frequent itemsets	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Frequent itemset mining; Association rule mining; Partition-based method; MiniBatch K-means; Maximal frequent itemset mining	GENERATION; TREE	The problem of frequent itemset mining is one of the more important problems in data mining which has been extensively employed across a wide range of other relevant tasks such as market basket analysis in marketing, or text analysis in text mining applications. The majority of the deterministic frequent itemset mining algorithms which have been proposed in recent years use some sort or another of an optimal data structures to reduce the overall execution time of the algorithm. In this paper, however, we have tried instead to introduce an approximation algorithm which works by converting the problem into a clustering problem where similar transactions are grouped together. Each cluster centroid represents an itemset which may be assumed to be a candidate frequent itemsets. The validity of this assumption is simply verified by calculating the support count of these itemsets. Those who meet the min-support condition are considered to be an actual frequent itemset. As for the remaining itemsets, they are then passed to MAFIA which extract all maximal frequent itemsets therefrom. Experimentations made on several well-known and diverse datasets show that the proposed algorithm performs almost always faster, and in some cases up to 10 times faster, than the existing deterministic algorithms, and all this by retaining up to 95% of its accuracy.																	1868-8071	1868-808X															10.1007/s13042-020-01177-5		AUG 2020											
J								DetectGAN: GAN-based text detector for camera-captured document images	INTERNATIONAL JOURNAL ON DOCUMENT ANALYSIS AND RECOGNITION										Text detection; Camera-captured document images; Multi-scale context features; Generative adversarial networks	SEGMENTATION	Nowadays, with the development of electronic devices, more and more attention has been paid to camera-based text processing. Different from scene image, the recognition system of document image needs to sort out the recognition results and store them in the structured document for the subsequent data processing. However, in document images, the fusion of text lines largely depends on their semantic information rather than just the distance between the characters, which causes the problem of learning confusion in training. At the same time, for multi-directional printed characters in document images, it is necessary to use additional directional information to guide subsequent recognition tasks. In order to avoid learning confusion and get recognition-friendly detection results, we propose a character-level text detection framework, DetectGAN, based on the conditional generative adversarial networks (abbreviation cGAN used in the text). In the proposed framework, position regression and NMS process are removed, and the problem of text detection is directly transformed into an image-to-image generation problem. Experimental results show that our method has an excellent effect on text detection of camera-captured document images and outperforms the classical and state-of-the-art algorithms.																	1433-2833	1433-2825				DEC	2020	23	4					267	277		10.1007/s10032-020-00358-w		AUG 2020											
J								A bi-objective procedure to deliver actionable knowledge in sport services	EXPERT SYSTEMS										actionable knowledge; business utility; retention intervention; sport services	RETENTION; CUSTOMERS	The increase in retention of customers in gyms and health clubs is nowadays a challenge that requires concrete and personalized actions. Traditional data mining studies focused essentially on predictive analytics, neglecting the business domain. This work presents an actionable knowledge discovery system that uses the following pipeline (data collection, predictive model and retention interventions). In the first step, it extracts and transforms existing real data from databases of the sports facilities. In the second step, predictive models are applied to identify user profiles more susceptible to dropout, where actionable withdrawal rules are based on actionable attributes. Finally, in the third step, based on the previous actionable knowledge, some of the values of the actionable attributes should be changed in order to increase retention. Simulation of scenarios is carried out, with test and control groups, where business utility and associated cost are measured. This document presents a bi-objective study in order to choose the more efficient scenarios.																	0266-4720	1468-0394														e12617	10.1111/exsy.12617		AUG 2020											
J								A comparison between Box-Behnken design and artificial neural network: Modeling of removal of Phenol Red from water solutions by nanocobalt hydroxide	JOURNAL OF CHEMOMETRICS										artificial neural network; Box-Behnken design; nanocobalt hydroxide; Phenol Red	RESPONSE-SURFACE METHODOLOGY; ULTRASONIC-ASSISTED REMOVAL; CENTRAL COMPOSITE DESIGN; MULTIVARIATE OPTIMIZATION; PHOTOCATALYTIC DEGRADATION; MALACHITE GREEN; METHYLENE-BLUE; BINARY-MIXTURE; ALIZARIN RED; ADSORPTION	Here, we describe the successful removal of Phenol Red from aqueous solutions by nanocobalt hydroxide. Also two approaches, artificial neural network (ANN) and Box-Behnken design (BBD), are used to investigate predictive models for simulation and optimization of the dye removal process. The effect of process variables (such as pH, sorbent dosage, and Phenol Red concentration) on the removal efficiency are investigated through performing the BBD. A training set for ANN is obtained using the same design. Statistical values show ANN model is superior to BBD model. Based on the validation data set, ANN model has higher value of coefficient of determination (R-2: 0.99 ANN > 0.84 BBD), lower values of root mean square error of prediction (RMSEP: 3.17 ANN < 3.99 BBD), mean square error (MSE: 10.08 ANN < 15.95 BBD), and relative standard error of prediction (REP: 3.89 ANN < 4.89 BBD). According to obtained results, ANN has better prediction performance in comparison with BBD. In addition, the maximum removal efficiency (%R-max= 99.9) is found at the initial concentration of dye = 68.08 (mg/L), pH = 2.0, and sorbent mass = 9.57 g/L by performing the BBD.																	0886-9383	1099-128X				SEP	2020	34	9							e3283	10.1002/cem.3283		AUG 2020											
J								Densifying Supervision for Fine-Grained Visual Comparisons	INTERNATIONAL JOURNAL OF COMPUTER VISION										Fine-grained; Ranking; Image generation; Relative attributes		Detecting subtle differences in visual attributes requires inferring which of two images exhibits a property more, e.g., which face is smiling slightly more, or which shoe is slightly more sporty. While valuable for applications ranging from biometrics to online shopping, fine-grained attributes are challenging to learn. Unlike traditional recognition tasks, the supervision is inherently comparative. Thus, the space of all possible training comparisons is vast, and learning algorithms face asparsity of supervisionproblem: it is difficult to curate adequate subtly different image pairs for each attribute of interest. We propose to overcome this problem bydensifyingthe space of training images with attribute-conditioned image generation. The main idea is to create synthetic but realistic training images exhibiting slight modifications of attribute(s), obtain their comparative labels from human annotators, and use the labeled image pairs to augment real image pairs when training ranking functions for the attributes. We introduce two variants of our idea. The first passively synthesizes training images by "jittering" individual attributes in real training images. Building on this idea, our second model actively synthesizes training image pairs that would confuse the current attribute model, training both the attribute ranking functions and a generation controller simultaneously in an adversarial manner. For both models, we employ a conditional Variational Autoencoder (CVAE) to perform image synthesis. We demonstrate the effectiveness of bootstrapping imperfect image generators to counteract supervision sparsity in learning-to-rank models. Our approach yields state-of-the-art performance for challenging datasets from two distinct domains.																	0920-5691	1573-1405				NOV	2020	128	10-11			SI		2704	2730		10.1007/s11263-020-01344-9		AUG 2020											
J								On deep ensemble CNN-SAE based novel agro-market price forecasting	EVOLUTIONARY INTELLIGENCE										Deep learning; Convolutional neural network; Stacked autoencoder; Ensemble learning; Spider monkey optimization; Genetic algorithm; Particle swarm optimization; R-2(coefficient of multiple determination)	PREDICTION	The prices of agro-commodities are highly volatile. Hence it is a challenge to the farmers to ensure fair and remunerative prices of these commodities. As a result, there is a need for prediction of agro market price appropriately. The closing price prediction of one soft commodity product, Cotton 29 mm and one agro-commodity product, Guar gum are chosen. The existing reported methods exhibit poor prediction performance. To alleviate this problem, the current investigation is undertaken for better prediction of closing prices. The deep ensemble approach using convolutional neural network (CNN) and stacked autoencoder (SAE) is employed to improve the prediction performance. For the ensemble strategy, the weights are optimized using three bio-inspired techniques such as genetic algorithm (GA), particle swarm optimization (PSO) and spider monkey optimization (SMO). Eighteen attributes relating to the closing price of each products are considered as input to the proposed models. The simulation based experimental results demonstrate the following contribution of the paper. Firstly, it is observed that CNN outperforms the SAE model in terms of short range prediction and vice versa for long range prediction. Secondly, the prediction performance of all the three ensemble models has been determined. Thirdly, out of three ensemble models, ensemble-SMO (ESMO) shows the best prediction performance in terms of mean square error and coefficient of multiple determination (R-2). It is then followed by ensemble-PSO and ensemble-GA respectively. The performance of proposed best ESMO is compared with the Grey wolf optimization based multiquadratic kernel KELM model (GWO-KELM) and it is observed that the proposed ESMO outperforms the GWO-KELM model.																	1864-5909	1864-5917															10.1007/s12065-020-00466-w		AUG 2020											
J								An ensemble topic extraction approach based on optimization clusters using hybrid multi-verse optimizer for scientific publications	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Ensemble method; Topic extraction; Scientific publications clustering; Multi-verse optimizer; Hybridization; K-means	METAHEURISTIC ALGORITHM; KEYWORD EXTRACTION; KRILL HERD; PERFORMANCE; PARAMETERS; SEARCH	For text document clustering (TDC), a novel hybrid of the multi-verse optimizer (MVO) algorithm and k-means (also called H-MVO) are proposed in this work. Moreover, a new ensemble method for an automatic topic extraction (TE) has been proposed in this paper, from a set of scientific publications in the form of text documents with the purpose of extracting topics from clustered documents. Often, the existing TE methods draw upon the statistical theory. However, the results might be different when the same clustered document is utilized. Consequently, there can be imprecise results, which are related to the extracted topics from the clustered documents owing to the behavior of the TE methods. As a result, the vigorous characteristics of the TE methods are ensembled, thereby empowering the accuracy of the extracted topics. The results, which were yielded by H-MVO for TDC, were compared against 14 well-regarded methods, involving five clustering methods, in addition to seven metaheuristic algorithms, as well as two hybrid optimization algorithms. Also, the results, which were generated by the introduced ensembled TE method, were compared against those, which were produced by five established statistical methods in the literature. As a result, the findings revealed that the suggested ensembled TE method outperformed the entire comparative methods, thereby utilizing all the external measurements for almost the entire datasets. Moreover, the new method can complement the advantages of the five previously proposed methods. Accordingly, more advanced results were obtained.																	1868-5137	1868-5145															10.1007/s12652-020-02439-4		AUG 2020											
J								Spatial and temporal analysis of flood hazard assessment of Cuddalore District, Tamil Nadu, India. Using geospatial techniques	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Flood; Cuddalore; Flood prediction; Remote sensing; NDVI; NDWI; NDBI	LAND-COVER CHANGE; DIFFERENCE WATER INDEX; NDVI TIME-SERIES; VEGETATION; DYNAMICS; URBAN; CLASSIFICATION; VARIABILITY; NDWI; PARK	Flood is a natural calamity and causes damage of life and property devastation. The main objective of this study was to analyse flood hazard and inundation area mapping of Paravanar sub-Basin. Flood generating factors, like elevation, slope, drainage density, rainfall, soil type and land use were analyzed and delineated flood zones using Geospatial techniques and multi criteria evaluation. The flood generating factor was computed by data integration, floodplain depiction, mapping, and evaluation. Various parameters such as Normalized difference vegetation index (NDVI), Normalized difference build-up index (NDBI), Normalized difference water index (NDWI) were acquired for performing Regression Analysis. This information can be further utilized for hydro-meteorological figures and Surge forecast. These data can be used in building hazard and risk maps for pre-flood prediction. The flood hazard threats in the southeast and in between southeast and west part of the Paravanar River basin are high flood hazard threat zone.																	1868-5137	1868-5145															10.1007/s12652-020-02415-y		AUG 2020											
J								Novel medical image encryption using DWT block-based scrambling and edge maps	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Medical image; Discrete wavelet transform; Edge map; Encryption; Scrambling	CONTRAST-ENHANCEMENT	This paper proposes the discrete wavelet transform (DWT) block-based scrambling algorithm used for medical image encryption which applies the edge maps extracted from a source image. The method comprises three major stages: DWT-plane decomposition, generation of edge map sequences, and DWT-level scrambling. In the first stage, the original medical images are decomposed into numerous DWT-planes. In the second stage, the deriche edge detector method has been presented to estimate the edge maps, which must be a similar size to the DWT-bit scales. The DWT-level scrambling has been used to isolate the neighboring pixels into various rows and columns, thus it weakens the strong correlation among the neighboring pixels efficiently. This proposed method shows the Number of Pixel Change Rate as 99.592% and Unified Average Hanged Intensity of 34.268%. Furthermore, simulations and security analysis shows strong resistance to different security attacks and perform better other conventional methods.																	1868-5137	1868-5145															10.1007/s12652-020-02399-9		AUG 2020											
J								Q-rung orthopair fuzzy multiple attribute group decision-making method based on normalized bidirectional projection model and generalized knowledge-based entropy measure	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										q-rung orthopair fuzzy sets; Multiple attribute group decision-making method; Normalized bidirectional projection model; Generalized p-norm knowledge-based measure	SIMILARITY MEASURE; SETS; INFORMATION; TOPSIS	The q-rung orthopair fuzzy sets (q-ROFSs) can serve as a generalization of intuitionistic fuzzy sets (IFSs) and Pythagorean fuzzy sets (PFSs). q-ROFSs provide more freedom for decision makers in describing their opinions than other ordinary orthopair fuzzy sets. In this paper, a novel multiple attribute group decision making(MAGDM) method is constructed under q-rung orthopair fuzzy (q-ROF) environment. First, considering the projection measure provides the distance and the angle between two alternatives simultaneously, this work investigates a new normalized bidirectional projection model (NBPM) of q-ROFSs. By combining the proposed NBPM with Jaynes maximum entropy method, a nonlinear programming model is constructed to calculate the objective attribute weight information. Second, we present a new entropy measure based on the proposed generalized p-norm knowledge-based measure which takes into account both the membership and non-membership functions and the inherent fuzziness of q-ROFSs. Then the weights of decision makers are given by the proposed entropy measure. Furthermore, an integrated MAGDM framework is presented by using the weight determination methods of decision makers and attributes under q-ROF environment. Finally, an illustrative example is given to illustrate the operation process of the proposed decision-making method, sensitivity analysis and comparison analysis are also performed to show the effectiveness and superiority of the proposed method.																	1868-5137	1868-5145															10.1007/s12652-020-02433-w		AUG 2020											
J								Domain-adaptive intelligence for fault diagnosis based on deep transfer learning from scientific test rigs to industrial applications	NEURAL COMPUTING & APPLICATIONS										Intelligent fault diagnosis; Rolling element bearing; Transfer learning; SE-ResNet; Domain adaption	CONVOLUTIONAL NEURAL-NETWORK; BEARING	With the accumulation of data, the intelligent fault diagnosis of rolling bearings has achieved fruitful results, but it is costly to acquire and label data for industrial application. A series of studies achieve the reuse of knowledge from bearings in scientific test rigs (BSTRs) to bearings in industrial applications (BIAs) via transfer learning. Nevertheless, most of the cases ignore a constraint: the industrial datasets suffers from class imbalance. In general, industrial datasets lack data samples of fault states. To this end, we propose a pseudo-categorized maximum mean discrepancy (PCMMD), and use it to drive the multi-input multi-output convolutional network (MIMOCN) to narrow the cross-domain distribution discrepancy of various categories in the deep feature space. Firstly, the domain-shared encoder and classifier are pre-trained based on the source-domain labeled dataset. Then, the labeled source-domain data and unlabeled target-domain data are jointly used to train the MIMOCN. The pseudo-labels enable the PCMMD to measure the intra-class cross-domain distribution discrepancy. The target-domain reconstruction object function improves the credibility of target-domain deep feature. The transfer cases from artificial damaged BSTRs to real damage BSTRs verify the stability of the proposed PCMMD. In the case of transfer learning from balanced BSTRs to BIAs lacking fault samples, the diagnosis accuracy of BIAs is higher than that of existing methods.																	0941-0643	1433-3058															10.1007/s00521-020-05275-x		AUG 2020											
J								A novel density-based adaptiveknearest neighbor method for dealing with overlapping problem in imbalanced datasets	NEURAL COMPUTING & APPLICATIONS										Nearest neighbor classification; Imbalanced datasets; Overlapping problem; Density-based method	CLASSIFICATION; REPRESENTATION; SMOTE	Although a large number of solutions have been proposed to handle imbalanced classification problems over past decades, many researches pointed out that imbalanced problem does not degrade learning performance by its own but together with other factors. One of these factors is the overlapping problem which plays an even larger role in the classification performance deterioration but is always ignored in previous study. In this paper, we propose a density-based adaptiveknearest neighbor method, namely DBANN, which can handle imbalanced and overlapping problems simultaneously. To do so, a simple but effective distance adjustment strategy is developed to adaptively find the most reliable query neighbors. Concretely, we first partition training data into six parts by density-based method. Next, for each part, we modify distance metric by considering both local and global distribution. Finally, output is made by the query neighbors selected in the new distance metric. Noticeably, the query neighbors of DBANN are adaptively changed according to the degree of imbalance and overlap. To show the validity of our proposed method, experiments are carried out on 16 synthetic datasets and 41 real-world datasets. The results supported by the proper statistical tests show that our proposed method significantly outperforms the state-of-the-art methods.																	0941-0643	1433-3058															10.1007/s00521-020-05256-0		AUG 2020											
J								Out of the laboratory and into the classroom: the future of artificial intelligence in education	AI & SOCIETY										Artificial intelligence; Education technology; Social implications of technology; Educational agents; Responsible research and innovation	TUTORING SYSTEMS; LEARNING TECHNOLOGIES; CHALLENGES; REVOLUTION; ARTIFACTS; EVOLUTION; TEACHERS; SCIENCE; DESIGN; TRENDS	Like previous educational technologies, artificial intelligence in education (AIEd) threatens to disrupt the status quo, with proponents highlighting the potential for efficiency and democratization, and skeptics warning of industrialization and alienation. However, unlike frequently discussed applications of AI in autonomous vehicles, military and cybersecurity concerns, and healthcare, AI's impacts on education policy and practice have not yet captured the public's attention. This paper, therefore, evaluates the status of AIEd, with special attention to intelligent tutoring systems and anthropomorphized artificial educational agents. I discuss AIEd's purported capacities, including the abilities to simulate teachers, provide robust student differentiation, and even foster socio-emotional engagement. Next, to situate developmental pathways for AIEd going forward, I contrast sociotechnical possibilities and risks through two idealized futures. Finally, I consider a recent proposal to use peer review as a gatekeeping strategy to prevent harmful research. This proposal serves as a jumping off point for recommendations to AIEd stakeholders towards improving their engagement with socially responsible research and implementation of AI in educational systems.																	0951-5666	1435-5655															10.1007/s00146-020-01033-8		AUG 2020											
J								A texture feature based approach for person verification using footprint bio-metric	ARTIFICIAL INTELLIGENCE REVIEW										Footprint biometrics; Texture feature; Newborn identification; Personal recognition; Ensemble subspace discriminant	PALMPRINT RECOGNITION; IDENTIFICATION; IMAGES; EAR	Biometrics is the study of unique characteristics present in the human body such as fingerprint, palm-print, retina, iris, footprint, etc. While other traits have been explored widely, only a few people have been considered the foot-palm region, despite having unique properties. Prior work has explored the foot shape features using length, width, major axis, minor axis, centroid, etc. but they are not reliable for personal verification due to similarity in the physical composition of two persons. It increases the demand for more unique features based on the footprint. Footprint texture features coming from creases of foot palm are unique and permanent like palmprint texture features. Hence the main objective of the paper is to investigate various kinds of texture feature techniques. These techniques will be further used in correct extraction of footprint features. After extraction of footprint features a detailed experimental analysis is performed to discover the uniqueness in foot texture. It is further utilized to test its viability as a human recognition trait. We describe a detailed feature extraction and classification technique applied to a collected footprint data-set. For feature extraction, we use three techniques: Gray Level Co-occurrence Matrix (GLCM), Histogram Oriented Gradient (HOG), and Local Binary Patterns (LBP). Feature classification is performed using four techniques: Linear Discriminant Analysis (LDA), Support Vector Machine (SVM), K-Nearest Neighbor (KNN), and Ensemble Subspace Discriminant (ESD).GLCMprovides less accuracy, while HOG generates a big feature vector which takes more execution time. LBP provides a trade-off between the accuracy and the execution time. Detailed quantitative experiments show:GLCM with LDA provides an accuracy of 88.5%, HOG with Fine-KNN achieves 86.5% accuracy and LBP with LDA achieves the accuracy of 97.9%.																	0269-2821	1573-7462															10.1007/s10462-020-09887-6		AUG 2020											
J								Plug-and-play supervisory control using muscle and brain signals for real-time gesture and error detection	AUTONOMOUS ROBOTS										Human-robot interaction; EMG control; EEG control; Hybrid control; Gesture detection; Error-related potentials; Plug-and-play supervisory control	HUMAN-MACHINE INTERFACE; EMG-BASED CONTROL; COMPUTER INTERFACES; EXOSKELETON; POTENTIALS; EEG; BCI; SELECTION; HYBRID; MODEL	Effective human supervision of robots can be key for ensuring correct robot operation in a variety of potentially safety-critical scenarios. This paper takes a step towards fast and reliable human intervention in supervisory control tasks by combining two streams of human biosignals: muscle and brain activity acquired via EMG and EEG, respectively. It presents continuous classification of left and right hand-gestures using muscle signals, time-locked classification of error-related potentials using brain signals (unconsciously produced when observing an error), and a framework that combines these pipelines to detect and correct robot mistakes during multiple-choice tasks. The resulting hybrid system is evaluated in a "plug-and-play" fashion with 7 untrained subjects supervising an autonomous robot performing a target selection task. Offline analysis further explores the EMG classification performance, and investigates methods to select subsets of training data that may facilitate generalizable plug-and-play classifiers.																	0929-5593	1573-7527				SEP	2020	44	7			SI		1303	1322		10.1007/s10514-020-09916-x		AUG 2020											
J								Using multiple classifier behavior to develop a dynamic outlier ensemble	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Outlier detection; Multiple classifier system; Dynamic classifier selection; Machine learning	IMBALANCED DATA; SELECTION; SYSTEMS	Outlier ensembles that use more base detectors recently become an attractive approach to solving problems of single detectors. However, existing outlier ensembles often assume that base detectors make independent errors, which is difficult to satisfy in practical applications. To this end, this paper proposes a dynamic outlier ensemble to loose this error independence assumption. In our method, it is desired that the most competent base detector(s) can be singled out by the dynamic selection mechanism for each test pattern. The usage of the concept of multiple classifier behavior (MCB) has two purposes. One is to generate artificial outlier examples used for competence estimates. This strategy is different from other methods since we do not make any assumption regarding the data distribution. On the other hand, MCB is used to refine validation sets initialized by the K-nearest neighbors (KNN) rule. It is desired that objects in the refined validation sets are more representative than those found by KNN. With the refined validation sets, competences of all base detectors will be estimated by a probabilistic method, before which we have transformed outputs of base detectors into a probabilistic form. Finally, a switching mechanism that determines whether one detector should be nominated to make the decision or a fusion method should be applied instead is proposed in order to achieve a robust detection result. We carry out experiments on 20 benchmark data sets to verify the effectiveness of our detection method.																	1868-8071	1868-808X															10.1007/s13042-020-01183-7		AUG 2020											
J								Real-time cheating immune secret sharing for remote sensing images	JOURNAL OF REAL-TIME IMAGE PROCESSING										Remote sensing images; Multicore environment; Parallel processing; Meaningful shares; Real-time processing; Cheating immune; Statistical image processing	VISUAL CRYPTOGRAPHY	To observe the earth surface and its atmospheric interaction, various advanced optical and radar sensors are utilized. This observation returns a huge amount of optical multidimensional remote sensing images which may be used in multidisciplinary fields. The processing of these images in real time is a challenging task because of their high spatial resolution and complex data structure. At the same time, these images are quite confidential in various applications such as in the military and intelligence sectors. For secretly transmitting the remote sensing images in real time, a real-time cheating immune secret sharing approach is introduced in this paper. The proposed approach minimizes the time as well as space complexity for the secret sharing effectively. It also generates meaningful shares without the restriction for any fixed number participants. Generated shares by the proposed approach are cheating immune. That means they can authenticate themselves if tampered with. Experimental results show the effectiveness of the proposed approach.																	1861-8200	1861-8219															10.1007/s11554-020-01005-7		AUG 2020											
J								A Dynamic Programming Framework for Large-Scale Online Clustering on Graphs	NEURAL PROCESSING LETTERS										Online graph clustering; Large-scale graphs; supernodes; Running time; Efficiency	COMMUNITY DETECTION; ALGORITHM	As a fundamental technique for data analysis, graph clustering grouping graph data into clusters has attracted great attentions in recent years. In this paper, we presentDPOCG, a dynamic programming framework for large-scale online clustering on graphs, which improves the scalability of a wide range of graph clustering algorithms. Specifically,DPOCGfirst identifies the nodes whose states are unchanged compared with the states at the previous time on a large-scale graph, then constructs these unchanged nodes as supernodes, which greatly reduces the size of the graph at the current time, and collapses nodes whose degrees are less than a predefined threshold. Based on our density-based graph clustering algorithm (DGCM),DPOCGpartitions the reduced graph into clusters. In addition, we theoretically analyzeDPOCGin terms of supernode generation, clustering on reduced graph, and computational complexity. We evaluateDPOCGon a synthetic dataset and seven real-world datasets, respectively, and the experimental results show thatDPOCGconsumes less running time and improves the efficiency of clustering.																	1370-4621	1573-773X				OCT	2020	52	2			SI		1613	1629		10.1007/s11063-020-10329-1		AUG 2020											
J								A multi-agent-based algorithm for data clustering	PROGRESS IN ARTIFICIAL INTELLIGENCE										Data clustering; Multi-agent systems; Cluster validation	VALIDATION	Clustering algorithms aim to detect groups based on similarity, from a given set of objects. Many clustering techniques have been proposed, most requiring the user to set critical parameters, such as the number of groups. This work presents the implementation and evaluation of a clustering algorithm based on a multi-agent system, which automatically detects the number of groups and the group labels for a given dataset. Groups formed during the clustering process emerge as patterns from the interaction among agents. The proposed algorithm is experimentally validated over benchmark datasets from the literature. The quality of clustering results is computed using seven internal indexes and one external index. Under this methodology, the proposed algorithm is compared to K-means and DBSCAN (density-based spatial clustering of applications with noise).																	2192-6352	2192-6360															10.1007/s13748-020-00213-3		AUG 2020											
J								Hybrid deep neural network with adaptive galactic swarm optimization for text extraction from scene images	SOFT COMPUTING										Text extraction; Guided image filter; Watershed segmentation; Naive Bayes; Deep neural network (DNN); Emperor penguin optimization (EPO)	SEGMENTATION; VERIFICATION; RECOGNITION	Text obtained in natural scenes contains various information; therefore, it is extensively used in various applications to understand the image scenarios and also to retrieve the visual information. The semantic information provided by this scene image is very much valuable for human beings to realize the whole environment. But the text in such natural images depicts a flexible appearance in an unconstrained environment which makes the text identification and character recognition process a more challenging one. Therefore, a weighted naive Bayes classifier (WNBC)-based deep learning process is used in this framework to effectively detect the text and to recognize the character from the scene images. Normally, the natural scene images may carry some kind of noise in it, and to remove that, the guided image filter is introduced at the pre-processing stage. The features that are useful for the classification process are extracted using the Gabor transform and stroke width transform techniques. Finally, with these extracted features, the text detection and character recognition is successfully achieved by WNBC and deep neural network-based adaptive galactic swarm optimization. Then, the performance metrics such as accuracy, F1-score, precision, mean absolute error, mean square error and recall metrics are evaluated to estimate the adeptness of the proposed method.																	1432-7643	1433-7479															10.1007/s00500-020-05245-4		AUG 2020											
J								Advanced spatial network metrics for cognitive management of 5G networks	SOFT COMPUTING										5G networks; Topology management; Spatial network metrics; Cognitive management	CENTRALITY	The emerging fifth-generation (5G) mobile networks are empowered by softwarization and programmability, leading to the huge potentials of unprecedented flexibility and capability in cognitive network management such as self-reconfiguration and self-optimization. To help unlock such potentials, this paper proposes a novel framework that is able to monitor and calculate 5G network topological information in terms of advanced spatial metrics. These metrics, together with enabling and optimization algorithms, are purposely designed to address the complexity of 5G network topologies introduced by network virtualization and infrastructure sharing among operators (multi-tenancy). Consequently, this new framework, centred on a topology monitoring agent (TMA), enables on-demand 5G networks' spatial knowledge and topological awareness required by 5G cognitive network management in making smart decisions in various autonomous network management tasks including but not limited to virtual network function placement strategies. The paper describes several technical use cases enabled by the proposed framework, including proactive cache allocation, computation offloading, node overloading alerting, and load balancing. Finally, a realistic 5G testbed is deployed with the central component TMA, together with the new spatial metrics and associated algorithms, implemented. Experimental results empirically validate the proposed approach and demonstrate the scalability and performance of the TMA component.																	1432-7643	1433-7479															10.1007/s00500-020-05132-y		AUG 2020											
J								Mining top-rank-k frequent weighted itemsets using WN-list structures and an early pruning strategy	KNOWLEDGE-BASED SYSTEMS										Data mining; Pattern mining; Frequent weighted itemsets; N-list structure; Top-rank-k pattern	EFFICIENT ALGORITHMS; N-LIST; ERASABLE ITEMSETS; ASSOCIATION RULES; CLOSED ITEMSETS; PATTERNS	Frequent weighted itemsets (FWIs) are a variation of frequent itemsets (FIs) that take into account the different importance or weights for each item. Many algorithms have been introduced for mining FWIs recently. However, the traditional algorithms for mining FWIs produce a large number of FWIs which causes difficulties when applied with intelligent systems. Therefore, this study first introduces the problem of mining top-rank-k FWIs from weighted databases that combines the mining and ranking phases into one without finding all FWIs to increase their usability in practical applications. As the second contribution, three baseline algorithms for mining top-rank-k FWIs, namely TFWIT, TFWID and TFWIN that use state-of-the-art data structures, namely tidset, diffset and WN-list structures, are developed. Next, this study proposes the threshold raising strategy and the early pruning strategy supported by a new theorem to effectively mine top-rank-k FWIs. An improved version of TFWIN named TFWIN+ employs these strategies to improve the performance of mining top-rank-k FWIs and is more efficient when compared to the original version. Finally, the empirical evaluations in terms of processing time and memory usage among these algorithms were conducted to show the effectiveness of TFWIN+. The experimental results show that TFWIN+ outperforms TFWIT, TFWID and TFWIN for mining top-rank-k FWIs. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				AUG 9	2020	201								106064	10.1016/j.knosys.2020.106064													
J								DAM: Transformer-based relation detection for Question Answering over Knowledge Base	KNOWLEDGE-BASED SYSTEMS										Knowledge Base Question Answering; Transformer; Relation detection	WORD EMBEDDINGS	Relation Detection is a core component of Knowledge Base Question Answering (KBQA). In this paper, we propose a Transformer-based deep attentive semantic matching model (DAM), to identify the KB relations corresponding to the questions. The DAM is completely based on the attention mechanism and applies the fine-grained word-level attention to enhance the matching of questions and relations. On the basis of the DAM, we build a three-stage KBQA pipeline system. The experimental results on multiple benchmarks demonstrate that our DAM model outperforms previous methods on relation detection. In addition, our DAM-based KBQA system also achieves state-of-the-art results on multiple datasets. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				AUG 9	2020	201								106077	10.1016/j.knosys.2020.106077													
J								Random multi-scale kernel-based Bayesian distribution regression learning	KNOWLEDGE-BASED SYSTEMS										Distribution regression; Kernel mean embedding; Multi-scale kernel; Bayesian inference	CORRENTROPY; NETWORKS	The effective embedding estimation of distribution and the construction of regression model with strong representation ability are two key problems of distribution regression. This paper proposes a random multi-scale kernel-based Bayesian distribution regression (RMK-BDR) learning framework. Vector-valued kernel mean embedding (KME) estimators with a same dimension which is chosen adaptively to the data are introduced in the first stage of distribution regression learning. Then, a linear combination of multi-scale Gaussian kernels with different scale parameters randomly sampled from a predefined distribution is used as the regression model. Sparsity priors are added on those linear combination weights. Under the Bayesian inference theory, a prediction distribution of the response variable is obtained. A series of experiment results verify the effectiveness of the proposed algorithm. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				AUG 9	2020	201								106073	10.1016/j.knosys.2020.106073													
J								Uncovering overlapping community structure in static and dynamic networks	KNOWLEDGE-BASED SYSTEMS										Overlapping community detection; Density-based seeding method; Cluster influence; Static networks; Dynamic networks		Community detection is an important research area in complex networks, for which the existing methods are often inaccurate or inefficient (1) at dealing with large real networks, (2) at dealing with dynamic networks. In this paper, we propose DIS, a localized algorithm for uncovering overlapping community structure in real large-scale networks, and ADIS, an adaptive community update method for dynamic networks. Experiments in large-scale real-world networks demonstrate that DIS achieves competitive performance among the baselines, in particular, DIS is over 100x faster than the global algorithms with better quality, and it obtains much more accurate communities than the local algorithms without utilizing priori information. Experiments in dynamic networks demonstrate that ADIS achieves competitive community structure compared to other dynamic methods. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				AUG 9	2020	201								106060	10.1016/j.knosys.2020.106060													
J								Solving combinatorial optimization problems with single seekers society algorithm	KNOWLEDGE-BASED SYSTEMS										Algorithmic coalition; Single seekers society algorithm; Combinatorial optimization; Vehicle routing problems; Scheduling problems	VEHICLE-ROUTING PROBLEM; HYBRID GENETIC ALGORITHM; TABU SEARCH ALGORITHM; SIMULTANEOUS PICKUP; SIMULTANEOUS DELIVERY; SCHEDULING PROBLEMS; FLOW-SHOP; HEURISTICS	The single seekers society (SSS) algorithm is a recently developed meta-heuristic algorithm for solving complex continuous optimization problems. The aim of this paper is to adapt the SSS algorithm to handle combinatorial optimization problems. As the original SSS algorithm does, the combinatorial SSS algorithm also brings several single-solution based search algorithms together while making them to communicate through an information mechanism based on the superposition principle and reproduction procedure. Therefore, the algorithmic logic remains the same for the combinatorial SSS algorithm; however, some components are modified to suit combinatorial problems. Performance of the combinatorial SSS algorithm is tested on the well-known combinatorial optimization problems such that the vehicle routing problem with simultaneous pickup and delivery, the vehicle routing problem with mixed pickup and delivery, the flow shop scheduling problem, and the job shop scheduling problem. This paper also compares the SSS algorithm against different solution approaches in the related literature on routing and scheduling problems. Experimental results indicate that the SSS algorithm has satisfactory performance and high capability in solving combinatorial optimization problems. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				AUG 9	2020	201								106036	10.1016/j.knosys.2020.106036													
J								Integrated production planning and scheduling under uncertainty: A fuzzy bi-level decision-making approach	KNOWLEDGE-BASED SYSTEMS										Fuzzy bi-level decision making; Production planning; Scheduling; Particle swarm optimization	GENETIC ALGORITHM; DECOMPOSITION; MODELS	Production planning and scheduling are two core decision layers, constrained and affected by one another in manufacturing systems. Owing to different time scales and objectives, planning and scheduling are often separately handled in a sequential way, which frequently results in infeasible or suboptimal solutions. Moreover, uncertain issues, e.g. the fuzzy startup time of a machine and the fuzzy processing time for a task, are inherent to manufacturing systems due to mechanized and/or man-made factors. Motivated by these challenges, this paper aims to develop fuzzy bi-level decision-making techniques to handle integrated planning and scheduling problems in the fuzzy manufacturing system. First, the integrated problem is formulated into a fuzzy bi-level decision model in which solving the higher-level planning problem has to take into account lower-level implicit scheduling reactions in advance. Second, a hybrid solution method is developed to solve the resulting bi-level decision model, in which a particle swarm optimization (PSO) algorithm is applied to update planning decisions, and then, in view of each given planning decision, a heuristic algorithm is presented to find an optimal schedule under fuzzy manufacturing conditions. Lastly, a set of computational study is constructed to demonstrate the effectiveness of the proposed fuzzy bi-level decision-making techniques. Compared with existing works, they can find better planning decisions fulfilled by schedules and perform much better in terms of computational efficiency. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				AUG 9	2020	201								106056	10.1016/j.knosys.2020.106056													
J								Modeling relation paths for knowledge base completion via joint adversarial training	KNOWLEDGE-BASED SYSTEMS										Joint adversarial training; Hierarchical attention mechanism; Knowledge base completion		Knowledge Base Completion (KBC), which aims at determining the missing relations between entity pairs, has received increasing attention in recent years. Most existing KBC methods focus on either embedding the Knowledge Base (KB) into a specific semantic space or leveraging the joint probability of Random Walks (RWs) on multi-hop paths. Only a few unified models take both semantic and path-related features into consideration with adequacy. In this paper, we propose a novel method to explore the intrinsic relationship between the single relation (i.e. 1-hop path) and multi-hop paths between paired entities. We use Hierarchical Attention Networks (HANs) to select important relations in multi-hop paths and encode them into low-dimensional vectors. By treating relations and multi-hop paths as two different input sources, we use a feature extractor, which is shared by two downstream components (i.e. relation classifier and source discriminator), to capture shared/similar information between them. By joint adversarial training, we encourage our model to extract features from the multi-hop paths which are representative for relation completion. We apply the trained model (except for the source discriminator) to several large-scale KBs for relation completion. Experimental results show that our method outperforms existing path information-based approaches. Since each sub-module of our model can be well interpreted, our model can be applied to a large number of relation learning tasks. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				AUG 9	2020	201								105865	10.1016/j.knosys.2020.105865													
J								Online job scheduling for distributed machine learning in optical circuit switch networks	KNOWLEDGE-BASED SYSTEMS										Distributed machine learning (DML); Optical circuit switch (OCS); Online job scheduling; Weighted Job Completion Time (WJCT)	MINIMIZE	Networking has become a well-known performance bottleneck for distributed machine learning (DML). Although lots of works have focused on accelerating the communication process of DML, they ignore the impact of the physical network on the DML performance. Concurrently, optical circuit switches (OCSes) are increasingly applied in data centers and clusters, which can fundamentally improve DML performance. It is worth noting that the non-negligible OCS reconfiguration delay makes OCS scheduling algorithms have a great impact on the upper application performance. However, existing OCS scheduling solutions are not suitable for DML jobs due to the iterative nature of DML jobs and their interleaving characteristics of communication and computation stages. Therefore, in this paper, we study the online multi-job scheduling for DML in OCS networks. Firstly, we propose heaviest-load-first (HLF), a heuristic algorithm for intra-job scheduling, which is based on the fact that the completion time of flows on the heaviest load port has a significant impact on the job completion time. Furthermore, we present Shortest Weighted Remaining Time First (SWRTF) algorithm for inter-job scheduling. In SWRTF, an available DML job is scheduled when the served job moves from communication stage to the computation stage, which significantly improves the circuit utilization. Based on large-scale simulations, we demonstrate HLF can significantly reduce the iteration communication time by up to 64.97% compared to the state-of-the-art circuit scheduler Sunflow. Besides, SWRTF can save up to 42.9%, 54.2%, 27.2% of Weighted-Job-Completion-Time (WJCT) compared to Shortest-Job-First, Baraat and Weighted-First inter-job scheduling algorithms, respectively. (C) 2020 Published by Elsevier B.V.																	0950-7051	1872-7409				AUG 9	2020	201								106002	10.1016/j.knosys.2020.106002													
J								Sparse neighbor constrained co-clustering via category consistency learning	KNOWLEDGE-BASED SYSTEMS										Machine learning; Co-clustering; Nonnegative matrix factorization; Category consistency; Neighbor constraint; Dual regularization	NONNEGATIVE MATRIX FACTORIZATION; TRI-FACTORIZATION	Clustering has long been an enduring and promising task in machine learning. However, developed one-side clustering is still insufficient to explore the context of data, such as texts and genes. Hence, developing two-way clustering has drawn more attention in recent years, which tends to cluster samples and features simultaneously. This paper proposes a sparse neighbor constrained co-clustering via category consistency learning, for alleviating the misclassification of close points. Following an additional observation, samples often fall into the same category as their neighbors, as do features. Accordingly, the co-clustering problem is formulated as nonnegative matrix tri-factorization appended dual regularizers, considering coherence between data affinity and label assignment. Then, a multiplicative alternating scheme is raised for objective optimization, whose convergence and correctness are theoretically guaranteed. Furthermore, the proposed approach is validated on six datasets using three evaluation metrics, whose parameter sensitivity is analyzed as well. Finally, comprehensive experiments show that our algorithm is competitive against existing ones. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				AUG 9	2020	201								105987	10.1016/j.knosys.2020.105987													
J								A novel meta-matching approach for ontology alignment using grasshopper optimization	KNOWLEDGE-BASED SYSTEMS										Ontology matching; Grasshopper optimization algorithm; Ontology alignment evaluation initiative	ALGORITHM	Ontology alignment is a fundamental task to support information sharing and reuse in heterogeneous information systems. Optimizing the combination of matchers by evolutionary algorithms to align ontology is an effective method. However, such methods have two significant shortcomings: weights need to be set manually to combine matchers, and a reference alignment is required during the optimization process. In this paper, a meta-matching approach GSOOM for automatically configuring weights and threshold using grasshopper optimization algorithm (GOA) has been proposed. In this approach, the ontology alignment problem is modeled as optimizing individual fitness of GOA. A fitness function is proposed, which includes two goals: maximizing the number of matching and the similarity score. Since it does not require an expert to provide a reference alignment, it is more suitable for real-world scenarios. To demonstrate the advantages of the approach, we conduct exhaustive experiments tasks on several standard datasets and compare its performance to other state-of-the-art methods. The experimental results illustrate that our approach is more efficiently and is significantly superior to other metaheuristic-based methods. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				AUG 9	2020	201								106050	10.1016/j.knosys.2020.106050													
J								Robust orthogonal nonnegative matrix tri-factorization for data representation	KNOWLEDGE-BASED SYSTEMS										Nonnegative matrix tri-factorization; Correntropy; Orthogonality constraint; Outliers	CORRENTROPY; MINIMIZATION; ALGORITHM	Nonnegative matrix factorization (NMF) has been a vital data representation technique, and has demonstrated significant potential in the field of machine learning and data mining. Nonnegative matrix tri-factorization (NMTF) is an extension of NMF, and provides more degrees of freedom than NMF. In this paper, we propose the correntropy based orthogonal nonnegative matrix tri-factorization (CNMTF) algorithm, which is robust to noisy data contaminated by non-Gaussian noise and outliers. Different from previous NMF algorithms, CNMTF firstly applies correntropy to NMTF to measure the similarity, and preserves double orthogonality conditions and dual graph regularization. We adopt the half-quadratic technique to solve the optimization problem of CNMTF, and derive the multiplicative update rules. The complexity issue of CNMTF is also presented. Furthermore, the robustness of the proposed algorithm is analyzed, and the relationships between CNMTF and several previous NMF based methods are discussed. Experimental results demonstrate that the proposed CNMTF method has better performance on real world image and text datasets for clustering tasks, compared with several state-of-the-art methods. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				AUG 9	2020	201								106054	10.1016/j.knosys.2020.106054													
J								Evaluating a dendritic neuron model for wind speed forecasting	KNOWLEDGE-BASED SYSTEMS										Neuron model; Dendrite; Evolution algorithm; Wind speed forecasting	NUMERICAL WEATHER PREDICTION; TIME-SERIES PREDICTION; SHORT-TERM; MULTIOBJECTIVE OPTIMIZATION; NETWORKS; DECOMPOSITION; MULTISTEP; SYSTEMS; SCHEME; ANN	Because of the intrinsic complexity and chaotic nature of wind speed time series, an appropriate model for accurately forecasting the moving tendency is required. In this paper, we propose an evolutionary dendritic neuron model (EDNM) to carry out wind speed forecasting. The model is trained via adaptive differential evolution with the linear population size reduction (L-SHADE) algorithm. Specifically, a mutual-information-based approach and the false nearest neighbours method are used to calculate the time delay and embedding dimensions, respectively. Then, the phase space of the wind speed time series is reconstructed based on the time delay and embedding dimensions, and the characteristics are analysed. The maximum Lyapunov exponent is applied to confirm the chaotic properties of the wind speed time series. Finally, EDNM trained by L-SHADE is used to predict the wind speed for Sotavento, which is located near Galicia, Spain. This study is the first, to the best of our knowledge, to use a dendritic neuron model to implement such real-world prediction. Extensive experimental results show that the proposed EDNM can perform better than other state-of-the-art models in terms of different assessment criteria. Therefore, the proposed method has high potential for practical applications in electric power systems. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				AUG 9	2020	201								106052	10.1016/j.knosys.2020.106052													
J								Evolution of Image Segmentation using Deep Convolutional Neural Network: A Survey	KNOWLEDGE-BASED SYSTEMS										Convolutional neural network; Deep learning; Semantic segmentation; Instance segmentation; Panoptic segmentation; Survey	ARCHITECTURE; FEATURES	From the autonomous car driving to medical diagnosis, the requirement of the task of image segmentation is everywhere. Segmentation of an image is one of the indispensable tasks in computer vision. This task is comparatively complicated than other vision tasks as it needs low-level spatial information. Basically, image segmentation can be of two types: semantic segmentation and instance segmentation. The combined version of these two basic tasks is known as panoptic segmentation. In the recent era, the success of deep convolutional neural networks (CNN) has influenced the field of segmentation greatly and gave us various successful models to date. In this survey, we are going to take a glance at the evolution of both semantic and instance segmentation work based on CNN. We have also specified comparative architectural details of some state-of-the-art models and discuss their training details to present a lucid understanding of hyper-parameter tuning of those models. We have also drawn a comparison among the performance of those models on different datasets. Lastly, we have given a glimpse of some state-of-the-art panoptic segmentation models. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				AUG 9	2020	201								106062	10.1016/j.knosys.2020.106062													
J								Evolving interval-based representation for multiple classifier fusion	KNOWLEDGE-BASED SYSTEMS										Ensemble method; Multiple classifiers; Classifiers fusion; Combining classifiers; Ensemble system	SWARM OPTIMIZATION ALGORITHM; ENSEMBLE; COMBINATION; PREDICTION; SMOTE; RULE	Designing an ensemble of classifiers is one of the popular research topics in machine learning since it can give better results than using each constituent member. Furthermore, the performance of ensemble can be improved using selection or adaptation. In the former, the optimal set of base classifiers, meta-classifier, original features, or meta-data is selected to obtain a better ensemble than using the entire classifiers and features. In the latter, the base classifiers or combining algorithms working on the outputs of the base classifiers are made to adapt to a particular problem. The adaptation here means that the parameters of these algorithms are trained to be optimal for each problem. In this study, we propose a novel evolving combining algorithm using the adaptation approach for the ensemble systems. Instead of using numerical value when computing the representation for each class, we propose to use the interval-based representation for the class. The optimal value of the representation is found through Particle Swarm Optimization. During classification, a test instance is assigned to the class with the interval-based representation that is closest to the base classifiers' prediction. Experiments conducted on a number of popular dataset confirmed that the proposed method is better than the well-known ensemble systems using Decision Template and Sum Rule as combiner, L2-loss Linear Support Vector Machine, Multiple Layer Neural Network, and the ensemble selection methods based on GA-Meta-data, META-DES, and ACO. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				AUG 9	2020	201								106034	10.1016/j.knosys.2020.106034													
J								Heterogeneous data release for cluster analysis with differential privacy	KNOWLEDGE-BASED SYSTEMS										Data publishing; Heterogeneous data; Differential privacy; Cluster analysis	SET-VALUED DATA; DATA PUBLICATION; CLASSIFICATION; ANONYMIZATION; ALGORITHMS; MODEL	Many models have been proposed to preserve data privacy for different data publishing scenarios. Among these models, epsilon-differential privacy has drawn increasing attention in recent years due to its rigorous privacy guarantees. While many existing solutions using epsilon-differential privacy deal with relational data and set-valued data separately, most of the real-life data, such as electronic health records, are in heterogeneous form. Privacy protection on heterogeneous data has not been widely studied. Furthermore, many existing works in privacy protection consider preserving the utility for the tasks of frequent itemset mining or classification analysis, but few works have focused on data publication for cluster analysis. In this paper, we propose the first differentially-private solution to release heterogeneous data for cluster analysis. The challenge facing us is how to mask raw data without any explicit guidance. Our approach addresses this challenge by converting a clustering problem to a classification problem, in which class labels can be used to encode the cluster structure of the raw data and assist the masking process. The approach generalizes the raw data probabilistically and adds noise to them for satisfying epsilon-differential privacy. Through extensive experiments on real-life datasets, we validate the performance of our approach. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				AUG 9	2020	201								106047	10.1016/j.knosys.2020.106047													
J								A similarity-based two-view multiple instance learning method for classification	KNOWLEDGE-BASED SYSTEMS										Multiple instance learning; Image classification	CLUSTERING-ALGORITHM; KERNEL; FRAMEWORK	Multiple instance learning (MIL) has been proposed to classify the bag of instances. In practice, we may meet the problems which have more than one view data. For example, in the image classification, textual information is always used to describe the image, which can be considered as two-view data. In this paper, we propose a new similarity-based two-view multi-instance learning (STMIL) method that can incorporate two-view data into learning so as to improve classification accuracy of MIL. In order to obtain the predictive classifier, we first convert the proposed model into a convex optimization problem, and then propose a new alternative framework to solve the proposed method. We then analyze the convergence of the proposed STMIL method. The experiments have been conducted to compare the performance of our proposed method and the previous methods. The results show that our method can deliver superior performance than other methods. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				AUG 9	2020	201								105661	10.1016/j.knosys.2020.105661													
J								Memory-aware gated factorization machine for top-N recommendation	KNOWLEDGE-BASED SYSTEMS										Factorization machine; Memory-ware; Gated filtering; Neural network	NONNEGATIVE MATRIX FACTORIZATION; WORD EMBEDDINGS	Factorization machine (FM) has recently become one of the most popular methods in collaborative filtering due to its flexibility of incorporating auxiliary information, e.g., user demographics and item genres. However, standard FM method and its deep variants (e.g., NFM and DeepFM) suffer from two key issues: (1) failing to effectively leverage user historical records, i.e., all historical records are treated equally without considering the relevance to the targeted user-item pair and (2) failing to adaptively weigh the importance of auxiliary information, i.e., auxiliary information may have negative effects on the accuracy in certain cases but existing methods cannot effectively detect and eliminate the negative effects. To this end, this paper proposes a memory-aware gated factorization machine (MAGFM), which improves the FM method by introducing two new components: (1) an external user memory matrix is introduced to each user, which can enrich the representation capacity by leveraging user historical items and the auxiliary information associated with the historical items and (2) gated filtering units are applied on top of the embedding of user/item auxiliary information, which can adaptively filter out the features with negative effects to achieve higher accuracy. Experimental studies on real-world datasets demonstrate that MAGFM can substantially outperform FM, NFM and DeepFM methods by 0.31% - 12.77% relatively in top-N recommendation. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				AUG 9	2020	201								106048	10.1016/j.knosys.2020.106048													
J								Automated intra-patient and inter-patient coronary artery disease and congestive heart failure detection using EFAP-Net	KNOWLEDGE-BASED SYSTEMS										Electrocardiogram (ECG) classification; ECG fragment alignment - PCA; convolutional network (EFAP-Net); Support vector machine (SVM); Coronary artery disease (CAD); Congestive heart failure (CHF)	CONVOLUTIONAL NEURAL-NETWORK; MYOCARDIAL-INFARCTION; ECG SIGNALS	Coronary artery disease (CAD) and congestive heart failure (CHF) occur worldwide, putting patients at risk of death. Researchers have developed many automatic methods for CAD and CHF classification. However, most have neglected evaluating the performance of these methods in inter-patient experiments that can guarantee their generalization in practical applications. Furthermore, the applicability of these methods to noisy and extremely unbalanced data has also not been validated. To address these issues, we propose a novel CAD and CHF classification method based on ECG fragment alignment (EFA)-principal component analysis (PCA) convolutional network (EFAP-Net). EFA is a method for eliminating heart rate differences that can ensure the component consistency of heartbeats between individuals. To deeply mine discriminative features, a network containing two convolutional layers and an output layer is used to extract high-dimensional abstract features from heartbeats. In this network, we employed PCA with a certain noise robustness to extract the suitable convolutional kernels for each layer, providing a network that is quickly trained and yielding remarkable performance in processing unbalanced data. Finally, a linear support vector machine specializing in classifying high-dimensional features is adopted as the classifier. In the intra-patient experiment, 99.84%, 99.92%, and 99.80% accuracies were achieved on balanced datasets A (normal+CAD), B (normal+CHF), and C (normal+CAD+CHF), respectively. In the inter-patient experiment, 94.64%, 98.94% and 86.86% accuracies were achieved on datasets G (normal+CAD), H (normal+CHF), and I (normal+CAD+CHF), respectively. Additionally, multi-level noisy and unbalanced ECG data were classified well. Hence, this method can be implemented to diagnose CAD and CHF effectively. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				AUG 9	2020	201								106083	10.1016/j.knosys.2020.106083													
J								Online random forests regression with memories	KNOWLEDGE-BASED SYSTEMS										Random forests regression; Long-term memory; Online weight learning; Leaf-level; Adaptive learning rate; Stochastic gradient descent	ALGORITHM	In recent years, the online schema of the conventional Random Forests(RFs) have attracted much attention because of its ability to handle sequential data or data whose distribution changes during the prediction process. However, most research on online RFs focuses on structural modification during the training stage, overlooking critical aspects of the sequential dataset, such as autocorrelation. In this paper, we demonstrate how to improve the predictive accuracy of the regression model by exploiting data correlation. Instead of modifying the structure of the off-line trained RFs, we endow RFs with memory during regression prediction through an online weight learning approach, which is called Online Weight Learning Random Forest Regression(OWL-RFR). Specifically, the weights of leaves are updated based on a novel adaptive stochastic gradient descent method, in which the adaptive learning rate considers the current and historical prediction bias ratios, compared with the static learning rate. Thus, leaf-level weight stores the learned information from the past data points for future correlated prediction. Compared with tree-level weight which only has immediate memory for current prediction, the leaf-level weight can provide long-term memory. Numerical experiments with OWL-RFR show remarkable improvements in predictive accuracy across several common machine learning datasets, compared to traditional RFs and other online approaches. Moreover, our results verify that the weight approach using the long-term memory of leaf-level weight is more effective than immediate dependency on tree-level weight. We show the improved effectiveness of the proposed adaptive learning rate in comparison to the static rate for most datasets, we also show the convergence and stability of our method. (C) 2020 Published by Elsevier B.V.																	0950-7051	1872-7409				AUG 9	2020	201								106058	10.1016/j.knosys.2020.106058													
J								Graph-structure constraint and Schatten p-norm-based unsupervised domain adaptation for image classification	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Unsupervised domain adaptation; Image classification; Distribution shift; Geometric structure; Statistical characteristic	LOW-RANK	Unsupervised domain adaptation, which aims to classify a target domain correctly only using a labeled source domain, has achieved promising performance yet remains a challenging problem. Most traditional methods focus on exploiting either geometric or statistical characteristics to reduce domain shifts. To take advantage of both sides, in this paper, we propose a unified framework incorporating both the geometric and statistical characteristics by adopting the non-convex Schattenp-norm and graph Laplacian constraints to preserve global and local structure information and constructing marginal and conditional distribution minimization terms to reduce the distribution shifts. Moreover, a classification error term on the source domain is embedded into the objective function to increase the discriminability. The proposed method has been evaluated on six datasets and the experimental results demonstrate the superiority of the proposed method over several state-of-the-art methods. The MATLAB code of our method will be publicly available at https://github.com/HeyouChang/unsupervised-domain-adaptation.																	1868-5137	1868-5145															10.1007/s12652-020-02350-y		AUG 2020											
J								Multi-modality medical image fusion technique using multi-objective differential evolution based deep neural networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Fusion; Diagnosis; CNN; Multi-modality; Differential evolution	CONTOURLET TRANSFORM	The advancements in automated diagnostic tools allow researchers to obtain more and more information from medical images. Recently, to obtain more informative medical images, multi-modality images have been used. These images have significantly more information as compared to traditional medical images. However, the construction of multi-modality images is not an easy task. The proposed approach, initially, decomposes the image into sub-bands using a non-subsampled contourlet transform (NSCT) domain. Thereafter, an extreme version of the Inception (Xception) is used for feature extraction of the source images. The multi-objective differential evolution is used to select the optimal features. Thereafter, the coefficient of determination and the energy loss based fusion functions are used to obtain the fused coefficients. Finally, the fused image is computed by applying the inverse NSCT. Extensive experimental results show that the proposed approach outperforms the competitive multi-modality image fusion approaches.																	1868-5137	1868-5145															10.1007/s12652-020-02386-0		AUG 2020											
J								Agro Suraksha: pest and disease detection for corn field using image analysis	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Color equalisation; Face verification; Linear iterative clustering; Texture features; SVM	DIAGNOSIS; IDENTIFICATION; CLASSIFICATION; SYSTEM	In today's world, due to irregular climatic patterns and other environmental issues various pests will affect the crops. These issues may affect the soil nutrition too. Due to this deficiency in nutrition, several diseases may affect the crops. In large agricultural field farmers feel difficult to monitor the pest in every nook and corner. Before identifying the pests, it may spread over vast area and cause severe damage to crops. Also they may not aware of bacterial disease that may affect the crops and their symptoms. Besides farmers may not know which type of pesticide and chemicals they need to use to prevent the further damage of crops. In some cases excess usage of those pesticides and other chemicals may affect the corn fields. Improper usage of those chemicals may affect the yield. To monitor the field periodically we need more human power. These problems are overcome by our proposed system. In our system diseases/pests are identified at early stages by capturing the images periodically in the agricultural field. Using image processing techniques the captured image will be segmented. For this purpose Texture based Segmentation and Simple Linear Iterative Clustering (SLIC) are used. From the segmented images the features for identifying pests and diseases will be extracted. The extracted features are used for classification. The presence of pest/disease will be identified at the first stage. In second stage the type of pest/disease will be detected. For classification both Binary Support Vector Machine (BSVM) and Multi class Support Vector Machine (MSVM) are used. And also, the pesticides and other chemicals which are needed to protect the field from further damage will be recommended along with the amount of chemicals needed and the method of usage of those chemicals.																	1868-5137	1868-5145															10.1007/s12652-020-02413-0		AUG 2020											
J								A semi-supervised self-training method based on density peaks and natural neighbors	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Self-training method; Semi-supervised classification; Semi-supervised learning; Natural neighbors; Density peaks	CLASSIFICATION; SEARCH	The semi-supervised self-training method is one of the successful methodologies of semi-supervised classification and can train a classifier by exploiting both labeled data and unlabeled data. However, most of the self-training methods are limited by the distribution of initial labeled data, heavily rely on parameters and have the poor ability of prediction in the self-training process. To solve these problems, a novel self-training method based on density peaks and natural neighbors (STDPNaN) is proposed. In STDPNaN, an improved parameter-free density peaks clustering (DPCNaN) is firstly presented by introducing natural neighbors. The DPCNaN can reveal the real structure and distribution of data without any parameter, and then helps STDPNaN restore the real data space with the spherical or non-spherical distribution. Also, an ensemble classifier is employed to improve the predictive ability of STDPNaN in the self-training process. Intensive experiments show that (a) STDPNaN outperforms state-of-the-art methods in improving classification accuracy ofknearest neighbor, support vector machine and classification and regression tree; (b) STDPNaN also outperforms comparison methods without any restriction on the number of labeled data; (c) the running time of STDPNaN is acceptable.																	1868-5137	1868-5145															10.1007/s12652-020-02451-8		AUG 2020											
J								Cooperative Task Assignment and Track Planning For Multi-UAV Attack Mobile Targets	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Moving target; Cooperative task assignment; Track planning; Ant colony optimization; Particle swarm optimization	ALLOCATION	This paper proposes a system framework for solving the problem of multi-UAV cooperative task assignment and track planning for ground moving targets. For the combinatorial optimization model, it is solved by a new particle swarm optimization algorithm based on guidance mechanism. In order to plan an effective track for the target more rapidly, a new ant colony optimization algorithm based on adaptive parameter adjustment and bidirectional search is proposed. Furthermore, in the case of target movement, a method of the predicted meeting point is proposed to solve the problem that the moving point cannot be used as the target point of the track planning algorithm. In addition, the track planning problem in the UAV tracking mode is also considered. An online re-planning method is proposed for time-sensitive uncertainties. Finally,the simulation results show that compared with other algorithms, the proposed method can not only effectively plan a reasonable track, but also solve the uncertainty problem, and obtain the optimal task allocation plan, which improves the multi-UAV cooperative combat capability.																	0921-0296	1573-0409															10.1007/s10846-020-01241-w		AUG 2020											
J								Object reachability via swaps under strict and weak preferences	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										Resource allocation; Social choice theory; Pareto efficiency; Computational complexity; Coordination and cooperation	HOUSE ALLOCATION	TheHousing Marketproblem is a widely studied resource allocation problem. In this problem, each agent can only receive a single object and has preferences over all objects. Starting from an initial endowment, we want to reach a certain assignment via a sequence of rational trades. We first consider whether an object is reachable for a given agent under a social network, where a trade between two agents is allowed if they are neighbors in the network and no participant has a deficit from the trade. Assume that the preferences of the agents are strict (no tie among objects is allowed). This problem is polynomial-time solvable in a star-network and NP-complete in a tree-network. It is left as a challenging open problem whether the problem is polynomial-time solvable when the network is a path. We answer this open problem positively by giving a polynomial-time algorithm. Then we show that when the preferences of the agents are weak (ties among objects are allowed), the problem becomes NP-hard when the network is a path and can be solved in polynomial time when the network is a star. Besides, we consider the computational complexity of finding different optimal assignments for the problem in the special case where the network is a path or a star.																	1387-2532	1573-7454				AUG 8	2020	34	2							51	10.1007/s10458-020-09477-4													
J								Joint multi-task cascade for instance segmentation	JOURNAL OF REAL-TIME IMAGE PROCESSING										Cascade structure; Instance segmentation; Multi-task; Feature fusion		Instance segmentation requires both pixel-level classification accuracy and high-level semantic features at the target instance level, which is very challenging, and the cascade structure can effectively improve both of these problems. To make full use of the relationship between detection and segmentation, this paper proposes a joint multi-tasking cascade structure, which is not simply to cascade the two tasks of detection and segmentation, but to unitedly put them into multi-stage processing, and especially to integrate the information at different stages of the mask branch. The entire structure can effectively utilize the superior characteristics of each stage in the matter of detection and segmentation, thus improving the quality of mask prediction. The feature fusion process is introduced in the full convolution networks (FCN) branch, and the high-level and low-level features are effectively fused to enhance the contextual information of the picture semantic features. The experiments demonstrate the better results on the COCO dataset.																	1861-8200	1861-8219															10.1007/s11554-020-01007-5		AUG 2020											
J								Deep spatial-temporal structure learning for rumor detection on Twitter	NEURAL COMPUTING & APPLICATIONS										Rumor detection; Spatial-temporal structure learning		The widespread of rumors on social media, carrying unreal or even malicious information, brings negative effects on society and individuals, which makes the automatic detection of rumors become particularly important. Most of the previous studies focused on text mining using supervised models based on feature engineering or deep learning models. In recent years, another parallel line of works, which focuses on the spatial structure of message propagation, provides an alternative and promising solution. However, these existing methods in this parallel line largely overlooked the temporal structure information associated with the spatial structure in message propagation. Actually the addition of temporal structure information can make the message propagations be classified from the perspective of spatial-temporal structure, a more fine-grained perspective. Under these observations, this paper proposes a spatial-temporal structure neural network for rumor detection, termed as STS-NN, which treats the spatial structure and the temporal structure as a whole to model the message propagation. All the STS-NN units are parameter shared and consist of three components, including spatial capturer, temporal capturer and integrator, to capture the spatial-temporal information for the message propagation. The results show that our approach obtains better performance than baselines in both rumor classification and early detection.																	0941-0643	1433-3058															10.1007/s00521-020-05236-4		AUG 2020											
J								A span-based model for aspect terms extraction and aspect sentiment classification	NEURAL COMPUTING & APPLICATIONS										Sentiment analysis; Span-based; Aspect terms extraction; Aspect sentiment classification		Sentiment analysis is a field of natural language processing, which is used to identify and extract opinions and attitudes from text. Aspect-based sentiment analysis aims to extract aspect terms and predict sentiment categories of the opinion aspects. It includes two subtasks: aspect terms extraction and aspect sentiment classification. However, previous studies regarded them as two independent tasks and solve them, respectively, which has limitations for practical application. In this paper, we combine the requirements of two subtasks to propose a new aspect-based sentiment analysis framework based on span, which is a simple and effective joint model to generate all aspects and corresponding sentiment polarities of the input sentences. Specifically, dual gated recurrent units which are used to extract the respective representation of each task can process sequence information better, and an interaction layer which is used to consider the relationship between the representations. Experiments on three benchmark datasets show that the proposed framework outperforms the state-of-the-art baseline models.																	0941-0643	1433-3058															10.1007/s00521-020-05221-x		AUG 2020											
J								Twin-parametric margin support vector machine with truncated pinball loss	NEURAL COMPUTING & APPLICATIONS										TSVM; Truncated; Pinball loss; CCCP		In this paper, we propose a novel classifier termed as twin-parametric margin support vector machine with truncated pinball loss (TPin-TSVM), which is motivated by the twin-parametric margin support vector machine (TPMSVM). The proposed TPin-TSVM has the following characteristics. Firstly, it can preserve both sparsity and feature noise insensitivity simultaneously, because it deals with the quantile distance which makes it less sensitive to noises, and most of the correctly classified samples are given equal penalties which makes it have the precious sparsity. Secondly, it is a non-differentiable non-convex optimization problem, we adopt the popular and effective concave-convex procedure (CCCP) to solve it. In each iteration of CCCP, the TPMSVM is utilized as a core of our TPin-TSVM, because it determines two nonparallel hyperplanes by solving two smaller sized quadratic programming problems, which greatly improves the computational speed. Thirdly, we investigate its theoretical properties of noise insensitivity and sparsity, and the proposed TPin-TSVM realizes the between-class distance maximization, within-class scatter and misclassification error minimization together. The experiments on two artificial datasets also verify the properties. We perform numerical experiments on thirty-five benchmark datasets to investigate the validity of our proposed algorithm. Experimental results indicate that our algorithm yields the comparable generalization performance compared with three state-of-the-art algorithms.																	0941-0643	1433-3058															10.1007/s00521-020-05225-7		AUG 2020											
J								Learning Chinese word representation better by cascade morphologicaln-gram	NEURAL COMPUTING & APPLICATIONS										Chinese word embedding; n-gram; Deep learning; Natural language processing		Word embedding refers to mapping words or phrases to vectors of real numbers. This is the precondition of text classification, sentiment analysis and text mining in natural language processing using deep neural networks. Taking English as an example, most of current word embedding algorithms obtain the vectors by learning the distribution of word's prefix, suffix, etyma and the entire word itself. Unlike English, Chinese words are composed of components and strokes. Furthermore, those components and strokes usually hint the meaning of the word. Thus, components and strokes distribution must be fully considered and learnt when one's doing Chinese word embedding. In this paper, we propose a component-based cascaden-gram (CBCn-gram) model and a stroke-based cascaden-gram (SBCn-gram) model. By overlaying component and stroken-gram vectors on word vectors, we successfully improve Chinese word embedding so as to preserve as more morphological information as possible at different granularity levels. We evaluate our models on word similarity, word analogy and text classification tasks using wordsim-240, wordsim-296, Chinese word analogy dataset and Fudan Corpus, respectively. Experimental and comparison results show that our models outperform other state-of-the-art methods.																	0941-0643	1433-3058															10.1007/s00521-020-05198-7		AUG 2020											
J								A City Monitoring System Based on Real-Time Communication Interaction Module and Intelligent Visual Information Collection System	NEURAL PROCESSING LETTERS										Network communication; Interactive data; Computer vision; Image acquisition; Monitoring; Urban development		With the rapid development of society, the improvement of material level and the current situation of the large-scale population flow in China, the awareness of security is becoming more and more important in people's life. With the rapid development of image processing and computer vision technology, people try to analyze, process and understand the collected video image automatically without human intervention. The intelligent video monitoring system collects video signals of interested objects in a dynamic scene through a camera, and processes and analyzes image information by a computer. Only by establishing a reasonable and effective urban video monitoring management system can government departments find out problems in the first time. The traditional highway monitoring and commanding traffic scheduling system based on GIS, which can obtain road traffic information and conduct traffic scheduling by remote sensing, has the disadvantage of poor effect on traffic scheduling. In this paper, real-time communication technology and computer vision acquisition technology are used to build a city monitoring system. The experimental results show that this method has strong timeliness and good monitoring effect. Compared with the state-of-the-art methodologies, the proposed framework is efficient and accurate.																	1370-4621	1573-773X															10.1007/s11063-020-10325-5		AUG 2020											
J								Orthomodular lattices asL-algebras	SOFT COMPUTING										Orthomodular lattice; L-algebra; Self-similar closure; OM-L-algebra		We first prove that the axioms system of orthomodularL-algebra (O-L-algebras for short) as given in [Rump: Forum Mathematicum, 30(4), 2018: 973-995] are not independent by giving an independent axiom one. Then, two conditions forKL-algebras to be Boolean are provided. Furthermore, some theorems of Holland are reproved using the self-similar closure ofOM-L-algebras. In particular, the monoid operation of the self-similar closure is shown to be commutative.																	1432-7643	1433-7479				OCT	2020	24	19					14391	14400		10.1007/s00500-020-05242-7		AUG 2020											
J								Deep learning and control algorithms of direct perception for autonomous driving	APPLIED INTELLIGENCE										Self-driving cars; Autonomous driving; Deep learning; Image perception; Control algorithms	NEURAL-NETWORKS; VISION	We propose an end-to-end machine learning model that integrates multi-task (MT) learning, convolutional neural networks (CNNs), and control algorithms to achieve efficient inference and stable driving for self-driving cars. The CNN-MT model can simultaneously perform regression and classification tasks for estimating perception indicators and driving decisions, respectively, based on the direct perception paradigm of autonomous driving. The model can also be used to evaluate the inference efficiency and driving stability of different CNNs on the metrics of CNN's size, complexity, accuracy, processing speed, and collision number, respectively, in a dynamic traffic. We also propose new algorithms for controllers to drive a car using the indicators and its short-range sensory data to avoid collisions in real-time testing. We collect a set of images from a camera of The Open Racing Car Simulator in various driving scenarios, train the model using this dataset, test it in unseen traffics, and find that it outperforms earlier models in highway traffic. The stability of end-to-end learning and self driving depends crucially on the dynamic interplay between CNN and control algorithms. The source code and data of this work are available on our website, which can be used as a simulation platform to evaluate different learning models on equal footing and quantify collisions precisely for further studies on autonomous driving.																	0924-669X	1573-7497															10.1007/s10489-020-01827-9		AUG 2020											
J								Online coverage and inspection planning for 3D modeling	AUTONOMOUS ROBOTS										Active sensing; Exploration planning; Autonomous inspection; Next-best-view; Motion planning	VISUAL SLAM; EXPLORATION; RECONSTRUCTION; ENVIRONMENTS; ALGORITHMS	In this study, we address an exploration problem when constructing complete 3D models in an unknown environment using a Micro-Aerial Vehicle. Most previous exploration methods were based on the Next-Best-View (NBV) approaches, which iteratively determine the most informative view, that exposes the greatest unknown area from the current partial model. However, these approaches sometimes miss minor unreconstructed regions like holes or sparse surfaces (while these can be important features). Furthermore, because the NBV methods iterate the next-best path from a current partial view, they sometimes produce unnecessarily long trajectories by revisiting known regions. To address these problems, we propose a novel exploration algorithm that integrates coverage and inspection strategies. The suggested algorithm first computes a global plan to cover unexplored regions to complete the target model sequentially. It then plans local inspection paths that comprehensively scans local frontiers. This approach reduces the total exploration time and improves the completeness of the reconstructed models. We evaluate the proposed algorithm in comparison with other state-of-the-art approaches through simulated and real-world experiments. The results show that our algorithm outperforms the other approaches and in particular improves the completeness of surface coverage.																	0929-5593	1573-7527				NOV	2020	44	8					1431	1450		10.1007/s10514-020-09936-7		AUG 2020											
J								Angular momentum-based control of an underactuated orthotic system for crouch-to-stand motion	AUTONOMOUS ROBOTS										Underactuated pendulum; Angular momentum-based controller; Crouch-to-stand; Orthosis	EXOSKELETON	This paper presents an angular momentum-based controller for crouch-to-stand motion of a powered pediatric lower-limb orthosis. The control law is developed using an underactuated triple pendulum model representing the legs of an orthosis-dummy system where the hip and knee joints are actuated but the ankle joint is unpowered. The control law is conceived to drive the angular momentum of the system to zero, thereby bringing the system to a statically balanced upright configuration. The parameters of the dynamic model of the orthosis-dummy system are experimentally identified and used to synthesize the momentum-based controller. Control parameters are selected using closed-loop pole placement of the linearized system via numerical optimization to ensure local closed-loop stability with adequate damping and satisfactory response time without too large controller gains. The controller is applied in simulation to determine the region of viable initial conditions resulting in no knee hyperextension or loss of balance, as determined from a zero-moment point analysis. The controller is then implemented in experiment showing feasibility of the control strategy in practice. Results are compared against a similarly-synthesized linear-quadratic regulator.																	0929-5593	1573-7527				NOV	2020	44	8					1469	1484		10.1007/s10514-020-09938-5		AUG 2020											
J								Machine learning-based left ventricular hypertrophy detection using multi-lead ECG signal	NEURAL COMPUTING & APPLICATIONS										Left Ventricular Hypertrophy; Continuous Wavelet Transform; ECG signal; Levenberg-Marquardt (LMNN); SCG NN; MLP	MYOCARDIAL-INFARCTION; CLASSIFICATION; OPTIMIZATION	This work proposes a novel method for the detection of Left Ventricular Hypertrophy (LVH) from a multi-lead ECG signal. Left Ventricle walls become thick due to prolonged hypertension which may fail to pump heart effectively. The imaging techniques can be used as an alternative diagnose LVH; however, they are more expensive and time-consuming than proposed LVH. To overcome this issue, an algorithm to the diagnosis of LVH using ECG signal based on machine learning techniques were designed. In LVH detection, the pathological attributes such as R wave, S wave, inversion of QRS complex, changes in ST segment noticed in the ECG signal. This clinical information extracted as a feature by applying continuous wavelet transform. The signals were reconstructed with the frequency between 10 and 50 Hz from the wavelet. This followed by the detection of R wave and S wave peaks to obtain the relevant LVH diagnostic features. The Support Vector Machine (SVM), K-Nearest Neighbor (KNN), Ensemble of Bagged Tree, AdaBoost classifiers were employed and the results are compared with four neural network classifiers including Multilayer Perceptron (MLP), Scaled Conjugate Gradient Backpropagation Neural Network (SCG NN), Levenberg-Marquardt Neural Network (LMNN) and Resilient Backpropagation Neural network (RPROP). The data source includes Left Ventricular Hypertrophy and healthy ECG signal from PTB diagnostic ECG database and St Petersburg INCART 12-Lead Arrhythmia Database. The results revealed that the proposed work can diagnose LVH successfully using neural network classifiers. The accuracy in detecting LVH is 86.6%, 84.4%, 93.3%,75.6%, 95.6%, 97.8%, 97.8%, 88.9% using SVM, KNN, Ensemble of Bagged Tree, AdaBoost, MLP, SCG NN, LMNN and RPROP classifiers, respectively.																	0941-0643	1433-3058															10.1007/s00521-020-05238-2		AUG 2020											
J								Densely Connected Deep Extreme Learning Machine Algorithm	COGNITIVE COMPUTATION										Extreme learning machine (ELM); Densely connections; Deep learning; Representation learning		As a single hidden layer feed-forward neural network, the extreme learning machine (ELM) has been extensively studied for its short training time and good generalization ability. Recently, with the deep learning algorithm becoming a research hotspot, some deep extreme learning machine algorithms such as multi-layer extreme learning machine (ML-ELM) and hierarchical extreme learning machine (H-ELM) have also been proposed. However, the deep ELM algorithm also has many shortcomings: (1) when the number of model layers is shallow, the random feature mapping makes the sample features cannot be fully learned and utilized; (2) when the number of model layers is deep, the validity of the sample features will decrease after continuous abstraction and generalization. In order to solve the above problems, this paper proposes a densely connected deep ELM algorithm: dense-HELM (D-HELM). Benchmark data sets of different sizes have been employed for the property of the D-HELM algorithm. Compared with the H-ELM algorithm on the benchmark dataset, the average test accuracy is increased by 5.34% and the average training time is decreased by 21.15%. On the NORB dataset, the proposed D-HELM algorithm still maintains the best classification results and the fastest training speed. The D-HELM algorithm can make full use of the features of hidden layer learning by using the densely connected network structure and effectively reduce the number of parameters. Compared with the H-ELM algorithm, the D-HELM algorithm significantly improves the recognition accuracy and accelerates the training speed of the algorithm.																	1866-9956	1866-9964				SEP	2020	12	5					979	990		10.1007/s12559-020-09752-2		AUG 2020											
J								Analysis of basic neural network types for automated skin cancer classification using Firefly optimization method	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cancer; Melanoma; Fuzzy C means; Firefly optimization; Error rate; Neural network types; Accuracy; Mean; Sensitivity	DIGITAL EPILUMINESCENCE MICROSCOPY; ATYPICAL NEVI; EARLY MELANOMA; COMMON NEVI; FOLLOW-UP; DERMOSCOPY; ALGORITHMS; PATTERNS; FEATURES; DESIGN	In recent days, cancer is a deadly disease because of its spreading nature to other cells, and this disease is not identified at an early detection stage. Generally, the cancer is detected with the help of a biopsy method, which is a painful approach. Due to the development of technology, nowadays, it is identified with the help of image processing methods. Here, the image processing approach is used for identifying and classifying the skin cancer types, namely melanoma, common and atypical nevi. The methods used earlier for the detection and classification are artificial skin leison merging, Raman spectroscopy and back-propagation networks. Cancer is classified into many types like blood cancer, bone, colon, and stomach and skin cancer. Among these cancer types, skin cancer can be a dreadful disease, which is detected and then treated at the starting stage of the disease. Hence, this paper proposed an optimized neural and fuzzy approach for skin cancer classification. The fuzzy c-means segmentation is used for the detection of the cancer region. Firefly optimization determines the dominant feature for the training of the neural network. The dominant feature is determined by reducing the error rate of the classifier. The overall process is evaluated with the help of evaluation metrics like accuracy, specificity and sensitivity. In this proposed method, the best result is achieved for the pattern net by improving its accuracy by 4.9% from its previous Moth-Flame Optimization based classification in its evaluation.																	1868-5137	1868-5145															10.1007/s12652-020-02394-0		AUG 2020											
J								Ternary subset difference revocation in public key framework supporting outsider anonymity	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Anonymous broadcast encryption; Outsider-anonymity; Ternary subset difference; Revocation	BROADCAST ENCRYPTION; SHORT CIPHERTEXTS; PRIVACY	Broadcast encryption (BE) is a cryptographic primitive which sends encrypted message to the users securely. The BE scheme proposed by Naor, Naor, and Lotspiech (NNL) in 2001 is a popular BE scheme which uses a binary tree. The advanced access content system standard suggested to use it for digital right management in Blue-ray and DVD-discs. This paper puts forward an efficient broadcast encryption inpublic key settingemployingternary tree subset differencemethod for revocation. Our approach utilizes composite order bilinear group setting to achieve the tree based construction in public key setting. Our second construction is an extension of our first construction and providesoutsider-anonymityby disabling the revoked users from getting any information of message andconcealingthe set of subscribed users from the revoked users. The construction of Fazio and Perera is the closest one to that of our second scheme (as both of these construction are in public key setting and provides outsider-anonymity). We have reduced the ciphertext size from r log N/r to min{N/3, N - r, 2r - 1}. Thus reduces the communication bandwidth. We have also reduced the public key size. Our constructions enjoy the revocation property. Both of our constructions achieve selective semantic security in the standard model under reasonable assumptions and new users can join without updating the pre-existing setup.																	1868-5137	1868-5145															10.1007/s12652-020-02319-x		AUG 2020											
J								A novel cognitive Wallace compressor based multi operand adders in CNN architecture for FPGA	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										CNN; GPU; Wallace compressor adders; Binary tree adders; Computer vision; image processing		Convolutional neural networks is one of the most popular method in recent times to solve the computer vision and image processing applications. CNN has become more intensive as its computation increases day by day and it needs more dedicated hardware for an effective implementation. In recent times, graphics processing unit (GPU) and field programmable gate arrays (FPGA) are finding its high probability for the research in terms of low complexity execution and implementation of CNN. FPGA outperforms GPU in terms of its flexible architecture and high performance with less energy consumption. Hence FPGA finds more suitable for an effective implementation of CNN. How ever, optimization is required for an accelerator designs for CNN to accommodate more computations. One of the real dark side of the accelerator design is to perform the addition of intermediate results obtained during the convolution. Therefore multi operand adders are needed to employ for convolution operation but consumes the more area which in turn reduces the performance of the system. Hence the paper proposes the new cognitive Wallace compressor adder structures which is used for optimization in the adder layers of the convolutional neural networks (CNN). The proposed adders replaces the traditional binary tree adders for the CNN accelerator design. Also the paper provides the insight view of experimentation in ARTIX-7 EDGE FPGA and compared with the existing adders in which power consumption is reduced to 20-25% and area utilization has reduced to 30% respectively.																	1868-5137	1868-5145															10.1007/s12652-020-02402-3		AUG 2020											
J								Reallocation of unoccupied beds among requesting wards	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Bed management; Collaboration; Resilience engineering; Simulated annealing; Mixed Integer Programming	RESILIENCE ANALYSIS; SEISMIC RESILIENCE; HEALTH-CARE; OPTIMIZATION; SIMULATION; ALLOCATION; FRAMEWORK; MANAGEMENT; RESOURCE; DELIVERY	Disruptions can cause demand fluctuation, thus overcrowding at hospital wards, and can make the waiting list of patients longer. Bed management at hospitals is one of the solutions to deal with overcrowding. In addition, Resilience Engineering (RE) is an approach that can help organizations to bounce back to their desired performance state after a disruption. In this paper, the concept of RE has been used to improve the bed management of hospitals during and after disruption. More precisely, bed sharing among hospital wards has been introduced as a collaboration strategy and its impact on the length of patients' waiting list as the major performance index is investigated. Relationship priority between different wards, patients' gender, patients' length of stay and the number of rooms in every ward are the major factors considered in our modeling. A mixed integer linear programming optimization model with the objective of minimizing the patients' waiting time in a hospital has been proposed for the real-world problems. The main contribution of the present paper is proposing a resiliency-based modeling of bed management in hospitals. Due to the complexity and making the proposed model applicable to the real world problems, a simulated annealing algorithm is used to solve the model and a new procedure for creating initial solution is presented. The results show that applying resilience strategy has a considerable impact on improving the hospital's performance index.																	1868-5137	1868-5145															10.1007/s12652-020-02215-4		AUG 2020											
J								Sliding mode learning algorithm based adaptive neural observer strategy for fault estimation, detection and neural controller of an aircraft	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Sliding mode control; Aircraft; Sensors; Fault detection; Neural networks	NETWORK-BASED SENSOR; NONLINEAR-SYSTEMS	In this paper, two different adaptive strategies are presented for continuous time uncertain nonlinear systems with unknown disturbances and faults. In first strategy, a sliding mode control based adaptive neural observer approach is anticipated for estimation of unknown disturbances and faults by using the multi-layer perceptron, the weight parameters are updated by using the sliding mode online learning strategy. Conventionally, gradient descent back-propagation adaptation methods are used for neural networks training, within these adaptation methods a new theory of sliding mode control is added to conventional gradient descent back-propagation procedure. In this nonlinear control concept, the Sliding Mode Control is employed as a learning strategy, in which the neural network is considered as a control process and computes the stable and dynamic learning rates of neural network. By considering the unknown faults approximation and reconstruction, this online learning strategy shows a rapid sensor fault detection, approximation, and reconstruction with high preciseness and rapidness compared to conventional strategy and algorithms presented in literature. Approaches used in literature do not have much higher preciseness and fast response to fault occurrence compared to the strategy proposed in this study. In second strategy, the neural network controller strategy is proposed with concept of filtered error scheme. Online weight updating strategy comprise of additional term to back-propagation, plus an additional robustifying term, assures the stability and rapid convergence of the faulty system. The stability analysis of the proposed fault tolerance control is also provided. While considering stability of system, this robust online adaptive fault tolerance control shows a fast convergence in the presence of unknown disturbances and faults. The robust adaptive neural controller is compared with the conventional gradient descent based controller in the existence of various sensor faults and failures. The proposed strategies are validated on Boeing 747 100/200 aircraft, results show the efficiency, preciseness and robustness of strategies compared to the algorithm presented in literature.																	1868-5137	1868-5145															10.1007/s12652-020-02390-4		AUG 2020											
J								Attribute-based regularization of latent spaces for variational auto-encoders	NEURAL COMPUTING & APPLICATIONS										Representation learning; Latent space disentanglement; Latent space regularization; Generative modeling		Selective manipulation of data attributes using deep generative models is an active area of research. In this paper, we present a novel method to structure the latent space of a variational auto-encoder to encode different continuous-valued attributes explicitly. This is accomplished by using an attribute regularization loss which enforces a monotonic relationship between the attribute values and the latent code of the dimension along which the attribute is to be encoded. Consequently, post training, the model can be used to manipulate the attribute by simply changing the latent code of the corresponding regularized dimension. The results obtained from several quantitative and qualitative experiments show that the proposed method leads to disentangled and interpretable latent spaces which can be used to effectively manipulate a wide range of data attributes spanning image and symbolic music domains.																	0941-0643	1433-3058															10.1007/s00521-020-05270-2		AUG 2020											
J								A neural integrator model for planning and value-based decision making of a robotics assistant	NEURAL COMPUTING & APPLICATIONS										Dynamic Neural Field; Neural integrator; Assembly robot; Value-based decision making; Sequence learning	ARTIFICIAL COGNITION; MATCHING BEHAVIOR; PREFRONTAL CORTEX; REPRESENTATION; NEURODYNAMICS; ARCHITECTURE; AUTOMATION; MECHANISMS; STABILITY; DYNAMICS	Modern manufacturing and assembly environments are characterized by a high variability in the built process which challenges human-robot cooperation. To reduce the cognitive workload of the operator, the robot should not only be able to learn from experience but also to plan and decide autonomously. Here, we present an approach based on Dynamic Neural Fields that apply brain-like computations to endow a robot with these cognitive functions. A neural integrator is used to model the gradual accumulation of sensory and other evidence as time-varying persistent activity of neural populations. The decision to act is modeled by a competitive dynamics between neural populations linked to different motor behaviors. They receive the persistent activation pattern of the integrators as input. In the first experiment, a robot learns rapidly by observation the sequential order of object transfers between an assistant and an operator to subsequently substitute the assistant in the joint task. The results show that the robot is able to proactively plan the series of handovers in the correct order. In the second experiment, a mobile robot searches at two different workbenches for a specific object to deliver it to an operator. The object may appear at the two locations in a certain time period with independent probabilities unknown to the robot. The trial-by-trial decision under uncertainty is biased by the accumulated evidence of past successes and choices. The choice behavior over a longer period reveals that the robot achieves a high search efficiency in stationary as well as dynamic environments.																	0941-0643	1433-3058															10.1007/s00521-020-05224-8		AUG 2020											
J								Population-based Tabu search with evolutionary strategies for permutation flow shop scheduling problems under effects of position-dependent learning and linear deterioration	SOFT COMPUTING										Permutation flow shop scheduling; Learning effect; Deterioration effect; Iterated greedy; Discrete differential equation; Makespan; Tabu search; Evolutionary strategy	HYBRID GENETIC ALGORITHM; TOTAL COMPLETION-TIME; ITERATED GREEDY ALGORITHM; EARLINESS/TARDINESS COSTS; MINIMIZE MAKESPAN; PARALLEL MACHINE; TOTAL TARDINESS; HEURISTICS; JOBS; CLASSIFICATION	This paper investigates permutation flow shop scheduling (PFSS) problems under the effects of position-dependent learning and linear deterioration. In a PFSS problem, there arenjobs andmmachines in series. Jobs are separated into operations onm different machines in series, and jobs have to follow the same machine order with the same sequence. The PFSS problem under the effects of learning and deterioration is introduced with a mixed-integer nonlinear programming model. The time requirement for solving large-scale problems type of PFSS problem is exceedingly high. Therefore, well-known metaheuristic methods for the PFSS problem without learning and deterioration effects such as iterated greedy algorithms and discrete differential evolution algorithm are adapted for the problem with learning and deterioration effects in order to find a faster and near-optimal or optimal solution for the problem. Furthermore, this paper proposes a hybrid solution algorithm that is called population-based Tabu search algorithm (TSPOP) with evolutionary strategies such as crossover and mutation. The search algorithm is built on the basic structure of Tabu search and it searches for the best candidate from a solution population instead of improving the current best candidate at each iteration. Furthermore, the performances of these methods in view of solution quality are discussed in this paper by using test problems for 20, 50, and 100 jobs with 5, 10, 20 machines. Experimental results show that the proposed TS(POP)algorithm outperforms the other existing algorithms in view of solution quality.																	1432-7643	1433-7479															10.1007/s00500-020-05234-7		AUG 2020											
J								An efficient online sequential extreme learning machine model based on feature selection and parameter optimization using cuckoo search algorithm for multi-step wind speed forecasting	SOFT COMPUTING										Multi-step wind speed forecasting; Online sequential extreme learning machine; Optimized variational mode decomposition; Cuckoo search optimization	CORAL-REEFS OPTIMIZATION; SUPPORT VECTOR MACHINES; NEURAL-NETWORK; ADAPTIVE COMBINATION; HYBRID APPROACH; DECOMPOSITION; PREDICTION; WAVELET; ENSEMBLE	Accurate wind speed forecasting (WSF) has become increasingly important to overcome the adverse effects of stochastic nature of the wind on wind power generation. This paper proposes a multi-step hybrid online WSF model by combining online sequential extreme learning machine (OSELM), optimized variational mode decomposition (OVMD) and cuckoo search optimization algorithm (CSO). OVMD decomposes the wind speed series into subseries, and CSO selects the input features for each subseries. Multi-step forecasting for each subseries is performed using OSELM model optimized by CSO. Finally, the forecasting results are obtained by the aggregate calculations. The proposed model has been examined by using 10-min average wind speed data collected in monsoon and winter seasons from a supervisory control and data acquisition system of a 1.5 MW wind turbine situated in central dry zone of Karnataka, India. The results reveal that the model proposed captures the nonlinear characteristics of the wind speed in a better manner in comparison with the batch learning approach, giving accurate wind speed forecasts. This can help wind farms to estimate the wind power in a location efficiently.																	1432-7643	1433-7479															10.1007/s00500-020-05222-x		AUG 2020											
J								Minding morality: ethical artificial societies for public policy modeling	AI & SOCIETY										Multi-agent artificial intelligence; Social simulation; Public policy; Ethics; Morality; Cultural norms	HUMAN SIMULATION	Public policies are designed to have an impact on particular societies, yet policy-oriented computer models and simulations often focus more on articulating the policies to be applied than on realistically rendering the cultural dynamics of the target society. This approach can lead to policy assessments that ignore crucial social contextual factors. For example, by leaving out distinctive moral and normative dimensions of cultural contexts in artificial societies, estimations of downstream policy effectiveness fail to account for dynamics that are fundamental in human life and central to many public policy challenges. In this paper, we supply evidence that incorporating morally salient dimensions of a culture is critically important for producing relevant and accurate evaluations of social policy when using multi-agent artificial intelligence models and simulations.																	0951-5666	1435-5655															10.1007/s00146-020-01028-5		AUG 2020											
J								Classification based on underwater degradation using neural network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Underwater images; Underwater degradation; Image classification; Neural network; Feature extraction and confusion matrix	RECOGNITION	Images captured in underwater are degraded by the attenuation of light in water. The classification technique of underwater images based on various degradation has not been explored so far. The classification process is quite difficult due to the complex background of the underwater images. Generally multiple features are extracted to improve the classification accuracy. The significant feature selection plays a vital role in the classification process. In this work, the process is carried out in two steps namely, (i) first, the features of the underwater images are extracted, (ii) the extracted features are given as input to the neural network (NN) to classify these images into different classes of degradation. The efficiency of this classification technique is measured based on the accuracy and error percentage. The experimental results imply that NN performs well in case of 5 classes of degradation due to distinguishable features and produces an accuracy of about 100 percent.																	1868-5137	1868-5145															10.1007/s12652-020-02437-6		AUG 2020											
J								Trust and privacy based vertical handoff decision algorithm for telecardiology application in heterogeneous wireless networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Telecardiology; Ubiquitous; Prone; Telemedicine; Echocardiology	TELEMEDICINE SYSTEM; SELECTION	Telecardiology is one of the emerging fields of telemedicine to improve the cardiac patient's life quality. It uses the information and communication technologies to monitor the cardiac patients by healthcare professionals at distant. The telecardiology system demands for ubiquitous connection to assure promising services. Such a seamless service is provided by heterogeneous wireless technology. But the open network infrastructure of the heterogeneous networks is prone to various security attacks which makes patient privacy susceptible. Therefore, in this paper, the telecardiology system incorporated with Trust and Privacy based Multi-attribute Vertical Handoff decision algorithm towards decide on an best possible association amongst the available alternative candidates based on the patient health condition. Simulation results show enhancement in provisions of handoff rate, reduced blocking probability and improved performance throughput.																	1868-5137	1868-5145															10.1007/s12652-020-02422-z		AUG 2020											
J								A DWT based watermarking approach for medical image protection	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Watermarking; Medical images; Wavelet transform; Blind watermarking; Electronic patient record	JOINT WATERMARKING; JPEG-LS; FRAMEWORK; DOMAIN; ROBUST	In order to contribute to the security of medical image, we present in this paper a blind and robust watermarking technique that allows the integration of the electronic patient's record into a computerized tomography scan. In this approach, a discrete wavelet transform is applied to the image before the integration process, then, a topological reorganization of the coefficients of the LL sub-bands is done by the ZigZag scanning method. The obtained coefficients are then combined to integrate the watermark bits. A hash of the electronic patient record being integrated in the image, the integrity of the watermark can easily be verified. After the evaluation of our approach in terms of invisibility and robustness, the experimental results obtained show that our approach offers excellent imperceptibility (with a PSNR above 70 dB) and very good robustness against several geometric and destructive attacks.																	1868-5137	1868-5145															10.1007/s12652-020-02450-9		AUG 2020											
J								Wiener filter based deep convolutional network approach for classification of satellite images	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Deep convolution neural network; Satellite images; Object based classification; Wiener filter; GoogleNet		Semantic segmentation is a fundamental task in computer vision and image scenery detection. Many applications, such as urban planning, change detection, and environmental monitoring require accurate segmentation. Hence, most segmentation tasks are performed by humans. Currently, with the growth of deep convolutional neural network (DCNN), there are many works aimed to find the best network architecture fitting for this task. In this work, the GoogLeNet classifier is used to perform better segmentation as well as a classification for satellite images. The Wiener filter is used here for image denoising. Data Augmentation is performed to extract high information about the input picture. The output of the above steps helps in classification i.e. it identifies the scenery of the input image with four labels. The result shows that the GoogLeNet based image classification has reduced error rate and it also increases the accuracy of output. Additionally, the efficiency of the Wiener filters also described clearly in the result.																	1868-5137	1868-5145															10.1007/s12652-020-02410-3		AUG 2020											
J								Smart traffic management system in metropolitan cities	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Traffic management; Enhanced dijkstra's algorithm; Enhanced time forecasting algorithm; RDD	BIG DATA	Now-a-days, traffic congestion is one of the troubles in metro cities. It is mainly due to escalate in vehicle, population etc. Due to the raise in vehicle, the road is incapable of handling the traffic. Consequently it is very important to revamp the traffic management system. The advanced traffic management system intention is to ameliorate the safety and efficiency of the transportation system. Advanced traffic management system permit opportunities for new methods of evaluations and continuing assessment is provided. A few scenarios were analysed in the traffic issues and better performance is acquired in this research. The traffic map in New York City is taken as a database model and to discover the shortest path an advanced Dijkstra's algorithm is applied. The advanced form of Hadoop known as Apache spark is used. Spark is the open standard and it is easy to program as it has tons of high level operators which can be flexible in-memory data processing that enables batch, real-time, and advanced analytics on the Apache Hadoop platform. R-tool is an open source programming language and software environment for statistical computing is also employed. By using time forecasting algorithm, for each scenarios the vehicle speed, count, collusion, time, etc. is calculated and the Map Analysis is done with good performance by using R-tool. The overall performance of the system that enhances the traffic control efficiency is 96.23%																	1868-5137	1868-5145															10.1007/s12652-020-02453-6		AUG 2020											
J								Multi-agent architecture for fault recovery in self-healing systems	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Self-healing; Multi-agent architecture; Software automation; Fault recovery; Distributed systems	SOFTWARE SYSTEMS	Self-healing, a prominent property of self-adaptiveness provides reliability, availability, maintainability, and survivability to a software system. These qualitative factors are very salient to modern distributed systems in which components and their collaboration often vary. Survivability of such systems can be best addressed from an architectural viewpoint. When it comes to maintainability and reliability, architectural level adaptation is not often supported during the design phase. Adaptation to fault tolerance into the design phase of the system development process can increase the scope of software availability and thereby attaining self-healing. In distributed systems, most of the existing architectures are often associated with communication and correspondence as primary criteria. On the other hand, a multi-agent mechanism helps in schematic control of functionality, communication by emphasizing scalability. In this paper, a novel architecture was proposed that could support agent-based distributed systems to address fault recovery aspects for achieving self-adaptiveness. Unlike traditional multi-agent architecture, task-oriented functional multi-agent communication is incorporated for various activities during design phase designated to perform self-healing criteria. An adaptation of agent communication control flow is proposed using three novel mechanism such as planning, functioning and enacting as agents' critical responsibility. The paper also validates the proposed architecture for resource and availability based faults related to crash and resource unavailability using performance-based evaluation metrics. A case-based application with single thread connectivity is used to reflect the architecture during application design phase and is tested for success using mean response time as evaluation metric.																	1868-5137	1868-5145															10.1007/s12652-020-02443-8		AUG 2020											
J								Hybrid gravitational search algorithm based model for optimizing coverage and connectivity in wireless sensor networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Wireless sensor networks; Sensor deployment; Target coverage; Improved gravitational search algorithm; Social ski-driver optimization	FAULT-TOLERANCE; K-COVERAGE; OPTIMIZATION; DEPLOYMENT; MECHANISM; PROTOCOLS; ISSUES	Recently, the wireless sensor networks (WSNs) found its extensive application in surveillance and target tracking. For these two WSN applications, connectivity and coverage play a major role most particularly for target tracking. A large number of available sensor nodes track targets, during which a massive redundant data gets generated, which may minimize the system performance. Most particularly during the sensor node failure, the major intention of coverage and connectivity optimization model is to select less number of sensor nodes with maximum direct sensor node connectivity. But existing algorithms fail to achieve minimal node selection, therefore to mitigate the barriers of the traditional coverage algorithms, this paper proposed the hybrid Gravitational Search algorithm with social ski-driver (GSA-SSD) based model. This hybrid approach in target based WSN optimizes the coverage and connectivity requirement. By adapting the dynamic behaviour of SSD algorithm, the performance of GSA gets improved. Finally, the relative performance of the proposed hybrid GSA-SSD based optimization model is validated and compared with other optimization algorithms. On the basis of uncovered area rate and a number of sensor nodes the performance is evaluated. The results are implemented in the MATLAB simulation tool. Further, the performance enhancement in terms of uncovered area rate, number of selected active sensors, energy consumption, connectivity and network lifetime is achieved with randomly deployed nodes.																	1868-5137	1868-5145															10.1007/s12652-020-02442-9		AUG 2020											
J								A run-to-run controller for a chemical mechanical planarization process using least squares generative adversarial networks	JOURNAL OF INTELLIGENT MANUFACTURING										Chemical mechanical planarization; Run-to-run control; Least squares generative adversarial networks; Convolutional neural network; Bayesian optimization	VIRTUAL METROLOGY; RATE PREDICTION; SEMICONDUCTOR	Achieving high processing quality for chemical mechanical planarization (CMP) in semiconductor manufacturing is difficult due to the distinct process variations associated with this method, such as drift and shift. Run-to-run control aims to maintain the targeted process quality by reducing the effect of process variations. The goal of controller learning is to infer an underlying output-input reverse mapping based on input-output samples considering the process variations. Existing controllers learn reverse mapping by minimizing the total mapping error for sample data. However, this approach often fails to generate inputs for unseen target outputs because conditional input distributions on target outputs are not captured in the learning. In this study, we propose a controller based on a least squares generative adversarial network (LSGAN) that can capture the input distributions. GANs are deep-learning architectures composed of two neural nets: a generator and a discriminator. In the proposed model, the generator attempts to produce fake input distributions that are similar to the real input distributions considering the process variation features extracted using convolutional layers, while the discriminator attempts to detect the fake distributions. Competition in this game drives both networks to improve their performance until the generated input distributions are indistinguishable from the real distributions. An experiment using the data obtained from a work-site CMP tool verified that the proposed model outperformed the comparison models in terms of control accuracy and computation time.																	0956-5515	1572-8145															10.1007/s10845-020-01639-1		AUG 2020											
J								Efficient image encryption scheme based on generalized logistic map for real time image processing	JOURNAL OF REAL-TIME IMAGE PROCESSING										Chaos; Encryption; Logistic map; Computational complexity; Real time; Image processing	CRYPTOGRAPHY; SECURITY	In this era of the information age with digitalization, the transmission of sensitive real-time image information over insecure channels is highly-likely to be accessed or even attacked by an adversary. To prevent such unauthorized access, cryptography is being used to convert sensitive information in real-time images into unintelligible data. Most of the time, schemes are proposed with a high level of security. However, the challenge always remains the slower speeds due to their high complexity which makes them unusable in the applications of real-time images. In this paper, an efficient image encryption algorithm has been developed and tested for real-time images. The proposed scheme makes use of encryption with an efficient permutation technique based on a modular logistic map to bring down the size of the chaotic value vector, required to permute real-time image. We show that an efficient permutation is obtained using only root N chaotic numbers for a square image with 3N pixels (N Pixels in each color bit plane). The algorithm makes use of a 192-bit key; divided into smaller blocks and each block selected chaotically to diffuse the pixel using multiple XOR operations. The experimental analysis reveals that the proposed algorithm is immune to various statistical and differential attacks such as entropy, histogram analysis, spectral characteristic analysis, etc. A comparison of the proposed scheme with some state-of-the-art techniques show that it performs better, and as such, can be utilized for efficient real-time image encryption.																	1861-8200	1861-8219															10.1007/s11554-020-01008-4		AUG 2020											
J								Pythagorean fuzzy MULTIMOORA method based on distance measure and score function: its application in multicriteria decision making process	KNOWLEDGE AND INFORMATION SYSTEMS										Pythagorean fuzzy set; Distance measure; Score function; Multicriteria decision making; MOORA	AGGREGATION OPERATORS; SIMILARITY MEASURES; MEMBERSHIP GRADES; OPERATIONAL LAWS; SETS; TOPSIS; TODIM	The MULTIMOORA method is better than some of the existing decision making methods. However, it has not been improved to process Pythagorean fuzzy sets (PFSs). The decision results of the MULTIMOORA method greatly depend on the distance measure and score function. Although there are many studies focusing on proposing distance measures and score functions for PFSs, they still show some defects. In this paper, we propose two novel distance measures and a novel score function for PFSs for proposing a novel Pythagorean fuzzy MULTIMOORA method. To this end, two distance measures, Dice distance and Jaccard distance, are proposed for computing the deviation degree between two PFSs, and their general forms are also discussed. Afterward, a novel score function based on determinacy degree and indeterminacy degree is put forward for approximately representing PFSs. Then, the original MULTIMOORA method is extended by using the Dice distance and score function and it is used to solve the multicriteria decision making problems under the PFS information context. Finally, a real case for evaluating solid-state disk productions is handled using the proposed Pythagorean fuzzy MULTIMOORA method and another case for evaluating energy projects is given to verify the advantages of our studies by comparing them with the existing Pythagorean fuzzy distance measures, score functions, and decision making methods.																	0219-1377	0219-3116				NOV	2020	62	11					4373	4406		10.1007/s10115-020-01491-y		AUG 2020											
J								Semi-supervised Weighted Ternary Decision Structure for Multi-category Classification	NEURAL PROCESSING LETTERS										Semi-supervised learning; Multi-category classification; Weighted ternary decision structure	SUPPORT VECTOR MACHINE; SEMISUPERVISED CLASSIFICATION; IMAGE CLASSIFICATION; VIEWS	Semi-supervised learning has attracted researchers due to its advantages over supervised learning. In this paper, an extremely fast multi-category classification algorithm, termed as weighted ternary decision structure (WTDS) is proposed. WTDS is a generic algorithm that can extend any binary classifier into multi-category framework. This work also proposes a novel semi-supervised binary classifier termed as Weighted Laplacian least-squares twin support vector machine which is further extended using WTDS. The novel semi-supervised classifier obtains the solution by formulating a pair of Unconstrained Minimization Problems which are solved as systems of linear equation. WTDS takes advantage of the strengths of the classifier and efficiently constructs the multi-category classifier model in the form of a decision structure. WTDS outperforms other state-of-the-art multi-category approaches in terms of classification accuracy and time complexity. To confirm the feasibility and efficacy of proposed algorithm, experiments are conducted on benchmark UCI datasets.																	1370-4621	1573-773X				OCT	2020	52	2			SI		1555	1582		10.1007/s11063-020-10323-7		AUG 2020											
J								Pathological lung segmentation based on random forest combined with deep model and multi-scale superpixels	NEURAL PROCESSING LETTERS										Pathological lung segmentation; Convolutional neural network; Random forest; Divide-and-conquer strategy	IMAGE-ANALYSIS	Accurate segmentation of lungs in pathological thoracic computed tomography (CT) scans plays an important role in pulmonary disease diagnosis. However, it is still a challenging task due to the variability of pathological lung appearances and shapes. In this paper, we proposed a novel segmentation algorithm based on random forest (RF), deep convolutional network, and multi-scale superpixels for segmenting pathological lungs from thoracic CT images accurately. A pathological thoracic CT image is first segmented based on multi-scale superpixels, and deep features, texture, and intensity features extracted from superpixels are taken as inputs of a group of RF classifiers. With the fusion of classification results of RFs by a fractional-order gray correlation approach, we capture an initial segmentation of pathological lungs. We finally utilize a divide-and-conquer strategy to deal with segmentation refinement combining contour correction of left lungs and region repairing of right lungs. Our algorithm is tested on a group of thoracic CT images affected with interstitial lung diseases. Experiments show that our algorithm can achieve a high segmentation accuracy with an average DSC of 96.45% and PPV of 95.07%. Compared with several existing lung segmentation methods, our algorithm exhibits a robust performance on pathological lung segmentation. Our algorithm can be employed reliably for lung field segmentation of pathologic thoracic CT images with a high accuracy, which is helpful to assist radiologists to detect the presence of pulmonary diseases and quantify its shape and size in regular clinical practices.																	1370-4621	1573-773X				OCT	2020	52	2			SI		1631	1649		10.1007/s11063-020-10330-8		AUG 2020											
J								An adaptive enhancement method for low illumination color images	APPLIED INTELLIGENCE										Color image enhancement; Low illumination; Color space; Gamma correction; Particle swarm optimization	HISTOGRAM EQUALIZATION; CONTRAST ENHANCEMENT; ALGORITHM; RETINEX	In order to effectively improve the visual effect and image quality of color images under low illumination conditions, we propose an image enhancement method based on HSV and CIEL*a*b* color spaces for adaptively enhancing color image under low illumination conditions. The proposed method takes into account the characteristics of low illumination color images, and has the strategies of contrast, brightness enhancement, and color saturation correction. We utilize our proposed adaptive chaotic particle swarm optimization algorithm in this paper combined with gamma correction to improve the overall brightness of the image, and generate the best brightness adjustment effect in the proposed algorithm. In addition, our improved adaptive stretching function is used to enhance the image saturation. The experimental results show that compared with other traditional and latest color image enhancement algorithms, the proposed algorithm significantly enhances the visual effect of the low illumination color images. It can not only improve the contrast of low illumination color images and avoid color distortion, but also effectively improve the brightness of the image and provide more detail enhancement while maintaining the naturalness of the image.																	0924-669X	1573-7497															10.1007/s10489-020-01792-3		AUG 2020											
J								SLER: Self-generated long-term experience replay for continual reinforcement learning	APPLIED INTELLIGENCE										Continual reinforcement learning; Catastrophic forgetting; Dual experience replay; Experience replay model	NEURAL-NETWORKS; LEVEL; GAME; GO	Deep reinforcement learning has achieved significant success in various domains. However, it still faces a huge challenge when learning multiple tasks in sequence. This is because the interaction in a complex setting involves continual learning that results in the change in data distributions over time. A continual learning system should ensure that the agent acquires new knowledge without forgetting the previous one. However, catastrophic forgetting may occur as the new experience can overwrite previous experience due to limited memory size. The dual experience replay algorithm which retains previous experience is widely applied to reduce forgetting, but it cannot be applied in scalable tasks when the memory size is constrained. To alleviate the constrained by the memory size, we propose a new continual reinforcement learning algorithm called Self-generated Long-term Experience Replay (SLER). Our method is different from the standard dual experience replay algorithm, which uses short-term experience replay to retain current task experience, and the long-term experience replay retains all past tasks' experience to achieve continual learning. In this paper, we first trained an environment sample model called Experience Replay Mode (ERM) to generate the simulated state sequence of the previous tasks for knowledge retention. Then combined the ERM with the experience of the new task to generate the simulation experience all previous tasks to alleviate forgetting. Our method can effectively decrease the requirement of memory size in multiple tasks, reinforcement learning. We show that our method in StarCraft II and the GridWorld environments performs better than the state-of-the-art deep learning method and achieve a comparable result to the dual experience replay method, which retains the experience of all the tasks.																	0924-669X	1573-7497															10.1007/s10489-020-01786-1		AUG 2020											
J								Diagnosis of complications of type 2 diabetes based on weighted multi-label small sphere and large margin machine	APPLIED INTELLIGENCE										Type 2 diabetes; Complications; Hyper-sphere support vector machine; Multi-label learning	SUPPORT VECTOR MACHINE; NEURAL-NETWORKS; CLASSIFICATION; RISK	At present, type 2 diabetes mellitus (T2DM) is one of the most serious and critical health problems. Persistent hyperglycemia of diabetic patients can lead to other complications, such as macrovascular, microvascular, neuropathy, which are the main cause of death in diabetic patients. Therefore, it is an urgent task to diagnose the complications. To address the above issue, we turn it into a multi-label classification problem by taking macrovascular, microvascular, neuropathy as three labels. Furthermore, we find that it is an imbalanced classification problem for each label. Thus, a novel weighted multi-label small sphere and large margin machine (WML-SSLM) is proposed to diagnose the complications from T2DM in this paper, which is constructed by introducing the binary relevance (BR) method to SSLM. Compared with the BR method, WML-SSLM considers the relevance of labels by giving different weights for different instances. Taking the diabetes dataset from the Chinese PLA General Hospital as the research object, the diagnosis of the macrovascular, microvascular, and neuropathy from T2DM are studied by using our proposed WML-SSLM. The experimental results show that WML-SSLM can effectively deal with the prediction of complications of T2DM. Besides, the relevant features of each complication are analyzed by using the student's t-test.																	0924-669X	1573-7497															10.1007/s10489-020-01824-y		AUG 2020											
J								Time-efficient spliced image analysis using higher-order statistics	MACHINE VISION AND APPLICATIONS										Digital image forgery; Noise distribution; Fourth-order statistic; Time-efficient	NOISE; FORGERIES; ALGORITHM; FEATURES	Image forgery is gaining huge momentum as changing the content is no longer arduous. One of the leading techniques of this category is image splicing. This technique generates a composite image formed by combining regions of images. Once the image is forged, it becomes nearly impossible for the human expert to substantiate. Hence, for detecting and localizing the spliced region in the forged image, a tool is to be developed which has become the need of the hour. Articles have been reported that one of the key ingredients for such a tool is noise inconsistency, among others. The spliced region contains the non-homogeneous distribution of noise which acts as a feature to localize it. State-of-the-art techniques based on inconsistent noise are suffering from challenges like the requirement of prior knowledge about the image, localization of spliced region and estimation of inconsistent non-gaussian noise. In this paper, a blind local noise estimation technique has been introduced using a fourth-order central moment to localize the spliced region. This paper tries to overcome the challenges of state-of-the-art techniques. Experimental analysis has been done on images of three publicly available datasets. The results are evaluated on pixel level using confusion matrix and some other performance measures. The result of the given approach is compared with previously reported techniques and found better than them.																	0932-8092	1432-1769				AUG 7	2020	31	7-8							56	10.1007/s00138-020-01107-z													
J								A study on evolutionary computing based web service selection techniques	ARTIFICIAL INTELLIGENCE REVIEW										Web service; Web service selection; Evolutionary computing; Genetic algorithm; Ant colony optimization; Particle swarm optimization; Artificial bee colony	PARTICLE SWARM OPTIMIZATION; ANT COLONY OPTIMIZATION; GENETIC ALGORITHM; QUALITY	Many service providers are offering their business functionality as web services. The problem of web service selection is a complex and time-consuming activity. Among other techniques, a significant work has been reported on the use of evolutionary computing based algorithms in determining optimal web service for a task. A rigorous review of the state-of-the-art for efficient selection of web services using evolutionary computing based algorithms published over the last decade is presented. The existing works on web service selection using various evolutionary approaches with a discussion on algorithmic variations, their effect on selection, quality of service parameters used, contributions, limitations and research gaps of these works are explored.																	0269-2821	1573-7462															10.1007/s10462-020-09872-z		AUG 2020											
J								A dog food recommendation system based on nutrient suitability	EXPERT SYSTEMS										content-based recommendation; dog food; nutrient profiling; pet food; recommender		The demand for a food recommendation service for dogs has rapidly increased with the increasing number of pet owners, because it is generally difficult for dog owners to find food that is perfectly suitable for their dogs' health condition. The purpose of this study is to develop an algorithm for recommending dog food that contains appropriate nutrients based on the physical and health conditions of the dogs. This study proposes a nutrient profiling-based recommendation algorithm (NRA) for dog food. The proposed algorithm tries to recommend appropriate or inappropriate dog food by using collective intelligence based on user experience and the prior knowledge of experts. Based on the physical and health status of dogs, this study extracts which nutrients are necessary for the dogs and recommends the most suitable dog food containing these nutrients. A performance evaluation was implemented in terms of recall, precision, F1 and AUC. As a result of the performance evaluation, the AUC performance of this NRA is 20% higher than k-NN and 9.7% higher than the SVD model. In addition, the NRA proved to be an evolving system in which the performance of recommendations improves as users' feedback accumulates.																	0266-4720	1468-0394														e12623	10.1111/exsy.12623		AUG 2020											
J								Motion Guided LiDAR-Camera Self-calibration and Accelerated Depth Upsampling for Autonomous Vehicles	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Calibration; Depth reconstruction; Super-resolution; Autonomous vehicles; Lidar; Camera; Multimoda; Sensor fusion; Motion		This work proposes a novel motion guided method for targetless self-calibration of a LiDAR and camera and use the re-projection of LiDAR points onto the image reference frame for real-time depth upsampling. The calibration parameters are estimated by optimizing an objective function that penalizes distances between 2D and re-projected 3D motion vectors obtained from time-synchronized image and point cloud sequences. For upsampling, a simple, yet effective and time efficient formulation that minimizes depth gradients subject to an equality constraint involving the LiDAR measurements is proposed. Validation is performed on recorded real data from urban environments and demonstrations that our two methods are effective and suitable to mobile robotics and autonomous vehicle applications imposing real-time requirements is shown.																	0921-0296	1573-0409															10.1007/s10846-020-01233-w		AUG 2020											
J								A comparative study of pre-screening strategies within a surrogate-assisted multi-objective algorithm framework for computationally expensive problems	NEURAL COMPUTING & APPLICATIONS										Computationally expensive problem; Multi-objective evolutionary algorithm; Pre-screening strategy; Surrogate model; Multi-offspring method	PARTICLE SWARM OPTIMIZATION; EVOLUTIONARY ALGORITHM; IMPROVEMENT CRITERIA; GENETIC ALGORITHM; MODEL; APPROXIMATION; REGRESSION; ENSEMBLE; SEARCH; BUDGET	The multi-offspring method has been recognized as an efficient approach to enhance the performance of multi-objective evolutionary algorithms. However, some pre-screening strategies should be used when a multi-offspring-assisted multi-objective evolutionary algorithm is used to solve computationally expensive problems. So far, there is no any reported comprehensive study that compares the effects of different pre-screening strategies on the performance of the multi-offspring-assisted multi-objective evolutionary algorithms. In this paper, four pre-screening strategies (convergence-based, maximin distance-based expected improvement matrix (EIM-based), diversity-based and random-based strategies) for the multi-offspring-assisted multi-objective evolutionary algorithm are compared. The convergence-based strategy gives more priority to non-dominated solutions, and it is vital for exploiting the current promising areas. The diversity-based strategy gives more priority to solutions with greater uncertainties, and it is important for exploring the sparse areas. The EIM-based strategy considers the exploration and exploitation simultaneously, and the random-based strategy gives no priority to any solution. A series of benchmark problems whose dimensions vary from 8 to 30 and a reactive power optimization problem are used to test the multi-offspring-assisted multi-objective evolutionary algorithm under the four pre-screening strategies. The experimental results show that the convergence-based strategy performs best on most of the simple problems, while the EIM-based strategy performs best on most of the complex problems. The diversity-based strategy can produce positive effects on some problems, while the random-based strategy cannot improve the performance of its basic algorithm.																	0941-0643	1433-3058															10.1007/s00521-020-05258-y		AUG 2020											
J								Empirical evaluation of multi-task learning in deep neural networks for natural language processing	NEURAL COMPUTING & APPLICATIONS										Natural language processing; Multi-task learning; Deep learning		Multi-task learning (MTL) aims at boosting the overall performance of each individual task by leveraging useful information contained in multiple-related tasks. It has shown great success in natural language processing (NLP). Currently, a number of MTL architectures and learning mechanisms have been proposed for various NLP tasks, including exploring linguistic hierarchies, orthogonality constraints, adversarial learning, gate mechanism, and label embedding. However, there is no systematic exploration and comparison of different MTL architectures and learning mechanisms for their strong performance in-depth. In this paper, we conduct a thorough examination of five typical MTL methods with deep learning architectures for a broad range of representative NLP tasks. Our primary goal is to understand the merits and demerits of existing MTL methods in NLP tasks, thus devising new hybrid architectures intended to combine their strengths. Following the empirical evaluation, we offer our insights and conclusions regarding the MTL methods we have considered.																	0941-0643	1433-3058															10.1007/s00521-020-05268-w		AUG 2020											
J								Minimum-cost capacitated fuzzy network, fuzzy linear programming formulation, and perspective data analytics to minimize the operations cost of American airlines	SOFT COMPUTING										Fuzzy network; Fuzzy logic; Fuzzy linear programming; Airline industry	INEQUALITY CONSTRAINTS; DECISION-MAKING; SIMPLEX-METHOD; ALGORITHM; MODELS	Minimum-cost capacitated fuzzy network is formulated as a fuzzy linear programming problem. A novel fuzzy linear programming formulation for minimum-cost capacitated fuzzy network where the total resource constraints are fuzzy is proposed. The proposed model is then implemented to minimize the operations cost of American Airlines. The research is helpful to identify the most profitable destinations for the American Airlines. Twelve origin/destination pairs are taken into considerations namely Atlantic (A), Latin American (L), Pacific (P) and Domestic (D). Flight operations capacity, Available Seat Miles ASM, is taken as a measure of capacity. The goal is to minimize the flight operations cost while ensuring maximum flight operations capacities to all destinations. This is followed by perspective data analytics for "What American Airline should do to be more profitable?" Perspective analytics suggest the airline to extend flight operations capacity in certain origin/destination pairs, whereas to maintain the previous approximate average capacity for those having high operations costs. The solution of the proposed fuzzy model suggests that flight operations capacity ASM can be significantly increased by 22345148 (000) with relatively small increase 1539356 (000) USD in operations cost. The fuzzy model is superior for it emphasizes to increase flight operations capacity ASM for the origin/destination pairs with minimum flight operations costs.																	1432-7643	1433-7479															10.1007/s00500-020-05228-5		AUG 2020											
J								Model selection techniques for sparse weight-based principal component analysis	JOURNAL OF CHEMOMETRICS										model selection; multiblock data; sparse PCA	REGULARIZATION; REGRESSION; TUTORIAL; JOINT	Many studies make use of multiple types of data that are collected for the same set of samples, resulting in so-called multiblock data (e.g., multiomics studies). A popular analysis framework is sparse principal component analysis (PCA) of the concatenated data. The sparseness in the component weights of these models is usually induced by penalties. A crucial factor in the use of such penalized methods is a proper tuning of the regularization parameters used to give more or less weight to the penalties. In this paper, we examine several model selection procedures to tune these regularization parameters for sparse PCA. The model selection procedures include cross-validation, Bayesian information criterion (BIC), index of sparseness, and the convex hull procedure. Furthermore, to account for the multiblock structure, we present a sparse PCA algorithm with a group least absolute shrinkage and selection operator (LASSO) penalty added to it, to either select or cancel out blocks of data in an automated way. Also, the tuning of the group LASSO parameter is studied for the proposed model selection procedures. We conclude that when the component weights are to be interpreted, cross-validation with the one standard error rule is preferred; alternatively, if the interest lies in obtaining component scores using a very limited set of variables, the convex hull, BIC, and index of sparseness are all suitable.																	0886-9383	1099-128X														e3289	10.1002/cem.3289		AUG 2020											
J								IoT-based system to measure thermal insulation efficiency	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Thermal insulation; Weather condition; Internet of things; Environmental parameters measurement		The main purpose of thermal insulation is to reduce the effect of weather conditions, which can reduce energy consumption and help preserve the natural environment. Due to increasing demand for thermal insulation in modern buildings, it is necessary to monitor the effects of weather condition on the efficiency of thermal insulation. To measure this effect, Internet of Things (IoT)-based sensor networks play an important role. This paper proposes an IoT-based monitoring system that measures and analyses the effect of weather conditions on thermal insulation efficiency. The proposed system includes an Arduino as a central unit that interfaces at the input with temperature and relatively humidity sensors, and at the output with a WiFi module that transmit the collected data via the Internet to an IoT analytics platform to aggregate, analyze, and visualize the data. The proposed system is low cost and provides insight into the design and implementation of a complete application with important IoT features, e.g., sensing and transmitting data to the cloud, data processing, and data retrieval through the ThingSpeak open-source API. In a proof of concept, the proposed system was implemented and tested in different environments and at different altitudes, and the results were analyzed and compared to standard weather data. The sensed data can be further analyzed and leveraged by end users to mitigate the effects of weather conditions on thermal insulation.																	1868-5137	1868-5145															10.1007/s12652-020-02459-0		AUG 2020											
J								Hybridization of Moth flame optimization algorithm and quantum computing for gene selection in microarray data	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Gene expression; Feature selection; Moth flame optimization algorithm; Quantum computing; Microarray data; Cancer classification; Bio-inspired algorithms; Molecular biology; Optimization algorithms; Evolutionary algorithms; Swarm intelligence	MOLECULAR CLASSIFICATION; MUTUAL INFORMATION; EXPRESSION; HYBRID; CANCER; PREDICTION; PATTERNS; TUMOR; CARCINOMAS	Ever-increasing data in various fields like Bioinformatics field, which has led to the need to find a way to reduce the data dimensionality. Gene selection problem has a large number of genes (relevant, redundant or noise), which needs an effective method to help us in detecting diseases and cancer. In this problem, computational complexity is reduced by selecting a small number of genes, but it is necessary to choose the relevant genes to keep a high level of accuracy. Therefore, in order to find the optimal gene subset, it is essential to devise an effective exploration approach that can investigate a large number of possible gene subsets. In addition, it is required to use a powerful evaluation method to evaluate the relevance of these gene subsets. In this paper, we present a novel swarm intelligence algorithm for gene selection called quantum moth flame optimization algorithm (QMFOA), which based on hybridization between quantum computation and moth flame optimization (MFO) algorithm. The purpose of QMFOA is to identify a small gene subset that can be used to classify samples with high accuracy. The QMFOA has a simple two-phase approach, the first phase is a pre-processing that uses to address the difficulty of high-dimensional data, which measure the redundancy and the relevance of the gene, in order to obtain the relevant gene set. The second phase is a hybridization among MFOA, quantum computing, and support vector machine with leave-one-out cross-validation, etc., in order to solve the gene selection problem. We use quantum computing to guarantee a good trade-off between the exploration and the exploitation of the search space, while a new update moth operation using Hamming distance and Archimedes spiral allows an efficient exploration of all possible gene-subsets. The main objective of the second phase is to determine the best relevant gene subset of all genes obtained in the first phase. In order to assess the performance of the proposed QMFOA, we test QMFOA on thirteen microarray datasets (six binary-class and seven multi-class) to evaluate and compare the classification accuracy and the number of genes selected by the QMFOA against many recently published algorithms. Experimental results show that QMFOA provides greater classification accuracy and the ability to reduce the number of selected genes compared to the other algorithms.																	1868-5137	1868-5145															10.1007/s12652-020-02434-9		AUG 2020											
J								Human adaptive mechatronics system integrated with cybernetics loop using neuromuscular controller in occupational therapy for elderly person with disability	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Human adaptive mechatronics; Cybernetics; EMG signal; Human interface loop; Occupational therapy	FUNCTIONAL ELECTRICAL-STIMULATION; DESIGN; ROBOTICS	Human adaptive mechatronics (HAM) device is operated using neuromuscular interface controller. The muscle activity is monitored with the help of electromyogram (EMG) signals. The change in signal strength due to the volunteering movement of body parts is detected by placing few electrodes on the skin surface. EMG signal is used as the control signal and it gives the movement coordination. The amplitude of the signal changes with respect to the muscle activity. The HAM device is made to operate depending upon the amplitude changes in the generated EMG signal. The option of using cybernetic loop facilitates the balance of the control error and the delayed response time of the system. This paper elaborates the working of human adaptive neuromuscular interface controller in mechatronics device and the simulation is carried out in real time approach for the different muscle activity mainly between the two actions: volunteer motion and intended motion.																	1868-5137	1868-5145															10.1007/s12652-020-02405-0		AUG 2020											
J								An optimized deep learning network model for EEG based seizure classification using synchronization and functional connectivity measures	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Multicast security; Multiple logical key trees; Group key management; One-way key derivation; Rekeying process; False prediction rate; Convolutional neural networks	PREDICTION; INDEX	Epilepsy is a brain disorder related to alteration in the nervous system which affects around 65 million people among the world's population. Few works are focused on prediction of seizure relied on deep learning approaches, but the capability of optimal design has no longer been absolutely exploited. This work is focused on the seizure prediction obtained from long-short time records using optimized deep learning network model (ODLN). In this paper, the synchronization patterns and its feasibility of distinguishing the pre-ictal from inter-ictal states are examined by utilizing the interaction graph model as a functional connectivity measure. An optimized deep learning network with short- long-term memory is computed for the prediction of epileptic seizures occurrences. For, the modelling of ODLN, pre-analysis is performed with three modules and memory layers. It is finalized from these results; a two-layer ODLN is optimum to perform the epileptic seizure prediction for four different window sizes from 15 to 120 min. The assessment is implemented on the CHB-MIT Scalp EEG data set, providing 100% sensitivity and low false prediction rate ranges from 0.10 to 0.02 for seizure prediction. The proposed ODLN methodology reveals a notable increase in the performance rate of seizure prediction when compared with existing machine learning and Convolutional neural networks methods.																	1868-5137	1868-5145															10.1007/s12652-020-02383-3		AUG 2020											
J								Secured transmission using trust strategy-based dynamic Bayesian game in underwater acoustic sensor networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Underwater acoustic sensor networks; Game-theoretic approach; Trust beliefs; Vulnerability analysis; Secure communication	COMMUNICATION; PROTOCOL	Cooperation among sensor nodes and the unreliability of acoustic channels are the significant challenges in underwater acoustic sensor networks (UASNs). In UASNs, the unreliability may results in high packet loss due to the high bit-error-rate and packets being dropped due to network congestion. High packet loss decreases the transmission rate in a network. An effective secure transmission mechanism is very much essential among the nodes in UASNs. In this paper, a trust strategy-based dynamic Bayesian game (TSDBG) model is proposed to resolve these problems. In TSDBG, a secure suite is created among the nodes in the network. Trust and Payoff are calculated for each node to evaluate the particular node involved in the packet-dropping and misbehaving activities that occurred during the transmission. Each node updates its trust value using Bayes' rule. Regular nodes are continuously monitored to analyze their neighbor nodes based on trust value. The simulation results reveal that the proposed scheme significantly reduces the packet-dropping attack, misbehaving activities of malicious nodes, propagation delay, and thereby enhancing secure transmission.																	1868-5137	1868-5145															10.1007/s12652-020-02418-9		AUG 2020											
J								Enhancement of energy efficiency in massive MIMO network using superimposed pilots	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										LTE; Pilot; Precoding schemes; Zero forcing; Superimposed pilot	CHANNEL ESTIMATION; SYSTEMS	Fifth generation wireless communication intends to provide extensive improvement in energy efficiency of long-term evolution-advanced network. This feature can be accomplished by incorporating spatial multiplexing which involves base station antenna in large number known as massive multiple input multiple output system. The potential constraint of this system is pilot contamination which introduces interference instigated by reusing pilots from neighbor cells in a multicell system. The pilot contamination can be reduced by varying the length of pilot sequences. The conventional system known as regular pilot uses data and pilot symbols disjointly for transmission. The proposed system known as superimposed pilots overlays the pilot and data symbols for transmission. This superimposed pilot allows to use pilot symbols in an extended duration and in turn reduces the pilot contamination. To analyze the performance of both the pilots, precoding schemes like maximal ratio combing and zero forcing is used. To enhance the performance further antenna selection algorithm has been implemented along with the precoding scheme. This algorithm will choose the antennas with threshold level for maximization of energy efficiency. The simulation results will prove that superimposed pilot with antenna selection algorithm will archive high average rate and energy efficiency than the conventional random pilots.																	1868-5137	1868-5145															10.1007/s12652-020-02414-z		AUG 2020											
J								Predictive analysis of identification and disease condition monitoring using bioimpedance data	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Bio impedance data; LabVIEW; SPSS software-surface electrodes; Impedance cardiography; Computational algorithm	BIOELECTRICAL-IMPEDANCE METHOD; BODY-COMPOSITION; DIAGNOSIS; VALIDATION	Bio-impedance is the manner in which a person's body opposes the applied alternating current in a unique manner. Acquired bio-impedance data is being used for diagnosis of critical diseases condition of various systems in human body. Electrode placement on body surface to sense impedance signal for various systems is unique format for each system. This paper first part focus on design of electrical bio-impedance based acquisition system with patient identification and later part deals with visualizing, analyzing and prediction of disease condition using software tools. Impedance data from 100 patients were acquired using designed system made with surface electrodes, signal generator with variable frequency and amplitude, Microcontroller board to process acquired data. Obtained impedance values at low frequencies are a good source of cardiac related information. Data obtained from bio impedance based multi-parameter monitoring system was visualized and analyzed for the prediction of age and disease condition. The validity of this system was made using intelligent diagnosis system based on IBM SPSS computational algorithms.																	1868-5137	1868-5145															10.1007/s12652-020-02452-7		AUG 2020											
J								Decentralized data outsourcing auditing protocol based on blockchain	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Blockchain; Data auditing; Fair trade; Public auditing; Decentralized framework	EFFICIENT; SERVICES; STORAGE	The rapid popularization of cloud computing and ultra-high-speed Internet has promoted the vigorous development of data sharing and collaborative office, especially in Big Data and AI. Aiming at protecting the security of users' data, researchers developed PDP scheme to verify data. However, existing schemes all rely on semi-trusted Third Party Auditor (TPA) or group management to store verification information. In order to solve this problem, we propose a distributed data integrity audit scheme based on blockchain. This scheme provides a brand-new method, which allows customers to store data safely without relying on any specific TPA and protect users' privacy at a lower cost. For the new concept, this paper points out the problems of the existing scheme and puts forward system model and security model. Then, a decentralized data integrity audit scheme using blockchain is designed. The proposed private PDP scheme based on blockchain is provably secure. At the same time, the security analysis and efficiency analysis show that the proposed PDP scheme is safe, efficient and practical.																	1868-5137	1868-5145															10.1007/s12652-020-02432-x		AUG 2020											
J								A Fuzzy Reinforcement Learning Approach for Continuum Robot Control	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Continuum robot; Fuzzy control; Reinforcement learning; Evolutionary algorithms; Taguchi method	MOTION CONTROL; MODEL; MANIPULATORS	Continuum robots (CRs) hold great potential for many medical and industrial applications where compliant interaction within the potentially confined environment is required. However, the navigation of CRs poses several challenges due to their limited actuation channels and the hyper-flexibility of their structure. Environmental uncertainty and characteristic hysteresis in such procedures add to the complexity of their operation. Therefore, the quality of trajectory tracking for continuum robots plays an essential role in the success of the application procedures. While there are a few different actuation configurations available for CRs, the focus of this paper will be placed on tendon-driven manipulators. In this research, a new fuzzy reinforcement learning (FRL) approach is introduced. The proposed FRL-based control parameters are tuned by the Taguchi method and evolutionary genetic algorithm (GA) to provide faster convergence to the Nash Equilibrium. The approach is verified through a comprehensive set of simulations using a Cosserat rod model. The results show a steady and accurate trajectory tracking capability for a CR.																	0921-0296	1573-0409															10.1007/s10846-020-01237-6		AUG 2020											
J								On the use of the Infinity Computer architecture to set up a dynamic precision floating-point arithmetic	SOFT COMPUTING										Infinity Computer; Dynamic precision floating-point arithmetic; Conditioning	STRONG HOMOGENEITY; INFINITESIMALS; COMPUTATIONS; METHODOLOGY; WORKING	We devise a variable precision floating-point arithmetic by exploiting the framework provided by the Infinity Computer. This is a computational platform implementing the Infinity Arithmetic system, a positional numeral system which can handle both infinite and infinitesimal quantities expressed using the positive and negative finite or infinite powers of the radix (1). The computational features offered by the Infinity Computer allow us to dynamically change the accuracy of representation and floating-point operations during the flow of a computation. When suitably implemented, this possibility turns out to be particularly advantageous when solving ill-conditioned problems. In fact, compared with a standard multi-precision arithmetic, here the accuracy is improved only when needed, thus not affecting that much the overall computational effort. An illustrative example about the solution of a nonlinear equation is also presented.																	1432-7643	1433-7479															10.1007/s00500-020-05220-z		AUG 2020											
J								Adoption and realization of deep learning in network traffic anomaly detection device design	SOFT COMPUTING										Deep learning; CNN; Circulatory neural network; Network traffic; Anomaly detection	NEURAL-NETWORKS; SYSTEM; IDENTIFICATION; PERFORMANCE; PREDICTION; PARAMETERS; SECURITY	In order to study the application of deep learning in the design of network traffic anomaly detection device, aiming at two common problems in the field of network anomaly detection: characteristic dependence and high false positive rate, the convolutional neural network (CNN) is combined with recurrent neural network (RNN) to propose the network anomaly detection method based on hierarchical spatiotemporal feature learning (HAST-NAD) based on deep learning. It automatically learns the traffic characteristics and improves the network traffic anomaly detection efficiency. First, the CNN is used to learn the spatial feature algorithm of data, and long-short term memory of RNN is used to learn the temporal feature algorithm of data. Then the two original data sets DARPA1998 and ISCX2012 are preprocessed. The accuracy, detection rate, and false positive rate of normal traffic and Dos, Probe, U2R, and R2L attack traffic are compared in DARPA1998 data set. The accuracy, detection rate, and false positive rate of normal traffic and Brute force SSH, DDoS, HttpDoS, and buffering attack traffic are compared in ISCX2012 data set. Finally, it is compared with other network traffic anomaly detection methods. The results show that when the network flow length is 800, the model shows good performance on the DARPA1998 data set (accuracy, detection rate and false positive rate are 98.68%, 97.78%, and 0.07%, respectively). When the network flow length is 600, the model performs better on the ISCX2012 dataset (accuracy, detection rate and false positive rate are 99.69%, 96.91%, and 0.22%, respectively). At the same time, when the packet length is 100 and the number of packets is 6, the model shows high precision, high detection rate, and low false positive rate on ISCX2012 data set. In the same data set, the temporal feature algorithm has better performance and lower false positive rate than the spatial feature algorithm. Compared with other network traffic anomaly detection methods, HAST-NAD has better comprehensive test results. In conclusion, the combination of CNN and RNN can better realize abnormal detection of network traffic, which has practical application and theoretical value.																	1432-7643	1433-7479															10.1007/s00500-020-05210-1		AUG 2020											
J								Transfer learning based hybrid 2D-3D CNN for traffic sign recognition and semantic road detection applied in advanced driver assistance systems	APPLIED INTELLIGENCE										Deep learning; Traffic sign recognition; Semantic road detection; Transfer learning; Hybrid 2D-3D CNN models	DEEP NEURAL-NETWORK; EFFICIENT	Annually, deep learning algorithms have proven their effectiveness in many vision-based applications, such as autonomous driving, traffic, and congestion monitoring, and so on. In computer vision, accurate traffic sign recognition and semantic road detection are vital challenges for increased safety, which are becoming a major research topic for intelligent transport systems community. In this paper, a deep learning-based driving assistance system has been proposed. To this end, we present hybrid 2D-3D CNN models based on the transfer learning paradigm to achieve better performance on benchmark real-world datasets. The primary goal of transfer learning is to improve the learning process in the target domain while transferring relevant knowledge from the source domain. We combine a pre-trained deep 2D CNN and a shallow 3D CNN to significantly reduce complexity and speed-up the training algorithm. The first model, called Hybrid-TSR, is intended to effectively address the task of traffic sign recognition. Hybrid-SRD is the second architecture that allows the semantic detection of road space through a combination of up-sampling and deconvolutional operations. The experimental results show that the proposed methods have considerable relevance in terms of efficiency and accuracy.																	0924-669X	1573-7497															10.1007/s10489-020-01801-5		AUG 2020											
J								Solving multi-objective optimization problem using cuckoo search algorithm based on decomposition	APPLIED INTELLIGENCE										Cuckoo search; Multi-objective; Decomposition; Angle-based selection; Adaptive operator selection	ANT COLONY OPTIMIZATION; EVOLUTIONARY ALGORITHM; OPERATOR; PERFORMANCE; STRATEGIES; CROSSOVER; SELECTION; MOEA/D	In recent years, cuckoo search (CS) algorithm has been successfully applied in single-objective optimization problems. In addition, decomposition-based multi-objective evolutionary algorithms (MOEA/D) have high performance for multi-objective optimization problems (MOPs). Inspired by this, a new decomposition-based multi-objective CS algorithm is proposed in this paper. Two reproduction operators with different characteristics derived from the CS algorithm are constructed and they compose an operator pool. Then, a bandit-based adaptive operator selection method is used to determine the application of different operators. An angle-based selection strategy that achieves a better balance between convergence and diversity is adopted to preserve diversity. Compared with other improved strategies designed for MOEA/D on two suits of test instances, the proposed algorithm was demonstrated to be effective and competitive for MOPs.																	0924-669X	1573-7497															10.1007/s10489-020-01816-y		AUG 2020											
J								Automatic feature scaling and selection for support vector machine classification with functional data	APPLIED INTELLIGENCE										Feature selection; Functional data; Support vector machines; Classification; Feature scaling	K-MEANS; CANCER CLASSIFICATION; PRINCIPAL-COMPONENTS; VARIABLE SELECTION; GENE SELECTION; FILTER METHOD; KERNEL; REGRESSION; ALGORITHMS; DESIGN	FunctionalData Analysis (FDA) has become a very important field in recent years due to its wide range of applications. However, there are several real-life applications in which hybrid functional data appear, i.e., data with functional and static covariates. The classification of such hybrid functional data is a challenging problem that can be handled with the Support Vector Machine (SVM). Moreover, the selection of the most informative features may yield to drastic improvements in the classification rates. In this paper, an embedded feature selection approach for SVM classification is proposed, in which the isotropic Gaussian kernel is modified by associating a bandwidth to each feature. The bandwidths are jointly optimized with the SVM parameters, yielding an alternating optimization approach. The effectiveness of our methodology was tested on benchmark data sets. Indeed, the proposed method achieved the best average performance when compared to 17 other feature selection and SVM classification approaches. A comprehensive sensitivity analysis of the parameters related to our proposal was also included, confirming its robustness.																	0924-669X	1573-7497															10.1007/s10489-020-01765-6		AUG 2020											
J								An evacuation simulation method based on an improved artificial bee colony algorithm and a social force model	APPLIED INTELLIGENCE										Artificial bee colony algorithm; Crowd evacuation; Computer simulation; Swarm intelligence algorithm	CROWD EVACUATION; OPTIMIZATION; NAVIGATION; DYNAMICS	Simulation modeling is an important tool for simulating crowd behavior and studying the law of crowd evacuation. It is of great significance for exploring evacuation management methods in emergency situations. The real-time change of evacuation is the main challenge of simulation modeling. In the evacuation simulation, it is difficult for people to choose a suitable route according to the change of evacuation dynamics. This paper proposes a new evacuation simulation method which combines an improved artificial bee colony algorithm for dynamic path planning and SFM (Social Force Model) for simulating the movement of pedestrians, to providing pedestrians with timely route selection. In the path planning layer, we developed a MABCM (Multiple-subpopulations Artificial Bee Colony with Memory) algorithm and proposed a new exit evaluation strategy. These methods can plan a route with the shortest evacuation time for pedestrians according to the dynamic changes of evacuation and improve evacuation efficiency. In the simulated motion layer, we use the SFM to avoid collisions and achieve the reproduction of the evacuation scene. We verified the performance of the proposed MABCM on the CEC 2014 benchmark suite, and the results show that it is superior to the four existing artificial bee colony algorithms in most cases. The proposed crowd evacuation method is verified on an existing SFM platform. The experimental results indicate that the proposed method can efficiently evacuate a dense crowd in multiple scenes and can effectively shorten evacuation time.																	0924-669X	1573-7497															10.1007/s10489-020-01711-6		AUG 2020											
J								Using memetic algorithm for robustness testing of contract-based software models	ARTIFICIAL INTELLIGENCE REVIEW										Robustness testing; Model testing; Graph transformation specification; Specification testing; Coverage criteria		Graph Transformation System (GTS) can formally specify the behavioral aspects of complex systems through graph-based contracts. Test suite generation under normal conditions from GTS specifications is a task well-suited to evolutionary algorithms such as Genetic and Particle Swarm Optimization (PSO) metaheuristics. However, testing the vulnerabilities of a system under unexpected events such as invalid inputs is essential. Furthermore, the mentioned global search algorithms tend to make big jumps in the system's state-space that are not concentrated on particular test goals. In this paper, we extend the HGAPSO approach into a cost-aware Memetic Algorithm (MA) by making small local changes through a proposed local search operator to optimize coverage score and testing costs. Moreover, we test GTS specifications not only under normal events but also under unexpected situations. So, three coverage-based testing strategies are investigated, including normal testing, robustness testing, and a hybrid strategy. The effectiveness of the proposed test generation algorithm and the testing strategies are evaluated through a type of mutation analysis at the model-level. Our experimental results show that (1) the hybrid testing strategy outperforms normal and robustness testing strategies in terms of fault-detection capability, (2) the robustness testing is the most cost-efficient strategy, and (3) the proposed MA with the hybrid testing strategy outperforms the state-of-the-art global search algorithms.																	0269-2821	1573-7462															10.1007/s10462-020-09881-y		AUG 2020											
J								Risk assessment in discrete production processes considering uncertainty and reliability: Z-number multi-stage fuzzy cognitive map with fuzzy learning algorithm	ARTIFICIAL INTELLIGENCE REVIEW										Failure mode and effects analysis; Multi-stage fuzzy cognitive map; Z-number theory; Fuzzy learning algorithm; Risk assessment	MULTICRITERIA DECISION-MAKING; FAILURE MODE; EXTENDED MULTIMOORA; PARTICLE SWARM; MCDM MODEL; TOPSIS; FMEA; PRIORITIZATION; METHODOLOGY; ENTROPY	The Failure Mode and Effects Analysis (FMEA) technique due to its proactive nature can identify failures and their causes as well as potential effects, and provide preventive/controlling measures before they occur. Nevertheless, some of the shortcomings of the FMEA technique like lack of a mental framework for considering the relationships between risks, lack of systematic perspective in confronting with risks, and weakness of Risk Priority Number (RPN) score in mathematical basis and disregarding the uncertainty of problem reduce the reliability of the outputs. In this study, an approach based on the Multi-Stage Fuzzy Cognitive Map and the Z-number theory (Z-MSFCM) is proposed to simultaneously consider the concept of uncertainty and reliability in quantities of risk factors and the weights of causal relationships in the MSFCM. Besides, a novel learning approach for Z-MSFCM has been applied based on the combination of the Particle Swarm Optimization (PSO) and S-shaped transfer function (PSO-STF) to preserve the uncertain environment of the problem. The proposed approach has been applied in a manufacturing automotive parts company and results indicate that: first, Z-MSFCM by considering the causal relationships between risks and their uncertainty and reliability in comparison with traditional RPN can provide better process-oriented insight into the impact of risks on the system; and second, the PSO-STF has high potential in generating solutions with high separability compared to Nonlinear Hebbian Learning and PSO algorithms. To put it differently, the mentioned advantages of the proposed approach can help decision-makers to analyze the problem with high reliability.																	0269-2821	1573-7462															10.1007/s10462-020-09883-w		AUG 2020											
J								Classification of trapezoidal bipolar neutrosophic number, de-bipolarization technique and its execution in cloud service-based MCGDM problem	COMPLEX & INTELLIGENT SYSTEMS										Trapezoidal bipolar neutrosophic number; De-bipolarization; Multi criterion group decision making problem	GROUP DECISION-MAKING; AGGREGATION OPERATORS; TOPSIS METHOD; FUZZY-SETS; DEFUZZIFICATION; EXTENSION; SELECTION; RANKING	Neutrosophic set can deal with the uncertainties related to the information of any decision making problem in real life scenarios, where fuzzy set may fail to handle those uncertainties properly. In this study, we present the perception of trapezoidal bipolar neutrosophic numbers and its classification in different frame. We introduce the idea of disjunctive structures of trapezoidal bipolar neutrosophic numbers namely type-1 trapezoidal bipolar neutrosophic number, type-2 trapezoidal bipolar neutrosophic numbers, and type-3 trapezoidal bipolar neutrosophic number based on the perception of dependency among membership functions in neutrosophic set. In any neutrosophic decision-making problem, the decision maker uses the comparison of neutrosophic numbers to choose among alternatives solutions. Here, we introduce a ranking method, i.e., De-bipolarization scheme for trapezoidal bipolar neutrosophic number (TrBNN) using removal area technique. We also describe the utility of trapezoidal bipolar neutrosophic number and its appliance in a multi criteria group decision making problem (MCGDM) for distinct users in trapezoidal bipolar arena which is more ethical, precise and reliable in neutrosophic field.																	2199-4536	2198-6053															10.1007/s40747-020-00170-3		AUG 2020											
J								Massive MIMO perspective: improved sea lion for optimal antenna selection	EVOLUTIONARY INTELLIGENCE										MIMO; Massive MIMO; Transmit antenna selection; Optimization; Sea lion algorithm	SYSTEMS; ALGORITHM	The massive MIMO is an advanced technology in the wideband wireless communication system's future, which provokes extensive attention in both academia and telecommunication industry. Pilot contamination is considered as a fundamental issue in the system of massive MIMO. The designing of wireless systems has the main concern over the system throughput. Though, the environmental protection and energy-saving have concerned as inevitable trends and global demands. Hence, on considering all these consequences, this paper intends to introduce a new improved sea lion optimization algorithm to select the optimal transmit antennas selection by accounting the multi-objective issue that maximizes both the relative energy efficiency and capacity. In fact, the proposed algorithm is the enhanced version of traditional sea lion optimization algorithm, which optimally tunes the count of transmit antennas and determines which antenna to be selected. Finally, the performance of proposed work is compared and proved over other conventional models regarding capacity analysis, relative efficiency analysis, and optimal antenna selection analysis as well.																	1864-5909	1864-5917															10.1007/s12065-020-00457-x		AUG 2020											
J								The emergence of social media data and sentiment analysis in election prediction	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Sentiment analysis; Opinion mining; Election prediction; Social media; Twitter	140 CHARACTERS; TWITTER; CLASSIFICATION; CAMPAIGNS; POWER	This work presents and assesses the power of various volumetric, sentiment, and social network approaches to predict crucial decisions from online social media platforms. The views of individuals play a vital role in the discovery of some critical decisions. Social media has become a well-known platform for voicing the feelings of the general population around the globe for almost decades. Sentiment analysis or opinion mining is a method that is used to mine the general population's views or feelings. In this respect, the forecasting of election results is an application of sentiment analysis aimed at predicting the outcomes of an ongoing election by gauging the mood of the public through social media. This survey paper outlines the evaluation of sentiment analysis techniques and tries to edify the contribution of the researchers to predict election results through social media content. This paper also gives a review of studies that tried to infer the political stance of online users using social media platforms such as Facebook and Twitter. Besides, this paper highlights the research challenges associated with predicting election results and open issues related to sentiment analysis. Further, this paper also suggests some future directions in respective election prediction using social media content.																	1868-5137	1868-5145															10.1007/s12652-020-02423-y		AUG 2020											
J								Learning Adaptive Regularization for Image Labeling Using Geometric Assignment	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Image labeling; Assignment manifold; Assignment flow; Dynamical systems; Replicator equation; Evolutionary dynamics; Sensitivity analysis; Parameter learning; Adaptive regularization	EQUATIONS	We study the inverse problem of model parameter learning for pixelwise image labeling, using the linear assignment flow and training data with ground truth. This is accomplished by a Riemannian gradient flow on the manifold of parameters that determines the regularization properties of the assignment flow. Using the symplectic partitioned Runge-Kutta method for numerical integration, it is shown that deriving the sensitivity conditions of the parameter learning problem and its discretization commute. A convenient property of our approach is that learning is based on exact inference. Carefully designed experiments demonstrate the performance of our approach, the expressiveness of the mathematical model as well as its limitations, from the viewpoint of statistical learning and optimal control.																	0924-9907	1573-7683															10.1007/s10851-020-00977-2		AUG 2020											
J								Novel multivariate compositional data's model for structurally analyzing sub-industrial energy consumption with economic data	NEURAL COMPUTING & APPLICATIONS										Sub-industrial energy consumption structure; Compositional data; Simplex space; Fractional-order accumulation; Gray wolf optimizer	PRINCIPAL COMPONENT ANALYSIS; GREY FORECASTING-MODEL; STATISTICAL-ANALYSIS; PREDICTION MODEL; PERTURBATION; SYSTEM	Structural prediction and analysis of sub-industrial energy consumption with economic data across three industrial sectors are an important basis for reflecting the coordinated development relationship between energy consumption and industrial development. Empirically, the sub-industrial energy consumption and economic structure have numerous compositional data. The multivariate compositional data's fractional gray multivariate model on the basis of the Simplex space and its algebraic system is proposed in this study aiming at the multi-dimensional small sample size with high uncertainties. First, the fractional accumulative generation operation sequence of multivariate compositional data is defined according to Aitchison geometry. Then, the novel model with the form of the compositional data vector is obtained. Second, the least square parameter estimation of the model is studied. A derived model is deduced and selected as the time-response expression of the model solution. Moreover, the gray wolf optimizer is introduced and designed to determine the optimal value of the fractional order. Detailed modeling procedures, including the computational steps and the intelligent optimization algorithm, have been clearly presented. Furthermore, 10-year economic structural data of 14 provinces in China are used to validate the effectiveness of the proposed model. The validation presents that the proposed model performs better in fitting, prediction, stability, and applicability, in comparison with the two other models in the Simplex space. Last, from the updated real-time datasets from 2008 to 2018, the proposed model is applied to analyze and forecast the sub-industrial energy consumption structure and industrial structure of Beijing. Results show that the proposed model presents high accuracy and is efficient in addressing the multivariate compositional data in some structural energy and economic issues.																	0941-0643	1433-3058															10.1007/s00521-020-05227-5		AUG 2020											
J								GA-based design of optimal discrete wavelet filters for efficient wind speed forecasting	NEURAL COMPUTING & APPLICATIONS										Wind power forecasting; Discrete wavelet transform; Genetic algorithm (GA); Neural networks; Artificial intelligence	NEURAL-NETWORK; DECOMPOSITION; TRANSFORM	Wind energy is getting more and more integrated into power grids, giving rise to some challenges because of its inherent intermittent and irregular nature. Wind speed forecasting plays a fundamental role in overcoming such challenging issues and, thus, assisting the power utility manager in optimizing the supply-demand balancing through wind energy generation. This paper suggests a new hybrid scheme WNN, based on discrete wavelet transform (DWT) combined with artificial neural network (ANN), for wind speed forecasting. More specifically, this work aims at designing the most appropriate discrete wavelet filters, best adapted to a one day ahead wind speed forecasting. The optimized DWT filters are intended to effectively preprocess the wind speed time series data in order to enhance the prediction accuracy. Using wind speed data collected from three different locations in the Magherbian region, the obtained simulation results indicate that the proposed approach outperforms other conventional wavelet-based forecasting structures regarding the wind speed prediction precision. Moreover, compared to the standard wavelet 'db4' based approach, the optimized wavelet filter-based structure leads to a forecasting accuracy improvement, in terms of RMSE and MAPE index errors, that amounts to nearly 13% and 19%, respectively.																	0941-0643	1433-3058															10.1007/s00521-020-05251-5		AUG 2020											
J								Find optimal capacity and location of distributed generation units in radial distribution networks by using enhanced coyote optimization algorithm	NEURAL COMPUTING & APPLICATIONS										Coyote optimization algorithm; Distributed generation; Real power loss; Operation cost	PARTICLE SWARM OPTIMIZATION; POWER LOSS MINIMIZATION; OPTIMAL PLACEMENT; VOLTAGE STABILITY; DGS	This paper proposes a novel effective optimization algorithm called enhanced coyote optimization algorithm (ECOA). This proposed method is applied to optimally select the position and capacity of distributed generators (DGs) in radial distribution networks. It is a multi-objective optimization problem where properly installing DGs should simultaneously reduce the power loss, operating costs as well as improve voltage stability. Based on the original coyote optimization algorithm (COA), ECOA is developed to be able to expand the search area and retain a good solution group in each generation. It includes two modifications to improve the efficiency of the original COA approach where the first one is replacing the central solution by the best current solution in the first new solution generation technique and the second focuses on reducing the computation burden and process time in the second new solution generation step. In this research, various experiments have been implemented by applying ECOA, COA as well as salp swarm algorithm (SSA), Sunflower optimization (SOA) for three IEEE radial distribution power networks with 33, 69 and 85 buses. Obtained results have been statistically analyzed to investigate the appropriate control parameters and to verify the performance of the proposed ECOA method. In addition, the performance of ECOA is also compared to various similar meta-heuristic methods such as genetic algorithm (GA), particle swarm optimization (PSO), hybrid genetic algorithm and particle swarm optimization (HGA-PSO), simulated annealing, bacterial foraging optimization algorithm, backtracking search optimization algorithm, harmony search algorithm, whale optimization algorithm (WOA) and combined power loss index-whale optimization algorithm (PLI-WOA). Detailed comparisons show that ECOA can determine more effective location and size of DGs with faster speed than other methods. Specifically, the improvement levels of the proposed method over compared to SFO, SSA, and COA can be up to 2.1978%, 0.7858% and 0.2348%. Furthermore, as compared to other existing methods in references, ECOA achieves the significant improvements which are up to 31.7491%, 20.2143% and 22.7213% for the three test systems, respectively. Thus, the proposed method is a favorable method in solving the optimal determination of DGs in radial distribution networks.																	0941-0643	1433-3058															10.1007/s00521-020-05239-1		AUG 2020											
J								Futures price prediction of agricultural products based on machine learning	NEURAL COMPUTING & APPLICATIONS										Machine learning; Agricultural products; Futures; Price prediction	STOCK; DISCOVERY	Agricultural product futures are crucial to economic development, and the prediction of agricultural product futures prices has an important impact on the stability of the market economy. In order to improve the accuracy of agricultural product futures price prediction, based on machine learning algorithms, this study mainly uses machine learning methods to predict futures prices based on the analysis of fundamental factors affecting agricultural product futures prices. Moreover, in this study, wavelet analysis method is used to smooth the data and then build a model to process the hierarchical information after signal decomposition. In addition, this study conducts model validity studies through cases to draw comparative statistical diagrams to analyze the accuracy of model prediction data. The research shows that the model proposed in this paper has certain effects and can provide theoretical reference for subsequent related research.																	0941-0643	1433-3058															10.1007/s00521-020-05250-6		AUG 2020											
J								A Novel Solution of Using Deep Learning for White Blood Cells Classification: Enhanced Loss Function with Regularization and Weighted Loss (ELFRWL)	NEURAL PROCESSING LETTERS										White blood cells; Modified loss function; Regularization; Deep convolutional neural network; Intra-class compactness; Inter-class separability	ACUTE LYMPHOBLASTIC-LEUKEMIA; ALGORITHM	Deep learning has been successfully applied in classification of white blood cells (WBCs), however, accuracy and processing time are found to be less than optimal hindering it from getting its full potential. This is due to imbalanced dataset, intra-class compactness, inter-class separability and overfitting problems. The main research idea is to enhance the classification and prediction accuracy of blood images while lowering processing time through the use of deep convolutional neural network (DCNN) architecture by using the modified loss function. The proposed system consists of a deep neural convolution network (DCNN) that will improve the classification accuracy by using modified loss function along with regularization. Firstly, images are pre-processed and fed through DCNN that contains different layers with different activation function for the feature extraction and classification. Along with modified loss function with regularization, weight function aids in the classification of WBCs by considering weights of samples belonging to each class for compensating the error arising due to imbalanced dataset. The processing time will be counted by each image to check the time enhancement. The classification accuracy and processing time are achieved using the dataset-master. Our proposed solution obtains better classification performance in the given dataset comparing with other previous methods. The proposed system enhanced the classification accuracy of 98.92% from 96.1% and a decrease in processing time from 0.354 to 0.216 s. Less time will be required by our proposed solution for achieving the model convergence with 9 epochs against the current convergence time of 13.5 epochs on average, epoch is the formation white blood cells (WBCs) and the development of granular cells. The proposed solution modified loss function to solve the adverse effect caused due to imbalance dataset by considering weight and use regularization technique for overfitting problem.																	1370-4621	1573-773X				OCT	2020	52	2			SI		1517	1553		10.1007/s11063-020-10321-9		AUG 2020											
J								360 degree view of cross-domain opinion classification: a survey	ARTIFICIAL INTELLIGENCE REVIEW										Opinion mining; Sentiment analysis; Cross-domain opinion classification; Domain adaptation; Transfer learning; Machine learning	PRODUCT FEATURE-EXTRACTION; SENTIMENT ANALYSIS; POLARITY CLASSIFICATION; ONLINE REVIEWS; AUTOMATIC CONSTRUCTION; PERCEIVED USEFULNESS; STRENGTH DETECTION; CUSTOMER REVIEWS; CONSUMER REVIEWS; SUBJECTIVE DATA	In the field of natural language processing and text mining, sentiment analysis (SA) has received huge attention from various researchers' across the globe. By the prevalence of Web 2.0, user's became more vigilant to share, promote and express themselves along with any issues or challenges that are being encountered on daily activities through the Internet (social media, micro-blogs, e-commerce, etc.) Expression and opinion are a complex sequence of acts that convey a huge volume of data that pose a challenge for computational researchers to decode. Over the period of time, researchers from various segments of public and private sectors are involved in the exploration of SA with an aim to understand the behavioral perspective of various stakeholders in society. Though the efforts to positively construct SA are successful, challenges still prevail for efficiency. This article presents an organized survey of SA (also known as opinion mining) along with methodologies or algorithms. The survey classifies SA into categories based on levels, tasks, and sub-task along with various techniques used for performing them. The survey explicitly focuses on different directions in which the research was explored in the area of cross-domain opinion classification. The article is concluded with an objective to present an exclusive and exhaustive analysis in the area of opinion mining containing approaches, datasets, languages, and applications used. The observations made are expected to support researches to get a greater understanding on emerging trends and state-of-the-art methods to be applied for future exploration.																	0269-2821	1573-7462															10.1007/s10462-020-09884-9		AUG 2020											
J								IPBSM: An optimal bribery selfish mining in the presence of intelligent and pure attackers	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										blockchain; game theory; reinforcement learning; selfish mining	BITCOIN	Blockchain is a "decentralized" system, where the security heavily depends on that of the consensus protocols. For instance, attackers gain illegal revenues by leveraging the vulnerabilities of the consensus protocols. Such attacks consist of selfish mining (SM1), optimal selfish mining (epsilon-optimal), bribery selfish mining (BSM), and so forth. In existing works, the attacks only consider the circumstances, where part of miners are rational. However, miners are hardly nonrational in the blockchain system since they hope to maximize their revenues. Furthermore, attackers prefer intelligent tools to increase their power for more additional revenues. Therefore, new models are urgently needed to formulate the scenarios, where attackers are purely rational and intelligent. In this paper, we propose a new BSM model, where all miners are rational. Moreover, rational attackers are intelligent such that they optimize their strategies by utilizing reinforcement learning to boost their revenues. More specifically, we propose a new selfish mining algorithm: intelligent bribery selfish mining (IPBSM), where attackers choose optimal strategies resorting to reinforcement learning when they interact with the external environment. The external environment can be further modeled as a Markov decision process to facilitate the construction of reinforcement learning. The simulation results manifest that IPBSM, compared with SM1 and epsilon-optimal, has lower power thresholds and higher revenues. Therefore, IPBSM is a threat no to be neglected to the blockchain system.																	0884-8173	1098-111X				NOV	2020	35	11					1735	1748		10.1002/int.22270		AUG 2020											
J								Extraction of information about structural changes in a semisolid pharmaceutical formulation from near-infrared and Raman images by multivariate curve resolution-alternating least squares and ComDim	JOURNAL OF CHEMOMETRICS										ComDim; MCR-ALS; NIR mapping; Raman mapping; semisolid pharmaceutical formulation	MULTIBLOCK DATA-ANALYSIS; ATORVASTATIN CALCIUM; SOLID DISPERSIONS; IDENTIFICATION; COMPONENTS; BEHAVIOR; NIR; TRANSFORMATIONS; QUANTIFICATION; SPECTROSCOPY	Solid dispersions are an interesting option to improve the solubility of Class II (high permeability, low water solubility) drugs of the Biopharmaceutical Classification System without the use of organic solvents. However, structural changes (polymorphism) may be present in both active pharmaceutical ingredients (API) and excipients, which require evaluation during pharmaceutical development. Thus, the aim of this work was to demonstrate the feasibility of using Raman and near-infrared (NIR) mapping associated with multivariate curve resolution-alternating least squares (MCR-ALS) and common components and specific weights analysis (CCSWA or ComDim) chemometric methods to assess the solid dispersions of atorvastatin calcium in Gelucire (R) 48/16 under controlled (3.0 +/- 1.0 degrees C, sealed flask) and accelerated (33.0 +/- 1.0 degrees C, 75% of relative humidity, open flask) aging conditions. MCR-ALS allowed the identification of the amorphous and crystalline fractions of the drug within the solid dispersion, which is not easily achieved by traditional techniques such as differential scanning calorimetry and X-ray diffraction, especially for semisolid formulations. ComDim results indicated that Raman seemed more sensitive to the presence of the API, whereas NIR was found to be more sensitive to alterations in the spectra of the excipient. The use of the ComDim method is not yet widespread in the pharmaceutical area; therefore, this paper shows its potential to support pharmaceutical development in preformulation stages for stability studies.																	0886-9383	1099-128X														e3288	10.1002/cem.3288		AUG 2020											
J								Empirical evaluation and pathway modeling of visual attention to virtual humans in an appearance fidelity continuum	JOURNAL ON MULTIMODAL USER INTERFACES										Virtual humans; Eye tracking; Human computer interaction; Rendering style; Virtual Agents; Virtual training simulator		In this contribution we studied how different rendering styles of a virtual human impacted users' visual attention in an interactive medical training simulator. In a mixed design experiment, 78 participants interacted with a virtual human representing a sample from the non-photorealistic (NPR) to the photorealistic (PR) rendering continuity. We presented five rendering style samples scenarios, namely All Pencil Shaded (APS), Pencil Shaded (PS), All Cartoon Shaded (ACT), Cartoon Shaded (CT), and Human-Like (HL), and compared how visual attention differed between groups of users. For this study, we employed an eye tracking system for collecting and analyzing users' gaze during interaction with the virtual human in a failure to rescue medical training simulation. Results shows that users spent more total time in the APS and ACT conditions but users visually attended more to virtual humans in the PS, CT and HL appearance conditions.																	1783-7677	1783-8738															10.1007/s12193-020-00341-z		AUG 2020											
J								ClothFace: A Passive RFID-Based Human-Technology Interface on a Shirtsleeve	ADVANCES IN HUMAN-COMPUTER INTERACTION											STRAIN SENSOR	This paper introduces ClothFace, a shirtsleeve-integrated human-technology interface platform, which comprises two wrist antennas and three radio frequency identification (RFID) integrated circuits (ICs), each with a unique ID. The platform prototype, which is created on a shirtsleeve by cutting the antennas and antenna-IC interconnections from copper tape, can be used for push button and swipe controlling. Each IC can be activated, i.e., electrically connected to the two antennas, by touching the IC. These ICs can act as wireless input buttons to the technology around us. Due to the used passive ultrahigh-frequency (UHF) RFID technology, there is no need for clothing-integrated energy sources, but the interface platform gets all the needed energy from an external RFID reader. The platform prototype was found to be readable with an external RFID reader from all directions at distances of 70-80 cm. Further, seven people giving altogether 1400 inputs tested the prototype sleeves on a table and on body. In these first tests, 96-100% (table) and 92-100% (on-body) success rates were achieved in a gamelike testing setup. Further, the platform was proved to be readable with an off-the-shelf handheld RFID reader from a distance of 40 cm. Based on these initial results, this implementation holds the potential to be used as a touch interface blended into daily clothing, as well as a modular touch-based interaction platform that can be integrated into the surfaces of electronic devices, such as home appliances.																	1687-5893	1687-5907				AUG 5	2020	2020								8854042	10.1155/2020/8854042													
J								Purposeful tool use in early lithic technologies	ADAPTIVE BEHAVIOR										Purposeful behaviour; tool use; dynamic systems; marrow extraction	FLAKING IMPLICATIONS; PHASE-TRANSITIONS; OLDUVAI GORGE; MOTOR SKILL; STONE; ARCHAEOLOGY; PERCUSSION; HOMINID; ORGANIZATION; VARIABLES	Tool use can be considered in terms of purposeful behaviour. This emphasis on 'purpose' hides a host of assumptions about the nature of cognition and its relationship with physical activity. In particular, a notion of 'purpose' might assume that this is teleological which, in turn, requires a model of a desired end state of an action that can be projected onto the environment. Such a model is fundamental to traditional descriptions of cognition and a version of this can be found in the 'template' theory of stone-tool production (i.e. where the maker of the tool has a model in mind and attempts to reproduce this model in stone). Against this cognitive perspective, a number of approaches have been proposed that share their roots in the work of Gibson (i.e. ecological psychology) or Bernstein (i.e. dynamic systems). From these perspectives, 'purpose' is not a matter of a projection but opportunity; put simply, an action is performed until it need not be performed further. Trivial though this might sound, it has implications for how we define purpose and how this might apply to our understanding of tool use. We argue from a dynamic systems perspective and demonstrate the use of tools to crack bones for marrow extraction.																	1059-7123	1741-2633														1059712320941543	10.1177/1059712320941543		AUG 2020											
J								Non-local configuration of component interfaces by constraint satisfaction	CONSTRAINTS										Interface configuration; Constraint language; Constraint satisfaction	ORCHESTRATION; NETWORKS	Service-oriented computing is the paradigm that utilises services as fundamental elements for developing applications. Service composition, where data consistency becomes especially important, is still a key challenge for service-oriented computing. We maintain that there is one aspect of Web service communication on the data conformance side that has so far escaped the researchers attention. Aggregation of networked services gives rise to long pipelines, or quasi-pipeline structures, where there is a profitable form of inheritance called flow inheritance. In its presence, interface reconciliation ceases to be a local procedure, and hence it requires distributed constraint satisfaction of a special kind. We propose a constraint language for this, and present a solver which implements it. In addition, our approach provides a binding between the language and C++, whereby the assignment to the variables found by the solver is automatically translated into a transformation of C++ code. This makes the C++ Web service context compliant without any further communication. Besides, it uniquely permits a very high degree of flexibility of a C++ coded Web service without making public any part of its source code.																	1383-7133	1572-9354															10.1007/s10601-020-09309-y		AUG 2020											
J								Sun-sky model estimation from outdoor images	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Outdoor illumination estimation; Convolution block attention module; Two-branch network structure	ILLUMINATION	When a virtual object is inserted into an outdoor image, the recovery of scene illumination has a critical effect on the mix of virtual objects and actual reality. There are two main parts of the object in the outdoor scene: the sun and the sky. In order to represent the illumination conditions of these two natural illumination, this paper uses the Lalonde-Matthew outdoor illumination model to perform the sky and sun in the image. Model use seven parameters represent the illumination of the scene. So the original illumination estimation problem is transformed into a prediction problem of seven illumination parameters. For this problem, this paper proposes a new two-branch network structure, one branch is used to estimate the sun orientation, and the other branch is used to estimate the remaining six parameters. This paper also introduces convolution block attention module (CBAM) based on this structure. The introduction of this module enables the network to select the most important information for the current task target from a large number of information when extracting image features, while suppressing other useless information.																	1868-5137	1868-5145															10.1007/s12652-020-02367-3		AUG 2020											
J								Sample size calculations for the experimental comparison of multiple algorithms on multiple problem instances	JOURNAL OF HEURISTICS										Experimental comparison of algorithms; Statistical methods; Sample size estimation; Iterative sampling; Multiple hypotheses testing	EVOLUTIONARY ALGORITHMS; PERFORMANCE; OPTIMIZATION; INTELLIGENCE; ACCURACY; DESIGN; TESTS	This work presents a statistically principled method for estimating the required number of instances in the experimental comparison of multiple algorithms on a given problem class of interest. This approach generalises earlier results by allowing researchers to design experiments based on the desired best, worst, mean or median-case statistical power to detect differences between algorithms larger than a certain threshold. Holm's step-down procedure is used to maintain the overall significance level controlled at desired levels, without resulting in overly conservative experiments. This paper also presents an approach for sampling each algorithm on each instance, based on optimal sample size ratios that minimise the total required number of runs subject to a desired accuracy in the estimation of paired differences. A case study investigating the effect of 21 variants of a custom-tailored Simulated Annealing for a class of scheduling problems is used to illustrate the application of the proposed methods for sample size calculations in the experimental comparison of algorithms.																	1381-1231	1572-9397				DEC	2020	26	6					851	883		10.1007/s10732-020-09454-w		AUG 2020											
J								Depicting probabilistic context awareness knowledge in deliberative architectures	NATURAL COMPUTING										Social robotics; Context awareness; Deliberative; PDDL; Conceptual graphs; Metrics	REPRESENTATION	Facing long-term autonomy with a cognitive architecture raises several difficulties for processing symbolic and sub-symbolic information under different levels of uncertainty, and deals with complex decision-making scenarios. For reducing environment uncertainty and simplify the decision-making process, this paper establishes a method for translating robot knowledge to a conceptual graph to later extract probabilistic context information that allows to bound of the actions present at the deliberative layer. This research develops two ROS components, one for translating robot knowledge to the conceptual graphs and one for extracting context knowledge from this graph using Bayesian networks. We evaluate these components in a real-world scenario, performing a task where a robot notifies to a user a message of an event at home. Our results show an improvement in task completion when using our approach, decreasing the planning requests by 65% and doing the task in a third of the time.																	1567-7818	1572-9796															10.1007/s11047-020-09798-z		AUG 2020											
J								Joint deep separable convolution network and border regression reinforcement for object detection	NEURAL COMPUTING & APPLICATIONS										Object detection; Separable convolution network; Semantic segmentation; Reinforcement learning		The improvement of object detection performance mainly depends on the extraction of local information near the target area of interest, which is also the main reason for the lack of feature semantic information. Considering the importance of scene and semantic information for visual recognition, in this paper, the improvement of the object detection algorithm is realized from three parts. Firstly, the basic residual convolution module is fused with the separable convolution module to construct a depth-wise separable convolution network (D_SCNet-127 R-CNN). Then, the feature map is sent to the scene-level region proposal self-attention network to re-identify the candidate area. This part is composed of three parallel branches: semantic segmentation module, region proposal network, and region proposal self-attention module. Finally, this paper uses deep reinforcement learning combined with a border regression network to achieve precise location of the object, and improve the calculation speed of the entire model through a light-weight head network. This model can effectively solve the limitation of feature extraction in traditional object detection and obtain more comprehensive detailed features. The experimental on MSCOCO17, Pascal VOC07, and Cityscapes datasets shows that the proposed method has good validity and scalability.																	0941-0643	1433-3058															10.1007/s00521-020-05255-1		AUG 2020											
J								New finite-time synchronization of memristive Cohen-Grossberg neural network with reaction-diffusion term based on time-varying delay	NEURAL COMPUTING & APPLICATIONS										Finite-time synchronization; Memristor; Reaction-diffusion; Cohen-Grossberg neural networks	EXPONENTIAL SYNCHRONIZATION; STABILITY; FEEDBACK	This paper focuses on the finite-time synchronization of memristive Cohen-Grossberg neural networks with time delays based on the reaction-diffusion term. Two new finite-time synchronous lemmas, Lemmas 2.3 and 2.4, have been obtained through some integration techniques. Since the proposal of Lemma 2.5 solves the X-phi (u(t)) problem in the denominator, and by designing two different controllers and inequality techniques, two finite-time synchronization theorems are finally obtained. Simulations are performed according to two examples to verify the validity of the results in this paper.																	0941-0643	1433-3058															10.1007/s00521-020-05259-x		AUG 2020											
J								Prediction of energy photovoltaic power generation based on artificial intelligence algorithm	NEURAL COMPUTING & APPLICATIONS										Artificial intelligence algorithm; Energy; Neural network; Power generation; Prediction model	FORECAST; PARAMETERS; MODEL	The key to the coordination of photovoltaic power generation and conventional energy power load lies in the accurate prediction of photovoltaic power generation. At present, prediction models have problems with accuracy and system operation stability. Based on the neural network algorithm, this research carries the prediction of energy photovoltaic power generation and establishes a BP neural network prediction model and a wavelet neural network prediction model. Moreover, this research studies the influence of various factors on the prediction t of photovoltaic power generation, and analyzes the relationship between the various factors. In addition, in this study, a comparative test is constructed to analyze the model performance, and a statistical graph is drawn to take a visual comparison of performance. The research shows that the model proposed in this paper has certain effects and has certain advantages in the prediction of photovoltaic power generation.																	0941-0643	1433-3058															10.1007/s00521-020-05249-z		AUG 2020											
J								Mobile neural intelligent information system based on edge computing with interactive data	NEURAL COMPUTING & APPLICATIONS										Edge computing; Communication network; System design; Interactive data acquisition; Intelligent information system	TECHNOLOGIES; ARCHITECTURE	With the increasing popularity of smart mobile terminals, smartphones, tablet computers and other mobile devices will gradually replace personal computers and become the most important computing platform for consumers. Mobile edge computing (MEC), as a key technology for the evolution of communication network architecture, can meet the requirements of the system for throughput, delay, network scalability and intelligence. Based on the MEC, the content and services provided by information system are closer to users to increase mobile network speed, reduce latency and improve connection reliability. In this paper, we propose an interactive data information system based on mobile edge detection, which is divided into client and server. The server mainly provides services such as content and user login and rights management for the client. The client needs to log into the server to run normally. Among them, the interactive data server is a favorable support and guarantee for the client. The experimental results show that the proposed method has higher efficiency and fault tolerance.																	0941-0643	1433-3058															10.1007/s00521-020-05269-9		AUG 2020											
J								Large-width machine learning algorithm	PROGRESS IN ARTIFICIAL INTELLIGENCE										Large-margin learning; k-Nearest neighbor; Lazy learning; Nonparametric classification	BOUNDS	We introduce an algorithm, called Large Width (LW), that produces a multi-category classifier (defined on a distance space) with the property that the classifier has a large 'sample width.' (Width is a notion similar to classification margin.) LW is an incremental instance-based (also known as 'lazy') learning algorithm. Given a sample of labeled and unlabeled examples, it iteratively picks the next unlabeled example and classifies it while maintaining a large distance between each labeled example and its nearest-unlike prototype. (A prototype is either a labeled example or an unlabeled example which has already been classified.) Thus, LW gives a higher priority to unlabeled points whose classification decision 'interferes' less with the labeled sample. On a collection UCI benchmark datasets, the LW algorithm ranks at the top when compared to 11 instance-based learning algorithms (or configurations). When compared to the best candidate from instance-based learners, MLP, SVM, decision tree learner (C4.5) and Naive Bayes, LW is ranked at second place after only MLP which comes at first place by a single extra win against LW. The LW algorithm can be implemented in parallel distributed processing to yield a high speedup factor and is suitable for any distance space, with a distance function which need not necessarily satisfy the conditions of a metric.																	2192-6352	2192-6360				SEP	2020	9	3					275	285		10.1007/s13748-020-00212-4		AUG 2020											
J								Multi-strategy synergy-based backtracking search optimization algorithm	SOFT COMPUTING										Improved backtracking search optimization algorithm; Combined mutation strategy; Crossover equation improvement strategy; Niche improvement strategy; Numerical simulation analysis	FLOW	In order to solve the problems of backtracking search optimization algorithm with slow convergence speed and easy to fall into local optimum, an improved backtracking search optimization algorithm based on multi-strategy synergy is proposed. Foremost, a combination mutation strategy based on chaotic map and gamma distribution is introduced. The poor individuals are mutated to generate better quality individuals under a certain probability. Next, the global optimal individual information is introduced into the cross equation to guide the population update. Last but not least, a small habitat displacement method based on simulated annealing is designed. The poor individual is found by the niche radius, and the rich individuals are reconstructed by the global optimal individual information and the Gaussian distribution random function, and the convergence speed of the algorithm is improved. The simulated annealing algorithm is integrated on the niche technology to ensure the diversity of the new population and improve the convergence speed of the algorithm. In this paper, some standard test functions are selected, and numerical simulations are carried out in low-dimensional and high-dimensional states, compared with seven well-performing algorithms. The improved algorithm was analyzed by complexity,Ttest and ANOVA test. Simulation experiments on 20 standard test functions show that the improved algorithm has a faster convergence speed and higher convergence accuracy. Even in a high-dimensional multi-peak function, the convergence accuracy of the improved algorithm after the same number of iterations is 15 times higher than the original algorithm above the magnitude.																	1432-7643	1433-7479				OCT	2020	24	19					14305	14326		10.1007/s00500-020-05225-8		AUG 2020											
J								Machine learning techniques to identify mind-wandering and predict hazard response time in fully immersive driving simulation	SOFT COMPUTING										Mind-wandering; Attention; Hazard response time; Driving simulation; Time-series; Machine learning	DRIVER COGNITIVE DISTRACTION; DROWSINESS; SYSTEM; INATTENTION; PERFORMANCE; ATTENTION; ALCOHOL	This work presents machine learning-based techniques for detecting mind-wandering and predicting hazard response time in driving using only easily measurable driving performance data (speed, horizontal and frontal acceleration, lane gap, and brake pressure). Such classifiers are relevant as research tools in the driving simulation community. We present a simple method, and a feature extraction-based method, of representing time-series driving performance data that both support machine learning-based predictions. We use the two types of representations to compare the effectiveness of support vector machines, random forest, and multi-layer perceptrons on data from 120 drives performed by 40 participants during a previous study in the high-fidelity driving simulator at the University of Guelph. Classification of mind-wandering and prediction of hazard response time was successful when compared to baseline measures. Specifically, random forest methods were most effective in both types of prediction and feature extraction supported the strongest random forest prediction of hazard response time. A discussion of the reasoning for this is included. While this has previously been studied using a single screen experimental setup, to our knowledge this is the first driving pattern-based classification of mind-wandering in a fully immersive driving simulator.																	1432-7643	1433-7479															10.1007/s00500-020-05217-8		AUG 2020											
J								A model integrating environmental concerns and supply risks for dynamic sustainable supplier selection and order allocation	SOFT COMPUTING										Dynamic supplier selection; Order allocation; Sustainable supply chain; Computing; Soft computing	DECISION-MAKING METHOD; MULTIOBJECTIVE OPTIMIZATION; PROGRAMMING APPROACH; CHAIN; NETWORK; PERFORMANCE; ALGORITHM; DESIGN; DECOMPOSITION; PREVENTION	Environmental factors are increasingly being considered in supply chain risk management (SCRM), which itself represents a growing trend. Accordingly, dynamic supplier selection and order allocation have become very important. Both qualitative and quantitative factors should be considered in the selection of eligible suppliers. In this study, a novel two-stage comprehensive mathematical model is developed for selecting a set of suppliers and assigning an order quantity to each supplier. The first stage involves a primary selection of alternative suppliers according to the risk value, which is determined using qualitative and quantitative methods based on the best-worst method, and the second stage involves establishing a multiobjective mathematical model for dealing with dynamic supplier selection and order allocation. The proposed approach, which helps enterprises manage uncertainty in SCRM, is applied in this study to the new energy vehicles industry.																	1432-7643	1433-7479															10.1007/s00500-020-05165-3		AUG 2020											
J								Particle Swarm Optimization Based Swarm Intelligence for Active Learning Improvement: Application on Medical Data Classification	COGNITIVE COMPUTATION										Active learning; Uncertainty sampling strategy; Particle swarm optimization; Global optimization problem	ALGORITHM; UNCERTAINTY; TIME	Semi-supervised learning targets the common situation where labeled data are scarce but unlabeled data are abundant. It uses unlabeled data to help supervised learning tasks. In practice, it may make sense to utilize active learning in conjunction with semi-supervised learning. That is, we might allow the learning algorithm to pick a set of unlabeled instances to be labeled by a domain expert, which will then be used as the labeled data set. However, existing approaches are computationally expensive and require searching through an entire unlabeled dataset, which may contain redundant instances that provide no instructive information to the classifier and can decrease the performance. To address this optimization problem, a hybrid system that combines active learning (AL) and particle swarm optimization (PSO) algorithms is proposed to reduce the cost of labeling while building a more efficient classifier. The novelty of this work resides in the integration of a bio-inspired optimization algorithm in the machine learning strategy. Furthermore, a novel uncertainty measure was integrated into the particle swarm optimization algorithm as an objective function to select from massive amounts of medical instances those that are deemed mostinformative.To evaluate the effectiveness of the proposed approach, eighteen (18) benchmark datasets were used and compared against three best-known classifiers with different learning paradigms:AL-NBan active learning algorithm using Naive Base classifier and Margin Sampling strategy,SVM(Support Vector Machine),ELM(Extreme Learning Machine) with supervised learning, andTSVM(Transductive Support Vector Machine) with the semi-supervised learning. Experiments showed that the proposed approach is effective in reducing the efforts required by experts for medical data annotation to produce an accurate classifier. The active learning approach has been utilized to optimize the expensive task of labeling. Based on a novel uncertainty measure, the nature-inspired algorithm PSO attempts to select from massive amounts of unlabeled medical instances those consideredinformative, at the same time improving the classifier performance. The experiments carried out confirm that the proposed strategy significantly enhances the performance of the AL algorithm compared with the commonly used uncertainty strategies. It achieves a performance similar to that of fully supervised and semi-supervised algorithms while requiring much less labeling. As a future extension of this work, it would be interesting to integrate other evolutionary optimization algorithms and compare them with our approach. In addition, it is beneficial to test the impact of using other variants of PSO algorithm in our approach. Also, it is aimed to test more classification algorithms in the experimentation process.																	1866-9956	1866-9964				SEP	2020	12	5					991	1010		10.1007/s12559-020-09739-z		AUG 2020											
J								A type-II fuzzy collaborative forecasting approach for productivity forecasting under an uncertainty environment	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Fuzzy collaborative forecasting; Type-II fuzzy number; Mixed binary nonlinear programming; Productivity	INTELLIGENCE APPROACH; LINEAR-REGRESSION	Forecasting factory productivity is a critical task. However, it is not easy owing to the uncertainty of productivity. Existing methods often forecast productivity using a fuzzy number. However, the range of a fuzzy productivity forecast is wide owing to the consideration of extreme cases. In this study, a fuzzy collaborative forecasting approach is proposed to forecast factory productivity using a type-II fuzzy number and by narrowing the forecast's range. The outer section of the type-II fuzzy number determines the range of productivity, while the inner section is defuzzified to derive the most likely value. Based on the experimental results, the proposed methodology surpassed existing methods in improving forecasting precision and accuracy, with a reduction in the mean absolute percentage error (MAPE) of up to 74%.																	1868-5137	1868-5145															10.1007/s12652-020-02435-8		AUG 2020											
J								iADA*: Improved Anytime Path Planning and Replanning Algorithm for Autonomous Vehicle	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Anytime algorithm; Path planning and replanning; Concurrent path planner; Real-time path planner; Dynamic environment		Path planning of autonomous mobile robots in a real-world environment presents several challenges which are usually not raised in other areas. The real world is inherently complex, uncertain and dynamic. Therefore, accurate models of path planning are difficult to obtain and quickly become outdated. Anytime planners are ideal for this type of problem as they can find an initial solution very quickly and then improve it as time allows. This paper proposes a new anytime incremental search algorithm named improved Anytime Dynamic A*(iADA*). The algorithm is based on the currently popular anytime heuristic search algorithm, which is Anytime Dynamic A*(ADA*). The iADA* algorithm improves the calculation of the path lengths and decreases the calculating frequency of the path throughout the search, making it significantly faster. The algorithm is designed to provide an efficient solution to a complex, dynamic search environment when the locally changes affected. Our study shows that the two-dimensional path-planning iADA* experiments were between 2.0 to 3.7 times faster than ADA*, both in partially known and fully unknown dynamic environments. Additionally, in this paper shows the experiment results of the comparison with other four existing algorithms based on computing time and path lengths. iADA* was an average 2.57 times reduced on the computational time for the environment which locally changes effected. For the path length is little increase, but it is not the worst case. According to the experiments, the more the environmental problems and complexity increases, the more iADA* provides a rapid in-search time and total time to obtain the final solution.																	0921-0296	1573-0409															10.1007/s10846-020-01240-x		AUG 2020											
J								Multifactorial evolutionary algorithm for solving clustered tree problems: competition among Cayley codes Case studies on the clustered shortest-path tree problem and the minimum inter-cluster routing cost clustered tree problem	MEMETIC COMPUTING										Multifactorial evolutionary algorithm; Clustered tree problems; Evolutionary algorithms; Cayley code; Prufer code; Dandelion code; Blob code	MULTITASKING	The Multifactorial Evolutionary Algorithm (MFEA) has emerged as an effective variant of the evolutionary algorithm. MFEA has been successfully applied to deal with various problems with many different types of solution encodings. Although clustered tree problems play an important role in real life, there haven't been much research on exploiting the strengths of MFEA to solve these problems. One of the challenges in applying the MFEA is to build specific evolutionary operators of the MFEA algorithm. To exploit the advantages of the Cayley Codes in improving the MFEA's performance, this paper introduces MFEA with representation scheme based on the Cayley Code to deal with the clustered tree problems. The new evolutionary operators in MFEA have two different levels. The purpose of the first level is to construct a spanning tree which connects to a vertex in each cluster, while the objective of the second one is to determine the spanning tree for each cluster. We focus on evaluating the efficiency of the new MFEA algorithm on known Cayley Codes when solving clustered tree problems. In the aspect of the execution time and the quality of the solutions found, each encoding type of the Cayley Codes is analyzed when performed on both single-task and multi-task to find the solutions of one or two different clustered tree problems respectively. In addition, we also evaluate the effect of those encodings on the convergence speed of the algorithms. Experimental results show the level of effectiveness for each encoding type and prove that the Dandelion Code outperforms the remaining encoding mechanisms when solving clustered tree problems.																	1865-9284	1865-9292				SEP	2020	12	3					185	217		10.1007/s12293-020-00309-2		AUG 2020											
J								Design of new fractional order PI-fractional order PD cascade controller through dragonfly search algorithm for advanced load frequency control of power systems	SOFT COMPUTING										Load frequency control; Governor dead band; Generation rate constraint; Fractional order PI-fractional order PD (FOPI-FOPD) cascade controller; Dragonfly search algorithm; Optimization	AUTOMATIC-GENERATION CONTROL; DIFFERENTIAL EVOLUTION ALGORITHM; COMPARATIVE PERFORMANCE ANALYSIS; LEARNING BASED OPTIMIZATION; AGC; FLOW	Owing to integrating the dense range of distinct electric power sources, high volume of power generation units, abrupt and continuous changes in load demand, and rising utilization of power electronics, the electric power system (EPS) is striving for high-performance control schemes to counterwork the concerns depicted above. Additionally, it is highly creditable to have the controller structure as simple as possible from a viewpoint of practical implementation. Thus, this paper describes a virgin application of fractional order proportional integral-fractional order proportional derivative (FOPI-FOPD) cascade controller for load frequency control (LFC) of electric power generating systems. The proposed controller includes fractional order PI and fractional order PD controllers connected in cascade wherein orders of integrator (lambda) and differentiator (mu) may be fractional. The gains and fractional order parameters of the controller are concurrently tuned using recently proposed dragonfly search algorithm (DSA) by minimizing the integral time absolute error (ITAE) of frequency and tie-line power deviations. DSA is the mathematical model and computer simulation of static and dynamic swarming behaviors of dragonflies in nature, and its implementation in LFC studies is very rare, unveiling additional research gap to be bridged. Performance of the advocated approach is first explored on popular two-area thermal PS with/without governor dead band (GDB) nonlinearity and then on three-area hydrothermal PS with suitable generation rate constraints. To highlight the prominence and universality of our proposal, the work is extended to single-/multi-area multi-source EPSs. Several comparisons with DSA optimized FOPID controller and the relevant recent works for each test system indicate the contribution of proposed DSA optimized FOPI-FOPD cascade controller in alleviating settling time/undershoot/overshoot of frequency and tie-line power oscillations.																	1432-7643	1433-7479															10.1007/s00500-020-05215-w		AUG 2020											
J								Deep learning for biomedical image reconstruction: a survey	ARTIFICIAL INTELLIGENCE REVIEW										Image reconstruction; Modality; Deep learning; Inverse problem; Analytical approach; Iterative approach; Limited data representation	GENERATIVE ADVERSARIAL NETWORKS; CONVOLUTIONAL NEURAL-NETWORKS; INVERSE PROBLEMS; ITERATIVE RECONSTRUCTION; THRESHOLDING ALGORITHM; CT; TOMOGRAPHY; PROJECTION; SPARSE; DOMAIN	Medical imaging is an invaluable resource in medicine as it enables to peer inside the human body and provides scientists and physicians with a wealth of information indispensable for understanding, modelling, diagnosis, and treatment of diseases. Reconstruction algorithms entail transforming signals collected by acquisition hardware into interpretable images. Reconstruction is a challenging task given the ill-posedness of the problem and the absence of exact analytic inverse transforms in practical cases. While the last decades witnessed impressive advancements in terms of new modalities, improved temporal and spatial resolution, reduced cost, and wider applicability, several improvements can still be envisioned such as reducing acquisition and reconstruction time to reduce patient's exposure to radiation and discomfort while increasing clinics throughput and reconstruction accuracy. Furthermore, the deployment of biomedical imaging in handheld devices with small power requires a fine balance between accuracy and latency. The design of fast, robust, and accurate reconstruction algorithms is a desirable, yet challenging, research goal. While the classical image reconstruction algorithms approximate the inverse function relying on expert-tuned parameters to ensure reconstruction performance, deep learning (DL) allows automatic feature extraction and real-time inference. Hence, DL presents a promising approach to image reconstruction with artifact reduction and reconstruction speed-up reported in recent works as part of a rapidly growing field. We review state-of-the-art image reconstruction algorithms with a focus on DL-based methods. First, we examine common reconstruction algorithm designs, applied metrics, and datasets used in the literature. Then, key challenges are discussed as potentially promising strategic directions for future research.																	0269-2821	1573-7462															10.1007/s10462-020-09861-2		AUG 2020											
J								Optimal solution of the Generalized Dubins Interval Problem: finding the shortest curvature-constrained path through a set of regions	AUTONOMOUS ROBOTS										Dubins vehicle; Multi-goal planning; Generalized Dubins Interval Problem; Dubins Touring Regions Problem	SEQUENCE	The Generalized Dubins Interval Problem (GDIP) stands to determine the minimal length path connecting two disk-shaped regions where the departure and terminal headings of Dubins vehicle are within the specified angle intervals. The GDIP is a generalization of the existing point-to-point planning problem for Dubins vehicle with a single heading angle per particular location that can be solved optimally using closed-form expression. For the GDIP, both the heading angles and locations need to be chosen from continuous sets which makes the problem challenging because of infinite possibilities how to connect the regions by Dubins path. We provide the optimal solution of the introduced GDIP based on detailed problem analysis. Moreover, we propose to employ the GDIP to provide the first tight lower bound for the Dubins Touring Regions Problem which stands to find the shortest curvature-constrained path through a set of regions in the prescribed order.																	0929-5593	1573-7527				SEP	2020	44	7			SI		1359	1376		10.1007/s10514-020-09932-x		AUG 2020											
J								IoTcloud platform for information processing in smart city	COMPUTATIONAL INTELLIGENCE										mobility management; modern carriage systems; transportation and traffic pattern	SOFTWARE-DEFINED NETWORKING; RESOURCE-MANAGEMENT; MOBILE NETWORKS; 5G; CHALLENGES; TECHNOLOGIES; SERVICE	The construction of a sustainable transport network for people or goods will gain and harm mobility depending on their nature and implementation, using new technology and business models. The regular mobile handoff and replacement of connections allow the efficient mobility management of asynchronous wireless transfer mode (ATM) networks. Hence, the design of the broad spectrum usability network architecture for public ATM systems has been given considerable attention in the literature. Since it has been done on the critical subject of estimating user mobility and the prediction, which aims to improve communication efficiency and the bandwidth efficiency of the underlying system architecture, this article addresses the problem by developing the impact of hierarchical framework of service on transportation and traffic pattern (HFSTTP). The growth in mobile Internet and the use of mobile devices over the last decade allowed for omnipresent information on traffic. With the growing use of unique mobile apps, the number of routing users has evolved to be sufficiently large to interrupt the traffic flow pattern. Modern carriage systems and urban logistics improve the efficiency of the transport of goods and slow down the use of road infrastructure. Numerical examples of vehicle traffic include proof of ties between the network traffic and the routing decision-making layer. The partnership between a cooperative control layer and a lower control layer to optimize freight transport is a second example. The congestion price in all over the world shows how to incorporate the social plan layer in future mobility services.																	0824-7935	1467-8640															10.1111/coin.12387		AUG 2020											
J								Adaptive weighted dynamic differential evolution algorithm for emergency material allocation and scheduling	COMPUTATIONAL INTELLIGENCE										adaptive dynamic weighting; differential evolution algorithm; emergency material allocation and scheduling; smart cities research	OPTIMIZATION	Emergency material allocation and scheduling is a combination optimization problem, which is essentially a Non-deterministic Polynomial (NP) problem. Aiming at the problems such as slow convergence, easy prematurely falling into local optimum, and parameter constraints to solve high-dimensional and multi-modal combination optimization problems, this article proposes an adaptive weighted dynamic differential evolution (AWDDE) algorithm. The algorithm uses a chaotic mapping strategy to initialize the population. By weighting the standard differential evolution (DE) mutation strategy, a new weighted mutation operator is proposed. The scaling factor and cross probability can be adaptively adjusted. A disturbance operator is introduced to randomly generate the perturbation mutation and to accelerate the premature individuals to jump out of the local optimum. The algorithm is applied to the problem of emergency material allocation and scheduling, and a two-stage emergency material allocation and scheduling model is established. Compared with the standard DE algorithm and the chaos adaptive particle swarm algorithm, the results show that the AWDDE algorithm has the characteristics of stronger global optimization ability and faster convergence speed compared with other optimization algorithms, which provide assistance for smart cities research, including smart city services, applications, case studies, and policymaking considerations for emergency management.																	0824-7935	1467-8640															10.1111/coin.12389		AUG 2020											
J								Using error decay prediction to overcome practical issues of deep active learning for named entity recognition	MACHINE LEARNING										Active learning; Transparency; Robustness to labeling noise; Black-box models; Clustering; Named entity recognition	MINIMIZATION	Existing deep active learning algorithms achieve impressive sampling efficiency on natural language processing tasks. However, they exhibit several weaknesses in practice, including (a) inability to use uncertainty sampling with black-box models, (b) lack of robustness to labeling noise, and (c) lack of transparency. In response, we propose a transparent batch active sampling framework by estimating the error decay curves of multiple feature-defined subsets of the data. Experiments on four named entity recognition (NER) tasks demonstrate that the proposed methods significantly outperform diversification-based methods for black-box NER taggers, and can make the sampling process more robust to labeling noise when combined with uncertainty-based methods. Furthermore, the analysis of experimental results sheds light on the weaknesses of different active sampling strategies, and when traditional uncertainty-based or diversification-based methods can be expected to work well.																	0885-6125	1573-0565				SEP	2020	109	9-10			SI		1749	1778		10.1007/s10994-020-05897-1		AUG 2020											
J								Unsupervised Optical Flow Estimation Based on Improved Feature Pyramid	NEURAL PROCESSING LETTERS										Optical flow estimation; Deep learning; Feature pyramid; Dilated convolution		Deep learning methods for optical flow estimation usually increase the receptive field of convolution through reducing image resolution, which results in loss of spatial detail information during feature extraction. In this paper, we introduce dilated convolution into feature pyramid network, which can extract multi-scale features containing more motion details and can further improve the accuracy of optical flow estimation. The unsupervised loss function is based on forward-backward consistency check and robust census transform that has a good constraint performance in the case of illumination changes, which can train an unsupervised learning optical flow model with higher accuracy. Our network is trained on FlyingChairs and KITTI raw datasets with an unsupervised manner and tested on MPI-Sintel, KITTI 2012 and KITTI 2015 benchmarks. The experimental results show the advantages of our method in unsupervised learning approaches.																	1370-4621	1573-773X				OCT	2020	52	2			SI		1601	1612		10.1007/s11063-020-10328-2		AUG 2020											
J								Single shot multi-oriented text detection based on local and non-local features	INTERNATIONAL JOURNAL ON DOCUMENT ANALYSIS AND RECOGNITION										Text detection; Natural scene text; Convolutional neural network; Attention mechanism		In order to improve the robustness of text detector on scene text of various scales, a single shot text detector that combines local and non-local features is proposed in this paper. A dilated inception module for local feature extraction and a text self-attention module for non-local feature extraction are presented, and these two kinds of modules are integrated into single shot detector (SSD) of generic object detection so as to perform multi-oriented text detection in natural scene. The proposed modules make a contribution to richer and wider receptive field and enhance feature representation. Furthermore, the performance of our text detector is improved. In addition, compared with previous text detectors based on SSD which classify positive and negative samples depending on default boxes, we exploit pixels as reference for more accurate matching with ground truth which avoids complex anchor design. Furthermore, to evaluate the effectiveness of the proposed method, we carry out several comparative experiments on public standard benchmarks and analyze the experimental results in detail. The experimental results illustrate that the proposed text detector can compete with the state-of-the-art methods.																	1433-2833	1433-2825				DEC	2020	23	4					241	252		10.1007/s10032-020-00356-y		AUG 2020											
J								IIoT and cyber-resilience Could blockchain have thwarted the Stuxnet attack?	AI & SOCIETY										Resilience; Cybersecurity; Enterprise; Supply chain; Digitalization; Industry 4; 0; Blockchain; USB; Stuxnet; Digital twin		Contemporary business (including those with integrated AI capabilities) often encompasses or aspires towards the automated, networked production of industrial goods across transnational supply chains that have many digitalized interfaces. This allows competitive operations in time, costs, and quality, which have been widely discussed. On the downside, it entails cyber threats with significant risks for society in areas including business, environment, and health. Hence, to adequately manage these risks in the emerging digital world, there is a vital necessity to raise awareness, establish, maintain, and further develop cyber-security measures to ensure an appropriate level of protection along the entire value chain and supply chain. Blockchain capabilities are introduced to improve the technical and organizational basis for secured operations in industrial networks. Its advantages are explained by a simple USB-device use case, that has often been the root cause to subsequent security incidents, especially in the Stuxnet incident.																	0951-5666	1435-5655															10.1007/s00146-020-01023-w		AUG 2020											
J								Classifying the valence of autobiographical memories from fMRI data	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Analysis of cognitive processes; Autobiographical memories; Classification; Machine learning; Feature selection	RETRIEVAL	We show that fMRI analysis using machine learning tools are sufficient to distinguish valence (i.e., positive or negative) of freely retrieved autobiographical memories in a cross-participant setting. Our methodology uses feature selection (ReliefF) in combination with boosting methods, both applied directly to data represented in voxel space. In previous work using the same data set, Nawa and Ando showed that whole-brain based classification could achieve above-chance classification accuracy only when both training and testing data came from the same individual. In a cross-participant setting, classification results were not statistically significant. Additionally, on average the classification accuracy obtained when using ReliefF is substantially higher than previous results - 81% for the within-participant classification, and 62% for the cross-participant classification. Furthermore, since features are defined in voxel space, it is possible to show brain maps indicating the regions of that are most relevant in determining the results of the classification. Interestingly, the voxels that were selected using the proposed computational pipeline seem to be consistent with current neurophysiological theories regarding the brain regions actively involved in autobiographical memory processes.																	1012-2443	1573-7470				DEC	2020	88	11-12					1261	1274		10.1007/s10472-020-09705-3		AUG 2020											
J								Group-based generalized q-rung orthopair average aggregation operators and their applications in multi-criteria decision making	COMPLEX & INTELLIGENT SYSTEMS										q-rung orthopair fuzzy sets; Gq-ROFSs; GGq; ROFSs; Gq-ROF aggregation operators; GGq-ROF aggregation operators; MCDM	FUZZY INFORMATION AGGREGATION; PYTHAGOREAN MEMBERSHIP GRADES; SETS; VIEW	The objective of this manuscript is to investigate the concept of generalized q-rung orthopair fuzzy sets (Gq-ROFSs) and group generalized q-rung orthopair fuzzy sets (GGq-ROFSs) by incorporating the concept of generalized parameter and group generalized parameters in q-rung orthopair fuzzy environment. The main advantage of generalized parameter in q-rung orthopair fuzzy environment is to reduce uncertain errors in the original information to ensure the expert's level of trust and improve the accuracy of final decision. On the base of generalized parameter, some aggregation operators are introduced such as generalized q-rung orthopair fuzzy average aggregation operators and group generalized q-rung orthopair fuzzy average aggregation operators and studied their related properties. Furthermore, a multi-criteria decision-making method technique based on proposed approach is presented. Finally, a numerical example is provided to illustrate the feasibility of the proposed methods and deliver the sensitivity analysis and comparative analysis, which show the superiority of developed approached than existing methods.																	2199-4536	2198-6053															10.1007/s40747-020-00176-x		AUG 2020											
J								Machine learning approach for threat detection on social media posts containing Arabic text	EVOLUTIONARY INTELLIGENCE										Detection; CNN; Images; Comments; Threat; Transfer learning; TensorFlow; Inception v-3		Recently, social media has become a part of daily people's routine. People frequently share images, text, and videos in social media (e.g., Twitter, Snapchat, Facebook, and Instagram). Consequently, there is a demand for an automated method to monitor and analyze the shared social media content. This research developed a method that aims to detect any threat in the images or comments in the shared content. Instagram has gained popularity as the most famous social media website and mobile application for media sharing. Instagram enables users to upload, view, share, and comment on a media post (image or video). There are many unwanted contents in Instagram posts, such as threats, which may cause problems for society and national security. The purpose of this research is to construct a model that can be utilized to classify Instagram content (images and Arabic comments) for threat detection. The model was built using Convolutional Neural Network, which is a deep learning algorithm. The dataset was collected utilizing the Instagram API and search engine and then labeled manually. The model used was retrained on the images and comments training set with the classes of threat and non-threat. The results show that the accuracy of the developed model is 96% for image classification and 99% for comment classification. The result of this research will be useful in tracking and monitoring social media posts for threat detection.																	1864-5909	1864-5917															10.1007/s12065-020-00458-w		AUG 2020											
J								A multistep priority-based ranking for top-N recommendation using social and tag information	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Recommender system; Collaborative filtering; Social network; Tag set; Rating data; Recommendation accuracy; Item ranking	MATRIX FACTORIZATION; SYSTEMS	Recommender system is a collection of information retrieval tools and techniques used for recommending items to users based on their choices. For improving recommendation accuracy, the use of extra information (e.g., social, trust, item tags, etc.) other than user-item rating data remains an active area of research since last one and half decade. In this paper, we propose a novel methodology for top-N item recommendation, which uses three different kinds of information: user-item rating data, social network among the users, and tags associated with the items. The proposed method has mainly five steps: (i) creation of neighbor users' item set, (ii) construction of the user-feature matrix, (iii) computation of user priority, (iv) computation of item priority, and finally, (v) recommendation based on the item priority. We implement the proposed methodology with three recommendation dataset. We compare our results with that of the obtained from some state-of-the-art ranking methods and observe that recommendation accuracy is improved in the case of the proposed algorithm for both all users and cold-start users scenarios. The algorithm is also able to generate more cold-start items in the recommended item list.																	1868-5137	1868-5145															10.1007/s12652-020-02388-y		AUG 2020											
J								A fire detection model based on power-aware scheduling for IoT-sensors in smart cities with partial coverage	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Internet of things (IoT); Fire detection; Smart cities; Partial coverage; Quality of service (QoS); Wireless sensor networks (WSNs)		Fire detection techniques have received considerable critical attention over the past ten years. Regardless of the progress in the area of fire detection, questions have been raised about the cost, complexity, consumed power from a large number of sensors to analyze sensors' data. Debate continues about the best strategies for the management of consumed power and how to accelerate the processing of real-time data in fire detection through the internet of things. This paper presents a partial coverage and a power-aware IoT-based fire detection model with different multi-functional sensors in smart cities. In the proposed model, the sleep scheduling approach used for saving sensors energy. This approach will significantly help in saving consumed energy and thus the need for any extra number of nodes required for continuously covering the target area. Moreover, fog computing is applied to process real-time data aggregated from a large number of sensors for running systems more efficiently. Validation of the proposed model was carried out via simulation and experimental testbed implementation with Arduino, sensors, and Raspberry pi. The results obtained indicate how the proposed technique can efficiently determine the sensors to meet the constraints imposed. The most striking finding to emerge from this experimental and simulation study is that the proposed technique helps in ensuring excellent performance in terms of the number of active nodes and the network lifetime, over other state-of-the-art techniques. The most striking finding of this work compared to MWSAC and PCLA, while covering the same area, is the minimization of the number of active nodes by 64.33% and 15%. It raises the network's lifetime by 91.32% compared to MWSAC and 12% compared to PCLA, respectively.																	1868-5137	1868-5145															10.1007/s12652-020-02425-w		AUG 2020											
J								Medical image integrated possessions assisted soft computing techniques for optimized image fusion with less noise and high contour detection	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Medical image processing; Soft computing; Intelligent image fusion; Fuzzy mid matrix; Data learning process	SEGMENTATION; ENHANCEMENT	This paper introduces an intellectual image fusion technique which is much focused on Medical Image Integrated Possessions assisted Soft Computing Techniques (MIPSCT) with fuzzy sets. This dual image fusion design uses a fuzzy mid matrix method and a smooth adjustment process which helps to eliminate impulsive noise from extremely distorted images that is included in a smart image agent when fusing image on various image processing environment. The fuzzy function used in the filter is intended to remove impulses without losing fine details and textures which are more important in image fusion modelling. Furthermore, adjust filter parameters from a set of exercise data with an image culture process based on genetic algorithm has been implemented on MIPSCT to improve contour detection. The experimental results have been analyzed based on intelligent soft computing tools in assistance with matrix laboratory to achieve better output in accordance with SDROM, AWFM, SFVQ, and DCT modelling for brain image datasets. The validation at lab scale shows promising results on Peak Signal to Noise Ratio and Absolute Mean Error (AME) parameters in accordance with conventional methods.																	1868-5137	1868-5145															10.1007/s12652-020-02316-0		AUG 2020											
J								Early diagnose breast cancer with PCA-LDA based FER and neuro-fuzzy classification system	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Principal-component-analysis (PCA); Linear-discriminant-analysis (LDA); Adaptive neural network based fuzzy-rule interference system (ANNFIS); Feature extraction and reduction (FER)	ANFIS; SELECTION; HYBRID	In recent years, breast cancer is recognized as critical disease that needs to be detected at early stage. Breast cancer can be detected either by mammography or biopsy technique. Biopsy is cost effective procedure for breast cancer detection. Fine needle aspiration (FNA) digital image is pre-processed and extracted features from it used for classification. Based on the parameters of feature attributes, they are classified into two classes: malignant (cancerous) and benign (not cancerous) tumor cell. Manual analysis and classification of this image is very difficult and challenging task. The need of automation for detecting and classifying these tumor cells in cost effective and accurate manner provokes many researchers in this field. With this aim we proposed PCA-LDA based feature extraction and reduction (FER) technique that reduce the original feature space to large extent and performing training over this reduced set that give excellent accuracy of 98.6%. For classification we use ANNFIS classifier that uses the neural-network concept with some fuzzy rule logic. We perform comparative performance analysis study amongst our proposed work over two other classifiers i.e. support vector machine (SVM) and multi-layer perceptron (MLP). The experimental result shows that proposed framework outperform over SVM and MLP with an accuracy of 98.6%.																	1868-5137	1868-5145															10.1007/s12652-020-02395-z		AUG 2020											
J								Fuzzy traceability: using domain knowledge information to estimate the followed route of process instances in non-exhaustive monitoring environments	JOURNAL OF INTELLIGENT MANUFACTURING										Traceability; Process mining; BPMN; Monitoring point; Domain knowledge; Non-exhaustive monitoring	PROCESS MODELS; FOOD TRACEABILITY; SYSTEM; QUALITY; DESIGN; TRENDS	In the frame of the process monitoring domain, traceability in environments with non-exhaustive event logs remains as a challenge. In these scenarios, the discovery of the route followed by a particular item along the entire process is a difficult task since it is not possible to access all the required pieces of information from processes under monitorization. To tackle this issue, the concept of fuzzy traceability is brought into the scene. The gist of the latter is to use contextual information derived from the domain of interest itself to infer the most probable route followed. To carry out this task, the proposed algorithm takes advantage of additional sources of machine readable information that describes in a more detailed manner the process models under study. This information is included in the process models using the advanced features supported by the BPMN-E-2(Business Process Model and Notation - Enhanced Expressiveness) specification, an extension of the well-known BPMN notation. In this way , it is possible to properly use as inputs: time restrictions of the activities included in the process; decision-making and monitoring points included; and the effects derived from the activities undergone. As a consequence, a probabilistic estimation of the route followed is generated by combining this information according to the presented algorithm. After the validation and simulation of the fuzzy traceability algorithm using real-world models, the results obtained are positive and show that, as the contextual information included grows, the route estimation gets more acurated. This high success rate suggests that the fuzzy traceability proposal is useful for the analysis of processes with poor quality of monitoring information, and outdoes the application of more conventional traceability techniques.																	0956-5515	1572-8145															10.1007/s10845-020-01636-4		AUG 2020											
J								Cluster Consensus for Nonlinear Multi-Agent Systems	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Multi-agent systems; Cluster consensus; Nonlinear systems; General high-order systems	NETWORKS	A cluster consensus algorithm for nonlinear multi-agent systems under directed graph topology is proposed in this paper. Cluster consensus is the convergence of states/outputs of agents in the same cluster to consistent values which are different from those of other clusters. Cluster consensus has been obtained based on Lyapunov stability and matrix theory in terms of some sufficient conditions. A feedback control law is provided using Linear Matrix Inequality (LMI) to achieve cluster consensus for multi-agent systems. Moreover, cluster consensus for nonlinear multi-agent systems in the presence of time delay has been studied in this paper. Finally, simulation results are presented for different number of clusters to validate theoretical analysis. Examples are provided for first-order and second-order and also general high-order systems. Furthermore, first-order system with time delay is simulated for a single-link flexible joint manipulator.																	0921-0296	1573-0409															10.1007/s10846-020-01218-9		AUG 2020											
J								A structure-self-organizing DBN for image recognition	NEURAL COMPUTING & APPLICATIONS										Deep learning; Structure self-organizing; Regularization; Image recognition		A structural self-organized DBN (S-DBN) is proposed in this paper to improve the ability of feature learning in unsupervised training. In S-DBN, the strategy of dropout is designed for unsupervised learning to reduce inner cooperation between feature detectors. Then, the regularization-reinforced transfer function is put forward, in order to further reduce the insignificant weights, and to raise the abilities of feature learning and generation. The fast training method of contrastive divergence is designed, and backpropagation is used in supervised training. Finally, two experiments on regression and classification using MNIST show that S-DBN has better generation and faster convergence rate than other methods, in particular, in regression experiment, the proposed model beats traditional DBN by 1.50%; in image classification, the proposed model achieves smaller testing error in much less computing time.																	0941-0643	1433-3058															10.1007/s00521-020-05262-2		AUG 2020											
J								Classification of gene expression patterns using a novel type-2 fuzzy multigranulation-based SVM model for the recognition of cancer mediating biomarkers	NEURAL COMPUTING & APPLICATIONS										Microarray data; Multigranulation; Type-2 fuzzy sets; Rough sets; Classifier; Support vector machine	FEATURE-SELECTION; ALGORITHM	In this article, we propose a novel type-2 fuzzy multigranulation-based SVM model for gene expression pattern classification on human breast cancer dataset. Firstly, a type-2 fuzzy multigranulation system has been designed for the classification task dealing with noisy and nonlinear microarray gene expression patterns. Thereafter, the fuzzy if-then rules have been devised on the feature vectors of microarray to enable accurate bilinear classification process. The fuzzy if-then rules in the domain of type-2 fuzzy multigranulation system are able to identify efficient expression patterns that have been deferentially expressed from normal state to carcinogenic state. The proposed method reduces the structural complexity of the fuzzy if-then rules (type 1) since it works on upper and lower membership functions instead of a single membership function. In addition, a fuzzy rough approximation has been utilized in the model to reduce the computational cost. Lastly, the association among genes consisting of significantly different expression patterns from normal state to malignant state has been recognized with respect to their nature. The effectiveness of the proposed method has been implemented on eight microarray gene expression datasets for human breast cancer patients. Moreover, we have validated the results by F-score and NCBI database which signify that the proposed model performs better in comparison with the existing methods.																	0941-0643	1433-3058															10.1007/s00521-020-05241-7		AUG 2020											
J								Density-weighted support vector machines for binary class imbalance learning	NEURAL COMPUTING & APPLICATIONS										Density weight; Class imbalance; Least squares; Support vector machine	ERROR; OPTIMIZATION; REGRESSION; NOISY	In real-world binary classification problems, the entirety of samples belonging to each class varies. These types of problems where the majority class is notably bigger than the minority class can be called as class imbalance learning (CIL) problem. Due to the CIL problem, model performance may degrade. This paper presents a new support vector machine (SVM) model based on density weight for binary CIL (DSVM-CIL) problem. Additionally, an improved 2-norm-based density-weighted least squares SVM for binary CIL (IDLSSVM-CIL) is also proposed to increase the training speed of DSVM-CIL. In IDLSSVM-CIL, the least squares solution is obtained by considering 2-norm of slack variables and solving the primal problem of DSVM-CIL with equality constraints instead of inequality constraints. The basic ideas behind the algorithms are that the training datapoints are given weights during the training phase based on their class distributions. The weights are generated by using a density-weighted technique (Cha et al. in Expert Syst Appl 41(7):3343-3350, 2014) to reduce the effects of CIL. Experimental analyses are performed on some interesting imbalanced artificial and real-world datasets, and their performances are measured using the area under the curve and geometric mean (G-mean). The results are compared with SVM, least squares SVM, fuzzy SVM, improved fuzzy least squares SVM, affinity and class probability-based fuzzy SVM and entropy-based fuzzy least squares SVM. Similar or better generalization results indicate the efficacy and applicability of the proposed algorithms.																	0941-0643	1433-3058															10.1007/s00521-020-05240-8		AUG 2020											
J								Research on information steganography based on network data stream	NEURAL COMPUTING & APPLICATIONS										Network protocol; Data steganography; Timestamp; Network data flow		To protect the information from being intercepted by third parties during the network communication process, this paper proposes a new type of data steganography technology based on network data flow. Using the network protocol itself and the relationship between data packets in the entire network data stream to perform network data steganography, transfer hidden data, and perform secondary identity authentication. Different from the traditional steganography method, this method can encode the hidden data and send the interval value by embedding the data packet, thereby hiding and transmitting the hidden data. In this technology, the operation of hidden data does not affect the user's access request for real network data, and it can perform processes such as hidden data transfer and secondary authentication without the user being able to detect it. Through experimental verification and evaluation, our method improves the concealment of the steganographic channel, is not easy to attract attention and has no obvious statistical characteristics of the traffic, and can improve the concealment and robustness of the steganography technology based on network data streams.																	0941-0643	1433-3058															10.1007/s00521-020-05260-4		AUG 2020											
J								SaS-BCI: a new strategy to predict image memorability and use mental imagery as a brain-based biometric authentication	NEURAL COMPUTING & APPLICATIONS										Brain-Computer Interface; Biometric authentication; EEG; Signal acquisition	PERSON AUTHENTICATION; EEG	Security authentication is one of the most important levels of information security. Nowadays, human biometric techniques are the most secure methods for authentication purposes that cover the problems of older types of authentication like passwords and pins. There are many advantages of recent biometrics in terms of security; however, they still have some disadvantages. Progresses in technology made some specific devices, which make it possible to copy and make a fake human biometric because they are all visible and touchable. According to this matter, there is a need for a new biometric to cover the issues of other types. Brainwave is human data, which uses them as a new type of security authentication that has engaged many researchers. There are some research and experiments, which are investigating and testing EEG signals to find the uniqueness of human brainwave. Some researchers achieved high accuracy rates in this area by applying different signal acquisition techniques, feature extraction and classifications using Brain-Computer Interface (BCI). One of the important parts of any BCI processes is the way that brainwaves could be acquired and recorded. A new Signal Acquisition Strategy is presented in this paper for the process of authorization and authentication of brain signals specifically. This is to predict image memorability from the user's brain to use mental imagery as a visualization pattern for security authentication. Therefore, users can authenticate themselves with visualizing a specific picture in their minds. In conclusion, we can see that brainwaves can be different according to the mental tasks, which it would make it harder using them for authentication process. There are many signal acquisition strategies and signal processing for brain-based authentication that by using the right methods, a higher level of accuracy rate could be achieved which is suitable for using brain signal as another biometric security authentication.																	0941-0643	1433-3058															10.1007/s00521-020-05247-1		AUG 2020											
J								Hyperbox-based machine learning algorithms: a comprehensive survey	SOFT COMPUTING										Hyperboxes; Membership function; Fuzzy min-max neural network; Hybrid classifiers; Data classification; Clustering; Online learning	MAX NEURAL-NETWORK; RULE EXTRACTION; FAULT-DETECTION; MISSING VALUES; FUZZY; CLASSIFIER; COMBINATION; SUPPORT; DESIGN; SPACE	With the rapid development of digital information, the data volume generated by humans and machines is growing exponentially. Along with this trend, machine learning algorithms have been formed and evolved continuously to discover new information and knowledge from different data sources. Learning algorithms using hyperboxes as fundamental representational and building blocks are a branch of machine learning methods. These algorithms have enormous potential for high scalability and online adaptation of predictors built using hyperbox data representations to the dynamically changing environments and streaming data. This paper aims to give a comprehensive survey of the literature on hyperbox-based machine learning models. In general, according to the architecture and characteristic features of the resulting models, the existing hyperbox-based learning algorithms may be grouped into three major categories: fuzzy min-max neural networks, hyperbox-based hybrid models and other algorithms based on hyperbox representations. Within each of these groups, this paper shows a brief description of the structure of models, associated learning algorithms and an analysis of their advantages and drawbacks. Main applications of these hyperbox-based models to the real-world problems are also described in this paper. Finally, we discuss some open problems and identify potential future research directions in this field.																	1432-7643	1433-7479															10.1007/s00500-020-05226-7		AUG 2020											
J								Association rule mining using fuzzy logic and whale optimization algorithm	SOFT COMPUTING										Data mining; Dimensionality reductionality; Fuzzy association rule mining; Classical association rule mining; Whale optimization algorithm		Association rule mining (ARM) is a well-known data mining scheme that is used to discover the commonly co-occurred itemsets from the transactional datasets. Two considerable steps of ARM are frequent item recognition and association rule generation. Minimum support and confidence measures are used in the generation of association rules. Many algorithms have been projected by the researchers to generate association rules. Fuzzy logic is incorporated to uncover the recurrent itemsets and interesting fuzzy association rules. In general, huge volume of datasets could be analyzed which in turn needs more number of database scans. In addition to this, all the transactions and items are not required for data analysis. Hence, the first step of this research work uses a dimensionality reduction technique which drastically reduces the size of the data set. This dimensionality reduction technique uses low variance and hash table methods. The proposed algorithm effectively identifies the significant transactions and items from the database. The issues of dimensionality reduction appear when the items in the databases are higher dimension than endure. The proposed algorithm reduces the irrelevant items and transactions from the transactional database. The proposed dimensionality reduction technique dimensionality reduction in transactions and items is compared with the extend frequent pattern (EFP) and intersection set theory EFP and dimensionality reduction using frequency count. Item reduction, transaction reduction, execution time and memory space are the performance factors. Second step proposes fuzzy and whale optimization for frequent item identification and association rule generation. The efficiency of the proposed algorithm is compared with particle swarm optimization genetic algorithm and fuzzy frequent itemset-Miner. Performance metrics used in this step are number of frequent items, association rules generated, execution time and memory required. Experimental results proved that the proposed techniques have produced the good results.																	1432-7643	1433-7479															10.1007/s00500-020-05229-4		AUG 2020											
J								A new compound wind speed forecasting structure combining multi-kernel LSSVM with two-stage decomposition technique	SOFT COMPUTING										Wind speed forecasting; Two-stage decomposition; Complementary ensemble empirical mode decomposition with adaptive noise; Wavelet transform; Multi-kernel LSSVM; Hybrid particle swarm optimization gravitational search algorithm	EMPIRICAL MODE DECOMPOSITION; EXTREME LEARNING-MACHINE; SUPPORT VECTOR MACHINES; FEATURE-SELECTION; WAVELET TRANSFORM; NEURAL-NETWORK; POWER PREDICTION; OPTIMIZATION; FARMS; METHODOLOGY	The aims of this study are to develop a novel compound structure that consists of two-stage decomposition (TSD), hybrid particle swarm optimization gravitational search algorithm (HPSOGSA) and multi-kernel least square support vector machine (MLSSVM) for improving forecasting accuracy. In the most previous wind speed forecasting studies, only one wind speed signal decomposition method is considered, which is insufficient. To better deal with the wind speed time series, TSD method combining complementary ensemble empirical mode decomposition with adaptive noise with wavelet transform is firstly employed in the proposed model to preprocess the wind speed samples; then, binary-valued particle swarm optimization gravitational search algorithm is exploited as feature selection to identify and eliminate the abnormal noise signal within the input candidate matrix that is determined by partial autocorrelation function. The kernel function and the kernel parameters have great influence on the regression performance of LSSVM. To solve these problems, integrations of radial basis function, polynomial (poly) and linear kernel functions by optimal weighted coefficients are constructed as multi-kernel function for LSSVM, namely MKLSSVM, and the parameter combination is tuned by conventional PSOGSA. The feature selection and parameter optimization are realized by hybrid PSOGSA (HPSOGSA) simultaneously. Finally, comprehensive comparison and analysis are carried out using the historical wind speed data from one wind farm of China to illustrate the excellent forecasting performance of TSD-HPSOGSA-MKLSSVM.																	1432-7643	1433-7479															10.1007/s00500-020-05233-8		AUG 2020											
J								Assembly sequence optimization based on hybrid symbiotic organisms search and ant colony optimization	SOFT COMPUTING										Assembly sequence optimization; Symbiotic organisms search; Ant colony optimization	ALGORITHM; GENERATION; MODEL	Assembly sequence optimization aims to find the optimal or near-optimal assembly sequences under multiple assembly constraints. Since it is NP-hard for complex assemblies, the heuristic algorithms are widely used to find the optimal or near-optimal assembly sequences in an acceptable computation time. Considering the multiple assembly constraints, an assembly model is presented for assembly sequence optimization. Then, the hybrid symbiotic organisms search and ant colony optimization is used to find the optimal or near-optimal assembly sequences. The symbiotic organisms search has a relatively strong global optimization capability but weak local optimization capability. On the other hand, the ant colony optimization has the relatively strong local optimization capability for assembly sequence optimization even though the parameters are not optimized. The hybrid symbiotic organisms search and ant colony optimization take advantages of their capacities for assembly sequence optimization. The case study demonstrates that the hybrid symbiotic organisms search and ant colony optimization finds the better assembly sequences within less iteration than the individual ant colony optimization and symbiotic organisms search in most experiments under the same preconditions.																	1432-7643	1433-7479															10.1007/s00500-020-05230-x		AUG 2020											
J								Image Matching from Handcrafted to Deep Features: A Survey	INTERNATIONAL JOURNAL OF COMPUTER VISION										Image matching; Graph matching; Feature matching; Registration; Handcrafted features; Deep learning	INTEREST POINT DETECTORS; LEARNING BINARY-CODES; OBJECT RECOGNITION; MUTUAL INFORMATION; CORNER DETECTION; PERFORMANCE EVALUATION; SIMILARITY MEASURE; SET REGISTRATION; SURFACE-FEATURE; SCALE-SPACE	As a fundamental and critical task in various visual applications, image matching can identify then correspond the same or similar structure/content from two or more images. Over the past decades, growing amount and diversity of methods have been proposed for image matching, particularly with the development of deep learning techniques over the recent years. However, it may leave several open questions about which method would be a suitable choice for specific applications with respect to different scenarios and task requirements and how to design better image matching methods with superior performance in accuracy, robustness and efficiency. This encourages us to conduct a comprehensive and systematic review and analysis for those classical and latest techniques. Following the feature-based image matching pipeline, we first introduce feature detection, description, and matching techniques from handcrafted methods to trainable ones and provide an analysis of the development of these methods in theory and practice. Secondly, we briefly introduce several typical image matching-based applications for a comprehensive understanding of the significance of image matching. In addition, we also provide a comprehensive and objective comparison of these classical and latest techniques through extensive experiments on representative datasets. Finally, we conclude with the current status of image matching technologies and deliver insightful discussions and prospects for future works. This survey can serve as a reference for (but not limited to) researchers and engineers in image matching and related fields.																	0920-5691	1573-1405															10.1007/s11263-020-01359-2		AUG 2020											
J								Genetic algorithm-based tabu search for optimal energy-aware allocation of data center resources	SOFT COMPUTING										Cloud computing; Energy-aware resource allocation; Genetic algorithm; Artificial bee colony; Tabu search; Tabu Job Master	MANAGEMENT; EFFICIENT; SYSTEM	Cloud computing delivers practical solutions for long-term image archiving systems. Cloud data centers consume enormous amounts of electrical energy that increases their operational costs. This shows the importance of investing on energy consumption techniques. Dynamic placement of virtual machines to appropriate physical nodes using metaheuristic algorithms is among the methods of reducing energy consumption. In metaheuristic algorithms, there should be a balance between both exploration and exploitation aspects so that they can find better solutions in a search space. Exploration means looking for a solution in a wider area, while exploitation is producing new solutions from existence ones. Artificial bee colony optimization, which is a biological metaheuristic algorithm, is a sign-oriented approach. It has a strong exploration ability, but a relatively weaker exploitation power. On the other hand, tabu search is a popular algorithm that shows better exploitation in comparison with ABC. In this study, cloud computing environments are detailed with an allocation protocol for efficient energy and resource management. The technique of energy-aware allocation splits data centers (DCs) resources among client applications end routes to enhance energy efficacy of DCs and also achieves anticipated quality of service (QoS) for everyone. Heuristic protocols are exercised for optimizing the distribution of resources to upgrade the efficiency of DC. In the current paper, energy-aware resources allotment technique is employed and optimized in clouds via a new approach called Tabu Job Master (JM). Tabu JM claims the benefits of some variables and also rapid convergence speeds. Results are duly achieved for energy consumption-the count of virtual machines (VMs) migration and also makespan. The results shown by Tabu JM are benchmarked by using genetic algorithm (GA), artificial bee colony (ABC), ABC with crossover and technique of mutation, the basic tabu search techniques, and Tabu Job Master.																	1432-7643	1433-7479				NOV	2020	24	21					16705	16718		10.1007/s00500-020-05240-9		AUG 2020											
J								Selecting data adaptive learner from multiple deep learners using Bayesian networks	NEURAL COMPUTING & APPLICATIONS										Time-series forecasting; Deep learning; Bayesian network; Mixture of experts	ENSEMBLE APPROACH; MODELS	A method to predict time series using multiple deep learners and a Bayesian network is proposed. In this study, the input explanatory variables are Bayesian network nodes that are associated with learners. Training data are divided usingK-means clustering, and multiple deep learners are trained depending on the cluster. A Bayesian network is used to determine which deep learner is in charge of predicting a time series. We determine a threshold value and select learners with a posterior probability equal to or greater than the threshold value, which could facilitate more robust prediction. The proposed method is applied to financial time-series data, and the predicted results for the Nikkei 225 index are demonstrated.																	0941-0643	1433-3058															10.1007/s00521-020-05234-6		AUG 2020											
J								Automatic detection of HFOs based on singular value decomposition and improved fuzzy c-means clustering for localization of seizure onset zones	NEUROCOMPUTING										Epilepsy; Seizure onset zones; High-frequency oscillations; Singular value decomposition; Fuzzy c-means clustering algorithm	HIGH-FREQUENCY OSCILLATIONS; QUANTITATIVE-ANALYSIS; NEURAL-NETWORK; 80-500 HZ; K-MEANS; EPILEPSY; ALGORITHM; SYSTEMS; SPIKES	This paper devises a new detector based on singular value decomposition (SVD) and improved fuzzy c-means (FCM) clustering for automatically detecting high-frequency oscillations (HFOs) that are used for localizing seizure onset zones (SOZs) in epilepsy. First, HFO candidates (HFOCs) are obtained by the root mean square method. Next, a time-frequency analysis method is applied to eliminate spikes from HFOCs, which consists of the Stockwell transform, SVD combined with the k-medoids clustering algorithm, Stockwell inverse transform, and threshold method. Then, four kinds of distinctive features, i.e. mean singular values, line lengths, power ratios and spectral centroid of the rest of HFOCs, are extracted and augmented as feature vectors. These vectors are used as the input of the improved FCM clustering algorithm optimized by the simulated annealing algorithm combined with the genetic algorithm. Finally, the localization of SOZs is accomplished based on the concentrations of the detected HFOs. The superiority of the devised detector over other five existing ones is demonstrated by comparing their localization performance. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 4	2020	400						1	10		10.1016/j.neucom.2020.03.010													
J								Sparse metric-based mesh saliency	NEUROCOMPUTING										Mesh saliency; Visual attention; Metric; Sparsity	VISUAL-ATTENTION; OBJECT DETECTION; MODEL	In this paper, we propose an accurate and robust approach to salient region detection for 3D polygonal surface meshes. The salient regions of a mesh are those that geometrically stand out from their contexts and therefore are semantically important for geometry processing and shape analysis. However, a suitable definition of region contexts for saliency detection remains elusive in the field, and the previous methods fail to produce saliency maps that agree well with human annotations. We address these issues by computing saliency in a global manner and enforcing sparsity for more accurate saliency detection. Specifically, we represent the geometry of a mesh using a metric that globally encodes the shape distances between every pair of local regions. We then propose a sparsity-enforcing rarity optimization problem, solving which allows us to obtain a compact set of salient regions globally distinct from each other. We build a perceptually motivated 3D eye fixation dataset and use a large-scale Schelling saliency dataset for extensive benchmarking of saliency detection methods. The results show that our computed saliency maps are closer to the ground-truth. To showcase the usefulness of our saliency maps for geometry processing, we apply them to feature point localization and achieve higher accuracy compared to established feature detectors. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 4	2020	400						11	23		10.1016/j.neucom.2020.02.106													
J								Deep semantic similarity adversarial hashing for cross-modal retrieval	NEUROCOMPUTING										Hashing learning; Semantic similarity; Adversarial learning; Cross-Modal retrieval	REPRESENTATION	Cross-modal retrieval has attracted considerate attention due to the rapid development of Internet and social media, and cross-modal hashing has been widely and successfully used in this domain. However, most existing hashing methods consider much little the semantic similarity levels between instances, whereas simply classified the semantic relationship as either similar or dissimilar. Besides, the issue of preservation of semantic similarity of original data between the extracted features is less explored from the existing methods. Due to the heterogeneity between different modalities, similarity of different modality features cannot be calculated directly. Therefore, in this paper, we propose a deep semantic similarity adversarial hashing (DSSAH) for cross-modal retrieval. We first calculate semantic similarity by using both label and feature information to provide a more accurate value for similarity between instances. And then an adversarial modality discriminator is introduced to establish a common feature space where similarity of each modality features can be calculated. Finally, two loss functions referring to inter-modal loss and intra-modal loss are designed to generate high quality hash codes. Experiments on three common datasets for cross-modal retrieval show that DSSAH outperforms state-of-the-art cross-modal hashing methods in cross-modal retrieval applications. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 4	2020	400						24	33		10.1016/j.neucom.2020.03.032													
J								LSTM variants meet graph neural networks for road speed prediction	NEUROCOMPUTING										Neural network; LSTM; LSTM Variant; GNN; Road speed prediction	TRAFFIC FLOW	Traffic flow prediction is a fundamental issue in smart cities and plays an important role in urban traffic planning and management. An accurate predictive model can help individuals make reliable travel plans and choose optimal routes while efficiently helping administrators maintain traffic order. Road speed prediction, which is a sub-task of traffic flow forecasting, is challenging due to the complicated spatial dependencies characterizing road networks and dynamic temporal traffic patterns. Given the power of recurrent neural networks (RNNs) in learning temporal relations and graph neural networks (GNNs) in integrating graph-structured and node-attributed features, in this paper, we design a novel graph LSTM (GLSTM) framework to capture spatial-temporal representations in road speed forecasting. More specifically, we first present a temporal directed attributed graph to model complex traffic flow. Then, to take advantage of the structure properties and graph features, we employ a message-passing mechanism for feature aggregation and updating. Finally, we further implement several variants of LSTMs with a GN block under the encoder-decoder framework to model spatial-temporal dependencies. The experiments show that our proposed model is able to fully utilize both the road latent graph structure and traffic speed to forecast the road state during future periods. The results on two real-world datasets show that our GLSTM can outperform state-of-the-art baseline methods by up to 32.8% in terms of MAE, 43.2% in terms of MAPE and 23.1% in terms of RMSE. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 4	2020	400						34	45		10.1016/j.neucom.2020.03.031													
J								Pinning synchronization of coupled fractional-order time-varying delayed neural networks with arbitrary fixed topology	NEUROCOMPUTING										Fractional-order neural networks; Time-varying delays; Pinning control; Synchronization	GLOBAL SYNCHRONIZATION; ROBUST SYNCHRONIZATION; MEMRISTOR; STABILITY; CALCULUS	This paper is concerned with pinning synchronization of coupled fractional-order time-varying delayed neural networks. For an arbitrary fixed topology, an effective control law is introduced to select the pinned nodes, under which several sufficient conditions are derived to ensure the asymptotic synchronization of coupled fractional-order delayed neural networks. Moreover, sufficient conditions are derived for the synchronization in Mittag-Leffler sense if there is no delay in coupled fractional-order neural networks. A numerical example is proposed to substantiate the effectiveness of the theoretical results. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 4	2020	400						46	52		10.1016/j.neucom.2020.03.029													
J								Attention shake siamese network with auxiliary relocation branch for visual object tracking	NEUROCOMPUTING										Visual object tracking; Siamese network; Attention shake layer; Auxiliary relocation branch; Switch function	HISTOGRAMS; FILTER	Siamese network is highly regarded in the visual object tracking filed because of its unique advantages of pairwise input and pairwise training. It can measure the similarity between two image patches, which coincides with the principle of the matching-based tracking algorithm. In this paper, a variant Siamese network based tracker is proposed to introduce attention module into traditional Siamese network, and relocate the object with some auxiliary relocation methods, when the proposed tracker runs under an untrusted state. Firstly, a novel attention shake layer is proposed to replace the max pooling layer in Siamese network. This layer could introduce and train two different kinds of attention modules at the same time, which means the proposed attention shake layer could also help to improve the expression power of Siamese network without increasing the depth of the network. Secondly, an auxiliary relocation branch is proposed to assist in object relocation and tracking. According to the prior assumptions of visual object tracking, some weights are involved in the auxiliary relocation branch, such as structure similarity weight, motion similarity weight, motion smoothness weight and object saliency weight. Thirdly, a novel response map based switch function is proposed to monitor the tracking process and control the effect of auxiliary relocation branch. Furthermore, in order to discuss the effect of pooling layer in Siamese network, 9 pooling and attention architectures are proposed and discussed in this paper. Some empirical results are shown in the experiment part. Comparing with the state-of-the-art trackers, the proposed tracker could achieve comparable performance in multiple benchmarks. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 4	2020	400						53	72		10.1016/j.neucom.2020.02.120													
J								Prevention of catastrophic interference and imposing active forgetting with generative methods	NEUROCOMPUTING										Catastrophic interference; Generative replay; Bayesian learning; Langevin dynamics; Active forgetting; Generative neural network	COMPLEMENTARY LEARNING-SYSTEMS; PROBABILISTIC MODELS; MEMORY CONSOLIDATION; EPISODIC MEMORY; NEURAL-NETWORKS; HIPPOCAMPAL; REPLAY; SLEEP; RECONSOLIDATION; FUTURE	Artificial neural networks experience serious catastrophic forgetting (or interference) when information is learned sequentially. A significant effort in the machine learning community is devoted to the solution of this problem. Many approaches to overcome the catastrophic interference (CI) find parallels with an organization of the human memory system. In this paper, we provide a review of biologically inspired approaches for CI prevention. The main emphasis is made on the development of methods inspired by generative properties of the brain. We developed and tested several methods for preventing CI using an artificial dataset generated on the base of previous experience of neural network. The proposed methods include the activation maximization approach, the method based on Bayesian learning, and the method based on generative neural networks. The methods based on a combination of episodic memory (several stored samples) and semantic memory (sampling of posterior probability function) show superiority compared to other recent methods devoted to CI prevention. Based on generative approaches, the biologically plausible mechanisms of active forgetting and memory reconsolidation are also demonstrated. The proof of concept experiments were performed on several publicly available datasets. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 4	2020	400						73	85		10.1016/j.neucom.2020.03.024													
J								Attention-based face alignment: A solution to speed/accuracy trade-off	NEUROCOMPUTING										Face alignment; Deep learning; Attention mechanism; Speed accuracy trade-off		Accuracy and speed are two important aspects of face alignment. One of the main concerns is the tradeoff between speed and accuracy. To this end, we propose a novel attention-based deep learning framework consisting of a feature extraction module and an attention module. The feature extraction module is based on a two-branch architecture with a trunk branch and a mask branch. The trunk branch extracts shallow features, while mask branch is intermediately supervised to generate coarse masks with rough geometric information. An attention module is designed to select relevant features from trunk features under the guidance of geometric information from the coarse masks. The above attention module helps increase robustness of our framework to unconstrained faces while reducing complexity of our deep network. Extensive experiments are conducted on two public benchmark datasets, 300-W and AFLW. Ablation studies indicate its superior performance on challenging faces. Comparing with the state of the art, our method provides a competent trade-off between speed and accuracy. Our approach achieves 3.35% mean error with 0.29% failure rate on 300-W fullset, and 1.44% mean error with 0.40% failure rate on AFLW-Full datasets. Meanwhile, our method is capable of processing images at real-time speed (13ms) on 300-W with 68 landmarks. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 4	2020	400						86	96		10.1016/j.neucom.2020.03.023													
J								Adaptive kernel sparse representation based on multiple feature learning for hyperspectral image classification	NEUROCOMPUTING										Hyperspectral image classification; Kernel sparse representation; Multiple kernel learning; Multiple feature extraction	SIGNAL RECOVERY; PURSUIT	For hyperspectral image classification, this paper proposes a novel adaptive kernel sparse representation method based on multiple feature learning (AKSR-MFL). Firstly, multiple types of feature, including different kinds of spectral and spatial information, are extracted from the original HSI to describe the characteristics of pixels from different perspectives, which is beneficial to enhance the classification accuracy significantly. To further explore contextual information and conform the spatial structure as far as possible, we employ shape-adaptive algorithm to construct a shape-adaptive region for each test pixel at the same time. Then, we design adaptive kernel sparse representation (AKSR) method by applying kernel joint sparse pattern to address the linearly inseparable problem of classification in multiple feature space and make the pixels with the same distribution more easily grouped and linearly separable. Moreover, composite kernel constructed by multiple kernel learning (MKL) is embedded into AKSR to effectively construct base kernels for different feature descriptors and determine the weights of base kernels optimally, which can take the similarity and diversity of different types of feature descriptor into full consideration. Experimental results on three widely used real HSI data demonstrate that the proposed AKSR-MFL classifier outperforms several state-of-the-art classification methods. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 4	2020	400						97	112		10.1016/j.neucom.2020.03.022													
J								Visualising basins of attraction for the cross-entropy and the squared error neural network loss functions	NEUROCOMPUTING										Fitness landscape analysis; Neural networks; Cross-entropy; Squared error; Local minima; Loss functions	LOCAL MINIMA	Quantification of the stationary points and the associated basins of attraction of neural network loss surfaces is an important step towards a better understanding of neural network loss surfaces at large. This work proposes a novel method to visualise basins of attraction together with the associated stationary points via gradient-based stochastic sampling. The proposed technique is used to perform an empirical study of the loss surfaces generated by two different error metrics: quadratic loss and entropic loss. The empirical observations confirm the theoretical hypothesis regarding the nature of neural network attraction basins. Entropic loss is shown to exhibit stronger gradients and fewer stationary points than quadratic loss, indicating that entropic loss has a more searchable landscape. Quadratic loss is shown to be more resilient to overfitting than entropic loss. Both losses are shown to exhibit local minima, but the number of local minima is shown to decrease with an increase in dimensionality. Thus, the proposed visualisation technique successfully captures the local minima properties exhibited by the neural network loss surfaces, and can be used for the purpose of fitness landscape analysis of neural networks. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 4	2020	400						113	136		10.1016/j.neucom.2020.02.113													
J								Deep plug-and-play prior for low-rank tensor completion	NEUROCOMPUTING										Tensor completion; Tensor nuclear norm; Denoising neural network; Alternating direction method of multipliers; Plug-and-play framework	IMAGE-RESTORATION; MATRIX FACTORIZATION; NEURAL-NETWORKS; RECOVERY; SPARSE	Multi-dimensional images, such as color images and multi-spectral images (MSIs), are highly correlated and contain abundant spatial and spectral information. However, real-world multi-dimensional images are usually corrupted by missing entries. By integrating deterministic low-rankness prior to the data-driven deep prior, we suggest a novel regularized tensor completion model for multi-dimensional image completion. In the objective function, we adopt the newly emerged tensor nuclear norm (TNN) to characterize the global low-rankness prior of multi-dimensional images. We also formulate an implicit regularizer by plugging a denoising neural network (termed as deep denoiser), which is convinced to express the deep image prior learned from a large number of natural images. The resulting model can be solved by the alternating directional method of multipliers algorithm under the plug-and-play (PnP) framework. Experimental results on color images, videos, and MSIs demonstrate that the proposed method can recover both the global structure and fine details very well and achieve superior performance over competing methods in terms of quality metrics and visual effects. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 4	2020	400						137	149		10.1016/j.neucom.2020.03.018													
J								A three-stage method for batch-based incremental nonnegative matrix factorization	NEUROCOMPUTING										Non-negative matrix factorization; RMSE; 3S-INMF; Balance coefficient	ALGORITHMS; SYSTEMS	The main issue in incremental nonnegative matrix factorization (INMF) is how to update base matrix and coefficient matrix. The re-training scheme(RT-NMF) and the scheme proposed by Bucak and Gunsel(BG-INMF) are two common methods. However, both of them have problems in balancing root mean square error(RMSE) and time cost when incremental samples appear in a batch form. In this paper, a three-stage method(3S-INMF) is proposed to derive a good balance between RMSE and time cost. In the first stage, only the coefficient matrix of incremental samples is updated while the base matrix and the coefficient matrix of old samples are fixed. If the RMSE does not meet the required precision after this stage, the second stage, i.e. BG-INMF, is carried out. In the second stage, the base matrix and the coefficient matrix of incremental samples are updated alternatively while the coefficient matrix of old samples is fixed. If the RMSE still does not meet with the required precision after BG-INMF, the coefficient matrix of old samples will be updated in the third stage while the base matrix and the coefficient matrix of incremental samples are fixed. In the three consecutive stages, the initial values of base matrix and coefficient matrix in each stage are the corresponding output values in the previous stage. In addition, extensive experiments on the three popular datasets show that 3S-INMF obtains the best balance between RMSE and time cost compared with RT-NMF and BG-INMF. Furthermore, the 3S-INMF is extended to graph nonnegative matrix factorization(GNMF) and kernel nonnegative matrix factorization(KNMF), which also has a superior performance examined by further experiments. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 4	2020	400						150	160		10.1016/j.neucom.2020.03.017													
J								A(2): Extracting cyclic switchings from DOB-nets for rejecting excessive disturbances	NEUROCOMPUTING										Reinforcement learning; Finite-state machine; Disturbance rejection; Multiple POMDPs; Hybrid system	FINITE-STATE AUTOMATA; HYBRID CONTROL; SYSTEMS SUBJECT; MOTION CONTROL; MODEL; OBSERVER; FRAMEWORK	Reinforcement Learning (RL) is limited in practice by its poor explainability, which is responsible for insufficient trustiness from users, unsatisfied interpretation for human intervention, inadequate analysis for future improvement, etc. This paper seeks to partially characterize the interplay between dynamical environments and a previously-proposed Disturbance OBserver net (DOB-net). The DOB-net is trained via RL and offers optimal control for a set of Partially Observable Markovian Decision Processes (POMDPs). The transition function of each POMDP is largely determined by the environments (excessive external disturbances). This paper proposes an Attention-based Abstraction (A(2)) approach to extract a finite-state automaton, referred to as a Key Moore Machine Network (KMMN), to capture the switching mechanisms exhibited by the DOB-net in dealing with multiple such POMDPs. A(2) first quantizes the controlled platform by learning continuous-discrete interfaces. Then it extracts the KMMN by finding the key hidden states and transitions that attract sufficient attention from the DOB-net. Within the resultant KMMN, three patterns of cyclic switchings (between key hidden states) are found, and saturated controls are shown synchronized with unknown disturbances. Interestingly, the found switchings have previously appeared in the control design for often-saturated systems. They are interpreted via an analogy to the discrete-event subsystem of hybrid control. (C) 2020 Published by Elsevier B.V.																	0925-2312	1872-8286				AUG 4	2020	400						161	172		10.1016/j.neucom.2020.03.014													
J								Intuitionistic fuzzy grey cognitive maps for forecasting interval-valued time series	NEUROCOMPUTING										Fuzzy cognitive maps; Forecasting; Interval-valued time series	NEURAL-NETWORK; PREDICTION; MODELS; NEWS	In many real-world forecasting problems, the time series under investigation can be approximated. In that case, instead of dealing with its exact values, only their minima and maxima achieved in the predefined periods are considered. Such an approximation forms interval-valued time series (ITS). To forecast ITS, we propose a new method that relies on fuzzy cognitive maps (FCMs). We adapt standard FCMs to the forecasting of ITS using interval-valued intuitionistic fuzzy sets. In this way, we develop a forecasting model called the Intuitionistic Fuzzy Grey Cognitive Map (IFGCM). We validate our IFGCM using publicly available stock market data for 10 indexes for which the estimation of potential investment losses (minima) and gains (maxima) is crucial. The results of these experiments prove the high efficiency of the IFGCM, especially compared with state-of-the-art models. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 4	2020	400						173	185		10.1016/j.neucom.2020.03.013													
J								Further research on exponential stability for quaternion-valued neural networks with mixed delays	NEUROCOMPUTING										Quaternion-valued neural networks; Vector Lyapunov function method; M matrix theory; Global exponential stability; Mixed delays; Parameter uncertainties	TIME-VARYING DELAYS; ROBUST STABILITY; LEAKAGE DELAY; SYNCHRONIZATION; DISCRETE; DISSIPATIVITY; CONVERGENCE	This paper addresses the global exponential stability of a class of quaternion-valued neural networks (QVNNs) with mixed delays including time-varying delays and infinite distributed delays. Because of the noncommutativity of quaternion multiplication, the concerned quaternion-valued models separated into four real-valued parts to form the equivalent real-valued systems. Based on M-matrix properties and homomorphism mapping theories, some sufficient conditions are derived to guarantee the existence and uniqueness of the equilibrium point of the system. Conditions for ensuring the global exponential stability of the equilibrium point of the system are obtained on the basis of the vector Lyapunov function method instead of the linear matrix inequality method. Using a similar method, the mixed-delay QVNNs with parameter uncertainties are also studied, and the conditions for ensuring the global robust exponential stability of the system are established directly. The adopted approach and the obtained results in this paper complement already the existing ones. Finally, three numerical examples are provided to illustrate the feasibility and the less level conservatism of the main results. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 4	2020	400						186	205		10.1016/j.neucom.2020.03.004													
J								Recycling weak labels for multiclass classification	NEUROCOMPUTING										Classification; Weak label; Loss function; Cost-sensitive learning		This paper explores the mechanisms to efficiently combine annotations of different quality for multiclass classification datasets, as we argue that it is easier to obtain large collections of weak labels as opposed to true labels. Since labels come from different sources, their annotations may have different degrees of reliability (e.g., noisy labels, supersets of labels, complementary labels or annotations performed by domain experts), and we must make sure that the addition of potentially inaccurate labels does not degrade the performance achieved when using only true labels. For this reason, we consider each group of annotations as being weakly supervised and pose the problem as finding the optimal combination of such collections. We propose an efficient algorithm based on expectation-maximization and show its performance in both synthetic and real-world classification tasks in a variety of weak label scenarios. (C) 2020 The Authors. Published by Elsevier B.V.																	0925-2312	1872-8286				AUG 4	2020	400						206	215		10.1016/j.neucom.2020.03.002													
J								L-1-norm low-rank linear approximation for accelerating deep neural networks	NEUROCOMPUTING										DCNNs acceleration; Low-rank approximation; Augmented Lagrange function	ALGORITHM	In this paper, we develop a L-1-norm based low-rank matrix approximation method to decompose large high-complexity convolution layers into a set of low-complexity convolution layers with low-ranks to accelerate deep neural networks. Based on the alternating direction method (ADM), we derive a mathematical solution for this new L-1-norm based low-rank decomposition problem. Our experimental results on public datasets, including CIFAR-10 and ImageNet, demonstrate that this new decomposition scheme outperforms the recently developed L-2-norm based nonlinear decomposition method, which achieved the state-of-the-art performance. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 4	2020	400						216	226		10.1016/j.neucom.2020.01.113													
J								Exploiting geographical-temporal awareness attention for next point-of-interest recommendation	NEUROCOMPUTING										Location-based social network; POI recommendation; Geographical-temporal awareness; Attention mechanism	MODEL	With the prosperity of the location-based social networks, next point-of-interest (POI) recommendation has become an increasingly significant requirement since it can benefit both users and business. Obtaining insight into user mobility for the next POI recommendations is a vital yet challenging task. Existing approaches to understanding user mobility mainly gloss over the check-in sequence, making it fail to explicitly capture the subtle POI-POI interactions across the entire user check-in history and distinguish relevant check-ins from the irrelevant. In this paper, we proposed a novel recommendation approach, namely geographical-temporal awareness hierarchical attention network (GT-HAN) to resolve those issues. We first establish a geographical-temporal attention network to simultaneously uncover the overall sequence dependence and the subtle POI-POI relationships. Then, a context-specific co-attention network was designed to learn to change user preferences by adaptively selecting relevant check-in activities from check-in histories, which enabled GT-HAN to distinguish degrees of user preference for different checkins. Finally, we make a POI recommendation using a conditional probability distribution function. Experimental results on real world datasets (obtained from Foursquare and Gowalla) show that the GT-HAN model significantly outperforms current state-of-the-art approaches, and demonstrating the benefits produced by new technologies incorporated into GT-HAN. (C) 2020 Published by Elsevier B.V.																	0925-2312	1872-8286				AUG 4	2020	400						227	237		10.1016/j.neucom.2019.12.122													
J								Dynamic gesture recognition by using CNNs and star RGB: A temporal information condensation	NEUROCOMPUTING										Dynamic gesture recognition; Convolutional neural network; Temporal information representation	NEURAL-NETWORKS	Due to technological advances, machines are increasingly present in people's daily lives. Thus, there has been more and more effort to develop interfaces that provide an intuitive way of interaction, such as dynamic gestures. Currently, the most common trend is to use multimodal data, as depth and skeleton information, to enable dynamic gesture recognition. However, it would be more interesting if only color information was used, since RGB cameras are usually available in almost every public place, and could be used for gesture recognition without the need of installing additional equipment. The main problem with such approach is the difficulty of representing spatio-temporal information using just color. With this in mind, we propose a technique capable of condensing a dynamic gesture, shown in a video, in just one RGB image. We call this technique star RGB. This image is then passed to a classifier formed by two Resnet CNNs, a soft-attention ensemble, and a fully connected layer, which indicates the class of the gesture present in the input video. Experiments were carried out using Montalbano, GRIT, and isoGD datasets. For Montalbano dataset, the proposed approach achieved an accuracy of 94.58%. Such result reaches the state-of-the-art when considering this dataset and only color information. For GRIT dataset, our proposal achieves more than 98% of accuracy, recall, precision, and F1-score, outperforming the authors' approach by more than 6%. Regarding the large scale isoGD dataset, the proposal achieved 52.18% of accuracy. However, taking into account the complexity of the dataset (eight different gestures categories) and the amount of classes (249), we consider that our approach is competitive with previous ones, since we employed only color information to recognize gestures instead of all the multimodal data available, usually used by other methods. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 4	2020	400						238	254		10.1016/j.neucom.2020.03.038													
J								Self-constraining and attention-based hashing network for bit-scalable cross-modal retrieval	NEUROCOMPUTING										Deep cross-modal hashing; Attention mechanism; Memory bank mechanism; Bit-scalable hashing	DEEP; ALGORITHM	Recently deep cross-modal hashing (CMH) have received increased attention in multimedia information retrieval, as it is able to combine the benefit from the low storage cost and search efficiency of hashing, and the strong capabilities of feature abstraction by deep neural networks. CMH can effectively integrates hash representation learning and hash function optimization into an end-to-end framework. However, most of existing deep cross-modal hashing methods use a one-size-fits-all high level representation resulting in the loss of the spatial information of data. Also, previous methods mostly generated fixed length hashing codes. Here, the significance level of different bits are equally weighted thereby restricting their practical flexibility. To address these issues, we propose a self-constraining and attention-based hashing network (SCAHN) for bit-scalable cross-modal hashing. SCAHN integrates the label constraints from early and late-stages as well as their fused features into the hash representation and hash function learning. Moreover, as the fusion of early and late-stages features is based on an attention mechanism, each bit of the hashing codes can be unequally weighted so that the code lengths can be manipulated by ranking the significance of each bit without extra hash-model training. Extensive experiments conducted on four benchmark datasets demonstrate that our proposed SCAHN outperforms the current state-of-the-art CMH methods. Moreover, it is also shown that the generated bit-scalable hashing codes well-preserve the discriminative power with varying code lengths and obtain competitive results comparing to the state-of-the-art. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 4	2020	400						255	271		10.1016/j.neucom.2020.03.019													
J								Tracking control of redundant mobile manipulator: An RNN based metaheuristic approach	NEUROCOMPUTING										Mobile-manipulator; Tracking control; Metaheuristic optimization; Recurrent neural network; Nature-inspired algorithm; Redundancy resolution	NEURAL-NETWORK CONTROL; PARAMETER-ESTIMATION; ROBOT MANIPULATORS; OPTIMIZATION; RESOLUTION; SYSTEMS; SCHEME	In this paper, we propose a topology of Recurrent Neural Network (RNN) based on a metaheuristic optimization algorithm for the tracking control of mobile-manipulator while enforcing nonholonomic constraints. Traditional approaches for tracking control of mobile robots usually require the computation of Jacobian-inverse or linearization of its mathematical model. The proposed algorithm uses a nature-inspired optimization approach to directly solve the nonlinear optimization problem without any further transformation. First, we formulate the tracking control as a constrained optimization problem. The optimization problem is formulated on position-level to avoid the computationally expensive Jacobian-inversion. The nonholonomic limitation is ensured by adding equality constraints to the formulated optimization problem. We then present the Beetle Antennae Olfactory Recurrent Neural Network (BAORNN) algorithm to solve the optimization problem efficiently using very few mathematical operations. We present a theoretical analysis of the proposed algorithm and show that its computational cost is linear with respect to the degree of freedoms (DOFs), i.e., O(m). Additionally, we also prove its stability and convergence. Extensive simulation results are prepared using a simulated model of IIWA14, a 7-DOF industrial-manipulator, mounted on a differentially driven cart. Comparison results with particle swarm optimization (PSO) algorithm are also presented to prove the accuracy and numerical efficiency of the proposed controller. The results demonstrate that the proposed algorithm is several times (around 75 in the worst case) faster in execution as compared to PSO, and suitable for real-time implementation. The tracking results for three different trajectories; circular, rectangular, and rhodonea paths are presented. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 4	2020	400						272	284		10.1016/j.neucom.2020.02.109													
J								Creating functionally favorable neural dynamics by maximizing information capacity	NEUROCOMPUTING										Information capacity; Neural dynamics; Empowerment	EMPOWERMENT	A ubiquitous problem in optimization and machine learning pertains to the design of systems that enact a desired behavior in dynamical environments. For example, the classical example of a control system that stabilizes an inverted pendulum. In this paper, we consider a complementary and less well-studied problem: the design of the environment itself. That is, can we create a dynamical system that in a general but mathematically rigorous way, is readily 'usable' by an unknown agent. We are especially interested in the synthesis of neuronal dynamics that are maximally labile with respect to afferent inputs. That is, can we create neural dynamics that propagate information well. To do so, we blend ideas from control and information theories, by turning specifically to the notion of empowerment, or the information capacity of a dynamical system in an input-to-state sense. We devise a strategy to optimize the dynamics of a system using empowerment over its state space as an objective function. This results in dynamics that are generically conducive to information propagation. For example, the optimized environment would be expected to perform well as an encoder (of afferent input distributions). We outline the key technical innovations needed in order to perform the optimization and, by means of example, discuss emergent dynamical characteristics of systems optimized according to this principle. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 4	2020	400						285	293		10.1016/j.neucom.2020.03.008													
J								Single image dehazing by approximating and eliminating the additional airlight component	NEUROCOMPUTING										Image dehazing; Transmission estimation; Naive bayes classification; Nearest neighbor regularization; Airlight	HAZE-RELEVANT FEATURES; VISIBILITY	This paper proposes a novel technique for single image dehazing using adaptive nearest neighbor regularization to obtain a haze-free transmission map and then approximating the additional airlight component present in the hazy image. The proposed method relies on the intensity distribution in the small image patches of the image, exhibited from the Y channel of the YCbCr representation of the image, in order to preserve the texture information of the image. We substitute the commonly used soft matting technique in assessing the refined transmission map for haze removal, by adaptive nearest neighbor classifier. We assume that the actual color of the haze-free pixels in the image is approximated by a set of discrete colors. We discover the haze-free pixels using the Nearest-Neighbor (NN) regularization. Finally, unlike the state-of-the-art methods, we approximate the additional airlight present in the image patch and eliminate that to clear the haze, instead of estimating the transmission of the medium. The proposed nearest neighbor regularization technique automatically changes the patch size, which helps in dealing with the high depth region (e.g., sky region) of the image. We experimented on standard synthetic and real hazy image datasets and observed that, the proposed method outperforms the state-of-the-art, especially for images with sky regions. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 4	2020	400						294	308		10.1016/j.neucom.2020.03.027													
J								Simulation, visualization and analysis tools for pattern recognition assessment with spiking neuronal networks	NEUROCOMPUTING										Spiking neuronal networks; Visualization tools; Spike-timing-dependent plasticity; Pattern detection; Simulation	TIMING-DEPENDENT PLASTICITY	Computational modeling is becoming a widely used methodology in modern neuroscience. However, as the complexity of the phenomena under study increases, the analysis of the results emerging from the simulations concomitantly becomes more challenging. In particular, the configuration and validation of brain circuits involving learning often require the processing of large amounts of action potentials and their comparison to the stimulation being presented to the input of the system. In this study we present a systematic work-flow for the configuration of spiking-neuronal-network-based learning systems including evolutionary algorithms for information transmission optimization, advanced visualization tools for the validation of the best suitable configuration and customized scripts for final quantitative evaluation of the learning capabilities. By integrating both grouped action potential information and stimulation-related events, the proposed visualization framework provides qualitatively assessment of the evolution of the learning process in the simulation under study. The proposed work-flow has been used to study how receptive fields emerge in a network of inhibitory interneurons with excitatory and inhibitory spike-timing dependent plasticity when it is exposed to repetitive and partially overlapped stimulation patterns. According to our results, the output population reliably detected the presence of the stimulation patterns, even when the fan-in ratio of the interneurons was considerably restricted. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 4	2020	400						309	321		10.1016/j.neucom.2020.02.114													
J								Enhancing the feature representation of multi-modal MRI data by combining multi-view information for MCI classification	NEUROCOMPUTING										Multi-modal MRI data; Multi-view information; Multi-task feature selection; Multi-kernel learning; MCI classification	MILD COGNITIVE IMPAIRMENT; FUNCTIONAL CONNECTIVITY; QUANTITATIVE-ANALYSIS; NETWORK; DISEASE; DIAGNOSIS; REGRESSION; SELECTION; SUPPORT	The classification of mild cognitive impairment (MCI), which is a early stage of Alzheimer's disease and is associated with brain structural and functional changes, is still a challenging task. Recent studies have shown great promise for improving the performance of MCI classification by combining multiple structural and functional features, such as grey matter volume and clustering coefficient. However, extracting which features and how to combine multiple features to improve the performance of MCI classification have always been challenging problems. To address these problems, in this study we propose a new method to enhance the feature representation of multi-modal MRI data by combining multi-view information to improve the performance of MCI classification. Firstly, we extract two structural features (including grey matter volume and cortical thickness) and two functional features (including clustering coefficient and shortest path length) of each cortical brain region based on automated anatomical labeling (AAL) atlas from both T1w MRI and rs-fMRI data of each subject. Then, in order to obtain features that are more helpful in distinguishing MCI subjects, an improved multi-task feature selection method, namely MTES-gLASSO-TTR, is proposed. Finally, a multi-kernel learning algorithm is adopted to combine multiple features to perform the MCI classification task. Our proposed MCI classification method is evaluated on 315 subjects (including 105 LMCI subjects, 105 EMCI subjects and 105 NCs) with both T1w MRI and rs-fMRI data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database. Experimental results show that our proposed method achieves an accuracy of 88.5% and an area under the receiver operating characteristic (ROC) curve (AUC) of 0.897 for LMCl/NC classification, an accuracy of 82.7% and an AUC of 0.832 for EMCl/NC classification, and an accuracy of 79.6% and an AUC of 0.803 for LMCI/EMCI classification, respectively. In addition, by comparison, the accuracy and AUC values of our proposed method are better than those of some existing state-of-the-art methods in MCI classification. Overall, our proposed MCI classification method is effective and promising for automatic diagnosis of MCI in clinical practice. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 4	2020	400						322	332		10.1016/j.neucom.2020.03.006													
J								Sparse semantic map building and relocalization for UGV using 3D point clouds in outdoor environments	NEUROCOMPUTING										Relocalization; Semantic map; 3d point clouds; Outdoor environment; Unmanned Ground Vehicles	SIMULTANEOUS LOCALIZATION; SLAM; ROBUST; EFFICIENT	In this paper, we proposed a sparse semantic map building method and an outdoor relocalization strategy based on this map. Most existing semantic mapping approaches focus on improving semantic understanding of single frames and retain a large amount of environmental data. Instead, we don't want to locate the UGV precisely, but use the imprecise environmental information to determine the general position of UGV in a large-scale environment like human beings. For this purpose, we divide the environment into environment nodes according to the result of scene understanding. The semantic map of the outdoor environment is obtained by generating topological relations between the environment nodes. In the semantic map, only the information of the nodes is saved, so that the storage space can be kept at a very small level with the increasing size of environment. When the UGV receives a new local semantic map, we evaluate the similarity between local map and global map to determine the possible position of the UGV according to the categories of the left and right nodes and the distance between the current position and the nodes. In order to validate the proposed approach, experiments have been conducted in a large-scale outdoor environment with a real UGV. Depending on the semantic map, the UGV can redefine its position from different starting points. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 4	2020	400						333	342		10.1016/j.neucom.2020.02.103													
J								Semantic head enhanced pedestrian detection in a crowd	NEUROCOMPUTING										Pedestrian detection; Occlusion; Semantic head; Head-Body alignment		Pedestrian detection in a crowd is a challenging task because of intra-class occlusion. More prior information is needed for the detector to be robust against it. Human head area is naturally a strong cue because of its stable appearance, visibility and relative location to body. Inspired by it, we adopt an extra branch to conduct semantic head detection in parallel with traditional body branch. Instead of manually labeling the head regions, we use weak annotations inferred directly from body boxes, which are named as 'semantic head'. In this way, the head detection is formulated into using a special part of labeled box to detect the corresponding part of human body, which surprisingly improves the performance and robustness to occlusion. Moreover, the head-body alignment structure is explicitly explored by introducing Alignment Loss, which functions in a self-supervised manner. Based on these, we propose the head-body alignment net (HBAN) in this work, which aims to enhance pedestrian detection by utilizing the human head prior. Comprehensive evaluations are conducted to demonstrate the effectiveness of HBAN on CityPersons dataset. HBAN achieves about 5% increase for heavily occluded pedestrians over baseline and reaches the state-of-the-art performance. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 4	2020	400						343	351		10.1016/j.neucom.2020.03.037													
J								A systematic density-based clustering method using anchor points	NEUROCOMPUTING										Density based clustering; Anchor data points; Local density analysis	FAST SEARCH; PEAKS; FIND	Clustering is an important unsupervised learning method in machine learning and data mining. Many existing clustering methods may still face the challenge in self-identifying clusters with varying shapes, sizes and densities. To devise a more generic clustering method that considers all the aforementioned properties of the natural clusters, we propose a novel clustering algorithm named Anchor Points based Clustering (APC). The anchor points in APC are characterized by having a relatively large distance from data points with higher densities. We take anchor points as centers to obtain intermediate clusters, which can divide the whole dataset more appropriately so as to better facilitate further grouping. In essence, based on the analysis of the identified anchor points, the relationship among the corresponding intermediate clusters can be better revealed. In short, the difference in local densities (densities within neighboring data points) of the anchor points characterizes their different properties, that is to say, all the intermediate clusters may fall into one or multiple identified levels with different densities. Finally, based on the properties of anchor points, APC spontaneously chooses the appropriate clustering strategies and reports the final clustering results. To evaluate the performances of APC, we conduct experiments on twelve two-dimensional synthetic datasets and twelve multi-dimensional real-world datasets. Moreover, we also apply APC to the Olivetti Face dataset to further assess its effectiveness in terms of face recognition. All experimental results indicate that APC outperforms four classical methods and two state-of-the-art methods in most cases. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 4	2020	400						352	370		10.1016/j.neucom.2020.02.119													
J								Fixed-time synchronization control for a class of nonlinear coupled Cohen-Grossberg neural networks from synchronization dynamics viewpoint	NEUROCOMPUTING										Synchronization dynamics; Cohen-Grossberg neural networks; Nonlinear coupling; Fixed-time synchronization; Time-varying delay	GLOBAL EXPONENTIAL STABILITY; FINITE-TIME; PINNING SYNCHRONIZATION; COMPLEX NETWORKS; VARYING DELAYS; PASSIVITY; STABILIZATION; SYSTEMS; BOUNDEDNESS	This paper is mainly concerned with fixed-time synchronization control for a class of Cohen-Grossberg neural networks (CGNNs) with nonlinear coupling and time-varying delay based on synchronization dynamics. First, by using a derived sufficient condition of fixed-time synchronization (FTS) for nonlinear coupled CGNNs (NCCGNNs) with time-varying delay, a novel fixed-time synchronization controller with time-varying delay is designed. Second, by combining the synchronization dynamic viewpoint and the obtained fixed-time synchronization control rule, several useful fixed-time synchronization controllers (FTSCs) are derived. Third, comparisons with known conclusions are provided to show the innovation and feasibility of the proposed fixed-time synchronization control methods. Finally, two examples are given to verify the effectiveness of the theoretical results. Compared with related works, the advantages of this paper are as follows: (1) The designed novel FTSCs can scientifically and efficiently process the time-varying delay of the addressed coupled CGNNs; (2) The derived settling time of the FTS built on FTSCs can reflect the effect of the proposed system's parameters on the synchronization convergence time. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 4	2020	400						371	380		10.1016/j.neucom.2020.02.111													
J								Adaptive learning-based finite-time performance of nonlinear switched systems with quantization behaviors and unmodeled dynamics	NEUROCOMPUTING										Adaptive learning; Finite-time performance; Nonlinear switched systems; Quantization; Unmodeled dynamics	COOPERATIVE CONTROL; LINEAR-SYSTEMS; STABILIZATION; STABILITY; CONSENSUS	This paper proposes a novel finite-time adaptive learning strategy for a class of nonlinear switched systems with quantization behaviors and unmodeled dynamics under unpredictable switchings. The neural network learning framework is introduced to manifest the characteristics of nonlinear systems to obtain better performances. The system nonlinearities include more complicated non-strict feedback structure and full-state dependent unmodeled dynamics. In virtue of the nonlinear decomposition technique and the finite-time criterion, an adaptive learning strategy is developed step by step. Under the proposed strategy, a common Lyapunov function for all the subsystems is constructed to guarantee that in finite time all the signals of the switched system converge to a small domain near the origin under arbitrary switchings. An illustrative example is finally given to verify the effectiveness of the proposed main results. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 4	2020	400						384	392		10.1016/j.neucom.2019.03.096													
J								Adaptive neural containment seeking of stochastic nonlinear strict-feedback multi-agent systems	NEUROCOMPUTING										Containment seeking; Stochastic nonlinear multi-agent system; Backstepping; Graph theory; Neural networks	TIME-DELAY SYSTEMS; DYNAMIC SURFACE CONTROL; TRACKING-CONTROL; COOPERATIVE CONTROL; NETWORK CONTROL; STABILIZATION; CONSENSUS	The paper addresses a distributed containment issue for a stochastic nonlinear strict-feedback multi-agent system. In combination with the graph theory and neural networks technique, an adaptive neural containment protocol is developed within the traditional backstepping framework. Such a containment protocol allows one to extend some existing results to the case that multiple followers are confined to stochastic nonlinear dynamics. Furthermore, with the help of the quartic Lyapunov function, it is proved that all the signals in the resulting closed-loop system remain bounded in probability and the containment error converges to a small neighborhood of the origin in the sense of mean square by a suitable choice of parameters. Finally, an illustrative example is given to demonstrate the effectiveness of the proposed protocol. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 4	2020	400						393	400		10.1016/j.neucom.2019.03.091													
J								Distributed H-infinity-consensus filtering for target state tracking over a wireless filter network with switching topology, channel fading and packet dropouts	NEUROCOMPUTING										Distributed H-infinity-consensus estimation; Target state tracking; Wireless filter network; Switching network topology; Packet dropouts; Channel fading	DATA SYNCHRONIZATION CONTROL; MISSING MEASUREMENTS; NEURAL-NETWORKS; KALMAN FILTER; STABILITY-CRITERIA; SENSOR NETWORKS; DELAYS; LOGIC	This paper is concerned with the problem of distributed H-infinity-consensus filtering for target state tracking over a wireless filter network. The wireless filter network consists of a large number of filter nodes. The network topology is supposed to be switching or changeable over time. Data communication between filter nodes is subject to some network-induced constraints including packet dropouts and channel fading. Different from some existing results, packet dropouts and channel fading are assumed to occur not only in the measurement process for sensors but also in the transmission process among filter nodes. Within such settings, a distributed H-infinity-consensus filtering method is developed to ensure both the estimation accuracy of filters and the robustness of the filtering error system against network-induced constraints. Criteria on designing desired distributed H(infinity )consensus-based filter are derived in terms of a set of linear matrix inequalities. Numerical examples are given to verify the effectiveness of the proposed target state tracking filtering algorithm. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 4	2020	400						401	411		10.1016/j.neucom.2019.04.081													
J								Distributed recursive filtering for discrete time-delayed stochastic nonlinear systems based on fuzzy rules	NEUROCOMPUTING										Stochastic nonlinear systems; Sensor networks; Distributed recursive filtering; Takagi-Sugeno fuzzy model; Time-delays	CONSENSUS FILTER; SENSOR NETWORKS; STABILITY; STATE	The distributed recursive filtering problem is investigated in this paper for discrete time-delayed nonlinear stochastic systems, where the well-known Takagi-Sugeno (T-S) fuzzy model is used to approximate the nonlinearities. According to obtain the system dynamics, a novel structure of distributed filters is developed, where the difference of estimated states from neighboring sensors is exploited to improve the one-step prediction, and the desired estimation is obtained by fusing the estimation under different rules. Attention is focused on the design of a distributed recursive filter such that, in the presence of time-delays and defuzzifying operations, an upper bound of the filtering error covariance is obtained and then minimized by properly designing filter parameters via elaborate mathematical analysis. With the exception of the desired gains with the online recursive form are dependent on the solutions of two Riccati-type difference equations, and the upper bound is further optimized via the introduced parameters. As a final point, a simulation examples is exploited to show the applicability of the developed filtering scheme. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 4	2020	400						412	419		10.1016/j.neucom.2019.04.083													
J								Distributed event-triggered scheduling in networked interconnected systems with sparse connections	NEUROCOMPUTING										Networked interconnected systems; Cardinality constraint; Event-triggered transmission; Mixed-integer programming; Sparse structure	NEURAL-NETWORKS; CONTROL DESIGN; STABILITY; CONSENSUS	This paper is concerned with the problem of distributed event-triggered scheduling for a class of interconnected systems with limited coupling connections. Different from the existing studies, in the considered interconnected system, each remote control station is allowed to retrieve event-triggered sampled-data from interacting control stations, while the number of coupling communication connections is upper bounded in a kappa-sparse sense rather than permanently fixed. The distributed event-triggered scheduling problem subject to such a cardinality constraint is nontrivial due to its combinatorial nature. Different from the existing l(1) norm based iterative convexification approach, a direct design approach based on mixed-integer programming is proposed to handle the cardinality constraint. Furthermore, an explicit K-sparse distributed scheduling algorithm is designed for the networked interconnected system via the Lyapunov functional method and some matrix manipulations. The proposed approach is finally applied to a three-machine power system to illustrate its effectiveness. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 4	2020	400						420	428		10.1016/j.neucom.2019.04.080													
J								Resilience analysis and design of event-triggered offshore steel jacket structures	NEUROCOMPUTING										Offshore structure; Active control; Sampled-data; Event-triggered mechanism; Data packet processor	SAMPLED-DATA CONTROL; H-INFINITY; STABILITY-CRITERIA; PLATFORMS SUBJECT; VIBRATION CONTROL; NEURAL-NETWORKS; SYSTEMS; FEEDBACK; FORCES	This paper addresses the analysis and synthesis issues of event-triggered offshore steel jacket structures with an active tuned mass damper mechanism. First, to save communication resources, an event-triggered data packet processor (DPP) is proposed such that the transmissions of the sensor data can be regulated over the communication channel. Second, by virtue of the time-delay system approach, a unified closed-loop system model is presented which can well accommodate the simultaneous presence of network-induced delays, nonlinearity and uncertainty in the offshore structure. To further increase resilience, the control law is allowed to possess gain perturbations. Then, via the Lyapunov functional method, criteria for designing a resilient event-based synthesis strategy are derived. Finally, an illustrative example is given to verify that the developed resilient strategy can guarantee the prescribed performance of the offshore structure while alleviating the frequent occupancy of the scarce communication resources. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 4	2020	400						429	439		10.1016/j.neucom.2019.04.084													
J								A novel set-membership estimation approach for preserving security in networked control systems under deception attacks	NEUROCOMPUTING										Deception attacks; Security; Set-membership estimation; Networked control systems	QUANTIZATION	This paper is concerned with the security problem for a networked control system subject to deception attacks. An adversary is present to corrupt both the sensor-to-controller channel and the controller-toactuator channel with deception signals when sensor and control data are transmitted via the network medium. We model the malicious deception signals as unknown-but-bounded signals in such a way that no priori knowledge of the attack signals is required. First, a delicate set-membership performance index is proposed for meeting the system security requirement. Second, a novel set-membership estimation method is developed to obtain the desired ellipsoidal estimation set which guarantees the enclosing of true state of the system at every instant of time. Such an ellipsoidal estimation set is then used for deriving the sufficient conditions such that the resulting closed-loop system is set-membership input-tostate-stable and achieves the prescribed security requirement. Finally, a target tracking system is given to demonstrate the effectiveness of the proposed method. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 4	2020	400						440	449		10.1016/j.neucom.2019.04.082													
J								Network-based filtering for positive systems with random communication delays and deception attacks	NEUROCOMPUTING										Network-based filtering; Communication delays; Deception attacks; Positive systems; Markov jump linear systems	DATA SYNCHRONIZATION CONTROL; DISCRETE-TIME-SYSTEMS; JUMP LINEAR-SYSTEMS; NEURAL-NETWORKS; INFINITY; PERFORMANCE; STABILITY; OBSERVERS; DESIGN	This paper is concerned with the problem of network-based filtering for a discrete-time positive system subject to random communication delays and deception attacks. The delays and attacks are considered as phenomena occurring randomly via network communication, either passively or actively, and thus they can be suitably characterized by mutually independent Markov stochastic process and Bernoulli random variable. A novel resilient filter of a mode-dependent structure based on delayed and tampered sensor measurement is proposed. With such a mode-dependent filter, the network-based filtering error system is modeled by an augmented Markov jump linear system. Criteria by means of linear programming are then developed to determine the filter gain matrices such that the filtering error system guarantees its resiliency in terms of stochastic stability under a prescribed l(1)-gain while mitigating the simultaneous effects of random communication delays, deception attacks, disturbance, noise, and jumping system parameters. An illustrative example is given to test the feasibility of the conditions and verify the effectiveness of the resilient filter. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 4	2020	400						450	457		10.1016/j.neucom.2019.03.090													
J								Distributed event-triggered consensus of multi-agent systems under periodic DoS jamming attacks	NEUROCOMPUTING										Multi-agent systems (MASs); Distributed event-triggered mechanism; Denial-of-service (DoS) jamming attacks; Switched system	CYBER-PHYSICAL SYSTEMS; SECURE ESTIMATION; RESILIENT CONTROL; NETWORKS; DESIGN	This paper is concerned with distributed event-triggered consensus of a generally linear multi-agent system subjected to periodic denial-of-service (DoS) jamming attacks. First, a novel switched time-varying delay system model is established to describe consensus error dynamics. By virtue of the switched system and time delay system approaches, one constraint is obtained as a tradeoff between exponential convergence rate of consensus error and uniform lower bound of sleep intervals of periodic DoS jamming attacks. Second, sufficient conditions are derived to guarantee exponential consensus of the multi-agent system. Third, a co-design method is presented to determine the parameters of both the distributed event-triggered mechanism and consensus protocol. Furthermore, an algorithm is given to find the allowable uniform lower bound of sleep intervals and suboptimal distributed event-triggering parameters. Finally, an illustrative example is provided to verify the effectiveness of the proposed resilient event-triggering consensus design method against periodic DoS jamming attacks. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 4	2020	400						458	466		10.1016/j.neucom.2019.03.089													
J								Event-triggered generalized dissipative filtering for delayed neural networks under aperiodic DoS jamming attacks	NEUROCOMPUTING										Denial-of-service jamming attacks; Event-triggered scheme; Generalized dissipative; Delayed neural networks; Piecewise Lyapunov functional	TIME-VARYING DISCRETE; STATE ESTIMATION; STABILITY ANALYSIS; SYSTEMS; SYNCHRONIZATION; STABILIZATION	This paper is concerned with event-triggered generalized dissipative filtering for delayed neural networks under aperiodic denial-of-service (DoS) attacks. Note that DoS attacks impede the wireless communications on measurement from time to time. This paper aims at designing resilient filters against DoS attacks to estimate neuronal states in the sense of dissipative. For this goal, a switched filter is introduced to cope with DoS attacks and unavailability of state information. In order to save precious communication resources, an event-triggered communication scheme is devised to transmit only necessary information to the filter. With such setting, the filtering error system is modeled as a switched system with time-delay. By employing the piecewise Lyapunov-Krasovskii functional approach and linear matrix inequality techniques, some criteria on the existence of suitable filters are presented against aperiodic DoS attacks while suitable filtering performance can be ensured. It should be mentioned that since a generalized dissipative performance index is introduced, several kinds of resilient event-based filtering issues, such as H-infinity filtering, passive filtering, mixed H(infinity )and passive filtering, (Q, S, R)-dissipative filtering are solved in a unified framework in the presence of aperiodic DoS attacks. Finally, a numerical example is given to illustrate the effectiveness of the proposed method. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				AUG 4	2020	400						467	479		10.1016/j.neucom.2019.03.088													
J								Simple and effective neural-free soft-cluster embeddings for item cold-start recommendations	DATA MINING AND KNOWLEDGE DISCOVERY										Recommender systems; Item recommendation; Item cold-start problem; Soft-cluster embeddings	MATRIX FACTORIZATION; SYSTEMS	Recommender systems are widely used in online platforms for easy exploration of personalized content. The best available recommendation algorithms are based on using the observed preference information among collaborating entities. A significant challenge in recommender system continues to be item cold-start recommendation: how to effectively recommend items with no observed or past preference information. Here we propose a two-stage algorithm based on soft clustering to provide an efficient solution to this problem. The crux of our approach lies in representing the items as soft-cluster embeddings in the space spanned by the side-information associated with the items. Though many item embedding approaches have been proposed for item cold-start recommendations in the past-and simple as they might appear-to the best of our knowledge, the approach based on soft-cluster embeddings has not been proposed in the research literature. Our experimental results on four benchmark datasets conclusively demonstrate that the proposed algorithm makes accurate recommendations in item cold-start settings compared to the state-of-the-art algorithms according to commonly used ranking metrics like Normalized Discounted Cumulative Gain (NDCG) and Mean Average Precision (MAP). The performance of our proposed algorithm on the MovieLens 20M dataset clearly demonstrates the scalability aspect of our algorithm compared to other popular algorithms. We also propose the metricCold Items Precision(CIP) to quantify the ability of a system to recommend cold-start items. CIP can be used in conjunction with relevance ranking metrics like NDCG and MAP to measure the effectiveness of the cold-start recommendation algorithm.																	1384-5810	1573-756X				SEP	2020	34	5			SI		1560	1588		10.1007/s10618-020-00708-6		AUG 2020											
J								Long short-term memory neural network for glucose prediction	NEURAL COMPUTING & APPLICATIONS										Artificial neural network; Long short-term memory (LSTM); Type 1 diabetes; Times-series forecasting; Glucose prediction	TIME; COMPLICATIONS; INDEX	Diabetes is a chronic disease that affects a high percentage of the world population and produces different and serious complications to patients. Most diabetes complications may be avoided by controlling the blood glucose levels exhaustively. Moreover, a prediction of future glucose levels has shown to be fundamental in helping patients to plan and modify their treatment in real-time. In this paper, a glucose predictor based on long short-term memory neural networks is designed. Three input parameters are fed to the predictor: past glucose levels obtained from a continuous glucose monitoring sensor, the insulin units administered by an insulin pump and the patient's carbohydrates intake. Different prediction times and input dimensions have been evaluated in order to provide the best prediction to patients. Results encourage the use of glucose predictions to avoid the occurrence of hypoglycemias, anticipate correction actions, and to increase the quality of life of these patients.																	0941-0643	1433-3058															10.1007/s00521-020-05248-0		AUG 2020											
J								Nonlinear models based on enhanced Kriging interpolation for prediction of rock joint shear strength	NEURAL COMPUTING & APPLICATIONS										Shear strength of rock joints; Kriging; Logistic function; Predicted models	HYBRID RELIABILITY-ANALYSIS; RESPONSE-SURFACE METHOD; HARMONY SEARCH; BEHAVIOR; CRITERION; FAILURE	One of the most basic topics in rock mechanic is the shear strength criteria for rock joints. Thus, it is of high importance to accurately predict the shear strength of rock joints. In this study, the abilities for accuracy and agreement of Kriging model-based nonlinear interpolation strategy are investigated in terms of predicting the shear strength of rock joints. Totally 84 datasets were used to construct the Kriging models; the datasets were divided into two main parts: training and testing. The prepared database was applied to the training phase in the Kriging model; this way, several nonlinear basic functions were introduced to enhance the predictions of the Kriging model. The examined functions in this paper were linear, 2-order, 3-order, exponential, logarithmic, logistic, hyperbolic tangent, and hyperbolic sine. The sigmoid forms of the basic functions, including logistic and hyperbolic tangent, provide the superior predictions compared to other mathematical functions, while the 2-order and 3-order forms provide the worst performances than the linear, exponential, and logarithmic functions. According to the obtained results, the logistic-based model with coefficient of determination (R-2) of 0.916 was found the optimal model that can be successfully applied to estimating the shear strength of rock joints.																	0941-0643	1433-3058															10.1007/s00521-020-05252-4		AUG 2020											
J								Source localization in resource-constrained sensor networks based on deep learning	NEURAL COMPUTING & APPLICATIONS										Artificial neural network (ANN); Decentralized detection; Deep learning; Error type I; Error type II; Internet of things (IoT); Source localization; Target tracking; Wireless sensor networks (WSN)	DISTRIBUTED DETECTION; DECISION FUSION; ALGORITHMS; HYPOTHESIS; ENERGY	Source localization with a network of low-cost motes with limited processing, memory, and energy resources is considered in this paper. The state-of-the-art methods are mostly based on complicated signal processing approaches in which motes send their (processed) data to a fusion center (FC) wherein the source is localized. These methods are resource-demanding and mostly do not meet the limitations of motes and network. In this paper, we consider distributed detection where each mote performs a binary hypothesis test to detect locally the existence of a desired source and sends its (potentially erroneous) decision to FC during just one bit (1 indicates source existence and 0 otherwise). Hence, both processing and bandwidth constraints are met. We propose to use an artificial neural network (ANN) to correct erroneous local decisions. After error correction, the region affected by the source is specified by nodes with decision 1. Moreover, we propose to localize the source by deep learning in FC which converts the network of decisions 1 and 0 to a black and white image with white pixels in the locations of motes with decision 1. The proposed schemes of error correction by ANN (ECANN) and source localization with deep learning (SoLDeL) were evaluated in a fire detection application. We showed that SoLDeL performs appropriately and scales well into large networks. Moreover, the applicability of ECANN in delineation of farm management zones was illustrated.																	0941-0643	1433-3058															10.1007/s00521-020-05253-3		AUG 2020											
J								Application of the group method of data handling and variable importance analysis for prediction and modelling of saltwater intrusion processes in coastal aquifers	NEURAL COMPUTING & APPLICATIONS										GMDH prediction models; Saltwater intrusion; Coastal aquifers; Variable selection	MULTIPLE-OBJECTIVE MANAGEMENT; SURROGATE MODELS; SIMULATION-OPTIMIZATION; PUMPING OPTIMIZATION; PERFORMANCE; STRATEGIES	Data-driven mathematical models are powerful prediction tools, which are utilized to approximate solution responses obtained using numerical saltwater intrusion simulation models. Employing data-driven prediction models as a replacement of the complex groundwater flow and transport models enables prediction of future scenarios. Most important, it also helps save computational time, effort and requirements when developing optimal coastal aquifer management methodologies using complex and large-scale coupled simulation-optimization models. In this study, a new data-driven mathematical model, namely group method of data handling (GMDH)-based prediction models, is developed and utilized to predict salinity concentration in a coastal aquifer by mimicking the responses of a variable-density flow and solute transport numerical simulation model. For comparison and evaluation purpose, the prediction performances of GMDH models were compared with well-established support vector machine regression and genetic programming based models. In addition, one important characteristic of the GMDH models is explored and evaluated, i.e. the ability to identify a set of most influential input predictor variables (pumping rates) that had the most significant impact on the outcomes (salinity concentration at monitoring locations). To confirm variable importance, 3 tests are conducted in which new GMDH models are constructed using subsets of the original datasets. In TEST 1, new GMDH models are constructed using a set of most influential variables (consisting of pumping rates at selected locations) only. In TEST 2, a subset of 20 variables (10 most and least influential variables) is used to develop new GMDH models. In TEST 3, a subset of the least influential variables is used to develop GMDH models. The performance evaluation results demonstrate that GMDH models developed using the entire dataset had reasonable prediction accuracy and efficiency. The comparison performance evaluation results for the three test scenarios highlighted the importance of the appropriate selection of relevant input pumping rates when developing accurate prediction models. The results suggested that incorporating the least influential variables deteriorate the accuracy of the prediction models; thus, considering the most influential pumping rates it is possible to develop more accurate and efficient salinity prediction models. Overall, the evaluation results from this study establish that the GMDH models and the inherent input variable ranking capability can be utilized as accurate and efficient coastal saltwater intrusion prediction models. Hence, GMDH models are viable saltwater intrusion modelling tools, which can be employed in future regional-scale saltwater intrusion prediction and management investigations.																	0941-0643	1433-3058															10.1007/s00521-020-05232-8		AUG 2020											
J								Image pattern recognition in identification of financial bills risk management	NEURAL COMPUTING & APPLICATIONS										Image pattern recognition; Financial instruments; Authenticity; Financial risk; Feature extraction	CHARACTER	The automatic identification system of financial bills needs to have high recognition rate, high anti-interference and real-time to ensure its recognition effect. Based on the image recognition theory, this study uses the differential projection method to define the boundary of the bill. The horizontal projection gradation of the boundary area of the bill character line is significantly reduced from large to small, and the horizontal difference projection of the grayscale image can be performed to locate the boundary of the bill image. The word height of bills studied in this paper is constant, the character line spacing is equal, and the horizontal differential projection is used to realize row positioning. The main process of the system is to first select the check image, and then the software part realizes the process of image analysis, preprocessing, character segmentation, feature extraction, character recognition, etc. Finally, the recognized amount is output to the interface. It is proved by experiments that the recognition rate of this algorithm is high, which can provide theoretical reference for subsequent related research.																	0941-0643	1433-3058															10.1007/s00521-020-05261-3		AUG 2020											
J								The feasibility of PSO-ANFIS in estimating bearing capacity of strip foundations rested on cohesionless slope	NEURAL COMPUTING & APPLICATIONS										Particle swarm optimization; Fuzzy inference system; FEM simulation; Strip footings; Bearing capacity	ARTIFICIAL NEURAL-NETWORKS; SHALLOW FOUNDATIONS; FEEDFORWARD NETWORKS; OPTIMIZATION; FOOTINGS; PARAMETERS; SOILS; FIELD	This study aimed to assess particle swarm optimization (PSO) optimized with an adaptive network-based fuzzy inference system (ANFIS) for the prediction of the ultimate bearing capacity of strip footing resting on a sloping crest. To make datasets, a 2524 finite element model (FEM) simulation analysis was performed. In this sense, thek-fold validation technique selected for the selection of the testing and validation purposes. Also, the variables of the ANFIS algorithm (i.e., the number of clusters), and the PSO algorithm (the population size were optimized using a series of trial and error process. The second main objective of the current study was to assess the applicability of PSO-ANFIS in estimating the ultimate bearing capacity of the strip footing resting on a single cohesionless slope when it was subjected to an external vertical applied stress. The input parameters were the soil properties (S-type) (classified into five different classes from the weakest, shown asS(1), to the strongest type, displayed in higher order), slope angle (beta), setback distance ratio (b/B) (the distance of the footing from the slope crest to the footing width), and vertical settlement (U-y) of the footing, while the main output was the applied vertical stresses (F-y) that can be applied on the footing. Note that the ultimate bearing capacity (F-ult) is the maximum vertical stress whenU(y) = 0.1 B. A thorough comparison between the measured FEM simulations, the proposed ANFIS and PSO-ANFIS models were carried out to demonstrate their effectiveness in predicting a reliableF(ult)resting on cohesionless slopes.																	0941-0643	1433-3058															10.1007/s00521-020-05231-9		AUG 2020											
J								Innovation of enterprise technology alliance based on BP neural network	NEURAL COMPUTING & APPLICATIONS										BP neural network; Technology alliance; Technological innovation; Enterprise	MODEL	Enterprise technology alliance innovation is the power force of enterprise development. At present, the enterprise science and technology alliance is affected by many factors in the innovation process. It is difficult to grasp the direction of science and technology innovation in an economic market environment, which has a certain impact on the development of enterprises. Based on the BP neural network model, this study uses relationship embedding and structure embedding in network embedding as predetermined variables and inter-organizational knowledge transfer as intermediary variables to study the internal mechanism of technological innovation capabilities of China's high-tech enterprise technology alliances. Furthermore, a corresponding model was constructed for simulation analysis, and a valid factor analysis was performed on the valid data with Lisrel 8.70. The results show that network embedding is the power force of enhancing the technological innovation capability of China's high-tech enterprise technology alliances.																	0941-0643	1433-3058															10.1007/s00521-020-05254-2		AUG 2020											
J								ATP-DenseNet: a hybrid deep learning-based gender identification of handwriting	NEURAL COMPUTING & APPLICATIONS										Gender identification; Handwriting; DenseNet; CBAM; Convolutional neural networks	INDEPENDENT WRITER IDENTIFICATION; DIGITAL FORENSICS; SYSTEM; OPTIMIZATION; EXTRACTION; FEATURES; DATASET	Digital forensics has a vital effect in several domains and mainly focuses on reactive measures, especially when facing digital incidents. Gender identification becomes the important problem in the realm of forensic techniques and handwriting recognition. In this paper, attention-based two-pathway Densely Connected Convolutional Networks (ATP-DenseNet) is proposed to identify the gender of handwriting. There are two pathways in ATP-DenseNet: Feature pyramid could extract hierarchical page feature, and attention-based DenseNet (A-DenseNet) could extract the word feature by fusing Convolutional Block Attention Module (CBAM) and dense connected block. Finally, ATP-DenseNet makes the final prediction combining the two pathways. Experimental results show the efficiency of ATP-DenseNet, and the proposed method performs better than other researches. And the visualization of the feature maps can help us to know which part of the image contributes most to the gender identity.																	0941-0643	1433-3058															10.1007/s00521-020-05237-3		AUG 2020											
J								Evolution of cooperation in malicious social networks with differential privacy mechanisms	NEURAL COMPUTING & APPLICATIONS										Evolution of cooperation; Reinforcement learning; Differential privacy; Social network	TIT-FOR-TAT; STRATEGY; NOISE	Cooperation is an essential behavior in multi-agent systems. Existing mechanisms have two common drawbacks. The first drawback is that malicious agents are not taken into account. Due to the diverse roles in the evolution of cooperation, malicious agents can exist in multi-agent systems, and they can easily degrade the level of cooperation by interfering with agent's actions. The second drawback is that most existing mechanisms have a limited ability to fit in different environments, such as different types of social networks. The performance of existing mechanisms heavily depends on some factors, such as network structures and the initial proportion of cooperators. To solve these two drawbacks, we propose a novel mechanism which adopts differential privacy mechanisms and reinforcement learning. Differential privacy mechanisms can be used to relieve the impact of malicious agents by exploiting the property of randomization. Reinforcement learning enables agents to learn how to make decisions in various social networks. In this way, the proposed mechanism can promote the evolution of cooperation in malicious social networks.																	0941-0643	1433-3058															10.1007/s00521-020-05243-5		AUG 2020											
J								Pedestrian detection using multi-scale squeeze-and-excitation module	MACHINE VISION AND APPLICATIONS										Pedestrian detection; Deep learning; Feature pyramid network; Squeeze-and-excitation network; Soft non-maximum suppression		Computer vision systems are major research items for autonomous vehicles. However, it is often challenging to understand the road scene, especially when objects are small and overlapping. To address these problems, this paper proposes a deep learning-based pedestrian detection method for small and overlapping objects. The proposed method adopts a parallel feature pyramid network with multi-scale feature layers, and the multi-scale squeeze-and-excitation (MSSE) module is proposed for better selection of multi-scale features. The proposed MSSE module helps to detect small objects by increasing the final feature resolution. In addition, channel-wise feature representation emphasizes important channels with reduced influence of weakly related features. Finally, the object's proposals are regressed using soft non-maximum suppression to differentiate the overlapped objects. The experiments show significant performance enhancement with the proposed method in an ablation study.																	0932-8092	1432-1769				AUG 3	2020	31	6							55	10.1007/s00138-020-01105-1													
J								Artificial intelligence and moral rights	AI & SOCIETY										Copyrights; Moral rights; Artificial Intelligence; Authorship; Attribution; Contributorship	LEGAL PERSONALITY; COPYRIGHT; PROTECTION; PRIVACY; AUTHOR	Whether copyrights should exist in content generated by an artificial intelligence is a frequently discussed issue in the legal literature. Most of the discussion focuses on economic rights, whereas the relationship of artificial intelligence and moral rights remains relatively obscure. However, as moral rights traditionally aim at protecting the author's "personal sphere", the question whether the law should recognize such protection in the content produced by machines is pressing; this is especially true considering that artificial intelligence is continuously further developed and increasingly hard to comprehend for human beings. This paper first provides the background on the protection of moral rights under existing international, U.S. and European copyright laws. On this basis, the paper then proceeds to highlight special issues in connection with moral rights and content produced by artificial intelligence, in particular whether an artificial intelligence itself, the creator or users of an artificial intelligence should be considered as owners of moral rights. Finally, the present research discusses possible future solutions, in particular alternative forms of attribution rights or the introduction of related rights.																	0951-5666	1435-5655															10.1007/s00146-020-01027-6		AUG 2020											
