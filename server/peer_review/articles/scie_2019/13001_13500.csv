PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	RP	EM	RI	OI	FU	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	D2	EA	PG	WC	SC	GA	UT	PM	OA	HC	HP	DA
J								Indicator & crowding distance-based evolutionary algorithm for combined heat and power economic emission dispatch	APPLIED SOFT COMPUTING										Cogeneration; Economic emission dispatch; Valve-point effects; Indicator-based evolutionary algorithm; Crowding-distance; Convergence	MULTIOBJECTIVE OPTIMIZATION; SWARM OPTIMIZATION; SELECTION; PARETO	Heat and power have become the most indispensable resources. However, the traditional ways of generating power and heat are inefficient and cause high pollution; a CHP (Combined Heat and Power) unit can solve these problems well. In recent years, more attention has been paid to energy conservation and environmental protection, and Combined Heat and Power Economic Emission Dispatch (CHPEED) has become an important multi-objective optimization problem. In this paper, an Indicator & crowding Distance-based Evolutionary Algorithm (IDBEA) is put forward for handling this non-convex and non-linear problem. With consideration of the valve-point effects and power transmission loss, IDBEA is tested on three standard test systems with different types, including four units, five units and seven units. In the experiment, IDBEA is compared with several evolutionary algorithms, the simulation results demonstrate that IDBEA has strong stability and superiority, while the solutions show better convergence and diversity than several typical algorithms. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106158	10.1016/j.asoc.2020.106158													
J								Collaborative filtering based recommendation of sampling methods for software defect prediction	APPLIED SOFT COMPUTING										Defect prediction; Sampling methods; Recommendation	MACHINE LEARNING TECHNIQUES; CLASS IMBALANCE; CLASSIFICATION; FRAMEWORK; MODELS; SMOTE	The performance of software defect prediction have been hindered by the imbalanced nature of software defect data. Fortunately, a variety of sampling methods have been employed to improve defect prediction performance. However, researchers and practitioners are usually burdened with selecting the optimal sampling methods for the defect data at hand. In practice, no sampling method has been found to perform best in theory and practice. Therefore it is necessary and valuable to study how to select applicable sampling methods according to the current data characteristics. This paper presents a collaborative filtering based sampling methods recommendation algorithm (CFSR) for automatically recommending applicable sampling methods for the new defect data. CFSR firstly ranks existing sampling methods with historical defect data, and then mines the data similarity between the new and historical defect data with meta-features. Finally, all the information of ranked sampling methods and data similarity are combined to build a recommendation network, with which the user-based collaborative filtering algorithm is employed to recommend appropriate sampling methods for the new defect data. A thorough experiment with five classification algorithms, two prediction performance, five recommendation performance and 12 popular sampling methods was conducted over 20 imbalanced software defect data. The experimental results firstly demonstrate the importance and necessity of present study, and then show that the proposed CFSR method is feasible and effective. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106163	10.1016/j.asoc.2020.106163													
J								Island-based Crow Search Algorithm for solving optimal control problems	APPLIED SOFT COMPUTING										Crow Search Algorithm; Island model; Hierarchical structured population; Optimal control	PARTICLE SWARM OPTIMIZATION; KRILL HERD ALGORITHM; DYNAMIC OPTIMIZATION; GLOBAL OPTIMIZATION; EVOLUTIONARY ALGORITHMS; HARMONY SEARCH; LOCAL SEARCH; TOPOLOGY; MODELS	Crow Search Algorithm (CROW) is one of the members of recently developed swarm-based meta-heuristic algorithms. Literature includes different applications of this algorithm on engineering design problems. However, this optimization method suffers from some drawbacks such as premature convergence and trapping into local optima at the early phase of iterations. In order to conquer this algorithm specific inabilities, many research studies have been conducted in the literature dealing with the improvements and enhancements on the search mechanism of CROW. Structured population mechanism plays a vital role in preserving and controlling diversity, and thus increases the solution efficiency in evolutionary algorithms. Among the different types of methods used in structured algorithms, the island model is one of the widely applied solution strategies, in which the population individuals are subdivided into a predefined number of subpopulations. Migration mechanism is the key factor increasing population diversity, which takes place between independently running subpopulations during iterations to exchange valuable and useful solution information. This study embeds the fundamentals of the island model concepts into the Crow Search Algorithm to improve its probing capabilities of the search domain, by means of the periodically interacting subpopulations on the course of iterations. In addition, four different hierarchical migration topologies have been proposed, and their search effectiveness have been evaluated and compared over 45 optimization test functions. The optimization function test set includes classic benchmark optimization problems and CEC 2015 benchmark functions. Furthermore, each hierarchical island model is applied for solving six different optimal control problems in order to investigate their efficiencies on multi-dimensional real world optimization problems. The investigated optimal control problems are parallel reaction, continuous stirred tank reactor, batch reactor consecutive reaction, nonlinear constrained mathematical system, nonlinear continuous stirred tank reactor and nonlinear crane container problems. It is found out that the island model concepts improved the optimization performance of CROW. The proposed island models outperformed or showed similar performance compared to the six selected literature optimizers for 27-29 classic benchmark optimization problems. Moreover, incorporating the master sub-population to the island model improved the optimization capability of the algorithm further in most cases. The island models that employ the master sub-population came up with more favorable results compared to their non-master sub-population peers in all optimal control problems. The island model that includes the master sub-population and has the migration topology entitled "82'' found the most desirable solutions for 4-6 optimal control problems. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106170	10.1016/j.asoc.2020.106170													
J								Generative adversarial network and texture features applied to automatic glaucoma detection	APPLIED SOFT COMPUTING										Glaucoma; Generative adversarial network; Taxonomic diversity indices; Texture features	TAXONOMIC DISTINCTNESS; DIVERSITY; SEGMENTATION	Glaucoma is a neurodegenerative disease that has a multifactorial etiology. The main characteristic of this illness is the progressive lesion of the optic nerve. This disease is chronic and causes permanent blindness at an advanced stage. Early diagnosis is essential to ensure a favorable prognosis and improve the patient's quality of life. Digital Image Processing together with computational techniques of machine learning allow the creation of methods for automatic detection of glaucoma. In this context, this work aims at the early diagnosis of glaucoma through a Generative Adversarial Network allied to texture attributes defined from indexes of taxonomic diversity. The method we propose can be divided into: (i) image acquisition through the RIM-ONE and Drishti-GS public databases; (ii) training of a conditional Generative Adversarial Network for segmentation of the optical discs into retinal images; (iii) pre-processing through enhancement and hole fill-in techniques; (iv) extraction of texture attributes using the index of taxonomic diversity; and (v) validation of the proposal through three classifiers evaluated according to four performance metrics. The results are promising and indicate that the method is robust, initially reaching 77.9% accuracy. However, as we apply improvements and adjustments in the method employed, we reach 100% accuracy and a ROC curve of 1. Therefore, we propose a second opinion on the diagnosis of glaucoma, assisting the specialist precisely. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106165	10.1016/j.asoc.2020.106165													
J								Financial distress prediction: Regularized sparse-based Random Subspace with ER aggregation rule incorporating textual disclosures	APPLIED SOFT COMPUTING										Financial distress prediction; Random subspace; Textual disclosures; Grouping features; Sparse group lasso; Evidence reasoning rule	MAJORITY VOTING COMBINATION; CHINESE LISTED COMPANIES; SUPPORT VECTOR MACHINES; NEURAL-NETWORKS; BANKRUPTCY PREDICTION; FEATURE-SELECTION; DISCRIMINANT-ANALYSIS; EVALUATING SENTIMENT; CORPORATE FAILURE; LEARNING-MODELS	For the sake of risks management, losses reduction, and costs saving, financial distress prediction (FDP) has attracted extensive attention from various communities including academic researchers, industrial practitioners, and government regulators. In addition to the conventional financial information, the textual disclosures regarding companies have received especial concern nowadays and are demonstrated to be effective for FDP. Ensemble methods have become a prevalent research line in the field of FDP incorporating financial and non-financial features. Feature quality is an important factor determining the accuracy in ensemble, however, traditional ensemble methods integrate these different types of features directly and ignore their grouping structures, hence weakening the feature quality and ultimately deteriorating the prediction accuracy. Moreover, although diversity can be obtained by virtue of the randomness of feature sampling in ensemble, the problem is that such randomness leads to the ambiguities among base classifiers, resulting in that the prediction accuracy of each classifier could not be ensured. Having noted these deficiencies, we propose a novel and robust meta FDP framework, which incorporates the feature regularizing module for identifying discriminatory predictive power of multiple features and the probabilistic fusion module for enhancing the aggregation over base classifiers. To validate our proposed regularized sparse-based Random Subspace with Evidential Reasoning rule (RS2_ER), we conducted extensive experiments on the datasets collected from the China Security Market Accounting Research Database (CSMARD), and the experimental results indicate that the proposed RS2_ER method enables the prediction effectiveness on FDP to be significantly facilitated by dealing with the features grouping property and the ambiguities among base classifiers. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106152	10.1016/j.asoc.2020.106152													
J								Tracking a dynamic invading target by UAV in oilfield inspection via an improved bat algorithm	APPLIED SOFT COMPUTING										Oilfield inspection; UAV; Dynamic target tracking; Trajectory prediction and optimization; Swarm intelligence	UNMANNED AERIAL VEHICLES; GROUND TARGET; OPTIMIZATION; FILTER	A novel dynamic invading target tracking method for the oilfield inspection by unmanned aerial vehicle (UAV) is presented in this paper. In this study the quad-rotor UAV is used to track an invading target, because the traditional manual inspection method and fixed-points video monitoring method has some drawbacks such as low efficiency, high cost, blind spot, and so on. A trajectory prediction method for the ground dynamic invading target is firstly proposed to predict the moving trajectory of the invading target. Then, the swarm intelligence based optimization algorithm is used to optimize the tracking trajectory of UAV, which in order to keep the distance between the UAV and the target closing to the desired distance during tracking process. In order to overcome some drawbacks such as easily being fallen into the local optimal solution and poor stability of the optimization, an improved bat algorithm (named FOBA) is proposed to improve the local searching ability of the bat algorithm (BA), which uses a food searching mechanism in the fruit fly optimization algorithm (FOA). Case studies are conducted with the desired distance is 50m between the UAV and the target, and experimental results show that the FOBA algorithm can effectively keep the tracking distance between the UAV and the target being about 55m, which is better than some other methods. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106150	10.1016/j.asoc.2020.106150													
J								Modified hybrid bat algorithm with genetic crossover operation and smart inertia weight for multilevel image segmentation	APPLIED SOFT COMPUTING										Multilevel image segmentation; Bat algorithm; Smart inertia weight; Crossover operation; Beta distribution	CUCKOO SEARCH ALGORITHM; PARTICLE SWARM OPTIMIZATION; ENTROPY; SELECTION	Multilevel thresholding is one of the most commonly used methods in image segmentation. However, the exhaustive search method is computationally expensive for selecting the optimal thresholds. Therefore, a hybrid bat algorithm with genetic crossover operation and smart inertia weight (SGA-BA) is proposed to choose the optimal thresholds. Furthermore, between-class variance (the Otsu method) and Kapur's entropy are used as objective functions. In the novel SGA-BA, the smart inertia weight balances the SGA-BA's exploration and exploitation based on the number of iterations and fitness values. Moreover, the local search capability of SGA-BA is strengthened by the crossover operation of the genetic algorithm. Meanwhile, the random vector is replaced by the beta distribution, which updates the frequency of bat in a smart way. The proposed SGA-BA was evaluated by a set of benchmark images with various levels of thresholds. Additionally, SGA-BA was compared with some well-known and recent heuristic algorithms, such as the genetic algorithm (GA), gravitational search algorithm (GSA), particle swarm optimization (PSO), whale optimization algorithm (WOA), improved salp swarm algorithm (LSSA) and basic bat algorithm (BA). The experimental results show that the proposed SGA-BA provides better outcomes than the other algorithms. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106157	10.1016/j.asoc.2020.106157													
J								From Big Data to business analytics: The case study of churn prediction	APPLIED SOFT COMPUTING										Data streams; ETL; Business analytics; Hadoop; Spark; Churn prediction	INTELLIGENCE; SYSTEMS	The success of companies hugely depends on how well they can analyze the available data and extract meaningful knowledge. The Extract-Transform-Load (ETL) process is instrumental in accomplishing these goals, but requires significant effort, especially for Big Data. Previous works have failed to formalize, integrate, and evaluate the ETL process for Big Data problems in a scalable and cost-effective way. In this paper, we propose a cloud-based ETL framework for data fusion and aggregation from a variety of sources. Next, we define three scenarios regarding data aggregation during ETL: (i) ETL with no aggregation; (ii) aggregation based on predefined columns or time intervals; and (iii) aggregation within single user sessions spanning over arbitrary time intervals. The third scenario is very valuable in the context of feature engineering, making it possible to define features as "the time since the last occurrence of event X''. The scalability was evaluated on Amazon AWS Hadoop clusters by processing user logs collected with Kinesis streams with datasets ranging from 30 GB to 2.6 TB. The business value of the architecture was demonstrated with applications in churn prediction, service-outage prediction, fraud detection, and more generally - decision support and recommendation systems. In the churn prediction case, we showed that over 98% of churners could be detected, while identifying the individual reason. This allowed support and sales teams to perform targeted retention measures. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106164	10.1016/j.asoc.2020.106164													
J								An adaptive dual-population evolutionary paradigm with adversarial search: Case study on many-objective service consolidation	APPLIED SOFT COMPUTING										Evolutionary algorithm; Dual-population; Adversarial search; Angle based selection; Service portfolio	ARTIFICIAL BEE COLONY; ALGORITHM; OPTIMIZATION; SELECTION; DECOMPOSITION	Optimizing many conflicting objectives simultaneously is one of the most challenging topics in the multi-criterion decision-making. This paper develops a dual-population co-evolutionary paradigm for solving many-objective service selection problems. It evolves two co-evolving populations separately with different scalarizing functions (SFs) and adversarial search orientations in parallel. In particular, one population, driven by convergence-oriented SF with ideal point, pulls the solutions toward the Pareto front; the other one, driven by diversity-oriented SF with nadir point, pushes the solutions backward from the nadir point. Accordingly, the search behaviors of the two populations are arguably complement to each other. Moreover, corner solutions and angle-based similarity are employed to enhance the coverage of population as widely as possible, the interaction and collaboration among populations are leveraged by a carefully crafted elitism pairing strategy. A series of experimental studies have been performed on challenging real-world service composition problems. Empirical results have demonstrated the competitiveness of our proposal against the state-of-the-art peers. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106160	10.1016/j.asoc.2020.106160													
J								Ra-dominance: A new dominance relationship for preference-based evolutionary multiobjective optimization	APPLIED SOFT COMPUTING										Evolutionary algorithm; Reference point; Decision maker; Multiobjective optimization; Dominance relationship	ALGORITHM	While traditional Pareto-based evolutionary multi-objective optimization (EMO) algorithms have shown an excellent balance between convergence and diversity on a wide range of practical problems with two or three objectives in real applications, the decision maker (DM) is interested in a unique set of solutions rather than the whole population on Pareto optimal front (POF). In addition, Paretobased EMO algorithms have some shortcomings in dealing with many-objective problems because of insufficient selection pressure toward trade-off solutions. Due to the above, it is crucial to incorporate DM preference information into EMO and seek a representative subset of Pareto optimal solutions with an increase in the number of objectives. This paper proposes a new dominance relationship, called Ra-dominance, which can improve diversity among the Pareto-equivalent solutions increase the selection pressure in evolutionary process. It has the ability to guide the population toward areas more responsive to the needs of the DM according to a reference point and preference angle. We use the new dominance relationship in the NSGA-II algorithm, and the efficacy and usefulness of the modified procedure are assessed through two- to ten-objective problems. Experimental results show that the algorithm applying this new dominance relationship is highly competitive when compared with four state-of-the-art preference-based EMO methods. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				MAY	2020	90								106192	10.1016/j.asoc.2020.106192													
J								Normalization-based Neighborhood Model for Cold Start Problem in Recommendation System	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Recommender system; cold start; collaborative filtering; normalization		Existing approaches for Recommendation Systems (RS) are mainly based on users' past knowledge and the more popular techniques such as the neighborhood models focus on finding similar users in making recommendations. The cold start problem is due to inaccurate recommendations given to new users because of lack of past data related to those users. To deal with such cases where prior information on the new user is not available, this paper proposes a normalization technique to model user involvement for cold start problem or user likings based on the details of items used in the neighborhood models. The proposed normalization technique was evaluated using two datasets namely MovieLens and GroupLens. The results showed that the proposed technique is able to improve the accuracy of the neighborhood model, which in turn increases the accuracy of an RS.																	1683-3198					MAY	2020	17	3					281	290		10.34028/iajit/17/3/1													
J								Incorporating Reverse Search for Friend Recommendation with Random Walk	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Social networks; friend recommendation; reverse search	PREDICTION	Recommending friends is an important mechanism for social networks to enhance their vitality and attractions to users. The huge user base as well as the sparse user relationships give great challenges to propose friends on social networks. Random walk is a classic strategy for recommendations, which provides a feasible solution for the above challenges. However, most of the existing recommendation methods based on random walk are only weighing the forward search, which ignore the significance of reverse social relationships. In this paper, we proposed a method to recommend friends by integrating reverse search into random walk. First, we introduced the FP-Growth algorithm to construct both web graphs of social networks and their corresponding transition probability matrix. Second, we defined the reverse search strategy to include the reverse social influences and to collaborate with random walk for recommending friends. The proposed model both optimized the transition probability matrix and improved the search mode to provide better recommendation performance. Experimental results on real datasets showed that the proposed method performs better than the naive random walk method which considered the forward search mode only.																	1683-3198					MAY	2020	17	3					291	298		10.34028/iajit/17/3/2													
J								A Deep Learning based Arabic Script Recognition System: Benchmark on KHAT	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Handwritten Arabic text recognition; deep learning; data augmentation		This paper presents a deep learning benchmark on a complex dataset known as KFUPM Handwritten Arabic TexT (KHATT). The KHATT data-set consists of complex patterns of handwritten Arabic text-lines. This paper contributes mainly in three aspects i.e., (1) pre-processing, (2) deep learning based approach, and (3) data-augmentation. The pre-processing step includes pruning of white extra spaces plus de-skewing the skewed text-lines. We deploy a deep learning approach based on Multi-Dimensional Long Short-Term Memory (MDLSTM) networks and Connectionist Temporal Classification (CTC). The MDLSTM has the advantage of scanning the Arabic text-lines in all directions (horizontal and vertical) to cover dots, diacritics, strokes and fine inflammation. The data-augmentation with a deep learning approach proves to achieve better and promising improvement in results by gaining 80.02% Character Recognition (CR) over 75.08% as baseline.																	1683-3198					MAY	2020	17	3					299	305		10.34028/iajit/17/3/3													
J								A Fog Computing-based Framework for Privacy Preserving IoT Environments	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Internet of thing; cloud computing; fog computing; privacy; security	INTERNET	Privacy is becoming an indispensable component in the emerging Internet of Things (IoT) context. However, the IoT based devices and tools are exposed to several security and privacy threats, especially that these devices are mainly used to gather data about users' habits, vital signs, surround environment, etc., which makes them a lucrative target to intruders. Up to date, conventional security and privacy mechanisms are not well optimized for IoT devices due to their limited energy, storage capacity, communication functionality and computing power, which influenced researchers to propose new solutions and algorithms to handle these limitations. Fog and cloud computing have been recently integrated in IoT environment to solve their resources' limitations, thus facilitating new life scenarios-oriented applications. In this paper, a security and privacy preserving framework is proposed, which utilizes Fog and cloud computing in conjunction with IoT devices that aims at securing the users' data and protecting their privacy. The framework has been implemented and tested using available technologies. Furthermore, a security analysis has been verified by simulating several hypothetical attack scenarios, which showed the effectiveness of the proposed framework and its capability of protecting the users' information.																	1683-3198					MAY	2020	17	3					306	315		10.34028/iajit/17/3/4													
J								Self-Organizing Map vs Initial Centroid Selection Optimization to Enhance K-Means with Genetic Algorithm to Cluster Transcribed Broadcast News Documents	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Clustering; k-means; self-organizing maps; genetic algorithm; speech transcripts; cenfroid selection	CLASSIFICATION	A compilation of artificial intelligence techniques are employed in this research to enhance the process of clustering transcribed text documents obtained from audio sources. Many clustering techniques suffer from drawbacks that may cause the algorithm to tend to sub optimal solutions, handling these drawbacks is essential to get better clustering results and avoid sub optimal solutions. The main target of our research is to enhance automatic topic clustering of transcribed speech documents, and examine the difference between implementing the K-means algorithm using our Initial Cenfroid Selection Optimization (ICSO) [16] with genetic algorithm optimization with Chi-square similarity measure to cluster a data set then use a self-organizing map to enhance the clustering process of the same data set, both techniques will be compared in terms of accuracy. The evaluation showed that using K-means with ICSO and genetic algorithm achieved the highest average accuracy.																	1683-3198					MAY	2020	17	3					316	324		10.34028/iajit/17/3/5													
J								A Semantic Framework for Extracting Taxonomic Relations from Text Corpus	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Taxonomic relation; ontology construction; word sense disambiguation; knowledge acquisition	LEARNING CONCEPT HIERARCHIES; ONTOLOGIES; CONSTRUCTION	Nowadays, ontologies have been exploited in many current applications due to the abilities in representing knowledge and inferring new knowledge. However, the manual consfruction of ontologies is tedious and time-consuming. Therefore, the automated ontology consfruction from text has been investigated. The extraction of taxonomic relations between concepts is a crucial step in constructing domain ontologies. To obtain taxonomic relations from a text corpus, especially when the data is deficient, the approach of using the web as a source of collective knowledge (a.k.a web-based approach) is usually applied. The important challenge of this approach is how to collect relevant knowledge from a large amount of web pages. To overcome this issue, we propose a framework that combines Word Sense Disambiguation (WSD) and web approach to extract taxonomic relations from a domain-text corpus. This framework consists of two main stages: concept extraction and taxonomic-relation extraction. Concepts acquired from the concept-extraction stage are disambiguated through WSD module and passed to stage of extraction taxonomic relations afterward. To evaluate the efficiency of the proposed framework, we conduct experiments on datasets about two domains of tourism and sport. The obtained results show that the proposed method is efficient in corpora which are insufficient or have no training data. Besides, the proposed method outperforms the state of the art method in corpora having high WSD results.																	1683-3198					MAY	2020	17	3					325	337		10.34028/iajit/17/3/6													
J								A Contrivance to Encapsulate Virtual Scaffold with Comments and Notes	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Virtual scaffold; Multi-Tenant common gateway; pattern; model view controller; role-based access control; JavaScript object notation; not only structured query language; software as a service		CLOUD is an elision of Common Location-independent Online Utility available on-Demand and is based on Service Oriented Architecture (SOA). Today a chunk of researchers were working towards contrivance based on multi-tenant aware Software as a Service (SaaS) application development and still a precise pragmatic solution remains a challenge among the researchers. The first step towards resolving solution is to enhance the virtual scaffold and propose it as a System under Test (SuT). The entire work is proposed as a Model View Controller (MVC) where the tenant login through the View and write their snippet code for encapsulation. The proposed VirScaff schema acts as Controller and provides authentication and authorization by role/session assignment for tenant and thus helps to access data from the dashboard (Viz., Create, Read, Update and Delete (CRUD)). The SuT supports and accommodates both SQL and Not only Structured Query Language (NoSQL) dataset. Finally, this paper construed that SuT behaves well for both SQL and NoSQL dataset in terms of time and space complexities. To sum-up, the entire work addresses the challenges towards multitenant aware SaaS application development and highly commendable while using NoSQL dataset.																	1683-3198					MAY	2020	17	3					338	346		10.34028/iajit/17/3/7													
J								Using Total Probability in Image Template Matching	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Digital surface model; template image matching; normalised cross-correlation; probability	REGISTRATION; GENERATION	Image template matching is a main task in photogrammetry and computer vision. The matching can be used to automatically determine the 3D coordinates of a point. A firstborn image matching method in fields of photogrammetry and computer vision is area-based matching, which is based on correlation measuring that uses normalised cross-correlation. However, this method fails at a discontinuous edge and at the area of low illumination or at geometric distortion because of changes in imaging location. Thus, these points are considered outliers. The proposed method measures correlations, which is based on normalised cross-correlation, at each point by using various sizes of window and then considering the probability of correlations for each window. Thereafter, the determined probability values are integrated. On the basis of a specific threshold value, the point of maximum total probability correlation is recognised as a corresponding point. The algorithm is applied to aerial images for Digital Surface Model (DSM) generation. Results show that the corresponding points are identified successfully at different locations, especially at a discontinuous point, and that a Digital Surface Model of high resolution is generated.																	1683-3198					MAY	2020	17	3					347	357		10.34028/iajit/17/3/8													
J								A Novel Physical Machine Overload Detection Algorithm Combined with Quiescing for Dynamic Virtual Machine Consolidation in Cloud Data Centers	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Distributed systems; cloud computing; dynamic consolidation; overload detection and energy efficiency	ENERGY; MANAGEMENT; PERFORMANCE; POWER; PREDICTION; MIGRATION; QUALITY; COST	Further growth of computing performance has been started to be limited due to increasing energy consumption of cloud data centers. Therefore, it is important to pay attention to the resource management. Dynamic virtual machines consolidation is a successful approach to improve the utilization of resources and energy efficiency in cloud environments. Consequently, optimizing the online energy-performance trade off directly influences Quality of Service (QoS). In this paper, a novel approach known as Percentage of Overload Time Fraction Threshold (POTFT) is proposed that decides to migrate a Virtual Machine (T/A/) if the current Overload Time Fraction (OTF) value of Physical Machine (PM) exceeds the defined percentage of maximum allowed OTF value to avoid exceeding the maximum allowed resulting OTF value after a decision of VA/ migration or during VA/ migration. The proposed POTFT algorithm is also combined with VA/ quiescing to maximize the time until migration, while meeting QoS goal. A number of benchmark PM overload detection algorithms is implemented using different parameters to compare with POTFT with and without VM quiescing. We evaluate the algorithms through simulations with real world workload traces and results show that the proposed approaches outperform the benchmark PM overload detection algorithms. The results also show that proposed approaches lead to better time until migration by keeping average resulting OTF values less than allowed values. Moreover, POTFT algorithm with VM quiescing is able to minimize number of migrations according to QoS requirements and meet OTF constraint with a few quiescings.																	1683-3198					MAY	2020	17	3					358	366		10.34028/iajit/17/3/9													
J								Issues of Dialectal Saudi Twitter Corpus	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Microblogs; tweets; Saudi colloquial; corpus and modern standard Arabic	SENTIMENT ANALYSIS	Text mining research relies heavily on the availability of a suitable corpus. This paper presents a dialectal Saudi corpus that contains 207452 tweets generated by Saudi Twitter users. In addition, a comparison between the Saudi tweets dataset, Egyptian Twitter corpus and Arabic top news raw corpus (representing Modern Standard Arabic (MSA) in various aspects, such as the differences between formal and colloquial texts was carried out. Moreover, investigation into the issues and phenomena, such as shortening, concatenation, colloquial language, compounding, foreign language, spelling errors and neologisms on this type of dataset was performed.																	1683-3198					MAY	2020	17	3					367	374		10.34028/iajit/17/3/10													
J								An Enhanced MSER Pruning Algorithm for Detection and Localization of Bangla Texts from Scene Images	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										MSER; scene image; ICDAR; aspect ratio; euler number; bangla text	READING TEXT	Text detection and localization have great importance for content based image analysis and text based image indexing. The efficiency of text recognition depends on the efficiency of text localization. So, the main goal of the proposed method is to detect and localize text regions with high accuracy. To achieve this goal, a new and efficient method has been introduced for localization of Bangla text from scene images. In order to improve precision and recall as well as f-measure, Maximally Stable Extremal Region (MSER) based method along with double filtering techniques have been used. As MSER algorithm generates many false positives, we have introduced double filtering method for removing these false positives to increase the f-measure to a great extent. Our proposed method works at three basic levels. Firstly, MSER regions are generated from the input color image by converting it into gray scale image. Secondly, some heuristic features are used to filter out most of the false positives or non-text regions. Lastly, Stroke Width Transform (SWT) based filtering method is used to filter out remaining non-text regions. Remaining components are then grouped into candidate text regions marked by bounding box over each region. As there is no benchmark database for Bangla text, the proposed method is implemented on our own prepared database consisting of 200 scene images of Bangla texts and has got prominent performance. To evaluate the performance of our proposed approach, we have also tested the proposed method on International Conference on Document Analysis and Recognition( ICDAR) 2013 benchmark database and have got a better result than the related existing methods.																	1683-3198					MAY	2020	17	3					375	385		10.34028/iajit/17/3/11													
J								A Smart Card Oriented Secure Electronic Voting Machine Built on NTRU	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										EVM; blind signature; homomorphic tally; smart card; NTRU	ENCRYPTION	Free and fair elections are indispensable to quantify the sentiments of the populace for forming the government of representatives in democratic countries. Due to its procedural variation from country to country and complexity, to maneuverer, it is a challenging task. Since the Orthodox paper-based electoral systems are slow and error-prone, therefore, a secure and efficient electoral system always remained a key area of research. Although a lot of literature is available on this topic. However, due to reported anomalies and weaknesses in American and France election in 2016, it once again has become a pivotal subject of research. In this article, we proposed a new secure and efficient electronic voting scheme based on public key cryptosystem dubbed as Number Theory Research Unit (NTRU). Furthermore, an efficient and robust three factors authentication protocol based on a personalized memorable password, a smartcard, and bioHash is proposed to validate the legitimacy of a voter for casting a legal vote. NTRU based blind signatures are used to preserve the anonymity and privacy of vote and voters, whereas the proficiency of secure and efficient counting of votes is achieved through NTRU based homomorphic tally. Non-coercibility and individual verifiability are attained through Mark Pledge scheme. The proposed applied electronic voting scheme is, secure, transparent and efficient for large scale elections.																	1683-3198					MAY	2020	17	3					386	393		10.34028/iajit/17/3/12													
J								Direct Text Classifier for Thematic Arabic Discourse Documents	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Text mining; Arabic discourse; text classification; topic modling; n-gram language model; topical coherence		Maintaining the topical coherence while writing a discourse is a major challenge confronting novice and non-novice writers alike. This challenge is even more intense with Arabic discourse because of the complex morphology and the widespread of synonyms in Arabic language. In this research, we present a direct classification of Arabic discourse document while writing. This prescriptive proposed framework consists of the following stages: data collection, pre-processing, construction of Language Model (LM), topics identification, topics classification, and topic notification. To prove and demonstrate our proposed framework, we designed a system and applied it on a corpus of 2800 Arabic discourse documents synthesized into four predefined topics related to: Culture, Economy, Sport, and Religion. System performance was analysed, in terms of accuracy, recall, precision, and F-measure. The results demonstrated that the proposed topic modeling-based decision framework is able to classify topics while writing a discourse with accuracy of 91.0%.																	1683-3198					MAY	2020	17	3					394	403		10.34028/iajit/17/3/13													
J								A Novel Image Retrieval Technique using Automatic and Interactive Segmentation	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										CBIR; information retrieval; image segmentation; multimedia image refrieval		In this paper, we present a new region-based image refrieval technique based on robust image segmentation. Traditional content-based image refrieval deals with the global description of a query image. We combine the state-of-the-art segmentation algorithms with the traditional approach to narrow the area of interest to a specific region within a query image. In case of automatic segmentation, the algorithm divides a query image automatically and computes Zernike moments for each region. For interactive segmentation, our proposed scheme takes as input a query image and some information regarding the region of interest. The proposed scheme then works by computing the Geodesic-based segmentation of the query image. The segmented image is our region of interest which is then used for computing the Zernike moments. The Euclidean distance is then used to retrieve different relevant images. The experimental results clearly show that the proposed scheme works efficiently and produces excellent results.																	1683-3198					MAY	2020	17	3					404	410		10.34028/iajit/17/3/14													
J								A New Metric for Class Cohesion for Object Oriented Software	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Class cohesion; metrics; OO software; maintenance-effort; metric validation	MAINTAINABILITY; GUIDELINES; SUITE	Various class cohesion metrics exist in literature both at design level and source code level to assess the quality of Object Oriented (OO) software. However, the idea of cohesive interactions (or relationships) between instance variables (i.e., attributes) and methods of a class for measuring cohesion varies from one metric to another. Some authors have used instance variable usage by methods of the class to measure class cohesion while some focus on similarity of methods based on sharing of instance variables. However, researchers believe that such metrics still do not properly capture cohesiveness of classes. Therefore, measures based on different perspective on the idea of cohesive interactions should be developed. Consequently, in this paper, we propose a source code level class cohesion metric based on instance variable usage by methods. We first formalize three types of cohesive interactions and then categorize these cohesive interactions by providing them ranking and weights in order to compute our proposed measure. To determine the usefulness of the proposed measure, theoretical validation using a property based axiomatic framework has been done. For empirical validation, we have used Pearson correlation analysis and logistic regression in an experimental study conducted on 28 Java classes to determine the relationship between the proposed measure and maintenance-effort of classes. The results indicate that the proposed cohesion measure is strongly correlated with maintenance-effort and can serve as a good predictor of the same.																	1683-3198					MAY	2020	17	3					411	421		10.34028/iajit/17/3/15													
J								Gene Expression Prediction Using Deep Neural Networks	INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY										Gene expression; regression; deep learning; autoencoder; multilayer perceptron	REPRESENTATIONS	In the field of molecular biology, gene expression is a term that encompasses all the information contained in an organism's genome. Although, researchers have developed several clinical techniques to quantitatively measure the expressions of genes of an organism, they are too costly to be extensively used. The NIH LINCS program revealed that human gene expressions are highly correlated. Further research at the University of California, Irvine (UCI) led to the development of D-GEX, a Multi Layer Perceptron (MLP) model that was trained to predict unknown target expressions from previously identified landmark expressions. But, bowing to hardware limitations, they had split the target genes into different sets and constructed separate models to profile the whole genome. This paper proposes an alternative solution using a combination of deep autoencoder and MLP to overcome this bottleneck and improve the prediction performance. The microarray based Gene Expression Omnibus (GEO) dataset was employed to train the neural networks. Experimental result shows that this new model, abbreviated as E-GEX, outperforms D-GEX by 16.64% in terms of overall prediction accuracy on GEO dataset. The models were further tested on an RNA-Seq based 1000G dataset and E-GEX was found to be 49.23% more accurate than D-GEX.																	1683-3198					MAY	2020	17	3					422	431		10.34028/iajit/17/3/16													
J								An efficient algorithm for identifying (l, d) motif from huge DNA datasets	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Transcription factor binding sites; Suffix array; Longest common prefixes; MapReduce; Mining emerging substring; Combining emerging substring; MCL algorithm	REGULATORY ELEMENTS; EM ALGORITHM; SEQUENCE; IDENTIFICATION; RECOGNITION; DISCOVERY; GENES; MODEL	Discovering Transcription Factor Binding Sites (TFBS) has immense significance in terms of developing techniques and evaluating regulatory processes in biological systems. The DNA gene sequence encompasses large volume of datasets so a new methodology is needed to analyze them in the quickest possible time. Over the past decades, the planted (l, d) motif discovery methodology has been used for locating TFBS in the genetic region. This paper focuses on developing a new approach for motif identification using planted (l, d) motif discovery algorithm. The proposed algorithm is named ESMD (Emerging Substring based Motif Detection), which is based on two processes: Mining and Combining Emerging Substrings. In the mining step, an array is initially created, based on the suffix array (SA) and the longest common prefix array (LCP). A MapReduce programming model handles the mining of emerging substring process since DNA gene sequences constitute huge data. The next step combines the emerging substrings of different lengths. The resulting models have been evaluated using two different metrics, the Pearson Correlation Coefficient (PCC) and the Area Under Curve (AUC). Both have produced much better results than existing methods.																	1868-5137	1868-5145															10.1007/s12652-020-02013-y		MAY 2020											
J								Non invasive detection of moss and crack in monuments using image processing techniques	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Non destructive; Clustering; Local binary pattern; Decay	EXTRACTION; CLASSIFICATION; SEGMENTATION; ALGORITHM	In the World's Cultural Heritage, stone monuments are a valuable treasure. The alarming increase of weathering damage on an ancient monument is leading to complete or partial destruction of heritage structures in the world. Early detection of such decaying is essential to preserve the monuments. A non-destructive technique is used for the analysis of monument decay. Digital image processing is one of the powerful methods of non-destructive techniques. This paper presents non-invasive detection of moss and crack in monuments using Integrated K-means clustering and Canny edge detection. Moss is detected using integrated clustering and compared with K-means image segmentation. In this study, mean square error and power signal to noise ratio are the two parameter metrics of moss detection. PSNR and MSE of proposed method is about 15.42 and 1882 improves than LBP-Mean with other methods. The crack is detected using integrated canny edge detection and the crack properties such as perimeter, orientation, and minor and major axis length are measured. Area of moss and cracks are determined. The proposed results improve the performance of detection than the existing method.																	1868-5137	1868-5145															10.1007/s12652-020-02006-x		MAY 2020											
J								GPU-accelerated uncapacitated facility location and semi-dense SymStereo pipelines for piecewise-planar-based 3D reconstruction	JOURNAL OF REAL-TIME IMAGE PROCESSING										Piecewise-planar reconstruction; SymStereo; Uncapacitated facility location; Parallel image processing; 3D reconstruction; High-resolution images; Multi-GPU systems	REAL-TIME	Planar 3D reconstruction presents advantages over point cloud representations. This work focuses on the acceleration of piecewise-planar-based 3D reconstruction, a StereoScan method. We identify the SymStereo (logN) and uncapacitated facility location (UFL) algorithms as the most computationally expensive tasks, consuming nearly 80 x of total runtime, when detecting planes in a single stereo pair on a sequential CPU pipeline. Consequently, these algorithms have been parallelized using single- and multi-GPU architectures to perform significantly faster than previous sequential approaches. Experimental results show that accelerated parallel implementations of SymStereo (logN) can process up to 56 frames per second, achieving a speedup of 38 x against the sequential C implementation (Intel Core i7-4790k). The parallel version of the message-passing algorithm (max-sum) for the UFL problem processes up to five matrices per second and outperforms the sequential C baseline for computing UFL by 38 x.																	1861-8200	1861-8219															10.1007/s11554-020-00974-z		MAY 2020											
J								Combined kernel for fast GPU computation of Zernike moments	JOURNAL OF REAL-TIME IMAGE PROCESSING										Zernike moments; GPU; Combined kernel; Octant symmetry	EFFICIENT; ALGORITHM; LEGENDRE; FORM	Zernike moments, as a representative orthogonal moment, have been widely applied in the fields of image processing and pattern recognition. The calculations are time-consuming due to the complexity of definition. Based on the GPU octant symmetry algorithm in our previous work, this paper presents a novel algorithm to increase the resource utilization by the combined kernel. Also, it optimizes radial polynomials of Zernike moments to reduce amount of calculations. The experimental results demonstrated that the proposed algorithm achieved overall computational performance improvement for any sized images. Moreover, there is no compromise in terms of precision compared to the typical accurate algorithm.																	1861-8200	1861-8219															10.1007/s11554-020-00979-8		MAY 2020											
J								Application of KPCA and AdaBoost algorithm in classification of functional magnetic resonance imaging of Alzheimer's disease	NEURAL COMPUTING & APPLICATIONS										Kernel principal component analysis (KPCA); AdaBoost algorithm; Alzheimer's disease (AD); Image classification		With the rapid development of modern brain imaging techniques and big data analysis that measures brain processes, researchers are increasingly looking to reveal the pathogenesis of Alzheimer's disease. In order to find effective classification of magnetic resonance images of Alzheimer's disease, this paper constructed a feature classification model for Alzheimer's disease based on AdaBoost algorithm and KPCA algorithm, and selected 21 patients with Alzheimer's disease (AD). The trial included 6 patients with advanced Alzheimer's disease (LAD), 7 patients with early Alzheimer's disease, and 8 healthy individuals (HC) who underwent different levels of analysis. The results show that the article uses the KPCA algorithm to obtain the highest classification accuracy of the two groups: 94.77%, the single feature distinguishing ability is the node degree, and the accuracy of 90.94% can be achieved in the imaging diagnosis of AD. The article can significantly improve the classification of magnetic resonance images of Alzheimer's disease. This result is a good test of the effectiveness of the selected algorithm and has profound clinical significance for the diagnosis and classification of AD using magnetic resonance imaging.																	0941-0643	1433-3058				MAY	2020	32	10			SI		5329	5338		10.1007/s00521-020-04707-y													
J								Convolution operators for visual tracking based on spatial-temporal regularization	NEURAL COMPUTING & APPLICATIONS										Target tracking; Correlation filter; Online PA; Reliability of channel		In recent years, the method based on discriminative correlation filter has been shown excellent performance in short-term visual tracking. However, discriminative correlation filter-based method heavily suffers from the problem of the multiple peaks and model drift in responds maps incurred by occlusion and rotation. To solve the above problem, we proposed convolution operators for visual tracking based on spatial-temporal regularization. Firstly, we add spatial-temporal regularization in loss function, which will guarantee continuity of the model in time. And we use preconditioned conjugate gradient algorithm to obtain filter coefficients. Secondly, we proposed channel reliability to estimate quality of the learned filter and fuse the different reliability coefficients to weight response map in location. We set a threshold to reduce the number of iteration in location and accelerate the compute speed of algorithm. Finally, we use two different correlation filters to estimate location and scale of target, respectively. Extensively experiment in five video sequences show that our tracker has been significantly improved performance in case of occlusion and rotation. The AUC in success plot improves 33.2% than ECO-HC and 41.5% than STRCF, respectively.																	0941-0643	1433-3058				MAY	2020	32	10			SI		5339	5351		10.1007/s00521-020-04704-1													
J								Stock price forecast based on combined model of ARI-MA-LS-SVM	NEURAL COMPUTING & APPLICATIONS										Stock price forecasting; Support vector machine; Least squares; Attribute reduction; Cumulative auto-regressive moving average	AUTOREGRESSIVE MOVING AVERAGE	Stock forecasting is a very complex non-stationary, nonlinear time series forecasting, and is often affected by many factors, making it difficult to predict it with a simple model. Support vector machine (SVM) is one of the common data mining methods in the field of machine learning. It has outstanding advantages compared with other methods and it is widely used in various fields. However, there are still many problems in the practical application of the method, and the model itself has many fields that need to be improved. The purpose of this paper is to accurately predict the trend of stock prices, providing a reference model for the trend of stock market and the tracking method of stock price prediction, and provide value reference for research on the forecasting model of stock market and investor's investment decision. Research using a combined model to predict stock market trends whether will have a significant improvement compared to using a single model to forecast that. The method of this paper is to analyze the shortcomings of current stock market forecasting methods and standard support vector machines firstly, at the same time, based on this, a cumulative auto-regressive moving average is proposed, which combines the least squares support vector machine synthesis model (ARI-MA-LS-SVM) to make basic predictions for the stock market. Secondly, process the data first for the predictive indicators by using cumulative auto-regressive moving average. Then, use the least squares support vector machine of simple indicator system to predict stock price fluctuations. Therefore, it can be concluded that the combined model based on ARI-MA-LS-SVM is more suitable for stock price forecasting than the single forecasting model, and the actual performance is better. At the same time, a large number of simulation experiments show that the algorithm of multiple model's fusion can achieve the expected effect, which indicate that the model has universal applicability, market applicability and stability feasibility. This model can bring some guidance and reference value for many investors and market regulators.																	0941-0643	1433-3058				MAY	2020	32	10			SI		5379	5388		10.1007/s00521-019-04698-5													
J								Unstructured big data analysis algorithm and simulation of Internet of Things based on machine learning	NEURAL COMPUTING & APPLICATIONS										Machine learning; Internet of Things; Big data analysis; Unstructured data	IOT; ANALYTICS; CLOUD	Big data values data processing to ensure effective value-added data. With the rapid development of the cloud era, the coverage of big data has gradually expanded, and it has received wide attention from all walks of life. In the process of modern social development, big data analysis is gradually applied to the future development planning, risk evaluation and integration of market development status. With the rapid development of many fields of society, the flow of information has gradually expanded, and the Internet has developed more rapidly, prompting the application of big data in various fields. Machine learning is a multidisciplinary study of how computers use data or past experience. With the ability to independently improve specific algorithms, the computer acquires knowledge through learning and achieves the goal of artificial intelligence. Big data and machine learning are the major technological changes in the modern computer world, and these technologies have had a huge impact on all walks of life. At present, with the rapid development of the Internet, mobile communications, social networks and the Internet of Things, these networks generate large amounts of data every day, and data become the most important information resource of today. Some studies have shown that in many cases, the larger the amount of data, the better the data will be for machine learning. On this basis, this paper proposes an online client algorithm based on machine learning algorithm for IoT unstructured big data analysis and uses it in other big data analysis scenarios. Use the online data entered by the customer to implement background data mining, the parallel way to verify its efficiency through machine learning algorithms such as K-nearest neighbor algorithm.																	0941-0643	1433-3058				MAY	2020	32	10			SI		5399	5407		10.1007/s00521-019-04682-z													
J								Multi-way matching based fine-grained sentiment analysis for user reviews	NEURAL COMPUTING & APPLICATIONS										R-Net; Fine-grained sentiment analysis; Self-attention; Multi-way matching mechanism		While sentiment analysis has been widely used in public opinion to explore tendency of users for a target product from large online review data, less work focus on aspect-level or fine-grained sentiment analysis in which the polarity of not only the aspect of a target object but also the attribute of that given aspect should be determinated. Recent work regards aspect-level sentiment analysis as two separate tasks, i.e., aspect classification and sentiment analysis, and this pipeline method leads to error propagation. To address this issue, this paper proposes an improved multi-way matching deep neural network model for fine-grained sentiment analysis, which jointly models the two tasks in one phase and improves current attention by directly capturing past attention in the multi-round alignment architecture, so as to prevent error propagation and attention deficiency problems. Experimental results on fine-grained sentiment analysis data sets of catering industry indicate that the F1 score of this model in actual test set reaches 0.7302 and EM score 87.1973, which are higher than baseline DocRNN model by 3.8% and 0.88% in F1 and EM, and are higher than SVM by 15.4% and 25.6%, which verified that our model could effectively predict fine-grained sentiment and have better generalization performance.																	0941-0643	1433-3058				MAY	2020	32	10			SI		5409	5423		10.1007/s00521-019-04686-9													
J								Multimodality registration for ocular multispectral images via co-embedding	NEURAL COMPUTING & APPLICATIONS										Medical image analysis; Image registration; Multispectral Imaging	OPPORTUNISTIC SPECTRUM ACCESS; ATTRIBUTE-BASED ENCRYPTION; MUTUAL-INFORMATION; FEATURE-EXTRACTION; CROWD EVACUATION; MEDICAL IMAGES; NETWORK; ALGORITHM; OPTIMIZATION; SELECTION	Image registration of sequential multispectral images plays a vital role in retinal image analysis, since the appearance of ocular tissues significantly relates to the diagnosis, treatment, and evaluation of various diseases in ophthalmology. State-of-the-art multimodality image registration techniques greatly rely on mutual information between paired images to obtain their correspondence. However, it has been observed that mutual information-based image registration approaches suffer from inaccuracy especially when they are applied to small-sized images. Bearing this in mind, a novel groupwise registration approach is proposed by mapping the extracted features from multimodality images into the same latent space. To evaluate the proposed approach, the comparison experiments are conducted between state-of-the-art methods and the proposed approach. Experimental results demonstrate the superior accuracy of the proposed approach over the state-of-the-art techniques. Therefore, the proposed algorithm could be an invaluable tool for multimodality image registration applications.																	0941-0643	1433-3058				MAY	2020	32	10			SI		5435	5447		10.1007/s00521-019-04685-w													
J								Embedded adaptive cross-modulation neural network for few-shot learning	NEURAL COMPUTING & APPLICATIONS										Few-shot; Image classification; Embedded adaptive; Cross-modulation		Although deep neural networks have made great success in several scenarios of machine learning, they face persistent challenges in small training datasets learning scenarios. Few-shot learning aims to learn from a few labeled examples. However, the limited training samples and weakly distinguishable embedding vectors in a metric space often lead to unsatisfactory test results and directly calculating the distance between tensors can cause ambiguity. This paper proposes an embedded adaptive cross-modulation (EACM) method for few-shot learning which combines the information between support and query examples. Specifically, the inter-class categorizability between the support set prototype representations is enhanced by the adaptive cosine metric module to improve the accuracy of the few-shot recognition result. The learning is performed by using the cross-modulation module at many levels of abstraction layers along the prediction pipeline. The support set and query set feature cross-enhance, which improves the generalization ability and robustness of image recognition. Afterward, we further combine above two methods by a weight balance scalar to determine the task-related metric space and construct a joint loss function. Theoretical analysis demonstrates the generalization ability of EACM. We conduct comprehensive experiments on mini-ImageNet and CUB datasets. Experimental results show that our approach is the state-of-the-art approach by significant margins.																	0941-0643	1433-3058				MAY	2020	32	10			SI		5505	5515		10.1007/s00521-019-04605-y													
J								Workload-driven coordination between virtual machine allocation and task scheduling	NEURAL COMPUTING & APPLICATIONS										VM allocation; Task scheduling; Workload characteristic; Markov model	CLOUD	The current task scheduling is separated from the virtual machine (VM) allocation, which, to some extent, wastes resources or degrades application performance. The scheduling algorithm influences the demand of VMs in terms of service-level agreement, while the number of VMs determines the performance of task scheduling. Workload plays an indispensable role in both dynamic VM allocation and task scheduling. To address this problem, we coordinate task scheduling and VM allocation based on workload characteristics. Workload is empirically time-varying and stochastic. We demonstrate that the acquired workload data set has Markov property which can be modeled as a Markov chain. Then, three workload characteristic operators are extracted: persistence, recurrence and entropy, which quantify the relative stability, burstiness, and unpredictability of the workload, respectively. Experiments indicate that the persistence and recurrence of workloads has a direct bearing on the average response time and resource utilization of the system. A nonlinear model between the load characteristic operators and the number of VMs is established. In order to test the performance of the collaborative framework, we design a scheduling algorithm based on genetic algorithm (GA), which takes the estimated number of VMs as input and the task completion time as the optimization target. Simulation experiments have been performed on the CloudSim platform, testifying that the estimated average absolute VMs error is only 2.6%. The GA-based task scheduling algorithm could improve resource utilization and reduce task completion time compared with the first come first serve and greedy algorithm. The proposed coordination mechanism in this paper has proved able to find the optimal match and reduce the resource cost by utilizing the interaction between VM allocation and task scheduling.																	0941-0643	1433-3058				MAY	2020	32	10			SI		5535	5551		10.1007/s00521-019-04022-1													
J								QL-HEFT: a novel machine learning scheduling scheme base on cloud computing environment	NEURAL COMPUTING & APPLICATIONS										Cloud computing; Directed acyclic graph; Makespan; Reinforcement learning; Task scheduling	ALGORITHM; SYSTEMS; VIEW	Cloud computing is a computing model that fully utilizes the resources on the Internet to maximize the utilization of resources. Due to a large number of users and tasks, it is important to achieve efficient scheduling of tasks submitted by users. Task scheduling is one of the crucial and challenging non-deterministic polynomial-hard problems in cloud computing. In task scheduling, obtaining shorter makespan is an important objective and is related to the pros and cons of the algorithm. Machine learning algorithms represent a new method for solving this type of problem. In this paper, we propose a novel task scheduling algorithm called QL-HEFT that combines Q-learning with the heterogeneous earliest finish time (HEFT) algorithm to reduce the makespan. The algorithm uses the upward rank (rank(u)) value of HEFT as the immediate reward in the Q-learning framework. The agent can obtain better learning results to update the Q-table through the self-learning process. The QL-HEFT algorithm is divided into two major phases: a task sorting phase based on Q-learning for obtaining an optimal order and a processor allocation phase using the earliest finish time strategy. Experiments show that QL-HEFT achieves a shorter makespan compared to three other classical scheduling algorithms as well as good performances in terms of the average response time.																	0941-0643	1433-3058				MAY	2020	32	10			SI		5553	5570		10.1007/s00521-019-04118-8													
J								Optimizing partitioned CSR-based SpGEMM on the Sunway TaihuLight	NEURAL COMPUTING & APPLICATIONS										General sparse matrix-sparse matrix multiplication; Matrix partition; Parallel; Sunway TaihuLight	MATRIX-MATRIX MULTIPLICATION; OPTIMIZATION; SPMV	General sparse matrix-sparse matrix (SpGEMM) multiplication is one of the basic kernels in a great many applications. Several works focus on various optimizations for SpGEMM. To fully exploit the powerful computing capability of the Sunway TaihuLight supercomputer for SpGEMM, this paper designs the partitioning method and parallelization of CSR-based SpGEMM to make it well match to the Sunway architecture. In addition, this paper optimizes the partitioning method based on the distribution of the floating-point calculations of the CSR-based SpGEMM to achieve the load balance and performance improvement on the Sunway. We, respectively, analyze the performance, including the memory footprint and the execution time, of the parallel CSR-based SpGEMM and the optimized CSR-based SpGEMM on the Sunway. The experimental results show that the optimized CSR-based SpGEMM outperforms over the parallel CSR-based SpGEMM and has good scalability on the Sunway.																	0941-0643	1433-3058				MAY	2020	32	10			SI		5571	5582		10.1007/s00521-019-04121-z													
J								Prediction of mechanical properties of micro-alloyed steels via neural networks learned by water wave optimization	NEURAL COMPUTING & APPLICATIONS										Neural networks; Water wave optimization; Meta-Lamarckian learning; Prediction of mechanical properties	FIREFLY ALGORITHM; DIFFERENTIAL EVOLUTION; GENETIC ALGORITHMS; MEMETIC ALGORITHM; SYSTEM; ARCHITECTURE	Searching optimal parameters for neural networks can be formulated as a multi-modal optimization problem. This paper proposes a novel water wave optimization (WWO)-based memetic algorithm to identify the optimal weights for neural networks. In the proposed water wave optimization-based memetic algorithm (WWOMA), we employ WWO to perform global search by both individual improvement and population co-evolution and then employ several local search components to enhance its local refinement ability. Moreover, an effective Meta-Lamarckian learning strategy is utilized to choose a proper local search component to concentrate computational efforts on more promising solutions. We carry out simulation experiments on six well-known neural network designing benchmark problems, both the simulation results and statistical comparisons demonstrate the feasibility, effectiveness and efficiency of applying WWOMA to design neural networks. Furthermore, we apply WWOMA to design neural networks and use well-trained neural networks to predict tensile strength of micro-alloyed steels. Evaluation on a practical industrial case with 2489 sample data shows that, in comparison with other algorithms, WWOMA-based neural networks can obtain notable and robust prediction accuracy, which further demonstrates that WWOMA is a promising and efficient algorithm for designing neural networks. It is worth mentioning that, to the best of our knowledge, this is the first report about applying water wave optimization to train neural networks.																	0941-0643	1433-3058				MAY	2020	32	10			SI		5583	5598		10.1007/s00521-019-04149-1													
J								Multifractal detrended fluctuation analysis parallel optimization strategy based on openMP for image processing	NEURAL COMPUTING & APPLICATIONS										Multifractal detrended fluctuation analysis; Hurst parameter; Parallel optimization; OpenMP	MULTI; TASKS	In the past few years, multifractal detrended fluctuation analysis (MF-DFA) method has been widely applied in the field of agricultural image processing. However, the agricultural image feature MF-DFA analyses involves a great deal of iterative processes and complex matrix operations, which require massive computation and processing time. In order to reduce processing time and improve analysis efficiency, we first develop a MF-DFA program that involves image preprocessing, image segmentation, local area accumulation matrix calculation, local area trend fitting, local area trend elimination, a global qth-order fluctuation function, and the Hurst index. Then, we analyze and compare MF-DFA each modules' performance characteristics and explore its parallelism according to various segmentation scales s. Lastly, we propose a parallel optimization scheme based on OpenMP for the MF-DFA. The results of our rigorous performance evaluation clearly demonstrate that our proposed parallel optimization scheme can efficiently use multicore capability to extract rape leaf image texture characteristics.																	0941-0643	1433-3058				MAY	2020	32	10			SI		5599	5608		10.1007/s00521-019-04164-2													
J								Trust-based security routing mechanism in mobile social networks	NEURAL COMPUTING & APPLICATIONS										Mobile social network; Trust value; Network security; Routing mechanism	SCHEME; ARCHITECTURES; ALGORITHM	Malicious and selfish behaviors represent a serious security threat against routing in mobile social networks (MSN). Due to MSN's unique network characteristics, such as sociability, mobility and diversity, it is a challenge to design a misbehavior detection scheme in MSN. To improve the security in MSN routing, a trust-based security routing mechanism is presented in this paper, i.e., a malicious behavior detection mechanism with identity verification scheme. The main idea is to introduce the behavior trust and the identity trust to guide the routing. Firstly, we judge a node's behavior based on both comprehensive trust and social relationship strength. And then, we forward messages according to the different measurement within and outside the friend groups. We also propose a distributed key management scheme which can issue, verify or revoke certificates based on the evaluation of the social relationship strength. We further improve the efficiency of the proposed mechanism by verifying the identity trust of the node. The simulation results indicate that the proposed routing mechanism can lower the impact of malicious behavior of nodes effectively and it has greater security performance than some classical routing algorithms do.																	0941-0643	1433-3058				MAY	2020	32	10			SI		5609	5620		10.1007/s00521-019-04167-z													
J								Parallel multi-view concept clustering in distributed computing	NEURAL COMPUTING & APPLICATIONS										Multi-view clustering; Concept factorization; Manifold learning; Distributed computing	VIEW	Multi-view clustering (MvC) is an emerging task in data mining. It aims at partitioning the data sampled from multiple views. Although a great deal of research has been done, this task remains to be very challenging. We found an important problem in performing the MvC task. MvC needs large amounts of computation. To address this problem, we propose a parallel MvC method in a distributed computing environment. The proposed method builds upon concept factorization with local manifold learning, denoted by parallel multi-view concept clustering (PMCC). Concept factorization learns a compressed representation for the data. Local manifold learning preserves the locally intrinsic geometrical structure in the data. The weight of each view is learned automatically and a cooperative normalized approach is proposed to better guide the learning of a consensus representation for all views. For the proposed PMCC architecture, the calculation of each part is independent. It is clear that our PMCC can be performed in a distributed computing environment. Experimental results using real-world datasets demonstrate the effectiveness of the proposed method.																	0941-0643	1433-3058				MAY	2020	32	10			SI		5621	5631		10.1007/s00521-019-04243-4													
J								Multi-task cascade deep convolutional neural networks for large-scale commodity recognition	NEURAL COMPUTING & APPLICATIONS										Convolutional neural networks; Object detection; Image classification; Hierarchical category tree; Tree-CNN model	ARCHITECTURE	In recent years, deep convolutional neural network have achieved remarkable performance in object detection and image classification. However, there are still some practical challenges in large-scale image recognition tasks. To be specific, the visual separability between different object categories is extremely uneven, and some categories have strong inter-class similarities. Existing CNN networks are trained as flat n-way classifiers, which is usually not sufficient to meet the challenges. Hence, we propose a framework: multi-task cascade deep convolutional neural network (MTCD-CNN), which contains two phases: object detection and hierarchical image classification, for large-scale commodity recognition. First, the object detection framework is utilized to locate and crop the areas that may contain objects. Then, hierarchical spectrum clustering is adopted to construct a category and a tree-like image classification model. During the testing phase, the indistinguishable objects are classified from coarse to fine by searching the path of the category tree. The proposed hierarchical image classification method provides an insight into the data by identifying the group of classes that are hard to classify and require more attention when compared to others. Through extensive experiments and comparative analyses of commodity detection in supermarkets and stores of Jinzhou city, the performance of MTCD-CNN has proved to be superior to other advanced methods, indicating that our proposed method has effectively solved the problem of excessive similarity of confusingly similar categories.																	0941-0643	1433-3058				MAY	2020	32	10			SI		5633	5647		10.1007/s00521-019-04311-9													
J								A monetary policy prediction model based on deep learning	NEURAL COMPUTING & APPLICATIONS										Neural network; Time series; Monetary policy; Financial risks	TIME-SERIES; NEURAL-NETWORK; ACTIVATION FUNCTION; MARKET PREDICTION; STOCK; OPTIMIZATION; ALGORITHM	Applying neural network and error t-value test, this study trains and analyzes 28 interest rate changes of China's macro-monetary policy and the mutual influences between reserve adjustments and financial markets for 51 times from 2000 to 2018 according to the data correlation between financial market and monetary policy. Through the principal component analysis, the bilateral financial risk system and data set are established, and the data set pre-process and dimensionality reduction are carried out to extract the most informative features. Six training cases are designed with processed features, and then the cases are input to each neural network model for combined prediction. Firstly, based on backpropagation neural network (BP), the forecasting model of monetary policy is established. Then, considering the importance characteristics of financial index data, expert weights based on BP, are introduced to propose weights backpropagation (WBP) model. On the basis of the timing characteristics of financial market, the WBP model is improved and the timing weights backpropagation (TWBP) model is proposed. Experiments show that different training cases bring out various effects. The accuracy rate of interest rate and reserve change value is lower than the original value after training. The mutation after data processing affects the learning of neural network. At the same time, the WBP and TWBP models improve according to the importance and timing characteristics of financial indicators have less errors in results, and the TWBP model has higher accuracy. When the number of hidden layers is 3, good results can be obtained, but in manifold training of the timing cycle, the efficiency of that is not as good as the WBP model.																	0941-0643	1433-3058				MAY	2020	32	10			SI		5649	5668		10.1007/s00521-019-04319-1													
J								An end-to-end CNN and LSTM network with 3D anchors for mitotic cell detection in 4D microscopic images and its parallel implementation on multiple GPUs	NEURAL COMPUTING & APPLICATIONS										Mitosis detection; 4D microscope data; 3D anchor; Parallel computing multi-GPUs	STEM-CELLS	The detection and observation of mitotic event are the key to studying the behavior of the cell and used to examine various diseases. The existing cell detection methods are performed on two-dimensional images with time sequence. However, the complexity of mitotic and normal cells and the orientation of the mitosis generate high false positive when using 2D methods. On the other hand, 3D methods can perform higher performance than 2D methods but also face the problem of overfitting due to the limit of training data. With those problems, we propose a 2.5-dimensional convolutional neural network with convolutional long short-term memory to extract the information time sequence and combined with 3D anchors to gather the spatial information for final mitotic detection. Furthermore, we also propose the method with a parallel model on multi-GPUs to speed up the detection time. Compared with state-of-the-art methods, our method can reach high precision and also recall rate with detection time is speed up about 1.9 times by the use of the parallel model on 4GPUs.																	0941-0643	1433-3058				MAY	2020	32	10			SI		5669	5679		10.1007/s00521-019-04374-8													
J								A reformed task scheduling algorithm for heterogeneous distributed systems with energy consumption constraints	NEURAL COMPUTING & APPLICATIONS										Energy constraint; Distributed computing; Heterogeneous computing; Parallel computing; Task scheduling	RELIABILITY	As the scale increases and performance improves, the energy consumption of high-performance computer systems is rapidly increasing. The energy-aware task scheduling for high-performance computer systems has become a hot spot for major supercomputing centers and data centers. In this paper, we study the task scheduling problem to minimize the schedule length of parallel applications while satisfying the energy constraints in heterogeneous distributed systems. The existing approaches mainly allocate unassigned tasks with minimal energy consumption which cannot achieve optimistic scheduling length in most cases. Based on this situation, we propose a reformed scheduling method with energy consumption constraint algorithm, which is based on an energy consumption level to pre-allocate energy consumption for unassigned tasks. The experimental results show that compared with the existing algorithms, our new algorithm can achieve better scheduling length under the energy consumption constraints.																	0941-0643	1433-3058				MAY	2020	32	10			SI		5681	5693		10.1007/s00521-019-04415-2													
J								Hierarchical attributes learning for pedestrian re-identification via parallel stochastic gradient descent combined with momentum correction and adaptive learning rate	NEURAL COMPUTING & APPLICATIONS										Parallel SGD; Pedestrian attributes; Parameter exchanging; Pedestrian re-identification; CNN	RECOGNITION; FEATURES; CASCADE	Convolutional neural networks (CNNs) have obtained high accuracy results for pedestrian re-identification in the past few years. There is always a trade-off between high accuracy and computational time in CNNs. Training CNN is always very difficult as it may take a long time to produce high accuracy results. To overcome this limitation, a novel method parallel stochastic gradient descent (PSGD) is proposed to train a five-hierarchical parallel CNNs that is designed according to pedestrian attributes. Moreover, the momentum correction and adaptive adjustment of learning rate are applied during training process and the time interval for updating parameters is inspected during optimization of parameters selection. The results of this paper prove the effectiveness of proposed PSGD that successfully decreases the training process by five times and surpasses the state-of-the-art methods of pedestrian re-identification in terms of both accuracy and time. The minimum reported running time of the proposed method is 8.7 s which is minimum among all other state-of-the-art methods. These promising results show the efficiency and performance of the proposed model.																	0941-0643	1433-3058				MAY	2020	32	10			SI		5695	5712		10.1007/s00521-019-04485-2													
J								Research on an olfactory neural system model and its applications based on deep learning	NEURAL COMPUTING & APPLICATIONS										Olfactory neural system; EEG recognition; KIII model; Deep learning	RECOGNITION; PERFORMANCE; BULB	The idea of constructing the biological neural system model as realistic as possible can not only provide a new artificial neural network (ANN), but also offer an effective object to study biological neural systems. As a very meaningful attempt about the idea, a bionic model of olfactory neural system, KIII model, is introduced in this paper. There are the unique characteristics of KIII model different from those of general ANNs. The KIII model realistically simulates the structure of the real olfactory neural system and the process of odor molecules gradually transformed by the core components of the olfactory system including olfactory receptor, olfactory bulb and olfactory cortex. The neuron model of the KIII model is constructed and optimized based on neurophysiological experimental data and accurately reflects the response of olfactory neurons to odor stimulation. In particular, the noise introduced to KIII model can further improve the performance of the model. In addition, the KIII model is analyzed based on the idea of deep learning. The qualitative analysis shows that there are obvious similarities between the KIII model and the deep learning model. Furthermore, with the epileptic electroencephalograph (EEG) recognition task, two groups of experiments are designed to comprehensively analyze the performance of the KIII model. In the first group of experiments, a typical pattern recognition experiment with feature extraction stage is shown. The features of epileptic EEG were extracted based on Empirical Mode Decomposition (EMD), and the KIII model was used as a classifier. The experimental results show that the KIII model only needs a small number of iterations to memorize different modes and has a high recognition rate, over 91%. In the second group of experiments, a direct recognition experiment without feature extraction stage is shown. The original epileptic EEG signals as KIII model inputs directly were recognized. The experimental results show that there is still an excellent performance in the KIII model, over 96%, and the recognition result is similar to the characteristics of the deep learning model. The theoretical analysis and experimental results prove that KIII model with the idea of deep learning is an excellent bionic model of olfactory neural system and gets a good balance between high bionics and good performance, which is a good reference for related research.																	0941-0643	1433-3058				MAY	2020	32	10			SI		5713	5724		10.1007/s00521-019-04498-x													
J								Fingerprint pattern identification and classification approach based on convolutional neural networks	NEURAL COMPUTING & APPLICATIONS										Convolutional neural network; Fingerprint; Identification; Pattern feature		Fingerprint pattern recognition and classification can be of assistance in the research on human personality. In some previous studies, fingerprints were classified into four categories to speed up recognition, but the method of that classification is not suitable for researching the diversity of human personalities. Therefore, in this paper, fingerprint patterns were classified into six types and the accuracy of the recognition was improved to facilitate the research on human personality characteristics. Based on this idea, a six-category fingerprint database is annotated manually and a convolutional neural network (CNN) is proposed for identifying real fingerprint patterns. The new CNN consists of four convolutional layers, three max-pooling layers, two norm layers, and three fully connected layers. The best accuracy the model achieved was 94.87% for a six-category fingerprint database and 92.9% accuracy for a four-category fingerprint database. The results of experimental tests show that the proposed model can recognize the pattern features from a large fingerprint database using the automatic learning and feature extraction abilities of the CNN to get a greater accuracy than in previous experiments.																	0941-0643	1433-3058				MAY	2020	32	10			SI		5725	5734		10.1007/s00521-019-04499-w													
J								Balanced training of a hybrid ensemble method for imbalanced datasets: a case of emergency department readmission prediction	NEURAL COMPUTING & APPLICATIONS										Class imbalance; Hospital readmission; Ensemble learning; Extreme learning machine	EXTREME LEARNING MACHINES; CLASSIFICATION; TRENDS	Dealing with imbalanced datasets is a recurrent issue in health-care data processing. Most literature deals with small academic datasets, so that results often do not extrapolate to the large real-life datasets, or have little real-life validity. When minority class sample generation by interpolation is meaningless, the recourse to undersampling the majority class is mandatory in order to reach some acceptable results. Ensembles of classifiers provide the advantage of the diversity of their members, which may allow adaptation to the imbalanced distribution. In this paper, we present a pipeline method combining random undersampling with bootstrap aggregation (bagging) for a hybrid ensemble of extreme learning machines and decision trees, whose diversity improves adaptation to the imbalanced class dataset. The approach is demonstrated on a realistic greatly imbalanced dataset of emergency department patients from a Chilean hospital targeted to predict patient readmission. Computational experiments show that our approach outperforms other well-known classification algorithms.																	0941-0643	1433-3058				MAY	2020	32	10			SI		5735	5744		10.1007/s00521-017-3242-y													
J								Neural controller for the smoothness of continuous signals: an electrical grid example	NEURAL COMPUTING & APPLICATIONS										Artificial neural network; Multi-agent system; distributed coordination; Demand-side management	DEMAND-SIDE MANAGEMENT; PV SELF-CONSUMPTION; NETWORK; SYSTEM	In this paper, the use of artificial neural networks (ANNs) is proposed to manage the local demand of different electric grid elements to smooth their aggregated consumption. The ANNs are based on the load automation of the local electric behavior, following a local strategy but affecting to the global system. In an electrical grid, there is no possibility to share information between the users because anonymity must be warranted. Therefore, a solution to the problem is elaborated with the minimum information possible without the need for communication between the users. A grid environment and behavior of different users is simulated.																	0941-0643	1433-3058				MAY	2020	32	10			SI		5745	5760		10.1007/s00521-019-04139-3													
J								Mathematical modeling and intelligent optimization of submerged arc welding process parameters using hybrid PSO-GA evolutionary algorithms	NEURAL COMPUTING & APPLICATIONS										Hybrid algorithms; Intelligent optimization; Mathematical modeling; PSO-GA; Submerged arc welding	PARTICLE SWARM OPTIMIZATION; GENETIC ALGORITHM; TAGUCHI METHOD; BEAD GEOMETRY; PREDICTION; STEEL	Now-a-days, submerged arc welding processes (SAW) are immensely being applied for joining the thick plates and surfacing application. However, the selection of optimal SAW process parameters is indeed an intricate task which aims to accomplish the desired quality of welded part at an economic way. Therefore, in the present paper, the research efforts are made on an implementation of efficient hybrid intelligent algorithms, i.e., hybrid particle swarm optimization and genetic algorithm (hybrid PSO-GA) for the optimization of SAW process parameters. The emphasis was given on different direct parameters such as voltage, wire feed rate, welding speed and nozzle to plate distance and indirect parameters such as flux condition and plate thickness, respectively. The parameters were chosen at two levels using fractional factorial design to study their effect on responses including flux consumption, metal deposition rate and heat input. Besides, the linear regression technique and analysis of variance were used for mathematical modeling of each response. Then, the direct effect and interaction effect on selected responses were investigated by 3D surface plots. At the end, the performance of hybrid PSO-GA is compared with general PSO and GA algorithms for indices including success rate, best solution, mean, computational time, standard deviation and mean absolute percentage error between. The overall results suggested that the hybrid PSO-GA is better option than other two algorithms, i.e., PSO and GA for obtaining the optimum SAW process parameters.																	0941-0643	1433-3058				MAY	2020	32	10			SI		5761	5774		10.1007/s00521-019-04404-5													
J								Detection and segmentation of iron ore green pellets in images using lightweight U-net deep learning network	NEURAL COMPUTING & APPLICATIONS										Iron ore pellets; U-net deep neural network; Image segmentation; Particle size distribution	PARTICLE-SIZE DISTRIBUTION; ALGORITHM; SYSTEM	In steel manufacturing industry, powdered iron ore is agglomerated in a pelletizing disk to form iron ore green pellets. The agglomeration process is usually monitored using a camera. As pellet size distribution is one of the major measures of product quality monitoring, pellets detection and segmentation from the image are the key steps to determine the pellet size. Traditional image processing algorithms are not only challenged by the complicated constitution of pellets, sediment and residuals in the image, but also by the harsh and unbalanced light reflection on the pellet centrum area and the background which results in tedious parameter adjustment work and pool performance. To solve these problems, we design a lightweight U-net deep learning network to automatically detect pellets from images and to obtain the probability maps of pellet contours. Compared to classic U-net, the proposed network has fewer parameters and introduces batch normalization layers, which greatly reduces the computing time and improves generalization ability of the network. A concentric circle model is then used to separate clumped contours of the pellets, and the pellets shapes are detected via ellipse fitting. The proposed method is verified using images captured from an industrial pelletizing disk, and its performance is compared with traditional methods and the classic U-net. Results show that the proposed method achieves better segmentation performance in DICE and ROC indexes and shows good robustness to uneven illumination. Tests on temporal image sequences demonstrate that the proposed method is effective in monitoring the pellet size distribution and the pellet shape as well. Results of this work have potential usage in online detection of iron ore green pellets and other types of particles.																	0941-0643	1433-3058				MAY	2020	32	10			SI		5775	5790		10.1007/s00521-019-04045-8													
J								A self-adaptive estimation of distribution algorithm with differential evolution strategy for supermarket location problem	NEURAL COMPUTING & APPLICATIONS										In-plant material delivery; Supermarket; Location; Estimation of distribution algorithm	GENETIC ALGORITHM; OPTIMIZATION ALGORITHM	In modern production systems, an ever-rising product variety has imposed great challenges for in-plant part supply systems used to feed mixed-model assembly lines with required parts. In recent years, many automotive manufacturers have identified the supermarket concept as an efficient part feeding strategy to enable JIT (Just-in-time) deliveries at low costs. This paper studies a discrete supermarket location problem which considers the utilization rate and capacity constraint of the supermarkets simultaneously. Firstly, a mathematical model is developed with the objective of minimizing the total system cost consisting of operating cost and transportation cost. Then, a self-adaptive estimation of distribution algorithm with differential evolution strategy, named DE/AEDA, is proposed to solve the problem. Finally, computational experiments are carried out to analyze the performance of the proposed algorithm compared with the benchmark algorithm by using a non-parametric test method. The results indicate that the proposed algorithm is valid and efficient.																	0941-0643	1433-3058				MAY	2020	32	10			SI		5791	5804		10.1007/s00521-019-04052-9													
J								Ultimate boundedness of discrete stochastic time-delay systems with logic impulses	NEURAL COMPUTING & APPLICATIONS										Discrete stochastic system; Logic; Lyapunov function; Ultimate boundedness	STABILITY ANALYSIS; NEURAL-NETWORKS; DIFFERENTIAL-EQUATIONS; DYNAMICS; STABILIZATION	In this paper, we introduce a discrete stochastic time-delay system with impulses suffered by logic choice and a pth moment (mu,rho)-ultimate boundedness concept for this new system. Based on the pth moment ultimate boundedness concept, we give some pth moment (mu,rho)-ultimate boundedness criteria for this new system. In the end, we illustrate the effectiveness of the criterion by an example.																	0941-0643	1433-3058				MAY	2020	32	10			SI		5805	5813		10.1007/s00521-019-04054-7													
J								Collaborative representation-based discriminant neighborhood projections for face recognition	NEURAL COMPUTING & APPLICATIONS										Collaborative representation; Manifold learning; Dimensionality reduction; Discriminant learning; Face recognition	NONLINEAR DIMENSIONALITY REDUCTION; SPARSE REPRESENTATION; IMAGE CLASSIFICATION; EIGENFACES; MANIFOLD	Manifold learning as an efficient dimensionality reduction method has been extensively used. However, manifold learning suffers from the problem of manual selection of parameters, which seriously affects the algorithm performance. Recently, applications of collaborative representation have received concern in some fields such as image processing and pattern recognition research. Based on manifold learning and collaborative representation, this paper develops a new algorithm for feature extraction, which is called collaborative representation-based discriminant neighborhood projections (CRDNP). In CRDNP, we first construct intra-class and inter-class neighborhood graphs of the input data as well as a weight matrix based on collaborative representation model and class label information. Then, a projection to a reduced subspace is obtained by margin maximization between the between-class neighborhood scatter and within-class neighborhood scatter. CRDNP not only characters the inherent geometry relationship of the dataset using L2-graph, but also enhances the between-class submanifold separability. In addition, the discriminating capability of CRDNP is further improved by obtaining the orthogonal projection vectors. Experiment results on public face datasets prove that CRDNP can achieve more accurate results compared with the existing related algorithms.																	0941-0643	1433-3058				MAY	2020	32	10			SI		5815	5832		10.1007/s00521-019-04055-6													
J								Fuzzy logic-based modelling of yield strength of as-cast A356 alloy	NEURAL COMPUTING & APPLICATIONS										Fuzzy logic; Membership functions; Artificial neural networks; Prediction accuracy; Mechanical properties prediction; A356 alloy; Cast components	MECHANICAL-PROPERTIES; PARAMETERS; PREDICT; BEHAVIOR; ERROR	Uncertain and imprecise data are inherent to many domains, e.g. casting lightweight components. Fuzzy logic offers a way to handle such data, which makes it possible to create predictive models even with small and imprecise data sets. Modelling of cast components under fatigue load leads to understanding of material behaviour on component level. Such understanding is important for the design for minimum warranty risk and maximum weight reduction of lightweight cast components. This paper contributes with a fuzzy logic-based approach to model fatigue-related mechanical properties of as-cast components, which has not been fully addressed by the current research. Two fuzzy logic models are constructed to map yield strength to the chemical composition and the rate of solidification of castings for two A356 alloys. Artificial neural networks are created for the same data sets and then compared to the fuzzy logic approach. The comparison shows that although the neural networks yield similar prediction accuracy, they are less suitable for the domain because they are opaque models. The prediction errors exhibited by the fuzzy logic models are 3.53% for the model and 3.19% for the second, which is the same error level as reported in related work. An examination of prediction errors indicated that these are affected by parameters of the membership functions of the fuzzy logic model.																	0941-0643	1433-3058				MAY	2020	32	10			SI		5833	5844		10.1007/s00521-019-04056-5													
J								Pedestrian detection via deep segmentation and context network	NEURAL COMPUTING & APPLICATIONS										Pedestrian detection; Segmentation information; Context information; Multi-channel feature; Deep network		For pedestrian detection, many deep learning approaches have shown effectiveness, but they are not accurate enough for the positioning of obstructed pedestrians. A novel segmentation and context network (SCN) structure is proposed that combines the segmentation and context information for improving the accuracy of bounding box regression for pedestrian detection. The SCN model contains the segmentation sub-model and the context sub-model. For separating the pedestrian instance from the background and solving the pedestrian occlusion problem, this paper uses the segmentation sub-model for extracting pedestrian segmentation information to generate more accurate pedestrian regions. Considering that different pedestrian instances need different context information, this paper uses context regions with different scales to extract context information. For improving the detection performance, this paper uses the hole algorithm in the context sub-model to expand the receptive field of the output feature maps and connect the multi-channel features with the skip layer. Finally, the loss functions of the two sub-models outputs are fused. The experimental results on different datasets validate the effectiveness of our SCN model, and the deeply supervised algorithm has a good trade-off between accuracy and complexity.																	0941-0643	1433-3058				MAY	2020	32	10			SI		5845	5857		10.1007/s00521-019-04057-4													
J								Tree-structured multilayer neural network for classification	NEURAL COMPUTING & APPLICATIONS										Tree-structured NN; Deep multilayer neural network; Genetic algorithm		In traditional neural trees (NTs), each internal node is designed as a neural network (NN), such as single- or two-layer neural networks, to determine which branch should be followed for an input sample. Because each NN contained in the internal nodes is designed separately, the produced NT does not consider overall effectiveness. Thus, the designed NT is usually not an optimal NT. In this study, the tree-structured multilayer neural network (TSMLNN) is proposed for classification. The TSMLNN is similar to an NT, which is the result of dividing a deep multilayer NN into many small sub-networks. The TSMLNN has the advantages of both a multilayer NN and an NT. In addition, the split method is proposed to determine how to split the network in the TSMLNN. The genetic algorithm is proposed to automatically search for the weights, activation threshold of each node and the proper number of nodes at each layer according to both the computing complexity and classification error rate in the TSMLNN, and the proposed TSMLNN tends to be optimal. A heuristic method is also proposed to help users to decide which TSMLNN is the best within the classification error rate range. Finally, the performance of the proposed TSMLNN is compared with that of state-of-the-art neural networks in experiments.																	0941-0643	1433-3058				MAY	2020	32	10			SI		5859	5873		10.1007/s00521-019-04058-3													
J								A novel (U)MIDAS-SVR model with multi-source market sentiment for forecasting stock returns	NEURAL COMPUTING & APPLICATIONS										Mixed-frequency data; Support vector regression; (U)MIDAS-SVR; Market sentiment	MIDAS REGRESSIONS; NEURAL-NETWORK; INFORMATION; BEHAVIOR; SINGLE; MATTER; NEWS; OIL	From the point view of behavioral finance, market sentiment plays an important role in forecasting stock returns. How to accurately measure the impact of market sentiment is a challenge work. Two issues on nonlinear relationship and mixed-frequency data have to be addressed. To this end, we introduce methods of mixed-frequency data into SVRs and develop a novel (U)MIDAS-SVR model. It can be estimated by solving the Lagrange duality technique of quadratic programming. We then apply the (U)MIDAS-SVR model to predict weekly returns of SHSE and SZSE in China using the mixed-frequency market sentiment as covariates. The empirical results show that the (U)MIDAS-SVR model is promising and MIDAS-SVR is superior to those competing models in terms of MAE and RMSE. In addition, we design seven scenarios by considering different data source combinations and find that the multi-source market sentiment is helpful to improve forecasting performance on stock returns.																	0941-0643	1433-3058				MAY	2020	32	10			SI		5875	5888		10.1007/s00521-019-04063-6													
J								Transfer learning features for predicting aesthetics through a novel hybrid machine learning method	NEURAL COMPUTING & APPLICATIONS										Convolutional neural networks; Feature extraction; Machine learning; Prediction; Classification; Aesthetics assessment; Hybrid model; Transfer learning	NEURAL-NETWORKS; PHOTO	The automatic assessment of the aesthetic value of an image is a task with many applications but really complex and challenging, due to the subjective component of the aesthetics for humans. The computational systems that carry out this task are usually composed of a set of ad hoc metrics proposed by the researchers and a machine learning system. We propose a new approach that fully automates the metrics creation process, its filtering and adjustment without human subjectivity. Thus, it does not depend on the authors' human aesthetic intuitions. Our proposal is therefore based on the integration of two machine learning algorithms: CNN, which works as a feature extractor, and Correlation by Genetic Search (CGS)-a novel regression method, working as a supervised learning method. CGS is based on the creation of an adjusted linear regression model using Pearson's correlation as a measure of performance in an evolutionary process. Experiments were conducted on a very well-known aesthetics database called "Photo.net" with more than a million images from over 400,000 users. The comparison of results with other approaches using the same dataset demonstrates that the fusion of CNN transfer learning features with this specific machine learning method has achieved robust and significantly better results than other state-of-the-art methods and hybrid approaches in terms of AUROC (0.93), accuracy (0.93) and Pearson's correlation value (0.94).																	0941-0643	1433-3058				MAY	2020	32	10			SI		5889	5900		10.1007/s00521-019-04065-4													
J								Amelioration of task scheduling in cloud computing using crow search algorithm	NEURAL COMPUTING & APPLICATIONS										Algorithms; Cloud computing; Task scheduling; Crow search algorithm; Nature-inspired		Cloud computing is a dynamic and diverse environment across different geographical locations. In reality, it consists of a vast number of tasks and computing resources. In cloud, task scheduling algorithm is the core player which identifies the suitable virtual machine (VM) for a task. The task scheduling algorithm is responsible for reducing the makespan of the schedule. In recent years, nature-inspired algorithms are applied to task scheduling which performs better than conventional algorithms. In this paper, crow search algorithm (CSA) is proposed for task scheduling in cloud. It is inspired from the food collecting habits of crow. In reality, the crow keeps on eyeing on its other mates to find a better food source than current food source. In this way, the CSA finds a suitable VM for the task and minimizes the makespan. Experiments are carried out using cloudsim to measure the performance of the CSA along with Min-Min and ant algorithms. Simulation results reveal that CSA algorithm performs better compared to Min-Min and Ant algorithms.																	0941-0643	1433-3058				MAY	2020	32	10			SI		5901	5907		10.1007/s00521-019-04067-2													
J								Statistical modelling and parametric optimization in document fragmentation	NEURAL COMPUTING & APPLICATIONS										Parametric optimization; Fragmentation; Taguchi; Analysis of variance		In recent days, most of the business enterprises and individuals are attracted towards cloud computing due to its cost efficiency and scalability. Though the cloud adoption is significant, its security has become nightmare due to its multi-tenancy property. Generally cloud service providers commit to ensure data reliability and security, but they may get depleted due to the rapid growth rate of cloud customers. To overcome the security issues and to protect the documents uploaded in cloud, cryptography is more used. Data security can further be improved with a technique called fragmentation which helps in outsourcing data partitions instead of entire document. The fragmentation becomes a difficult and time-consuming process when the size of document grows. In this paper, an efficient fragmentation process with virtualization is proposed. CPU cycles are efficiently used by the generation of VMs which reduce the time complexity of fragmentation process. The factors such as document size, processor capacity, storage capacity and number of VMs are taken into consideration to analyse their influence on the fragmentation time. Healthcare documents' fragmentation process is conducted, and measured real-time values are analysed statistically. For experimentation purpose, a private cloud OpenStack on Oracle virtual box is used. Taguchi technique (L27 orthogonal array) is employed to find the optimum levels of the parameters on the fragmentation time, while analysis of variance is used to analyse the contribution of the parameters towards the performance of fragmentation process. Results reveal that the document size is the most dominant factor influencing the fragmentation time followed by processor speed. By parallelizing the fragmentation process with the help of multiple VMs, the time complexity of the process gets reduced.																	0941-0643	1433-3058				MAY	2020	32	10			SI		5909	5918		10.1007/s00521-019-04068-1													
J								Improved social spider optimization algorithm for optimal reactive power dispatch problem with different objectives	NEURAL COMPUTING & APPLICATIONS										Improved social spider optimization; Optimal reactive power dispatch; Power loss; Voltage deviation; Voltage stabilization index	PARTICLE SWARM OPTIMIZATION; EVOLUTION ALGORITHM; SEARCH ALGORITHM	This paper proposes an improved social spider optimization (ISSO) for achieving different objectives of optimal reactive power dispatch (ORPD). The proposed ISSO method is developed by applying two modifications on new solution generation process. The proposed method uses only one modified equation for producing the first new solution generation and the second new solution generation while the standard SSO uses two equations for each process. The improvement in the proposed method is confirmed by solving benchmark optimization functions, IEEE 30-bus system and IEEE 118-bus system. Obtained results from ISSO are compared to those from other existing methods available in other studies together with other popular and state-of-the-art methods, which are implemented in the work. As compared to standard SSO for application to ORPD problem, ISSO can reduce the number of computation steps and one control parameter, and shorten simulation time. About the result comparisons with SSO and other remaining methods, ISSO can find more favorable solutions with higher quality and ISSO can stabilize solution search function with approximately all trial runs finding lower value of fitness. Furthermore, the strong search ability of ISSO is also indicated because it uses less value for control parameters. As a result, the proposed ISSO method can be a very effective optimization tool for dealing with the ORPD problem.																	0941-0643	1433-3058				MAY	2020	32	10			SI		5919	5950		10.1007/s00521-019-04073-4													
J								Ensemble feature selection for high-dimensional data: a stability analysis across multiple domains	NEURAL COMPUTING & APPLICATIONS										Feature selection; Stability of feature selection algorithms; Ensemble approaches; High-dimensional data analysis	ROBUST FEATURE-SELECTION; CLASSIFICATION; SIMILARITY; DIAGNOSIS; SYSTEM	Selecting a subset of relevant features is crucial to the analysis of high-dimensional datasets coming from a number of application domains, such as biomedical data, document and image analysis. Since no single selection algorithm seems to be capable of ensuring optimal results in terms of both predictive performance and stability (i.e. robustness to changes in the input data), researchers have increasingly explored the effectiveness of "ensemble" approaches involving the combination of different selectors. While interesting proposals have been reported in the literature, most of them have been so far evaluated in a limited number of settings (e.g. with data from a single domain and in conjunction with specific selection approaches), leaving unanswered important questions about the large-scale applicability and utility of ensemble feature selection. To give a contribution to the field, this work presents an empirical study which encompasses different kinds of selection algorithms (filters and embedded methods, univariate and multivariate techniques) and different application domains. Specifically, we consider 18 classification tasks with heterogeneous characteristics (in terms of number of classes and instances-to-features ratio) and experimentally evaluate, for feature subsets of different cardinalities, the extent to which an ensemble approach turns out to be more robust than a single selector, thus providing useful insight for both researchers and practitioners.																	0941-0643	1433-3058				MAY	2020	32	10			SI		5951	5973		10.1007/s00521-019-04082-3													
J								Neural network approach based on a bilevel optimization for the prediction of underground blast-induced ground vibration amplitudes	NEURAL COMPUTING & APPLICATIONS										Underground; Excavation; Blast; Ground vibrations; Bilevel optimization; Neural network	FREQUENCY; VELOCITY	Due to its technical and economic advantages, the use of explosives in underground rock excavation is widely adopted. However, some safety and, especially, environmental issues arise when using this technique, mainly concerning ground vibrations induced by blasts. Thus, to minimize dynamic environmental impacts, prediction of blast-induced vibrations is imperative. In the last few years, artificial neural networks (ANNs) have been applied to model blast-induced ground vibrations. Nevertheless, ANN's architecture, mainly the number of neurons in the hidden layer, has been selected manually concerning ANN's performance parameters. To avoid over-fitting and reduce model's complexity, this paper presents a bilevel optimization of ANN architecture, considering two transfer functions, based on the maximization of quality of the adjustment and model's complexity, this last one as a penalty criterion. An ANN approach based on this bilevel optimization was successfully applied on a database of 1188 samples obtained from underground blast-induced ground vibration monitoring in a granitic rock mass. A residual analysis of the best-fitted model is performed to ensure the quality of the adjustment. It is demonstrated that the determined ANN model offers much higher generalization ability than the traditional prediction models usually used for blast-induced ground vibration amplitude predictions and other ANN architectures applied to ground vibration prediction.																	0941-0643	1433-3058				MAY	2020	32	10			SI		5975	5987		10.1007/s00521-019-04083-2													
J								Lagrangian twin parametric insensitive support vector regression (LTPISVR)	NEURAL COMPUTING & APPLICATIONS										Machine learning; Prediction; Support vector regression; Twin support vector regression; Parametric insensitive model	STATISTICAL COMPARISONS; CLASSIFIERS; MACHINE	In this paper, motivated by the works on twin parametric insensitive support vector regression (TPISVR) (Peng in Neurocomputing 79(1):26-38, 2012), and Lagrangian twin support vector regression (Balasundaram and Tanveer in Neural Comput Appl 22(1):257-267, 2013), a new efficient approach is proposed as Lagrangian twin parametric insensitive support vector regression (LTPISVR). In order to make the objective function strongly convex, we consider square of 2-norm of slack variables in the optimization problem. To reduce the computation cost, the solution of proposed LTPISVR is obtained by solving simple linearly convergent iterative schemes, instead of quadratic programming problems as in TPISVR. There is no requirement of any optimization toolbox for proposed LTPISVR. To demonstrate the effectiveness of proposed method, we present numerical results on well-known synthetic and real-world datasets. The results clearly show similar or better generalization performance of proposed method with lesser training time in comparison with support vector regression, twin support vector regression and twin parametric insensitive support vector regression.																	0941-0643	1433-3058				MAY	2020	32	10			SI		5989	6007		10.1007/s00521-019-04084-1													
J								Two-dimensional joint local and nonlocal discriminant analysis-based 2D image feature extraction for deep learning	NEURAL COMPUTING & APPLICATIONS										Face recognition; Feature extraction; Manifold learning; Two-dimensional projection	FACE RECOGNITION; PRESERVING PROJECTIONS; DESCRIPTORS; NETWORKS; PCA	This paper proposes a new two-dimensional manifold learning algorithm called two-dimensional joint local/nonlocal discriminant analysis (2DJLNDA) for 2D image feature extraction, which directly extracts projective vectors from 2D image matrices rather than image vectors. Different from other typical 2D methods, e.g., two-dimensional principal component analysis (2DPCA), two-dimensional linear discriminative analysis (2DLDA), two-dimensional locality-preserving projection (2DLPP), 2DJLNDA preserves not only local/nonlocal intrinsic structure but also local/nonlocal penalization structure of the image data in the high-dimensional space, which can be powerful in extracting intrinsic information of the image data in the low-dimensional space. The experimental results on the ORL, Yale, AR and UMIST face datasets indicate that 2DJLNDA is capable of extracting effective image features and outperforms 2DPCA, 2DLDA and 2DLPP. The 2D image features extracted by 2DJLNDA further improve the performance of deep neural networks (DNNs), e.g., stacked denoising autoencoder, and convolutional neural network (CNN) significantly. These studying results illustrate that the feature face images will provide more discriminant features than the original face images for DNNs. Therefore, 2DJLNDA-based 2D feature image extraction can be used as an effective preprocessing of DNNs (e.g., CNN) for face recognition.																	0941-0643	1433-3058				MAY	2020	32	10			SI		6009	6024		10.1007/s00521-019-04085-0													
J								High-accuracy numerical methods for a parabolic system in air pollution modeling	NEURAL COMPUTING & APPLICATIONS										Air pollution model; Semilinear parabolic systems; Compact finite difference schemes; Richardson extrapolation	RICHARDSON EXTRAPOLATION; SCHEMES; EQUATION	We present two approaches for enhancing the accuracy of the second-order finite difference approximations of two-dimensional semilinear parabolic systems. These are the fourth-order compact difference scheme and the fourth-order scheme based on Richardson extrapolation. Our interest is concentrated on a system of ten parabolic partial differential equations in air pollution modeling. We analyze numerical experiments to compare the two approaches with respect to accuracy, computational complexity, nonnegativity preserving, etc. The sixth-order approximation based on the fourth-order compact difference scheme combined with Richardson extrapolation is also discussed numerically.																	0941-0643	1433-3058				MAY	2020	32	10			SI		6025	6040		10.1007/s00521-019-04088-x													
J								Efficient dynamic performance of brushless DC motor using soft computing approaches	NEURAL COMPUTING & APPLICATIONS										Brushless DC (BLDC) motor; PI-speed controller; Soft computing methods; Landsman converter; Photovoltaic (PV)-battery hybrid system	PID CONTROL; ALGORITHM; OPERATION; SYSTEM	A novel attempt to employ moth swarm algorithm (MSA) to generate the optimal gains of a proportional-integral (PI) speed controller of brushless DC (BLDC) motor is addressed to assure its satisfactory dynamic performance. For torque ripples minimization, a dual-loop speed controller is adapted. The agreed objective function is formulated to minimize the integral time absolute speed error (ITAE) subjects to set of constraints. The effectiveness of the MSA is verified through many test cases along with the detailed comparisons to those obtained by well known genetic algorithm and particle swarm optimization. At this stage, the numerical results of the MSA are used to train and test an artificial neural network which shall be used as an adaptive controller to give the optimal PI gains under different operating conditions. At final stage, the performance of the BLDC motor powered from photovoltaic (PV)-battery hybrid system with the proposed controller is demonstrated. A Landsman converter is controlled by an incremental conductance technique to maximize the extracted PV array power. A bidirectional converter is used to control battery charging/discharging states. Various demonstrated case studies indicate that the MSA is effective in generating the optimal gains of the PI controller.																	0941-0643	1433-3058				MAY	2020	32	10			SI		6041	6054		10.1007/s00521-019-04090-3													
J								Empirical approach for bearing capacity prediction of geogrid-reinforced sand over vertically encased stone columns floating in soft clay using support vector regression	NEURAL COMPUTING & APPLICATIONS										Geosynthetics; Support vector regression (SVR); Adaptive neuro-fuzzy inference system (ANFIS); Bearing capacity; Vertically encased stone columns; Geogrid-reinforced sand bed (GRSB)	SHALLOW FOUNDATIONS; FUZZY-LOGIC; NEURAL-NETWORKS; MODEL TESTS; MACHINES; SYSTEMS; ANFIS; IDENTIFICATION; PLATFORMS; MODULUS	Due to the complex, elaborate and expensive estimation of bearing capacity (q(rs)) of geogrid-reinforced sand bed resting over a group of vertically encased stone columns floating in soft clay, it is required to develop a precise empirical model, which is supposed to be nonlinear. To date, there is no established bearing capacity equation available on this topic. The aim of this work is to develop a precise q(rs) prediction model using support vector regression (SVR) technique. A total of 245 experimental datasets were collected and used to train and test the SVM models estimating the q(rs). Three SVR models were developed based on three different kernel functions, namely exponential radial basis kernel function (ERBF), radial basis kernel function (RBF) and polynomial kernel function (POLY), and their performances were examined. Out of the three SVR models, one with ERBF was found to be the best one, having the lowest statistical error and maximum generalization ability of the training and testing data. The performance of SVR-ERBF model was compared with adaptive neuro-fuzzy inference system (ANFIS) model, and it was observed that SVR-ERBF model outperforms ANFIS model to predict q(rs). A sensitivity analysis was also conducted to identify the relative importance and contribution of each input variable on output (q(rs)) prediction. Finally, using the SVR-ERBF model, an empirical equation is proposed to predict q(rs) for practical application purposes. Obtained results approve that the SVR-ERBF model can be used as a powerful and reliable alternative to solve highly nonlinear problems such as indirect estimation of q(rs).																	0941-0643	1433-3058				MAY	2020	32	10			SI		6055	6074		10.1007/s00521-019-04092-1													
J								SPN: short path network for scene text detection	NEURAL COMPUTING & APPLICATIONS										Scene text detection; Multiple scales; Short path network; Feature representation	VIDEO; RECOGNITION	Although tremendous strides are boosted in scene text detection, one of the remaining open challenges is to detect multi-scale text. The existing text detection methods based on deep neural network address the challenge using multiple size filters, more feature layers with different scales, and multi-scale testing. In this paper, we propose a novel text detector, named short path network (SPN), with superior performance in both accuracy and efficiency. Different from the strategy of increasing feature layers and receptive fields, SPN focuses on alleviating the loss of text feature transmission and enhancing the representation of text feature in each scale layer. Specifically, SPN can use the distilled high-semantic features to complement the propagated loss of deep features by establishing the short feature path in a deep-to-shallow manner. Besides, by adding the short supervision path in a shallow-to-deep manner, SPN can ease the errors between the outputs of each scale and the corresponding ground truths. The proposed method achieves the state-of-the-art performance on three scene text benchmarks, in terms of F-score and frame per second (FPS). Specifically, SPN can run at 11.7 FPS on the ICDAR 2015 incidental text dataset with 84.30% F-score, 16.9 FPS on the COCO-Text dataset with 44.20% F-score, and 12.1 FPS on the ICDAR 2013 video text dataset with 67.28% F-score. The code of SPN will be available for public.																	0941-0643	1433-3058				MAY	2020	32	10			SI		6075	6087		10.1007/s00521-019-04093-0													
J								On large appearance change in visual tracking	NEURAL COMPUTING & APPLICATIONS										Superpixel-based appearance model; Spatial objectness cues; Complementary feature set; Adaptive online update	ROBUST OBJECT TRACKING	This paper concerns on overcoming the challenges caused by drastic appearance change in visual tracking, especially the long-term appearance variation due to occlusion or large object deformation. We aim to build a long-term appearance model for robust tracking against large appearance change in two new respects: using historical and distinguishing cues to model target representation and extracting effective spatial objectness features from each frame to distinguish outliers. For the first purpose, an adaptive superpixel-based appearance model is formulated. Different from previous superpixel-based trackers, a complementary feature set is defined for the update model to preserve the features of those temporally disappeared object parts especially under occlusion and large deformation. For the second purpose, three new spatial objectness cues specially designed for tracking are defined, including surrounding comparison, edge density change and weighted superpixel straddling. With these spatial objectness cues, our method facilitates target object localization and ensures the target has similar edge distribution between adjacent frames. These cues greatly improve the ability of our method to distinguish the target from its surrounding background. The adaptive appearance model retains valuable features of historical results, and the spatial objectness cues are extracted from the current frame, and thus they are finally combined to complement with each other to solve large appearance changes. The extensive evaluations on the CVPR 2013 online object tracking benchmark and VOT 2014 datasets demonstrate the effectiveness of our method as compared with related trackers.																	0941-0643	1433-3058				MAY	2020	32	10			SI		6089	6109		10.1007/s00521-019-04094-z													
J								A transfer convolutional neural network for fault diagnosis based on ResNet-50	NEURAL COMPUTING & APPLICATIONS										Fault diagnosis; Convolutional neural network; Feature transferring; ResNet-50	MACHINE; SIGNAL; MODEL	With the rapid development of smart manufacturing, data-driven fault diagnosis has attracted increasing attentions. As one of the most popular methods applied in fault diagnosis, deep learning (DL) has achieved remarkable results. However, due to the fact that the volume of labeled samples is small in fault diagnosis, the depths of DL models for fault diagnosis are shallow compared with convolutional neural network in other areas (including ImageNet), which limits their final prediction accuracies. In this research, a new TCNN(ResNet-50) with the depth of 51 convolutional layers is proposed for fault diagnosis. By combining with transfer learning, TCNN(ResNet-50) applies ResNet-50 trained on ImageNet as feature extractor for fault diagnosis. Firstly, a signal-to-image method is developed to convert time-domain fault signals to RGB images format as the input datatype of ResNet-50. Then, a new structure of TCNN(ResNet-50) is proposed. Finally, the proposed TCNN(ResNet-50) has been tested on three datasets, including bearing damage dataset provided by KAT datacenter, motor bearing dataset provided by Case Western Reserve University (CWRU) and self-priming centrifugal pump dataset. It achieved state-of-the-art results. The prediction accuracies of TCNN(ResNet-50) are as high as 98.95% +/- 0.0074, 99.99% +/- 0 and 99.20% +/- 0, which demonstrates that TCNN(ResNet-50) outperforms other DL models and traditional methods.																	0941-0643	1433-3058				MAY	2020	32	10			SI		6111	6124		10.1007/s00521-019-04097-w													
J								Intrusion detection system based on a modified binary grey wolf optimisation	NEURAL COMPUTING & APPLICATIONS										Intrusion detection system; Anomaly-based detection; Modified binary grey wolf optimisation; Grey wolf optimisation		One critical issue within network security refers to intrusion detection. The nature of intrusion attempts appears to be nonlinear, wherein the network traffic performance is unpredictable, and the problematic space features are numerous. These make intrusion detection systems (IDSs) a challenge within the research arena. Hence, selecting the essential aspects for intrusion detection is crucial in information security and with that, this study identified the related features in building a computationally efficient and effective intrusion system. Accordingly, a modified feature selection (FS) algorithm called modified binary grey wolf optimisation (MBGWO) is proposed in this study. The proposed algorithm is based on binary grey wolf optimisation to boost the performance of IDS. The new FS algorithm selected an optimal number of features. In order to evaluate the proposed algorithm, the benchmark of NSL-KDD network intrusion, which was modified from 99-data set KDD cup to assess issues linked with IDS, had been applied in this study. Additionally, the support vector machine was employed to classify the data set effectively. The proposed FS and classification algorithms enhanced the performance of the IDS in detecting attacks. The simulation outcomes portrayed that the proposed algorithm enhanced the accuracy of intrusion detection up to 99.22% and reduction in the number of features from 41 to 14.																	0941-0643	1433-3058				MAY	2020	32	10			SI		6125	6137		10.1007/s00521-019-04103-1													
J								Detecting tumours by segmenting MRI images using transformed differential evolution algorithm with Kapur's thresholding	NEURAL COMPUTING & APPLICATIONS										Optimal; Variance; PSNR; MSE	GENETIC ALGORITHM; SEGMENTATION; OPTIMIZATION; ENTROPY	The speed and accuracy with which the patient affected with brain tumour is diagnosed and monitored, plays a very crucial role in providing treatment to the patient. During the diagnosis of the diseased part, a constant demand is anticipated to easily extract the specific region of interest within the complex medical image. This task of extracting only the diseased portion amid the complex body parts in the complex medical image can be achieved by image segmentation. Accuracy and speed of extracting the points or area of interest within the multipart medical image can be improved by using various evolutionary techniques. Differential evolution (DE) is an efficient evolutionary technique that can be used for solving optimisation problem like image segmentation. The main disadvantage of classical evolutionary technique is its inability to adapt its solution algorithm to a given problem. Owing to this need, more adaptable and flexible algorithms are in demand. Numerous variants of DE exist which differ in their solutions. Here, a variant of differential evolution named as transformed differential evolution (TDE) is presented which has an improved mutation strategy that is optimised to fewer function evaluations. This variant is combined with the Kapur's multi-level thresholding for segmenting magnetic resonance imaging (MRI) images and to extract only the regions of tumour. The results obtained using TDE with Kapur's multi-level thresholding were compared with the results using traditional Kapur's technique and the new results improved profoundly. By introducing TDE in multilevel thresholding, the computational time significantly reduced and the resultant image quality improved greatly.																	0941-0643	1433-3058				MAY	2020	32	10			SI		6139	6149		10.1007/s00521-019-04104-0													
J								Shear strength prediction of FRP reinforced concrete members using generalized regression neural network	NEURAL COMPUTING & APPLICATIONS										Glass fiber reinforced polymer; Shear strength; Neural network model; Reinforced concrete	BEAMS; POLYMER; RESISTANCE; CAPACITY	The behavior of FRP reinforced members in shear differs from that of steel reinforced members. Consequently, the design equations are continually changing and becoming more intricate. This paper presents a model based on generalized regression neural network (GRNN) for the predictions of shear strength of FRP reinforced concrete members with no transverse reinforcement. A database of 196 test specimens, failed in shear, is used to train and test the GRNN model. The results of training and testing set of the database are compared with the experimental results. It was observed that GRNN proves to be an effective method for predicting the shear strength of FRP reinforced concrete members without shear reinforcement. The database is also used to assess the accuracy of ACI 440.1R, CSA S806, JSCE, and BISE shear design procedures and to compare their predictions with the GRNN model. The GRNN model prediction shows more consistent and less scattered results in contrast to the four design codes and guidelines.																	0941-0643	1433-3058				MAY	2020	32	10			SI		6151	6158		10.1007/s00521-019-04107-x													
J								Multi-modal forest optimization algorithm	NEURAL COMPUTING & APPLICATIONS										Multi-modal forest optimization algorithm (MMFOA); Multi-modal optimization; Niching methods	PEAK DETECTION; GENETIC ALGORITHM; SEARCH	Multi-modal optimization algorithms are one of the most challenging issues in the field of optimization. Most real-world problems have more than one solution; therefore, the potential role of multi-modal optimization algorithms is rather significant. Multi-modal problems consider several global and local optima. Therefore, during the search process, most of the points should be detected by the algorithm. The forest optimization algorithm has been recently introduced as a new evolutionary algorithm with the capability of solving unimodal problems. This paper presents the multi-modal forest optimization algorithm (MMFOA), which is constructed by applying a clustering technique, based on niching methods, to the unimodal forest optimization algorithm. The MMFOA operates by dividing the population of the forest into subpopulations to locate existing local and global optima. Subpopulations are generated by the Basic Sequential Algorithmic Scheme with a radius neighborhood. As population size is self-adaptive in MMFOA, population size can be increased in functions with too many local and global optima. The proposed algorithm is evaluated by a set of multi-modal benchmark functions. The experiment results show that not only is the population size low, but also that the convergence speed is high, and that the algorithm is efficient in solving multi-modal problems.																	0941-0643	1433-3058				MAY	2020	32	10			SI		6159	6173		10.1007/s00521-019-04113-z													
J								Event-triggered asynchronous distributed optimization algorithm with heterogeneous time-varying step-sizes	NEURAL COMPUTING & APPLICATIONS										Distributed optimization; Asynchronous algorithm; Event-triggered scheme; Heterogeneous time-varying step-sizes; Linear convergence	RECURRENT NEURAL-NETWORK; MULTIAGENT SYSTEMS; SYNCHRONIZATION; CONVERGENCE	This paper concerns distributed convex optimization problems over time-varying undirected graphs, in which the global objective function is expressed as the sum of individual objective functions of the agents. Each agent only knows its local objective functions. To figure out such problems, an event-triggered asynchronous distributed optimization algorithm (termed as EV-ADOA) with time-varying heterogeneous step-sizes is proposed, which is suitable for undirected graphs changing over time. Under two standard assumptions on strongly convex and smoothness of local objective functions, the EV-ADOA can achieve linear convergence with a proper upper bound of the heterogeneous time-varying step-sizes. EV-ADOA with event-triggered scheme can decrease network communication, and the Zeno-like behavior strictly is excluded. The efficiency of EV-ADOA is demonstrated by experiments.																	0941-0643	1433-3058				MAY	2020	32	10			SI		6175	6184		10.1007/s00521-019-04116-w													
J								A set of efficient heuristics for a home healthcare problem	NEURAL COMPUTING & APPLICATIONS										Home health care; Lagrangian relaxation theory; Heuristics; Hybrid constructive metaheuristic	LOOP SUPPLY CHAIN; LEVEL PROGRAMMING-MODEL; SCHEDULING PROBLEM; NETWORK DESIGN; LAGRANGIAN-RELAXATION; ALGORITHM; SEARCH; SOLVE	Nowadays, the aging population and the little availability of informal care are two of the several factors leading to an increased need for assisted living support. Hence, home healthcare (HHC) operations including a set of nurses and patients have been developed recently by both academia and health practitioners to consider elderlies' preferences willing to receive their cares at their homes instead of hospitals or retirement homes. Commonly, different services, e.g., nursing, physiotherapy, housekeeping and cleaning, for an HHC system are performed by nurses at patients' homes after scheduling and routing the nurses by decision makers. Due to the difficulty of the problem, recent studies show a great deal of interest in applying various metaheuristics and heuristics to solve this problem. To alleviate the drawbacks of previous works and make HHC more practical, this paper develops not only a new mathematical formulation considering new suppositions in this research area but also a lower bound based on Lagrangian relaxation theory has been employed. As such, three new heuristics and a hybrid constructive metaheuristic are utilized in this study to solve the proposed model. Finally, the performance of the proposed algorithms is validated by the developed lower bound and also analyzed by different criteria and also the efficiency of developed formulation is probed through some sensitivity analyses.																	0941-0643	1433-3058				MAY	2020	32	10			SI		6185	6205		10.1007/s00521-019-04126-8													
J								A conceptual comparison of several metaheuristic algorithms on continuous optimisation problems	NEURAL COMPUTING & APPLICATIONS										Metaheuristics; Population-based metaheuristics; Swarm intelligence; Continuous domain optimisation	SYMBIOTIC ORGANISMS SEARCH; ANT COLONY OPTIMIZATION; DIFFERENTIAL EVOLUTION; GLOBAL OPTIMIZATION; BAT ALGORITHM	The field of continuous optimisation has witnessed an explosion of the so-called new or novel metaheuristic algorithms. Though not all of these algorithms are efficient as proclaimed by their inventors, a few of them have proved to be very efficient and thus have become popular tools for solving complex optimisation problems. Therefore, there is a need for a systematic analysis approach to fairly evaluate and compare the results of some of these optimisation algorithms. In this paper, a set of well-known mathematical benchmark functions are compiled to provide an easily accessible collection of standard benchmark test problems for continuous global optimisation. This set of test problems are used to investigate the computational capabilities and the microscopic behaviour of twelve different metaheuristic algorithms. The required number of function evaluations for reaching the best solution and the run-time complexity of the algorithms are compared. Furthermore, statistical tests are conducted to validate the concluding remarks.																	0941-0643	1433-3058				MAY	2020	32	10			SI		6207	6251		10.1007/s00521-019-04132-w													
J								Novel computing paradigms for parameter estimation in power signal models	NEURAL COMPUTING & APPLICATIONS										Power signals; Parameter estimation; Differential evolution; Genetic algorithm; Pattern search algorithm; Evolutionary computing	DIFFERENTIAL EVOLUTION; GENETIC ALGORITHMS; DYNAMICS; OPTIMIZATION; INTELLIGENT; ANALYZE; SYSTEM; HYBRID; DISPATCH; DESIGN	The strength of evolutionary computational heuristic paradigms is exploited for parameter estimation of power signal modeling problems by incorporating differential evolution (DE), genetic algorithms (GAs) and pattern search (PS) methodologies. The objective function of power signal harmonics is constructed by utilizing the power of approximation theory in mean squared error sense. The stiff optimization task of signal harmonics is performed with heuristic solvers DE, GAs and PS that provide efficacy, fast convergence rate and avoid getting trapped in local minima. Statistics reveal that DE outperforms its counterparts in terms of accuracy, robustness and complexity measures.																	0941-0643	1433-3058				MAY	2020	32	10			SI		6253	6282		10.1007/s00521-019-04133-9													
J								Multi-scale multi-block covariance descriptor with feature selection	NEURAL COMPUTING & APPLICATIONS										Face texture representation; Feature selection; Face recognition	INVARIANT TEXTURE CLASSIFICATION; REPRESENTATION; ALGORITHM	This paper investigates a compact face texture representation able to cover the most discriminant features of facial images. The compactness is achieved by the proposed Pyramid Multi-Level (PML) covariance texture descriptor and the feature selection process that is applied on the raw extracted features. In fact, we introduce a framework based mainly on two new aspects. Firstly, we consider an extension of the original covariance descriptor that relies on de-noised covariance matrices obtained using texture descriptors such as local binary pattern and quaternionic local ranking binary pattern images. Secondly, we exploit the resulting covariance descriptor using a PML face representation which allows a multi-level multi-scale feature extraction. Experiments conducted on four public face datasets show the efficacy of the proposed face descriptor and the associated selection schemes.																	0941-0643	1433-3058				MAY	2020	32	10			SI		6283	6294		10.1007/s00521-019-04135-7													
J								Facial expression recognition with dynamic cascaded classifier	NEURAL COMPUTING & APPLICATIONS										Facial expression recognition; Compressive sensing; Support vector machine; Radial basis function kernel	GABOR WAVELETS; PARAMETERS; EIGENFACES; PATTERNS; FACE	In this paper, a new approach for facial expression recognition has been proposed. The approach has imbedded a new feature extraction technique, new multiclass classification approach and a new kernel parameter optimization for support vector machines. The scheme of the approach begins with feature extraction from the input vectors, and the extracted features are transformed into a Gaussian space using compressive sensing techniques. This process ensures feature vector dimensionality reduction and matches the features vectors with radial basis function kernel used in support vector machines for classification. Prior to classification, an optimized parameter for support vector machines training is automatically determined based on an approach proposed which relies on the receiver operating characteristics of the support vector machine classifier. With the optimized kernel parameter, new proposed multiclass classification approach is used to finally classify any vector. In all the experiments conducted on the two facial expression databases with different cross-validation techniques, the proposed approach outperforms its counterparts under the same database and settings. The results further confirmed the validity and advantages of the proposed approach over other approaches currently used in the literature.																	0941-0643	1433-3058				MAY	2020	32	10			SI		6295	6309		10.1007/s00521-019-04138-4													
J								Multiple attribute group decision making using J-divergence and evidential reasoning theory under intuitionistic fuzzy environment	NEURAL COMPUTING & APPLICATIONS										Intuitionistic fuzzy sets; Multiple attribute group decision making; J-divergence; Evidential reasoning theory	PROGRAMMING METHODOLOGY; SETS; MULTIPERSON; WEIGHTS; FUSION	The theory of intuitionistic fuzzy sets has been proved to be an effective and convenient tool in the construction of fuzzy multiple attribute group decision-making models to deal with the uncertainty in developing complex decision support systems. Concerning this topic, the current studies mainly focus on their attention on two aspects including aggregation operators on intuitionistic fuzzy sets and determining the weights of both decision makers and attributes. However, some challenges have not been fully considered including existing aggregation operators which may induce unreasonable results in some situations and how to objectively determine the weights of both attributes and decision makers to meet different decision-making demands. To overcome the challenges of existing decision-making models and to satisfy much more decision-making situations, a novel intuitionistic fuzzy multiple attribute group decision-making method via J-divergence and evidential reasoning theory is proposed in this paper as a supplement of conventional models. On the one hand, a weighted J-divergence of intuitionistic fuzzy sets and a J-divergence between two intuitionistic fuzzy matrices are introduced. Following the two concepts, two consensus-based approaches are proposed to determine the weights of both decision makers and attributes. The weights obtained from the proposed method can more accurately reflect the importance levels of both attributes and decision makers from the perspective of consensus by comparison with existing models. On the other hand, an evidential reasoning theory-based operator is established to replace conventional operators for aggregating intuitionistic fuzzy information. The fusion result via this operator is consistent with most of intuitionistic fuzzy numbers. With these works, the proposed method can provide more accurate and reasonable decision results than existing algorithms.																	0941-0643	1433-3058				MAY	2020	32	10			SI		6311	6326		10.1007/s00521-019-04140-w													
J								Decomposition algorithm for depth image of human health posture based on brain health	NEURAL COMPUTING & APPLICATIONS										Brain health; Decomposition algorithm; Feature extraction; Depth map sequence	GESTURE RECOGNITION; INTELLIGENT CONTROL; TEMPERATURE; EXTRACTION; SIMULATION	At this stage, brain health can be directly expressed in the human hand posture estimation. Therefore, model estimation of healthy human hand posture can also be used as a criterion for brain health. The recognition algorithm of healthy human hand gesture based on global feature extraction of depth map sequence is not enough to analyze the motion correlation of healthy human hand posture, which leads to the need to improve the accuracy of human body hand gesture description and the change of movement speed of robustness. After analyzing the characteristics of healthy human hand movement in detail, this paper proposes a hand posture decomposition algorithm based on depth map sequence. The goal is to find information that plays a key role in hand gesture recognition in the depth map sequence. The algorithm can remove redundant information and improve the robustness of the recognition algorithm.																	0941-0643	1433-3058				MAY	2020	32	10			SI		6327	6342		10.1007/s00521-019-04141-9													
J								Surface EMG hand gesture recognition system based on PCA and GRNN	NEURAL COMPUTING & APPLICATIONS										sEMG; Gesture recognition; Feature reduction; PCA; GRNN; Machine learning	INTELLIGENT CONTROL; PATTERN-RECOGNITION; VARIABLE SELECTION; STRESS-ANALYSIS; PROSTHESES; ELECTROMYOGRAPHY; TEMPERATURE; SIMULATION; PARAMETERS; MODEL	The principal component analysis method and GRNN neural network are used to construct the gesture recognition system, so as to reduce the redundant information of EMG signals, reduce the signal dimension, improve the recognition efficiency and accuracy, and enhance the feasibility of real-time recognition. Using the means of extracting key information of human motion, the specific action mode is identified. In this paper, nine static gestures are taken as samples, and the surface EMG signal of the arm is collected by the electromyography instrument to extract four kinds of characteristics of the signal. After dimension reduction and neural network learning, the overall recognition rate of the system reached 95.1%, and the average recognition time was 0.19 s.																	0941-0643	1433-3058				MAY	2020	32	10			SI		6343	6351		10.1007/s00521-019-04142-8													
J								Novel chroma subsampling patterns for wireless capsule endoscopy compression	NEURAL COMPUTING & APPLICATIONS										Wireless capsule endoscopy; Compression; Chroma subsampling; Golomb-Rice coder	IMAGE COMPRESSION; EFFICIENT; ALGORITHM; DESIGN; SYSTEM	The recently established wireless capsule endoscopy promises itself to be a great advancement in the field of medical diagnosis. The amount of data generated due to a single inspection of gastrointestinal tract is huge, and the transmission demands more cost and power. Due to the inherent nature of the capsule, a low complexity and low-power design are highly recommended with the ability to reduce the data. This paper explains the design of a compression module which satisfies the above requirements. The compression algorithm is developed around some features of the endoscopic images with its suitability for hardware implementation. The preprocessing operations followed by two-stage encoder using differential coder and Golomb-Rice coder are implemented on the experimental dataset. The core module in the preprocessing stage is the chroma subsampling which can considerably reduce the data to be transmitted. Several patterns for subsampling have been analyzed in this paper to evaluate the performance of the proposed system. With the help of a suitably designed up-sampler at the receiver, a near-lossless compression with good reconstructed image quality can be achieved. The 22:1:2 pattern yields better results and could bring about an average peak signal-to-noise ratio of 37 dB with compression rate of 70%. The results show that the proposed chroma subsampling patterns performs better than the competing methods towards wireless capsule endoscopy compression.																	0941-0643	1433-3058				MAY	2020	32	10			SI		6353	6362		10.1007/s00521-019-04143-7													
J								Assessing gender bias in machine translation: a case study with Google Translate	NEURAL COMPUTING & APPLICATIONS										Machine bias; Gender bias; Machine learning; Machine translation		Recently there has been a growing concern in academia, industrial research laboratories and the mainstream commercial media about the phenomenon dubbed as machine bias, where trained statistical models-unbeknownst to their creators-grow to reflect controversial societal asymmetries, such as gender or racial bias. A significant number of Artificial Intelligence tools have recently been suggested to be harmfully biased toward some minority, with reports of racist criminal behavior predictors, Apple's Iphone X failing to differentiate between two distinct Asian people and the now infamous case of Google photos' mistakenly classifying black people as gorillas. Although a systematic study of such biases can be difficult, we believe that automated translation tools can be exploited through gender neutral languages to yield a window into the phenomenon of gender bias in AI. In this paper, we start with a comprehensive list of job positions from the U.S. Bureau of Labor Statistics (BLS) and used it in order to build sentences in constructions like "He/She is an Engineer" (where "Engineer" is replaced by the job position of interest) in 12 different gender neutral languages such as Hungarian, Chinese, Yoruba, and several others. We translate these sentences into English using the Google Translate API, and collect statistics about the frequency of female, male and gender neutral pronouns in the translated output. We then show that Google Translate exhibits a strong tendency toward male defaults, in particular for fields typically associated to unbalanced gender distribution or stereotypes such as STEM (Science, Technology, Engineering and Mathematics) jobs. We ran these statistics against BLS' data for the frequency of female participation in each job position, in which we show that Google Translate fails to reproduce a real-world distribution of female workers. In summary, we provide experimental evidence that even if one does not expect in principle a 50:50 pronominal gender distribution, Google Translate yields male defaults much more frequently than what would be expected from demographic data alone. We believe that our study can shed further light on the phenomenon of machine bias and are hopeful that it will ignite a debate about the need to augment current statistical translation tools with debiasing techniques-which can already be found in the scientific literature.																	0941-0643	1433-3058				MAY	2020	32	10			SI		6363	6381		10.1007/s00521-019-04144-6													
J								SRTM: a supervised relation topic model for multi-classification on large-scale document network	NEURAL COMPUTING & APPLICATIONS										Document classification; Link analysis; Topic modeling; Multi-classification		The increasing use of social networking platforms has raised the need to develop automated multi-classifications on document network. In this paper, we propose a supervised relation topic model that leverages the links between documents to learn the latent content of documents and enhance performance of prediction. Our model takes advantage of Bayesian generative model to exploit the relation between word feature and link feature in a document network. We evaluate our model on large-scale data collections that include scientific citation community and medical article network. We demonstrate its effectiveness and efficiency on document classification with SLDA model and collective classification approaches.																	0941-0643	1433-3058				MAY	2020	32	10			SI		6383	6392		10.1007/s00521-019-04145-5													
J								FEMa: a finite element machine for fast learning	NEURAL COMPUTING & APPLICATIONS										Finite element method; Pattern classification; Pattern recognition	OPTIMUM-PATH FOREST; NEURAL-NETWORK; DAMAGE DETECTION; ALGORITHM; CLASSIFICATION	Machine learning has played an essential role in the past decades and has been in lockstep with the main advances in computer technology. Given the massive amount of data generated daily, there is a need for even faster and more effective machine learning algorithms that can provide updated models for real-time applications and on-demand tools. This paper presents FEMa-a finite element machine classifier-for supervised learning problems, where each training sample is the center of a basis function, and the whole training set is modeled as a probabilistic manifold for classification purposes. FEMa has its theoretical basis in the finite element method, which is widely used for numeral analysis in engineering problems. It is shown FEMa is parameterless and has a quadratic complexity for both training and classification phases when basis functions are used that satisfy certain properties. The proposed classifier yields very competitive results when compared to some state-of-the-art supervised pattern recognition techniques.																	0941-0643	1433-3058				MAY	2020	32	10			SI		6393	6404		10.1007/s00521-019-04146-4													
J								A compact Crank-Nicholson scheme for the numerical solution of fuzzy time fractional diffusion equations	NEURAL COMPUTING & APPLICATIONS										Double parametric form of fuzzy number; Caputo formula; Fuzzy time fractional diffusion equation; Compact Crank-Nicholson scheme	DIFFERENCE-METHODS	Fuzzy fractional partial differential equations are a generalization of classical fuzzy partial differential equation which can, in certain circumstances, provide a better explanation for certain phenomena. In this paper, a compact Crank-Nicholson scheme is developed, analyzed and applied to solve the fuzzy time diffusion equation with fractional order 0<alpha <= 1 It is based on double parametric form of fuzzy numbers and the time fractional derivative is defined using the Caputo definition. The truncation error and stability of the proposed scheme is discussed. The compact Crank-Nicholson scheme is shown to be a feasible scheme via a numerical example.																	0941-0643	1433-3058				MAY	2020	32	10			SI		6405	6412		10.1007/s00521-019-04148-2													
J								Optimal Control Based on Neuro Estimator for Fractional Order Uncertain Non-linear Continuous-Time Systems	NEURAL PROCESSING LETTERS										Fractional order system; Neural network; Adaptive estimation; Optimal control; Predictive control	NUMERICAL-SOLUTION; CONSERVATION-LAWS; SOLUTION SCHEME; FORMULATION; EQUATIONS; DESIGN	In this paper, a novel method is presented for optimal control of fractional order systems in the presence of an unknown term in system dynamic where fractional order derivative is considered to be between zero and one. In this method, neural network is used to estimate the unknown term in system dynamic. Neural network coefficients are updated adaptively and online. Updating laws are presented considering system requirements to achieve a homogeneous fractional order system. Another problem is formulating optimal control laws for fractional order system which is solved through fractional differential calculus. Since optimal fractional order control is non-causal and does not have a online solution, step-by-step progression and predictive control idea are used to obtain control signal and combine optimal controller and estimator. This method results in an optimal run-time control and resolves unknown terms in system dynamic. In addition, the closed loop system being uniform ultimate bounded is proved through direct Lyapunov method. Finally, simulation results are given to show efficiency of the proposed method.																	1370-4621	1573-773X				AUG	2020	52	1			SI		221	240		10.1007/s11063-020-10261-4		MAY 2020											
J								Joint Robust Multi-view Spectral Clustering	NEURAL PROCESSING LETTERS										Clustering; Multi-view; k-means clustering; Spectral clustering; Feature selection; Outlier reduction; Similarity measure	FEATURE-SELECTION; SYSTEM	Current multi-view clustering algorithms use multistage strategies to conduct clustering, or require cluster number or similarity matrix prior, or suffer influence of irrelevant features and outliers. In this paper, we propose a Joint Robust Multi-view (JRM) spectral clustering algorithm that considers information from all views of the multi-view dataset to conduct clustering and solves the issues, such as initialization, cluster number determination, similarity measure, feature selection, and outlier reduction around clustering, in a unified way. The optimal performance could be reached when all views are considered and the separated stages are combined in a unified way. Experiments have been performed on six real-world benchmark datasets and our proposed JRM algorithm outperforms the comparison clustering algorithms in terms of two evaluation metrics for clustering algorithms including accuracy and purity.																	1370-4621	1573-773X															10.1007/s11063-020-10257-0		MAY 2020											
J								Image classification algorithm based on stacked sparse coding deep learning model-optimized kernel function nonnegative sparse representation	SOFT COMPUTING										Sparse representation; Kernel function; Image classification; Stack sparse coding; Deep learning	CONVOLUTIONAL NEURAL-NETWORKS; AUTO-ENCODER; CNN; CANCER; DIMENSIONALITY; FEATURES	Image classification has received extensive attention as an important technical means of acquiring image information. It has been widely used in various engineering fields. Although the existing traditional image classification methods have been widely applied in practical problems, there are some problems in the application process, such as unsatisfactory effects, low classification accuracy and weak adaptive ability. This is because this type of method relies on the designer's prior knowledge and cognitive understanding of the classification task. At the same time, this method separates image feature extraction and classification into two steps for classification operation. However, the deep learning model has a powerful learning ability, which integrates the feature extraction and classification process into a whole to complete the image classification test, which can effectively improve the image classification accuracy. At the same time, the image classification method based on deep learning also has the following problems in the application process: First, it is impossible to effectively approximate the complex functions in the deep learning model. Second, the deep learning model comes with a low classifier with low accuracy. To this end, this paper introduces the idea of sparse representation into the architecture of deep learning network, comprehensively utilizes the sparse representation of good multidimensional data linear decomposition ability and the deep structural advantages of multi-layer nonlinear mapping to complete the complex function approximation in deep learning model. It constructs a deep learning model with adaptive approximation ability, which solves the function approximation problem of deep learning models. At the same time, in order to further improve the classification effect of the deep learning classifier, a sparse representation classification method based on the optimized kernel function is proposed to replace the classifier in the deep learning model, thereby improving the image classification effect. Based on the above explanation, this paper proposes an image classification algorithm based on the stacked sparse coding depth learning model-optimized kernel function nonnegative sparse representation. The experimental results show that the proposed method not only has a higher average accuracy than other mainstream methods, but also can be well adapted to various image databases. This is because the proposed method can extract more image feature information than the traditional image classification method and can better adaptively match the image information. Compared with other deep learning methods, it can better solve the problems of complex function approximation and poor classifier effect, thus further improving image classification accuracy.																	1432-7643	1433-7479				NOV	2020	24	22					16967	16981		10.1007/s00500-020-04989-3		MAY 2020											
J								The General Combinatorial Optimization Problem: Towards Automated Algorithm Design	IEEE COMPUTATIONAL INTELLIGENCE MAGAZINE											HYPER-HEURISTICS; EVOLUTIONARY ALGORITHMS; METAHEURISTICS; PORTFOLIOS	This paper defines a new combinatorial optimization problem, namely General Combinatorial Optimization Problem (GCOP), whose decision variables are a set of parametric algorithmic components, i.e. algorithm design decisions. The solutions of GCOP, i.e. compositions of algorithmic components, thus represent different generic search algorithms. The objective of GCOP is to find the optimal algorithmic compositions for solving the given optimization problems. Solving the GCOP is thus equivalent to automatically designing the best algorithms for optimization problems. Despite recent advances, the evolutionary computation and optimization research communities are yet to embrace formal standards that underpin automated algorithm design. In this position paper, we establish GCOP as a new standard to define different search algorithms within one unified model. We demonstrate the new GCOP model to standardize various search algorithms as well as selection hyperheuristics. A taxonomy is defined to distinguish several widely used terminologies in automated algorithm design, namely automated algorithm composition, configuration and selection. We would like to encourage a new line of exciting research directions addressing several challenging research issues including algorithm generality, algorithm reusability, and automated algorithm design.																	1556-603X	1556-6048				MAY	2020	15	2					14	23		10.1109/MCI.2020.2976182													
J								CIS Society Officers	IEEE COMPUTATIONAL INTELLIGENCE MAGAZINE											MULTIOBJECTIVE EVOLUTIONARY ALGORITHM; STRUCTURAL BALANCE; COMMUNITY STRUCTURE; GENETIC ALGORITHM; SOCIAL NETWORKS; ROBUSTNESS; EMERGENCE; ATTACKS	Provides a listing of current committee members and society officers.																	1556-603X	1556-6048				MAY	2020	15	2					24	35		10.1109/MCI.2020.2976183													
J								PaletteViz: A Visualization Method for Functional Understanding of High-Dimensional Pareto-Optimal Data-Sets to Aid Multi-Criteria Decision Making	IEEE COMPUTATIONAL INTELLIGENCE MAGAZINE											OPTIMIZATION; ALGORITHM; KNEES	To represent a many-objective Pareto-optimal front having four or more dimensions of the objective space, a large number of points are necessary. However, for choosing a single preferred point from a large set is problematic and time-consuming, as they provide a large cognitive burden on the part of the decision-makers (DMs). Hence, many-objective optimization and decision-making researchers and practitioners have been interested in effective visualization methods to filter down a few critical points for further analysis. While some ideas are borrowed from data analytics and visualization literature, they are generic and do not exploit the functionalities that DMs are usually interested. In this paper, we outline some such functionalities: a point's trade-off among conflicting objectives in its neighborhood, closeness of a point to the boundary or core of the high-dimensional Pareto set, specific desired geometric properties of points, spatial distance of one point to another, closeness of a point to constraint boundary, and others, in developing a new visualization technique. We propose a novel way to map a high-dimensional Pareto-optimal front (points or data-set) into two-and-half dimensions by revealing functional features of points that may be of great interest to DMs. As a proof-of-principle demonstration, we apply our proposed palette visualization (PaletteViz) technique to a number of different structures of Pareto-optimal data-sets and discuss how the proposed technique is different from a few popularly used visualization techniques.																	1556-603X	1556-6048				MAY	2020	15	2					36	48		10.1109/MCI.2020.2976184													
J								A Survey on Differentially Private Machine Learning	IEEE COMPUTATIONAL INTELLIGENCE MAGAZINE											CLASSIFICATION; FOUNDATIONS; NOISE	Recent years have witnessed remarkable successes of machine learning in various applications. However, machine learning models suffer from a potential risk of leaking private information contained in training data, which have attracted increasing research attention. As one of the mainstream privacy-preserving techniques, differential privacy provides a promising way to prevent the leaking of individual-level privacy in training data while preserving the quality of training data for model building. This work provides a comprehensive survey on the existing works that incorporate differential privacy with machine learning, so-called differentially private machine learning and categorizes them into two broad categories as per different differential privacy mechanisms: the Laplace/Gaussian/exponential mechanism and the output/objective perturbation mechanism. In the former, a calibrated amount of noise is added to the non-private model and in the latter, the output or the objective function is perturbed by random noise. Particularly, the survey covers the techniques of differentially private deep learning to alleviate the recent concerns about the privacy of big data contributors. In addition, the research challenges in terms of model utility, privacy level and applications are discussed. To tackle these challenges, several potential future research directions for differentially private machine learning are pointed out.																	1556-603X	1556-6048				MAY	2020	15	2					49	64		10.1109/MCI.2020.2976185													
J								An Effective Feature Learning Approach Using Genetic Programming With Image Descriptors for Image Classification [Research Frontier]	IEEE COMPUTATIONAL INTELLIGENCE MAGAZINE										Feature extraction; Image classification; Task analysis; Histograms; Image edge detection; Genetic programming; Classification algorithms	SCALE; MODEL	Being able to extract effective features from different images is very important for image classification, but it is challenging due to high variations across images. By integrating existing well-developed feature descriptors into learning algorithms, it is possible to automatically extract informative high-level features for image classification. As a learning algorithm with a flexible representation and good global search ability, genetic programming can achieve this. In this paper, a new genetic programming-based feature learning approach is developed to automatically select and combine five existing well-developed descriptors to extract high-level features for image classification. The new approach can automatically learn various numbers of global and/or local features from different types of images. The results show that the new approach achieves significantly better classification performance in almost all the comparisons on eight data sets of varying difficulty. Further analysis reveals the effectiveness of the new approach to finding the most effective feature descriptors or combinations of them to extract discriminative features for different classification tasks.																	1556-603X	1556-6048				MAY	2020	15	2					65	77		10.1109/MCI.2020.2976186													
J								xTML: A Unified Heterogeneous Transfer Metric Learning Framework for Multimedia Applications [Application Notes]	IEEE COMPUTATIONAL INTELLIGENCE MAGAZINE										XML; Training data; Search problems; Multimedia communication; Data analysis		Owing to the continual growth of multimodal data (or feature spaces), we have seen a rising interest in multimedia applications (e.g., object classification and searching) over these heterogeneous data. However, the accuracy of classification and searching tasks is highly dependent on the distance estimation between data samples, and simple Euclidean (EU) distance has been proven to be inadequate. Previous research has focused on learning a robust distance metric to quantify the relationships among data samples. In this context, existing distance metric learning (DML) algorithms mainly leverage on label information in the target domain for model training and may fail when the label information is scarce. As an improvement, transfer metric learning (TML) approaches are proposed to leverage information from other related domains. However, current TML algorithms assume that different domains explore the same representation; thus, they are not applicable in heterogeneous settings where the data representations of different domains vary. In this research, we propose xTML, a novel unified heterogeneous transfer metric learning framework, to improve the distance estimation of the domains of interest (i.e., the target domains in classification and searching tasks) when limited label information, complementary with extensive unlabeled data, is provisioned for model training. We further illustrate how our proposed framework can be applied to a selected list of multimedia applications, including opinion mining, deception detection and online product searching.																	1556-603X	1556-6048				MAY	2020	15	2					78	88		10.1109/MCI.2020.2976187													
J								Adaptive Fuzzy Finite-Time Control of Nonlinear Systems With Actuator Faults	IEEE TRANSACTIONS ON CYBERNETICS										Actuators; Nonlinear systems; Fault tolerance; Fault tolerant systems; Adaptive systems; Backstepping; Actuator faults; adaptive fuzzy control; backstepping	DYNAMIC SURFACE CONTROL; OUTPUT-FEEDBACK STABILIZATION; SLIDING-MODE CONTROL; TRACKING CONTROL; FAILURE COMPENSATION; DELAY SYSTEMS; DESIGN; AIRCRAFT; OBSERVER	This paper addresses the trajectory tracking control problem of a class of nonstrict-feedback nonlinear systems with the actuator faults. The functional relationship in the affine form between the nonlinear functions with whole state and error variables is established by using the structure consistency of intermediate control signals and the variable-partition technique. The fuzzy control and adaptive backstepping schemes are applied to construct an improved fault-tolerant controller without requiring the specific knowledge of control gains and actuator faults, including both stuck constant value and loss of effectiveness. The proposed fault-tolerant controller ensures that all signals in the closed-loop system are semiglobally practically finite-time stable and the tracking error remains in a small neighborhood of the origin after a finite period of time. The developed control method is verified through two numerical examples.																	2168-2267	2168-2275				MAY	2020	50	5					1786	1797		10.1109/TCYB.2019.2902868													
J								Temporal Pattern-Aware QoS Prediction via Biased Non-Negative Latent Factorization of Tensors	IEEE TRANSACTIONS ON CYBERNETICS										Quality of service; Hidden Markov models; Data models; Training; Web services; Time factors; Latent factor analysis (LFA); latent factorization of tensor; learning temporal pattern; linear bias (LB); non-negative latent factorization of tensor; non-negativity constraint; quality-of-service (QoS) prediction	ALGORITHM	Quality-of-service (QoS) data vary over time, making it vital to capture the temporal patterns hidden in such dynamic data for predicting missing ones with high accuracy. However, currently latent factor (LF) analysis-based QoS-predictors are mostly defined on static QoS data without the consideration of such temporal dynamics. To address this issue, this paper presents a biased non-negative latent factorization of tensors (BNLFTs) model for temporal pattern-aware QoS prediction. Its main idea is fourfold: 1) incorporating linear biases into the model for describing QoS fluctuations; 2) constraining the model to be non-negative for describing QoS non-negativity; 3) deducing a single LF-dependent, non-negative, and multiplicative update scheme for training the model; and 4) incorporating an alternating direction method into the model for faster convergence. The empirical studies on two dynamic QoS datasets from real applications show that compared with the state-of-the-art QoS-predictors, BNLFT represents temporal patterns more precisely with high computational efficiency, thereby achieving the most accurate predictions for missing QoS data.																	2168-2267	2168-2275				MAY	2020	50	5					1798	1809		10.1109/TCYB.2019.2903736													
J								Prescribed Performance Cooperative Control for Multiagent Systems With Input Quantization	IEEE TRANSACTIONS ON CYBERNETICS										Multi-agent systems; Fuzzy logic; Quantization (signal); Control systems; Nonlinear systems; Convergence; Cooperative control; prescribed performance; quantized control; unknown gains	UNKNOWN CONTROL DIRECTIONS; LEADER-FOLLOWING CONSENSUS; OUTPUT-FEEDBACK CONTROL; NONLINEAR-SYSTEMS; TRACKING CONTROL; ADAPTIVE TRACKING; CONTROL DESIGN; PARAMETER	This paper studies the quantized cooperative control problem for multiagent systems with unknown gains in the prescribed performance. Different from the finite-time control, a speed function is designed to realize that the tracking errors converge to a prescribed compact set in a given finite time for multiagent systems. Meanwhile, we consider the problem of unknown gains and input quantization, which can be addressed by using a lemma and Nussbaum function in cooperative control. Moreover, the fuzzy logic systems are proposed to approximate the nonlinear function defined on a compact set. A distributed controller and adaptive laws are constructed based on the Lyapunov stability theory and backstepping method. Finally, the effectiveness of the proposed approach is illustrated by some numerical simulation results.																	2168-2267	2168-2275				MAY	2020	50	5					1810	1819		10.1109/TCYB.2019.2893645													
J								Distributed Control of Nonlinear Multiagent Systems With Unknown and Nonidentical Control Directions via Event-Triggered Communication	IEEE TRANSACTIONS ON CYBERNETICS										Topology; Switches; Multi-agent systems; Monitoring; Laplace equations; Symmetric matrices; Directed interaction topology; event-triggered communication; jointly connected; nonlinear multiagent system (MAS); unknown nonidentical control direction	COOPERATIVE OUTPUT REGULATION; HETEROGENEOUS LINEAR-SYSTEMS; ADAPTIVE CONSENSUS; DESIGN; DYNAMICS	In this paper, the leader-following output consensus problem for a class of uncertain nonlinear multiagent systems with unknown control directions is investigated. Each agent system has nonidentical dynamics and is subject to external disturbances and uncertain parameters. The agents are connected through a directed and jointly connected switching network. A novel two-layer distributed hierarchical control scheme is proposed. In the upper layer, to save the communication resources and to handle the switching networks, an event-triggered communication scheme is proposed, and a Zeno-free event-triggered mechanism is designed for each agent to generate the asynchronous triggering time instants. Furthermore, to avoid the continuous monitoring of the system states, a Zeno-free self-triggering algorithm is proposed. In the lower layer, to handle the unknown control directions problem and to achieve the output tracking of the local references generated in the upper layer, the Nussbaum-type function-based technique is combined with internal model principle. With the proposed two-layer distributed hierarchical controller, the leader-following output consensus is achieved. The obtained result is further extended to the formation control problem. Finally, three numerical examples are provided to demonstrate the effectiveness of the proposed theoretical results.																	2168-2267	2168-2275				MAY	2020	50	5					1820	1832		10.1109/TCYB.2019.2908874													
J								Robust Graph Learning From Noisy Data	IEEE TRANSACTIONS ON CYBERNETICS										Noise measurement; Adaptation models; Laplace equations; Manifolds; Task analysis; Reliability; Data models; Clustering; graph construction; noise removal; robust principle component analysis (RPCA); semisupervised classification; similarity measure	LOW-RANK	Learning graphs from data automatically have shown encouraging performance on clustering and semisupervised learning tasks. However, real data are often corrupted, which may cause the learned graph to be inexact or unreliable. In this paper, we propose a novel robust graph learning scheme to learn reliable graphs from the real-world noisy data by adaptively removing noise and errors in the raw data. We show that our proposed model can also be viewed as a robust version of manifold regularized robust principle component analysis (RPCA), where the quality of the graph plays a critical role. The proposed model is able to boost the performance of data clustering, semisupervised classification, and data recovery significantly, primarily due to two key factors: 1) enhanced low-rank recovery by exploiting the graph smoothness assumption and 2) improved graph construction by exploiting clean data recovered by RPCA. Thus, it boosts the clustering, semisupervised classification, and data recovery performance overall. Extensive experiments on image/document clustering, object recognition, image shadow removal, and video background subtraction reveal that our model outperforms the previous state-of-the-art methods.																	2168-2267	2168-2275				MAY	2020	50	5					1833	1843		10.1109/TCYB.2018.2887094													
J								Non-Negativity Constrained Missing Data Estimation for High-Dimensional and Sparse Matrices from Industrial Applications	IEEE TRANSACTIONS ON CYBERNETICS										Computational modeling; Data models; Sparse matrices; Linear programming; Training; Convergence; Analytical models; Alternating-direction-method of multipliers; high-dimensional and sparse matrix; industrial application; non-negative latent factor analysis; recommender system	NONNEGATIVE MATRIX; FACTORIZATION; ALGORITHM; RECOMMENDATION; NETWORKS	High-dimensional and sparse (HiDS) matrices are commonly seen in big-data-related industrial applications like recommender systems. Latent factor (LF) models have proven to be accurate and efficient in extracting hidden knowledge from them. However, they mostly fail to fulfill the non-negativity constraints that describe the non-negative nature of many industrial data. Moreover, existing models suffer from slow convergence rate. An alternating-direction-method of multipliers-based non-negative LF (AMNLF) model decomposes the task of non-negative LF analysis on an HiDS matrix into small subtasks, where each task is solved based on the latest solutions to the previously solved ones, thereby achieving fast convergence and high prediction accuracy for its missing data. This paper theoretically analyzes the characteristics of an AMNLF model, and presents detailed empirical studies regarding its performance on nine HiDS matrices from industrial applications currently in use. Therefore, its capability of addressing HiDS matrices is justified in both theory and practice.																	2168-2267	2168-2275				MAY	2020	50	5					1844	1855		10.1109/TCYB.2019.2894283													
J								Event-Triggered Consensus Control for Multi-Agent Systems Against False Data-Injection Attacks	IEEE TRANSACTIONS ON CYBERNETICS										Event-triggered mechanism; false data-injection attacks (FDIAs); multiagent systems (MASs)	MULTIAREA POWER-SYSTEMS; LOAD FREQUENCY CONTROL; COOPERATIVE CONTROL; SYNCHRONIZATION; NETWORKS	In this article, the event-triggered security consensus problem is studied for time-varying multiagent systems (MASs) against false data-injection attacks (FDIAs) and parameter uncertainties over a given finite horizon. In the process of information transmission, the malicious attacker tries to inject false signals to destroy consensus by compromising the integrity of measurements and control signals. The randomly occurring stealthy FDIAs on sensors and actuators are modeled by the Bernoulli processes. In order to reduce the unnecessary utilization of communication resources, an event-triggered control mechanism with state-dependent threshold is adopted to update the control input signal. The main objective of this article is to design a controller such that, under randomly occurring FDIAs and admissible parameter uncertainties, the MASs achieve consensus. By utilizing stochastic analysis method, two sufficient criteria are derived to ensure that the prescribed $H_{\infty }$ consensus performance can be achieved. Then, the desired controller gains are derived by solving recursive linear matrix inequalities. Simulation results are presented to illustrate the effectiveness and applicability of the proposed control method.																	2168-2267	2168-2275				MAY	2020	50	5					1856	1866		10.1109/TCYB.2019.2937951													
J								Adaptive Tracking Control of Wheeled Inverted Pendulums With Periodic Disturbances	IEEE TRANSACTIONS ON CYBERNETICS										Stability analysis; Adaptive control; Frequency modulation; Sun; Trajectory; Adaptation models; Adaptive tracking control; mobile-wheeled inverted pendulums (MWIPs); period disturbance; repetitive learning	MOTION/FORCE CONTROL; FUZZY CONTROL; LEARNING CONTROL; SYSTEMS; DESIGN	This paper reports our study on adaptive tracking control for a mobile-wheeled inverted pendulum with periodic disturbances and parametric uncertainties. With an appropriate reduced dynamic model, incorporating repetitive learning strategies with dynamic decoupling and related adaptive control techniques, a novel controller is successfully constructed to ensure that the output tracking errors of the system will stay within a small neighborhood around zero and all of the other signals are semiglobal uniform bounded. Meanwhile, only one parameter estimation is used for adaptive controller design, which overcomes the problem of over-parametrization. Furthermore, a required condition of period identifier mechanisms is proposed. Finally, detailed simulation results are presented to demonstrate the effectiveness of the proposed control schemes.																	2168-2267	2168-2275				MAY	2020	50	5					1867	1876		10.1109/TCYB.2018.2884707													
J								Multiple Lyapunov Functions for Adaptive Neural Tracking Control of Switched Nonlinear Nonlower-Triangular Systems	IEEE TRANSACTIONS ON CYBERNETICS										Switches; Adaptive systems; Artificial neural networks; Switched systems; Lyapunov methods; Nonlinear systems; Adaptive tracking control; multiple Lyapunov functions; neural networks (NNs); nonlower-triangular structure; switched nonlinear systems	STABILIZATION; STABILITY; APPROXIMATION; CONSENSUS; GAIN	In this paper, the problem of adaptive neural tracking control for a type of uncertain switched nonlinear nonlower-triangular system is considered. The innovations of this paper are summarized as follows: 1) input to state stability of unmodeled dynamics is removed, which is an indispensable assumption for the design of nonswitched unmodeled dynamic systems; 2) the design difficulties caused by the nonlower-triangular structure is handled by applying the universal approximation ability of radial basis function neural networks and the inherent properties of Gaussian functions, which avoids the restriction that the monotonously increasing bounding functions of the nonlower-triangular system functions must exist; and 3) multiple Lyapunov functions are utilized to develop a backstepping-like recursive design procedure such that the solvability of the adaptive neural tracking control issue of all subsystems is unnecessary. Based on the proposed controller design methods, it can be obtained that all signals in the closed-loop switched system remain bounded and the tracking error can eventually converge to a small neighborhood of the origin. In the simulation study, two examples are supplied to prove the practicability and feasibility of the developed design schemes.																	2168-2267	2168-2275				MAY	2020	50	5					1877	1886		10.1109/TCYB.2019.2906372													
J								Guidance-Error-Based Robust Fuzzy Adaptive Control for Bottom Following of a Flight-Style AUV With Saturated Actuator Dynamics	IEEE TRANSACTIONS ON CYBERNETICS										Adaptation models; Mathematical model; Actuators; Vehicle dynamics; Nonlinear dynamical systems; Backstepping; Sea surface; Actuator dynamics; actuator saturation; autonomous underwater vehicle (AUV); fuzzy adaptive control; robust control	AUTONOMOUS UNDERWATER VEHICLE; TRAJECTORY TRACKING CONTROL; MARINE SURFACE VEHICLES; DEPTH CONTROL; SUBJECT; VESSEL; TRENDS	This paper addresses the problem of robust bottom following control for a flight-style autonomous underwater vehicle (AUV) subject to system uncertainties, actuator dynamics, and input saturation. First, the actuator dynamics that is approximated by a first-order differential equation is inserted into the AUV dynamics model, which renders a high-order nonlinear dynamics analysis and design in the model-based backstepping controller by utilizing guidance errors. Second, to overcome the shaking control behavior resulted by the model-based high-order derivative calculation, a fuzzy approximator-based model-free controller is proposed, in order to online approximate the unknown part of the ideal backstepping architecture. In addition, the adaptive error estimation technology is resorted to compensate the system approximation error, ensuring that all the position and orientation errors of robust bottom following control tend to zero. Third, to further tackle the potential unstable control behavior from inherent saturation of control surfaces driven by rudders, an additional adaptive fuzzy compensator is introduced, in order to compensate control truncation between the unsaturated and saturation inputs. Subsequently, Lyapunov theory and Barbalat lemma are adopted to synthesize asymptotic stability of the entire bottom following control system. Finally, comparative numerical simulations with different controllers, environmental disturbances and initial states are provided to illustrate adaptability and robustness of the proposed bottom following controller for a flight-style AUV with saturated actuator dynamics.																	2168-2267	2168-2275				MAY	2020	50	5					1887	1899		10.1109/TCYB.2018.2890582													
J								Hidden Markov Model-Based Nonfragile State Estimation of Switched Neural Network With Probabilistic Quantized Outputs	IEEE TRANSACTIONS ON CYBERNETICS										Hidden Markov models; Artificial neural networks; Probabilistic logic; State estimation; Switches; Symmetric matrices; Hidden Markov model (HMM); nonfragile state estimation; probabilistic quantized output; switched neural network (SNN)	H-INFINITY; JUMP SYSTEMS; STABILIZATION; STABILITY; PASSIVITY; DESIGN; DELAYS	This paper focuses on the state estimator design problem for a switched neural network (SNN) with probabilistic quantized outputs, where the switching process is governed by a sojourn probability. It is assumed that both packet dropouts and signal quantization exist in communication channels. Asynchronous estimator and quantification function are described by two different hidden Markov model between the SNNs and its estimator. To deal with the small uncertain of estimators in a random way, a probabilistic nonfragile state estimator is introduced, where uncertain information is described by the interval type of gain variation. A sufficient condition on mean square stable of the estimation error system is obtained and then the desired estimator is designed. Finally, a simulation result is provided to verify the effectiveness of the proposed design method.																	2168-2267	2168-2275				MAY	2020	50	5					1900	1909		10.1109/TCYB.2019.2909748													
J								Distributed Set-Membership Filtering for Multirate Systems Under the Round-Robin Scheduling Over Sensor Networks	IEEE TRANSACTIONS ON CYBERNETICS										Protocols; Ellipsoids; Time-varying systems; Symmetric matrices; Kalman filters; Network topology; Topology; Distributed filtering; multirate mechanism; round-Robin (RR) protocol; sensor networks; set-membership filtering	H-INFINITY CONTROL; STATE ESTIMATION; FUSION ESTIMATION; MULTISENSOR	In this paper, the distributed set-membership filtering problem is dealt with for a class of time-varying multirate systems in sensor networks with the communication protocol. For relieving the communication burden, the round-Robin (RR) protocol is exploited to orchestrate the transmission order, under which each sensor node only broadcasts partial information to both the corresponding local filter and its neighboring nodes. In order to meet the practical transmission requirements as well as reduce communication cost, the multirate strategy is proposed to govern the sampling/update rate of the plant, the sensors, and the filters. By means of the lifting technique, the augmented filtering error system is established with a unified sampling rate. The main purpose of the addressed filtering problem is to design a set of distributed filters such that, in the simultaneous presence of the RR transmission protocol, the multirate mechanism, and the bounded noises, there exists a certain ellipsoid that includes all possible error states at each time instant. Then, the desired distributed filter gains are obtained by minimizing such an ellipsoid in the sense of the minimum trace of the weighted matrix. The proposed resource-efficient filtering algorithm is of a recursive form, thereby facilitating the online implementation. A numerical simulation example is given to demonstrate the effectiveness of the proposed protocol-based distributed filter design method.																	2168-2267	2168-2275				MAY	2020	50	5					1910	1920		10.1109/TCYB.2018.2885653													
J								Fault-Tolerant Consensus Tracking Control for Linear Multiagent Systems Under Switching Directed Network	IEEE TRANSACTIONS ON CYBERNETICS										Switches; Network topology; Fault tolerance; Fault tolerant systems; Topology; Actuators; Cooperative fault-tolerant control (CFTC); distributed reference observer (DRO); intermittent communications; linear multiagent systems (MASs); switching directed topology	TIME; SYNCHRONIZATION; COMMUNICATION; TOPOLOGY; TEAM	In this paper, for linear leader-follower networks with multiple heterogeneous actuator faults, including partial loss of effectiveness fault and actuator bias fault, a cooperative fault-tolerant control (CFTC) approach is developed. Assume that the interaction network topology among all nodes is a switching directed graph. To address the difficulty of designing the distributed compensation control laws under the time-varying asymmetrical network structure, a novel distributed-reference-observer-based fault-tolerant tracking control approach is established, under which the global tracking errors are proved to be asymptotically convergent in the presence of actuator failures. First, by constructing a group of distributed reference observers based on neighborhood state information, all followers can estimate the leader's state trajectories directly. Second, a decentralized adaptive fault-tolerant tracking controller via local estimation is designed to achieve the global synchronization. Furthermore, the reliable coordination problem under switching directed topology with intermittent communications is solved by utilizing the presented CFTC approach. Finally, the effectiveness of the proposed coordination control protocol is illustrated by its applications to a networked aircraft system.																	2168-2267	2168-2275				MAY	2020	50	5					1921	1930		10.1109/TCYB.2019.2901542													
J								Dissipative Filtering for Switched Fuzzy Systems With Missing Measurements	IEEE TRANSACTIONS ON CYBERNETICS										Switches; Fuzzy systems; Switched systems; Mathematical model; Standards; Stability analysis; Symmetric matrices; Filtering; fuzzy systems; missing measurements; switched systems	NETWORKED SYSTEMS; LINEAR-SYSTEMS; TIME; STABILITY	This paper investigates the dissipative filtering problem for a class of discrete-time switched fuzzy systems with missing measurements. The fuzzy plant under consideration incorporates characteristics of Takagi-Sugeno fuzzy systems and switched systems simultaneously. The occurrence of missing measurements is described by a stochastic variable that satisfies the Bernoulli binary distribution, which characterizes the effect of data loss in information transmission between the plant and the filter. Utilizing the Lyapunov function technique, sufficient conditions are developed to ensure that the resultant filtering error system is exponentially stable and strictly dissipative. Two simulation examples are presented to illustrate the validity of the proposed method.																	2168-2267	2168-2275				MAY	2020	50	5					1931	1940		10.1109/TCYB.2019.2908430													
J								Reliable Filter Design of Takagi-Sugeno Fuzzy Switched Systems With Imprecise Modes	IEEE TRANSACTIONS ON CYBERNETICS										Fuzzy systems; hybrid systems; mode asynchronization; reliable filter; sensor failures	MARKOV JUMP SYSTEMS; TRANSITION-PROBABILITIES; NONLINEAR-SYSTEMS; SPEED CONTROL; DELAY; NETWORKS	This paper is concerned with the problem of asynchronous and reliable filter design with performance constraint for nonlinear Markovian jump systems which are modeled as a kind of Takagi-Sugeno fuzzy switched systems. The nonstationary Markov chain is adopted to represent the asynchronous situation between the designed filter and the considered system. By using the mode-dependent Lyapunov function approach and the relaxation matrix technique, a sufficient condition is proposed to ensure the filtering error system, which is a dual randomly switched system, is stochastically stable and satisfies a given performance index simultaneously. Two different approaches are developed to construct the asynchronous and reliable filter. Owing to the Finsler's lemma, the second approach has fewer decision variables and less conservatism than the first one. Finally, two examples are provided to show the correctness and effectiveness of the proposed methods.																	2168-2267	2168-2275				MAY	2020	50	5					1941	1951		10.1109/TCYB.2018.2885505													
J								Observer-Based Event-Triggered Control for Networked Linear Systems Subject to Denial-of-Service Attacks	IEEE TRANSACTIONS ON CYBERNETICS										Jamming; Observers; Denial-of-service attack; Power system stability; Stability analysis; Intelligent sensors; Cyber-physical systems (CPSs); denial-of-service (DoS) attacks; networked control systems (NCSs); piecewise Lyapunov-Krasovskii functional; resilient event-triggered communication scheme	H-INFINITY; STABILIZING CONTROL; RESILIENT; INEQUALITY	This paper is concerned with the observer-based event-triggered control for a continuous networked linear system subject to denial-of-service (DoS) attacks, where the attacks are launched periodically to block the data transmission in control channels. First, a new observer state-based resilient event-triggering scheme is developed in the presence of DoS attacks. Second, a novel event-based switched system model is established by considering the effect of the event-triggering scheme and DoS attacks simultaneously. By virtue of this new model combined with a piecewise Lyapunov-Krasovskii functional method, the sufficient conditions are derived to guarantee exponential stability of the resulting switched system. It is shown that the proposed results can establish a quantitative relationship among the launching/sleeping periods of the attacks, the event-triggering parameters, the sampling period, and the exponential decay rate. Third, criteria for designing a desired observer-based event-triggered controller are provided and expressed in terms of a set of linear matrix inequalities. Finally, an offshore structure model is presented to illustrate the efficiency of the developed control method.																	2168-2267	2168-2275				MAY	2020	50	5					1952	1964		10.1109/TCYB.2019.2903817													
J								Spectral Analysis of Epidemic Thresholds of Temporal Networks	IEEE TRANSACTIONS ON CYBERNETICS										Silicon; Markov processes; Analytical models; Computer science; Matrix decomposition; Cybernetics; Spectral analysis; Bursts; discrete-time Markov chain approach; epidemic threshold; spectral radius; temporal networks; time-reversed characteristic	INFORMATION DISSEMINATION; SPREAD; PREDICTION	Many complex systems can be modeled as temporal networks with time-evolving connections. The influence of their characteristics on epidemic spreading is analyzed in a susceptible-infected-susceptible epidemic model illustrated by the discrete-time Markov chain approach. We develop the analytical epidemic thresholds in terms of the spectral radius of weighted adjacency matrix by averaging temporal networks, e.g., periodic, nonperiodic Markovian networks, and a special nonperiodic non-Markovian network (the link activation network) in time. We discuss the impacts of statistical characteristics, e.g., bursts and duration heterogeneity, as well as time-reversed characteristic on epidemic thresholds. We confirm the tightness of the proposed epidemic thresholds with numerical simulations on seven artificial and empirical temporal networks and show that the epidemic threshold of our theory is more precise than those of previous studies.																	2168-2267	2168-2275				MAY	2020	50	5					1965	1977		10.1109/TCYB.2017.2743003													
J								Output-Based Dynamic Event-Triggered Mechanisms for Disturbance Rejection Control of Networked Nonlinear Systems	IEEE TRANSACTIONS ON CYBERNETICS										Generalized proportional-integral observer (GPIO); networked nonlinear systems; nonlinear uncertainties and disturbances; output-based dynamic event-triggered mechanism (ETM)	LINEAR MULTIAGENT SYSTEMS; SENSOR NETWORKS; H-INFINITY; CONSENSUS; GAIN	This paper proposes a new output-based dynamic event-triggered mechanism (ETM) for disturbance rejection control of a class of networked nonlinear uncertain systems subject to additive time-varying disturbance. In the proposed control method, a new robust output feedback controller is first designed based on a generalized proportional-integral observer to attenuate/compensate the undesirable influence of nonlinear uncertainties and disturbances. Different from the static ETM, two new dynamic variables are defined, and thereafter, two kinds of different discrete-time dynamic ETMs are developed only using the sampled-data output signal, such that a better tradeoff between the communication properties and the control properties can be obtained. It is shown that under the proposed control methods, the global bounded stability of the closed-loop hybrid system can be guaranteed by choosing some appropriate parameters. Finally, the numerical simulations of a single link robot arm are conducted to demonstrate the feasibility and efficacy of the proposed control approach.																	2168-2267	2168-2275				MAY	2020	50	5					1978	1988		10.1109/TCYB.2018.2877413													
J								Selection of Robust and Relevant Features for 3-D Steganalysis	IEEE TRANSACTIONS ON CYBERNETICS										Feature extraction; Training; Shape; Robustness; Machine learning algorithms; Machine learning; Data mining; 3-D steganalysis; cover source mismatch; feature selection	MUTUAL INFORMATION; WATERMARKING; STEGANOGRAPHY; ALGORITHM	While 3-D steganography and digital watermarking represent methods for embedding information into 3-D objects, 3-D steganalysis aims to find the hidden information. Previous research studies have shown that by estimating the parameters modeling the statistics of 3-D features and feeding them into a classifier we can identify whether a 3-D object carries secret information. For training the steganalyzer, such features are extracted from cover and stego pairs, representing the original 3-D objects and those carrying hidden information. However, in practical applications, the steganalyzer would have to distinguish stego-objects from cover-objects, which most likely have not been used during the training. This represents a significant challenge for existing steganalyzers, raising a challenge known as the cover source mismatch (CSM) problem, which is due to the significant limitation of their generalization ability. This paper proposes a novel feature selection algorithm taking into account both feature robustness and relevance in order to mitigate the CSM problem in 3-D steganalysis. In the context of the proposed methodology, new shapes are generated by distorting those used in the training. Then a subset of features is selected from a larger given set, by assessing their effectiveness in separating cover-objects from stego-objects among the generated sets of objects. Two different measures are used for selecting the appropriate features: 1) the Pearson correlation coefficient and 2) the mutual information criterion.																	2168-2267	2168-2275				MAY	2020	50	5					1989	2001		10.1109/TCYB.2018.2883082													
J								Transforming Cooling Optimization for Green Data Center via Deep Reinforcement Learning	IEEE TRANSACTIONS ON CYBERNETICS										Cooling; Optimization; Mathematical model; Computational modeling; Software algorithms; Data models; Atmospheric modeling; Data center (DC) cooling optimization; deep learning; reinforcement learning (RL)	SYSTEMS; ALGORITHM	Data center (DC) plays an important role to support services, such as e-commerce and cloud computing. The resulting energy consumption from this growing market has drawn significant attention, and noticeably almost half of the energy cost is used to cool the DC to a particular temperature. It is thus an critical operational challenge to curb the cooling energy cost without sacrificing the thermal safety of a DC. The existing solutions typically follow a two-step approach, in which the system is first modeled based on expert knowledge and, thus, the operational actions are determined with heuristics and/or best practices. These approaches are often hard to generalize and might result in suboptimal performances due to intrinsic model errors for large-scale systems. In this paper, we propose optimizing the DC cooling control via the emerging deep reinforcement learning (DRL) framework. Compared to the existing approaches, our solution lends itself an end-to-end cooling control algorithm (CCA) via an off-policy offline version of the deep deterministic policy gradient (DDPG) algorithm, in which an evaluation network is trained to predict the DC energy cost along with resulting cooling effects, and a policy network is trained to gauge optimized control settings. Moreover, we introduce a de-underestimation (DUE) validation mechanism for the critic network to reduce the potential underestimation of the risk caused by neural approximation. Our proposed algorithm is evaluated on an EnergyPlus simulation platform and on a real data trace collected from the National Super Computing Centre (NSCC) of Singapore. The resulting numerical results show that the proposed CCA can achieve up to 11% cooling cost reduction on the simulation platform compared with a manually configured baseline control algorithm. In the trace-based study of conservative nature, the proposed algorithm can achieve about 15% cooling energy savings on the NSCC data trace. Our pioneering approach can shed new light on the application of DRL to optimize and automate DC operations and management, potentially revolutionizing digital infrastructure management with intelligence.																	2168-2267	2168-2275				MAY	2020	50	5					2002	2013		10.1109/TCYB.2019.2927410													
J								Finite-Time Passivity of Adaptive Coupled Neural Networks With Undirected and Directed Topologies	IEEE TRANSACTIONS ON CYBERNETICS										Synchronization; Artificial neural networks; Couplings; Network topology; Topology; Adaptation models; Adaptive systems; Adaptive coupling weights; coupled neural networks (CNNs); finite-time passivity (FTP); finite-time synchronization (FTS)	ROBUST SYNCHRONIZATION; PINNING CONTROL	In this paper, the finite-time passivity (FTP) problem for two classes of coupled neural networks (CNNs) with adaptive coupling weights is discussed. By selecting appropriate adaptive laws and controllers, several FTP conditions are given for CNNs with undirected and directed topologies. Furthermore, some finite-time synchronization conditions are also established by employing the FTP of the CNNs. At last, two numeral examples are used to check the correctness of the obtained criteria.																	2168-2267	2168-2275				MAY	2020	50	5					2014	2025		10.1109/TCYB.2018.2882252													
J								Quasi-Synchronization of Discrete-Time Lur'e-Type Switched Systems With Parameter Mismatches and Relaxed PDT Constraints	IEEE TRANSACTIONS ON CYBERNETICS										Switches; Synchronization; Switched systems; Receivers; Observers; Transmitters; Symmetric matrices; Average dwell time (ADT); error reachable set; Lur'e-type switched systems; parameter mismatches; persistent dwell time (PDT); synchronization; transmission channel noise	MASTER-SLAVE SYNCHRONIZATION; REACHABLE SET ESTIMATION; LINEAR-SYSTEMS; NEURAL-NETWORKS; STABILITY; STABILIZATION; FRAMEWORK; OBSERVER	This paper investigates the problem of quasi-synchronization for a class of discrete-time Lur'e-type switched systems with parameter mismatches and transmission channel noises. Different from the previous studies referring to the persistent dwell-time (PDT) switching signals, the average dwell-time (ADT) constraints combined with the PDT are considered simultaneously in this paper to relax the limitation of dwell-time requirements and to improve the flexibility of the PDT switching signal design. By virtue of the semi-time-varying (STV) Lyapunov function, the synchronization criteria for transmitter-receiver systems in a switched version are obtained to satisfy a prescribed synchronization error bound. An estimate of the synchronization error bound is provided via the reachable set approach and, further, an explicit description of the error bounds is given. Then, sufficient conditions on the existence of STV observers are derived with a predetermined error bound, and the corresponding observer gains are calculated via solving a group of linear matrix inequalities. Finally, the effectiveness and validness of the developed theoretical results are demonstrated via a numerical example.																	2168-2267	2168-2275				MAY	2020	50	5					2026	2037		10.1109/TCYB.2019.2930945													
J								Multiview Semantic Representation for Visual Recognition	IEEE TRANSACTIONS ON CYBERNETICS										Image classification; multiview; object categorization; semantic representation; visual recognition	IMAGE CLASSIFICATION; LOW-RANK; OBJECT CATEGORIZATION; FUSION; FACE	Due to interclass and intraclass variations, the images of different classes are often cluttered which makes it hard for efficient classifications. The use of discriminative classification algorithms helps to alleviate this problem. However, it is still an open problem to accurately model the relationships between visual representations and human perception. To alleviate these problems, in this paper, we propose a novel multiview semantic representation (MVSR) algorithm for efficient visual recognition. First, we leverage visually based methods to get initial image representations. We then use both visual and semantic similarities to divide images into groups which are then used for semantic representations. We treat different image representation strategies, partition methods, and numbers as different views. A graph is then used to combine the discriminative power of different views. The similarities between images can be obtained by measuring the similarities of graphs. Finally, we train classifiers to predict the categories of images. We evaluate the discriminative power of the proposed MVSR method for visual recognition on several public image datasets. Experimental results show the effectiveness of the proposed method.																	2168-2267	2168-2275				MAY	2020	50	5					2038	2049		10.1109/TCYB.2018.2875728													
J								Embedding Attention and Residual Network for Accurate Salient Object Detection	IEEE TRANSACTIONS ON CYBERNETICS										Object detection; Feature extraction; Visualization; Image color analysis; Semantics; Noise measurement; Task analysis; Deep supervision; residual refinement; salient object detection; top-down residual attention		Salient object detection is usually used as a preprocessing step to facilitate a variety of subsequent applications which should take little time cost. With the quick development of deep learning recently, profound progresses have been made to achieve a new state-of-the-art performance. However, the learned features of the existing deep learning-based methods are not accurate enough thus leading to unsatisfactory detection in complex scenes, such as low contrast or very similar between salient object and background region and multiple (small) salient objects with diverse characteristics. In addition, some post-processing techniques are usually needed for refinement, which is time consuming. To address these issues, this paper presents an efficient fully convolutional salient object detection network. Specifically, we first introduce a visual attention mechanism to guide feature learning in side output layers. In detail, attention weight is employed in a top-down manner which can bridge high level semantic information to help shallow layers better locate salient objects and also filter out noisy response in the background region. Second, we propose a residual refinement network to fuse the learned multilevel features gradually. Not to simply add or concatenate them step by step as previous works, we introduce a second-order term into element-wise addition to learn stage-wise residual features for refinement. Such a second-order term not only benefits efficient gradient propagation but also increases network nonlinearity. Extensive experiments on seven standard benchmarks demonstrate that the proposed approach achieves consistently superior performance and performs well on small salient object detection in comparison with the very recent state-of-the-arts, especially in the metric of structure-measure.																	2168-2267	2168-2275				MAY	2020	50	5					2050	2062		10.1109/TCYB.2018.2879859													
J								Second-Order Consensus for Multiagent Systems via Intermittent Sampled Position Data Control	IEEE TRANSACTIONS ON CYBERNETICS										Multi-agent systems; Protocols; Topology; Eigenvalues and eigenfunctions; Laplace equations; Delay effects; Couplings; Communication width; intermittent sampled position data; multiagent system; sampling period; second-order consensus	SUFFICIENT CONDITIONS; AGENTS; CONTROLLABILITY; NETWORKS; DYNAMICS; LEADER	In this paper, a second-order consensus for multiagent systems with a directed communication topology is studied. A novel consensus strategy is first proposed, where a periodic intermittent control strategy only with casual sampled position data is used, which not only decreases the operating time and the update rates of conditioners for every individual but also responds effectively to the case of missing velocity information. A necessary and sufficient consensus condition based on the coupling gains, the sampling period, the communication width, and the spectrum of the Laplacian matrix is established to reach the consensus, and the right intervals of the sampling period are given. Furthermore, a delay-induced consensus protocol is designed, and a necessary and sufficient condition is also given, by which the sampling period and the communication width can easily be chosen to achieve the consensus. At last, some simulation examples are given to verify the correctness of the theoretical results.																	2168-2267	2168-2275				MAY	2020	50	5					2063	2072		10.1109/TCYB.2018.2879327													
J								Fast Covariance Matrix Adaptation for Large-Scale Black-Box Optimization	IEEE TRANSACTIONS ON CYBERNETICS										Covariance matrices; Optimization; Matrix decomposition; Search problems; Adaptation models; Complexity theory; Task analysis; Ensemble model; evolution strategies; large scale optimization; low-rank model	CMA EVOLUTION STRATEGY; ES; CONVERGENCE; ALGORITHMS; SEARCH; TIME	Covariance matrix adaptation evolution strategy (CMA-ES) is a successful gradient-free optimization algorithm. Yet, it can hardly scale to handle high-dimensional problems. In this paper, we propose a fast variant of CMA-ES (Fast CMA-ES) to handle large-scale black-box optimization problems. We approximate the covariance matrix by a low-rank matrix with a few vectors and use two of them to generate each new solution. The algorithm achieves linear internal complexity on the dimension of search space. We illustrate that the covariance matrix of the underlying distribution can be considered as an ensemble of simple models constructed by two vectors. We experimentally investigate the algorithm's behaviors and performances. It is more efficient than the CMA-ES in terms of running time. It outperforms or performs comparatively to the variant limited memory CMA-ES on large-scale problems. Finally, we evaluate the algorithm's performance with a restart strategy on the CEC'2010 large-scale global optimization benchmarks, and it shows remarkable performance and outperforms the large-scale variants of the CMA-ES.																	2168-2267	2168-2275				MAY	2020	50	5					2073	2083		10.1109/TCYB.2018.2877641													
J								e-RNSP: An Efficient Method for Mining Repetition Negative Sequential Patterns	IEEE TRANSACTIONS ON CYBERNETICS										Data mining; Databases; Insurance; Companies; DNA; Medical services; Automobiles; Negative sequential patterns (NSPs); repetition NSPs (RNSPs); repetition patterns; sequence analysis	FREQUENT PATTERNS; PERIODIC PATTERNS; RULES	Negative sequential patterns (NSPs), which capture both frequent occurring and nonoccurring behaviors, become increasingly important and sometimes play a role irreplaceable by analyzing occurring behaviors only. Repetition sequential patterns capture repetitions of patterns in different sequences as well as within a sequence and are very important to understand the repetition relations between behaviors. Though some methods are available for mining NSP and repetition positive sequential patterns (RPSPs), we have not found any methods for mining repetition NSP (RNSP). RNSP can help the analysts to further understand the repetition relationships between items and capture more comprehensive information with repetition properties. However, mining RNSP is much more difficult than mining NSP due to the intrinsic challenges of nonoccurring items. To address the above issues, we first propose a formal definition of repetition negative containment. Then, we propose a method to convert repetition negative containment to repetition positive containment, which fast calculates the repetition supports by only using the corresponding RPSP's information without rescanning databases. Finally, we propose an efficient algorithm, called e-RNSP, to mine RNSP efficiently. To the best of our knowledge, e-RNSP is the first algorithm to efficiently mine RNSP. Intensive experimental results on the first four real and synthetic datasets clearly show that e-RNSP can efficiently discover the repetition negative patterns; results on the fifth dataset prove the effectiveness of RNSP which are captured by the proposed method; and the results on the rest 16 datasets analyze the impacts of data characteristics on mining process.																	2168-2267	2168-2275				MAY	2020	50	5					2084	2096		10.1109/TCYB.2018.2869907													
J								Robust Point Set Registration Using Signature Quadratic Form Distance	IEEE TRANSACTIONS ON CYBERNETICS										Robustness; Iterative closest point algorithm; Optimization; Gaussian distribution; Three-dimensional displays; Linear programming; Measurement; Gaussian mixture model (GMM); point cloud matching; point set registration; rigid registration; signature quadratic form distance	TRANSFORMATION; ALGORITHM; ICP	Point set registration is a problem with a long history in many pattern recognition tasks. This paper presents a robust point set registration algorithm based on optimizing the distance between two probability distributions. A major problem in point to point algorithms is defining the correspondence between two point sets. This paper follows the idea of some probability-based point set registration methods by representing the point sets as Gaussian mixture models (GMMs). By optimizing the distance between the two GMMs, rigid transformations (rotation and translation) between two point sets can be obtained without having to find a correspondence. Previous studies have used L2, Kullback Leibler, etc. distance to measure similarity between two GMMs; however, these methods have problems with robustness to noise and outliers, especially when the covariance matrix is large, or a local minimum exists. Therefore, in this paper, the signature quadratic form distance is derived to measure the distribution similarity. The contribution of this paper lies in adopting the signature quadratic form distance for the point set registration algorithm. The experimental results show the precision and robustness of this algorithm and demonstrate that it outperforms other state-of-the-art point set registration algorithms regarding factors, such as noise, outliers, missing partial structures, and initial misalignment.																	2168-2267	2168-2275				MAY	2020	50	5					2097	2109		10.1109/TCYB.2018.2845745													
J								A Stabilized Feedback Episodic Memory (SF-EM) and Home Service Provision Framework for Robot and IoT Collaboration	IEEE TRANSACTIONS ON CYBERNETICS										Subspace constraints; Smart homes; Memory architecture; Cognition; Robot sensing systems; Service robots; Ambient intelligence; Internet of Things (IoT); learning systems; memory architecture; service robots; Smart Home	WIRELESS SENSOR; INTERNET; SYSTEM; THINGS	The automated home referred to as Smart Home is expected to offer fully customized services to its residents, reducing the amount of home labor, thus improving human beings' welfare. Service robots and Internet of Things (IoT) play the key roles in the development of Smart Home. The service provision with these two main components in a Smart Home environment requires: 1) learning and reasoning algorithms and 2) the integration of robot and IoT systems. Conventional computational intelligence-based learning and reasoning algorithms do not successfully manage dynamic changes in the Smart Home data, and the simple integrations fail to fully draw the synergies from the collaboration of the two systems. To tackle these limitations, we propose: 1) a stabilized memory network with a feedback mechanism which can learn user behaviors in an incremental manner and 2) a robot-IoT service provision framework for a Smart Home which utilizes the proposed memory architecture as a learning and reasoning module and exploits synergies between the robot and IoT systems. We conduct a set of comprehensive experiments under various conditions to verify the performance of the proposed memory architecture and the service provision framework and analyze the experiment results.																	2168-2267	2168-2275				MAY	2020	50	5					2110	2123		10.1109/TCYB.2018.2882921													
J								Multiview Classification With Cohesion and Diversity	IEEE TRANSACTIONS ON CYBERNETICS										Kernel; Support vector machines; Training; Optimization; Data models; Linear programming; Hilbert space; Cohesion; complementarity; consensus; diversity; multiview classification	IMAGE FEATURES; REPRESENTATION; SCALE	Different views of multiview data share certain common information (consensus) and also contain some complementary information (complementarity). Both consensus and complementarity are of significant importance to the success of multiview learning. In this paper, we explicitly formulate both of them for multiview classification. On the one hand, a cohesion-increasing loss term with a learnable label-adjusting matrix is designed to facilitate consensus among views in the training stage. With this kind of loss, the learned classifiers of all views are more likely to obtain the correct classification, thereby maximizing the agreement among views. On the other hand, an independence measurement is adopted as the diversity-promoting regularization to encourage classifiers to be diverse such that more complementary information can be captured by these "diversified" classifiers. Overall, the resultant model is capable of achieving more comprehensive and accurate classification by exploring and exploiting the common and complementary information across multiple views more thoroughly. An iterative optimization algorithm with proved convergence is proposed for training the model. Extensive experimental results on various datasets have demonstrated the efficacy of the proposed method.																	2168-2267	2168-2275				MAY	2020	50	5					2124	2137		10.1109/TCYB.2018.2881474													
J								Recursive Discriminative Subspace Learning With l(1)-Norm Distance Constraint	IEEE TRANSACTIONS ON CYBERNETICS										Discriminative features; l(1)-norm; recursive algorithm; subspace learning	MATRIX FACTORIZATION; L1-NORM; EFFICIENT	In feature learning tasks, one of the most enormous challenges is to generate an efficient discriminative subspace. In this paper, we propose a novel subspace learning method, named recursive discriminative subspace learning with an l(1)-norm distance constraint (RDSL). RDSL can robustly extract features from the contaminated images and learn a discriminative subspace. With the use of an inequation-based l(1)-norm distance metric constraint, the minimized l(1)-norm distance metric objective function with slack variables induces samples in the same class to cluster as close as possible, meanwhile samples from different classes can be separated from each other as far as possible. By utilizing l(1)-norm items in both the objective function and the constraint, RDSL can well handle the noisy data and outliers. In addition, the large margin formulation makes the proposed method insensitive to initializations. We describe two approaches to solve RDSL with a recursive strategy. Experimental results on six benchmark datasets, including the original data and the contaminated data, demonstrate that RDSL outperforms the state-of-the-art methods.																	2168-2267	2168-2275				MAY	2020	50	5					2138	2151		10.1109/TCYB.2018.2882924													
J								Outdoor Shadow Estimating Using Multiclass Geometric Decomposition Based on BLS	IEEE TRANSACTIONS ON CYBERNETICS										Lighting; Sun; Estimation; Learning systems; Feature extraction; Neural networks; Classification algorithms; Broad learning system (BLS); illumination estimating; Markov random field (MRF); multiclass integrating; shadow synthesis	ILLUMINATION; RECOGNITION; SYNOPSIS; VIDEO	Illumination is a significant component of an image, and illumination estimation of an outdoor scene from given images is still challenging yet it has wide applications. Most of the traditional illumination estimating methods require prior knowledge or fixed objects within the scene, which makes them often limited by the scene of a given image. We propose an optimization approach that integrates the multiclass cues of the image(s) [a main input image and optional auxiliary input image(s)]. First, Sun visibility is estimated by the efficient broad learning system. And then for the scene with visible Sun, we classify the information in the image by the proposed classification algorithm, which combines the geometric information and shadow information to make the most of the information. And we apply a respective algorithm for every class to estimate the illumination parameters. Finally, our approach integrates all of the estimating results by the Markov random field. We make full use of the cues in the given image instead of an extra requirement for the scene, and the qualitative results are presented and show that our approach outperformed other methods with similar conditions.																	2168-2267	2168-2275				MAY	2020	50	5					2152	2165		10.1109/TCYB.2018.2875983													
J								Event-Based Control for Networked T-S Fuzzy Systems via Auxiliary Random Series Approach	IEEE TRANSACTIONS ON CYBERNETICS										Delays; Propagation losses; Data models; Bandwidth; Closed loop systems; Stability analysis; Asynchronous membership functions; auxiliary random series; data losses; networked control systems; transmission delays	H-INFINITY CONTROL; COOPERATIVE CONTROL; STABILITY ANALYSIS; FAULT-DETECTION; LINEAR-SYSTEMS; MODEL; STABILIZATION	This paper presents an auxiliary random series approach to model the effect of network induced problems, such as data losses and transmission delay subject to event-based communication scheme for nonlinear continuous time systems. T-S fuzzy model is employed to describe the nonlinear systems. In order to save the bandwidth and energy, we introduce the event-triggered mechanism to reduce the number of data for transmission and computation. Thus, it is necessary to consider the influence of data losses, data disorder, and transmission delay since the transmitted data packets become more important. Consequently, it is very complicated to analyze the performance of such networked system and one of the most difficult part, in the authors' opinion, is to construct the mathematical model of closed-loop systems. In this paper, we present an auxiliary random series approach to describe the data transmitted in the system, and therefore, the closed-loop systems can be obtained. Associated with a tailor-made Lyapunov-Krasovskii functional, the stability analysis is processed and a fuzzy controller is designed. Asynchronous membership functions are considered to obtain more relaxed stability conditions. To clarify the effectiveness of the proposed method, a cart-damper-spring system is employed for simulation.																	2168-2267	2168-2275				MAY	2020	50	5					2166	2175		10.1109/TCYB.2018.2869418													
J								Convergent Estimation Mechanism Design for Nonlinear Fuzzy Systems With Faults	IEEE TRANSACTIONS ON CYBERNETICS										Fuzzy systems; Observers; Estimation error; Convergence; Time-varying systems; Nonlinear systems; Convergent estimation mechanism (CEM); fuzzy iterative estimation observers (FIEOs); linear matrix inequalities (LMIs); mean sequence of estimation errors; Takagi-Sugeno (T-S) fuzzy systems	TOLERANT CONTROLLER-DESIGN; STATE ESTIMATION; TRACKING CONTROL; FILTER DESIGN; OBSERVER; STABILITY; ACCOMMODATION; DELAY	The convergent estimation for a class of nonlinear Takagi-Sugeno fuzzy systems is concerned, where time-varying process faults and input disturbances are both involved. A convergent estimation mechanism (CEM) based on a set of fuzzy iterative estimation observers is constructed for the nonlinear fuzzy system; meanwhile, the convergence of the mean sequence of estimation errors (for both states and faults) to zero (vector) is proved. However, in the existing literature, the estimation errors can only be proved to be uniformly ultimately bounded when the fault is time varying. In the design procedure, the disturbances on systems in consideration can be isolated effectively in the obtained fuzzy iterative error dynamics through introducing a suitable isolation technique. Numerical examples give the simulation results to show the effectiveness and merits of the proposed CEM.																	2168-2267	2168-2275				MAY	2020	50	5					2176	2185		10.1109/TCYB.2018.2884221													
J								The Set-Based Hypervolume Newton Method for Bi-Objective Optimization	IEEE TRANSACTIONS ON CYBERNETICS										Optimization; Newton method; Evolutionary computation; Jacobian matrices; Approximation algorithms; Sociology; Statistics; Hessian matrix; hypervolume indicator; memetic algorithms; newton method; set-based local search	EVOLUTIONARY ALGORITHMS; SELECTION; HYBRID	In this paper, we propagate the use of a set-based Newton method that enables computing a finite size approximation of the Pareto front (PF) of a given twice continuously differentiable bi-objective optimization problem (BOP). To this end, we first derive analytically the Hessian matrix of the hypervolume indicator, a widely used performance indicator for PF approximation sets. Based on this, we propose the hypervolume Newton method (HNM) for hypervolume maximization of a given set of candidate solutions. We first address unconstrained BOPs and focus further on first attempts for the treatment of inequality constrained problems. The resulting method may even converge quadratically to the optimal solution, however, this property is-as for all Newton methods-of local nature. We hence propose as a next step a hybrid of HNM and an evolutionary strategy in order to obtain a fast and reliable algorithm for the treatment of such problems. The strengths of both HNM and hybrid are tested on several benchmark problems and comparisons of the hybrid to state-of-the-art evolutionary algorithms for hypervolume maximization are presented.																	2168-2267	2168-2275				MAY	2020	50	5					2186	2196		10.1109/TCYB.2018.2885974													
J								Distributed Event-Triggered Adaptive Control for Consensus of Linear Multi-Agent Systems with External Disturbances	IEEE TRANSACTIONS ON CYBERNETICS										Consensus; distributed control; event-triggered control; multi-agent systems; output feedback; robust adaptive control	SYNCHRONIZATION; NETWORKS; OBSERVER; FEEDBACK	This paper investigates the consensus problem of linear multi-agent systems subject to external disturbances via distributed event-triggered adaptive control. First, a distributed event-triggered adaptive output feedback control strategy is proposed for each agent. It is shown that under this control strategy, the consensus problem can be solved for any connected undirected communication graph in a fully distributed manner without using any global information. Then a distributed self-triggered adaptive output feedback control strategy is designed with which continuous monitoring of the measurement error is no longer needed. It is further shown that for the proposed event-triggered and self-triggered control strategies, no agent will exhibit Zeno behavior. Finally, the effectiveness of the proposed two control strategies is illustrated on a group of two-mass-spring systems.																	2168-2267	2168-2275				MAY	2020	50	5					2197	2208		10.1109/TCYB.2018.2881484													
J								A Many-Objective Particle Swarm Optimizer With Leaders Selected From Historical Solutions by Using Scalar Projections	IEEE TRANSACTIONS ON CYBERNETICS										Optimization; Particle swarm optimization; Density measurement; Computer science; Atmospheric measurements; Particle measurements; Leader selection; many-objective evolutionary algorithms (MaOEAs); many-objective optimization; particle swarm optimizer (PSO)	EVOLUTIONARY ALGORITHM; DECOMPOSITION; CONVERGENCE; DIVERSITY; PERFORMANCE; MOEA/D	The particle swarm optimizer (PSO), originally proposed for single-objective optimization problems, has been widely extended to other areas. One of them is multiobjective optimization. Recently, using the PSO to handle many-objective optimization problems (MaOPs) (i.e., problems with more than three objectives) has caught increasing attention from the evolutionary multiobjective community. In the design of a multiobjective/many-objective PSO algorithm, the selection of leaders is a crucial issue. This paper proposes an effective many-objective PSO where the above issue is properly addressed. For each particle, the leader is selected from a certain number of historical solutions by using scalar projections. In the objective space, historical solutions record potential search directions, and the leader is elected as the solution that is closest to the Pareto front in the direction determined by the nadir point and the point constructed by the objective vector of this particle. The proposed algorithm is compared with eight state-of-the-art many-objective optimizers on 37 test problems in terms of four performance metrics. The experimental results have shown the superiority and competitiveness of our proposed algorithm. The new algorithm is free of a set of weight vectors and can handle Pareto fronts with irregular shapes. Given the high performance and good properties of the proposed algorithm, it can be used as a promising tool when dealing with MaOPs.																	2168-2267	2168-2275				MAY	2020	50	5					2209	2222		10.1109/TCYB.2018.2884083													
J								Stackelberg-Theoretic Approach for Performance Improvement in Fuzzy Systems	IEEE TRANSACTIONS ON CYBERNETICS										Games; Uncertainty; Robust control; Cost function; Control design; Fuzzy set theory; Fuzzy systems; leader-follower; optimal design; Riccati equation; robust control; Stackelberg strategy	UNIFORM ULTIMATE BOUNDEDNESS; ROBUST-CONTROL; UNCERTAIN SYSTEMS; CONTROL DESIGN; LOGIC	This paper investigates the robust control for dynamical systems subject to uncertainty. The uncertainty is assumed to be (possibly fast) time varying and bounded. The bound is unknown but lies within a prescribed fuzzy set (hence the fuzzy dynamical system). We propose an approach for the robust control design which is implemented in two steps. First, a class of robust controls is proposed based on tunable parameters. The proposed controls are deterministic and are not conventionally IF-THEN rules based. By the Lyapunov minimax approach, we prove that the proposed controls are able to guarantee deterministic system performance, namely, uniform boundedness and ultimate uniform boundedness. Second, optima seeking from the proposed controls is considered to improve fuzzy system performance. We formulate the optima-seeking problem as a two-player (one leader and one follower) Stackelberg game by developing two cost functions, each of which is in charge of one tunable parameter (i.e., the player). Each cost function consists of an average fuzzy system performance index and the associated player's control effort. We show that the solution of the optimal design problem (i.e., the optima of the tunable parameters), which is called the Stackelberg strategy, always exists and how to obtain the backwards-induction outcome is provided. Simulation results on the walking control of a biped robot model are presented for demonstration.																	2168-2267	2168-2275				MAY	2020	50	5					2223	2236		10.1109/TCYB.2018.2883729													
J								Generative and Discriminative Fuzzy Restricted Boltzmann Machine Learning for Text and Image Classification	IEEE TRANSACTIONS ON CYBERNETICS										Data models; Training; Neurons; Image reconstruction; Feature extraction; Computational modeling; Cybernetics; Discriminative learning; fuzzy number; Gaussian fuzzy restricted Boltzmann machine (GFRBM); image classification	POSSIBILISTIC MEAN-VALUE; NEURAL-NETWORK; RECOGNITION; SYSTEMS; LOGIC	The restricted Boltzmann machine (RBM) is an excellent generative learning model for feature extraction. By extending its parameters from real numbers to fuzzy ones, we have developed the fuzzy RBM (FRBM) which is demonstrated to possess better generative capability than RBM. In this paper, we first propose a generative model named Gaussian FRBM (GFRBM) to deal with real-valued inputs. Then, motivated by the fact that the discriminative variant of RBM can provide a self-contained framework for classification with competitive performance compared with some traditional classifiers, we establish the discriminative FRBM (DFRBM) and discriminative GFRBM (DGFRBM) that combine both the generative and discriminative facility by adding extra neurons next to the input units. Specifically, they can be trained into excellent stand-alone classifiers and retain outstanding generative capability simultaneously. The experimental results including text and image (both clean and noisy) classification indicate that DFRBM and DGFRBM outperform discriminative RBM models in terms of reconstruction and classification accuracy, and they behave more stable when encountering noisy data. Moreover, the proposed learning models show some promising advantages over other standard classifiers.																	2168-2267	2168-2275				MAY	2020	50	5					2237	2248		10.1109/TCYB.2018.2869902													
J								On Complete Stability of Recurrent Neural Networks With Time-Varying Delays and General Piecewise Linear Activation Functions	IEEE TRANSACTIONS ON CYBERNETICS										Complete stability; neural networks; piecewise linear activation functions; time-varying delays	GLOBAL EXPONENTIAL STABILITY; MULTIPLE EQUILIBRIA; MULTISTABILITY; CONVERGENCE; MULTIPERIODICITY	This paper addresses the problem of complete stability of delayed recurrent neural networks with a general class of piecewise linear activation functions. By applying an appropriate partition of the state space and iterating the defined bounding functions, some sufficient conditions are obtained to ensure that an n-neuron neural network is completely stable with exactly equilibrium points, among which equilibrium points are locally exponentially stable and the others are unstable, where are non-negative integers which depend jointly on activation functions and parameters of neural networks. The results of this paper include the existing works on the stability analysis of recurrent neural networks with piecewise linear functions as special cases and hence can be considered as the improvement and extension of the existing stability results in the literature. A numerical example is provided to illustrate the derived theoretical results.																	2168-2267	2168-2275				MAY	2020	50	5					2249	2263		10.1109/TCYB.2018.2884836													
J								H-infinity Output Consensus for Markov Jump Multiagent Systems With Uncertainties	IEEE TRANSACTIONS ON CYBERNETICS										H-infinity performance; hidden Markov model (HMM); Markov jump; multiagent systems (MASs); output feedback (OF) controller	FEEDBACK CONTROL; DISTRIBUTED CONSENSUS; SYNCHRONIZATION; PROTOCOLS; DELAYS	This paper investigates the H-infinity output consensus problem for multiagent systems with Markov jump and external disturbance in both continuous-time and discrete-time domains. The communication network is directed and fixed with uncertainties. Based on the hidden Markov model, an output feedback controller is constructed. Then, the original system is transformed into a reduced-order system, which features the error dynamics. By using a Lyapunov function, sufficient conditions are developed to ensure that all agents can reach the consensus with the desired H-infinity performance in the mean-square sense. Finally, simulation results are presented to illustrate the efficiency of the proposed approaches.																	2168-2267	2168-2275				MAY	2020	50	5					2264	2273		10.1109/TCYB.2018.2884762													
J								Noise-Tolerant Techniques for Decomposition-Based Multiobjective Evolutionary Algorithms	IEEE TRANSACTIONS ON CYBERNETICS										Adaptive sampling strategies (ASs); decomposition-based multiobjective evolutionary algorithm (DMOEA); mixed objective (MO) evaluation; mixed repair (MR) mechanism; nadir point estimation (PNE); noisy optimization	DIFFERENTIAL EVOLUTION; OPTIMIZATION; MOEA/D; ENVIRONMENTS	Over the last few decades, the decomposition-based multiobjective evolutionary algorithms (DMOEAs) have became one of the mainstreams for multiobjective optimization. However, there is not too much research on applying DMOEAs to uncertain problems until now. Usually, the uncertainty is modeled as additive noise in the objective space, which is the case this paper concentrates on. This paper first carries out experiments to examine the impact of noisy environments on DMOEAs. Then, four noise-handling techniques based upon the analyses of empirical results are proposed. First, a Pareto-based nadir point estimation strategy is put forward to provide a good normalization of each objective. Next, we introduce two adaptive sampling strategies that vary the number of samples used per solution based on the differences among neighboring solutions and their variance to control the tradeoff between exploration and exploitation. Finally, a mixed objective evaluation strategy and a mixed repair mechanism are proposed to alleviate the effects of noise and remedy the loss of diversity in the decision space, respectively. These features are embedded in two popular DMOEAs (i.e., MOEA/D and DMOEA-), and DMOEAs with these features are called noise-tolerant DMOEAs (NT-DMOEAs). NT-DMOEAs are compared with their various variants and four noise-tolerant multiobjective algorithms, including the improved NSGA-II, the classical algorithm Bayesian (1+1)-ES (BES), and the state-of-the-art algorithms MOP-EA and rolling tide evolutionary algorithm to show the superiority of proposed features on 17 benchmark problems with different strength levels of noise. Experimental studies demonstrate that two NT-DMOEAs, especially NT-DMOEA-C, show remarkable advantages over competitors in the majority of test instances.																	2168-2267	2168-2275				MAY	2020	50	5					2274	2287		10.1109/TCYB.2018.2881227													
J								Blind Audio-Visual Localization and Separation via Low-Rank and Sparsity	IEEE TRANSACTIONS ON CYBERNETICS										Visualization; Feature extraction; Sparse matrices; Matrix decomposition; Task analysis; Microphones; Spectrogram; Audio separation; audio-visual localization; low-rank; multimodal analysis; sparsity	RECOGNITION; TRACKING	The ability to localize visual objects that are associated with an audio source and at the same time to separate the audio signal is a cornerstone in audio-visual signal-processing applications. However, available methods mainly focus on localizing only the visual objects, without audio separation abilities. Besides that, these methods often rely on either laborious preprocessing steps to segment video frames into semantic regions, or additional supervisions to guide their localization. In this paper, we aim to address the problem of visual source localization and audio separation in an unsupervised manner and avoid all preprocessing or post-processing steps. To this end, we devise a novel structured matrix decomposition method that decomposes the data matrix of each modality as a superposition of three terms: 1) a low-rank matrix capturing the background information; 2) a sparse matrix capturing the correlated components among the two modalities and, hence, uncovering the sound source in visual modality and the associated sound in audio modality; and 3) a third sparse matrix accounting for uncorrelated components, such as distracting objects in visual modality and irrelevant sound in audio modality. The generality of the proposed method is demonstrated by applying it onto three applications, namely: 1) visual localization of a sound source; 2) visually assisted audio separation; and 3) active speaker detection. Experimental results indicate the effectiveness of the proposed method on these application domains.																	2168-2267	2168-2275				MAY	2020	50	5					2288	2301		10.1109/TCYB.2018.2883607													
J								Outlier Detection Using Structural Scores in a High-Dimensional Space	IEEE TRANSACTIONS ON CYBERNETICS										Anomaly detection; Adaptation models; Weight measurement; Engines; Data models; Cybernetics; Euclidean distance; Discrimination; outlier detection; outlier factor; structural scores	ALGORITHM	Outlier detection has drawn significant interest from both academia and industry, such as network intrusion detection. Most existing methods implicitly or explicitly rely on distances in Euclidean space. However, the Euclidean distance may be incapable of measuring the similarity among high-dimensional data due to the curse of dimensionality, thus leading to inferior performance in practice. This paper presents an innovative approach for outlier detection from the view of meaningful structure scores. If two points have similar features, the difference between their structural scores is small and vice versa. The scores are calculated by measuring the variance of angles weighted by data representation, which takes the global data structure into the measurement. Thus, it could consistently rank more similar points. Compared with existing methods, our structural scores could be better to reflect the characteristics of data in a high-dimensional space. The proposed method consistently ranks more similar points. Experiments on synthetic and several real-world datasets have demonstrated the effectiveness and efficiency of our proposed methods.																	2168-2267	2168-2275				MAY	2020	50	5					2302	2310		10.1109/TCYB.2018.2876615													
J								Hilbert Transform Design Based on Fractional Derivatives and Swarm Optimization	IEEE TRANSACTIONS ON CYBERNETICS										Evolutionary technique (ET); fractional derivative (FD); Hilbert transform (HT); particle swarm optimization (PSO)		This paper presents a new efficient method for implementing the Hilbert transform using an all-pass filter, based on fractional derivatives (FDs) and swarm optimization. In the proposed method, the squared error difference between the desired and designed responses of a filter is minimized. FDs are introduced to achieve higher accuracy at the reference frequency, which helps to reduce the overall phase error. In this paper, two approaches are used for finding the appropriate values of the FDs and reference frequencies. In the first approach, these values are estimated from a series of experiments, which require more computation time but produce less accurate results. These experiments, however, justify the behavior of the error function, with respect to the FD and as a multimodal and nonconvex problem. In the second approach, a variant of the swarm-intelligence-based multimodal search space technique, known as the constraint-factor particle swarm optimization, is exploited for finding the suitable values for the FD and w(0). The performance of the proposed FD-based method is measured in terms of fidelity aspects, such as the maximum phase error, total squared phase error, maximum group delay error, and total squared group delay error. The FD-based approach is found to reduce the total phase error by 57% by exploiting only two FDs.																	2168-2267	2168-2275				MAY	2020	50	5					2311	2320		10.1109/TCYB.2018.2875540													
J								Learning Through Deterministic Assignment of Hidden Parameters	IEEE TRANSACTIONS ON CYBERNETICS										Neural networks; Neurons; Uncertainty; Optimization; Cybernetics; Supervised learning; Robots; Bright parameters; hidden parameters; learning rate; neural networks; supervised learning	NEURAL-NETWORKS; APPROXIMATION; MACHINE; ENERGY; ENTROPY; POINTS	Supervised learning frequently boils down to determining hidden and bright parameters in a parameterized hypothesis space based on finite input-output samples. The hidden parameters determine the nonlinear mechanism of an estimator, while the bright parameters characterize the linear mechanism. In a traditional learning paradigm, hidden and bright parameters are not distinguished and trained simultaneously in one learning process. Such a one-stage learning (OSL) brings a benefit of theoretical analysis but suffers from the high computational burden. In this paper, we propose a two-stage learning scheme, learning through deterministic assignment of hidden parameters (LtDaHPs), suggesting to deterministically generate the hidden parameters by using minimal Riesz energy points on a sphere and equally spaced points in an interval. We theoretically show that with such a deterministic assignment of hidden parameters, LtDaHP with a neural network realization almost shares the same generalization performance with that of OSL. Then, LtDaHP provides an effective way to overcome the high computational burden of OSL. We present a series of simulations and application examples to support the outperformance of LtDaHP.																	2168-2267	2168-2275				MAY	2020	50	5					2321	2334		10.1109/TCYB.2018.2885029													
J								An efficient watermarking technique for tamper detection and localization of medical images	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Watermarking; Tamper detection; Tamper localization; Electronic health record	FRAGILE WATERMARKING; AUTHENTICATION SCHEME; HIGH-CAPACITY; ALGORITHM	With the exponential rise of multimedia technology and networked infrastructure, electronic healthcare is coming up a big way. One of the most important challenges in an electronic healthcare setup is the authentication of medical images, received by an expert at a far-off location from the sender. With an aim to address the critical authentication issue, this paper presents a fragile watermarking technique capable of tamper detection and localization in medical/general images. We divide the cover image into 4 x 4 non overlapping pixel blocks; with each block further sub-divided into two 4 x 2 blocks, called as Upper Half Block (UHB) and Lower Half Block (LHB). The information embedded in LHB facilitates tamper detection while as that embedded in UHB facilities tamper localization. The experimental results show that, in addition to tamper detection and localization capability, the proposed technique has lesser computational complexity when compared to other state-of-art techniques. Further, the proposed scheme results in average PSNR of 51.26 dB for a payload of one bit per pixel (1bpp) indicating that the watermarked images obtained are of high visual quality.																	1868-5137	1868-5145				MAY	2020	11	5			SI		1799	1808		10.1007/s12652-018-1158-8													
J								Video transcoding scheme of multimedia data-hiding for multiform resources based on intra-cloud	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Video transcoding; Distributed video transcoding; Cloud video transcoding; Video transcoding of multimedia data-hiding		Recently, intra-cloud research has been actively conducted to reduce the waste of idle resources in distributed desktops and to increase resource utilization. Intra-cloud integrates the idle resources of distributed desktops to provide computing and storage services to users. Existing intra-cloud have only studied storage of large files and simple computing services. Research is needed for computing services of multimedia field such as video and audio in the intra-cloud. This paper proposes a diversify scheme for multiform video resources (DSMVR), which is a video transcoding scheme of multimedia data-hiding based on the parallel computing framework and the intra-cloud environment, in order to transcode for multiform resource types within the intra-cloud, which composed to computing infrastructure using legacy desktops. Its target users are community user groups within a certain size. By using a small-scale server group, parallel processing framework and improved task assignment algorithm, high-speed video transcoding can be realized by using ffmpeg, which is a vast software suite of libraries and programs designed for handling video, audio, and other multimedia files and streams, and different-definition videos are generated step by step at high speed. By using the DSMVR scheme, the size of a task can be dynamically analyzed in order to select the number of task processing servers required, thus ensuring the high scalability of the DSMVR. Thanks to these operations, the user can smoothly play videos at resolutions that are suitable for different smart devices.																	1868-5137	1868-5145				MAY	2020	11	5			SI		1809	1819		10.1007/s12652-019-01279-1													
J								Secret image sharing scheme with encrypted shadow images using optimal homomorphic encryption technique	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Secret image sharing; Shadow; Homomorphic encryption; Discrete wavelet transform; Harmony search (OHS) algorithm; PSNR	ELLIPTIC CURVE CRYPTOGRAPHY; VISUAL CRYPTOGRAPHY; EFFICIENT; CREATION	Secret Image Sharing (SIS) scheme is to encrypt a secret image into 'n' specious shadows. It is unable to reveal any data on the secret image if at least one of the shadows is not achieved. In this paper, wavelet-based secret image sharing scheme is proposed with encrypted shadow images using optimal Homomorphic Encryption (HE) technique. Initially, Discrete Wavelet Transform (DWT) is applied on the secret image to produce sub bands. From this process, multiple shadows are created, encrypted and decrypted for each shadow. The encrypted shadow can be recovered just by choosing some subset of these 'n' shadows that makes transparent and stack over each other. To improve the shadow security, each shadow is encrypted and decrypted using HE technique. For the concern on image quality, the new Oppositional based Harmony Search (OHS) algorithm was utilized to generate the optimal key. From the analysis, it shows that the proposed scheme provide greater security compared to other existing schemes.																	1868-5137	1868-5145				MAY	2020	11	5			SI		1821	1833		10.1007/s12652-018-1161-0													
J								Hybrid domain watermarking technique for copyright protection of images using speech watermarks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Biometric; Color image; Curvelet transform; Speech signal; Security; TIMIT database; Watermarking	SINGULAR-VALUE DECOMPOSITION; ENHANCING SECURITY; DIGITAL IMAGE; ALGORITHM; FACE; FINGERPRINT; PRIVACY; SCHEME	When digital images are shared over an open access network such as the internet, facebook, WhatsApp, and other social media, then the security of these images are required. The digital watermarking is one approach for the security of images (e.g. copyright protection, ownership authentication). In most of the watermarking approaches, secret information such as owner binary logos and texts are used for protection of images. These days, biometric watermarks such as human speech signals are preferred for protection of images. In this paper, a watermarking technique based on various signal processing transforms is proposed and implemented for the security of image using human speech signal. In this technique, the first discrete cosine transform (DCT) and then singular value decomposition (SVD) are applied on the watermark speech signal to get its hybrid coefficients which are inserted into hybrid coefficients of the cover image to get a watermarked image. These hybrid coefficients of cover image are first generated using discrete wavelet transform (DWT) and then fast discrete curvelet transform (FDCuT) is applied on it. The performance of techniques is tested for standard speech database such as TIMIT in terms of imperceptibility, robustness and payload capacity. The experimental results and comparison show that the proposed watermarking technique performs better than the existing watermarking techniques available in the literature. This technique may also be used for security of speech signal against spoof attack.																	1868-5137	1868-5145				MAY	2020	11	5			SI		1835	1857		10.1007/s12652-019-01295-1													
J								A study on user recognition using 2D ECG based on ensemble of deep convolutional neural networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Biometrics; Electrocardiogram; Ensemble networks; CNNs; User recognition	CLASSIFICATION	The risk of tampering exists for conventional user recognition methods based on biometrics such as face and fingerprint. Recently, research on user recognition using biometric signals such as electrocardiogram (ECG), electroencephalogram (EEG), and electromyogram (EMG) has been actively performed to overcome this issue. We herein propose a user recognition method applying a deep learning technique based on ensemble networks after transforming ECG signals into two-dimensional (2D) images. A preprocessing process for one-dimensional ECG signals is performed to remove noise or distortion; subsequently, they are projected onto a 2D image space and transformed into image data. For the proposed algorithm, we designed deep learning-based ensemble networks to improve the degraded performance arising from overfitting in a single network. Our experimental results demonstrate that the proposed ensemble networks exhibit an accuracy that is 1.7% higher than that of the single network. In particular, the performance of the ensemble networks is up to 13% higher compared to the single network that degrades the recognition rate by displaying similar features between classes.																	1868-5137	1868-5145				MAY	2020	11	5			SI		1859	1867		10.1007/s12652-019-01195-4													
J								A robust blind watermarking framework based on Dn structure	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Watermarking; Discrete cosine transform; Random number generator; Dn-structure	IMAGE WATERMARKING; ADAPTIVE WATERMARKING	In this paper, a new robust watermarking framework is designed using the discrete cosine transform (DCT), binary decimal sequence (d-sequence) and Dn-structure. The core idea is to generate a binary d-sequence based on random number generator (RNG) and some secret keys. This binary sequence is then utilized to generate reference sets based on Dn-structure. For embedding purpose, the host image is transformed using DCT and selected coefficients are employed to construct a macro block. Each binary watermark bit is securely embedded into macro block by utilizing the reference set. The reverse process is finally formulated to extract the watermark at the receiver end. The proposed technique is experimentally analyzed using various quality metrics and attacks. The comprehensive experimental results illustrate that the proposed watermarking scheme is robust to both geometric distortions (resizing, cropping, wrapping and random row deletion) and general signal processing attacks (various noise addition, blurring, high pass filtering, image sharpening, contrast adjustment, Histogram equalization and gamma correction and JPEG compression) and outperforms state-of-the-art watermarking methods.																	1868-5137	1868-5145				MAY	2020	11	5			SI		1869	1887		10.1007/s12652-019-01296-0													
J								A robust image steganography based on the concatenated error correction encoder and discrete cosine transform coefficients	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Robust steganography; JPEG compression resistance; Detection resistance; High extraction accuracy; Concatenated error correction encoder	JPEG COMPRESSION; WATERMARKING; STEGANALYSIS; PERFORMANCE	Robust JPEG steganographic algorithms are proposed to protect the embedded message when the covert JPEG image is JPEG-compressed in some lossy channel. They usually perform much better on anti-compression ability than the traditional adaptive JPEG steganographic algorithms. However, massive recent experimental results reveal that the message extraction accuracy of the present robust JPEG steganographic methods are not high enough in some JPEG compressing channels. Thus, this paper proposes a new robust JPEG steganographic algorithm with high message extraction accuracy with equivalent detection resistance. First, the robust channel for the JPEG compression and steganographic embedding are analyzed. Second, a new error correction encoder that can protect the message in lossy channel is proposed. The structure and inside codes of the proposed encoder are different to the traditional codes used in adaptive steganography. Last, the proposed code, the relationship between coefficients and the minimal distortion model are combined to build new steganography. 10,000 images in the popular BOSSbase 1.01 image library are selected for magnanimous experiments. Compared with the adaptive JPEG steganographic algorithm and some current robust JPEG steganographic algorithms, the experimental results show that proposed method can not only resist detection efficiently, but also obtain higher extracting accuracy on resisting JPEG compression.																	1868-5137	1868-5145				MAY	2020	11	5			SI		1889	1901		10.1007/s12652-019-01345-8													
J								Effect of identity mapping, transfer learning and domain knowledge on the robustness and generalization ability of a network: a biometric based case study	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Deep learning; Transfer learning; Contact lens detection; Fingerprint sensor classification; Robustness analysis	RECOGNITION	This work evaluates two widely validated CNN models VGG and ResNet in the biometric domain based applications. These models have shown exceptional performance on the large ImageNet dataset but the predictive capability of these models for domain specific-tasks where limited data samples are present need to be checked. Here in this paper efforts have been made to analyze two interesting biometric problems (1) multi-class oculus classification (2) fingerprint sensor classification. The distinguish experimental results are evaluated on the benchmark datasets like IIIT-D, ND, IITK contact-lens datasets and FVC 2002, FVC 2004, FVC 2006, IIITD-MOLF and IITK fingerprint datasets. We have raised many interesting questions regarding the reliability and applicability of deep-learning models specifically for biometric based applications. Experimental results along with in-depth feature analysis have shown that indeed residual connections with pre-trained network provides good prior for model weights and thus helps in better generalization. [GRAPHICS] .																	1868-5137	1868-5145				MAY	2020	11	5			SI		1905	1922		10.1007/s12652-019-01297-z													
J								Personal recognition using convolutional neural network with ECG coupling image	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Biometrics; Electrocardiogram; Personal recognition; Coupling image; CNN	AUTHENTICATION	Personal identification method using the Electrocardiogram (ECG) signal is an active research area since the ECG signal cannot be forged and can be acquired without active awareness by the subject. In this paper, we propose a personal recognition system using the 2-D coupling image of the ECG signal. The proposed system uses the 2-D coupling image generated from three periods of the ECG signal as input data to the network whose design is based on a Convolutional Neural Network (CNN) that is specialized for image processing. Waveform of the 2-D coupling image which is the input data to the network cannot be visually confirmed and it has the advantage of being able to augment the QRS-complex which is a personal unique information. We confirm recognition performance of 99.2% from the experiment result for the proposed personal recognition system using MIT-BIH data.																	1868-5137	1868-5145				MAY	2020	11	5			SI		1923	1932		10.1007/s12652-019-01401-3													
J								Efficient user authentication protocol for distributed multimedia mobile cloud environment	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										AVISPA; SK; BAN logic; MBCSP; MCC; MSE	SMART-CARD; SCHEME; SECURE; BIOMETRICS; CRYPTANALYSIS; ATTACKS; DESIGN; KEYS	The rapid growth of smart-phone users, mobile services and mobile applications, poses the challenges of storage space, processing capability, and battery lifetime at the users smart phones. Mobile cloud computing helps to overcome these challenges. Presently, when a mobile user wants to subscribe to various Multimedia based cloud service providers (MBCSPs), he/she need to register separately for each of MBCSP. Although one can use single sign-on methods, they are unreliable due to the presence of any untrusted server. Hence, we propose a three-factor mobile user authentication protocol for Distributed Multimedia based cloud services. Our proposed method consists of strong authentication between the mobile user and multimedia-based cloud service providers using session key agreement, choice-based MBCSPs registration, initial mobile user identity registration checking, time of validity for secret key issued by Registration center (RC) to mobile user and time of validity for secret key issued by RC to MBCSPs respectively. We have verified our protocol with various attack scenarios using informal analysis, formal proof using BurrowsAbadiNeedham (BAN) logic and formal security analysis using Automated Validation of Internet Security Protocols and Applications tool (AVISPA) respectively. Our proposed protocol provides better performance and foolproof security.																	1868-5137	1868-5145				MAY	2020	11	5			SI		1933	1956		10.1007/s12652-019-01467-z													
J								Identifying tiny faces in thermal images using transfer learning	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Machine learning; Biometrics; Tiny faces; Thermal images	RECOGNITION	This article focuses on identifying tiny faces in thermal images using transfer learning. Although the issue of identifying faces in images is not new, the problem of tiny face identification is a recently identified research area. Indeed challenging, however, in this paper, we take the problem one step ahead and focus on recognizing tiny faces in thermal images. To do that, we use the paradigm of transfer learning. We use the famous residual network to extract the features in the target domain. Subsequently, with this model as a reference point, we then retrain it in the target domain of thermal images. Through testing performed in Terravic datasets, we have found that the method outperforms existing methods in literature to identify tiny faces in thermal images.																	1868-5137	1868-5145				MAY	2020	11	5			SI		1957	1966		10.1007/s12652-019-01470-4													
J								Profiling and improving the duty-cycling performance of Linux-based IoT devices	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Energy efficiency; Boot up; Shutdown; Edge and fog computing; Userspace optimization; Machine learning	ENERGY-CONSUMPTION; POWER	Minimizing the energy consumption of Linux-based devices is an essential step towards their wide deployment in various IoT scenarios. Energy saving methods such as duty-cycling aim to address this constraint by limiting the amount of time the device is powered on. In this work we study and improve the amount of time a Linux-based IoT device is powered on to accomplish its tasks. We analyze the processes of system boot up and shutdown on two platforms, the Raspberry Pi 3 and Raspberry Pi Zero Wireless, and enhance duty-cycling performance by identifying and disabling time-consuming or unnecessary units initialized in the userspace. We also study whether SD card speed and SD card capacity utilization affect boot up duration and energy consumption. In addition, we propose Pallex, a parallel execution framework built on top of the systemd init system to run a user application concurrently with userspace initialization. We validate the performance impact of Pallex when applied to various IoT application scenarios: (1) capturing an image, (2) capturing and encrypting an image, (3) capturing and classifying an image using the k-nearest neighbor algorithm, and (4) capturing images and sending them to a cloud server. Our results show that system lifetime is increased by 18.3%, 16.8%, 13.9% and 30.2%, for these application scenarios, respectively.																	1868-5137	1868-5145				MAY	2020	11	5			SI		1967	1995		10.1007/s12652-019-01197-2													
J								Visual process maps: a visualization tool for discovering habits in smart homes	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Visual process maps; Habit mining; Habit visualization	MANAGEMENT; ONTOLOGY	Models of human habits in smart spaces can be expressed by using a multitude of representations whose readability influences the possibility of being validated by human experts. The visual analysis by domain experts allows to identify stages of human habits that could be automatized or simplified by redesigning the environment. In this paper, we present a visual analysis pipeline for graphically visualizing human habits, starting from the sensor log of a smart space,. We apply techniques borrowed from the area of business process automation and mining on a version of the sensor log preprocessed in order to translate raw sensor measurements into human actions. The proposed method is employed to automatically extract models to be reused for ambient intelligence. A user evaluation demonstrates the effectiveness of the approach, and compares it with respect to a relevant state-of-the-art visual tool, namely Situvis.																	1868-5137	1868-5145				MAY	2020	11	5			SI		1997	2025		10.1007/s12652-019-01211-7													
J								An intelligent flying system for automatic detection of faults in photovoltaic plants	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING											MODULES; INSPECTION; DIAGNOSIS; TRACKING	For several years, fault diagnosis of photovoltaic (PV) plants has been manually performed by the human operator by a visual inspection or automatically, by evaluating electrical measures collected by sensors mounted on each PV module. In recent years, a notable interest of the scientific community has been devoted towards the definition of algorithms able to automatically analyse the sequence of images acquired by a thermal camera mounted on board of an unmanned aerial vehicle (UAV) for early PV anomaly detection. In this paper, we define a model-based approach for the detection of the panels, which uses the structural regularity of the PV string and a novel technique for local hot spot detection, based on the use of a fast and effective algorithm for finding local maxima in the PV panel region. Finally, we introduce the concept of global hot spot detection, namely a multi-frame recognition of PV faults which further improves the anomaly detection accuracy of the proposed method. The algorithm has been designed and optimized so as to run in real-time directly on an embedded system on board of the UAV. The accuracy of the proposed approach has been experimented on several video sequences with a standard protocol in terms of Precision, Recall and F-Score, so that our dataset and our quantitative results can be used for future comparisons and to evaluate the reliability of computer vision techniques designed for thermographic PV inspection.																	1868-5137	1868-5145				MAY	2020	11	5			SI		2027	2040		10.1007/s12652-019-01212-6													
J								Environmental assessment under uncertainty using Dempster-Shafer theory and Z-numbers	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Environmental assessment; Environmental risk; Dempster-Shafer theory; Z-number; Data fusion; Fuzzy reliability	CLOSTRIDIUM-DIFFICILE INFECTION; STOCHASTIC MODELING APPROACH; CRITERIA DECISION-ANALYSIS; SIMILARITY MEASURE; RISK-ASSESSMENT; BELIEF FUNCTION; FAILURE MODES; FUZZY; ENTROPY; WATER	Environmental assessment and decision making is complex leading to uncertainty due to multiple criteria involved with uncertain information. Uncertainty is an unavoidable and inevitable element of any environmental evaluation process. The published literatures rarely include the studies on uncertain data with variable fuzzy reliabilities. This research has proposed an environmental evaluation framework based on Dempster-Shafer theory and Z-numbers. Of which a new notion of the utility of fuzzy number is proposed to generate the basic probability assignment of Z-numbers. The framework can effectively aggregate uncertain data with different fuzzy reliabilities to obtain a comprehensive evaluation measure. The proposed model has been applied to two case studies to illustrate the proposed framework and show its effectiveness in environmental evaluations. Results show that the proposed framework can improve the previous methods with comparability considering the reliability of information using Z-numbers. The proposed method is more flexible comparing with previous work.																	1868-5137	1868-5145				MAY	2020	11	5			SI		2041	2060		10.1007/s12652-019-01228-y													
J								An improved industrial sub-pixel edge detection algorithm based on coarse and precise location	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Edge detection; Sub-pixel; Roberts operator; Zernike moment; Otsu's method		In this paper, an improved sub-pixel edge detection algorithm combining coarse and precise location is proposed. The algorithm fully considers the 8-neighborhood pixel information and keeps the Roberts operator's advantages of high location accuracy and fast speed. Meanwhile, it can effectively suppress noise and obtain better detection results. In order to solve the problem of low efficiency of the Zernike moment method in threshold selection, the Otsu's method is introduced to achieve accurate sub-pixel edge location. The experimental results show that the proposed algorithm effectively improves the detection efficiency and the detection accuracy.																	1868-5137	1868-5145				MAY	2020	11	5			SI		2061	2070		10.1007/s12652-019-01232-2													
J								Application of new multi-objective optimization algorithm for EV scheduling in smart grid through the uncertainties	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Multi-objective scheduling; EV; Renewables source; Demand response; Optimization	SUPPORT VECTOR MACHINE; ECONOMIC LOAD DISPATCH; MAXIMUM ABC INDEX; FEATURE-SELECTION; FORECAST ENGINE; POWER; MODEL; CONVERGENCE; PREDICTION; OPERATION	Ecological and economics issues are caused to give careful consideration to electric vehicles (EV) and sustainable power source assets. One of the proposed answers for increment the impact of these assets, is to utilize the electric vehicles potential. The capability of electric vehicles require planning for Smart Distribution Systems (SDS). Request reaction programs, as a suitable device to utilize endorsers' potential in ideal administration of the system, gives dynamic nearness of supporters in control framework execution change and these projects, in basic conditions, can give the request prerequisites diminishment, in a brief timeframe. In this work, attempts to presents a multi-objective scheduling of EV based on the sustainable assets in smart grid, cover uncertainty caused by inexhaustible assets and EVs, by considering of the request reaction projects and EV battery stockpiling framework, limit the working expenses and the measure of intensity framework contamination, with enhancing procedures. Improved optimization algorithm is utilized for taking care of the advancing issue. Operating costs dropped much further utilizing monetary model of the demand response and vehicle charge/discharge and smart program in the hours when the load is lower. Effectiveness of proposed method is applied on 33 bus standard power system.																	1868-5137	1868-5145				MAY	2020	11	5			SI		2071	2103		10.1007/s12652-019-01233-1													
J								Key feature identification for recognition of activities performed by a smart-home resident	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Activity recognition; Smart homes; Health care; Remote monitoring; Elderly care; Ambient assisted living; Key feature selection; Binary classification	FEATURE-SELECTION; INFORMATION	Activity recognition is beneficial for continuous health monitoring of smart-home residents, such as patients and elderly people, living in the privacy of their home. We propose an activity recognition approach apposite for a smart home environment. The observations are obtained through multiple sensors deployed at different locations within a smart home. The activities are represented by the features selected from the received observations. The inconsistent order of performing the activities, infrequent occurrences and the presence of overlapping activities make it challenging to select the features with high class representative ability and inter-class discriminative qualities. We select the key features locally within each activity class, which is least affected by the order of performance and the occurrence of other activities. Next, for association of activities, we solve the existing multi-class problem through a specifically designed binary classification with ranking solution, which learns on the correct and incorrect assignments of activities. A comparison of proposed approach with existing methods in terms of recognition accuracy is presented on publicly available 'Kasteren' and 'CASAS' datasets, representing a range of overlapping and well separated activities of daily life. Our approach tailored towards a smart home environment demonstrates a better accuracy than existing methods.																	1868-5137	1868-5145				MAY	2020	11	5			SI		2105	2115		10.1007/s12652-019-01236-y													
J								An adaptive mutual trust based access control model for electronic healthcare system	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Access control; Electronic healthcare system (EHS); Mutual trust; Beta distribution (BD) technique; Access control rules	REPUTATION MANAGEMENT; SELECTION; PRIVACY; ERROR	With the increasing growth of the Electronic healthcare system (EHS), the security of the EHS is an essential requirement because different types of users (patient, doctor, nurse, etc.) are accessing these systems for various purposes like treatment, research, drug analysis, etc. In the EHS, two major security challenges arise. First one is the selection of an access control mechanism without any prior information about the healthcare users. The second one is how much amount of data will be shared by the healthcare services and practitioner. Hence, a suitable access control technique is essential which not only provides the static access but also dynamically control the views of the requested data, so that the information will be shared in a controlled manner. In the healthcare system, trust can be viewed as an important judgment parameter for controlling the access of different stakeholders as it is an open system with different types of users. The main aim of the work is to control the access view so that only authorized user can access the information in a controlled manner. It also improves adaptivity of the access control model by integration of dynamic trust degree of communicating parties. To fulfill the above-discussed security requirements, in this paper, we have proposed an access control model, which is based on the trust degree of the healthcare user and service, named as mutual trust. The assessment of user and service trust degree is based on the beta distribution technique. A rule set has been developed based on this mutual trust degree to control the data access view, which is dynamically changed with the communicating parties trust level. The detail implementation of the proposed model shows that the accuracy and efficiency of the model are better as compared to other models.																	1868-5137	1868-5145				MAY	2020	11	5			SI		2117	2136		10.1007/s12652-019-01240-2													
J								Design and security analysis of two robust keyed hash functions based on chaotic neural networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Keyed hash functions; Chaotic neural networks; Chaotic activation function; Merkle-Damgard; Statistical tests; Brute force attacks; Cryptanalytical attacks; Speed analysis	SCHEME; PREIMAGES; MAP	In this paper, we designed, implemented, and analyzed the performance, in terms of security and speed, of two proposed keyed Chaotic Neural Network (CNN) hash functions based on Merkle-Damgard (MD) construction with three output schemes: CNN-Matyas-Meyer-Oseas, Modified CNN-Matyas-Meyer-Oseas, and CNN-Miyaguchi-Preneel. The first hash function's structure is composed of two-layer chaotic neural network while the structure of the second hash function is formed of one-layer chaotic neural network followed by non-linear layer functions. The obtained results of several statistical tests and cryptanalytic analysis highlight the robustness of the proposed keyed CNN hash functions, which is fundamentally due to the strong non-linearity of both the chaotic systems and the neural networks. The comparison of the performance analysis with some chaos-based hash functions of the literature and with standard hash functions make the proposed hash functions suitable for data integrity, message authentication, and digital signature applications.																	1868-5137	1868-5145				MAY	2020	11	5			SI		2137	2161		10.1007/s12652-019-01244-y													
J								An improved authentication and security scheme for LTE/LTE-A networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Authentication; Elliptic curve cryptography; LTE-A; Salsa20 stream cipher; Security; Shared key	KEY AGREEMENT PROTOCOL; PASSWORD AUTHENTICATION; SIGNATURE SCHEME; EFFICIENT; LTE; AKA; RSA; 4G	Long term evolution (LTE) and LTE-Advanced networks support highly developed authentication and encryption mechanisms. However, these systems still suffer from various security problems such as replay attack, impersonation attack, known key attack, eavesdropping attack and so on. To mitigate these security weaknesses, an improved authentication and security scheme has been proposed for LTE/LTE-A networks. The proposed scheme employs Elliptic Curve Cryptography (ECC), Elliptic Curve Diffie-Hellman (ECDH) and Salsa20 algorithm to improve end to end security and provide faster data transmission for 4G environment. The proposed scheme uses several powerful encryption techniques and also provides proper mutual authentication between User Equipment (UE) and Message Management Entity (MME). The performance of the proposed system has been compared with LTE-A and existing systems in terms of several security attributes and performance parameters. The comparative results show that the proposed scheme outperforms LTE-A as well as other existing schemes.																	1868-5137	1868-5145				MAY	2020	11	5			SI		2163	2185		10.1007/s12652-019-01248-8													
J								Multi-criteria PROMETHEE method based on possibility degree with Z-numbers under uncertain linguistic environment	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Z-numbers; Multi-criteria decision-making; PROMETHEE II; Possibility degree	FUZZY; MODEL; STRATEGIES; BACKORDER; INVENTORY; DISTANCE; RANKING; SETS	People make decisions based on their cognitive information about the objective world. Zadeh's Z-number allows people to better express their cognition of the real world by considering the fuzzy restriction and reliability restriction of information. However, the Z-number is a complex construct, and some important issues must be discussed in its study. Here, a computationally simple method of ranking Z-numbers for multi-criteria decision-making (MCDM) problems is proposed, and a comprehensive possibility degree of Z-numbers is defined, as inspired by the possibility degree concept of interval numbers. The outranking relations of Z-numbers are also discussed on the basis of the proposed method. Then, a weight acquisition algorithm relative to the possibility degree of Z-numbers is presented. Finally, an extended Preference Ranking Organization Method for Enrichment Evaluation II (PROMETHEE II) based on the possibility degree of Z-numbers is developed for the MCDM problem under Z-evaluation, and a numerical example about the selection of travel plans is used to illustrate the validity of the proposed method. The applicability and superiority of the proposed method is demonstrated through sensitivity and comparative analyses along with other existing methods.																	1868-5137	1868-5145				MAY	2020	11	5			SI		2187	2201		10.1007/s12652-019-01251-z													
J								Performance Limitations in Sensorimotor Control: Trade-Offs Between Neural Computation and Accuracy in Tracking Fast Movements	NEURAL COMPUTATION											OPTIMAL FEEDBACK-CONTROL; EYE TRACKING; MOTOR UNITS; MODEL; MOTONEURON; INTERMITTENCY; SUBMOVEMENTS; RECRUITMENT	The ability to move fast and accurately track moving objects is fundamentally constrained by the biophysics of neurons and dynamics of the muscles involved. Yet the corresponding trade-offs between these factors and tracking motor commands have not been rigorously quantified. We use feedback control principles to quantify performance limitations of the sensorimotor control system (SCS) to track fast periodic movements. We show that (1) linear models of the SCS fail to predict known undesirable phenomena, including skipped cycles, overshoot and undershoot, produced when tracking signals in the "fast regime," while nonlinear pulsatile control models can predict such undesirable phenomena, and (2) tools from nonlinear control theory allow us to characterize fundamental limitations in this fast regime. Using a validated and tractable nonlinear model of the SCS, we derive an analytical upper bound on frequencies that the SCS model can reliably track before producing such undesirable phenomena as a function of the neurons' biophysical constraints and muscle dynamics. The performance limitations derived here have important implications in sensorimotor control. For example, if the primary motor cortex is compromised due to disease or damage, the theory suggests ways to manipulate muscle dynamics by adding the necessary compensatory forces using an assistive neuroprosthetic device to restore motor performance and, more important, fast and agile movements. Just how one should compensate can be informed by our SCS model and the theory developed here.																	0899-7667	1530-888X				MAY	2020	32	5					865	886		10.1162/neco_a_01272													
J								Comparison of Different Spike Train Synchrony Measures Regarding Their Robustness to Erroneous Data From Bicuculline-Induced Epileptiform Activity	NEURAL COMPUTATION											PHASE SYNCHRONIZATION; NEURONAL NETWORKS; OSCILLATIONS	As synchronized activity is associated with basic brain functions and pathological states, spike train synchrony has become an important measure to analyze experimental neuronal data. Many measures of spike train synchrony have been proposed, but there is no gold standard allowing for comparison of results from different experiments. This work aims to provide guidance on which synchrony measure is best suited to quantify the effect of epileptiform-inducing substances (e.g., bicuculline, BIC) in in vitro neuronal spike train data. Spike train data from recordings are likely to suffer from erroneous spike detection, such as missed spikes (false negative) or noise (false positive). Therefore, different timescale-dependent (cross-correlation, mutual information, spike time tiling coefficient) and timescale-independent (Spike-contrast, phase synchronization (PS), A-SPIKE-synchronization, A-ISI-distance, ARI-SPIKE-distance) synchrony measures were compared in terms of their robustness to erroneous spike trains. For this purpose, erroneous spike trains were generated by randomly adding (false positive) or deleting (false negative) spikes (in silico manipulated data) from experimental data. In addition, experimental data were analyzed using different spike detection threshold factors in order to confirm the robustness of the synchrony measures. All experimental data were recorded from cortical neuronal networks on microelectrode array chips, which show epileptiform activity induced by the substance BIC. As a result of the in silico manipulated data, Spike-contrast was the only measure that was robust to false-negative as well as false-positive spikes. Analyzing the experimental data set revealed that all measures were able to capture the effect of BIC in a statistically significant way, with Spike-contrast showing the highest statistical significance even at low spike detection thresholds. In summary, we suggest using Spike-contrast to complement established synchrony measures because it is timescale independent and robust to erroneous spike trains.																	0899-7667	1530-888X				MAY	2020	32	5					887	911		10.1162/neco_a_01277													
J								Equivalence Projective Simulation as a Framework for Modeling Formation of Stimulus Equivalence Classes	NEURAL COMPUTATION											MATCHING-TO-SAMPLE; GO/NO-GO PROCEDURE; CONDITIONAL DISCRIMINATIONS; COMPOUND STIMULI; BEHAVIOR; MEMORY; CONNECTIONISM; ESTABLISHMENT; PROBABILITY; INSTRUCTION	Stimulus equivalence (SE) and projective simulation (PS) study complex behavior, the former in human subjects and the latter in artificial agents. We apply the PS learning framework for modeling the formation of equivalence classes. For this purpose, we first modify the PS model to accommodate imitating the emergence of equivalence relations. Later, we formulate the SE formation through the matching-to-sample (MTS) procedure. The proposed version of PS model, called the equivalence projective simulation (EPS) model, is able to act within a varying action set and derive new relations without receiving feedback from the environment. To the best of our knowledge, it is the first time that the field of equivalence theory in behavior analysis has been linked to an artificial agent in a machine learning context. This model has many advantages over existing neural network models. Briefly, our EPS model is not a black box model, but rather a model with the capability of easy interpretation and flexibility for further modifications. To validate the model, some experimental results performed by prominent behavior analysts are simulated. The results confirm that the EPS model is able to reliably simulate and replicate the same behavior as real experiments in various settings, including formation of equivalence relations in typical participants, nonformation of equivalence relations in language-disabled children, and nodal effect in a linear series with nodal distance five. Moreover, through a hypothetical experiment, we discuss the possibility of applying EPS in further equivalence theory research.																	0899-7667	1530-888X				MAY	2020	32	5					912	968		10.1162/neco_a_01274													
J								The Discriminative Kalman Filter for Bayesian Filtering with Nonlinear and Nongaussian Observation Models	NEURAL COMPUTATION											BRAIN-COMPUTER INTERFACES; DATA ASSIMILATION; MOTOR CORTEX; PARTICLE FILTERS; CORTICAL CONTROL; NEURAL-NETWORKS; SPACE; PREDICTION; MOVEMENT; TETRAPLEGIA	The Kalman filter provides a simple and efficient algorithm to compute the posterior distribution for state-space models where both the latent state and measurement models are linear and gaussian. Extensions to the Kalman filter, including the extended and unscented Kalman filters, incorporate linearizations for models where the observation model p(observation|state) is nonlinear. We argue that in many cases, a model for p(state|observation) proves both easier to learn and more accurate for latent state estimation. Approximating p(state|observation) as gaussian leads to a new filtering algorithm, the discriminative Kalman filter (DKF), which can perform well even when p(observation|state) is highly nonlinear and/or nongaussian. The approximation, motivated by the Bernstein-von Mises theorem, improves as the dimensionality of the observations increases. The DKF has computational complexity similar to the Kalman filter, allowing it in some cases to perform much faster than particle filters with similar precision, while better accounting for nonlinear and nongaussian observation models than Kalman-based extensions. When the observation model must be learned from training data prior to filtering, off-the-shelf nonlinear and nonparametric regression techniques can provide a gaussian model for p(observation|state) that cleanly integrates with the DKF. As part of the BrainGate2 clinical trial, we successfully implemented gaussian process regression with the DKF framework in a brain-computer interface to provide real-time, closed-loop cursor control to a person with a complete spinal cord injury. In this letter, we explore the theory underlying the DKF, exhibit some illustrative examples, and outline potential extensions.																	0899-7667	1530-888X				MAY	2020	32	5					969	1017		10.1162/neco_a_01275													
J								The Stochastic Delta Rule: Faster and More Accurate Deep Learning Through Adaptive Weight Noise	NEURAL COMPUTATION												Multilayer neural networks have led to remarkable performance on many kinds of benchmark tasks in text, speech, and image processing. Nonlinear parameter estimation in hierarchical models is known to be subject to overfitting and misspecification. One approach to these estimation and related problems (e.g., saddle points, colinearity, feature discovery) is called Dropout. The Dropout algorithm removes hidden units according to a binomial random variable with probability p prior to each update, creating random "shocks" to the network that are averaged over updates (thus creating weight sharing). In this letter, we reestablish an older parameter search method and show that Dropout is a special case of this more general model, stochastic delta rule (SDR), published originally in 1990. Unlike Dropout, SDR redefines each weight in the network as a random variable with mean mu wij and standard deviation sigma wij. Each weight random variable is sampled on each forward activation, consequently creating an exponential number of potential networks with shared weights (accumulated in the mean values). Both parameters are updated according to prediction error, thus resulting in weight noise injections that reflect a local history of prediction error and local model averaging. SDR therefore implements a more sensitive local gradient-dependent simulated annealing per weight converging in the limit to a Bayes optimal network. We run tests on standard benchmarks (CIFAR and ImageNet) using a modified version of DenseNet and show that SDR outperforms standard Dropout in top-5 validation error by approximately 13% with DenseNet-BC 121 on ImageNet and find various validation error improvements in smaller networks. We also show that SDR reaches the same accuracy that Dropout attains in 100 epochs in as few as 40 epochs, as well as improvements in training error by as much as 80%.																	0899-7667	1530-888X				MAY	2020	32	5					1018	1032		10.1162/neco_a_01276													
J								A survey on swarm intelligence approaches to feature selection in data mining	SWARM AND EVOLUTIONARY COMPUTATION										Feature selection; Swarm intelligence; Particle swarm optimization; Ant colony optimization; Classification	ANT COLONY OPTIMIZATION; ARTIFICIAL BEE COLONY; FEATURE SUBSET-SELECTION; SUPPORT VECTOR MACHINES; MUTUAL INFORMATION; GENETIC ALGORITHM; HYBRID APPROACH; LOCAL SEARCH; BINARY PSO; CLASSIFICATION	One of the major problems in Big Data is a large number of features or dimensions, which causes the issue of "the curse of dimensionality" when applying machine learning, especially classification algorithms. Feature selection is an important technique which selects small and informative feature subsets to improve the learning performance. Feature selection is not an easy task due to its large and complex search space. Recently, swarm intelligence techniques have gained much attention from the feature selection community because of their simplicity and potential global search ability. However, there has been no comprehensive surveys on swarm intelligence for feature selection in classification which is the most widely investigated area in feature selection. Only a few short surveys is this area are still lack of in-depth discussions on the state-of-the-art methods, and the strengths and limitations of existing methods, particularly in terms of the representation and search mechanisms, which are two key components in adapting swarm intelligence to address feature selection problems. This paper presents a comprehensive survey on the state-of-the-art works applying swarm intelligence to achieve feature selection in classification, with a focus on the representation and search mechanisms. The expectation is to present an overview of different kinds of state-of-the-art approaches together with their advantages and disadvantages, encourage researchers to investigate more advanced methods, provide practitioners guidances for choosing the appropriate methods to be used in real-world scenarios, and discuss potential limitations and issues for future research.																	2210-6502	2210-6510				MAY	2020	54								100663	10.1016/j.swevo.2020.100663													
J								Hybrid evolutionary multi-objective optimisation using outranking-based ordinal classification methods	SWARM AND EVOLUTIONARY COMPUTATION										Evolutionary multi-objective optimisation; Multi-criteria ordinal classification; Preference incorporation	MANY-OBJECTIVE OPTIMIZATION; MULTICRITERIA CLASSIFICATION; NSGA-II; PREFERENCES; ALGORITHMS; DECISION; CONSTRUCTION; DOMINANCE	A large number of real-world problems require optimising several objective functions at the same time, which are generally in conflict. Many of these problems have been addressed through multi-objective evolutionary algorithms. In this paper, we propose a new hybrid evolutionary algorithm whose main feature is the incorporation of the Decision Maker's (DM's) preferences through multi-criteria ordinal classification methods in early stages of the optimisation process, being progressively updated. This increases the selective pressure towards the privileged zone of the Pareto front more in agreement with the DM's preferences. An extensive experimental research was conducted to answer three main questions: i) to what extent the proposal improves the convergence towards the region of interest for the DM; ii) to what extent the proposal becomes more relevant as the number of objectives increases, and iii) to what extent the effectiveness of the hybrid algorithm depends on the particular multi-criteria method used to assign solutions to ordered classes. The issues used to evaluate our proposal and answer the questions were seven scalable test problems from the DTLZ test suite and some instances of project portfolio optimisation problems, with three and eight objectives. Compared to MOEA/D and MOEA/D-DE, the results showed that the proposed strategy obtains a better convergence towards the region of interest for the DM and also performs better characterisation of that zone on a wide range of objective functions.																	2210-6502	2210-6510				MAY	2020	54								100652	10.1016/j.swevo.2020.100652													
J								Push and pull search embedded in an M2M framework for solving constrained multi-objective optimization problems	SWARM AND EVOLUTIONARY COMPUTATION										Push and pull search; Constraint-handling mechanisms; Constrained multi-objective evolutionary algorithms; NSGA-II; Multi-objective to multi-objective (M2M) decomposition	REJECTIVE MULTIPLE TEST; EVOLUTIONARY ALGORITHM; DIFFERENTIAL EVOLUTION; HANDLING METHOD; DECOMPOSITION; MOEA/D; TESTS	In dealing with constrained multi-objective optimization problems (CMOPs), a key issue of multi-objective evolutionary algorithms (MOEAs) is to balance the convergence and diversity of working populations. However, most state-of-the-art MOEAs show poor performance in balancing them, and can cause the working populations to concentrate on part of the Pareto fronts, leading to serious imbalanced searching between preserving diversity and achieving convergence. This paper proposes a method which combines a multi-objective to multi-objective (M2M) decomposition approach with the push and pull search (PPS) framework, namely PPS-M2M. To be more specific, the proposed algorithm decomposes a CMOP into a set of simple CMOPs. Each simple CMOP corresponds to a sub-population and is solved in a collaborative manner. When dealing with constraints, each sub-population follows a procedure of "ignore the constraints in the push stage and consider the constraints in the pull stage", which helps each working sub-population get across infeasible regions. In order to evaluate the performance of the proposed PPS-M2M, it is compared with the other nine algorithms, including CM2M, MOEA/D-Epsilon, MOEA/D-SR, MOEA/D-CDP, C-MOEA/D, NSGA-II-CDP, MODE-ECHM, CM2M2 and MODE-SaE on a set of benchmark CMOPs. The experimental results show that the proposed PPS-M2M is significantly better than the other nine algorithms. In addition, a set of constrained and imbalanced multi-objective optimization problems (CIMOPs) are suggested to compare PPS-M2M and PPS-MOEA/D. The experimental results show that the proposed PPS-M2M outperforms PPS-MOEA/D on CIMOPs.																	2210-6502	2210-6510				MAY	2020	54								100651	10.1016/j.swevo.2020.100651													
J								Using evolutionary computation to infer the decision maker's preference model in presence of imperfect knowledge: A case study in portfolio optimization	SWARM AND EVOLUTIONARY COMPUTATION										Evolutionary algorithms; Multi-criteria decision aiding; Outranking approach; Preferences elicitation; Portfolio optimization	INDIRECT ELICITATION; CRITERIA WEIGHTS; PARAMETERS; SELECTION; CLASSIFICATION; ALGORITHM	It is usually very difficult to elicit the parameter values of models representing decision makers' preferences. Consequently, some imprecision, ill-determination and arbitrariness are unavoidable. Moreover, such elicitation cannot be performed by traditional optimization techniques in a reasonable time. Therefore, we present here a novel elicitation method guided by a genetic algorithm whose main contribution is coping with imperfect knowledge. The latter is done by using interval numbers representing all the possible values that the parameters can attain. The assessment of the method showed its high ability to reproduce the decision maker's preferences. Finally, as the method proposed in this paper is the complement of the authors' previous work regarding the optimization of stock portfolios, we provide a case study in such a field. We use differential evolution to obtain the most satisfactory portfolio. The results reported here show that the best portfolio returns are obtained when the elicitation method is exploited, and we conclude that the new overall approach might be an interesting alternative to the already-existing methods.																	2210-6502	2210-6510				MAY	2020	54								100648	10.1016/j.swevo.2020.100648													
J								Multi-objective differential evolution algorithm with fuzzy inference-based adaptive mutation factor for Pareto optimum design of suspension system	SWARM AND EVOLUTIONARY COMPUTATION										Multi-objective optimization; Differential evolution; Fuzzy logic; Mutation factor; Population diversity; Vehicle vibration model	PERFORMANCE ASSESSMENT; GENETIC ALGORITHM; OPTIMIZATION ALGORITHM; LOCAL SEARCH; ADAPTATION; PARAMETERS	In this paper, a multi-objective differential evolution with fuzzy inference-based dynamic adaptive mutation factor (MODE-FM) is proposed for Pareto optimization of problems using a combination of non-dominated sorting and crowding distance. In the proposed algorithm, fuzzy inference is employed to dynamically tune the mutation factor for a better exploration and exploitation ability. In the proposed work, to adapt the mutation factor, the generation count and population diversity in each generation are provided as inputs to fuzzy inference system and the mutation factor is obtained as an output. Performance of the suggested approach is first tested on popular benchmark functions adopted from IEEE CEC 2009. Secondly, vehicle vibration model with five degrees of freedom is selected to be optimally designed by the aforesaid proposed approach. Comparison of the obtained results of this work with those in the literature has confirmed the superiority of the proposed method.																	2210-6502	2210-6510				MAY	2020	54								100666	10.1016/j.swevo.2020.100666													
J								An efficient hybrid metaheuristic algorithm for cardinality constrained portfolio optimization	SWARM AND EVOLUTIONARY COMPUTATION										Portfolio optimization; Cardinality constraints; Metaheuristics; Continuous ant colony optimization; Artificial bee colony; Genetic algorithms		Portfolio optimization with cardinality constraints turns out to be a mixed-integer quadratic programming problem which is proven to be NP-Complete that limits the efficiency of exact solution approaches, often because of the long-running times. Therefore, particular attention has been given to approximate approaches such as metaheuristics which do not guarantee optimality, yet may expeditiously provide near-optimal solutions. The purpose of this study is to present an efficient hybrid metaheuristic algorithm that combines critical components from continuous ant colony optimization, artificial bee colony optimization and genetic algorithms for solving cardinality constrained portfolio optimization problem. Computational results on seven publicly available benchmark problems confirm the effectiveness of the hybrid integration mechanism. Moreover, comparisons against other methods' results in the literature reveal that the proposed solution approach is competitive with state-of-the-art algorithms.																	2210-6502	2210-6510				MAY	2020	54								100662	10.1016/j.swevo.2020.100662													
J								Differential evolution for the optimization of low-discrepancy generalized Halton sequences	SWARM AND EVOLUTIONARY COMPUTATION										Differential evolution; Combinatorial optimization; Quasirandom sequences; Discrepancy	PARTICLE SWARM OPTIMIZATION; ALGORITHM; POPULATION	Halton sequences are d-dimensional quasirandom sequences that fill the d-dimensional hyperspace in a uniform way. They can be used in a variety of applications such as multidimensional integration, uniform sampling, and, e.g., quasi-Monte Carlo simulations. Generalized Halton sequences improve the space-filling properties of original Halton sequences in higher dimensions by digit scrambling. In this work, an evolutionary optimization algorithm, the differential evolution, is used to optimize scrambling permutations of a cl-dimensional generalized Halton sequence so that the discrepancy of the generated point set is minimized.																	2210-6502	2210-6510				MAY	2020	54								100649	10.1016/j.swevo.2020.100649													
J								Adaptive Global WASF-GA to handle many-objective optimization problems	SWARM AND EVOLUTIONARY COMPUTATION										Many-objective optimization; Pareto optimal solutions; Achievement scalarizing function; Evolutionary algorithm; Weight vectors	MULTIOBJECTIVE EVOLUTIONARY ALGORITHMS; NONDOMINATED SORTING APPROACH; MOEA/D; PERFORMANCE	In this paper, a new version of the aggregation-based evolutionary algorithm Global WASF-GA (GWASF-GA) for many-objective optimization is proposed, called Adaptive Global WASF-GA (A-GWASF-GA). The fitness function of GWASF-GA is defined by an achievement scalarizing function (ASF) based on the Tchebychev distance, which considers two reference points (the nadir and utopian points) and a set of weight vectors. Despite of the benefits of using these two reference points simultaneously and a well-distributed set of weight vectors, it is necessary to go a step further to get better approximations in problems with complicated Pareto optimal fronts. For this, in A-GWASF-GA, some of the weight vectors are re-calculated during the optimization process based on the sparsity of the solutions found so far, and taking into account some theoretical results demonstrated in this paper regarding the ASF considered. Different strategies are carried out to accelerate the convergence and to maintain the diversity. The computational results, carried out in comparison with RVEA, NSGA-III, and different versions of MOEA/D, show the potential of A-GWASF-GA in well-known but also in novel many-objective optimization benchmark problems.																	2210-6502	2210-6510				MAY	2020	54								100644	10.1016/j.swevo.2020.100644													
J								Balanced crossover operators in Genetic Algorithms	SWARM AND EVOLUTIONARY COMPUTATION										Genetic algorithms; Crossover operators; Balanced bitstrings; Boolean functions; Orthogonal arrays; Bent functions		In several combinatorial optimization problems arising in cryptography and design theory, the admissible solutions must often satisfy a balancedness constraint, such as being represented by bitstrings with a fixed number of ones. For this reason, several works in the literature tackling these optimization problems with Genetic Algorithms (GA) introduced new balanced crossover operators which ensure that the offspring has the same balancedness characteristics of the parents. However, the use of such operators has never been thoroughly motivated, except for some generic considerations about search space reduction. In this paper, we undertake a rigorous statistical investigation on the effect of balanced and unbalanced crossover operators against three optimization problems from the area of cryptography and coding theory: nonlinear balanced Boolean functions, binary Orthogonal Arrays (OA) and bent functions. In particular, we consider three different balanced crossover operators (each with two variants: "left-to-right" and "shuffled"), two of which have never been published before, and compare their performances with classic one-point crossover. We are able to confirm that the balanced crossover operators perform better than one-point crossover. Furthermore, in two out of three crossovers, the "left-to-right" version performs better than the "shuffled" version.																	2210-6502	2210-6510				MAY	2020	54								100646	10.1016/j.swevo.2020.100646													
J								A better balance in metaheuristic algorithms: Does it exist?	SWARM AND EVOLUTIONARY COMPUTATION										Metaheuristic optimization; Balance; Exploration-exploitation; Population diversity	DIFFERENTIAL EVOLUTION; OPTIMIZATION ALGORITHM; SWARM; EXPLORATION	The constant development of new metaheuristic algorithms has led to a saturation in the field of stochastic search. There are now hundreds of different algorithms that can be used to solve any problem. To produce a good performance, every metaheuristic method needs to address a satisfactory equilibrium between exploration and exploitation of the search space. Although exploration and exploitation represent two fundamental concepts in metaheuristics, the main questions about their combination and balance have not been yet completely understood. Most of the existent analyzes conducted on metaheuristic techniques consider only the comparison of their final results which cannot evaluate the nature of a good or bad balance. This paper presents an experimental analysis that quantitatively evaluates the balance between exploration and exploitation of several of the most important and better-known metaheuristic algorithms. In the study, a dimension-wise diversity measurement is used to assess the balance of each scheme considering a representative set of 42 benchmark problems that involve multimodal, unimodal, composite and shifted functions. As a result, the analysis provides several observations that allow understanding how this balance affects the results in each type of functions, and which balance is producing better solutions.																	2210-6502	2210-6510				MAY	2020	54								100671	10.1016/j.swevo.2020.100671													
J								Evolutionary LSTM-FCN networks for pattern classification in industrial processes	SWARM AND EVOLUTIONARY COMPUTATION										Fully convolutional neural network; Long short term memory recurrent neural network; Evolutionary computation; Pattern classification; Industry 4.0	PARTICLE SWARM OPTIMIZATION; SUPPORT VECTOR MACHINES; NEURAL-NETWORKS	The Industry 4.0 revolution allows gathering big amounts of data that are used to train and deploy Artificial Intelligence algorithms to solve complex industrial problems, optimally and automatically. From those, Long-Short Term Memory Fully Convolutional Network (LSTM-FCN) networks are gaining a lot of attention over the last decade due to their capability of successfully modeling nonlinear feature interactions. However, they have not been yet fully applied for pattern classification tasks in time series data within the digital industry. In this paper, a novel approach based on an evolutionary algorithm for optimizing the networks hyperparameters and on the resulting deep learning model for pattern classification is proposed. In order to demonstrate the applicability of this method, a test scenario that involves a process related to blind fastener installation in the aeronautical industry is provided. The results achieved with the proposed approach are compared with shallow models and it is demonstrated that the proposed method obtains better results with an accuracy value of 95%.																	2210-6502	2210-6510				MAY	2020	54								100650	10.1016/j.swevo.2020.100650													
J								Ensemble of metaheuristics for energy-efficient hybrid flowshops: Makespan versus total energy consumption	SWARM AND EVOLUTIONARY COMPUTATION										Hybrid flowshop scheduling; Energy-efficient scheduling; Multi-objective optimization; Metaheuristics	SHOP SCHEDULING PROBLEM; MULTIOBJECTIVE GENETIC ALGORITHM; DIFFERENTIAL EVOLUTION ALGORITHM; ITERATED GREEDY ALGORITHM; TOTAL WEIGHTED TARDINESS; POWER-CONSUMPTION; OPTIMIZATION ALGORITHM; LOCAL SEARCH; FLOW SHOPS; MACHINE	Due to its practical relevance, the hybrid flowshop scheduling problem (HFSP) has been widely studied in the literature with the objectives related to production efficiency. However, studies regarding energy consumption and environmental effects have rather been limited. This paper addresses the trade-off between makespan and total energy consumption in hybrid flowshops, where machines can operate a varying speed levels. A bi-objective mixed-integer linear programming (MILP) model and a bi-objective constraint programming (CP) model are proposed for the problem employing speed scaling. Since the objectives of minimizing makespan and total energy consumption are conflicting with each other, the augmented epsilon (epsilon)-constraint approach is used for obtaining the Pareto-optimal solutions. While close approximations for the Pareto-optimal frontier are obtained for small-sized instances, sets of non-dominated solutions are obtained for large instances by solving the MILP and CP models under a time limit. As the problem is NP-hard, two variants of the iterated greedy algorithm, a variable block insertion heuristic and four variants of ensemble of metaheuristic algorithms are also proposed, as well as a novel constructive heuristic. The performances of the proposed seven bi-objective metaheuristics are compared with each other as well as the MILP and CP solutions on a set of well-known HFSP benchmarks in terms of cardinality, closeness, and diversity of the solutions. Initially, the performances of the algorithms are tested on small-sized instances with respect to the Pareto-optimal solutions. Then, it is shown that the proposed algorithms are very effective for solving large instances in terms of both solution quality and CPU time.																	2210-6502	2210-6510				MAY	2020	54								100660	10.1016/j.swevo.2020.100660													
J								Gene selection for cancer types classification using novel hybrid metaheuristics approach	SWARM AND EVOLUTIONARY COMPUTATION										Accuracy; Classification; Gene selection; Naive bayes; Teaching learning-based optimization	LEARNING-BASED OPTIMIZATION; PARTICLE SWARM OPTIMIZATION; DIFFERENTIAL EVOLUTION; GLOBAL OPTIMIZATION; SEARCH ALGORITHM; INFORMATION; FRAMEWORK; COLONY	With the advancement of microarray technology, gene expression profiling has shown remarkable effort to predict the different types of malignancy and their subtypes. In microarrays, predicting highly discriminative genes is a challenging task and existing hybrid methods fail to deal with efficiently. To mitigate the curse of dimensionality problem and to improve the interpretability of discriminative genes, in this study, we developed a new hybrid wrapper approach which integrates the characteristics of teaching learning-based algorithm (TLBO) and gravitational search algorithm (GSA), called TLBOGSA. A new encoding strategy is also integrated into TLBOGSA to transmute the continuous search space to binary search space and form binary TLBOGSA. In the proposed method, firstly, minimum redundancy maximum relevance (mRMR) feature selection is employed to select relevant genes from the gene expression datasets. Then, wrapper method is applied to select the informative genes from the reduced data produced by mRMR. To improve the search capability during the evolution process, we have incorporated the gravitational search mechanism in the teaching phase. The proposed method uses naive bayes classifier as a fitness function to select the extremely judicious genes which can help to classify cancer accurately. The efficiency of proposed method is tested on ten biological datasets and compared with state-of-art computational intelligence approaches for tumor prediction. Experimental results and statistical analysis demonstrate that proposed method is significantly outperforms existing metaheuristic approaches regarding convergence rate, classification accuracy and optimal number of feature sets. The proposed method reaches above 98% classification accuracy in six datasets and maximum accuracy is achieved as 99.62% in DLBCL dataset.																	2210-6502	2210-6510				MAY	2020	54								100661	10.1016/j.swevo.2020.100661													
J								Matrix adaptation evolution strategies for optimization under nonlinear equality constraints	SWARM AND EVOLUTIONARY COMPUTATION										Matrix adaptation evolution strategies; Nonlinear constraints; Nonlinear manifold; Experimental evaluation	DIFFERENTIAL EVOLUTION	This work concerns the design of matrix adaptation evolution strategies for black-box optimization under nonlinear equality constraints. First, constraints in form of elliptical manifolds are considered. For those constraints, an algorithm is proposed that evolves itself on that manifold while optimizing the objective function. The specialty about the approach is that it is possible to ensure that the population evolves on the manifold with closed-form expressions. Second, an algorithm design for general nonlinear equality constraints is presented. For those constraints considered, an iterative repair approach is presented. This allows the evolution to happen on the nonlinear manifold defined by the equality constraints for this more general case as well. For both cases, the algorithms are interior point methods, i.e., the objective function is only evaluated at feasible points in the parameter space, which is often required in the area of simulation-based optimization. For the experimental evaluation, different test problems are introduced. The proposed algorithms are evaluated on those providing insights into the working principles of the different approaches. It is experimentally shown that correcting the mutation vectors after the repair step is important for an effective evolution strategy. Additional experiments are conducted for providing a comparison to other evolutionary black-box optimization methods, which show that the developed algorithms are competitive.																	2210-6502	2210-6510				MAY	2020	54								100653	10.1016/j.swevo.2020.100653													
J								Solving dynamic overlapping community detection problem by a multiobjective evolutionary algorithm based on decomposition	SWARM AND EVOLUTIONARY COMPUTATION										Multi-objective evolutionary algorithm; Overlapping community detection; Dynamic community detection; Dynamic optimization	OPTIMIZATION ALGORITHM; NETWORKS; MOEA/D	Dynamic and overlapping are two common features of community structures for many real world complex networks. Although there are few studies on detecting dynamic overlapping communities, all those studies only consider a single optimization objective. In practice, it is necessary to evaluate the community detection by multiple metrics to reflect different aspects of a community structure and those metrics may conflict with each other. In this paper, we propose a multi-objective approach based on decomposition for the problem of dynamic overlapping community detection, with consideration of three optimization objectives: partition density (D), extended modularity (EQ), and improved mutual information (NMILFK). The dynamic overlapping network can be regarded as a set of network snapshots. The multi-objective evolutionary algorithm based on decomposition (MOEA/D) is used to detect the communities for each snapshot. To improve the search efficiency, the dynamic optimization technique and a dynamic resource allocation strategy are introduced into the approach. Experiments show that our approach can find uniformly distributed Pareto solutions for the problem and outperforms those comparative approaches.																	2210-6502	2210-6510				MAY	2020	54								100668	10.1016/j.swevo.2020.100668													
J								A multi-objective genetic algorithm based approach for dynamical bus vehicles scheduling under traffic congestion	SWARM AND EVOLUTIONARY COMPUTATION										Bus vehicle scheduling; Dynamic vehicle scheduling; Multi-objective genetic algorithm; Urban bus scheduling	EVOLUTIONARY ALGORITHM; OPTIMIZATION MODEL; NETWORK DESIGN; NSGA-II; STRATEGIES	Bus vehicle scheduling is very vital for bus companies to reduce operation cost and guarantee quality of service. Many big cities face the problem of traffic congestion, which leads to the planed vehicle scheduling scheme becoming infeasible. It is significant to study bus vehicle scheduling approaches under uncertain environments, such as traffic congestion. In this paper, a bus vehicle scheduling approach is proposed to handle the traffic congestion. It consists of three phases: firstly, a set of candidate vehicle blocks is generated once traffic congestion happens. Secondly, a non-dominated sorting genetic algorithm is adopted to select a subset of vehicle blocks from the set of candidate blocks to generate a set of Non-dominated solutions. Finally, a departure time adjustment procedure is applied to the Non-dominated solutions to further improve the quality of solutions. Experiments on a real-world bus line show that the proposed approach is able to dynamically generate scheduling schemes and significantly improve the quality of service compared to the comparative approaches.																	2210-6502	2210-6510				MAY	2020	54								100667	10.1016/j.swevo.2020.100667													
J								An improved genetic algorithm for the flexible job shop scheduling problem with multiple time constraints	SWARM AND EVOLUTIONARY COMPUTATION										Flexible job shop scheduling; Setup time; Transportation time; Genetic algorithm	HARMONY SEARCH ALGORITHM; OPTIMIZATION	The flexible job shop scheduling problem is a very important problem in factory scheduling. Most of existing researches only consider the processing time of each operation, however, jobs often require transporting to another machine for the next operation while machines often require setup to process the next job. In addition, the times associated with these steps increase the complexity of this problem. In this paper, the flexible job scheduling problem is solved that incorporates not only processing time but setup time and transportation time as well. After presenting the problem, an improved genetic algorithm is proposed to solve the problem, with the aim of minimizing the makespan time, minimizing total setup time, and minimizing total transportation time. In the improved genetic algorithm, initial solutions are generated through three different methods to improve the quality and diversity of the initial population. Then, a crossover method with artificial pairing is adopted to preserve good solutions and improve poor solutions effectively. In addition, an adaptive weight mechanism is applied to alter mutation probability and search ranges dynamically for individuals in the population. By a series of experiments with standard datasets, we demonstrate the validity of our approach and its strong performance.																	2210-6502	2210-6510				MAY	2020	54								100669	10.1016/j.swevo.2020.100669													
J								A perturbation adaptive pursuit strategy based hyper-heuristic for multi-objective optimization problems	SWARM AND EVOLUTIONARY COMPUTATION										Multi-objective optimization; Selection hyper-heuristic; Learning mechanism; Perturbation adaptive pursuit strategy	MANY-OBJECTIVE OPTIMIZATION; EVOLUTIONARY ALGORITHMS; PERFORMANCE; DIVERSITY; SEARCH; MECHANISM; SELECTION; MOEA/D	For multi-objective optimization problems, obtaining a uniformly distributed approximation set is among the most important issues. During the past decades, various diversity mechanisms have been proposed to address this challenge. However, the existing diversity mechanisms tend to be problem-specific, and may not generalize well over different problem domains. Inspired by the idea of utilizing multiple low-level heuristics to achieve better diversity performance in multi-discipline problem solving, we focus on efficient algorithm design based on the methodology of selection hyper-heuristics. This study proposes a novel selection hyper-heuristic operating over multiple diversity mechanisms. The unique feature of the proposed approach lies in its ability to intelligently learn, select, and combine different diversity mechanisms with the purpose of taking advantages of them to obtain well-distributed approximation sets. Moreover, this work develops a new learning mechanism, the perturbation adaptive pursuit strategy, which is incorporated into the proposed hyper-heuristic to improve the decision-making process of selecting suitable diversity mechanisms for the problem a hand. The performance of the proposed hyper-heuristic is tested on 2-objective ZDT, 3-objective DTLZ, and 5-objective WFG test suites. Additionally, experiments are also conducted to investigate the ability of the novel hyper-heuristic to integrate existing multiobjective meta-heuristics on MaOP test suite from 3- to 10-objectives. Experimental results demonstrate the effectiveness of the proposed selection hyper-heuristic for cross-domain capacity, particularly in producing well distributed approximation set with respect to Spacing metric and Hypervolume metric.																	2210-6502	2210-6510				MAY	2020	54								100647	10.1016/j.swevo.2020.100647													
J								Locally and globally explainable time series tweaking	KNOWLEDGE AND INFORMATION SYSTEMS										Time series classification; Interpretability; Explainability; Time series tweaking	CLASSIFICATION; FEATURES	Time series classification has received great attention over the past decade with a wide range of methods focusing on predictive performance by exploiting various types of temporal features. Nonetheless, little emphasis has been placed on interpretability and explainability. In this paper, we formulate the novel problem of explainable time series tweaking, where, given a time series and an opaque classifier that provides a particular classification decision for the time series, we want to find the changes to be performed to the given time series so that the classifier changes its decision to another class. We show that the problem is NPhard, and focus on three instantiations of the problem using global and local transformations. In the former case, we investigate the k-nearest neighbor classifier and provide an algorithmic solution to the global time series tweaking problem. In the latter case, we investigate the random shapelet forest classifier and focus on two instantiations of the local time series tweaking problem, which we refer to as reversible and irreversible time series tweaking, and propose two algorithmic solutions for the two problems along with simple optimizations. An extensive experimental evaluation on a variety of real datasets demonstrates the usefulness and effectiveness of our problem formulation and solutions.																	0219-1377	0219-3116				MAY	2020	62	5					1671	1700		10.1007/s10115-019-01389-4													
J								An FPA and GA-based hybrid evolutionary algorithm for analyzing clusters	KNOWLEDGE AND INFORMATION SYSTEMS										Clustering; Flower pollination algorithm; Genetic algorithm; Metaheuristic algorithms	OPTIMIZATION	Clustering is a technique employed for data mining and analysis. k-means is one of the algorithms utilized for clustering. However, the answer derived using this algorithm is dependent on the initial solution and hence easily retrieves the optimal local answers. To overcome the disadvantages of this algorithm, in this paper a combination of pollination of flowers algorithm and genetic algorithm, named FPAGA, is presented. Combination algorithms are used to diversify the search space of the solution and to improve its capability. To elaborate, crossover and discarding of pollens operator are utilized to increase the population diversity, while elitism operator is employed to improve the local search capabilities. Five datasets are selected to evaluate the performance of the proposed algorithm. The evaluation results demonstrate not only greater accuracy but also better stability compared to the FPA, GA, FA, DE, and k-means algorithms. Moreover, faster convergence is evident, according to the obtained statistical results.																	0219-1377	0219-3116				MAY	2020	62	5					1701	1722		10.1007/s10115-019-01413-7													
J								CDLFM: cross-domain recommendation for cold-start users via latent feature mapping	KNOWLEDGE AND INFORMATION SYSTEMS										Cross-domain recommendation; Collaborative filtering; Cold-start users; User similarity; Latent feature mapping		Collaborative filtering (CF) is a widely adopted technique in recommender systems. Traditional CF models mainly focus on predicting the user preference to items in a single domain, such as the movie domain or the music domain. A major challenge for such models is the data sparsity, and especially, CF cannot make accurate predictions for the cold-start users who have no ratings at all. Although cross-domain collaborative filtering (CDCF) is proposed for effectively transferring knowledge across different domains, it is still difficult for existing CDCF models to tackle the cold-start users in the target domain due to the extreme data sparsity. In this paper, we propose the cross-domain latent feature mapping (CDLFM) model for the cold-start users in the target domain. Firstly, in order to alleviate the data sparsity in single domain and provide essential knowledge for next step, we take users' rating behaviors into consideration and propose the matrix factorization by incorporating user similarities. Next, to transfer knowledge across domains, we propose the neighborhood-based cross-domain latent feature mapping method. For each cold-start user, we learn his/her feature mapping function based on his/her neighbor linked users. By adopting gradient boosting trees and multilayer perceptron to model the cross-domain feature mapping function, two CDLFM models named CDLFM-GBT and CDLFM-MLP are proposed. Experimental results on two real datasets demonstrate the superiority of our proposed model against other state-of-the-art methods.																	0219-1377	0219-3116				MAY	2020	62	5					1723	1750		10.1007/s10115-019-01396-5													
J								Queries of K-discriminative paths on road networks	KNOWLEDGE AND INFORMATION SYSTEMS										Path planning; Multi-objective optimization; Ant colony optimization; Evacuation plan	ANT COLONY OPTIMIZATION; WEIGHTED-SUM METHOD; SHORTEST; ROUTES	In this paper, we study the problem of searching k-discriminative paths on road networks. Given a source node src and a destination node dest on a road network, we aim to search k paths between src and dest, where these k paths satisfy the multi-objective goal including the minimization of the path overlapping and the minimization of the path length. Specifically, the requirement of minimizing the overlapping among paths, which is a NP-hard issue, is highly demanded in applications of disaster rescue management such as the evacuation plan. In this paper, we consider the deployment of k-discriminative paths for various applications, including queries for emergency-purpose applications, queries of multi-objective Pareto front for pre-schedule transportation plan, and queries with multiple sources and destinations for the regional evacuation. Due to its NP-hard nature, the heuristic strategy based on the ant colony optimization is devised in this work. As validated by our experimental studies on real road networks, the proposed algorithm can achieve the discovery of k-discriminative paths efficiently and effectively, showing its prominent advantages to be a practicable service for evacuation-related applications.																	0219-1377	0219-3116				MAY	2020	62	5					1751	1780		10.1007/s10115-019-01397-4													
J								Structured query construction via knowledge graph embedding	KNOWLEDGE AND INFORMATION SYSTEMS										Knowledge graph; Query construction; Knowledge graph embedding; Natural language question answering		In order to facilitate the accesses of general users to knowledge graphs, an increasing effort is being exerted to construct graph-structured queries of given natural language questions. At the core of the construction is to deduce the structure of the target query and determine the vertices/edges which constitute the query. Existing query construction methods rely on question understanding and conventional graph-based algorithms which lead to inefficient and degraded performances facing complex natural language questions over knowledge graphs with large scales. In this paper, we focus on this problem and propose a novel framework standing on recent knowledge graph embedding techniques. Our framework first encodes the underlying knowledge graph into a low-dimensional embedding space by leveraging generalized local knowledge graphs. Given a natural language question, the learned embedding representations of the knowledge graph are utilized to compute the query structure and assemble vertices/edges into the target query. Extensive experiments were conducted on the benchmark dataset, and the results demonstrate that our framework outperforms state-of-the-art baseline models regarding effectiveness and efficiency.																	0219-1377	0219-3116				MAY	2020	62	5					1819	1846		10.1007/s10115-019-01401-x													
J								An effective few-shot learning approach via location-dependent partial differential equation	KNOWLEDGE AND INFORMATION SYSTEMS										Few-shot learning; Face recognition; Navier-Stokes equation; PDE	FACE RECOGNITION	Recently, learning-based partial differential equation (L-PDE) has achieved success in few-shot learning area, while its feature weighting mechanism and recognition stability require further improvement. To address these issues, we propose a novel model called "location-dependent PDE" (LD-PDE) based on Navier-Stokes equation and rotational invariants in this paper. To our best knowledge, LD-PDE is the first application of the Navier-Stokes equation to achieve image recognition as a high-level vision task. Specifically, we formulate the feature variation with respect to each time step as a linear combination of rotational invariants in LD-PDE. Meanwhile, we design location-dependent mechanism to adaptively weight each invariant in an attention-based approach, which provides hierarchical discrimination in the spatial domain. Once the ultimate feature is learned, we measure the model error with the cross-entropy loss and update the parameters by the coordinate descent algorithm. As a verification, experimental results on face recognition datasets show that LD-PDE method outperforms the state-of-the-art approaches with few training samples. Moreover, compared to L-PDE, LD-PDE achieves a much more stable recognition with low sensitivity to its hyper-parameters.																	0219-1377	0219-3116				MAY	2020	62	5					1881	1901		10.1007/s10115-019-01400-y													
J								Improved algorithms for extrinsic author verification	KNOWLEDGE AND INFORMATION SYSTEMS										Author verification; Authorship analysis; Stylometry; Text categorization; Text mining	GENRE	Author verification is a fundamental problem in authorship attribution, and it suits most relevant applications where it is not possible to predefine a closed set of suspects. So far, the most successful approaches attempt to sample the non-target class (all documents by all other authors) and transform author verification to a binary classification task. Moreover, they follow the instance-based paradigm (all documents of known authorship are treated separately). In this paper, we propose two algorithms, one instance-based and one profile-based (all known documents are treated cumulatively) that are able to outperform state-of-the-art methods in several benchmark datasets. We demonstrate that the proposed methods are capable of taking advantage of the availability of multiple documents of known authorship and that they are robust when text length is reduced.																	0219-1377	0219-3116				MAY	2020	62	5					1903	1921		10.1007/s10115-019-01408-4													
J								Memory-based random walk for multi-query local community detection	KNOWLEDGE AND INFORMATION SYSTEMS										Local community detection; Local clustering; Memory-based random walk; Visiting history	SEARCH; NETWORK	Local community detection, which aims to find a target community containing a set of query nodes, has recently drawn intense research interest. The existing local community detection methods usually assume all query nodes are from the same community and only find a single target community. This is a strict requirement and does not allow much flexibility. In many real-world applications, however, we may not have any prior knowledge about the community memberships of the query nodes, and different query nodes may be from different communities. To address this limitation of the existing methods, we propose a novel memory-based random walk method, MRW, that can simultaneously identify multiple target local communities to which the query nodes belong. In MRW, each query node is associated with a random walker. Different from commonly used memoryless random walk models, MRW records the entire visiting history of each walker. The visiting histories of walkers can help unravel whether they are from the same community or not. Intuitively, walkers with similar visiting histories are more likely to be in the same community. Moreover, MRW allows walkers with similar visiting histories to reinforce each other so that they can better capture the community structure instead of being biased to the query nodes. We provide rigorous theoretical foundations for the proposed method and develop efficient algorithms to identify multiple target local communities simultaneously. Comprehensive experimental evaluations on a variety of real-world datasets demonstrate the effectiveness and efficiency of the proposed method.																	0219-1377	0219-3116				MAY	2020	62	5					2067	2101		10.1007/s10115-019-01398-3													
J								On enhancing the deadlock-preventing object migration automaton using the pursuit paradigm	PATTERN ANALYSIS AND APPLICATIONS										Object partitioning; Learning automata; Object migration automaton; Partitioning-based learning	LEARNING AUTOMATA	Probably, the most reputed solution for partitioning, which has applications in databases, attribute partitioning, processor-based assignment and many other similar scenarios, is the object migration automata (OMA). However, one of the known deficiencies of the OMA is that when the problem size is large, i.e., the number of objects and partitions are large, the probability of receiving a reward, which "strengthens" the current partitioning, from the Environment is not significant. This is because of an internal deadlock scenario which is discussed in this paper. As a result of this, it can take the OMA a considerable number of iterations to recover from an inferior configuration. This property, which characterizes learning automaton (LA) in general, is especially true for the OMA-based methods. In spite of the fact that various solutions have been proposed to remedy this issue for general families of LA, overcoming this hurdle is a completely unexplored area of research for conceptualizing how the OMA should interact with the Environment. Indeed, the best reported version of the OMA, the enhanced OMA (EOMA), has been proposed to mitigate the consequent deadlock scenario. In this paper, we demonstrate that the incorporation of the intrinsic properties of the Environment into the OMA's design leads to a higher learning capacity and to a more consistent partitioning. To achieve this, we incorporate the state-of-the-art pursuit principle utilized in the field of LA by estimating the Environment's reward/penalty probabilities and using them to further augment the EOMA. We also verify the performance of our proposed method, referred to as the pursuit EOMA (PEOMA), through simulation, and demonstrate a significant increase in the convergence rate, i.e., by a factor of about forty. It also yields a noticeable reduction in sensitivity to the noise in the Environment. The paper also includes some results obtained for a real-world application domain involving faulty sensors.																	1433-7541	1433-755X				MAY	2020	23	2					509	526		10.1007/s10044-019-00817-z													
J								Performance evaluation of psycho-acoustically motivated front-end compensator for TIMIT phone recognition	PATTERN ANALYSIS AND APPLICATIONS										VSP; Wavelet decomposition; ASR; Front-end compensator; PRA	HARMONIC ENERGY FEATURES; WAVELET PACKET FEATURES; SPEECH ENHANCEMENT; ROBUST	Wavelet-based front-end processing technique has gained popularity for its noise removing capability. In this paper, a robust automatic speech recognition system is proposed by utilizing the advantages of psycho-acoustically motivated wavelet-based front-end compensator. In the front-end compensator block, voiced speech probability-based voice activity detector system is designed to separate voiced and unvoiced frames and to update noise statistics. The wavelet packet decomposition tree is designed according to equal rectangular bandwidth (ERB) scale. Wavelet decomposition based on ERB scale is utilized here as the central frequency of the ERB distribution resembles frequency response of human cochlea. Voiced and unvoiced frames are separately decomposed into 24 sub-bands to estimate average sub-band energy (ASE) of each frame. ASE is then used to calculate threshold value. Lastly, Wiener filtering is employed for reducing the residual noise before final reconstruction stage. The proposed system is evaluated on TIMIT database under various noise conditions. The phoneme recognition accuracy of the proposed system is compared with different baseline and robust features as well as with existing front-end compensation techniques. Additionally, the proposed front-end compensator is evaluated in terms of phoneme classification accuracy. Performance improvement is observed in all above experiments.																	1433-7541	1433-755X				MAY	2020	23	2					527	539		10.1007/s10044-019-00816-0													
J								DBSCAN-like clustering method for various data densities	PATTERN ANALYSIS AND APPLICATIONS										Clustering; DBSCAN; Incremental algorithm; Various data densities; Clusters merging; Least Squares distance-like function	FAST PARTITIONING ALGORITHM; K-MEANS; OPTIMIZATION; TIME	In this paper, we propose a modification of the well-known DBSCAN algorithm, which recognizes clusters with various data densities in a given set of data points A = {a(i) epsilon R-n : i = 1,..., m}. First, we define the parameter MinPts = vertical bar ln vertical bar A vertical bar vertical bar and after that, by using a standard procedure from DBSCAN algorithm, for each a epsilon A we determine radius epsilon(a) of the circle containing MinPts elements from the set.. We group the set of all these radii into the most appropriate number (t) of clusters by using Least Squares distance-like function applying SymDIRECT or SepDIRECT algorithm. In that way, we obtain parameters epsilon(1) >... > epsilon(t). Furthermore, for parameters {MinPts, epsilon(1)} we construct a partition starting with one cluster and then add new clusters for as long as the isolated groups of at least MinPts data points in some circle with radius epsilon(1) exist. We follow a similar procedure for other parameters epsilon(2),..., epsilon(t). After the implementation of the algorithm, a larger number of clusters appear than can be expected in the optimal partition. Along with defined criteria, some of them are merged by applying a merging process for which a detailed algorithm has been written. Compared to the standard DBSCAN algorithm, we show an obvious advantage for the case of data with various densities.																	1433-7541	1433-755X				MAY	2020	23	2					541	554		10.1007/s10044-019-00809-z													
J								Smooth estimates of multiple quantiles in dynamically varying data streams	PATTERN ANALYSIS AND APPLICATIONS										Dynamically changing data stream; Incremental estimator; Multiple quantiles; Smooth quantile estimates		In this paper, we investigate the problem of estimating multiple quantiles when samples are received online (data stream). We assume a dynamical system, i. e., the distribution of the samples from the data stream changes with time. A major challenge of using incremental quantile estimators to track multiple quantiles is that we are not guaranteed that the monotone property of quantiles will be satisfied, i.e, an estimate of a lower quantile might erroneously overpass that of a higher quantile estimate. Surprisingly, we have only found two papers in the literature that attempt to counter these challenges, namely the works of Cao et al. (Proceedings of the first ACM workshop on mobile internet through cellular networks, ACM, 2009) and Hammer and Yazidi (Proceedings of the 30th international conference on industrial engineering and other applications of applied intelligent systems (IEA/AIE), France, Springer, 2017) where the latter is a preliminary version of the work in this paper. Furthermore, the state-of-the-art incremental quantile estimator called deterministic update-based multiplicative incremental quantile estimator (DUMIQE), due to Yazidi and Hammer (IEEE Trans Cybernet, 2017), fails to guarantee the monotone property when estimating multiple quantiles. A challenge with the solutions, in Cao et al.(2009) and Hammer and Yazidi(2017), is that even though the estimates satisfy the monotone property of quantiles, the estimates can be highly irregular relative to each other which usually is unrealistic from a practical point of view. In this paper, we suggest to generate the quantile estimates by inserting the quantile probabilities (e.g., 0.1, 0.2,., 0.9) into a monotonically increasing and infinitely smooth function (can be differentiated infinitely many times). The function is incrementally updated from the data stream. The monotonicity and smoothness of the function ensure that both the monotone property and regularity requirement of the quantile estimates are satisfied. The experimental results show that the method performs very well and estimates multiple quantiles more precisely than the original DUMIQE (Yazidi and Hammer 2017), and the approaches reported in Hammer and Yazidi(2017) and Cao et al.(2009).																	1433-7541	1433-755X				MAY	2020	23	2					555	566		10.1007/s10044-019-00794-3													
J								Confidence-based early classification of multivariate time series with multiple interpretable rules	PATTERN ANALYSIS AND APPLICATIONS										Multivariate time series; Early classification; Confidence estimation; Rule discovery	ENSEMBLE	In the process of early classification, earliness and accuracy are two key indicators to evaluate the performance of classification, and early classification usually weaken its accuracy to some degree. Therefore, how to find a tradeoff between two conflict objectives is a challenging work. So far, there are just a few work touched the quality of early classification on univariate time series, and the confidence estimation for early classification on multivariate time series (MTS) is still an open issue. In this paper, we focus on interpretably classifying MTS examples as early as possible while guaranteeing the quality of the classification results. First, a fast method is proposed to mine interpretable and local rules from the MTS training data. Second, a valid measure is advanced to estimate the confidence of early classification on MTS examples. Finally, a strategy is designed to execute confident early classification to assume the classification confidence meets customers' requirement. Experiment results on seven datasets show that the effectiveness and efficiency of our proposed algorithm for confident early classification on multivariate time series.																	1433-7541	1433-755X				MAY	2020	23	2					567	580		10.1007/s10044-019-00782-7													
J								A novel 3D dual active contours approach	PATTERN ANALYSIS AND APPLICATIONS										Local active contours; 3D segmentation; HRpQCT; Cortical bone	TRABECULAR COMPARTMENTS; SEGMENTATION; ENERGY	This paper investigates a 3D novel dual active contours approach to segment multiple regions in medical images. The locally based segmentation approaches can handle the heterogeneity of the image as well as the noise artefacts. In this light, a locally based dual active contours approach is proposed to separate among three regions constituting the image. The dual contours approach combines the local information along each point in the two curves conjointly with the information between them. Different parameters in this approach determine its accuracy, including the initial distance between the two curves and how much local the information is used in each curve. The approach's efficiency is evaluated on synthetic images as well as HRpQCT and MRI data compared to state-of-the-art techniques. The computational cost of this approach is reduced using the convolution operator and the FFT transform. The experimental evaluation of the approach demonstrates its segmentation performance on synthetic images and real medical images.																	1433-7541	1433-755X				MAY	2020	23	2					581	591		10.1007/s10044-019-00796-1													
J								Segmentation-free word spotting in historical Bangla handwritten document using Wave Kernel Signature	PATTERN ANALYSIS AND APPLICATIONS										Document image; Word spotting; Segmentation-free; Wave Kernel Signature (WKS); SIFT keypoint detector	RECOGNITION; RETRIEVAL; MODEL; SHAPE	In this paper, we present a segmentation-free word spotting method based on Wave Kernel Signature (WKS) under the foundation of quantum mechanics. The query word and the document page are smoothened first, then SIFT detector is used to obtain the keypoints in both the query image and the document page. A window is placed centered at each keypoint to obtain the WKS descriptors. The WKS descriptors represent the average probability of measuring a quantum mechanical particle at a specific location based on quantum energy. We use an efficient search technique which calculates minimum energy difference between query word and document image to spot where the query word appears in the document image. The proposed method is tested on three historical Bangla handwritten datasets, one Bangla handwritten dataset, one old Bangla-printed dataset and one historical English handwritten dataset. To substantiate the goodness of the proposed method, its performance is measured using standard metrics.																	1433-7541	1433-755X				MAY	2020	23	2					593	610		10.1007/s10044-019-00823-1													
J								Violence detection in videos for an intelligent surveillance system using MoBSIFT and movement filtering algorithm	PATTERN ANALYSIS AND APPLICATIONS										Violence detection; Abnormal activity detection; Action recognition; Video content analysis; Video event detection	RECOGNITION; FEATURES; MOVIES; SCENES; AUDIO; FLOW	Action recognition is an active research area in computer vision as it has enormous applications in today's world, out of which, recognizing violent action is of great importance since it is closely related to our safety and security. An intelligent surveillance system is the idea of automatically recognizing suspicious activities in surveillance videos and thereby supporting security personals to take up right action on the right time. Under this area, most of the researchers were focused on people detection and tracking, loitering, etc., whereas detecting violent actions or fights is comparatively a less studied area. Previous works considered the local spatiotemporal feature extractors; however, it accompanies the overhead of complex optical flow estimation. Even though the temporal derivative is a fast alternative to optical flow, it alone gives very low accuracy and scales-dependent result. Hence, here we propose a cascaded method of violence detection based on motion boundary SIFT (MoBSIFT) and movement filtering. In this method, the surveillance videos are checked through a movement filtering algorithm based on temporal derivative and avoid most of the nonviolent actions from going through feature extraction. Only the filtered frames may allow going through feature extraction. In addition to scale-invariant feature transform (SIFT) and histogram of optical flow feature, motion boundary histogram is also extracted and combined to form MoBSIFT descriptor. The experimental results show that the proposed MoBSIFT outperforms the existing methods in accuracy by its high tolerance to camera movements. Time complexity has also proved to be reduced by the use of movement filtering along with MoBSIFT.																	1433-7541	1433-755X				MAY	2020	23	2					611	623		10.1007/s10044-019-00821-3													
J								Segmentation of scanning tunneling microscopy images using variational methods and empirical wavelets	PATTERN ANALYSIS AND APPLICATIONS										Scanning tunneling microscopy; Segmentation; Chan-Vese; Empirical wavelets; Textures	SELF-ASSEMBLED MONOLAYERS; UNSUPERVISED TEXTURE SEGMENTATION; DIFFUSE INTERFACE METHODS; DIRECTIONAL FILTERS; CLASSIFICATION; APPROXIMATION; FRAMEWORK; DYNAMICS; CYANIDE; MUMFORD	In the fields of nanoscience and nanotechnology, it is important to be able to functionalize surfaces chemically for a wide variety of applications. Scanning tunneling microscopes (STMs) are important instruments in this area used to measure the surface structure and chemistry with better than molecular resolution. Self-assembly is frequently used to create monolayers that redefine the surface chemistry in just a single-molecule-thick layer (Love et al. in Chem Rev 105(4):1103-1170, 2005; Nuzzo and Allara in J Am Chem Soc 105(13):4481-4483, 1983; Smith et al. in Prog Surf Sci 75(1):1-68, 2004). Indeed, STM images reveal rich information about the structure of self-assembled monolayers since they convey chemical and physical properties of the studied material. In order to assist in and to enhance the analysis of STM and other images (Thomas et al. in ACS Nano 10(5):5446-5451, 2016; Thomas et al. in ACS Nano 9(5):4734-4742, 2015), we propose and demonstrate an image processing framework that produces two image segmentations: One is based on intensities (apparent heights in STM images) and the other is based on textural patterns. The proposed framework begins with a cartoon + texture decomposition, which separates an image into its cartoon and texture components. Afterward, the cartoon image is segmented by a modified multiphase version of the local Chan-Vese model (Wang et al. in Pattern Recognit 43(3):603-618, 2010), while the texture image is segmented by a combination of 2D empirical wavelet transform and a clustering algorithm. Overall, our proposed framework contains several new features, specifically in presenting a new application of cartoon + texture decomposition and of the empirical wavelet transforms and in developing a specialized framework to segment STM images and other data. To demonstrate the potential of our approach, we apply it to raw STM images of various monolayers and present their corresponding segmentation results.																	1433-7541	1433-755X				MAY	2020	23	2					625	651		10.1007/s10044-019-00824-0													
J								A robust tangent PCA via shape restoration for shape variability analysis	PATTERN ANALYSIS AND APPLICATIONS										Shape analysis; Robust statistics; Elastic metric; Shape space; Tangent PCA	OUTLIER DETECTION; PROJECTION PURSUIT; ALGORITHMS; STATISTICS; MATRIX	This paper presents a novel method for handling the effects of shape outliers in statistical shape analysis. Usually performed by a variant of classical principal component analysis (PCA), variability analysis may be highly affected by erroneous shapes. Principal components may thus imply aberrant modes, while eigenshapes may not accurately describe variability in a given set of shapes. Our robust analysis is performed using an elastic metric associated with the square-root velocity representation of shapes. This elastic shape analysis allows shape variability to be described with natural and intuitive deformations. The proposed method based on shape outlier detection applies the shape restoration procedure to rectify aberrant shapes. The resultant components are thus obtained from a tangent PCA on the restored database. By performing experiments based on MPEG-7 and HAND databases, we demonstrate that the proposed scheme is effective for shape variability analysis in the presence of outlying shapes. Our method is then compared with two existing schemes for robust data variability analysis: minimum covariance determinant-based PCA and projection pursuit-based PCA.																	1433-7541	1433-755X				MAY	2020	23	2					653	671		10.1007/s10044-019-00822-2													
J								PCAPooL: unsupervised feature learning for face recognition using PCA, LBP, and pyramid pooling	PATTERN ANALYSIS AND APPLICATIONS										Face recognition; Convolutional neural network; Local binary pattern; Pyramid pooling; Principal component analysis	GABOR; REPRESENTATION; CLASSIFICATION; PATTERNS; EIGENFACES	Human face is a widely used biometric modality for verification and revealing the identity of a person. In spite of a great deal of research on face recognition, it still is a challenging issue. Recently, the outstanding performance of deep learning has attracted a good deal of research interest for face recognition. In comparison with hand-engineered features, learning-based face features have proven their superiority in encoding discriminative information. Inspired by deep learning, we introduce a simple and efficient unsupervised feature learning scheme for face recognition. This scheme employs principle component analysis (PCA), local binary pattern (LBP), and pyramid pooling. Following the architecture of a convolutional neural network, the proposed scheme contains three types of layers: convolutional, nonlinear, and pooling layers. PCA is used to learn a filter bank for the convolutional layer. This is followed by LBP operator that encodes the local texture and adds nonlinearity to the feature maps of convolutional layer, which are then pooled using spatial pyramid pooling. To corroborate the effectiveness of the scheme (which we call as PCAPool), extensive experiments were performed on challenging benchmark databases: FERET, Yale, Extended Yale B, AR, and multi-PIE. The comparison reveals that PCAPool performs better than the state-of-the-art methods.																	1433-7541	1433-755X				MAY	2020	23	2					673	682		10.1007/s10044-019-00818-y													
J								Color image segmentation using proximal classifier and quaternion radial harmonic Fourier moments	PATTERN ANALYSIS AND APPLICATIONS										Color image segmentation; Proximal classifier with consistency; Quaternion radial harmonic Fourier moments; Twin support vector machines; Tsallis entropy	ALGORITHM; ENTROPY; SVM	Segmentation involves separating an object from the background in a given image. Image segmentation has a variety of applications and has received considerable attention in multimedia application and computer vision. Although numerous approaches have been introduced, image segmentation is still far from being solved due to most of image segmentation algorithms are often so complicated and some unsatisfactory results appear frequently. Therefore, developing a suitable technique of image segmentation is still a challenging problem. In this article, a novel color image segmentation will be introduced based on quaternion radial harmonic Fourier moments (QRHFMs) and proximal classifier. Firstly, the image feature of pixel-level is represented by the accurate and invariant QRHFMs holistically as a vector field, which can describe sufficiently the image pixel information due to take into account the relationship among different color channels. Secondly, the image feature from pixel-level is utilized as the input of the proximal classifier with consistency (PCC), which not only has lower computation time but also has better generalization compared to traditional support vector machines classifiers. Then, we choose the training samples by Tsallis entropy thresholding to train PCC classification model. Finally, the color image is classified by the trained PCC classification model. Our algorithm can make full use of the accurate and robust local image feature, as well the quickness and generalization ability of PCC classifier. A series of experimental results shows that this algorithm has better segmentation performance than the state-of-the-art method from the literature.																	1433-7541	1433-755X				MAY	2020	23	2					683	702		10.1007/s10044-019-00826-y													
J								A novel feature descriptor for image retrieval by combining modified color histogram and diagonally symmetric co-occurrence texture pattern	PATTERN ANALYSIS AND APPLICATIONS										Diagonally symmetric co-occurrence pattern; Gray level co-occurrence matrix; Histogram quantization; Corel 1K; Corel 5K; Corel 10K; MIT-VisTex database; STex database	LOCAL BINARY PATTERNS; WAVELET CORRELOGRAM; TERNARY PATTERNS; CLASSIFICATION; RECOGNITION; MATRICES	In this paper, we have proposed a novel feature descriptors combining color and texture information collectively. In our proposed color descriptor component, the inter-channel relationship between Hue (H) and Saturation (S) channels in the HSV color space has been explored which was not done earlier. We have quantized the H channel into a number of bins and performed the voting with saturation values and vice versa by following a principle similar to that of the HOG descriptor, where orientation of the gradient is quantized into a certain number of bins and voting is done with gradient magnitude. This helps us to study the nature of variation of saturation with variation in Hue and nature of variation of Hue with the variation in saturation. The texture component of our descriptor considers the co-occurrence relationship between the pixels symmetric about both the diagonals of a 3 x 3 window. Our work is inspired from the work done by Dubey et al. (IEEE Signal Process Lett 22(9):1215-1219, [2015]). These two components, viz. color and texture information individually perform better than existing texture and color descriptors. Moreover, when concatenated the proposed descriptors provide a significant improvement over existing descriptors for content base color image retrieval. The proposed descriptor has been tested for image retrieval on five databases, including texture image databases-MIT-VisTex database and Salzburg texture database and natural scene databases Corel 1K, Corel 5K and Corel 10K. The precision and recall values experimented on these databases are compared with some state-of-art local patterns. The proposed method provided satisfactory results from the experiments.																	1433-7541	1433-755X				MAY	2020	23	2					703	723		10.1007/s10044-019-00827-x													
J								CHSPAM: a multi-domain model for sequential pattern discovery and monitoring in contexts histories	PATTERN ANALYSIS AND APPLICATIONS										Ubiquitous computing; Pattern discovery; Context histories; Data mining	AWARE SYSTEM; RECOGNITION; COMPUTER	Context-aware applications adapt their functionalities based on users contexts. Complementarily, a context history has information about previous contexts visited by a user. Context history enables applications to explore users past behavior. Researchers have studied different ways to analyze these data. This article addresses a specific type of data analysis in contexts histories, which is the discovery and monitoring of sequential patterns. The article proposes a model, called CHSPAM, that allows the discovery of sequential patterns in contexts histories databases and keeps track of these patterns to monitor their evolution over time. There are two main contributions of this work. The first one is the use of a generic representation for stored context information on pattern recognition field, which enables the model to be used for different research domains. The second contribution is the fact that CHSPAM monitors discovered pattern evolution over time. We have build a functional prototype that allowed us to conduct experiments in two different applications. The first experiment used the model to perform pattern analysis and evaluate the prediction based on monitored sequential patterns. Prediction accuracy increased by up to 17% when compared to the use of common sequential patterns. On the second experiment, CHSPAM was used as a component of a learning object recommendation application. The application was able to recommend learning objects related to students interests based on monitored sequential patterns extracted from users session history. Usefulness for recommendations reached 84%.																	1433-7541	1433-755X				MAY	2020	23	2					725	734		10.1007/s10044-019-00829-9													
J								Locality preserving difference component analysis based on the Lq norm	PATTERN ANALYSIS AND APPLICATIONS										Difference component analysis; Lq norm; Convergence analysis; Dimensionality reduction	DISCRIMINANT-ANALYSIS; PCA	This paper develops locality preserving difference component analysis in which the intrinsic and global structure of data is exploited, and the model we propose also provides the flexibility to adapt some characteristics of data by applying the Lq norm. In order to solve the proposed model that is non-convex or non-smooth, we resort to the proximal alternating linearized optimization approach where each subproblem has good optimization properties. It is observed that the objective function in the proposed model is a semi-algebraic function. This allows us to give the convergence analysis of algorithms in terms of the Kurdyka-Lojasiewicz property. To be specific, the sequence of iterations generated by the proposed approach converges to a critical point of the objective function. The experiments on several data sets have been conducted to demonstrate the effectiveness of the proposed approach.																	1433-7541	1433-755X				MAY	2020	23	2					735	749		10.1007/s10044-019-00834-y													
J								Pedestrian detection using multiple feature channels and contour cues with census transform histogram and random forest classifier	PATTERN ANALYSIS AND APPLICATIONS										Pedestrian detection; Random forest; Contour detector; Visual cues; Census transform	OBJECT DETECTION; TIME	This paper presents a reliable and real-time method to detect pedestrians in image scenes that can vary greatly in appearance. To achieve greater reliability in what can be detected, a combination of visual cues is used in conjunction with edge-based features and colour information as a basis for training a random forest (RF) classifier to detect the local contour cues for pedestrian images. To achieve a real-time detection rate, the contour cues, edge-based features and colour information are incorporated and then trained using a cascade RF classifier with a census transform histogram visual descriptor that implicitly captures the global contours of the pedestrians. The contour detector favourably exceeded previous leading contour detectors and achieved a 95% detection rate. The reliability and specificity of the pedestrian detector are demonstrated on more than 5000 positive images containing street furniture, lamp posts and trees, structures that are frequently confused with persons by computer vision systems. Evaluation with over 220 video sequences with 640 x 480 pixel resolution presented a true positive rate of 96%. The proposed pedestrian detector outperforms previous competitive pedestrian detectors on many varied person data sets. The speed of execution in a robot is about 62 ms per frame for images of 640 x 480 pixels on an Intel Core i3-2310M (TM) processor running at 2.10 GHz with a RAM of 4 GB.																	1433-7541	1433-755X				MAY	2020	23	2					751	769		10.1007/s10044-019-00835-x													
J								Evaluating dynamic texture descriptors to recognize human iris in video image sequence	PATTERN ANALYSIS AND APPLICATIONS										Iris texture; Dynamic texture; Dynamic texture descriptor; Local descriptor	CLASSIFICATION; SYSTEM; FEATURES	In the last decades, iris features have been widely used in biometric systems. Because iris features are virtually unique for each person, their usage is highly reliable. However, biometric systems based on iris features are not completely fraud-resistant, as most systems use static images and do not distinguish between a live iris and a photograph. The iris structure and texture change with light variations, and traditional techniques for iris recognition always identify the iris texture in a controlled environment. However, in uncontrolled environments, live irises are recognized by their dynamic response to light: If the light changes, the pupils dilate or contract, and their texture dynamically changes. If a biometric system can identify people during the constriction or dilation time interval, that system will be more fraud-resistant. This paper proposes a new methodology to evaluate the "dynamic texture" from iris image sequences (motion analysis) and measure the discriminant power of these features for biometric system applications. We propose two new dynamic descriptors-dynamic local mapped pattern and dynamic sampled local mapped pattern-which are extensions of the local mapped pattern previously published for texture classification. We applied our proposed dynamic texture descriptors in a sequence of iris images segmented from video under light variation. Then, we compared our results with the well-known dynamic texture descriptor local binary pattern from three orthogonal planes (LBP-TOP). We used statistical measures to evaluate the performance of both descriptors and concluded that our methodology performed better than the LBP-TOP. Moreover, our descriptors can extract dynamic textures faster than the LBP-TOP.																	1433-7541	1433-755X				MAY	2020	23	2					771	784		10.1007/s10044-019-00836-w													
J								The modified generic polar harmonic transforms for image representation	PATTERN ANALYSIS AND APPLICATIONS										Modified polar harmonic transforms; Polar harmonic transforms; Rotation invariants; Image representation	FOURIER-MELLIN MOMENTS; ORTHOGONAL-MOMENTS; INVARIANTS; RECOGNITION; ERRATA; BLUR	This paper introduces four classes of orthogonal transforms by modifying the generic polar harmonic transforms. Then, the rotation invariant feature of the proposed transforms is investigated. Compared with the traditional generic polar harmonic transforms, the proposed transforms have the ability to describe the central region of the image with a parameter controlling the area of the region. Experimental results verified the image representation capability of the proposed transforms and showed better performance of the proposed transform in terms of rotation invariant pattern recognition.																	1433-7541	1433-755X				MAY	2020	23	2					785	795		10.1007/s10044-019-00840-0													
J								Local gradient of gradient pattern: a robust image descriptor for the classification of brain strokes from computed tomography images	PATTERN ANALYSIS AND APPLICATIONS										Brain stroke; Hemorrhagic; Ischemic; Feature extraction; Local binary patterns; Classification	FEATURE-SELECTION; BINARY PATTERNS; TEXTURE; RETRIEVAL	This paper presents a new feature extraction method for the classification of brain computed tomography (CT) scan images into hemorrhagic strokes, ischemic strokes and normal CT images. The most popular feature extraction method is local binary pattern (LBP), which works by thresholding the neighboring pixel values with the center pixel value of the image. Unlike LBP, our proposed method is based on comparing neighbors of center pixel and the mean of whole image intensities in the first step, and computing double gradients of local neighborhoods of a center pixel of the original image in x and y directions in the second step. Further, values obtained from the first step are compared with double gradients of neighbors in order to generate codes for the center pixel. We have also calculated the codes for the first step. Thereafter, histograms of all the codes are generated and finally concatenated to form a single feature vector. We termed this descriptor as the local gradient of gradient pattern. We have performed nine different experiments where images have been classified using various classifiers. The efficacy of our feature descriptor for image classification is identified by comparing it with seven different feature extraction methods. Performances of these methods are tested using metrics such as precision, true positive rate, false positive rate, F-measure and accuracies of the classifier. Results obtained show that our method is superior to other previous descriptors.																	1433-7541	1433-755X				MAY	2020	23	2					797	817		10.1007/s10044-019-00838-8													
J								Fractal dimension of synthesized and natural color images in Lab space	PATTERN ANALYSIS AND APPLICATIONS										Fractal dimension; Differential box-counting method; Fractal Brownian motion; Lab color space; Multiple linear regression	BOX-COUNTING METHOD; TEXTURE DESCRIPTION; CLASSIFICATION; SEGMENTATION; ART	Fractal dimension (FD) is a useful metric for the analysis of natural images that exhibit a high degree of complexity, randomness and irregularity in color and texture. Several approaches exist in the literature to measure FD of gray-scale images. The aim of this study is to introduce a FD estimation method for color images with color proximity in Lab space. The proposed method uses a xy-plane partitioning-shifting mechanism, where the divisors of image size are used as grid sizes. The proposed method simulates on synthesized color fractal Brownian motion (FBM) images, publicly available Brodatz database, Google color fractal images and noisy Brodatz database. The random midpoint displacement algorithm for the formation of gray-scale images is extended in this work to synthesize color FBM images. Noisy Brodatz database is obtained by adding salt-and-pepper noise with different noise densities to understand the behavior of FD. The experimental results illustrate that the proposed method is effective and efficient and outperforms the three state-of-the-art methods by observing the values of two proposed metrics, namely average error and average computed FD. A new mathematical expression for estimating FD of a color image is demonstrated, which relies on the number of edge pixels of individual color channel using multiple linear regression.																	1433-7541	1433-755X				MAY	2020	23	2					819	836		10.1007/s10044-019-00839-7													
J								A novel CT image segmentation algorithm using PCNN and Sobolev gradient methods in GPU frameworks	PATTERN ANALYSIS AND APPLICATIONS										Image segmentation; Level set; Pulse-coupled neural network; Sobolev gradient; CT images		Accurate brain tumor segmentation plays a significant role in the area of radiotherapy diagnosis and in the proper treatment for brain tumor detection. Typically, the brain tumor has poor boundary and low contrast between normal and lesion soft tissues that makes segmentation of brain tumor in the CT images a challenging task. This paper presents a novel approach to brain image segmentation using pulse-coupled neural network (PCNN) and zero level set (ZL) with Sobolev gradient (SG) method. In this article, PCNN is designed to use as an edge mapper to provide a regional description for the ZL to segregate the CT images based on contour maps. The PCNN is used to estimate the exact threshold to obtain the prominent edges of the images. Resulting edges are utilized in the ZL to extract image contour from the source image. Due to the over-sensitivity of the ZL method on the initial contour, a level set with the SG has been equipped to overcome the limitation of the ZL method. The experimental results show satisfactory segmentation outcomes with excellent accuracy and acceleration in comparison with the state-of-the-art methods.																	1433-7541	1433-755X				MAY	2020	23	2					837	854		10.1007/s10044-019-00837-9													
J								Local tangent space alignment based on Hilbert-Schmidt independence criterion regularization	PATTERN ANALYSIS AND APPLICATIONS										Dimensionality reduction; Local tangent space alignment; Reproducing kernel Hilbert spaces; Hilbert-Schmidt independence criterion	NONLINEAR DIMENSIONALITY REDUCTION; FRAMEWORK; EIGENMAPS	Local tangent space alignment (LTSA) is a famous manifold learning algorithm, and many other manifold learning algorithms are developed based on LTSA. However, from the viewpoint of dimensionality reduction, LTSA is only a local feature preserving algorithm. What the community of dimensionality reduction is now pursuing are those algorithms capable of preserving both local and global features at the same time. In this paper, a new algorithm for dimensionality reduction, called HSIC-regularized LTSA (HSIC-LTSA), is proposed, in which a HSIC regularization term is added to the objective function of LTSA. HSIC is an acronym for Hilbert-Schmidt independence criterion and has been used in many applications of machine learning. However, HSIC has not been directly applied to dimensionality reduction so far, neither used as a regularization term to combine with other machine learning algorithms. Therefore, the proposed HSIC-LTSA is a new try for both HSIC and LTSA. In HSIC-LTSA, HSIC makes the high- and low-dimensional data statistically correlative as much as possible, while LTSA reduces the data dimension under the local homeomorphism-preserving criterion. The experimental results presented in this paper show that, on several commonly used datasets, HSIC-LTSA performs better than LTSA as well as some state-of-the-art local and global preserving algorithms.																	1433-7541	1433-755X				MAY	2020	23	2					855	868		10.1007/s10044-019-00810-6													
J								Text area segmentation from document images by novel adaptive thresholding and template matching using texture cues	PATTERN ANALYSIS AND APPLICATIONS										Adaptive thresholding for document images; Thresholding; Gamma correction; Text binarization; Text area localization; Difference theoretic texture features	BINARIZATION; CLASSIFICATION; HANDWRITTEN; SCALE	This paper presents a new perspective of text area segmentation from document images using a novel adaptive thresholding for image enhancement. Using sliding windows, the texture of the enhanced image is matched with that of a fixed training template image containing the typed letters 'dB.' The affine-invariant, low-dimensional difference theoretic texture feature set is used for the texture measurement. The distance matrix is binarized using Otsu threshold, and the '0' pixels indicate the text area. One primary contribution of this paper is the novel adaptive thresholding for document image enhancement prior to the extraction of texture cues. The proposed adaptive thresholding mimics the ability of the human eye to iteratively adjust to varying light intensities through iterative gamma correction followed by contrast stretching so that the text becomes well defined against the background clutter. The text blobs so segmented are binarized using Yanowitz and Bruckstein method of text binarization, and the results are applied for evaluation with respect to the ground-truth annotations. We tested our algorithm on the benchmark DIBCO 2009, 2010, 2011, 2012, 2013 document image datasets in comparison with the state of the art. The high precision-recall and F-score values establish the efficiency of our approach.																	1433-7541	1433-755X				MAY	2020	23	2					869	881		10.1007/s10044-019-00811-5													
J								A novel contextual memory algorithm for edge detection	PATTERN ANALYSIS AND APPLICATIONS										Edge detection; Local context; Neural network; Probabilistic method; BSDS500 benchmark		Edge detection plays an important role in many computer vision systems. In this paper, we propose a novel application agnostic algorithm for prediction of probabilities based on the contextual information available and then apply the algorithm for estimating the probability of pixels belonging to an edge using surrounding pixel values as local contexts. We then proceed to test different image transformations as input layers, such as the Canny edge detector. We propose two different architectures, one single layered and one multilayered, which approach the scaling problem by creating scaled side outputs and combining them via a logistic regression layer. We tested our approach on the BSDS500 edge detection dataset with optimistic results.																	1433-7541	1433-755X				MAY	2020	23	2					883	895		10.1007/s10044-019-00808-0													
J								Parallel cycle-based branch-and-bound method for Bayesian network learning	PATTERN ANALYSIS AND APPLICATIONS										Medical diagnosis; Bayesian network; Structure learning; Branch-and-bound; Optimization; Cycle-based; Parallel computing	CLASSIFICATION	Bayesian networks (BNs) are one of the most commonly used models for representing uncertainty in medical diagnosis. Learning the exact structure of a BN is a challenging problem. This paper proposes a multi-threaded branch-and-bound (B&B) method, called parallel cycle-based branch-and-bound (parallel CB-B&B). On the one hand, CB-B&B improves the standard B&B method by leveraging two heuristics, namely the branching strategy and the bounding operators; on the other hand, the learning procedure is alleviated by executing CB-B&B over a set of parallel processors. In comparison with conventional exact structure learning approaches for BN, the obtained results demonstrate that the proposed CB-B&B is efficient. On average, it produces the exact structure for BN three times faster than the standard B&B version. We also present simulations on parallel CB-B&B which show a significant gain in terms of execution time.																	1433-7541	1433-755X				MAY	2020	23	2					897	911		10.1007/s10044-019-00815-1													
J								Finding patterns in the degree distribution of real-world complex networks: going beyond power law	PATTERN ANALYSIS AND APPLICATIONS										Degree distribution; Power-law distribution; Sigmoid function; Hyperbolic tangent function; KL-divergence; Goodness-of-fit	INCOME	The most important structural characteristics in the study of large-scale real-world complex networks in pattern analysis are degree distribution. Empirical observations on the pattern of the real-world networks have led to the claim that their degree distributions follow, in general, a single power law. However, a closer observation, while fitting, shows that the single power-law distribution is often inadequate to meet the data characteristics properly. Since the degree distribution in the log-log scale actually displays, under inspection, two different slopes unlike what happens while fitting with the single power law. These two slopes with a transition in between closely resemble the pattern of the sigmoid function. This motivates us to derive a novel double power-law distribution for accurately modeling the real-world networks based on the sigmoid function. The proposed modeling approach further leads to the identification of a transition degree which, it has been demonstrated, may have a significant implication in analyzing the complex networks. The applicability, as well as effectiveness of the proposed methodology, is shown using rigorous experiments and also validated using statistical tests.																	1433-7541	1433-755X				MAY	2020	23	2					913	932		10.1007/s10044-019-00820-4													
J								Synchronization correction-based robust digital image watermarking approach using Bessel K-form PDF	PATTERN ANALYSIS AND APPLICATIONS										Image watermarking; Synchronization correction; BKF probability density function; Nonsubsampled shearlet transform; Least squares support vector regression	SCHEME	Robustness has played extremely important role in the multiple applications of digital watermarking technology. The reason lies in its influence on the watermarking system's practicability. As one of the most difficult kinds of digital signal processing for a digital watermark to survive, geometric distortions have become a central problem in digital image watermarking research. Therefore, resigning a greatly robust digital image watermarking approach which can withstand geometric distortions is still a quite challenging work. On account of Bessel K-form (BKF) probability density function (PDF), this paper proposes an optimal synchronization correction-based digital image watermarking method which can resist geometric distortions. This approach consists of watermark embedding, synchronization correction and watermark extraction. Using the quantization index modulation (QIM), the watermark is inserted into the original host images in the nonsubsampled shearlet transform (NSST) domain by adjusting the selected blocks' low-frequency NSST coefficients. The BKF PDF describes the NSST coefficients' statistical features. Besides, the BKF statistical model parameters can be used in constructing a compact image characteristic space. Utilizing the compact image feature, the least squares support vector regression (LS-SVR) synchronization correction is performed to estimate the geometric distortions parameters. After LS-SVR synchronization correction, the inverse QIM is performed to recover blindly the inserted watermark. Simulation results demonstrate that the presented digital image watermarking method not only has good imperceptibility performance, but also can well resist challenging common signal processing operations as well as geometric distortions, which can be superior to the most advanced approaches.																	1433-7541	1433-755X				MAY	2020	23	2					933	951		10.1007/s10044-019-00828-w													
J								An automatic multi-camera-based event extraction system for real soccer videos	PATTERN ANALYSIS AND APPLICATIONS										Event extraction; Soccer videos; Multiple cameras; Deep neural network; U-encoder	BALL DETECTION; ABNORMALITY DETECTION; TRACKING; SEGMENTATION; ALGORITHM	In this article, we propose a novel and effective system based on multiple cameras to extract the events for soccer matches. A precise ontological definition of the soccer events is still an open point. According to our definition, the events include the free kick, corner kick, penalty kick and the goal, because they are the representative shots for the audience to watch. The events are very important for highlights selection and sport data analysis. At present, the events including the ball and players information are selected and labeled manually from the images, which is a big workload for the staffs. Addressing this problem, our system provides an automatic extraction of the events. For soccer videos, our system first uses the local-based deep neural network for the ball and player detection from the input images. Then, we handle with the ball and player bounding boxes separately. For players, a player can be labeled as one of the three types: two teams or the referee, and a novel unsupervised U-encoder is designed for the player labeling. For soccer ball, the application of multiple cameras allows us to refine the ball detection results. We can get the world coordinate of ball according to the camera parameters and then rebuild the ball trajectory and the court in a top view. Based on the reconstructed map, we get the soccer events by motion analysis of ball trajectory and then apply the ball location and player classification results to display the events for each camera. The test results on real videos of European soccer league show the good detection and labeling performance of our system. We find all the events in the test videos. Our proposed system can deal with many complex cases such as occlusion and pose variation that happen frequently in real applications.																	1433-7541	1433-755X				MAY	2020	23	2					953	965		10.1007/s10044-019-00830-2													
J								Manifold ranking graph regularization non-negative matrix factorization with global and local structures	PATTERN ANALYSIS AND APPLICATIONS										Non-negative matrix factorization; Clustering; Manifold ranking; Regularization		Non-negative matrix factorization (NMF) is a recently popularized technique for learning parts-based, linear representations of non-negative data. Although the decomposition rate of NMF is very fast, it still suffers from the following deficiency: It only revealed the local geometry structure; global geometric information of data set is ignored. This paper proposes a manifold ranking graph regularization non-negative matrix factorization with local and global geometric structure (MRLGNMF) to overcome the above deficiency. In particular, MRLGNMF induces manifold ranking to the non-negative matrix factorization with Sinkhorn distance. Numerical results show that the new algorithm is superior to the existing algorithm.																	1433-7541	1433-755X				MAY	2020	23	2					967	974		10.1007/s10044-019-00832-0													
J								BISDBx: towards batch-incremental clustering for dynamic datasets using SNN-DBSCAN	PATTERN ANALYSIS AND APPLICATIONS										Density based clustering; Incremental; Batch; Dynamic datasets; Insertion; Deletion		Many important applications such as recommender systems, e-commerce sites, web crawlers involve dynamic datasets. Dynamic datasets undergo frequent changes in the form of insertion or deletion of data that affects its size. A naive algorithm may not process these frequent changes efficiently as it involves the entire set of data points each time a change is inflicted. Fast incremental algorithms process these updates to datasets efficiently to avoid redundant computation. In this article, we propose incremental extensions to shared nearest neighbor density-based clustering (SNNDB) algorithm for both addition and deletion of data points. Existing incremental extension to SNNDB viz. InSDB cannot handle deletion and handles insertions one point at a time. Our method overcomes both these bottlenecks by efficiently identifying affected parts of clusters while processing updates to dataset in batch mode. We propose three incremental variants of SNNDB in batch mode for both addition and deletion with the third variant being the most effective. Experimental observations on real world and synthetic datasets showed that our algorithms are up to 4 orders of magnitude faster than the naive SNNDB algorithm and about 2 orders of magnitude faster than the pointwise incremental method.																	1433-7541	1433-755X				MAY	2020	23	2					975	1009		10.1007/s10044-019-00831-1													
J								A Chinese unknown word recognition method for micro-blog short text based on improved FP-growth	PATTERN ANALYSIS AND APPLICATIONS										Unknown word recognition; FP-growth algorithm; Mutual information; Information entropy		Unknown word recognition technology is of great significance to improve the precision of text segmentation and syntax analysis. Social network has become an important platform for sharing, disseminating, and acquiring information. Unknown word recognition based on micro-blog short text has become a research hot spot, while the micro-blog text contains a large number of nonstandard terms and network buzzwords, which has increased the difficulty of unknown word recognition. This paper proposes a Chinese unknown word recognition method for micro-blog short text based on improved FP-growth (POS-FP). Firstly, the POS-FP algorithm is used to get frequent itemsets from micro-blog, and the N-grams model is used to filter out unknown words from frequent itemsets. Secondly, the improved mutual information and left-right information entropy are used to verify the internal features of candidate unknown words. Then, context-dependent and open-source methods are used for external verification of candidate unknown words. Compared with traditional methods, this algorithm improves the recognition rate of unknown words in micro-blog short texts.																	1433-7541	1433-755X				MAY	2020	23	2					1011	1020		10.1007/s10044-019-00833-z													
J								Optimal face templates: the next step in surveillance face recognition	PATTERN ANALYSIS AND APPLICATIONS										Template creation; Multiple training images; Surveillance face recognition; Classifier training; Classifier learning	BIOMETRICS	The paper deals with surveillance face recognition in security applications such as surveillance camera systems or access control systems. Presented research is focused on enhancing recognition performance, reducing classification time and memory requirements. We aim to make it feasible to implement face recognition in end devices such as cameras, identification terminals or popular IoT devices. Therefore, we utilize algorithms that require low computational power and optimize them in order to reach higher recognition rates. We present a novel higher quantile method that enhances recognition performance via creation of robust and representative face templates for nearest neighbor classifier. Templates computed by the higher quantile method are determined by tolerance intervals which handle feature variability caused by face pose, expression, illumination and possible low image quality. The recognition performance evaluation has been conducted on images captured by surveillance camera system that are contained in unique IFaViD dataset. The IFaViD is the only one dataset captured by real surveillance camera system containing complex scenarios. The results show that the higher quantile method outperforms the contemporary approaches by 4%, respectively, 10% depending on the IFaViD's test subset.																	1433-7541	1433-755X				MAY	2020	23	2					1021	1032		10.1007/s10044-019-00842-y													
J								Rotation invariant features based on three dimensional Gaussian Markov random fields for volumetric texture classification	COMPUTER VISION AND IMAGE UNDERSTANDING										COPD; 3D-GMRF; Volumetric texture; Classification	CONVOLUTIONAL NEURAL-NETWORKS; COMPUTER-AIDED DIAGNOSIS; PULMONARY-EMPHYSEMA; LUNG-DISEASE; SCALE; CT; QUANTIFICATION; TOMOGRAPHY	This paper proposes a set of rotation invariant features based on three dimensional Gaussian Markov Random Fields (3D-GMRF) for volumetric texture image classification. In the method proposed here, the mathematical notion of spherical harmonics is employed to produce a set of features which are used to construct the rotation invariant descriptor. Our proposed method is evaluated and compared with other method in the literature for datasets containing synthetic textures as well as medical images. The results of our experiments demonstrate excellent classification performance for our proposed method compared with state-of-the-art methods. Furthermore, our method is evaluated using a clinical dataset and show good performance in discriminating between healthy individuals and COPD patients. Our method also performs well in classifying lung nodules in the LIDC-IDRI dataset. Our results indicate that our 3D-GMRF-based method enjoys more superior performance compared with other methods in the literature.																	1077-3142	1090-235X				MAY	2020	194								102931	10.1016/j.cviu.2020.102931													
J								Protuberance of depth : Detecting interest points from a depth image	COMPUTER VISION AND IMAGE UNDERSTANDING										Interest point detection; Depth image; Feature extraction; Protuberance of depth	PERFORMANCE EVALUATION; OBJECT RECOGNITION; 3D; FEATURES	Detecting distinctive interest points in a scene or an object allows estimating which details a human finds interesting in advance to understand the scene or the object. This also forms the important basis of a variety of latter tasks related to visual detection and tracking. In this paper, we propose a simple but effective approach to extract the feature from a depth image, namely Protuberance of Depth (PoD). The proposed approach semantically explores the inherent feature representing three-dimensional protuberance by using depth which only contains two-dimensional distance information. Our approach directly allows detecting consistent interest points in a depth image. The experimental results show that our method is effective against the isometric deformation and rotation of a depth region and is applicable for real-time applications.																	1077-3142	1090-235X				MAY	2020	194								102927	10.1016/j.cviu.2020.102927													
J								Efficient distance transformation for path-based metrics	COMPUTER VISION AND IMAGE UNDERSTANDING											LINEAR-TIME ALGORITHM; MEDIAL AXIS; CHAMFER DISTANCES; 2D	In many applications, separable algorithms have demonstrated their efficiency to perform high performance volumetric processing of shape, such as distance transformation or medial axis extraction. In the literature, several authors have discussed about conditions on the metric to be considered in a separable approach. In this article, we present generic separable algorithms to efficiently compute Voronoi maps and distance transformations for a large class of metrics. Focusing on path-based norms (chamfer masks, neighborhood sequences), we propose efficient algorithms to compute such volumetric transformation in dimension n. We describe a new O(n center dot N-n center dot logN center dot (n + logf)) algorithm for shapes in a N-n domain for chamfer norms with a rational ball of f facets (compared to O(f([n/2])center dot N-n) with previous approaches). Last we further investigate a more elaborate algorithm with the same worst-case complexity, but reaching a complexity of O(n center dot N-n center dot logf center dot (n+logf)) experimentally, under assumption of regularity distribution of the mask vectors.																	1077-3142	1090-235X				MAY	2020	194								102925	10.1016/j.cviu.2020.102925													
J								Graph convolutional neural network for multi-scale feature learning	COMPUTER VISION AND IMAGE UNDERSTANDING										Deep learning; Graph convolutional neural network; Medical image segmentation; Marginal space learning; Aortic root; Computerized tomography	SHAPE MODELS; SCALE-SPACE; SEGMENTATION; CNN; CT	Automatic deformable 3D modeling is computationally expensive, especially when considering complex position, orientation and scale variations. We present a volume segmentation framework to utilize local and global regularizations in a data-driven approach. We introduce automated correspondence search to avoid manually labeling landmarks and improve scalability. We propose a novel marginal space learning technique, utilizing multi-resolution pooling to obtain local and contextual features without training numerous detectors or excessively dense patches. Unlike conventional convolutional neural network operators, graph-based operators allow spatially related features to be learned on the irregular domain of the multi-resolution space, and a graph-based convolutional neural network is proposed to learn representations for position and orientation classification. The graph-CNN classifiers are used within a marginal space learning framework to provide efficient and accurate shape pose parameter hypothesis prediction. During segmentation, a global constraint is initially non-iteratively applied, with local and geometric constraints applied iteratively for refinement. Comparison is provided against both classical deformable models and state-of-the-art techniques in the complex problem domain of segmenting aortic root structure from computerized tomography scans. The proposed method shows improvement in both pose parameter estimation and segmentation performance.																	1077-3142	1090-235X				MAY	2020	194								102881	10.1016/j.cviu.2019.102881													
J								A spatial-spectral semisupervised deep learning framework using siamese networks and angular loss	COMPUTER VISION AND IMAGE UNDERSTANDING										Semi-supervised deep learning; Angular feature extraction; Angular softmax classification; Unsupervised pre-training; Spatial-spectral classification	CLASSIFICATION	Deep learning has gained popularity in recent times in the field of feature-extraction, object-identification, object-tracking, change-detection, image-classification, spatio-temporal-data analysis, and hyperspectral imaging. Most of the supervised tasks using deep learning require a large number of labeled samples, barring which the model tends to overfit and do not generalize well to the test data. Semi-supervised learning is very beneficial for hyperspectral images which contain abundant unlabeled data samples in comparison to labeled data. Furthermore, it is known that for datasets in which samples are related to each other in all three dimensions such as videos, three-dimensional biological images and hyperspectral images, the use of spatial-spectral/spatial-temporal based deep learning strategies, which can exploit the relationship between pixels in all three-dimensions, has also seen a rise in the past few years. Moreover, to date, deep feature extraction and classification has been done using euclidean distance based metrics. Foray into the field of angular feature extraction and classification, which is known to work better when samples are impacted by resolution or illumination differences, has not yet been made. We propose a novel spatial-spectral semisupervised deep learning approach based on angular distances by projecting the deep features onto the surface of an l(2)-normalized unit hypersphere.																	1077-3142	1090-235X				MAY	2020	194								102943	10.1016/j.cviu.2020.102943													
J								ALCN: Adaptive Local Contrast Normalization	COMPUTER VISION AND IMAGE UNDERSTANDING											EIGENFACES; SCALE	To make Robotics and Augmented Reality applications robust to illumination changes, the current trend is to train a Deep Network with training images captured under many different lighting conditions. Unfortunately, creating such a training set is a very unwieldy and complex task. We therefore propose a novel illumination normalization method that can easily be used for different problems with challenging illumination conditions. Our preliminary experiments show that among current normalization methods, the Difference-of-Gaussians method remains a very good baseline, and we introduce a novel illumination normalization model that generalizes it. Our key insight is then that the normalization parameters should depend on the input image, and we aim to train a Convolutional Neural Network to predict these parameters from the input image. This, however, cannot be done in a supervised manner, as the optimal parameters are not known a priori. We thus designed a method to train this network jointly with another network that aims to recognize objects under different illuminations: The latter network performs well when the former network predicts good values for the normalization parameters. We show that our method significantly outperforms standard normalization methods and would also be appear to be universal since it does not have to be re-trained for each new application. Our method improves the robustness to light changes of state-of-the-art 3D object detection and face recognition methods.																	1077-3142	1090-235X				MAY	2020	194								102947	10.1016/j.cviu.2020.102947													
J								Momental directional patterns for dynamic texture recognition	COMPUTER VISION AND IMAGE UNDERSTANDING										Dynamic texture; Dynamic texture recognition; LDP; SBP; LBP; Moment images; Video representation	LOCAL BINARY PATTERN; CLASSIFICATION; REPRESENTATION; SCHEME; COUNT; SCALE; VIDEO	Understanding the chaotic motions of dynamic textures (DTs) is a challenging problem of video representation for different tasks in computer vision. This paper presents a new approach for an efficient DT representation by addressing the following novel concepts. First, a model of moment volumes is introduced as an effective pre-processing technique for enriching the robust and discriminative information of dynamic voxels with low computational cost. Second, two important extensions of Local Derivative Pattern operator are proposed to improve its performance in capturing directional features. Third, we present a new framework, called Momental Directional Patterns, taking into account the advantages of filtering and local-feature-based approaches to form effective DT descriptors. Furthermore, motivated by convolutional neural networks, the proposed framework is boosted by utilizing more global features extracted from max-pooling videos to improve the discrimination power of the descriptors. Our proposal is verified on benchmark datasets, i.e., UCLA, DynTex, and DynTex++, for DT classification issue. The experimental results substantiate the interest of our method.																	1077-3142	1090-235X				MAY	2020	194								102882	10.1016/j.cviu.2019.102882													
J								Image dehazing based on a transmission fusion strategy by automatic image matting	COMPUTER VISION AND IMAGE UNDERSTANDING										Transmission fusion; Image matting; Image dehazing; Dark channel prior; Veil correction	SINGLE; VISIBILITY; FRAMEWORK; WEATHER	Most dehazing methods fail to estimate satisfactory transmission simultaneously in both normal and bright regions. To estimate more accurate transmission for these two kinds of regions, we propose a transmission fusion strategy based on automatic image matting for image dehazing. We first extract the mean and variance of a local patch around each pixel, and propose a binary classification method with the mean and variance of each patch to coarsely segment an input image into a binary map of normal and bright regions. Then we smooth and quantize the binary map to automatically generate a trimap of ternary values. Thus we can avoid the difficulty in manually labeling trimaps. Both the image and the trimap are input into a Bayesian matting method for soft segmentation of normal and bright regions to produce an alpha map. The dark channel prior (DCP) is adopted to extract a transmission map for normal regions, while an improved atmospheric veil correction (AVC) method is proposed to generate another transmission map for bright regions. Finally, we propose to use the alpha map to fuzzily fuse the two transmission maps for final image dehazing. Experimental results show that our method significantly outperforms existing methods.																	1077-3142	1090-235X				MAY	2020	194								102933	10.1016/j.cviu.2020.102933													
J								Quality-informed semi-automated event log generation for process mining	DECISION SUPPORT SYSTEMS										Process mining; Data quality; Event log; Log extraction	DESIGN SCIENCE	Process mining, as with any form of data analysis, relies heavily on the quality of input data to generate accurate and reliable results. A fit-for-purpose event log nearly always requires time-consuming, manual pre-processing to extract events from source data, with data quality dependent on the analyst's domain knowledge and skills. Despite much being written about data quality in general, a generalisable framework for analysing event data quality issues when extracting logs for process mining remains unrealised. Following the DSR paradigm, we present RDB2Log, a quality-aware, semi-automated approach for extracting event logs from relational data. We validated RDB2Log's design against design objectives extracted from literature and competing artifacts, evaluated its design and performance with process mining experts, implemented a prototype with a defined set of quality metrics, and applied it in laboratory settings and in a real-world case study. The evaluation shows that RDB2Log is understandable, of relevance in current research, and supports process mining in practice.																	0167-9236	1873-5797				MAY	2020	132								113265	10.1016/j.dss.2020.113265													
J								Continuous content contribution in virtual community: The role of status-standing on motivational mechanisms	DECISION SUPPORT SYSTEMS										Virtual community; Status-standing; Motivational mechanism; Self-interest motivation; Prosocial motivation	CONTRIBUTION BEHAVIOR; ONLINE COMMUNITIES; SELF-INTEREST; KNOWLEDGE; HIERARCHIES; DISTINCTION; DYNAMICS; CUSTOMER; SYSTEMS; MODEL	A central concern for the long-term vibrancy of virtual communities is how to effectively motivate continuous contributions from community members. To address this challenge, a series of motivation factors that range from self-interest driven to prosocial-oriented motives have been identified by scholars. As members participate in continuous contributions and community activities alike, their status or positions within the virtual communities would change, which may influence their cognition and behavior. While status-seeking can lead to greater contributions and a better performance, research has yet to study in a comprehensive manner how an individual's status-standing shape one's recognition and behavior in a systematic way. To fill this research gap, a comprehensive theoretical model based on the status theory of collective action is proposed to explain how status standing moderates the impact of different types of motivations (self-interest vs. prosocial) on continuous contributions in virtual communities. Our findings suggest that improved status-standing can enhance the motivational incentives of virtual rewards and peer recognition, while boosting the prosocial motivation of opinion leadership to contribute content in virtual communities. Overall, this study not only suggests a comprehensive view of our current understandings regarding contribution behavior in virtual communities, but also yields deep insights into how motivational mechanism of virtual communities can be reasonably designed and properly used to promote continuous contributions.																	0167-9236	1873-5797				MAY	2020	132								113283	10.1016/j.dss.2020.113283													
J								Secure attribute-based search in RFID-based inventory control systems	DECISION SUPPORT SYSTEMS										RFID secure search; Attribute-based search; EPC C1G2 passive tags; Security protocols; Formal verification	PROTOCOL; AUTHENTICATION; IDENTIFICATION; GENERATOR; PRIVACY	We develop a secure attribute-based search protocol for Radio Frequency Identification (RFID) systems. This protocol can be used to simultaneously identify groups of items that share a set of attribute values. To the best of our knowledge, this is the first such work with the potential to significantly enhance the security and intelligence of RFID-enabled applications in inventory control and supply chain management. The protocol is designed to be lightweight, suited for resource-constrained basic passive tags, and compliant with the Electronic Product Code (EPC) standards. This is achieved by exploiting the zero knowledge properties of quadratic residues. The security and privacy properties offered by the protocol are rigorously proven through formal verification.																	0167-9236	1873-5797				MAY	2020	132								113270	10.1016/j.dss.2020.113270													
J								Facilitating like Darwin: Supporting cross-fertilisation in crowdsourcing	DECISION SUPPORT SYSTEMS										Cross-fertilisation; Crowdsolving; Facilitation; Collective intelligence; Design science	DESIGN SCIENCE RESEARCH; INFORMATION-SYSTEMS; OPEN INNOVATION; DIVERSITY; PARTICIPATION; PERFORMANCE; CHALLENGES; MANAGEMENT; BUSINESS; CRITERIA	Humankind faces many "wicked" decision-making problems, which must be solved. One promising approach refers to crowdsourcing systems that hold the potential to solve any kind of problem - notably wicked ones. Crowdsourced solutions work well because crowds exchange knowledge from different domains - a concept known as "cross-fertilisation." Thereby, the "facilitator" of a crowdsourcing system is the primary decision maker when it comes to specifying and managing the crowd. The facilitator's role includes actively managing cross-fertilisation. However, in the light of technological advancements and large-scale data, facilitation proves difficult - especially in one particular type of crowdsourcing - crowdsolving. Thus, academia recently called for relieving some burden of facilitators and started developing tools for supporting or automated facilitation. Yet, the focus of existing tools is not on fostering the innermost core of crowdsolving endeavours - cross-fertilisation. By taking a design science perspective, we propose design principles and design guidelines for a decision-support tool aiding facilitators to (a) set the boundary conditions for, (b) measure, and (c) facilitate cross-fertilisation. We evaluate feasibility and value added of the abstract design by applying it to different crowdsolving platforms including a prototypical implementation and qualitative evaluation by facilitators.																	0167-9236	1873-5797				MAY	2020	132								113282	10.1016/j.dss.2020.113282													
J								An empirical examination of voluntary profiling: Privacy and quid pro quo	DECISION SUPPORT SYSTEMS										Information disclosing; Coupon redemption; Privacy calculus; Rx. cosmetics; Voluntary profiling; Customer data analytics	INFORMATION PRIVACY; E-COMMERCE; CALCULUS MODEL; PERSONAL INFORMATION; CONSUMER PRIVACY; COUPON PRONENESS; DISCLOSURE; ONLINE; WILLINGNESS; SERVICES	Evidence suggests that firms which use customer data analytics perform better than those that do not. However, the current policy of voluntary profiling allows firms to collect and use customer information only if customers voluntarily disclose information with them. Further, surveys and literature show that many customers are not comfortable with firms collecting their information due to privacy concerns. A vast literature has examined customer information disclosing behavior using the privacy calculus. The primary premise of the privacy calculus is that despite strong privacy concerns, customers disclose information if the benefits they can get from disclosure justify the costs of losing privacy, or privacy costs. Based on the privacy calculus, firms and marketers believe that customers voluntarily disclose information in exchange for monetary benefits such as discount coupons or cash rewards. Using transaction data that we collected from a firm that sells skin care cosmetic products on its website, we investigate if there is statistical evidence that shows customers disclose information in exchange for monetary benefits. In line with the privacy calculus, we find that customers with low privacy costs and that expect high benefits from personalized services such as product recommendations are more likely to disclose information. Monetary incentives only work as an effective means to elicit information from customers in the age range of 12-21 years old. Customers, on average, and especially customers in the age range of 22-54 years old are not likely to disclose information simply as a tradeoff for monetary benefits. Personalized services outweigh monetary benefits in enticing these customers to disclose information.																	0167-9236	1873-5797				MAY	2020	132								113285	10.1016/j.dss.2020.113285													
J								mHealth App recommendation based on the prediction of suitable behavior change techniques	DECISION SUPPORT SYSTEMS										Multi-source data; mHealth App; App recommendation; Behavior change techniques	SOCIAL SUPPORT; PHYSICAL-ACTIVITY; MOBILE APPS; PREFERENCES; GENDER; PERSONALIZATION; IMPLEMENTATION	In light of individuals' increasing concern regarding their physical health, mobile health applications (mHealth Apps) have gained popularity in recent years as important tools for addressing health problems. However, users find it challenging to choose appropriate mHealth Apps, as these Apps incorporate diverse behavior change techniques (BCTs), and their individual behavioral intervention effects on users vary. This study proposes a novel BCT-based mHealth App recommendation method to suggest suitable mHealth Apps to users. Specifically, we encode mHealth Apps to obtain information on the BCT adopted by the Apps. Based on the combination of BCTs in each mHealth App and its usage information, we construct a User-BCT matrix to represent users' preferences concerning BCTs. We also construct a user profile for each user, which considers their characteristics related to BCTs. Next, we build a prediction model that links each user's profile to BCTs, and use the AdaBoost algorithm to predict suitable BCTs for a target user. Finally, we recommend mHealth Apps with the highest BCT-matching levels to a target user. We also investigate the performance of the proposed method using a real dataset. The experimental results demonstrate the advantages of the proposed method.																	0167-9236	1873-5797				MAY	2020	132								113248	10.1016/j.dss.2020.113248													
J								Longitudinal healthcare analytics for disease management: Empirical demonstration for low back pain	DECISION SUPPORT SYSTEMS										Healthcare analytics; Disease management; Longitudinal monitoring; Time series analysis; Cohort data; Low back pain	DECISION-SUPPORT-SYSTEM; LATENT CLASS ANALYSIS; PREDICTION INTERVALS; UNIT-ROOT; RECOVERY; TESTS	Clinician guidelines recommend health management to tailor the form of care to the expected course of diseases. Hence, in order to decide upon a suitable treatment plan, health professionals benefit from decision support, i.e., predictions about how a disease is to evolve. In clinical practice, such a prediction model requires interpretability. Interpretability, however, is often precluded by complex dynamic models that would be capable of capturing the intrapersonal variability of disease trajectories. Therefore, we develop a cross-sectional ARMA model that allows for inference of the expected course of symptoms. Distinct from traditional time series models, it generalizes to cross-sectional settings and thus patient cohorts (i.e., it is estimated to multiple instead of single disease trajectories). Our model is evaluated according to a longitudinal 52-week study involving 928 patients with low back pain. It achieves a favorable prediction performance while maintaining interpretability. In sum, we provide decision support by informing health professionals about whether symptoms will have the tendency to stabilize or continue to be severe.																	0167-9236	1873-5797				MAY	2020	132								113271	10.1016/j.dss.2020.113271													
J								Stratifying no-show patients into multiple risk groups via a holistic data analytics-based framework	DECISION SUPPORT SYSTEMS										Data mining; Healthcare informatics; Medical decision making; Patient no-shows	MISSED APPOINTMENTS; BROKEN APPOINTMENTS; SMS REMINDERS; ATTENDANCE; URBAN; MODEL; ALGORITHM; DISEASE; SYSTEMS; SERVICE	Accurate prediction of no-show patients plays a crucial role as it enables researchers to increase the efficiency of their scheduling systems. The purpose of the current study is to formulate a novel hybrid data mining-based methodology to a) accurately predict the no-show patients, b) build a parsimonious model by employing a comprehensive variable selection procedure, c) build a model that does not suffer due to data imbalance, and d) provide healthcare agencies with a patient-specific risk level. Our study suggests that an Artificial Neural Network (ANN) model should be employed as a classification algorithm in predicting patient no-shows by using the variable set that is commonly selected by a Genetic Algorithm (GA) and Simulated Annealing (SA). In addition, we used Random Under Sampling (RUS) to improve the performance of the model in predicting the minority group (no-show) patients. The patient-specific risk scores were justified by applying a threshold sensitivity analysis. Also, the web-based decision support tool that can be adopted by clinics is developed. The clinics can incorporate their own intuition/incentive to make the final decision on the cases where the model is not confident enough (i.e. when the estimated probabilities fall near the decision boundary). These insights enable health care professionals to improve clinic utilization and patient outcomes.																	0167-9236	1873-5797				MAY	2020	132								113269	10.1016/j.dss.2020.113269													
J								Incremental updating probabilistic neighborhood three-way regions with time-evolving attributes	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Incremental learning; Neighborhood decision systems; Matrix; Time-evolving attributes	THEORETIC ROUGH SET; DYNAMIC DATA; FEATURE-SELECTION; DECISION-MAKING; REDUCTION; APPROXIMATIONS; MODEL; KNOWLEDGE; SYSTEMS	Neighborhood decision systems (NDSs) are commonly-used systems in real-world applications such as credit scoring. Probabilistic neighborhood rough sets (PNRSs) model, which is a generalized rough set model by combining neighborhood rough sets (NRSs) with probabilistic theory, can efficiently handle numerical data with noise values and allow tolerance of errors. Approximations of a target concept are fundamental and vital notions of the PNRSs, which can induce the certain and uncertain decision rules. Nevertheless, data evolves over time due to dynamic characteristics, especially, the addition of new attributes and the deletion of redundant or irrelevant attributes. Therefore, the potential meaningful knowledge may alter over time accordingly. To improve the efficiency of acquiring decision rules, the three-way regions (i.e., the positive, negative, and boundary regions) of the PNRSs need to be updated in an incremental fashion. To address this issue, two incremental algorithms for updating the three-way regions of each decision class of the PNRSs are investigated from the matrix perspective with time-evolving attributes (namely, adding attributes and deleting attributes). First, a matrix-based characterization of the three-way regions of each decision class of the PNRSs is constructed. Subsequently, considering the variation of the attributes, matrix-based updating mechanisms for relevant matrices are developed by means of the previously calculated information. Furthermore, the incremental algorithms are developed by taking advantage of the matrix-based updating mechanisms. Comprehensive experiments are performed on benchmark UCI data sets, and comparative results demonstrate our proposed algorithms consistently outperform the non-incremental algorithm in terms of computational efficiency. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				MAY	2020	120						1	23		10.1016/j.ijar.2020.01.015													
J								A short note on decomposition and composition of knowledge	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Knowledge representation; Decomposition; Conditional independence; Conditional knowledge; Associativity of partial knowledge	MODELS	Decomposition and subsequent composition of objects help us to better understand and handle objects, especially in the case when the object in question is too big or complex. This general statement is fully valid with reference to human knowledge. When passing knowledge on to students or colleagues (whether orally or in writing), there is no other possibility than to do it piece by piece. This paper contributes to a better understanding of the situations when knowledge can be decomposed. Using an abstract mathematical apparatus, we reveal what a suitable decomposition of knowledge is and suggest axioms under which the knowledge can be reconstructed from its pieces. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				MAY	2020	120						24	32		10.1016/j.ijar.2020.01.014													
J								Non-parametric learning of lifted Restricted Boltzmann Machines	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Restricted Boltzmann Machines; Learning lifted models; Functional gradient boosting		We consider the problem of discriminatively learning Restricted Boltzmann Machines in the presence of relational data. Unlike previous approaches that employ a rule learner (for structure learning) and a weight learner (for parameter learning) sequentially, we develop a gradient-boosted approach that performs both simultaneously. Our approach learns a set of weak relational regression trees, whose paths from root to leaf are conjunctive clauses and represent the structure, and whose leaf values represent the parameters. When the learned relational regression trees are transformed into a lifted RBM, its hidden nodes are precisely the conjunctive clauses derived from the relational regression trees. This leads to a more interpretable and explainable model. Our empirical evaluations clearly demonstrate this aspect, while displaying no loss in effectiveness of the learned models. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				MAY	2020	120						33	47		10.1016/j.ijar.2020.01.003													
J								Learning directed acyclic graph SPNs in sub-quadratic time	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Sum-product networks; Probabilistic graphical models; Directed acyclic graph; Structure learning; Bayesian networks	RESTRICTED ISOMETRY PROPERTY; ALGORITHM	In this paper, we present Prometheus, a graph partitioning based algorithm that creates multiple variable decompositions efficiently for learning Sum-Product Network structures across both continuous and discrete domains. Prometheus proceeds by creating multiple candidate decompositions that are represented compactly with an acyclic directed graph in which common parts of different decompositions are shared. It eliminates the correlation threshold hyperparameter often used in other structure learning techniques, allowing Prometheus to learn structures that are robust in low data regimes. Prometheus outperforms other structure learning techniques in 30 discrete and continuous domains. We also extend Prometheus to exploit sparsity in correlations between features in order to obtain an efficient sub-quadratic algorithm (w.r.t. the number of features) that scales better to high dimensional datasets. (C) 2020 The Authors. Published by Elsevier Inc.																	0888-613X	1873-4731				MAY	2020	120						48	73		10.1016/j.ijar.2020.01.005													
J								A binary water wave optimization for feature selection	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Classification; Feature selection; Metaheuristics; Rough set theory; Water wave optimization; Wrapper approaches	PARTICLE SWARM OPTIMIZATION; ROUGH SET APPROACH; ATTRIBUTE REDUCTION; INTELLIGENCE; ALGORITHM; NETWORKS; SEARCH	A search method that finds a minimal subset of features (over a feature space) that yields maximum classification accuracy is proposed. This method employs rough set theory (RST) along with a newly introduced binary version of the water wave optimization approach (WWO) which is denoted by BWWO. WWO simulates the phenomena of water waves, such as propagation, refraction, and breaking and is one of the newest nature inspired methods for global optimization problems. In our approach, BWWO utilizes the phenomena of water waves propagation, refraction, and breaking in a binary version. Two main experiments based on the rough set approach and wrapper method as a part of the objective function are carried out to verify the performance of the proposed algorithm. In the first experiment, the effectiveness of the proposed approach based on RST is demonstrated on 16 different datasets. The proposed approach is compared with various typical attribute reduction methods and popular optimizers in the literature, such as ant colony, nonlinear great deluge algorithm, scatter search and others. For the second experiment, a feature subset that maximizes the classification accuracy (using cross-validated kNN classifier) with minimizing the number of selected features is obtained over 17 different datasets. In wrapper experiment BWWO is compared with the binary gray wolf optimization, binary particle swarm optimizer, binary cat swarm optimization, binary dragonfly algorithm and the binary bat algorithm. The computational results demonstrate the efficiency and effectiveness of the proposed approach in finding a minimal features subset that maximize the classification accuracy. Furthermore, Friedman test and Wilcoxon's rank-sum test are carried out at 5% significance level in this study. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				MAY	2020	120						74	91		10.1016/j.ijar.2020.01.012													
J								Monotonicity of the system function of a SISO FRI system with neutrality and ordering property preserving fuzzy implications	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Fuzzy relational inference system; Fuzzy implication; Neutrality property; Ordering property; Monotone rule base; Monotonicity of the system function	BANDLER-KOHOUT SUBPRODUCT; MANY-VALUED IMPLICATIONS; MAMDANI-ASSILIAN MODELS; APPROXIMATION-THEORY	The monotonicity of the system function of a fuzzy relational inference (FRI) system is one of the most important issues that need attention. The unavailability of monotonicity of the system function of an FRI system is not desirable in many practical settings. Works existing in the literature have dealt with monotonicity of the system function of an FRI system where fuzzy implications have been widely used and those fuzzy implications are known to come from a residuated lattice structure. Moreover, there are very few studies where monotonicity has been studied under a non-residuated lattice structure. Building on our recent work on monotonicity under a non-residuated setting, in this work, we ensure that the system function of a certain type of FRI system with a Single-Input Single-Output (SISO) implicative rule base and ordering property preserving fuzzy implications is monotonic. Towards doing this, we represent the monotone fuzzy rule base by a fuzzy relation that involves fuzzy implications which satisfy the so-called neutrality property and ordering property, simultaneously. It should be highlighted that this work generalizes the class of fuzzy implications that can be employed in an FRI system without modifying the rule base to ensure the monotonic nature of the system function, giving more choices to the practitioners. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				MAY	2020	120						92	101		10.1016/j.ijar.2020.02.001													
J								Mean-entropy-based shadowed sets: A novel three-way approximation of fuzzy sets	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Fuzzy sets; Shadowed sets; Three-way approximations; Fuzzy entropy; Uncertainty	DECISION; IDENTIFICATION; FRAMEWORK; REGIONS; SYSTEM	Shadowed set provide a three-way approximation scheme for transforming a fuzzy set into three disjoint areas (elevated, reduced, and shadow areas). A fundamental issue in the construction of shadowed sets is the interpretation and determination of a pair of thresholds (alpha, beta). Several extended shadowed set models have been proposed to calculate (alpha, beta). However, the construction of a few of these models may have a large fuzzy entropy loss, and the determination of (alpha, beta) involves artificial subjective parameters. Therefore, in this study, a novel shadowed set model is proposed, namely, mean-entropy-based shadowed sets (MESS). At first, based on the principle of uncertainty invariance, a novel framework of three-way approximations of fuzzy sets is proposed based on the mean of fuzzy entropy. Secondly, new decision rules are generatedbased on the fuzzy entropy loss, and (alpha, beta) is obtained. Thirdly, the MESS model is optimized more reasonably usingan iterative method, and thus the fuzzy entropy loss of the MESS model can be minimized. Finally, the validity and rationality of the proposed model are verified by instances and experimental analysis. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				MAY	2020	120						102	124		10.1016/j.ijar.2020.02.006													
J								A heuristic representation learning based on evidential memberships: Case study of UCI-SPECTF	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Machine learning (ML); Granules; Information aggregation (IA); Heuristic representation learning (HRL); Evidential membership (EM)	FEATURE-EXTRACTION; DECISION; CLASSIFICATION; INFORMATION; ALGORITHM	The diagnosed features (samples) with multiple attributes of medical images always demand experts to reveal insight. Up to today, machine learning often cannot be a helpful expert. The reason lies in lacking evidential granules carrying knowledge and evidence for inferential learning. The shortage slows down representation learning which aims at discovering expressions for featuring concepts. Therefore, this paper proposes evidential memberships carrying preferential relevance to build a heuristic representation learning. Empirically, it solves local features and global representations with maximum coverage under challenges of shallow bury. For illustration, it is implemented on the testing data set of UCI-SPECTF. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				MAY	2020	120						125	137		10.1016/j.ijar.2020.02.002													
J								Topological operators of MW-topological rough approximations	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Topological operator; Locally finite covering approximation space; LFC-rough set; MW-topological rough set; Digital topological rough set; LFC-system	NEIGHBORHOOD OPERATORS; SETS; DIGITIZATION; CONTINUITY; SPACES	The present paper focuses on the studies of the topological (interior and closure) operators of the M-rough set structure (M-*, M*) and the Marcus-Wyse (MW-, for brevity) topological rough set structure (D-M(-), D-M(+)). While the concept approximations (M-*, M*) are topological operators, it is well known that the function D-M(-) is not an interior operator and D-M(+) is not a closure one either. Hence an issue can be posed as follows: What condition makes D-M(-) (resp. D-M(+)) an interior (resp. a closure) operator? This paper proposes a new frame addressing the issue, the so-called (Delta(-)(M), Delta(+)(M)) instead. This new version smoothly matches with (M-*, M*) w.r.t. the topological operators. Furthermore, both (M*, M*) and (Delta(-)(M), Delta(+)(M)) are shown to have many theoretical and mathematical properties in common. Therefore, they can efficiently be used in applied sciences with a strong combination. Besides, they can support certain decision rules without any limitations of studying something continuous or discrete (or digital) from the viewpoint of covering rough set theory. In this paper each of a universe U and a target set X of U need not be finite, and a covering Cis locally finite. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				MAY	2020	120						138	150		10.1016/j.ijar.2020.02.004													
J								Three-way decision with incomplete information based on similarity and satisfiability	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Three-way decision; Rough set; Incomplete information; Similarity; Satisfiability; Fuzzy logic	ATTRIBUTE REDUCTION; ROUGH SETS; FUZZY; SUBSETHOOD; APPROXIMATIONS; CONSTRUCTION; DEFINITION; INCLUSION; FRAMEWORK; SPACE	Three-way decision is widely applied with rough set theory to learn classification or decision rules. The approaches dealing with complete information are well established in the literature, including the two complementary computational and conceptual formulations. The computational formulation uses equivalence relations, and the conceptual formulation uses satisfiability of logic formulas. In this paper, based on a brief review of these two formulations, we generalize both formulations into three-way decision with incomplete information that is more practical in real-world applications. For the computational formulation, we propose a new measure of similarity degree of objects as a generalization of equivalence relations. Based on it, we discuss two approaches to three-way decision using alpha-similarity classes and approximability of objects, respectively. For the conceptual formulation, we propose a measure of satisfiability degree of formulas as a quantitative generalization of satisfiability with complete information. Based on it, we study two approaches to three-way decision using alpha-meaning sets of formulas and confidence of formulas, respectively. While using similarity classes is a common method of analyzing incomplete information in the literature, the proposed concept of approximability and the two approaches in conceptual formulation point out new promising directions. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				MAY	2020	120						151	183		10.1016/j.ijar.2020.02.005													
J								A model of three-way decisions for Knowledge Harnessing	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Three-way decisions; Knowledge-based systems; Variable forgetting; Knowledge Harnessing	ROUGH SET; ATTRIBUTE REDUCTION; SELECTION; OPTIMIZATION	The present work introduces the Knowledge Harnessing, by showing its theoretical foundations as well as a three-way decision model to deal with it. The problem poses how to extract valid information about a specific context from conflicting or uncertain information received by a system (or agent). With this aim, forgetting variable operators are used to both characterize the problem from the logical point of view and provide a theoretical solution as an acceptance-rejection problem. Since the formalization is semantic in nature (it considers the models of the knowledge base that admit the extracted knowledge), general bounds are provided for acceptance-rejection evaluation on boundary region. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				MAY	2020	120						184	202		10.1016/j.ijar.2020.02.010													
J								Attribute implications in L-concept analysis with positive and negative attributes: Validity and properties of models	INTERNATIONAL JOURNAL OF APPROXIMATE REASONING										Formal concept analysis; Negative information; Attribute implications; Fuzzy logic	CONCEPT LATTICES; 3-WAY; DEPENDENCIES	We further develop our study of the L-fuzzy setting of Formal Concept Analysis with positive and negative attributes (affirmations and denials). While concept lattices in this setting are well described in our previous works, in the present paper we explore the attribute implications; which we call fuzzy containment implications. Among the main results established in the present paper are: results regarding validity of fuzzy containment implications and their models; relationship with previously studied fuzzy attribute implications; definition and properties of semantic entailment. (C) 2020 Elsevier Inc. All rights reserved.																	0888-613X	1873-4731				MAY	2020	120						203	215		10.1016/j.ijar.2020.02.009													
J								Evaluating graph resilience with tensor stack networks: a Keras implementation	NEURAL COMPUTING & APPLICATIONS										Tensor stack network; Tensor algebra; Deep learning; Big data; Higher-order data; Graph mining; Graph resilience; Estrada index; Clustering coefficient; Multilinear classification; Sparsification; Regularization; TensorFlow; Keras	NEURAL-NETWORKS	In communication networks resilience or structural coherency, namely the ability to maintain total connectivity even after some data links are lost for an indefinite time, is a major design consideration. Evaluating resilience is a computationally challenging task since it often requires examining a prohibitively high number of connections or of node combinations, depending on the structural coherency definition. In order to study resilience, communication systems are treated in an abstract level as graphs where the existence of an edge depends heavily on the local connectivity properties between the two nodes. Once the graph is derived, its resilience is evaluated by a tensor stack network (TSN). TSN is an emerging deep learning classification methodology for big data which can be expressed either as stacked vectors or as matrices, such as images or oversampled data from multiple-input and multiple-output digital communication systems. As their collective name suggests, the architecture of TSNs is based on tensors, namely higher-dimensional vectors, which simulate the simultaneous training of a cluster of ordinary multilayer feedforward neural networks (FFNNs). In the TSN structure the FFNNs are also interconnected and, thus, at certain steps of the training process they learn from the errors of each other. An additional advantage of the TSN training process is that it is regularized, resulting in parsimonious classifiers. The TSNs are trained to evaluate how resilient a graph is, where the real structural strength is assessed through three established resiliency metrics, namely the Estrada index, the odd Estrada index, and the clustering coefficient. Although the approach of modelling the communication system exclusively in structural terms is function oblivious, it can be applied to virtually any type of communication network independently of the underlying technology. The classification achieved by four configurations of TSNs is evaluated through six metrics, including the F1 metric as well as the type I and type II errors, derived from the corresponding contingency tables. Moreover, the effects of sparsifying the synaptic weights resulting from the training process are explored for various thresholds. Results indicate that the proposed method achieves a very high accuracy, while it is considerably faster than the computation of each of the three resilience metrics. Concerning sparsification, after a threshold the accuracy drops, meaning that the TSNs cannot be further sparsified. Thus, their training is very efficient in that respect.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4161	4176		10.1007/s00521-020-04790-1													
J								An improved weight-constrained neural network training algorithm	NEURAL COMPUTING & APPLICATIONS										Artificial neural networks; Constrained optimization; L-BFGS; Scaling factor		In this work, we propose an improved weight-constrained neural network training algorithm, named iWCNN. The proposed algorithm exploits the numerical efficiency of the L-BFGS matrices together with a gradient-projection strategy for handling the bounds on the weights. Additionally, an attractive property of iWCNN is that it utilizes a new scaling factor for defining the initial Hessian approximation used in the L-BFGS formula. Since the L-BFGS Hessian approximation is defined utilizing a small number of correction vector pairs, our motivation is to further exploit them in order to increase the efficiency of the training algorithm and the convergence rate of the minimization process. The preliminary numerical experiments provide empirical evidence that the proposed training algorithm accelerates the training process.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4177	4185		10.1007/s00521-019-04342-2													
J								Genetic and deep learning clusters based on neural networks for management decision structures	NEURAL COMPUTING & APPLICATIONS										Genetic learning; Deep learning clusters; Reinforcement learning; Random neural network; Fintech; Smart investment	PREDICTION; ALGORITHMS; PERFORMANCE; REGRESSION; FRAMEWORK; MACHINE; SEARCH	Judgments are taken in a structured way; both human and business management decisions involve a hierarchical process that requires a level of compromise between risk, cost, reward, experience and knowledge. This article proposes a management decision structure that emulates the human brain approach based on genetic and deep learning cluster algorithms and the random neural network. Reinforcement learning takes quick and specific local decisions, deep learning clusters enables identity and memory, and deep learning management clusters make final strategic decisions. The presented genetic algorithm transmits the learned information to future generations in the network weights rather than the neurons. Because the subject's information, a combination of memory, identity and decision data, is never lost but transmitted, the genetic algorithm provides immortality. The management decision structure has been applied and validated in a smart investment Fintech application: an intelligent banker that makes buy and sell asset decisions with an associated market and risk that entirely transmits itself to a future generation. Results are rewarding; the management decision structure with genetics and machine learning based on the random neural network algorithm that emulates the human brain and biology transmits information to future generations and learns autonomously, gradually and continuously while adapting to the environment.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4187	4211		10.1007/s00521-019-04231-8													
J								Features extraction from human eye movements via echo state network	NEURAL COMPUTING & APPLICATIONS										Echo state network; Time series; Classification; Eye tracking	CLASSIFICATION	The paper develops a procedure for features extraction from eye movement's time series aimed at age-related classification of humans. It exploits the properties of the echo state network (ESN) reservoir state achieved after its intrinsic plasticity tuning. A novel, recently proposed approach for ranking of dynamic data series using as single feature the length of the reservoir state vector reached after consecutive feeding of each time series into the ESN was investigated in details using eye tracker recordings of human eye movements during visual stimulation and decision-making process. Inclusion of other features like variance of ESN extracted feature for multiple similar stimulations as well as decision correctness allowed for better classification of test subjects. The results support the view that the metrics and dynamics of the eye movements depend little on age, though they are strongly related to the visual stimulation characteristics.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4213	4226		10.1007/s00521-019-04329-z													
J								Continuous drone control using deep reinforcement learning for frontal view person shooting	NEURAL COMPUTING & APPLICATIONS										Deep reinforcement learning; Continuous control; Aerial cinematography; Drone control		Drones, also known as unmanned aerial vehicles, can be used to aid various aerial cinematography tasks. However, using drones for aerial cinematography requires the coordination of several people, increasing the cost and reducing the shooting flexibility, while also increasing the cognitive load of the drone operators. To overcome these limitations, we propose a deep reinforcement learning (RL) method for continuous fine-grained drone control, that allows for acquiring high-quality frontal view person shots. To this end, a head pose image dataset is combined with 3D models and face alignment/warping techniques to develop an RL environment that realistically simulates the effects of the drone control commands. An appropriate reward-shaping approach is also proposed to improve the stability of the employed continuous RL method. Apart from performing continuous control, it was demonstrated that the proposed method can be also effectively combined with simulation environments that support only discrete control commands, improving the control accuracy, even in this case. The effectiveness of the proposed technique is experimentally demonstrated using several quantitative and qualitative experiments.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4227	4238		10.1007/s00521-019-04330-6													
J								Spam detection on social networks using cost-sensitive feature selection and ensemble-based regularized deep neural networks	NEURAL COMPUTING & APPLICATIONS										Neural network; Social networks; Regularization; Ensemble learning; Misclassification cost	DETECTION SYSTEM; ACCOUNTS	Spam detection on social networks is increasingly important owing to the rapid growth of social network user base. Sophisticated spam filters must be developed to deal with this complex problem. Traditional machine learning approaches such as neural networks, support vector machines and Naive Bayes classifiers are not effective enough to process and utilize complex features present in high-dimensional data on social network spam. Moreover, the traditional objective criteria of social network spam filters cannot cope with different costs assigned to type I and type II errors. To overcome these problems, here we propose a novel cost-sensitive approach to social network spam filtering. The proposed approach is composed of two stages. In the first stage, multi-objective evolutionary feature selection is used to minimize both the misclassification cost of the proposed model and the number of attributes necessary for spam filtering. Then, the approach uses cost-sensitive ensemble learning techniques with regularized deep neural networks as base learners. We demonstrate that this approach is effective for social network spam filtering on two benchmark datasets. We also show that the proposed approach outperforms other popular algorithms used in social network spam filtering, such as random forest, Naive Bayes or support vector machines.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4239	4257		10.1007/s00521-019-04331-5													
J								Text synthesis from keywords: a comparison of recurrent-neural-network-based architectures and hybrid approaches	NEURAL COMPUTING & APPLICATIONS										Deep machine learning; Sequence modeling; Natural language processing; Text mining	SHORT-TERM-MEMORY; AUTOMATIC EXTRACTION; GENERATION; RECOGNITION; DOMAIN; MODEL	This paper concerns an application of recurrent neural networks to text synthesis in the word level, with the help of keywords. First, a Parts Of Speech tagging library is employed to extract verbs and nouns from the texts used in our work, a part of which are then considered, after automatic eliminations, as the aforementioned keywords. Our ultimate aim is to train a recurrent neural network to map the keyword sequence of a text to the entire text. Successive reformulations of the keyword and full-text word sequences are performed, so that they can serve as the input and target of the network as efficiently as possible. The predicted texts are understandable enough, and the model performance depends on the problem difficulty, determined by the percentage of full-text words that are considered as keywords, that ranges from 1/3 to 1/2 approximately, the training memory cost, mainly affected by the network architecture, as well as the similarity between different texts, which determines the best architecture.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4259	4274		10.1007/s00521-019-04435-y													
J								Deep Bayesian Self-Training	NEURAL COMPUTING & APPLICATIONS										Bayesian CNN; Variational inference; Self-training; Uncertainty weighting; Deep learning; Clustering; Representation learning; Adaptation		Supervised deep learning has been highly successful in recent years, achieving state-of-the-art results in most tasks. However, with the ongoing uptake of such methods in industrial applications, the requirement for large amounts of annotated data is often a challenge. In most real-world problems, manual annotation is practically intractable due to time/labour constraints; thus, the development of automated and adaptive data annotation systems is highly sought after. In this paper, we propose both a (1) deep Bayesian self-training methodology for automatic data annotation, by leveraging predictive uncertainty estimates using variational inference and modern neural network (NN) architectures, as well as (2) a practical adaptation procedure for handling high label variability between different dataset distributions through clustering of NN latent variable representations. An experimental study on both public and private datasets is presented illustrating the superior performance of the proposed approach over standard self-training baselines, highlighting the importance of predictive uncertainty estimates in safety-critical domains.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4275	4291		10.1007/s00521-019-04332-4													
J								Gryphon: a semi-supervised anomaly detection system based on one-class evolving spiking neural network	NEURAL COMPUTING & APPLICATIONS										Critical infrastructure; Industrial control systems; SCADA; Advanced persistent threat; Evolving spiking neural network; One-class classification; Anomaly detection; Semi-supervised learning	CLASSIFICATION	The backbone of the economy, security and sustainability of a state is inseparably linked to the security of its critical infrastructure. Critical infrastructures define goods, systems or subsystems that are essential to maintain the vital functions of society, health, physical protection, security plus economic and social well-being of citizens. The digital security of critical infrastructures is a very important priority for the well-being of every country, especially nowadays, because of the direct threats dictated by the current international conjuncture and due to the emerging interactions or interconnections developed between the National Critical Infrastructures, internationally. The aim of this research is the development and testing of an Anomaly Detection intelligent algorithm that has the advantage to run very fast with a small portion of the available data and to perform equally well with the existing approaches. Such a system must be characterized by high efficiency and very fast execution. Thus, we present the Gryphon advanced intelligence system. Gryphon is a Semi-Supervised Unary Anomaly Detection System for big industrial data which is employing an evolving Spiking Neural Network (eSNN) One-Class Classifier (eSNN-OCC). This machine learning algorithm corresponds to a model capable of detecting very fast and efficiently, divergent behaviors and abnormalities associated with cyberattacks, which are known as Advanced Persistent Threat (APT). The training process is performed on data related to the normal function of a critical infrastructure.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4303	4314		10.1007/s00521-019-04363-x													
J								An LBP encoding scheme jointly using quaternionic representation and angular information	NEURAL COMPUTING & APPLICATIONS										Quaternionic representation (QR); Local binary pattern (LBP); Quaternionic angular information; Image classification	TEXTURE CLASSIFICATION; COLOR; DESCRIPTOR; FEATURES; SCALE	Local descriptors play a crucial role in numerous computer vision and pattern recognition applications. This paper proposes a novel local descriptor, called the quaternionic local angular binary pattern (QLABP), for color image classification. QLABP is based on the quaternionic representation (QR) of color images such that it is able to handle all color components holistically as well as consider their relations. Using QR, the quaternionic angular information is further developed to account for more color characteristics. We provide two ways to derive the quaternionic angular information from different perspectives. A pattern encoding operation is finally conducted on the obtained angular information to obtain QLABP. The effectiveness of QLABP has successfully been evaluated by comparing with several state-of-the-art descriptors.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4317	4323		10.1007/s00521-018-03968-y													
J								Toward cognitive support for automated defect detection	NEURAL COMPUTING & APPLICATIONS										Minimum ratio; Defect detection; Visual inspection; Cognitive automation	LEARNING-BASED APPROACH; INSPECTION; MACHINE; NETWORK; SYSTEM; SENSOR; MODEL	With the development of cognitive computing, machine learning techniques, and big data analytics, cognitive support is crucial for automated industrial production. The real-time automated visual inspection in industrial production is a challenging task. Speed and accuracy are crucial factors for the process of automating the defect detection. Many statistical and spectrum analysis approaches have been introduced; however, they suffer from high computational cost with average performance. This paper proposes a neighborhood-maintaining approach, which is based on the minimum ratio for fast and reliable inspection of industrial products. The minimum ratio between local neighborhood sliding windows is used as a similarity measure for localizing defection. Extreme learning machine is then adapted to classify surfaces to defect or normal. A defect detection accuracy on textile fabrics has achieved 98.07% with 91.29% sensitivity and 99.67% specificity. The minimum ratio shows highly discriminant power to distinguish between normal and abnormal surfaces. A defective region produces a smaller value of minimum ratio than that of a defect-free region. Experimental results show superior speed and accuracy performance over many existing defect detection methods.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4325	4333		10.1007/s00521-018-03969-x													
J								Distributed representation learning via node2vec for implicit feedback recommendation	NEURAL COMPUTING & APPLICATIONS										Representation learning; Implicit feedback; Recommender system; Deep learning	NEURAL-NETWORKS	As an important technology of Internet products, the recommender system can help users to obtain the information they need and alleviate the problem of information overload. In the implicit feedback recommender system, the key issue is how to represent users and products. In recent years, deep learning has achieved good performance in many fields including speech recognition, computer vision and natural language processing. We propose a deep learning-enhanced framework for implicit feedback recommendation. In this framework, we simultaneously learn the new distributed representation of users and items via node2vec to improve the negative sampling strategy. Finally, we develop a deep neural network recommendation model to integrate user features, product features and interaction features. Experiments conducted on two real-world datasets demonstrate the effectiveness of the proposed framework and methods.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4335	4345		10.1007/s00521-018-03964-2													
J								Multi-layer security scheme for implantable medical devices	NEURAL COMPUTING & APPLICATIONS										Multi-layer security; Deep learning; Medical devices; Authentication	ECG	Internet of Medical Things (IoMTs) is fast emerging, thereby fostering rapid advances in the areas of sensing, actuation and connectivity to significantly improve the quality and accessibility of health care for everyone. Implantable medical device (IMD) is an example of such an IoMT-enabled device. IMDs treat the patient's health and give a mechanism to provide regular remote monitoring to the healthcare providers. However, the current wireless communication channels can curb the security and privacy of these devices by allowing an attacker to interfere with both the data and communication. The privacy and security breaches in IMDs have thereby alarmed both the health providers and government agencies. Ensuring security of these small devices is a vital task to prevent severe health consequences to the bearer. The attacks can range from system to infrastructure levels where both the software and hardware of the IMD are compromised. In the recent years, biometric and cryptographic approaches to authentication, machine learning approaches to anomaly detection and external wearable devices for wireless communication protection have been proposed. However, the existing solutions for wireless medical devices are either heavy for memory constrained devices or require additional devices to be worn. To treat the present situation, there is a requirement to facilitate effective and secure data communication by introducing policies that will incentivize the development of security techniques. This paper proposes a novel electrocardiogram authentication scheme which uses Legendre approximation coupled with multi-layer perceptron model for providing three levels of security for data, network and application levels. The proposed model can reach up to 99.99% testing accuracy in identifying the authorized personnel even with 5 coefficients.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4347	4360		10.1007/s00521-018-3819-0													
J								Face image super-resolution with pose via nuclear norm regularized structural orthogonal Procrustes regression	NEURAL COMPUTING & APPLICATIONS										Face hallucination; Pose variations; Nuclear norm; Low-rank constraint	SPARSE REPRESENTATION; RECOGNITION; HALLUCINATION; FRAMEWORK	In real applications, the observed low-resolution face images usually have pose variations. Conventional learning-based methods ignore these variations; thus, the hallucinated high-resolution faces are not reasonable for the following recognition task. For recognition purpose, we prefer to obtain near-frontal faces. To this end, we propose a nuclear norm regularized structural orthogonal Procrustes regression (N2SOPR) approach in this work to acquire pose-robust feature representations for face hallucination with pose. The orthogonal Procrustes regression is used to seek an appropriate transformation between two data matrixes. Additionally, the nuclear norm regularization is imposed on the representation residual to preserve image structural property. We also impose a low-rank restraint on the combination weight to automatically cluster each input into the same subspace with the training samples. Both hallucination and recognition experiments conducted on common face databases have verified that our N2SOPR can obtain reasonable performance than some related methods.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4361	4371		10.1007/s00521-018-3826-1													
J								Lung cancer prediction using higher-order recurrent neural network based on glowworm swarm optimization	NEURAL COMPUTING & APPLICATIONS										Higher-order neural network; Swarm optimization; Lung cancer prediction; Recurrent neural network-glowworm swarm optimization	DIAGNOSIS; SMOKING	Therapeutic issues are commonly found in every single person. Tumor is a standout among the most unsafe sicknesses a human can ever had. It is exceptionally hard to distinguish it in its beginning times as its side effects seem just in the progressed stages. Subsequently, the early forecast of lung growth is compulsory for the analysis procedure, and it gives the higher possibilities for fruitful treatment. It is the most difficult approach to upgrade a patient's possibility for survival. Henceforth, a higher-order neural network system called recurrent neural network with Levenberg-Marquardt model with the help of glowworm swarm optimization algorithm is proposed for managing multimodal disease information. The execution of the proposed strategies is tried with information and the benchmark dataset, and the outcomes demonstrate that the higher-order recurrent neural systems with glowworm swarm optimization give better accuracy of 98% in comparison with customary optimized neural network.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4373	4386		10.1007/s00521-018-3824-3													
J								How textual quality of online reviews affect classification performance: a case of deep learning sentiment analysis	NEURAL COMPUTING & APPLICATIONS										Online review; Textual quality; Cognitive computing; Deep learning; Sentiment classification	BIG DATA; INFORMATION LOAD; PRODUCT TYPE; HELPFULNESS; CHALLENGES; FORMULA	Cognitive computing is an interdisciplinary research field that simulates human thought processes in a computerized model. One application for cognitive computing is sentiment analysis on online reviews, which reflects opinions and attitudes toward products and services experienced by consumers. A high level of classification performance facilitates decision making for both consumers and firms. However, while much effort has been made to propose advanced classification algorithms to improve the performance, the importance of the textual quality of the data has been ignored. This research explores the impact of two influential textual features, namely the word count and review readability, on the performance of sentiment classification. We apply three representative deep learning techniques, namely SRN, LSTM, and CNN, to sentiment analysis tasks on a benchmark movie reviews dataset. Multiple regression models are further employed for statistical analysis. Our findings show that the dataset with reviews having a short length and high readability could achieve the best performance compared with any other combinations of the levels of word count and readability and that controlling the review length is more effective for garnering a higher level of accuracy than increasing the readability. Based on these findings, a practical application, i.e., a text evaluator or a website plug-in for text evaluation, can be developed to provide a service of review editorials and quality control for crowd-sourced review websites. These findings greatly contribute to generating more valuable reviews with high textual quality to better serve sentiment analysis and decision making.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4387	4415		10.1007/s00521-018-3865-7													
J								Big data analytics for preventive medicine	NEURAL COMPUTING & APPLICATIONS										Disease prevention; Data analytics; Healthcare; Knowledge discovery; Prevention methodologies	DATA MINING TECHNIQUES; ASSOCIATION RULES; WEIGHT-LOSS; SEMINAL QUALITY; SUPPORT; PREDICTION; DISEASE; SYSTEM; DISCOVERY; DIAGNOSIS	Medical data is one of the most rewarding and yet most complicated data to analyze. How can healthcare providers use modern data analytics tools and technologies to analyze and create value from complex data? Data analytics, with its promise to efficiently discover valuable pattern by analyzing large amount of unstructured, heterogeneous, non-standard and incomplete healthcare data. It does not only forecast but also helps in decision making and is increasingly noticed as breakthrough in ongoing advancement with the goal is to improve the quality of patient care and reduces the healthcare cost. The aim of this study is to provide a comprehensive and structured overview of extensive research on the advancement of data analytics methods for disease prevention. This review first introduces disease prevention and its challenges followed by traditional prevention methodologies. We summarize state-of-the-art data analytics algorithms used for classification of disease, clustering (unusually high incidence of a particular disease), anomalies detection (detection of disease) and association as well as their respective advantages, drawbacks and guidelines for selection of specific model followed by discussion on recent development and successful application of disease prevention methods. The article concludes with open research challenges and recommendations.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4417	4451		10.1007/s00521-019-04095-y													
J								A pricing method of online group-buying for continuous price function	NEURAL COMPUTING & APPLICATIONS										Incentive compatibility; Group-buying; VCG; Pricing		Group-buying has become a popular commodity trading mode in current business modes. However, the existing unified price of group-buying often determines the price by setting the ladder function according to the final quantities. This method not only ignores the contributions of participants to group-buying, but also leads to the phenomenon of buyers' false reports. In this paper, a pricing method of online group-buying based on continuous price function is proposed. We adopt an algorithm called Vickrey-Clarke-Groves for group-buying; buyers' payments are the sum of commodities' price and the extra amount by purchase quantity. The mechanism motivates buyers to report truthful preference through the compensatory payment. We prove that the mechanism has economic attributes such as incentive compatibility through theoretical proof and simulation experiments.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4453	4461		10.1007/s00521-019-04017-y													
J								Intelligent equipment design assisted by Cognitive Internet of Things and industrial big data	NEURAL COMPUTING & APPLICATIONS										Cognitive Internet of Things; Industrial big data; Intelligent equipment		In recent years, the development of emerging technologies has brought about a new era of industrial reform. The current industrial revolution will deeply integrate the new generation of information technology with modern manufacturing industry and production servicing businesses to promote transformation and upgrading. As it is the foundation of the manufacturing industry, intelligent equipment plays an important role in the reform. In this paper, we propose an innovative design method to help design intelligent equipment. Firstly, referring to the architecture of the Cognitive Internet of Things (CIoT) and industrial big data, we proposed the architecture of the method and defined the different layers to process the data. Then, for the acquired external data, we put forward an algorithm which was combined with the technology of CIoT and industrial big data, to help designers analyze and make decisions. Finally, we verified the validity and feasibility of this method through a case study. The results showed that this method could effectively mine the deep information of intelligent equipment and provide more valuable information about design-assisting designers in designing better intelligent equipment.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4463	4472		10.1007/s00521-018-3725-5													
J								High-quality-guided artificial bee colony algorithm for designing loudspeaker	NEURAL COMPUTING & APPLICATIONS										Loudspeaker; Artificial bee colony; Convergence rate; Updating dimensions	DIFFERENTIAL EVOLUTION; GLOBAL OPTIMIZATION	Designing the loudspeaker could be concerned as an optimization problem. Like most electromagnetic device design issues, it demonstrates multimodal, multidimensional and constrained. The traditional design method cannot achieve a satisfactory model within a certain period of time. In this paper, a high-quality-guided artificial bee colony algorithm is proposed to increase its convergence speed and search accuracy by gradually changing the number of updating dimensions and searching closer to better locations. The algorithm is first tested on some representative basic benchmark functions and then is applied to a loudspeaker design problem. By comparing with some classical algorithms, the performance of our algorithm is verified.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4473	4480		10.1007/s00521-018-3568-0													
J								NMF with local constraint and Deep NMF with temporal dependencies constraint for action recognition	NEURAL COMPUTING & APPLICATIONS										Nonnegative matrix factorization; Action recognition; Feature extraction; Spatiotemporal features	NONNEGATIVE MATRIX FACTORIZATION; RECOGNIZING HUMAN ACTIONS; COMPONENT REPRESENTATION; UNIFIED FRAMEWORK; MOTION; DICTIONARY; CONTEXT; DESCRIPTORS; APPEARANCE; ALGORITHM	In order to improve action recognition accuracy, a new nonnegative matrix factorization with local constraint (LC-NMF) is firstly presented. By applying it for effective trajectory clustering, complex backgrounds are removed and then the motion salient regions are obtained. Secondly, a nonnegative matrix factorization with temporal dependencies constraint (TD-NMF) is proposed, which fully mines the spatiotemporal relationship in a video not only between adjacent frames, but also between multi-interval frames. Meanwhile, the introduction of l2,1-norm makes the spatiotemporal features possess better sparseness and robustness. In addition, these features are directly learned from data and thus have an inherent generalization ability. Finally, a Deep NMF method is established, which takes the proposed TD-NMF as the unit algorithm of each layer. By introducing the hierarchical feature extraction strategy, the base matrix of the first layer is gradually decomposed; then, it is supplemented and completed layer by layer. Consequently, the more complete and accurate local feature estimations are obtained, and then the discriminative and expressive abilities of features are effectively enhanced and recognition performance is further improved. Adequate and extensive experiments verify the effectiveness of the proposed methods. Moreover, the update rules and convergence proofs for LC-NMF and TD-NMF are also given.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4481	4505		10.1007/s00521-018-3685-9													
J								A model for collective behaviour propagation: a case study of video game industry	NEURAL COMPUTING & APPLICATIONS										Ordinary differential equations; Collective behaviour propagation; Optimization; Network; Video game industry	SYSTEMS; NETWORK; GROWTH	Many markets include a product and a platform product, where the product can only achieve its intended functions and performance in conjunction with or under the operation of its platform, such as a video game can only run on its game console. The growth of the user population of these products or services is a kind of collective behaviour propagation phenomenon. Here, questions come: how can we describe the collective behaviour propagation as a function of time? How the endogenous and exogenous social effects influence the collective behaviour propagation and how to quantify these two effects? In order to answer all these questions, an ordinary differential equation model is proposed to describe the growth of the user population of this class of markets. Firstly, a networked community is constructed, where users and prospective users are considered as nodes, and their relationship provides the method of building edges. Then, two fundamental influences of decision-making can be realized based on the network. A useful application of the model can be conceived and illustrated by one new database containing weekly sales of 25,237 video games released in the home and handle consoles and personal computer in USA, UK, Germany, France and Japan from 1989 to 2018. Results show that historical sales profile of a video game follows the growth equation, and the numerical procedure for finding the model parameters allows the market size, and the relative effectiveness of customer service and promotional efforts to be estimated according to the available historical data.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4507	4517		10.1007/s00521-018-3686-8													
J								ClothingOut: a category-supervised GAN model for clothing segmentation and retrieval	NEURAL COMPUTING & APPLICATIONS										Generative adversarial network; Clothing segmentation; Clothing retrieval; Clothing retrieval; Video advertising		This paper presents a new framework, ClothingOut, which utilizes generative adversarial network (GAN) to generate tiled clothing images automatically. Specifically, we design a novel category-supervised GAN model by learning transformation rules between clothes on wearers and clothes that are tiled. Our method features in adding category attribute to a traditional GAN model. For model training, we built a large-scale dataset containing over 20,000 pairs of wearer images and their corresponding tiled clothing images. The learned model can be straightforwardly applied to video advertising and cross-scenario clothing image retrieval. We evaluated our generated images which can be regarded as the segmentation from the wearer images from two aspects: authenticity and retrieval performance. Experimental results demonstrate the effectiveness of our method.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4519	4530		10.1007/s00521-018-3691-y													
J								Optimal design of fuzzy PID controller for deregulated LFC of multi-area power system via mine blast algorithm	NEURAL COMPUTING & APPLICATIONS										Load frequency control; Fuzzy logic control; Mine blast algorithm	AUTOMATIC-GENERATION CONTROL; HYBRID FIREFLY ALGORITHM; LOAD-FREQUENCY CONTROL; ROBUST CONTROLLER; AGC	This paper presents a novel optimal fuzzy proportional-integral-derivative (fuzzy PID) controller for load frequency control (LFC) designed by a proposed approach of mine blast algorithm (MBA) for multi-interconnected areas. The system includes reheat thermal connected power systems with the effect of the governor dead zone and turbine generation rate constraint nonlinearity. The proposed approach is used to determine the optimal parameters of the fuzzy PID controller to minimize the integral time absolute error. The proposed controller is inserted in multi-interconnected power systems which are built in Simulink/MATLAB library; triangular membership function is used for fuzzy PID controller. Additionally, the optimum adjustment of the fuzzy PID controller parameters under contracted scenario for large step demands and disturbances is investigated by MBA. The obtained results are compared to those obtained via antlion optimizer, artificial bee colony, hybrid differential evolution particle swarm optimization and hybrid PSO pattern search algorithm. The obtained results confirmed the superiority of the proposed MBA in designing the fuzzy PID-LFC as it provides less error with best statistical parameters compared to the others.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4531	4551		10.1007/s00521-018-3720-x													
J								Location selection of electric vehicles charging stations by using a fuzzy MCDM method: a case study in Turkey	NEURAL COMPUTING & APPLICATIONS										Electric vehicles charging stations; Location selection; Intuitionistic fuzzy sets; Decision making; DEMATEL; AHP; TOPSIS	CUSTOMER REQUIREMENTS; IMPORTANCE WEIGHTS; AHP; MODEL; INFRASTRUCTURE; DESIGN; SETS	Pollution, climate change, fast natural resource depletion, deforestation and global warming have become major worldwide problems relevant with the petroleum-based powered vehicles and alternatives for this conventional transportation type have been started to change in the last decade. In this modification process, electric vehicles (EVs) have a leading position due to their low damage effect to the environment. Selecting the most sustainable location for charging station for EVs plays an important role in the life cycle of them. This process needs to consider some conflicting criteria and has a complex decision problem that can be modeled as a multi-criteria decision-making problem. The inclusion of such criteria in a location selection requires the fuzzy sets to be used in the decision-making methodology. For this aim, intuitionistic fuzzy sets have been used in this paper. By the way, a decision-making procedure based on intuitionistic fuzzy sets and consists of the decision-making trial and evaluation laboratory, analytic hierarchy process and technique for order preference by similarity to ideal solution has been suggested for the location selection of charge stations. The proposed fuzzy-based model is applied to a case study for Istanbul in Turkey.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4553	4574		10.1007/s00521-018-3752-2													
J								Flow and heat transfer over a permeable biaxial stretching/shrinking sheet in a nanofluid	NEURAL COMPUTING & APPLICATIONS										Nanofluid; Stretching; shrinking sheet; Numerical method; Dual solution; Linear stability analysis	BOUNDARY-LAYER-FLOW; STAGNATION-POINT FLOW; VERTICAL SURFACE; CONVECTION	This paper studies the biaxial boundary layer flow of a nanofluid past a stretching/shrinking sheet. A suitable similarity transformation simplifies the system of partial differential equations into a system of ordinary differential equations. The obtained system of the governing ordinary differential equations is then solved numerically by using the bvp4c function from MATLAB. The generated numerical results are presented graphically and discussed in the relevance of the governing parameters. Dual solutions are found as the sheet is shrunk in the horizontal direction. Stability analysis shows that the first solution is physically realizable whereas the second solution is not practicable.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4575	4582		10.1007/s00521-018-3770-0													
J								A novel beta differential evolution algorithm-based fast multilevel thresholding for color image segmentation	NEURAL COMPUTING & APPLICATIONS										Image segmentation; Color multilevel thresholding; Histogram; Beta differential evolution; Kapur's entropy and Tsallis entropy	CUCKOO SEARCH ALGORITHM; MINIMUM CROSS-ENTROPY; BACTERIAL FORAGING ALGORITHM; NATURE-INSPIRED OPTIMIZATION; TSALLIS ENTROPY; ENHANCEMENT; DWT; SCHEME; PERFORMANCE; CONTRAST	Multilevel thresholding for image segmentation is a crucial process in several applications such as feature extraction and pattern recognition. The meticulous search for the best values for the optimization of fitness function using classical operations needs profuse computational time, which also results in inaccuracy and instability. In this paper, a new beta differential evolution (BDE)-based fast color image multilevel thresholding scheme using two objective functions has been presented. The optimal threshold values are determined by maximizing Kapur's and Tsallis entropy (entropy criterion) thresholding functions coupled with BDE algorithm. The efficiency of the proposed method is examined over existing multilevel thresholding methods such as artificial bee colony, particle swarm optimization, wind-driven optimization and differential evolution. These approaches are aimed to determine optimum threshold values at different levels of thresholding for color image segmentation. The proficiency of the presented methodology is demonstrated visually and computationally on five real-life true color images as well as four satellite images. Experimental outcomes are exhibited in terms of the optimal threshold value, best objective function and computational cost (in seconds) for each method at different thresholding levels. Afterward, the proposed scheme is examined intensively regarding the superiority of quality. The experimentally evaluated results show that the proposed BDE-based approach for multilevel color image segmentation can accurately and efficiently examine for multiple thresholds, which are near to optimal ones searched using an exhaustive search process.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4583	4613		10.1007/s00521-018-3771-z													
J								Quasi-synchronization of stochastic memristor-based neural networks with mixed delays and parameter mismatches	NEURAL COMPUTING & APPLICATIONS										Quasi-synchronization; Stochastic memristor-based neural networks; Mixed delays; Parameter mismatches; Lyapunov function	EXPONENTIAL SYNCHRONIZATION; MULTIAGENT SYSTEMS; CONTROLLER-DESIGN; CONSENSUS; ALGORITHM; CRITERIA	This paper is concerned with quasi-synchronization of stochastic memristor-based neural networks with mixed delays and parameter mismatches. Due to the parameter mismatches, mean-square exponential synchronization generally cannot be achieved directly, then the concept of exponential quasi-synchronization in mean square is introduced. Furthermore, based on the differential inclusions theory, stochastic Lyapunov function method and inequality techniques, some sufficient conditions are derived to guarantee the mean-square exponential quasi-synchronization for stochastic memristor-based neural networks with mixed delays. Finally, two examples are given to show the effectiveness of the proposed theoretical results.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4615	4628		10.1007/s00521-018-3772-y													
J								Lung nodule detection and classification based on geometric fit in parametric form and deep learning	NEURAL COMPUTING & APPLICATIONS										Autoencoder; Deep learning; False positives; Lung nodule; Hybrid features; Segmentation	COMPUTER-AIDED DETECTION; IMAGE DATABASE CONSORTIUM; FALSE-POSITIVE REDUCTION; PULMONARY NODULES; AUTOMATIC DETECTION; NEURAL-NETWORKS; SEGMENTATION; CANCER; SHAPE; ENSEMBLE	This study presents an automated detection and classification method to facilitate the radiologists in the diagnosis process. The major problem in these systems is the inclusion of false positives in the results, which may lead to inaccurate diagnosis. A nodule detection and classification method is proposed that consists of four major phases. First, the lung region extraction is performed based on optimal gray level threshold that is computed by fractional-order Darwinian particle swarm optimization. Then, a novel nodule candidate detection method, based on geometric fit in parametric form incorporating the geometric properties of the nodules, is proposed. In the next phase, a hybrid geometric texture feature descriptor is created for better representation of the candidate nodules, which is a combination of 2D as well as 3D information about nodule candidates. Finally, a deep learning approach based on stacked autoencoder and softmax for feature reduction and classification is applied to reduce false positives. Performance analysis on the largest publically available dataset, Lung Image Database Consortium and Image Database Resource Initiative, depicts that the proposed method has significantly reduced the number of false positives to 2.8 per scan with a promising sensitivity of 95.6%. The results demonstrate the significance of the methodology in automatic lung nodule detection and classification. Furthermore, it will facilitate and provide assistance to radiologists in precise nodule detection.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4629	4647		10.1007/s00521-018-3773-x													
J								A distributed Newton-Raphson-based coordination algorithm for multi-agent optimization with discrete-time communication	NEURAL COMPUTING & APPLICATIONS										Convex optimization; Multi-agent; Event-triggered; Distributed algorithm; Newton-Raphson	OPTIMAL ENERGY MANAGEMENT; CONVEX-OPTIMIZATION; CONSENSUS; SYSTEMS; VOLTAGE	This paper proposes a novel distributed continuous-time Newton-Raphson algorithm for distributed convex optimization problem, where the components of the goal are obtainable at different agents. To accelerate convergence speed, we focus on introducing Newton descent idea in our algorithm and extending it in a distributed setting. It is proved that the proposed algorithm can converge to the global optimal point with exponential convergence rate under weight-balanced directed graphs. Motivated by practical considerations, an event-triggered broadcasting strategy is further developed for each agent. Therein, the implementation of communication is driven by the designed triggered condition monitored by agents. Consequently, the proposed continuous-time algorithm can be executed with discrete-time communication, thus being able to greatly save communication expenditure. Moreover, the strategy is proved to be free of Zeno behavior. Eventually, the simulation results illustrate the advantages of the proposed algorithm.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4649	4663		10.1007/s00521-018-3798-1													
J								G2P: a new descriptor for pedestrian detection	NEURAL COMPUTING & APPLICATIONS										Pedestrian detection; Descriptor; G2P; Computer vision	SCALE; IMAGE	Pedestrian detection plays an important role in many applications. Since its birth 13 years ago, Histogram Of Gradient (HOG) descriptor has become a popular descriptor for pedestrian detection. Besides its original instantiation, the HOG also reflects a general methodology of constructing descriptors based on histograms of gradients of certain image sub-blocks. Following this general methodology, a number of HOG-style descriptors have been reported in the literature. The generation process of these descriptors is summarized in this work, and a new descriptor is presented for pedestrian detection. Three contributions are made in this work. First, a general model called descriptor generation model (DGM) is proposed, which can be used to systematically construct a wide range of HOG-style descriptors for pedestrian detection. Second, based on the DGM, a pedestrian detection experimental framework (PDEF) is introduced to find the optimal HOG-style descriptor. In the PDEF, the performance of each descriptor can be evaluated. At last, the genetic algorithm is employed to search the optimal (or semi-optimal) HOG-style descriptor in the descriptor space. And a new descriptor named Second-order Gradient for Pedestrian detection (G2P) is presented. Experimental results demonstrate the advantage of the G2P descriptor over the standard HOG descriptor with ETH, CVC-02-system, NITCA and KITTI dataset, which also reflects the effectiveness of the DGM-based PDEF in finding better descriptors for pedestrian detection.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4665	4674		10.1007/s00521-018-3815-4													
J								A CNNs-based method for optical flow estimation with prior constraints and stacked U-Nets	NEURAL COMPUTING & APPLICATIONS										Optical flow estimation; Convolutional neural networks; Supervised learning; Stacked U-Nets; Prior constraints		Traditional approaches for optical flow estimation always build an energy function which contains data term and smoothness term. However, optimizing the complex function is usually time-consuming. Nowadays, convolution neural networks have been applied in optical flow area. Most of them use large dataset for learning optical flow end-to-end, which can learn motion information from a large amount of prior information prepared in advance. However, these methods rely excessively on the learning ability of the network while ignoring some of well-proven assumptions in traditional approaches. In this paper, inspired by traditional methods, we present a network for learning optical flow, which combines traditional constraints with a supervised network. In the process of network optimization, the brightness constancy, gradient constancy and spatial smoothness assumptions are used to guide the training of network. Moreover, we stack several sub-networks integrated with prior constraints to form a large network for iterative refinement. Our method is tested on several public datasets, such as MPI-Sintel, KITTI2012, KITTI2015, Middlebury. The experimental results show that adding the prior constraints during training can obtain more refined and accurate flow. Compared with other recent methods, our method can achieve state-of-the-art performance on several public benchmarks.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4675	4688		10.1007/s00521-018-3816-3													
J								A tree-BLSTM-based recognition system for online handwritten mathematical expressions	NEURAL COMPUTING & APPLICATIONS										Mathematical expression recognition; Tree-based BLSTM; Local CTC; Online handwriting	SOFT-DECISION APPROACH; SYMBOL SEGMENTATION; STRUCTURAL-ANALYSIS; FRAMEWISE; DROPOUT	Long short-term memory networks (LSTM) achieve great success in temporal dependency modeling for chain-structured data, such as texts and speeches. An extension toward more complex data structures as encountered in 2D graphic languages is proposed in this work. Specifically, we address the problem of handwritten mathematical expression recognition, using a tree-based BLSTM architecture allowing the direct labeling of nodes (symbol) and edges (relationship) from a graph modeling the input strokes. One major difference with the traditional approaches is that there is no explicit segmentation, recognition and layout extraction steps but a unique trainable system that produces directly a stroke label graph describing a mathematical expression. The proposed system, considering no grammar, achieves competitive results in online math expression recognition domain.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4689	4708		10.1007/s00521-018-3817-2													
J								An adaptive twin support vector regression machine based on rough and fuzzy set theories	NEURAL COMPUTING & APPLICATIONS										Support vector machine; <mml; math><mml; mi>nu</mml; mi></mml; math>; documentclass[12pt]{minimal}; usepackage{amsmath}; usepackage{wasysym}; usepackage{amsfonts}; usepackage{amssymb}; usepackage{amsbsy}; usepackage{mathrsfs}; usepackage{upgreek}; setlength{; oddsidemargin}{-69pt}; begin{document}$$; nu$$; end{document}<inline-graphic xlink; href="521_2018_3823_Article_IEq9; gif"; >-Twin support vector regression; Rough theory; Fuzzy theory		It is known that the existing nu-twin support vector regression (nu TWSVR) has the ability to optimize epsilon 1and epsilon 2automatically through the proper selections of the parameters nu 1 and nu 2 However, since only the points near the lower-bound and upper-bound regressors are considered, it often results in overfitting problems. Furthermore, the equal penalties are applied to all samples that normally have different effects on the regressor function. In this paper, we propose an adaptive twin support vector regression (ATWSVR) machine to reduce the negative impacts of the possible outliers in nu-twin support vector regression (nu-TWSVR) by incorporating the fuzzy and rough set theories. First, two optimization models are constructed to obtain the lower and upper-bound regressors involving the use of the tools in rough and fuzzy set theories. Consequently, Theorems 1 and 2 are derived, through the application of KKT conditions and duality theory, to provide the connections between the dual optimal values and the location regions of the data points. Then, the definitions of different types of support vectors and their fuzzy proportions are given and Theorems 3 and 4 are proved to provide the bounds for the fuzzy proportions of these support vectors. Finally, the training data points located in different regions are assigned different fuzzy membership values by using iterative methods. Moreover, this approach can achieve the structural risk minimization and automatically control the fuzzy proportions of support vectors. The proposed ATWSVR is more robust for the data sets with outliers, as evidenced by the experimental results on both simulated examples as well as the benchmark real-world data sets. These results also confirm the claims made in the theorems mentioned above.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4709	4732		10.1007/s00521-018-3823-4													
J								Automatic detection of sleep-disordered breathing events using recurrent neural networks from an electrocardiogram signal	NEURAL COMPUTING & APPLICATIONS										Sleep-disordered breathing; Recurrent neural network; Long short-term memory; Gated-recurrent unit	APNEA DETECTION; RISK-FACTOR	In this study, we propose a novel method for automatically detecting sleep-disordered breathing (SDB) events using a recurrent neural network (RNN) to analyze nocturnal electrocardiogram (ECG) recordings. We design a deep RNN model comprising six stacked recurrent layers for the automatic detection of SDB events. The proposed deep RNN model utilizes long short-term memory (LSTM) and a gated-recurrent unit (GRU). To evaluate the performance of the proposed RNN method, 92 SDB patients were enrolled. Single-lead ECG recordings were measured for an average 7.2-h duration and segmented into 10-s events. The dataset comprised a training dataset (68,545 events) from 74 patients and test dataset (17,157 events) from 18 patients. The proposed method achieved high performance with an F1-score of 98.0% for LSTM and 99.0% for GRU. The results demonstrate superior performance over conventional methods. The proposed method can be used as a precise screening and diagnosing tool for patients with SDB disorders.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4733	4742		10.1007/s00521-018-3833-2													
J								Unsupervised domain adaptation with target reconstruction and label confusion in the common subspace	NEURAL COMPUTING & APPLICATIONS										Domain adaptation; Autoencoder; Adversarial training; Cluster assumption		Deep neural networks can learn powerful and discriminative representations from a large number of labeled samples. However, it is typically costly to collect and annotate large-scale datasets, which limits the applications of deep learning in many real-world scenarios. Domain adaptation, as an option to compensate for the lack of labeled data, has attracted much attention in the community of machine learning. Although a mass of methods for domain adaptation has been presented, many of them simply focus on matching the distribution of the source and target feature representations, which may fail to encode useful information about the target domain. In order to learn invariant and discriminative representations for both domains, we propose a Cross-Domain Minimization with Deep Autoencoder method for unsupervised domain adaptation, which simultaneously learns label prediction on the source domain and input reconstruction on the target domain using shared feature representations aligned with correlation alignment in a unified framework. Furthermore, inspired by adversarial training and cluster assumption, a task-specific class label discriminator is incorporated to confuse the predicted target class labels with samples draw from categorical distribution, which can be regarded as entropy minimization regularization. Extensive empirical results demonstrate the superiority of our approach over the state-of-the-art unsupervised adaptation methods on both visual and non-visual cross-domain adaptation tasks.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4743	4756		10.1007/s00521-018-3846-x													
J								Heuristic orientation adjustment for better exploration in multi-objective optimization	NEURAL COMPUTING & APPLICATIONS										Decomposition; Multi-objective; Adaptive reference vector; Evolutionary algorithm	NONDOMINATED SORTING APPROACH; NORMAL-BOUNDARY INTERSECTION; REFERENCE-POINT; BALANCING CONVERGENCE; ALGORITHM; DECOMPOSITION; DIVERSITY; MOEA/D; ENTROPY	Decomposition strategy which employs predefined subproblem framework and reference vectors has significant contribution in multi-objective optimization, and it can enhance local convergence as well as global diversity. However, the fixed exploring directions sacrifice flexibility and adaptability; therefore, extra reference adaptations should be considered under different shapes of the Pareto front. In this paper, a population-based heuristic orientation generating approach is presented to build a dynamic decomposition. The novel approach replaces the exhaustive reference distribution with reduced and partial orientations clustered within potential areas and provides flexible and scalable instructions for better exploration. Numerical experiment results demonstrate that the proposed method is compatible with both regular Pareto fronts and irregular cases and maintains outperformance or competitive performance compared to some state-of-the-art multi-objective approaches and adaptive-based algorithms. Moreover, the novel strategy presents more independence on subproblem aggregations and provides an autonomous evolving branch in decomposition-based researches.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4757	4771		10.1007/s00521-018-3848-8													
J								On the evaluation of generative models in music	NEURAL COMPUTING & APPLICATIONS										Objective evaluation; Music generation; Computational creativity	TURING TEST; CREATIVITY; PATTERNS	The modeling of artificial, human-level creativity is becoming more and more achievable. In recent years, neural networks have been successfully applied to different tasks such as image and music generation, demonstrating their great potential in realizing computational creativity. The fuzzy definition of creativity combined with varying goals of the evaluated generative systems, however, makes subjective evaluation seem to be the only viable methodology of choice. We review the evaluation of generative music systems and discuss the inherent challenges of their evaluation. Although subjective evaluation should always be the ultimate choice for the evaluation of creative results, researchers unfamiliar with rigorous subjective experiment design and without the necessary resources for the execution of a large-scale experiment face challenges in terms of reliability, validity, and replicability of the results. In numerous studies, this leads to the report of insignificant and possibly irrelevant results and the lack of comparability with similar and previous generative systems. Therefore, we propose a set of simple musically informed objective metrics enabling an objective and reproducible way of evaluating and comparing the output of music generative systems. We demonstrate the usefulness of the proposed metrics with several experiments on real-world data.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4773	4784		10.1007/s00521-018-3849-7													
J								Multitask possibilistic and fuzzy co-clustering algorithm for clustering data with multisource features	NEURAL COMPUTING & APPLICATIONS										Multitask clustering; Possibilistic clustering; Fuzzy co-clustering; Robustness; Clustering accuracy	NUMBERS	People often encounter two major problems for the practical clustering problems. One is the problem arising from improper extraction of feature sets, such as the weakness of the features and the feature vector usually has the property of high-dimensional and multisource. The other is that the outliers interfere with the clustering results. In this paper, we use the idea of co-clustering to cluster datasets and feature sources at the same time, and use the information which received from the information sharing between tasks to improve the accuracy of clustering tasks through the idea of multitask. And we used the advantage of the typical degree to construct a new parameter selection index to identify the outliers, and to correct each parameter by weakening the influence of the identified outliers on the clustering results. In order to reflect the applicability and robustness of the algorithm, we extend the algorithm to the non-precise dataset and evaluate the algorithm from multiple aspects through experiments. Experiments show that the proposed algorithms not only improve the clustering accuracy, but also greatly reduce the interference of outliers to clustering results.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4785	4804		10.1007/s00521-018-3851-0													
J								Occluded offline handwritten Chinese character recognition using deep convolutional generative adversarial network and improved GoogLeNet	NEURAL COMPUTING & APPLICATIONS										Deep convolutional generative adversarial network; GoogLeNet; Occluded offline handwritten Chinese character recognition	IMAGE; COMPLETION	In this paper, we propose a novel method for recognizing occluded offline handwritten Chinese characters based on deep convolutional generative adversarial network (DCGAN) and improved GoogLeNet. Different from previous methods, our proposed method is capable of inpainting and recognizing occluded characters without needing to know the concrete positions of corrupted regions. First, the generator and discriminator of DCGAN are combined to generate realistic Chinese characters from corrupted images, and the contextual loss and the content loss are further used to inpaint generated images. Finally, we use the improved GoogLeNet with traditional feature extraction methods to recognize the recovered handwritten Chinese characters. The proposed method is evaluated on the extended CASIA-HWDB1.1 dataset for two challenging inpainting tasks with different portions of blocks or random missing pixels. Experimental results show that our method can achieve higher repair rates and higher recognition accuracies than most of existing methods.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4805	4819		10.1007/s00521-018-3854-x													
J								PHURIE: hurricane intensity estimation from infrared satellite imagery using machine learning	NEURAL COMPUTING & APPLICATIONS										Hurricane intensity prediction; Tropical cyclones; Machine learning-based forecasting; Support vector regression	TROPICAL CYCLONE INTENSITY; OBJECTIVE SCHEME	Automated prediction of hurricane intensity from satellite infrared imagery is a challenging problem with implications in weather forecasting and disaster planning. In this work, a novel machine learning-based method for estimation of intensity or maximum sustained wind speed of tropical cyclones over their life cycle is presented. The approach is based on a support vector regression model over novel statistical features of infrared images of a hurricane. Specifically, the features characterize the degree of uniformity in various temperature bands of a hurricane. Performance of several machine learning methods such as ordinary least squares regression, backpropagation neural networks and XGBoost regression has been compared using these features under different experimental setups for the task. Kernelized support vector regression resulted in the lowest prediction error between true and predicted hurricane intensities (approximately 10 knots or 18.5 km/h), which is better than previously proposed techniques and comparable to SATCON consensus. The performance of the proposed scheme has also been analyzed with respect to errors in annotation of center of the hurricane and aircraft reconnaissance data. The source code and webserver implementation of the proposed method called PHURIE (PIEAS HURricane Intensity Estimator) is available at the URL: .																	0941-0643	1433-3058				MAY	2020	32	9			SI		4821	4834		10.1007/s00521-018-3874-6													
J								TLVANE: a two-level variation model for attributed network embedding	NEURAL COMPUTING & APPLICATIONS										Attribute network; Embedding; Variational autoencoder	LAPLACIAN EIGENMAPS	Network embedding aims to learn low-dimensional representations for nodes in social networks, which can serve many applications, such as node classification, link prediction and visualization. Most of network embedding methods focus on learning the representations solely from the topological structure. Recently, attributed network embedding, which utilizes both the topological structure and node content to jointly learn latent representations, becomes a hot topic. However, previous studies obtain the joint representations by directly concatenating the one from each aspect, which may lose the correlations between the topological structure and node content. In this paper, we propose a new attributed network embedding method, TLVANE, which can address the drawback by exploiting the deep variational autoencoders (VAEs). Particularly, a two-level VAE model is built, where the first-level accounts for the joint representations while the second for the embeddings of each aspect. Extensive experiments on three real-world datasets have been conducted, and the results demonstrate the superiority of the proposed method against state-of-the-art competitors.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4835	4847		10.1007/s00521-018-3875-5													
J								An integrated particle swarm optimization approach hybridizing a new self-adaptive particle swarm optimization with a modified differential evolution	NEURAL COMPUTING & APPLICATIONS										Particle swarm optimization; Differential evolution; Enhanced particle swarm optimization; Convergence analysis of particle swarm optimization	ALGORITHM	Hybridizing particle swarm optimization (PSO) with differential evolution (DE), this paper proposes an integrated PSO-DE optimizer and examines the performance of this optimizer. Firstly, a new self-adaptive PSO (SAPSO) is established to guide movements of particles in the proposed hybrid PSO. Aiming at well trade-offing the global and local search capabilities, a self-adaptive strategy is proposed to adaptively update the three main control parameters of particles in SAPSO. Since the performance of PSO heavily relies on its convergence, the convergence of SAPSO is analytically investigated and a convergence-guaranteed parameter selection rule is provided for SAPSO in this study. Subsequently, a modified self-adaptive differential evolution is presented to evolve the personal best positions of particles in the proposed hybrid PSO in order to mitigant the potential stagnation issue. Next, the performance of the proposed method is validated via 25 benchmark test functions and two real-world problems. The simulation results confirm that the proposed method performs significantly better than its peers at a confidence level of 95% over the 25 benchmarks in terms of the solution optimality. Besides, the proposed method outperforms its contenders over the majority of the 25 benchmarks with respect to the search reliability and the convergence speed. Moreover, the computational complexity of the proposed method is comparable with those of some other enhanced PSO-DE methods compared. The simulation results over the two real-world issues reveal that the proposed method dominates its competitors as far as the solution optimality is considered.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4849	4883		10.1007/s00521-018-3878-2													
J								Single-image super-resolution via joint statistic models-guided deep auto-encoder network	NEURAL COMPUTING & APPLICATIONS										Non-local similarity; Split Bergman iteration; Steering kernel regression; Single-image super-resolution		Recent researches on super-resolution (SR) with deep learning networks have achieved amazing results. However, most of the existing studies neglect the internal distinctiveness of an image and the output of most methods tends to be of blurring, smoothness and implausibility. In this paper, we proposed a unified model which combines the deep model with the image restoration model for single-image SR. This model can not only reconstruct the SR image, but also keep the distinct fine structures for the low-resolution image. Two statistic priors are used to guide the updating of the output of the deep neural network: One is the non-local similarity and the other is the local smoothness. The former is modeled as the non-local total variation regularization, and the latter as the steering kernel regression total variation regularization. For this unified model, a new optimization function is formulated under a regularization framework. To optimize the total variation problem, a novel algorithm based on split Bregman iteration is developed with the theoretical proof of convergence. The experimental results demonstrate that the proposed unified model improves the peak signal-to-noise ratio of the deep SR model. Quantitative and qualitative results on four benchmark datasets show that the proposed model achieves better performance than the deep SR model without regularization terms.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4885	4896		10.1007/s00521-018-3886-2													
J								Novel approaches to one-directional two-dimensional principal component analysis in hybrid pattern framework	NEURAL COMPUTING & APPLICATIONS										Dimensionality reduction; Face recognition; Principal component analysis; Two-dimensional principal component analysis; Image reconstruction; Feature partitioning	FACE REPRESENTATION; PCA; 2DPCA; FUSION	In this paper, we present variations of one-directional two-dimensional principal component analysis (2DPCA) in hybrid pattern framework. Using hybrid pattern framework, we propose two novel methods, namely extended sub-image principal component analysis (ESIMPCA) and extended flexible principal component analysis (EFLPCA). The ESIMPCA operates on sub-image and full image at a time and captures the local and global variation of images. The dimensionality problem of ESIMPCA feature matrices is eliminated by further applying 2DPCA on two-dimensional ESIMPCA feature matrices to generate EFLPCA feature matrices. The summarization of variances, time and space complexities of the proposed methods and their relationship with some existing variations of one-directional 2DPCAs are addressed. The experiment is conducted on ORL and YALE face databases with different image resolutions. The experimental results, using EFLPCA, show superiority performance in terms of feature dimensionality, recognition accuracy and speed with reasonable space requirement over some existing variations of one-directional 2DPCA.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4897	4918		10.1007/s00521-018-3892-4													
J								Edge computing-based real-time passenger counting using a compact convolutional neural network	NEURAL COMPUTING & APPLICATIONS										Crowd counting; Edge computing; Compact convolutional neural network; Weighted Euclidean loss; Nvidia TX2	CROWD; PEOPLE	Crowd counting from low-resolution images is a challenging task, in particular in the edge computing system. An embedded equipment is commonly incompetent at patch-based crowd counting with real-time performance. This work develops a real-time method to count passengers in a bus by using Nvidia TX2. The videos of entry are recorded by a camera up ahead, and the data suffer from severe occlusion, which makes designing handcrafted features difficult. The counting is performed by summing up pixel values of the density map estimated using a compact convolutional neural network (CCNN), which is robust to scale variations by employing skip connections. A weighted Euclidean loss is proposed to handle cluttered backgrounds and blurry foregrounds. The loss increases the activations in dense regions, but can restrain the activations in background regions. The counting results are further improved by smoothing, which utilizes constraints between consecutive frames. Comparisons with existing counting approaches, including patch-based and whole image-based approaches, are made on two benchmarking datasets. The results indicate the accuracy of CCNN in counting dense crowds. Moreover, the evaluated bus datasets verify the feasibility of CCNN in counting passengers from low-resolution input images with real-time performance on TX2.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4919	4931		10.1007/s00521-018-3894-2													
J								Semi-parametric training of autoencoders with Gaussian kernel smoothed topology learning neural networks	NEURAL COMPUTING & APPLICATIONS										Autoencoder; Nonparametric learning; Kernel density estimation; Incremental learning		Autoencoders are essential for training multi-hidden layer neural networks. Parametric autoencoder trainings often require user selections of hidden neuron numbers and kernel types. In this paper, a semi-parametric autoencoder training method based on self-organized learning and incremental learning is proposed. The cost function is constructed incrementally by nonparametric learning, and the model parameter is trained by parametric learning. First, a topology learning neural network such as growing neural gas or self-organizing incremental neural network is trained to obtain a discrete representation of the training data. Second, the correlations between different dimensions are modeled as a joint distribution by the neural network representation and kernel smoothers. Finally, the loss function is defined to be the regression prediction errors with each dimension as a response variable in density regression. The parameter of kernels is selected by gradient descent which minimizes the reconstruction error on a data subset. The proposed architecture has the advantage of high training space efficiency because of incremental training, and the advantage of automated selection of hidden neuron numbers. Experiments are carried out on 4 UCI datasets and an image interpolation task. Results show that the proposed methods outperform the perceptron architecture autoencoders and the restricted Boltzmann machine in the task of nonlinear feature learning.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4933	4950		10.1007/s00521-018-3897-z													
J								Finite-time synchronization of delayed memristive neural networks via 1-norm-based analytical approach	NEURAL COMPUTING & APPLICATIONS										Memristive neural networks; Finite-time synchronization; Time-varying delay; 1-Norm-based analytical approach; Quantized controller	EXPONENTIAL SYNCHRONIZATION; CHAOTIC SYSTEMS; STABILITY; CONSENSUS; DYNAMICS	By using 1-norm-based analytical approach, this paper considers finite-time (FET) synchronization for memristive neural networks (MNNs) with time-varying delays. New quantized controllers are designed, which can save communication channel and play an important role in synchronizing MNNs. By constructing Lyapunov function, and developing 1-norm-based analytical methods, several conditions are derived to guarantee that the MNNs can be synchronized within a settling time. In addition, the settling time is also presented for the considered MNNs. Some numerical simulations are provided to illustrate the theoretical results.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4951	4960		10.1007/s00521-018-3906-2													
J								An efficient chaos-based image compression and encryption scheme using block compressive sensing and elementary cellular automata	NEURAL COMPUTING & APPLICATIONS										Image encryption; Elementary cellular automata (ECA); Block compressive sensing (BCS); Chaos	DNA-SEQUENCE OPERATION; ALGORITHM; MAP; SYSTEM; DESIGN; GENERATION; RECOVERY	In this paper, an efficient image compression and encryption scheme combining the parameter-varying chaotic system, elementary cellular automata (ECA) and block compressive sensing (BCS) is presented. The architecture of permutation, compression and re-permutation is adopted. Firstly, the plain image is transformed by DWT, and four block matrices are gotten, and they are a low-frequency block with important information and three high-frequency blocks with less important information. Secondly, ECA is used to scramble the four sparse block matrices, which can effectively change the position of the elements in the matrices and upgrade the confusion effect of the algorithm. Thirdly, according to the importance of each block, BCS is adopted to compress and encrypt four scrambled matrices with different compression ratios. In the BCS, the measurement matrices are constructed by a parameter-varying chaotic system, and thus few parameters may produce the large measurement matrices, which may effectively reduce memory space and transmission bandwidth. Finally, the four compressed matrices are recombined into a large matrix, and the cipher image is obtained by re-scrambling it. Moreover, the initial values of the chaotic system are produced by the SHA 256 hash value of the plain image, which makes the proposed encryption algorithm highly sensitive to the original image. Experimental results and performance analyses demonstrate its good security and robustness.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4961	4988		10.1007/s00521-018-3913-3													
J								A new wavelet conjunction approach for estimation of relative humidity: wavelet principal component analysis combined with ANN	NEURAL COMPUTING & APPLICATIONS										Relative humidity; Estimation; Principal component analysis; Wavelet transform; ANN	ARTIFICIAL NEURAL-NETWORKS; AIR-TEMPERATURE; PREDICTION; PCA	Relative humidity (RH) has an important effect on precipitation, especially in arid and semiarid regions. Prediction of RH has been the focus of attention of climate change researchers as well. In this investigation, the accuracy of six intelligent models, including an artificial neural network (ANN), a co-active neuro-fuzzy inference system (CANFIS), principal component analysis (PCA) combined with ANN (PCA-ANN) and three hybrid wavelet-artificial intelligence models, including WANN, WCANFIS and WPCA-ANN, was evaluated in daily RH prediction. Thirty weather stations located in different climates in Iran for the period 2000-2010 were selected for the evaluation and comparison of these models. The performance of each model was evaluated using correlation coefficient (r) and normal root mean square error (NRMSE). Based on the statistical evaluation criteria, the accuracy ranks of the six models were: WPCA-ANN, WCANFIS, WANN, PCA-ANN, ANN and CANFIS. The results indicated that the WPCA-ANN model was the optimal model for estimation of RH, and the range of NRMSE and r values were from 0.009 to 0.080 and from 0.996 to 0.999, respectively. Overall, WPCA-ANN is a new approach that can be successfully applied to predict RH with a high accuracy.																	0941-0643	1433-3058				MAY	2020	32	9			SI		4989	5000		10.1007/s00521-018-3916-0													
J								Synchronized stationary distribution of stochastic multi-group models with dispersal	NEURAL COMPUTING & APPLICATIONS										Synchronized stationary distribution; Kirchhoff's Matrix Tree Theorem; Stochastic multi-group models; Stochastic coupled oscillators	RECURRENT NEURAL-NETWORKS; EXPONENTIAL SYNCHRONIZATION; COMPLEX NETWORKS; DIFFERENTIAL-EQUATIONS; EPIDEMIC MODEL; STABILITY; SYSTEMS; DYNAMICS; PERTURBATION; CONSENSUS	This paper is concerned with a new stationary distribution named synchronized stationary distribution. It is the first time to apply such kind of distribution to stochastic multi-group models with dispersal. And the existing region of synchronized stationary distribution is closely related to the coupling structure, stochastic disturbance intensity as well as the coefficients of models. We propose two main theorems to ensure the existence of a synchronized stationary distribution by combining Kirchhoff's Matrix Tree Theorem in the graph theory as well as the Lyapunov method. Additionally, the value of our results is shown by applying them to stochastic coupled oscillators and stochastic coupled Rossler-like circuits with multiple dispersal. Correspondingly, two numerical examples are given to illustrate that our results are feasible and effective.																	0941-0643	1433-3058				MAY	2020	32	9			SI		5001	5013		10.1007/s00521-018-3918-y													
J								Variable input observer for nonstationary high-rate dynamic systems	NEURAL COMPUTING & APPLICATIONS										High-rate dynamics; Input space; Embedding; Adaptive observer; Neural network; Structural health monitoring	SELECTION; SPACE; PREDICTION	Engineering systems experiencing events of amplitudes higher than 100 g(n) for a duration under 100 ms, here termed high-rate dynamics, can undergo rapid damaging effects. If the structural health of such systems could be accurately estimated in a timely manner, preventative measures could be employed to minimize adverse effects. For complex high-rate problems, adaptive observers have shown promise due to their capability to deal with nonstationary, noisy, and uncertain systems. However, adaptive observers have slow convergence rates, which impede their applicability to the high-rate problems. To improve on the convergence rate, we propose a variable input space concept for optimizing the use of data history of high-rate dynamics, with the objective to produce an optimal representation of the system of interest. Using the embedding theory, the algorithm sequentially selects and adapts a vector of inputs that preserves the essential dynamics of the high-rate system. In this paper, the variable input space is integrated in a wavelet neural network, which constitutes a variable input observer. The observer is simulated using experimental data from a high-rate system. Different input space adaptation methods are studied, and the performance is also compared against an optimized fixed input strategy. It is found that a smooth transition of the input space eliminates error spikes and yields faster convergence. The variable input observer is further studied in a hybrid model-/data-driven formulation, and results demonstrate significant improvement in performance gained from the added physical knowledge.																	0941-0643	1433-3058				MAY	2020	32	9			SI		5015	5026		10.1007/s00521-018-3927-x													
J								Frequency variation analysis in neuronal cultures for stimulus response characterization	NEURAL COMPUTING & APPLICATIONS										MEA; Dissociated neurons; Neuronal stimulation; Hybrots; Machine learning	POTENTIATION; DEPRESSION; CORTEX	In vitro neuronal cultures embodied in a closed-loop control system have been used recently to study neuronal dynamics. This allows the development of neurons in a controlled environment with the purpose of exploring the computational capabilities of such biological neural networks. Due to the intrinsic properties of in vitro neuronal cultures and how the neuronal tissue grows in them, the ways in which signals are transmitted and generated within and throughout the culture can be difficult to characterize. The neural code is formed by patterns of spikes whose properties are in essence nonlinear and non-stationary. The usual approach for this characterization has been the use of the post-stimulus time histogram (PSTH). PSTH is calculated by counting the spikes detected in each neuronal culture electrode during some time windows after a stimulus in one of the electrodes. The objective is to find pairs of electrodes where stimulation in one of the pairs produces a response in the other but not in the rest of the electrodes in other pairs. The aim of this work is to explore possible ways of extracting relevant information from the global response to culture stimulus by studying the patterns of variation over time for the firing rate, estimated from inverse inter-spike intervals, in each electrode. Machine learning methods can then be applied to distinguish the electrode being stimulated from the whole culture response, in order to obtain a better characterization of the culture and its computational capabilities so it can be useful for robotic applications.																	0941-0643	1433-3058				MAY	2020	32	9			SI		5027	5032		10.1007/s00521-018-3942-y													
J								Analysis and design of genetic algorithm-based cascade control strategy for improving the dynamic performance of interleaved DC-DC SEPIC PFC converter	NEURAL COMPUTING & APPLICATIONS										DC-DC power conversion; Voltage control; Current control; Harmonic distortion; Modeling	SLIDING-MODE CONTROLLER; POWER-FACTOR CORRECTOR; INPUT-CURRENT; OUTPUT VOLTAGE; TOPOLOGY	Switched-mode power supplies used for powering new generation devices like microprocessors, utility grids and electric vehicles need to operate with faster dynamic response. This paper proposes cascade control technique using genetic algorithm to obtain the optimal proportional integral outer voltage and inner current controller parameters of interleaved DC-DC single-ended primary inductance converter for power factor correction in SMPS with fast dynamic response. The state space model of the interleaved DC-DC SEPIC converter is derived using state space averaging technique. The system is of higher order, and hence, the reduced-order model of the interleaved DC-DC SEPIC converter is realized using Hankel matrix approach to reduce the computational complexity in controller design. The optimal controller parameters are then obtained for the reduced-order system using genetic algorithm for improving the dynamic performance of the system. The performance of the closed-loop system is analyzed in terms of input power factor, % total harmonic distortion of source current, % efficiency and % load voltage regulation for variations in the line, load and reference voltage using Matlab/Simulink software tool. A prototype of the converter controlled by TMS320C2000 (TM) microcontroller for an output power of 200 W is tested and validated with the simulation results.																	0941-0643	1433-3058				MAY	2020	32	9			SI		5033	5047		10.1007/s00521-018-3944-9													
J								Neural abstractive summarization fusing by global generative topics	NEURAL COMPUTING & APPLICATIONS										Neural network; Variational auto-encoding; Abstractive summarization; Deep learning		Various efforts have been dedicated to automatically generate coherent, condensed and informative summaries. Most concentrate on improving the capability of generating neural language models locally, but do not consider global information. In real cases, a summary is comprehensively influenced by the full content of the source text and is especially guided by its core sense. To seamlessly integrate global semantic representation into a summarization generation system, we propose to incorporate a neural generative topic matrix as an abstractive level of topic information. By mapping global semantics into a local generative language model, the abstractive summarization is capable of generating succinct and recapitulative words or phrases. Extensive experiments on DUC-2004 and Gigaword datasets convincingly validate the proposed model.																	0941-0643	1433-3058				MAY	2020	32	9			SI		5049	5058		10.1007/s00521-018-3946-7													
J								Embedding of fuzzy graphs on topological surfaces	NEURAL COMPUTING & APPLICATIONS										Fuzzy graphs; Planar embedding; Sphere embedding; m-Torus embedding; Total fuzzy face value; Fuzzy planar triangulation		Planar graph is a special type in crisp as well as in fuzzy graphs. In fuzzy planar graphs, the planarity value is the amount of planarity of the crossed fuzzy edges, so that the intersection of fuzzy edges are possible in fuzzy graphs as compared to the planar graphs in crisp. Generally, the fuzzy planar graphs are depicted in the plane surface. In this paper, the embedding of fuzzy graphs are discussed in the surfaces like sphere and m-torus. Moreover, definition of fuzzy planar triangulation, straight-line, and piecewise embedding are also stated for planar embedding. Some of the effective definitions and theorems are illustrated with examples. Theorems like Euler's formula for plane and sphere surfaces are proved and formulated for fuzzy planar graphs.																	0941-0643	1433-3058				MAY	2020	32	9			SI		5059	5069		10.1007/s00521-018-3948-5													
J								Whale swarm algorithm with the mechanism of identifying and escaping from extreme points for multimodal function optimization	NEURAL COMPUTING & APPLICATIONS										Whale swarm algorithm; Multimodal optimization; Metaheuristic algorithm; Niching; Extreme point	SEARCH ALGORITHM	Most real-world optimization problems often come with multiple global optima or local optima. Therefore, increasing niching metaheuristic algorithms, which devote to finding multiple optima in a single run, are developed to solve these multimodal optimization problems. However, there are two difficulties urgently to be solved for most existing niching metaheuristic algorithms: how to set the niching parameter values for different optimization problems and how to jump out of the local optima efficiently. These two difficulties limit their practicality largely. Based on Whale Swarm Algorithm (WSA) we proposed previously, this paper presents a new multimodal optimizer named WSA with Iterative Counter (WSA-IC) to address these two difficulties. On the one hand, WSA-IC improves the iteration rule of the original WSA for multimodal optimization, which removes the need of specifying different values of attenuation coefficient for different problems to form multiple subpopulations, without introducing any niching parameter. On the other hand, WSA-IC enables the identification of extreme points during the iterations relying on two new parameters (i.e., stability threshold Tsand fitness threshold Tf to jump out of the located extreme points. Moreover, the convergence of WSA-IC is proved. Finally, the proposed WSA-IC is compared with several niching metaheuristic algorithms on CEC2015 niching benchmark test functions and on five additional high-dimensional multimodal functions. The experimental results demonstrate that WSA-IC statistically outperforms other niching metaheuristic algorithms on most test functions.																	0941-0643	1433-3058				MAY	2020	32	9			SI		5071	5091		10.1007/s00521-018-3949-4													
J								Multi-objective league championship algorithm for real-time task scheduling	NEURAL COMPUTING & APPLICATIONS										Heterogeneous multiprocessors; Global optimum; Scheduling	GENETIC ALGORITHM; GRAPHS; MULTIPROCESSORS; ASSIGNMENT	League championship algorithm is a recently proposed population-based evolutionary algorithm for finding global optimal solutions in continuous optimization problems. The proposed work adopts the algorithm by modifying the team formation step for solving real-time task scheduling problem in heterogeneous multiprocessors. Two different objectives: tardiness and energy consumption were considered for scheduling. Our proposed algorithm is implemented using Java and tested using the graphs generated by the benchmark tools: task graph for free and task graph generator. Simulation results prove the performance of the proposed algorithm is better in terms of the objective functions over the other existing metaheuristic algorithms such as genetic algorithm, ant colony optimization and particle swarm optimization.																	0941-0643	1433-3058				MAY	2020	32	9			SI		5093	5104		10.1007/s00521-018-3950-y													
J								Crowd density estimation based on classification activation map and patch density level	NEURAL COMPUTING & APPLICATIONS										Crowd density estimation; Image patch; Density level; Attention mechanism; Classification activation map		The task of crowd counting and density map estimation is riddled with many challenges, such as occlusions, non-uniform density, intra-scene and inter-scene variations in scale and perspective. Due to the development of deep learning and large crowd datasets in recent years, most crowd counting methods have achieved notable success. This paper aims to solve crowd density estimation problem for both sparse and dense conditions. To this end, we make two contributions: (1) a network named Patch Scale Discriminant Regression Network (PSDR). Given an input crowd image, it divides the image into patches and sends image patches of different density levels into different regression networks to get the corresponding density maps. It combines all patch density maps to predict the entire density map as the output. (2) A person classification activation map (CAM) method. CAM provides person location information and guides the generation of the entire density map in the final stage. Experiment confirms that CAM allows PSDR to gain another round of performance boost. For instance, on the SmartCity dataset, we achieve (8.6-1.1) MAE and (11.6-1.4) MSE. Our method combining above two methods performs better than state-of-the-art methods.																	0941-0643	1433-3058				MAY	2020	32	9			SI		5105	5116		10.1007/s00521-018-3954-7													
J								Sentiment analysis via semi-supervised learning: a model based on dynamic threshold and multi-classifiers	NEURAL COMPUTING & APPLICATIONS										Dynamic threshold; Multiple classifiers; Semi-supervised learning; Sentiment analysis; Social media; Weighted voting		Sentiment analysis has become a very popular research topic, especially for retrieving valuable information from various online environments. Most existing sentiment studies are based on supervised learning, which requires sufficient amount of labeled data. However, sentiment analysis often faces insufficient labeled data in practice, as it is very expensive and time-consuming to label large amount of data. To handle the scenario of insufficient initial labeled data, we propose a novel semi-supervised model based on dynamic threshold and multi-classifiers. In particular, the training data are auto-labeled in an iterative way based on the proposed dynamic threshold algorithm, where a dynamic threshold function is proposed to set thresholds for selecting the auto-labeled data. It considers both the quality and quantity of the auto-labeled data. In addition, the proposed weighted voting strategy combines multiple support vector machine classifiers by considering performance gap among different classifiers. The performance of the proposed model is validated through experiments on real datasets. Compared with two other existing models, the proposed model achieves the highest sentiment analysis accuracy across datasets with different sizes of initial labeled training data.																	0941-0643	1433-3058				MAY	2020	32	9			SI		5117	5129		10.1007/s00521-018-3958-3													
J								A scalable indoor localization algorithm based on distance fitting and fingerprint mapping in Wi-Fi environments	NEURAL COMPUTING & APPLICATIONS										Indoor localizaiton; Fingerprint mapping; Distance fitting; Wi-Fi signal processing	OPTIMIZATION; SENSORS; NETWORK	With ever-increasing demands on location-based services in indoor environments, indoor localization technologies have attracted considerable attention in both industrial and academic communities. In this work, we propose a scalable indoor localization algorithm (SILA) consisting of two components, namely an annulus-based localization (ABL) component and a local search-based localization (LSL) component, with the objectives of enhancing localization accuracy and reducing online computational overhead. First, the ABL component is developed based on distance fitting using received signal strength indicator (RSSI) of Wi-Fi-based devices. In particular, a distance-RSSI fitting model is proposed based on multinomial function fitting, which is adopted to estimate the distance between the Wi-Fi access point (AP) and the mobile device. On this basis, an annulus construction scheme is proposed to confine the online searching space for possible locations of the mobile device. In addition, based on the observation of signal attenuation characteristics in different physical environments, we design a subarea division scheme, which not only enables the system to choose proper distance-RSSI fitting functions in different areas, but also reduces the overhead of distance fitting. Second, the LSL component is developed based on fingerprint mapping using RSSIs collected at APs. In particular, an RSSI distribution probability model is derived to better map the signal features of an online point (OP) with that of reference points (RPs). Then, an online localization algorithm is proposed, which selects a set of candidate RPs based on Bayes theorem and estimates the final location of an OP using K-nearest-neighbor (KNN) method. Finally, we implement the system prototype and compare the performance of SILA with two representative solutions in the literature. An extensive performance evaluation is conducted in real-world environments, and the results conclusively demonstrate the superiority of SILA in terms of both localization accuracy and system scalability.																	0941-0643	1433-3058				MAY	2020	32	9			SI		5131	5145		10.1007/s00521-018-3961-8													
J								Q-learning-based simulated annealing algorithm for constrained engineering design problems	NEURAL COMPUTING & APPLICATIONS										Simulated annealing; Q-learning algorithm; Constrained engineering design problems	PARTICLE SWARM OPTIMIZATION; LOCAL SEARCH; PSO; MODEL	Simulated annealing (SA) was recognized as an effective local search optimizer, and it showed a great success in many real-world optimization problems. However, it has slow convergence rate and its performance is widely affected by the settings of its parameters, namely the annealing factor and the mutation rate. To mitigate these limitations, this study presents an enhanced optimizer that integrates Q-learning algorithm with SA in a single optimization model, named QLSA. In particular, the Q-learning algorithm is embedded into SA to enhance its performances by controlling its parameters adaptively at run time. The main characteristics of Q-learning are that it applies reward/penalty technique to keep track of the best performing values of these parameters, i.e., annealing factor and the mutation rate. To evaluate the effectiveness of the proposed QLSA algorithm, a total of seven constrained engineering design problems were used in this study. The outcomes show that QLSA was able to report a mean fitness value of 1.33 on cantilever beam design, 263.60 on three-bar truss design, 1.72 on welded beam design, 5905.42 on pressure vessel design, 0.0126 on compression coil spring design, 0.25 on multiple disk clutch brake design, and 2994.47 on speed reducer design problem. Further analysis was conducted by comparing QLSA with the state-of-the-art population optimization algorithms including PSO, GWO, CLPSO, harmony, and ABC. The reported results show that QLSA significantly (i.e., 95% confidence level) outperforms other studied algorithms.																	0941-0643	1433-3058				MAY	2020	32	9			SI		5147	5161		10.1007/s00521-019-04008-z													
J								The comprehensive evaluation model of power supply capacity for regional-oriented distribution network	NEURAL COMPUTING & APPLICATIONS										Evaluation model; Power supply capacity; "N-x" criterion; Evaluated indexes	GENERATION; CAPABILITY	With the rapid increase in power loads, power supply capacity plays a significant role in power distribution network. However, attributing to the tremendous differences in regional economic development and diversified characteristics of power distribution network, power supply capacity should be evaluated based on regional differences to better promote the efficiency of distribution network. In this paper, the evaluated indexes including supply capacity reserve, supply capacity margin and supply capacity balance are defined, and the evaluated models for single and inter-layer equipment are established. Furthermore, a case study with ten different regions is employed to validate the proposed models. Results illustrate that considering the effects of "N-x" criterion, network structures and overload features, the proposed comprehensive models could quantitatively evaluate the supply capacity and offer better supports for planning and operation of large-scale power distribution network.																	0941-0643	1433-3058				MAY	2020	32	9			SI		5163	5171		10.1007/s00521-019-04010-5													
J								Evolutionary algorithms application for improving the tire rolling resistance based on Wismer-Luth model	NEURAL COMPUTING & APPLICATIONS										Evolutionary algorithms; BAT; Rolling resistance; Wismer-Luth model; Multi-pass; Optimization	RADIAL PLY; TRACTIVE PERFORMANCE; INFLATION PRESSURE; VEHICLE; VELOCITY	Soil and tire interaction is a complex process that involves the exchange of variable stresses along the contact area of soil and tire. Despite this complexity, the description of this process in the form of mathematical models has long been of interest to the researchers. The same complexity has led the wheels and soil interaction patterns to be constantly evolving and optimizing. This evolution has coincided with the scientific progress of mathematics, modeling and computer until today. Nowadays, optimizing and predicting a model based on input variables using machine learning techniques and conventional evolutionary algorithms play an important role in predicting the relationships between input and output. These methods can be far better than the conventional statistical techniques. The modeling and prediction of wheel rolling resistance on the soil have many parameters. Using new techniques such as genetic, BAT and PSO algorithms to optimize them seems to be suitable approaches. The aim of this research is to investigate and optimize the parameters of the Wismer-Luth model using the evolutionary algorithms. To improve the model, the variables of multi-pass, forward velocity and depth of the cone index, are also incorporated to the Wismer-Luth model, and the corresponding parameters are optimized with the BAT algorithm. Analysis of experimental data showed that the correlation of the output of the proposed model with the experimental data is 0.87 where it is 0.77 for the Wismer-Luth model. Furthermore, experimental results in this study showed that there is a significant relationship between rolling resistance and multi-pass effect, neglected in most models.																	0941-0643	1433-3058				MAY	2020	32	9			SI		5173	5183		10.1007/s00521-019-04012-3													
J								An enhanced associative learning-based exploratory whale optimizer for global optimization	NEURAL COMPUTING & APPLICATIONS										Nature-inspired computing; Metaheuristic; Optimization; Swarm intelligence	DIFFERENTIAL EVOLUTION; PARAMETER-ESTIMATION; PV CELLS; ALGORITHM; SEARCH; IDENTIFICATION; STRATEGY; NETWORK; ELPSO	Whale optimization algorithm (WOA) is a recent nature-inspired metaheuristic that mimics the cooperative life of humpback whales and their spiral-shaped hunting mechanism. In this research, it is first argued that the exploitation tendency of WOA is limited and can be considered as one of the main drawbacks of this algorithm. In order to mitigate the problems of immature convergence and stagnation problems, the exploitative and exploratory capabilities of modified WOA in conjunction with a learning mechanism are improved. In this regard, the proposed WOA with associative learning approaches is combined with a recent variant of hill climbing local search to further enhance the exploitation process. The improved algorithm is then employed to tackle a wide range of numerical optimization problems. The results are compared with different well-known and novel techniques on multi-dimensional classic problems and new CEC 2017 test suite. The extensive experiments and statistical tests show the superiority of the proposed BMWOA compared to WOA and several well-established algorithms.																	0941-0643	1433-3058				MAY	2020	32	9			SI		5185	5211		10.1007/s00521-019-04015-0													
J								Multiple-attribute group decision making for interval-valued intuitionistic fuzzy sets based on expert reliability and the evidential reasoning rule	NEURAL COMPUTING & APPLICATIONS										Interval-valued intuitionistic fuzzy sets; Expert reliability; Evidential reasoning rule; Multiple-attribute group decision making	AGGREGATION OPERATORS; LOCATION SELECTION; RESIDENTIAL HOUSE; VIKOR METHOD; MULTIMOORA; MANAGEMENT; SYSTEM; MODEL	This study proposes a novel fuzzy multiple-attribute group decision-making approach based on expert reliability and the evidential reasoning (ER) rule in an interval-valued intuitionistic fuzzy environment. First, to determine the reliabilities of experts, an objective method is developed by combining the similarity between the assessments provided before and after group discussion. Second, the proposed approach extends the ER rule to the case where belief degrees are intervals and employs it to combine experts' assessments. Hereinto, several optimization models are established to produce the aggregated assessments of the alternatives. Then, the overall priority degree of each alternative can be obtained according to the aggregated assessments and further utilized to yield a ranking of alternatives. Finally, a shopping center site selection problem is analyzed by the proposed approach to demonstrate its validity and applicability.																	0941-0643	1433-3058				MAY	2020	32	9			SI		5213	5234		10.1007/s00521-019-04016-z													
J								Weighted Huber constrained sparse face recognition	NEURAL COMPUTING & APPLICATIONS										Sparse coding; Face recognition; Robustness; ADMM	ROBUST; REPRESENTATION	Recently sparse coding based on regression analysis has been widely used in face recognition research. Most existing regression methods add an extra constraint factor to the coding residual to make the fidelity term in the l2 loss approach the Gaussian or Laplace distribution. But the essence of these methods is that only the fidelity term of l1 loss or l2 loss is used. In this paper, weighted Huber constrained sparse coding (WHCSC) is used to study the robustness of face recognition in occluded environments, and alternating direction method of multipliers is used to solve the problem of model minimization. In WHCSC, we propose a sparse coding with weight learning and use Huber loss to determine whether the fidelity is a l2 loss or l1loss. For the WHCSC model, the two kinds of classification modes and the two kinds of weight coefficients are further studied for the intra-class difference and the inter-class difference in the face image classification. Through a large number of experiments on a public face database, WHCSC shows strong robustness in face occlusion, corrosion and illumination changes comparing to the state-of-the-art methods.																	0941-0643	1433-3058				MAY	2020	32	9			SI		5235	5253		10.1007/s00521-019-04024-z													
J								Selection of mine development scheme based on similarity measure under fuzzy environment	NEURAL COMPUTING & APPLICATIONS										Mine development scheme; Similarity measure; Linguistic neutrosophic number; Weight model	GROUP DECISION-MAKING; WEIGHTS; NUMBERS; AHP	Mine development scheme selection is an important project in mine construction, as the benefit of a mine is directly affected by the quality of the scheme. In this paper, we consider the selection problem of mine development scheme under fuzzy environment. Firstly, linguistic neutrosophic numbers (LNNs) are chosen to fully describe people's linguistic evaluation information. To advance the following study, a new similarity measure of LNNs based on consistency degree is defined, and some important properties are also proved. Then, a new method based on similarity measure is proposed to cope with linguistic neutrosophic decision-making issues. It contains two weight determination models. One model takes the advantages of majority rule to calculate the experts' weights objectively. Another model uses the idea of the technique for order preference by similarity to ideal solution to obtain the weights of criteria. Lastly, when the evaluation index system of mine development scheme selection is constructed, the feasibility and strengths of our approach are indicated through an illustration and some related comparisons.																	0941-0643	1433-3058				MAY	2020	32	9			SI		5255	5266		10.1007/s00521-019-04026-x													
J								Salp swarm optimizer to solve optimal power flow comprising voltage stability analysis	NEURAL COMPUTING & APPLICATIONS										Power system operations; Optimization approaches; Voltage stability; Modal analysis	DIFFERENTIAL EVOLUTION ALGORITHM; BEE COLONY ALGORITHM; SYSTEMS; SENSITIVITY; WIND	A new attempt of employing salp swarm algorithm (SSA) to tackle the optimal power flow (OPF) problem is demonstrated in the current study. This aforementioned problem has four fitness functions to be optimized such as (1) the sum of generating units' fuel costs, (2) total network real power losses, (3) entire sum of voltage deviation of load buses, and (4) static voltage stability (VS) of electric power systems. At initial stage, these objective are solved one by one, and at a later stage, different vector objective functions are solved simultaneously by the SSA. The VS study based on a modal analysis is taken into consideration as an objective function. In this issue, the eigenvalues and eigenvectors of a reduced Jacobian matrix due to the reactive power change are figured. The smaller magnitude of eigenvalues indicates the vicinity to system voltage instability. As the magnitude of eigenvalues increases, the incremental voltage decreases, which means strong VS. The output active power of generating units, their voltages, transformers tap setting, and capacitor devices represent the search field. Two electric grids such as IEEE 57- and 118-bus electric networks are demonstrated to examine the performance of the SSA. The effectiveness of the SSA-OPF methodology is compared with that obtained by using other competing optimization methods. Furthermore, statistical performance measures comprising parametric and nonparametric tests are made and the simulation results are extensively verified which indicate a competition of the SSA with others algorithms in solving the OPF problem.																	0941-0643	1433-3058				MAY	2020	32	9			SI		5267	5283		10.1007/s00521-019-04029-8													
J								DKD-DAD: a novel framework with discriminative kinematic descriptor and deep attention-pooled descriptor for action recognition	NEURAL COMPUTING & APPLICATIONS										Action recognition; Deep learning; Kinematic feature; Attention mechanism	REPRESENTATION	In order to improve action recognition accuracy, the discriminative kinematic descriptor and deep attention-pooled descriptor are proposed. Firstly, the optical flow field is transformed into a set of kinematic fields with more discriminativeness. Subsequently, two kinematic features are constructed, which more accurately depict the dynamic characteristics of action subject from the multi-order divergence and curl fields. Secondly, by introducing both of the tight-loose constraint and anti-confusion constraint, a discriminative fusion method is proposed, which guarantees better within-class compactness and between-class separability, meanwhile reduces the confusion caused by outliers. Furthermore, a discriminative kinematic descriptor is constructed. Thirdly, a prediction-attentional pooling method is proposed, which accurately focuses its attention on the discriminative local regions. On this basis, a deep attention-pooled descriptor (DKD-DAD) is constructed. Finally, a novel framework with discriminative kinematic descriptor and deep attention-pooled descriptor is presented, which comprehensively obtains the discriminative dynamic and static information in a video. Consequently, accuracies are improved. Experiments on two challenging datasets verify the effectiveness of our methods.																	0941-0643	1433-3058				MAY	2020	32	9			SI		5285	5302		10.1007/s00521-019-04030-1													
J								Minimization of test time in system on chip using artificial intelligence-based test scheduling techniques	NEURAL COMPUTING & APPLICATIONS										Artificial intelligence; Test scheduling; Ant colony optimization; Artificial bee colony algorithm; Bat algorithm; Firefly algorithm	ANT COLONY OPTIMIZATION; FIREFLY ALGORITHM; DESIGN	System on chip (SoC) is a microchip which integrates many semiconductor devices into a single chip. The complete system that is integrated with many components and circuits has to be tested for its performance. At the same time, testing of SoC should not affect the final cost of the chip. The production cost of each and every chip can be reduced by minimizing the test time of each SoC. The testing time of each SoC can be minimized by using test scheduling techniques more efficiently and effectively. In this paper, artificial intelligence-based natural-inspired techniques such as ACO, MACO, ABC, bat and firefly algorithms are proposed to perform effective test scheduling, thereby reducing the total cost of the chip. The proposed algorithms are implemented on d695 and p22810 benchmark circuits for various values of TAM widths. The performance of the various algorithms was evaluated, and it is inferred that among the several algorithms used bat algorithm performs much better in reducing the overall testing time of SoC, and hence, the SoC cost is also reduced.																	0941-0643	1433-3058				MAY	2020	32	9			SI		5303	5312		10.1007/s00521-019-04039-6													
J								Fair navigation planning: A resource for characterizing and designing fairness in mobile robots	ARTIFICIAL INTELLIGENCE										Motion planning; Robot navigation; Algorithmic fairness; Ethics; Responsible innovation		In recent years, the development and deployment of autonomous systems such as mobile robots have been increasingly common. Investigating and implementing ethical considerations such as fairness in autonomous systems is an important problem that is receiving increased attention, both because of recent findings of their potential undesired impacts and a related surge in ethical principles and guidelines. In this paper we take a new approach to considering fairness in the design of autonomous systems: we examine fairness by obtaining formal definitions, applying them to a system, and simulating system deployment in order to anticipate challenges. We undertake this analysis in the context of the particular technical problem of robot navigation. We start by showing that there is a fairness dimension to robot navigation, and we then collect and translate several formal definitions of distributive justice into the navigation planning domain. We use a walkthrough example of a rescue robot to bring out design choices and issues that arise during the development of a fair system. We discuss indirect discrimination, fairness-efficiency trade-offs, the existence of counter-productive fairness definitions, privacy and other issues. Finally, we elaborate on important aspects of a research agenda and reflect on the adequacy of our methodology in this paper as a general approach to responsible innovation in autonomous systems. (C) 2020 Elsevier B.V. All rights reserved.																	0004-3702	1872-7921				MAY	2020	282								103259	10.1016/j.artint.2020.103259													
J								Automated construction of bounded-loss imperfect-recall abstractions in extensive-form games	ARTIFICIAL INTELLIGENCE										Extensive-form games; Information abstraction; Imperfect recall; Nash equilibrium; Fictitious play; Counterfactual regret minimization	POKER	Extensive-form games (EFGs) model finite sequential interactions between players. The amount of memory required to represent these games is the main bottleneck of algorithms for computing optimal strategies and the size of these strategies is often impractical for real-world applications. A common approach to tackle the memory bottleneck is to use information abstraction that removes parts of information available to players thus reducing the number of decision points in the game. However, existing information-abstraction techniques are either specific for a particular domain, they do not provide any quality guarantees, or they are applicable to very small subclasses of EFGs. We present domain-independent abstraction methods for creating imperfect recall abstractions in extensive-form games that allow computing strategies that are (near) optimal in the original game. To this end, we introduce two novel algorithms, FPIRA and CFR+IRA, based on fictitious play and counterfactual regret minimization. These algorithms can start with an arbitrary domain specific, or the coarsest possible, abstraction of the original game. The algorithms iteratively detect the missing information they require for computing a strategy for the abstract game that is (near) optimal in the original game. This information is then included back into the abstract game. Moreover, our algorithms are able to exploit imperfect-recall abstractions that allow players to forget even history of their own actions. However, the algorithms require traversing the complete unabstracted game tree. We experimentally show that our algorithms can closely approximate Nash equilibrium of large games using abstraction with as little as 0.9% of information sets of the original game. Moreover, the results suggest that memory savings increase with the increasing size of the original games. (C) 2020 Elsevier B.V. All rights reserved.																	0004-3702	1872-7921				MAY	2020	282								103248	10.1016/j.artint.2020.103248													
J								Robust learning with imperfect privileged information	ARTIFICIAL INTELLIGENCE										Learning using privileged information; Classification; Support vector machine		In the learning using privileged information (LUPI) paradigm, example data cannot always be clean, while the gathered privileged information can be imperfect in practice. Here, imperfect privileged information can refer to auxiliary information that is not always accurate or perturbed by noise, or alternatively to incomplete privileged information, where privileged information is only available for part of the training data. Because of the lack of clear strategies for handling noise in example data and imperfect privileged information, existing learning using privileged information (LUPI) methods may encounter serious issues. Accordingly, in this paper, we propose a Robust SVM+ method to tackle imperfect data in LUPI. In order to make the SVM+ model robust to noise in example data and privileged information, Robust SVM+ maximizes the lower bound of the perturbations that may influence the judgement based on a rigorous theoretical analysis. Moreover, in order to deal with the incomplete privileged information, we use the available privileged information to help us in approximating the missing privileged information of training data. The optimization problem of the proposed method can be efficiently solved by employing a two-step alternating optimization strategy, based on iteratively deploying off-the-shelf quadratic programming solvers and the alternating direction method of multipliers (ADMM) technique. Comprehensive experiments on real-world datasets demonstrate the effectiveness of the proposed Robust SVM+ method in handling imperfect privileged information. (C) 2020 Elsevier B.V. All rights reserved.																	0004-3702	1872-7921				MAY	2020	282								103246	10.1016/j.artint.2020.103246													
J								Rethinking epistemic logic with belief bases	ARTIFICIAL INTELLIGENCE										Epistemic logic; Theory of Mind	KNOWLEDGE; CONTRACTION; INFORMATION; AWARENESS; COGNITION; DYNAMICS; KERNEL; MIND	We introduce a new semantics for a family of logics of explicit and implicit belief based on the concept of multi-agent belief base. Differently from standard semantics for epistemic logic in which the notions of possible world and doxastic/epistemic alternative are primitive, in our semantics they are non-primitive but are computed from the concept of belief base. We provide complete axiomatizations and prove decidability for our logics via finite model arguments. Furthermore, we provide polynomial embeddings of our logics into Fagin & Halpern's logic of general awareness and establish complexity results via the embeddings. We also present variants of the logics incorporating different forms of epistemic introspection for explicit and/or implicit belief and provide complexity results for some of these variants. Finally, we present a number of dynamic extensions of the static framework by informative actions of both public and private type, including public announcement, belief base expansion and forgetting. We illustrate the application potential of the logical framework with the aid of a concrete example taken from the domain of conversational agents. (C) 2020 Elsevier B.V. All rights reserved.																	0004-3702	1872-7921				MAY	2020	282								103233	10.1016/j.artint.2020.103233													
J								Autoepistemic equilibrium logic and epistemic specifications	ARTIFICIAL INTELLIGENCE										Answer set programming; Epistemic specifications; Epistemic logic programs; Here-and-there logic; Equilibrium logic; Modal logic	PLANNING ATTACKS; NEGATION	Epistemic specifications extend disjunctive answer-set programs by an epistemic modal operator that may occur in the body of rules. Their semantics is in terms of world views, which are sets of answer sets, and the idea is that the epistemic modal operator quantifies over these answer sets. Several such semantics were proposed in the literature. We here propose a new semantics that is based on the logic of here-and-there: we add epistemic modal operators to its language and define epistemic here-and-there models. We then successively define epistemic equilibrium models and autoepistemic equilibrium models. The former are obtained from epistemic here-and-there models in exactly the same way as Pearce's equilibrium models are obtained from here-and-there models, viz. by minimising truth; they provide an epistemic extension of equilibrium logic. The latter are obtained from the former by maximising the set of epistemic possibilities, and they provide a new semantics for Gelfond's epistemic specifications. For both semantics we establish a strong equivalence result: we characterise strong equivalence of two epistemic programs by means of logical equivalence in epistemic here-and-there logic. We finally compare our approach to the existing semantics of epistemic specifications and discuss which formalisms provide more intuitive results by pointing out some formal properties a semantics proposal should satisfy. (C) 2020 Elsevier B.V. All rights reserved.																	0004-3702	1872-7921				MAY	2020	282								103249	10.1016/j.artint.2020.103249													
J								A comparison study of optimal scale combination selection in generalized multi-scale decision tables	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Granular computing; Information systems; Multi-scale information tables; Rough sets; Scale combinations	DEMPSTER-SHAFER THEORY; RULE ACQUISITION; INFORMATION GRANULATION; KNOWLEDGE ACQUISITION; ROUGH SETS; APPROXIMATION; SYSTEMS	Traditional rough set approach is mainly used to unravel rules from a decision table in which objects can possess a unique attribute-value. In a real world data set, for the same attribute objects are usually measured at different scales. The main objective of this paper is to study optimal scale combinations in generalized multi-scale decision tables. A generalized multi-scale information table is an attribute-value system in which different attributes are measured at different levels of scales. With the aim of investigating knowledge representation and knowledge acquisition in inconsistent generalized multi-scale decision tables, we first introduce the notion of scale combinations in a generalized multi-scale information table. We then formulate information granules with different scale combinations in multi-scale information systems and discuss their relationships. Furthermore, we define lower and upper approximations of sets with different scale combinations and examine their properties. Finally, we examine optimal scale combinations in inconsistent generalized multi-scale decision tables. We clarify relationships among different concepts of optimal scale combinations in inconsistent generalized multi-scale decision tables.																	1868-8071	1868-808X				MAY	2020	11	5					961	972		10.1007/s13042-019-00954-1													
J								An efficient three-way clustering algorithm based on gravitational search	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Clustering; Uncertainty; Three-way clustering; Gravitational search; Three-way decisions	FUZZY; DECISIONS	There are three types of relationships between an object and a cluster, namely, belong-to definitely, uncertain and not belong-to definitely. Most of the existing clustering algorithms represent a cluster with a single set and they are the two-way clustering algorithms since they just reflect two relationships. By contrast, the three-way clustering can reflect intuitively the three types of relationships with a pair of sets. However, the three-way clustering algorithms usually need to know the thresholds in advance in order to obtain the three types of relationships. To address the problem, we propose an efficient three-way clustering algorithm based on the idea of universal gravitation in this paper. The proposed method can adjust the thresholds automatically in the process of clustering and obtain more detailed ascription relation between objects and clusters. Furthermore, to guarantee the integrity of the work, we also put forward a two-way clustering algorithm to obtain the conventional two-way result. The experimental results show that the proposed algorithm is not only effective to obtain the three-way clustering result from the two-way clustering result automatically, but also it is in a better performance at the accuracy, F-measure, NMI and RI than the compared algorithms in most cases.																	1868-8071	1868-808X				MAY	2020	11	5					1003	1016		10.1007/s13042-019-00988-5													
J								The construction of attribute (object)-oriented multi-granularity concept lattices	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Attribute (object)-oriented concept lattice; Attribute granularity; Granular computing; Zoom-in algorithm; Zoom-out algorithm	OPTIMAL SCALE SELECTION; GRANULATION	How to reduce the complexity of lattice construction is an important research topic in formal concept analysis. Based on granularity tree, the relationship between the extent and the intent of the attribute (object)-oriented concept before and after granularity transformation are investigated. Then, zoom algorithms for attribute (object)-oriented concept lattices are proposed. Specifically, zoom-in algorithm is applied to change the attribute granularity from coarse-granularity to fine-granularity, and zoom-out algorithm achieves changing the attribute granularity from fine-granularity to coarse-granularity. Zoom algorithms deal with the problems of fast construction of the attribute (object)-oriented multi-granularity concept lattices. By using zoom algorithms, the attribute (object)-oriented concept lattice based on different attribute granularity can be directly generated through the existing attribute (object)-oriented concept lattice. The proposed algorithms not only reduce the computational complexity of concept lattice construction, but also facilitate further data mining and knowledge discovery in formal contexts. Furthermore, the transformation algorithms among three kinds of concept lattice are proposed.																	1868-8071	1868-808X				MAY	2020	11	5					1017	1032		10.1007/s13042-019-00955-0													
J								Similarity-based attribute reduction in rough set theory: a clustering perspective	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Attribute reduction; Rough set theory; The intra-class similarity; The inter-class similarity	SELECTION; DISCERNIBILITY	Attribute reduction is one of the most important research issues in the rough set theory. The purpose of attribute reduction is to find a minimal attribute subset that satisfies some specific criteria, while the minimal attribute subset is called attribute reduct. In this paper, we define a similarity-based attribute reduct based on a clustering perspective. Each decision class is treated as a cluster, and the defined similarity-based attribute reduct can maintain or increase the discriminating ability of different clusters in the case of removing redundant attributes. In view of this, firstly, we define the intra-class similarity for objects in the same decision class and the inter-class similarity for objects between different decision classes. Secondly, we define a similarity-based attribute reduct by maximizing intra-class similarity and minimizing inter-class similarity in the rough set model. Thirdly, by considering the heuristic search strategy, we also design a corresponding reduction method for the proposed attribute reduct. The experimental results indicate that compared with other representative attribute reducts, our proposed attribute reduct can significantly improve the classification performance.																	1868-8071	1868-808X				MAY	2020	11	5					1047	1060		10.1007/s13042-019-00959-w													
J								Optimal scale selection by integrating uncertainty and cost-sensitive learning in multi-scale decision tables	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Multi-scale decision tables; Optimal scale selection; Sequential three-way decisions; Uncertainty; Cost-sensitive learning	SEQUENTIAL 3-WAY DECISION; ROUGH SETS; RULE ACQUISITION; MODEL; GRANULATION	Optimal scale selection is an important issue in the study of multi-scale decision tables. Most existing optimal scale selection methods have been designed from the perspective of consistency or uncertainty, and cost as well as user requirements or preferences in practical applications has not been considered. It is well known that the uncertainty of decision making in different levels of scale varies in sequential three-way decision models. Furthermore, test cost depends on the scale, and delayed decisions may cause delay cost. In practical applications, both uncertainty and cost are supposed to be considered. Therefore, it is worthwhile to introduce cost-sensitive learning into multi-scale decision tables and select the optimal scale by comprehensively considering uncertainty and cost. In this study, uncertainty is firstly quantified, and a novel cost constitution is defined in sequential three-way decision models. In addition, a multi-scale decision information system based on test cost and delay cost is proposed. Then, to obtain the optimal scale with the minimum uncertainty and cost, an optimal scale selection model is established with the constraint of user requirements. Furthermore, an improved optimal scale selection model considering user preferences is proposed by introducing the ideal solution to resolve conflicts among objectives. Finally, the effectiveness of the optimal scale selection model is verified through experiments, and a comparative experimental analysis demonstrates that the proposed model is more consistent with actual user requirements than existing models.																	1868-8071	1868-808X				MAY	2020	11	5					1095	1114		10.1007/s13042-020-01101-x													
J								Discernible neighborhood counting based incremental feature selection for heterogeneous data	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Incremental feature selection; Feature selection; Neighborhood rough set; Heterogeneous data	ATTRIBUTE REDUCTION APPROACH; ROUGH SET APPROACH; DYNAMIC DATA; ALGORITHM; DISCRETIZATION; APPROXIMATIONS; SYSTEMS	Incremental feature selection refreshes a subset of information-rich features from added-in samples without forgetting the previously learned knowledge. However, most existing algorithms for incremental feature selection have no explicit mechanisms to handle heterogeneous data with symbolic and real-valued features. Therefore, this paper presents an incremental feature selection method for heterogeneous data with the sequential arrival of samples in group. Discernible neighborhood counting that measures different types of features, is first introduced to establish a framework for feature selection from heterogeneous data. With the arrival of new samples, the discernible neighborhood counting of a feature subset is then updated to reveal the incremental feature selection scheme. This scheme determines the criterion for efficiently adding informative features and deleting redundant features. Based on the incremental scheme, our incremental feature selection algorithm is further formulated to select valuable features from heterogeneous data. Extensive experiments are finally conducted to demonstrate the effectiveness and the efficiency of the proposed incremental feature selection algorithm.																	1868-8071	1868-808X				MAY	2020	11	5					1115	1127		10.1007/s13042-019-00997-4													
J								A Projection-Based Method for Shape Measurement	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Circularity; Shape measurement; Evaluation criterion	FOURIER DESCRIPTOR; MOMENT INVARIANT; RADON-TRANSFORM; CIRCULARITY; ORIENTATION; ELLIPTICITY; APPROXIMATION; RECOGNITION; INFORMATION; ALGORITHM	This work addresses two main contributions for shape measurement: First, a new circularity measure for planar shapes is introduced based on their geometrical properties in the projection space of Radon transform. Second, a general-purpose evaluation criterion, power of discrimination, for assessing the efficiency of a shape measure is proposed. The new measure ranges over the interval [0, 1] and produces the value 1 if and only if the measured shape is a perfect circle. The proposed measure is invariant with respect to translation, rotation and scaling transformations. Moreover, it is also robust against border distortion of shapes. It is theoretically well founded and can be extended to other problems of shape measurement. Our approach can deal with complex shapes composed of connected components that cannot be handled by classical contour-based methods. Several experiments show its good behavior and demonstrate the efficiency and applicability of our proposed measure. Finally, we also consider our proposed evaluation criterion for assessing different circularity measures.																	0924-9907	1573-7683				MAY	2020	62	4					489	504		10.1007/s10851-019-00932-w													
J								Denoising Color Images Based on Local Orientation Estimation and CNN Classifier	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Color image; Impulse noise; Orientation estimation; Quaternion; Structure-adaptive filter; Convolutional neural network	IMPULSIVE NOISE REMOVAL; SPARSE REPRESENTATION; MATRIX; DIFFUSION; FILTER; REGULARIZATION; RESTORATION; ALGORITHM; GRADIENT	A structure-adaptive vector filter for removal of impulse noise from color images is presented. The proposed method is based on local orientation estimation. A color image is represented in quaternion form, and then, quaternion Fourier transform is used to compute the orientation of the pattern in a local neighborhood. Since the computation in quaternion frequency domain is extremely time-consuming, we prove a theorem that the integral of the product of frequency variables and the magnitude of quaternion frequency signals can be computed directly in spatial domain, which results that the color orientation detection problem can be solved in spatial domain. Based on the local orientation and orientation strength, the size, shape, and orientation of the support window of vector median filter (VMF) are adaptively determined, leading to an effective structure-adaptive VMF. Unlike the classical VMF restricting the output to the existing color samples, this paper computes the output of VMF over the entire 3D data space, which boosts the filtering performance effectively. To further improve denoising effect, a deep convolutional neural network is employed to detect impulse noise in color images and integrated into the proposed denoising framework. The experimental results exhibit the effectiveness of the proposed denoiser by showing significant performance improvements both in noise suppression and in detail preservation, compared to other color image denoising methods.																	0924-9907	1573-7683				MAY	2020	62	4					505	531		10.1007/s10851-019-00942-8													
J								Nonlocal Elastica Model for Sparse Reconstruction	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Euler's elastica; Nonlocal regularization; Sparse reconstruction; ADMM	COMPRESSED-SENSING MRI; EULERS ELASTICA; IMAGE; REGULARIZATION; SEGMENTATION; ALGORITHM; TV; DEPTH	In view of the exceptional ability of curvature in connecting missing edges and structures, we propose novel sparse reconstruction models via the Euler's elastica energy. In particular, we firstly extend the Euler's elastica regularity into the nonlocal formulation to fully take the advantages of the pattern redundancy and structural similarity in image data. Due to its non-convexity, non-smoothness and nonlinearity, we regard both local and nonlocal elastica functional as the weighted total variation for a good trade-off between the runtime complexity and performance. The splitting techniques and alternating direction method of multipliers (ADMM) are used to achieve efficient algorithms, the convergence of which is also discussed under certain assumptions. The weighting function occurred in our model can be well estimated according to the local approach. Numerical experiments demonstrate that our nonlocal elastica model achieves the state-of-the-art reconstruction results for different sampling patterns and sampling ratios, especially when the sampling rate is extremely low.																	0924-9907	1573-7683				MAY	2020	62	4					532	548		10.1007/s10851-019-00943-7													
J								Analysis of organophosphate pesticides in surface water-Comparison of method optimization approaches	JOURNAL OF CHEMOMETRICS										design of experiment; LC-MSMS; liquid-liquid micro-extraction; organophosphate pesticides; water	LIQUID-LIQUID MICROEXTRACTION; MASS-SPECTROMETRY; RESIDUES; CHROMATOGRAPHY; EXTRACTION; CONTAMINANTS; REMOVAL; DESIGN; TIME; SEA	A multiresidue method optimization was conducted using the design of experiment approach. Out of 43 tested organophosphate compounds, 27 could be validated in surface water, yielding limits of detection of 0.6 to 2.5 ng L -1 by dispersive liquid-liquid micro-extraction and subsequent liquid chromatography-tandem quadrupole mass spectrometry (LC-MSMS) detection. For the optimization of the sample preparation, the factors sonication time, sample volume, binary extraction ratio, and addition of salt have been investigated by the Taguchi design of experiment approach. One hundred fifty milliliters sample were extracted for 1 minute with 400 mu L acetone and 800 mu L tetrachloroethene (TCE) three times subsequently. No salts were added. Combined extracts were reconstituted in 65 mu L methanol, ready for analysis. The design of experiment approach for method optimization is discussed thoroughly. The comparison to one factor at a time approach focuses on overall experimental expenditure, factor space coverage, acquired information regarding factor interactions, and statistical confirmation of results. Based on two similar analytical methods, it is illustrated that fewer but statistically derived experiments yield more information about the overall system enabling enhanced evaluation of experiment results.																	0886-9383	1099-128X				MAY	2020	34	5							e3220	10.1002/cem.3220													
J								Fuzzy directional enlacement landscapes for the evaluation of complex spatial relations	PATTERN RECOGNITION										Evaluation of spatial relations; Fuzzy directional landscapes; Enlacement; Surrounding	POSITION; REPRESENTATION; DEFINITION; HISTOGRAMS	Structural spatial relations between image components are fundamental in the human perception of image similarity, and constitute a challenging topic in the domain of image analysis. By definition, some specific relations are ambiguous and difficult to formalize precisely by humans. In this work, we deal with the issue of evaluating complex spatial configurations, where objects can surround each other, potentially with multiple levels of depth. Based on a recently introduced spatial relation called enlacement, which generalizes the idea of surrounding for arbitrary objects, we propose a fuzzy landscape model that allows both to visualize and evaluate this relation directly in the image space, following different directions. Experiments on several characteristic examples highlight the interest and the behavior of this approach, allowing for rich interpretations of these complex spatial configurations. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				MAY	2020	101								107185	10.1016/j.patcog.2019.107185													
J								Contextual deconvolution network for semantic segmentation	PATTERN RECOGNITION										Semantic segmentation; Deconvolution network; Channel contextual module; Spatial contextual module		In this paper, we propose a Contextual Deconvolution Network (CDN) and focus on context association in decoder network. Specifically, in upsampling path, we introduce two types of contextual modules to model the interdependencies of features in channel and spatial dimensions respectively. The channel contextual module captures image-level semantic information by aggregating the feature maps across spatial dimensions, and clarifies global ambiguity of features. Meanwhile, the spatial contextual module obtains patch-level semantic context by learning a spatial weight map, and enhance the feature discrimination. We embed the two contextual modules into individual components of the decoder network, thus improving the representation power and gaining more precise segment results. Thorough evaluations are performed on four challenging datasets, i.e., PASCAL VOC 2012, ADE20K, PASCAL-Context and Cityscapes dataset. Our approach achieves competitive performance with state-of-the-art models on PASCAL VOC 2012, ADE20K and Cityscapes dataset, and new state-of-the-art performance on PASCAL-Context dataset. (C) 2019 Published by Elsevier Ltd.																	0031-3203	1873-5142				MAY	2020	101								107152	10.1016/j.patcog.2019.107152													
J								Identifying the best data-driven feature selection method for boosting reproducibility in classification tasks	PATTERN RECOGNITION										Feature selection methods; Multi-graph topological analysis; Feature reproducibility; Biomarker discovery; Morphological brain network; Neurological disorders; Connectomics; Cross-validation	ALZHEIMERS-DISEASE; MUTUAL INFORMATION; NETWORKS; ALGORITHMS; CENTRALITY; DIAGNOSIS; AUTISM; BIOMARKERS; STABILITY; PATTERN	Considering the proliferation of extremely high-dimensional data in many domains including computer vision and healthcare applications such as computer-aided diagnosis (CAD), advanced techniques for reducing data dimensionality and identifying the most relevant features for a given classification task such as distinguishing between healthy and disordered brain states are needed. Despite the existence of many works on boosting the classification accuracy using a particular feature selection (FS) method, choosing the best one from a large pool of existing FS techniques for boosting feature reproducibility within a dataset of interest remains a formidable challenge to tackle. Notably, a good performance of a particular FS method does not necessarily imply that the experiment is reproducible and that the features identified are optimal for the entirety of the samples. Essentially, this paper presents the first attempt to address the following challenge: "Given a set of different feature selection methods {FS1,...,FSK}, and a dataset of interest, how to identify the most reproducible and 'trustworthy' connectomic features that would produce reliable biomarkers capable of accurately differentiate between two specific conditions?" To this aim, we propose FS-Select framework which explores the relationships among the different FS methods using a multi-graph architecture based on feature reproducibility power, average accuracy, and feature stability of each FS method. By extracting the 'central' graph node, we identify the most reliable and reproducible FS method for the target brain state classification task along with the most discriminative features fingerprinting these brain states. To evaluate the reproducibility power of FS-Select, we perturbed the training set by using different cross-validation strategies on a multi-view small-scale connectomic dataset (late mild cognitive impairment vs Alzheimer's disease) and large-scale dataset including autistic vs healthy subjects. Our experiments revealed reproducible connectional features fingerprinting disordered brain states. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				MAY	2020	101								107183	10.1016/j.patcog.2019.107183													
J								Ensemble adversarial black-box attacks against deep learning systems	PATTERN RECOGNITION										Black-box attack; Vulnerability; Ensemble adversarial attack; Diversity; Transferability	NETWORK	Deep learning (DL) models, e.g., state-of-the-art convolutional neural networks (CNNs), have been widely applied into security sensitivity tasks, such as face payment, security monitoring, automated driving, etc. Then their vulnerability analysis is an emergent topic, especially for black-box attacks, where adversaries do not know the model internal architectures or training parameters. In this paper, two types of ensemble-based black-box attack strategies, selective cascade ensemble strategy (SCES) and stack parallel ensemble strategy (SPES), are proposed to explore the vulnerability of DL system and potential factors that contribute to the high-efficiency attacks are explored. SCES adopts a boosting structure of ensemble learning and SPES employs a bagging structure. Moreover, two pairwise and non-pairwise diversity measures are adopted to examine the relationship between the diversity in substitutes ensembles and transferability of generated adversarial examples. Experimental results show that proposed ensemble adversarial black-box attack strategies can successfully attack the DL system with some defense mechanism, such as adversarial training and ensemble adversarial training. The experimental results also show the greater the diversity in substitute ensembles enables stronger transferability. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				MAY	2020	101								107184	10.1016/j.patcog.2019.107184													
J								Towards explaining anomalies: A deep Taylor decomposition of one-class models	PATTERN RECOGNITION										Outlier detection; Explainable machine learning; Deep Taylor decomposition; Kernel machines; Unsupervised learning	NEURAL-NETWORK; SUPPORT VECTOR; VISUALIZATION	Detecting anomalies in the data is a common machine learning task, with numerous applications in the sciences and industry. In practice, it is not always sufficient to reach high detection accuracy, one would also like to be able to understand why a given data point has been predicted to be anomalous. We propose a principled approach for one-class SVMs (OC-SVM), that draws on the novel insight that these models can be rewritten as distance/pooling neural networks. This 'neuralization' step lets us apply deep Taylor decomposition (DTD), a methodology that leverages the model structure in order to quickly and reliably explain decisions in terms of input features. The proposed method (called 'OC-DTD') is applicable to a number of common distance-based kernel functions, and it outperforms baselines such as sensitivity analysis, distance to nearest neighbor, or edge detection. (C) 2020 The Authors. Published by Elsevier Ltd.																	0031-3203	1873-5142				MAY	2020	101								107198	10.1016/j.patcog.2020.107198													
J								Enhancing deep neural networks via multiple kernel learning	PATTERN RECOGNITION										Deep neural networks; Deep learning; Multiple kernel learning; Ensemble learning		Deep neural networks and Multiple Kernel Learning are representation learning methodologies of widespread use and increasing success. While the former aims at learning representations through a hierarchy of features of increasing complexity, the latter provides a principled approach for the combination of base representations. In this paper, we introduce a general framework in which the internal representations computed by a deep neural network are optimally combined by means of Multiple Kernel Learning. The resulting ensemble methodology is instantiated for Multi-layer Perceptrons architectures (both fully trained and with random-weights), and for Convolutional Neural Networks. Experimental results on several benchmark datasets concretely show the advantages and potentialities of the proposed approach. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				MAY	2020	101								107194	10.1016/j.patcog.2020.107194													
J								UcoSLAM: Simultaneous localization and mapping by fusion of keypoints and squared planar markers	PATTERN RECOGNITION										SLAM; KeyPoints; Fiducial Markers; Marker Mapping; ArUco	PLACE RECOGNITION; GENERATION; SLAM	Simultaneous Localization and Mapping is the process of simultaneously creating a map of the environment while navigating in it. Most of the SLAM approaches use natural features (e.g. keypoints) that are unstable over time, repetitive in many cases or their number insufficient for a robust tracking (e.g. in indoor buildings). Other researchers, on the other hand, have proposed the use of artificial landmarks, such as squared fiducial markers, placed in the environment to help tracking and relocalization. This paper proposes a novel SLAM approach by fusing natural and artificial landmarks in order to achieve long-term robust tracking in many scenarios. Our method has been compared to the start-of-the-art methods ORB-SLAM2 [1], LDSO [2] and SPM-SLAM [3] in the public datasets Kitti [4], Euroc-MAV [5], TUM [6] and SPM [3], obtaining better precision, robustness and speed. Our tests also show that the combination of markers and keypoints achieves better accuracy than each one of them independently. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				MAY	2020	101								107193	10.1016/j.patcog.2019.107193													
J								Three-step action search networks with deep Q-learning for real-time object tracking	PATTERN RECOGNITION										Object tracking; Deep Q-learning; Action search network	VISUAL TRACKING	Sliding window and candidate sampling are two widely used search strategies for visual object tracking, but they are far behind real-time. By treating the tracking problem as a three-step decision-making process, a novel tracking network, which explores only three small subsets of candidate regions, is developed to achieve faster (real-time) localization of the target object along the frames in a video. A convolutional neural network agent is formulated to interact with a video over time, and two action-value functions are exploited to learn a favorable policy off-line to determine the best action for visual object tracking. Our model is trained in a collaborative learning way by using action classification and cumulative reward approximation in reinforcement learning. We have evaluated our proposed tracker against a number of state-of-the-art ones over three popular tracking benchmarks including OTB-2013, OTB-2015, and VOT2017. The experimental results have demonstrated that our proposed method can achieve very competitive performance on real-time object tracking. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				MAY	2020	101								107188	10.1016/j.patcog.2019.107188													
J								Multi-task CNN for restoring corrupted fingerprint images	PATTERN RECOGNITION										Fingerprint image enhancement; Fingerprint recognition; Convolutional neural networks; Multi-task learning	GABOR FILTER; RIDGE STRUCTURE; ENHANCEMENT; ALGORITHM	Fingerprint image enhancement is one of the fundamental modules in an automated fingerprint recognition system (AFRS). While the performance of AFRS advances with sophisticated fingerprint matching algorithms, poor fingerprint image quality remains a major issue to achieve accurate fingerprint recognition. In this paper, we present a multi-task convolutional neural network (CNN) based method to recover fingerprint ridge structures from corrupted fingerprint images. By learning from the noises and corruptions caused by various undesirable conditions of finger and sensor, the proposed CNN model consists of two streams that reconstruct the fingerprint image and orientation field simultaneously. The enhanced fingerprint is further refined using the orientation field information. Moreover, we create a deliberately corrupted fingerprint image dataset associated with ground truth images to facilitate the supervised learning of the proposed CNN model. Experimental results show significant improvement on both image quality and fingerprint matching accuracy after applying the proposed fingerprint image enhancement technique to several well-known fingerprint datasets. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				MAY	2020	101								107203	10.1016/j.patcog.2020.107203													
J								Image analysis by log-polar Exponent-Fourier moments	PATTERN RECOGNITION										Exponent-Fourier moments; Log-polar coordinates; Pseudo-polar Fourier transform; Frequency domain interpolation; Scaling and rotation-invariant	ORTHOGONAL-MOMENTS; ROBUST; INVARIANTS; WATERMARKING; RECONSTRUCTION; REPRESENTATION; RECOGNITION; ACCURATE	Moments, as a popular class of the global invariant image descriptors, have been widely used in image analysis, pattern recognition and computer vision applications. Exponent-Fourier moments (EFMs) are a new set of orthogonal moments based on exponential functions, which are suitable for image analysis and rotation invariant pattern recognition. However, EFMs lack natively the scaling-invariant property. In addition, they always suffer from high time complexity, numerical instability, and reconstruction error, especially for higher order of moments. In this paper, we introduce a class of scaling and rotation-invariant orthogonal moments, named Log-Polar Exponent-Fourier moments (LPEFMs), by extending the classical EFMs to the log-polar coordinates. Firstly, we redefined the EFMs' basis functions in log-polar domain instead of Cartesian/polar coordinate domain in order to obtain the scaling-invariant property. Then, we develop a new framework for computing the LPEFMs by using pseudo-polar Fourier transform and frequency domain interpolation, which result in better image representation capability, numerical stability, and computational speed. Compared with the classical EFMs, the proposed LPEFMs have four advantages, scaling invariance, speed, accuracy and stability. Theoretical analysis and simulation results are provided to validate the proposed image moment and to compare its performance with previous works. (C) 2019 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				MAY	2020	101								107177	10.1016/j.patcog.2019.107177													
J								Discriminative distribution alignment: A unified framework for heterogeneous domain adaptation	PATTERN RECOGNITION										Heterogeneous domain adaptation; Subspace learning; Classifier adaptation; Distribution alignment; Discriminative embedding		Heterogeneous domain adaptation (HDA) aims to leverage knowledge from a source domain for helping learn an accurate model in a heterogeneous target domain. HDA is exceedingly challenging since the feature spaces of domains are distinct. To tackle this issue, we propose a unified learning framework called Discriminative Distribution Alignment (DDA) for deriving a domain-invariant subspace. The proposed DDA can simultaneously match the discriminative directions of domains, align the distributions across domains, and enhance the separability of data during adaptation. To achieve this, DDA trains an adaptive classifier by both reducing the distribution divergence and enlarging distances between class centroids. Based on the proposed DDA framework, we further develop two methods, by embedding the cross-entropy loss and squared loss into this framework, respectively. We conduct experiments on the tasks of categorization across domains and modalities. Experimental results clearly demonstrate that the proposed DDA outperforms several state-of-the-art models. (C) 2020 Elsevier Ltd. All rights reserved.																	0031-3203	1873-5142				MAY	2020	101								107165	10.1016/j.patcog.2019.107165													
J								On pseudo-eBE-algebras	SOFT COMPUTING										(Pseudo) BE-algebra; (Distributive; commutative) pseudo-eBE-algebra; Ideal; (Quasi; simple; pseudo) filter	COMMUTATIVE DEDUCTIVE SYSTEMS; BCK ALGEBRAS	In this paper, we define the notion of a pseudo-eBE-algebra as an extension of a pseudo-BE-algebra, and it is studied in detail. The construction of an eBE-algebra from a pseudo-eBE-algebra is given. Further, the notions of filters and ideals are considered. The classes of distributive and commutative pseudo-eBE-algebras are introduced and investigated. We prove that for a distributive pseudo-eBE-algebra filters coincide with ideals. Also, some types of filters are defined and the relationship between these is investigated.																	1432-7643	1433-7479				MAY	2020	24	10					7005	7020		10.1007/s00500-020-04810-1													
J								Probabilistic norms on the homeomorphisms of a group	SOFT COMPUTING										Probabilistic normed groups; Probabilistic Metric groups; Continuity of auto-homeomorphisms; Probabilistic norm admissibility condition; Equal norms		In this paper, a probabilistic metric on the set of all auto-homeomorphisms of a group is presented. We show that the defined probabilistic group metric is right invariant and it implies a probabilistic group norm. In addition, by the probabilistic norm admissibility condition, we study the uniform continuity of homeomorphisms, and finally, we prove some theorems about topologically equivalent probabilistic norms.																	1432-7643	1433-7479				MAY	2020	24	10					7021	7028		10.1007/s00500-020-04818-7													
J								Multiplicative derivations and d-filters of commutative residuated lattices	SOFT COMPUTING										(Multiplicative) derivations; Ideal derivations; Monotone; Residuated lattices		In this paper, we consider some properties of multiplicative derivations and d-filters of commutative residuated lattices and show that, for an ideal derivation d of a residuated lattice of all fixed points of d forms a residuated lattice and d is a homomorphism from L to Fixd(L)(2) for a d-filter F, a map d/F:L/F -> L/Fdefined by (d/F)(x/F)=dx/F is also an ideal derivation of L/F and (3) two quotient residuated lattices Fixd/F(L/F)$$\hbox {Fix}_{d/F}(L/F)$$\end{document} and Fixd(L)/d(F) are isomorphic as residuated lattices, that is, Fixd/F(L/F) approximately equal to Fixd(L)/d(F)$$\hbox {Fix}_{d/F}(L/F) \cong \hbox {Fix}_d (L)/d(F)$$\end{document}.																	1432-7643	1433-7479				MAY	2020	24	10					7029	7033		10.1007/s00500-020-04825-8													
J								On epsilon\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$\varepsilon $$\end{document}-soft topological semigroups	SOFT COMPUTING										Soft topological semigroup; <mml; math><mml; mo>o</mml; mo></mml; math>; documentclass[12pt]{minimal}; usepackage{amsmath}; usepackage{wasysym}; usepackage{amsfonts}; usepackage{amssymb}; usepackage{amsbsy}; usepackage{mathrsfs}; usepackage{upgreek}; setlength{; oddsidemargin}{-69pt}; begin{document}$$; bigtriangleup $$; end{document}<inline-graphic xlink; href="500_2020_4826_Article_IEq6; gif"; >-Soft topology; Point open soft topology; T-soft space	SOFT SET-THEORY	In this paper, we introduce e-right, e-left, e-semi and epsilon-soft topological semigroups and examine the way these are related to each other. To do so, we need to define o-soft and point open soft topologies, which are defined in the third and fourth sections, respectively. Also, soft separation axioms on these soft topologies will be studied.																	1432-7643	1433-7479				MAY	2020	24	10					7035	7046		10.1007/s00500-020-04826-7													
J								A sub-concept-based feature selection method for one-class classification	SOFT COMPUTING										One-class classification; Filter-based feature selection; Sub-concept; Multimodal data; Outlier detection; Cyber security	DATA COMPLEXITY	Similarly to binary classification methods, one-class classification methods could benefit from feature selection. However, the feature selection algorithms for the binary or multi-class are not applicable to one-class classification situations since only one class of instances is provided. Few techniques have been proposed so far for feature selection in one-class classification. This paper focuses on designing a filter-based feature selection method for one-class classification. Our approach is based on the observation that for some tasks such as outlier detection, anomaly detection, the training data (normal data) may contain multiple sub-concepts. The sub-concept is a source of data complexity. Our approach aims at searching the features that characterize the instances of the sub-concepts more compact, so as to reduce the data complexity. It firstly finds the sub-concepts using a clustering algorithm with a fixed cluster number and then applies combined feature measures to evaluate the relevance between each feature and the sub-concepts. A fixed number of features-those with the highest relevance scores-are selected as a feature subset. In the searching process, the Davies-Bouldin Index is used to assess the data complexity on the sub-concepts obtained with different number of clusters. The feature subset with the lowest DBI is selected as the final feature subset. Experiments on UCI benchmark and cyber security datasets demonstrate that our feature selection algorithm can select relevant features and improve the performance of one-class classification on multimodal data.																	1432-7643	1433-7479				MAY	2020	24	10					7047	7062		10.1007/s00500-020-04828-5													
J								Combined fitness-violation epsilon constraint handling for differential evolution	SOFT COMPUTING										Constrained optimization; Differential evolution; Constraint handling; <mml; math><mml; mi>epsilon</mml; mi></mml; math>; documentclass[12pt]{minimal}; usepackage{amsmath}; usepackage{wasysym}; usepackage{amsfonts}; usepackage{amssymb}; usepackage{amsbsy}; usepackage{mathrsfs}; usepackage{upgreek}; setlength{; oddsidemargin}{-69pt}; begin{document}$$; varepsilon $$; end{document}<inline-graphic xlink; href="500_2020_4835_Article_IEq4; gif"; >-Constraint; Selective pressure	OPTIMIZATION; ALGORITHM; SEARCH	Over recent decades, several efficient constraint-handling methods have been proposed in the area of evolutionary computation, and the epsilon constraint method is considered as a state-of-the-art method for both single and multiobjective optimization. Still, very few attempts have been made to improve this method when applied to the differential evolution algorithm. This study proposes several novel constraint-handling methods following similar ideas, where the epsilon level is defined based on the current violation in the population, individual epsilon levels are maintained for every constraint, and a combination of fitness and constraint violation is used for determining infeasible solutions. The proposed approaches demonstrate superior performance compared to other approaches in terms of the feasibility rate in high-dimensional search spaces, as well as convergence to global optima. The experiments are performed using the CEC'2017 constrained suite benchmark functions and a set of Economic Load Dispatch problems.																	1432-7643	1433-7479				MAY	2020	24	10					7063	7079		10.1007/s00500-020-04835-6													
J								Stability for feedback loops containing complex algorithms	SOFT COMPUTING											MODEL ARTICULATION CONTROLLER; CMAC NEURAL-NETWORKS; NONLINEAR-SYSTEMS	With the advent of neural networks, fuzzy logic, genetic algorithms, and other soft computing methodologies, many researchers have demonstrated successful designs for intelligent control strategies that appear to outperform traditional linear feedback controls; however, industry has mostly ignored the new technology due to the lack of stability guarantees (required in most formal engineering risk-management processes). In this paper, we offer a Lyapunov-stability framework where one can place any arbitrary computational algorithm and still get guarantees of uniformly ultimately bounded (UUB) signals. We would expect an intelligent algorithm to be well-designed such that the proposed framework would not come into play unless unanticipated disturbances affect the system. But even if the intelligent algorithm was poorly designed, the resulting performance (inside our framework) would just look similar to that of a typical nonlinear neural-adaptive control. In our strategy, the intelligent algorithm trains a cerebellar model articulation controller (CMAC) neural network with arbitrarily bounded weights in order to achieve good performance, while a second CMAC trains in parallel using direct-adaptive-control laws in order to provide stability (even in the case of the first CMAC weights reaching their imposed bound). We test the strategy using a previously proposed ad hoc CMAC weight smoothing strategy, serving as the intelligent algorithm, with simulations and experiment controlling a two-link flexible-joint robot.																	1432-7643	1433-7479				MAY	2020	24	10					7113	7124		10.1007/s00500-020-04851-6													
J								Hybrid Context Aware Recommendation System for E-Health Care by merkle hash tree from cloud using evolutionary algorithm	SOFT COMPUTING										Privacy; Recommendation system; Collaborative filtering; Communication complexity; Computation complexity	MATRIX FACTORIZATION; SEARCH; SECURE; STORAGE	Privacy preservation permits doctors to outsource the huge encrypted reports to the cloud and permits the authenticated patients to have a safe search over the reports without leaking the private information. The doctors in our proposed have used the merkle hash tree for storing the reports of all the patients in the hospital. The existing schemes have used many types of trees like binary tree, red-black tree, spanning tree, B+ tree, etc., for the index generation purpose. Since the security is less and the searching time is high for the above said trees, we have proposed the index generation phase based on the merkle hash tree based on the evolutionary algorithm and it takes less time for searching and highly secure for storing the patient reports. The evolutionary algorithm is used for breeding the new data's through crossover as well as mutation operations to give confinement to new children. When the patient submits the search request for specialized doctor, based on the patient disease our protocol will recommend the specialized doctors and send the recommended doctors information to the patients who have the highest rating in the online social networks. After receiving the recommended results, the patient can have the treatment via online booking appointment, video call or in person based on the appointment booked. After completely cured, the patients can rate the doctors based on the medicine satisfaction, doctors' fees and doctor's response over the call. In this mechanism, we have used the hybrid context aware recommendation system collaborative filtering for rating the doctors based on their performance. After rating the doctors, our protocol has measured the accuracy based on the predicted rating and the true rating. This kind of accuracy metrics is used for ranking the good doctors in the top rank for the patient use. Our proposed work Hybrid Context Aware Recommendation System for E-Health Care (HCARS-EHC) is implemented, and the implementation results of HCARS-EHC illustrate that our protocol is efficient based on the privacy preservation, recommendation and ranking with less computation and communication complexity.																	1432-7643	1433-7479				MAY	2020	24	10					7149	7161		10.1007/s00500-019-04322-7													
J								A clustering algorithm based on emotional preference and migratory behavior	SOFT COMPUTING										Emotional preference; Migration; Optimization algorithm; Data clustering	OPTIMIZATION ALGORITHM; K-MEANS	In this paper, a clustering algorithm based on emotional preference and migratory behavior (EPMC) is proposed for data clustering. The algorithm consists of four models: the migration model, the emotional preference model, the social group model and the inertial learning model. First, the migration model calculates the probability of individuals being learned, so that individuals can learn from the superior. Second, the emotional preference model is introduced to help individuals find the most suitable neighbor for learning. Third, the social group model divides the whole population into different groups and enhances the mutual cooperation between individuals under different conditions. Finally, the inertial learning model balances the exploration and exploitation during the optimization, so that the algorithm can avoid falling into the local optimal solution. In addition, the convergence of EPMC algorithm is verified by theoretical analysis, and the algorithm is compared with four clustering algorithms. Experimental results validate the effectiveness of EPMC algorithm.																	1432-7643	1433-7479				MAY	2020	24	10					7163	7179		10.1007/s00500-019-04333-4													
J								An approach based on knowledge exploration for state space management in checking reachability of complex software systems	SOFT COMPUTING										Model checking; State space explosion; Reachability property; Ensemble classification	FLY MODEL CHECKING; GRAPH TRANSFORMATION; DEADLOCK DETECTION; HEURISTIC SOLUTION; OPTIMIZATION; SEARCH	Model checking is one of the most efficient techniques in software system verification. However, state space explosion is a big challenge while using this technique to check different properties like safety ones. In this situation, one can search the state space to find a reachable state in which the safety property is violated. Hence, reachability checking can be done instead of checking safety property. However, checking reachability in the worst case may cause state space explosion again. To handle this problem, our idea is based on generating a small model consistent with the main model. Then by exploring the state space entirely, we search it to find the goal states. After finding the goal states, we label the paths which start from the initial state and leading to a goal state. Then using the ensemble classification technique, the necessary knowledge is extracted from these paths to intelligently explore the state space of the bigger model. Ensemble machine learning technique uses Boosting method along with decision trees. It follows sampling techniques by replacement. This method generates k predictive models after sampling k times. Finally, it uses a voting mechanism to predict the labels of the final path. Our proposed approach is implemented in GROOVE, which is an open source toolset for designing and model checking graph transformation systems. Our experiments show a significant improvement in terms of both speed and memory usage. In average, our approach consumes nearly 42% fewer memory than other approaches. Also, it generates witnesses nearly 20% shorter than others, in average.																	1432-7643	1433-7479				MAY	2020	24	10					7181	7196		10.1007/s00500-019-04334-3													
J								GORTS: genetic algorithm based on one-by-one revision of two sides for dynamic travelling salesman problems	SOFT COMPUTING										DTSP; Path optimisation; One-by-one revision of two slides; Genetic algorithm	OPTIMIZATION; IMMIGRANTS	The dynamic travelling salesman problem (DTSP) is a natural extension of the standard travelling salesman problem, and it has attracted significant interest in recent years due to is practical applications. In this article, we propose an efficient solution for DTSP, based on a genetic algorithm (GA), and on the one-by-one revision of two sides (GORTS). More specifically, GORTS combines the global search ability of GA with the fast convergence feature of the method of one-by-one revision of two sides, in order to find the optimal solution in a short time. An experimental platform was designed to evaluate the performance of GORTS with TSPLIB. The experimental results show that the efficiency of GORTS compares favourably against other popular heuristic algorithms for DTSP. In particular, a prototype logistics system based on GORTS for a supermarket with an online map was designed and implemented. It was shown that this can provide optimised goods distribution routes for delivery staff, while considering real-time traffic information.																	1432-7643	1433-7479				MAY	2020	24	10					7197	7210		10.1007/s00500-019-04335-2													
J								A co-evolutionary hybrid decomposition-based algorithm for bi-level combinatorial optimization problems	SOFT COMPUTING										Bi-level combinatorial optimization; Evolutionary algorithms; Local search method; Bi-level production-distribution planning in supply chain	BILEVEL OPTIMIZATION; MODEL; SEARCH; DESIGN	Bi-level programming problems are a special class of optimization problems with two levels of optimization tasks. These problems have been widely studied in the literature and often appear in many practical problem solving tasks. Although many applications fit the bi-level framework, however, real-life implementations are scarce, due mainly to the lack of efficient algorithms able to handle effectively this NP-hard problem. Several solution approaches have been proposed to solve these problems; however, most of them are restricted to the continuous case. Motivated by this observation, we have recently proposed a Co-evolutionary Decomposition-based Algorithm (CODBA) to solve bi-level combinatorial problems. CODBA scheme has been able to bring down the computational expense significantly as compared to other competitive approaches within this research area. In this paper, we further improve CODBA approach by incorporating a local search procedure to make the search process more efficient. The proposed extension called CODBA-LS includes a variable neighborhood search to the lower-level task to help in faster convergence of the algorithm. Further experimental tests based on the bi-level production-distribution problems in supply chain management model on a set of artificial and real-life data turned out to be effective on both computation time and solution quality.																	1432-7643	1433-7479				MAY	2020	24	10					7211	7229		10.1007/s00500-019-04337-0													
J								Using an evolutionary approach based on shortest common supersequence problem for loop fusion	SOFT COMPUTING										Optimizing compiler; Loop fusion; Evolutionary algorithm; Data reuse; Shortest common supersequence (SCS)	KRILL HERD ALGORITHM; COMPLEXITY; ALPHABET	In the literature, loop fusion is an effective optimization technique which tries to enhance parallelizing compilers' performance via memory hierarchy management, and all its competing criteria create an NP-hard problem. This paper proposes an evolutionary algorithm that aims to achieve a profitable loop order which maximizes fusion taking into account register size, parallelism and data reuse advancement. Besides, this method preserves prerequisite relations between the loops by encoding each distinct loop sequence as the shortest common supersequence (SCS) of the related dependence graph. Regarding the related optimization methods that only focus on fusion, this set of metrics, an evolutionary algorithm and also the shortest common supersequence problem have not been considered before in this area. Despite all the envisaged complexities, experimental results confirm the accuracy and advantage of the proposed approach. But due to evolutionary methods effect on raising the compilation time, the proposed algorithm is only applicable when this issue is not prominent, in comparison with the quality of the outcome.																	1432-7643	1433-7479				MAY	2020	24	10					7231	7252		10.1007/s00500-019-04338-z													
J								Modeling and state of health estimation of nickel-metal hydride battery using an EPSO-based fuzzy c-regression model	SOFT COMPUTING										Ni-MH batteries; State of health estimation; Euclidean particle swarm optimization; Fuzzy c-regression model	OPTIMIZATION; ALGORITHM; IDENTIFICATION; PROGNOSTICS; MANAGEMENT	The prognostic and health management of the batteries continued to attract interest from automobile manufacturers as the key for lowering life-cycle costs, reducing unexpected power outages, and one of the most important and efficient ways for energy storage for electric vehicle applications. Indeed, an effective battery health monitoring depends on accurate estimation of state of health (SOH). However, the SOH cannot be directly measured by sensors in the battery management system. Moreover, the SOH estimation based on a standard resistor-capacitor (RC) battery model is not so accurate because a RC model is obtained with some approximations and without taking into account more detailed knowledge about the chemical reactions happening inside the battery. In this paper, a combined battery modeling and SOH estimation method over the lifespan of a nickel-metal hydride (Ni-MH) battery is proposed. First, a fuzzy c-regression model based on Euclidean particle swarm optimization is applied to modeling a Ni-MH battery. Second, the SOH monitoring is determined according to the discharge rate of the battery model. The performance of the proposed method has been analyzed through the modeling and the estimation of the SOH using a real data set of the Ni-MH battery.																	1432-7643	1433-7479				MAY	2020	24	10					7265	7279		10.1007/s00500-019-04343-2													
J								A machine learning evolutionary algorithm-based formula to assess tumor markers and predict lung cancer in cytologically negative pleural effusions	SOFT COMPUTING										Machine learning; Genetic programming; Genetic algorithm; Evolutionary algorithm; Pleural effusion; Biochemical tumor marker; Thoracentesis; Thoracoscopy; Video-assisted thoracic surgery	DIAGNOSTIC-VALUE; CARCINOEMBRYONIC ANTIGEN; FLUID; MANAGEMENT; CEA; CYFRA-21-1; CA-19-9; UTILITY; CA-15-3; CA-125	Malignant pleural effusion is diagnostically challenging in presence of negative cytology. The assessment of tumor markers in serum has become a standard tool in cancer diagnosis, while pleural fluid sampling has not met universal consensus. The evaluation of a panel of markers both in serum and pleural fluid may be crucial to improve the diagnostic accuracy. Using a machine learning-based approach, we provide a mathematical formula capable to express the complex relation existing among the expressed markers in serum and pleural effusion and the presence of lung cancer. The formula indicates CEA and CYFRA21-1 in pleural fluid as the best diagnostic markers, with 97% accuracy, 98% sensitivity, 95% specificity, 96% area under curve, 98% positive predictive value, and 92% MCC (Matthews correlation coefficient).																	1432-7643	1433-7479				MAY	2020	24	10					7281	7293		10.1007/s00500-019-04344-1													
J								Adaptive Kaniadakis entropy thresholding segmentation algorithm based on particle swarm optimization	SOFT COMPUTING										Image segmentation; Thresholding; Kaniadakis entropy; Particle swarm optimization; Xie-Beni index		Kaniadakis entropy is a kind of generalized entropy based on the kappa\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$ \kappa $$\end{document} probability distribution, which has a good ability to deal with the distribution of long tail. The image thresholding algorithm based on Kaniadakis entropy can effectively segment images with long-tailed distribution histograms, such as nondestructive testing image. However, Kaniadakis entropy is a generalized information entropy with parameter. How to choose appropriate parameter kappa\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$ \kappa $$\end{document} is a problem to be solved. In this paper, we proposed an adaptive parameter selection Kaniadakis entropy thresholding algorithm. Based on a clustering effectiveness evaluation index, we transform the parameter selection problem into an optimization problem, then use particle swarm optimization search algorithm to optimize it and finally obtain the segmentation threshold under the optimal parameter. The presented algorithm can adaptively select parameters according to different images and obtain the optimal segmentation images. In order to show the effectiveness of the proposed method, the segmentation results are compared with several existing entropy-based thresholding algorithms. Experimental results both qualitatively and quantitatively demonstrate that the proposed method is effective.																	1432-7643	1433-7479				MAY	2020	24	10					7305	7318		10.1007/s00500-019-04351-2													
J								Ranking methodology of induced Pythagorean trapezoidal fuzzy aggregation operators based on Einstein operations in group decision making	SOFT COMPUTING										Pythagorean trapezoidal fuzzy numbers; Einstein operations; Aggregation operators; Group decision making	NUMBERS; SETS	The Pythagorean fuzzy number is a new tool for uncertainty and vagueness. It is a generalization of fuzzy numbers and intuitionistic fuzzy numbers. In this paper, we define some Einstein operations on Pythagorean trapezoidal fuzzy set and develop two averaging aggregation operators, which is an induced Pythagorean trapezoidal fuzzy Einstein ordered weighted averaging operator and an induced Pythagorean trapezoidal fuzzy Einstein hybrid averaging (I-PTFEHA) operator. We presented some new methods to deal with the multi-attribute group decision-making problems under the Pythagorean trapezoidal fuzzy environment. Finally, we used some practical examples to illustrate the validity and feasibility of the proposed methods by comparing with existing method. It shows that the proposed I-PTFEHA operator is much better and reliable than the existing one.																	1432-7643	1433-7479				MAY	2020	24	10					7319	7334		10.1007/s00500-019-04356-x													
J								Using data mining techniques to improve replica management in cloud environment	SOFT COMPUTING										Cloud computing; Data replication; Data mining; Simulation	DECREASE ACCESS LATENCY; STRATEGY; ALGORITHMS; PERFORMANCE; FRAMEWORK; ONLINE	Effective data management is a crucial problem in distributed systems such as data grid and cloud. This can be achieved by replicating file in a wise manner, which reduces data access time, increases data availability, reliability and system load balancing. Determining a reasonable number and appropriate location of replicas is essential decision in cloud computing. In this paper, a new dynamic replication strategy called Data Mining-based Data Replication (DMDR) is proposed, which determines the correlation of the data files accessed using the file access history. We focus particularly on how extracted knowledge with maximal frequent correlated pattern mining improves data replication. We can group files with high dependency in the same replica set. Through the DMDR strategy, replicas can be stored in the suitable locations, with reduced access latency according to the centrality factor. In addition, due to the finite storage space of each node, replicas that are useful for future tasks can be wastefully deleted and replaced with less beneficial ones. Results of simulation using CloudSim indicate that DMDR strategy has a relative advantage in effective network usage, average response time, hit ratio in comparison with current methods. It can be concluded from this investigation that data mining technique is effective and helpful in the finding of users' future access behavior in cloud environment.																	1432-7643	1433-7479				MAY	2020	24	10					7335	7360		10.1007/s00500-019-04357-w													
J								Algorithm for solving group decision-making problems based on the similarity measures under type 2 intuitionistic fuzzy sets environment	SOFT COMPUTING										Multiattribute decision making; Type 2 fuzzy set; Type 2 intuitionistic fuzzy set; Similarity measures; Group decision-making problems	AGGREGATION OPERATORS; SENTIMENT ANALYSIS; DISTANCE MEASURE; SOCIAL NETWORKS; RANKING; FUZZISTICS	Type 2 intuitionistic fuzzy set (T2IFS) is one of the most important concepts to describe fuzzy information by providing an additional degree of freedom to the decision maker to decide the decision-making process. The fundamental superiority of the T2IFS, over the intuitionistic fuzzy set, has been its ability to capture the degrees of membership of relevant membership values, where the uncertainty is handled more accurately, and hence accommodate more uncertainties. Keeping these features in mind, this paper explores the theory of T2IFS by defining some families of the similarity measures to measure the degree of similarity between the two or more T2IFSs. For this, the characteristic of each object is measured in terms of degrees of primary membership and non-membership, secondary membership and non-membership and the variance margin function between them. Based on that, several formulations of the similarities measures are defined and their properties are investigated . Also, some fundamental relation between the proposed measures is stated. Further, based on the similarity measure, a group decision-making approach is explored to rank the alternatives. An illustrative example is provided to demonstrate the approach, and the comparative analysis with some of the existing measures is performed to explore the results.																	1432-7643	1433-7479				MAY	2020	24	10					7361	7381		10.1007/s00500-019-04359-8													
J								Performance comparison of metaheuristic algorithms using a modified Gaussian fitness landscape generator	SOFT COMPUTING										Metaheuristic algorithms; Modified Gaussian fitness landscape generator; Optimization; Performance measurement	WATER CYCLE ALGORITHM; DIFFERENTIAL EVOLUTION; OPTIMIZATION	Various metaheuristic optimization algorithms are being developed to obtain optimal solutions to real-world problems. Metaheuristic algorithms are inspired by various metaphors, resulting in different search mechanisms, operators, and parameters, and thus algorithm-specific strengths and weaknesses. Newly developed algorithms are generally tested using benchmark problems. However, for existing traditional benchmark problems, it is difficult for users to freely modify the characteristics of a problem. Thus, their shapes and sizes are limited, which is a disadvantage. In this study, a modified Gaussian fitness landscape generator is proposed based on a probability density function, to make up for the disadvantages of traditional benchmark problems. The fitness landscape developed in this study contains a total of six features and can be employed to easily create various problems depending on user needs, which is an important advantage. It is applied to quantitatively evaluate the performance and reliability of eight reported metaheuristic algorithms. In addition, a sensitivity analysis is performed on the population size for population-based algorithms. Furthermore, improved versions of the metaheuristic algorithm are considered, to investigate which performance aspects are enhanced by applying the same fitness landscape. The modified Gaussian fitness landscape generator can be employed to compare the performances of existing optimization algorithms and to evaluate the performances of newly developed algorithms. In addition, it can be employed to develop methods of improving algorithms by evaluating their strengths and weaknesses.																	1432-7643	1433-7479				MAY	2020	24	10					7383	7393		10.1007/s00500-019-04363-y													
J								Grid-based dynamic robust multi-objective brain storm optimization algorithm	SOFT COMPUTING										Grid-based clustering; Hybrid mutation; Robust Pareto-optimum over time; Multi-objective brain storm optimization algorithm; Dynamic		Rich works have been done on brain storm optimization algorithm solving static single- or multi-objective optimization problems, but less reports for dynamic multi-objective optimization problems. Based on this, a grid-based multi-objective brain storming algorithm with hybrid mutation operation is proposed to find the robust Pareto-optimal solution set over time. Grid-based clustering method partitions the objective space evenly along each objective and classifies the individuals located in the same grid into a cluster. Its computational complexity is less than k-means- and group-based clustering strategies. Traditional Gaussian-, Cauchy- and Chaotic-based mutation operators have different mutation steps and generate the new individuals with various diversity. In order to enhance the diversity and avoiding the premature convergence, a hybrid mutation strategy integrating above three mutation operators is presented. Experimental results for eight dynamic multi-objective benchmark functions show that the proposed algorithm can find robust Pareto-optimal solutions approximating the true Pareto front under more subsequent environments with the acceptable fitness threshold. The longer survival time also indicates that grid-based clustering method and hybrid mutation strategy are apt to better robustness.																	1432-7643	1433-7479				MAY	2020	24	10					7395	7415		10.1007/s00500-019-04365-w													
J								SP-BRAIN: scalable and reliable implementations of a supervised relevance-based machine learning algorithm	SOFT COMPUTING										U-BRAIN algorithm; Machine learning; Scalable and reliable implementations; Distributed computing; Apache Spark; Non-standard MapReduce; Hadoop; Cloud execution environment	BIG DATA; PERFORMANCE; MAPREDUCE; SOFTWARE; SPARK	In this work, new implementations of the U-BRAIN (Uncertainty-managing Bach Relevance-Based Artificial Intelligence) supervised machine learning algorithm are described. The implementations, referred as SP-BRAIN (SP stands for Spark), aim to efficiently process large datasets. Given the iterative nature of the algorithm together with its dependence on in-memory data, a non-standard MapReduce paradigm is applied, taking into account several memory and performance problems, e.g., the granularity of the MAP task, the reduction in the shuffling operation, caching, partial data recomputing, and usage of clusters. The implementations benefit the whole Hadoop ecosystem components, such as HDFS, Yarn, and streaming. Testing is performed in cloud execution environments, using different configurations with up to 128 cores. The performance of the new implementations is evaluated on three known datasets, and the findings are compared to the ones of a previous U-BRAIN parallel implementation. The results show a speedup up to 20 x with a good scalability and reliability in cluster environments.																	1432-7643	1433-7479				MAY	2020	24	10					7417	7434		10.1007/s00500-019-04366-9													
J								Linking granular computing, big data and decision making: a case study in urban path planning	SOFT COMPUTING										Granular computing; Big data; Decision making; Urban path planning; Social media	VEHICLE-ROUTING PROBLEM; TRAFFIC FLOW; TRANSPORTATION; MANAGEMENT; INFORMATION; ANALYTICS; SYSTEMS	Granular computing, an emerging information processing paradigm transforming complex data into information granules at different scales so that different features and regularities can be revealed, offers an essential linkage between big data and decision making. By using innovative technologies of granular computing that transforms big data collections into information granules, we would be at position of recognizing and exploiting the meaningful pieces of knowledge present in data, and produce sound, and practically supported decisions. In this study, we first summarize a general scheme of big data-granular computing-decision making and then present a case study where we detect the important traffic event information by collecting and analyzing social media data, and transform them into probabilistic information granules that can be used for urban routing navigation. We propose a robust fastest path optimization model to incorporate the impact of traffic events and generate the optimal routing strategy. Real-life experiments are carried out in regional Chaoyang District, Beijing, as well as the backbone roadway network of Beijing, which illustrate the effectiveness of our proposed big data-driven decision-making method. Our study provides new evidence demonstrating that big data can be efficiently used to enhance decisions and granular computing with this regard. The concept of the proposed scheme can be easily extended for decision-making modeling in other domains.																	1432-7643	1433-7479				MAY	2020	24	10					7435	7450		10.1007/s00500-019-04369-6													
J								Sentiment classification using harmony random forest and harmony gradient boosting machine	SOFT COMPUTING										Sentiment analysis (SA); Harmony search (HS) algorithm; Gradient boosting and random forest (RF)	SEARCH ALGORITHM	The building of a system for exploring the opinions of users that are made in the blog posts, tweets, reviews or comments regarding a particular topic, policy or a product is known as sentiment analysis. The primary aim of this is the determination of the user attitude regarding a certain topic. The harmony search algorithm has proved to be extremely useful in a varied range of problems in optimization. This shows better performance compared to the other techniques of optimization. Another very powerful technique that is applied to machine learning which is now getting extremely popular is gradient boosting. There are several tree parameters which have been optimized for the random forest and the gradient boosting machine that make use of the harmony search algorithm.																	1432-7643	1433-7479				MAY	2020	24	10					7451	7458		10.1007/s00500-019-04370-z													
J								Indeterminate Likert scale: feedback based on neutrosophy, its distance measures and clustering algorithm	SOFT COMPUTING										Likert scale; Star rating; Questionnaire; Survey; Customer feedback; Neutrosophy; Neutrosophic logic; Indeterminacy; Indeterminate Likert scale; Triple refined neutrosophic set; Distance measures; Minimum spanning tree; Clustering algorithm	SETS	Likert scale is the most widely used psychometric scale for obtaining feedback. The major disadvantage of Likert scale is information distortion and information loss problem that arise due to its ordinal nature and closed format. Real-world responses are mostly inconsistent, imprecise and indeterminate depending on the customers' emotions. To capture the responses realistically, the concept of neutrosophy (study of neutralities and indeterminacy) is used. Indeterminate Likert scale based on neutrosophy is introduced in this paper. Clustering according to customer feedback is an effective way of classifying customers and targeting them accordingly. Clustering algorithm for feedback obtained using indeterminate Likert scaling is proposed in this paper. While dealing real-world scenarios, indeterminate Likert scaling is better in capturing the responses accurately.																	1432-7643	1433-7479				MAY	2020	24	10					7459	7468		10.1007/s00500-019-04372-x													
J								An exponential jerk system, its fractional-order form with dynamical analysis and engineering application	SOFT COMPUTING										Exponential jerk system (EJS); Fractional-order chaotic systems; Dynamic analyses; Random number generator (RNG); Sound steganography	LYAPUNOV EXPONENTS; IMPLEMENTATION; CHAOS; SYNCHRONIZATION; APPROXIMATION; DESIGN	A simple jerk system with only one exponential nonlinearity is proposed and discussed. Dynamic analysis of the integer-order jerk system shows the existence of chaotic oscillations. A model for the fractional-order jerk system is derived. The Adomian decomposition method is used to analyse the fractional-order jerk system. Stability analysis of the fractional-order jerk system shows that chaotic oscillations exist in orders less than one and bifurcation analysis shows the range of fractional orders for periodic and chaotic oscillations. To show the randomness of the fractional-order jerk system, a pseudorandom number generator is designed and tested. The NIST-800-22 tests show that the proposed fractional-order jerk system is effective in showing randomness. Finally, an image hiding application to the audio data has been realized by using the developed RNG algorithm. The encrypted image is hidden by being embedded in the audio data, and then, on the receiver side, the data are recovered by taking the image data from the hidden audio file.																	1432-7643	1433-7479				MAY	2020	24	10					7469	7479		10.1007/s00500-019-04373-w													
J								HSOS: a novel hybrid algorithm for solving the transient-stability-constrained OPF problem	SOFT COMPUTING										Critical clearing time; Differential evolution; Evolutionary algorithms; Optimal power flow; Transient-stability-constrained optimal power flow; Symbiotic organisms search	INTERIOR-POINT METHOD; DIFFERENTIAL EVOLUTION ALGORITHM; PARTICLE SWARM OPTIMIZATION; GENETIC ALGORITHM; SECURITY	This article presents a new algorithm aimed toward effective handling of the transient-stability-constrained optimal power flow (TSC_OPF) problem. The algorithm is a hybridized version of the existing differential evolution (DE) and symbiotic organism search (SOS) algorithms. It combines exploration and exploitation ability of both algorithms which results in its better performance as compared to DE and SOS acting alone. It was tested on IEEE 30 bus test system and the New England 39 bus test system. The results obtained by the proposed approach were compared with conventional TSC_OPF and also with other algorithms available in the literature. Results obtained using the proposed approach demonstrates superiority in comparison with other available algorithms in the literature.																	1432-7643	1433-7479				MAY	2020	24	10					7481	7510		10.1007/s00500-019-04374-9													
J								An extension to fuzzy ELECTRE	SOFT COMPUTING										Fuzzy ELECTRE; Membership function; Fuzzy weighted ratings; Ranking; Mean of maximum and minimum	CRITERIA DECISION-MAKING; REASONABLE PROPERTIES; RANKING; NUMBERS; PROJECTS; MCDM	An extension to fuzzy ELECTRE is proposed in this paper, where ratings of alternatives versus qualitative criteria and the importance weights of criteria are assessed in linguistic values represented by fuzzy numbers. Formulas for the membership functions of fuzzy weighted ratings of alternatives under criteria can be obtained, and some properties are investigated. The ranking method of mean of maximum and minimum based on the Chen method is suggested to rank those fuzzy weighted ratings to produce concordance and discordance sets to complete the model. A numerical example will be used to demonstrate feasibility of the proposed fuzzy ELECTRE method, in which the outranking relationships among the alternatives can be clearly displayed. A comparison between the proposed ranking method and the Chen method will also be conducted to show advantage of the proposed method.																	1432-7643	1433-7479				MAY	2020	24	10					7541	7555		10.1007/s00500-019-04381-w													
J								A composite machine-learning-based framework for supporting low-level event logs to high-level business process model activities mappings enhanced by flexible BPMN model translation	SOFT COMPUTING										Business process management systems; Business process intelligence; Low-level event logs; High-level business process model activities; BPMN model translation	BIG DATA; PROCESS MANAGEMENT; ALGORITHM	Process mining is an emerging discipline that aims to analyze business processes using event data logged by IT systems. In process mining, the focus is on how to effectively and efficiently predict the next process/trace to be activated among all the possible processes/traces that are available in the process schema (usually modeled as a graph). Most of the existing process mining techniques assume that there is a one-to-one mapping between process model activities and the events that are recorded during process execution. However, event logs and process model activities are at different level of granularity. In this paper, we present a machine-learning-based approach to map low-level event logs to high-level activities. With this work, we can bridge the abstraction levels when the high-level labels of the low-level events are not available. The proposed approach consists of two main phases: automatic labeling and machine-learning-based classification. In automatic labeling, a modified k-prototypes clustering approach has been used in order to obtain the labeled examples. Then, in the second phase, we trained different ML classifiers using the obtained labeled examples. Since, in real-life applications and systems, business processes are expressed according to the Business Process Model and Notation (BPMN) format, we improve our proposed framework by means of an innovative, flexible BPMN model translation methodology that acts at the first phase. We demonstrate the applicability of our proposed framework using two case studies with real-world event logs, and provide its experimental assessment and analysis.																	1432-7643	1433-7479				MAY	2020	24	10					7557	7578		10.1007/s00500-019-04385-6													
J								Estimation of number of foreign visitors with ANFIS by using ABC algorithm	SOFT COMPUTING										ANFIS; Neuro-fuzzy; Artificial bee colony; Estimation of the number of foreign visitors	ARTIFICIAL BEE COLONY; MODEL; OPTIMIZATION; SYSTEM	ANFIS is an artificial intelligence technique which is composed of a combination of artificial neural networks and fuzzy inference system. Due to its structure, it is used in modeling and identifying numerous systems in various fields. The training process of ANFIS is important to obtain effective results with it. So, a successful training algorithm should be used. In this study, the ANFIS is trained by using ABC algorithm for the solution of the real-world problem. For this purpose, the number of foreign visitors coming to Turkey from the USA, Germany, Bulgaria, France, Georgia, the Netherlands, England, Iran and Russia is estimated. In addition, total number of visitors coming to Turkey is also predicted. In applications, 150 months of data between July 2002 and December 2014 are utilized and a time series is created using these data. The results obtained using ABC algorithm are compared with GA, DE, HS and PSO. As a conclusion, it is seen that the results obtained using ABC algorithm for estimation of number of foreign visitors are more successful than other optimization methods.																	1432-7643	1433-7479				MAY	2020	24	10					7579	7591		10.1007/s00500-019-04386-5													
J								A fault diagnosis model of marine diesel engine cylinder based on modified genetic algorithm and multilayer perceptron	SOFT COMPUTING										Marine diesel engine cylinder; Fault diagnosis; Genetic algorithm; Multilayer perceptron; Chaotic mapping	ARTIFICIAL NEURAL-NETWORKS; AGGREGATION OPERATORS; FUZZY	The cylinder of marine diesel engine, as the power supply to the marine diesel engine, would lead dramatically damage to the engine once faults occurred. To avoid the situation, we develop a novel combinational approach using an improved genetic algorithm (GA) and multilayer perceptron (MLP). Firstly, chaos theory is carried out on the standard GA to prevent premature convergence. The improvements on the GA consist of population initialized by chaotic mapping, chaotic crossover, and chaotic mutation operator. Secondly, the Levenberg-Marquardt algorithm is used to train the MLP to accelerate the convergence speed of the MLP. Thirdly, the improved GA is used to optimize the initial weights and thresholds of MLP to further improve the performance of MLP. Finally, the proposed model is applied to the cylinder of a marine diesel engine for fault diagnosis. Compared with traditional approaches, the proposed method obtains more ideal solutions. Results demonstrate that the proposed method could effectively identify ten common faults of the marine diesel engine cylinder, and the average correct ratio of fault diagnosis can exceed 95%.																	1432-7643	1433-7479				MAY	2020	24	10					7603	7613		10.1007/s00500-019-04388-3													
J								Evidential model for intuitionistic fuzzy multi-attribute group decision making	SOFT COMPUTING										Intuitionistic fuzzy sets; Evidence theory; Multi-attribute group decision making; Intuitionistic fuzzy aggregation operator	SUPPLIER SELECTION; VAGUE SETS; COMBINATION; METHODOLOGY	Due to the uncertainty existing in real-world, intuitionistic fuzzy sets (IFSs) are used to model uncertain information in multi-attribute group decision making (MAGDM). The intuitionistic fuzzy MAGDM problems have gained great popularity recently. But, most of the current methods depend on various aggregation operators that may provide unreasonable collective intuitionistic fuzzy values of alternatives to be ranked. To solve such problem, a new method is developed based on evidence theory and IFSs. First, the mathematical relation between IFSs and evidence theory is analyzed, followed by the transformation from intuitionistic fuzzy evaluation information to basic belief assignment in evidence theory. Then, a new intuitionistic fuzzy weighted evidential (IFWE) average operator is introduced based on the operation of evidence discounting and evidence combination rule. We also develop a possibility-based ranking method for intuitionistic fuzzy values (IFVs) to obtain the linear ordering of IFVs. The proposed evidential model uses the IFWE average operator to aggregate the decision matrix and the attribute weight that is given by each decision maker, based on which each decision maker's aggregated decision matrix can be obtained. Based on the decision matrices of all decision makers and the weights of the decision makers, the aggregated intuitionistic fuzzy value of each alternative can be obtained by the IFWE average operator. Finally, the preference order of all alternatives can be obtained by the possibility-based ranking method. Comparative analysis based on several application examples of MAGDM demonstrates that the proposed method can overcome the drawbacks of existing methods for MAGDM in intuitionistic fuzzy environments.																	1432-7643	1433-7479				MAY	2020	24	10					7615	7635		10.1007/s00500-019-04389-2													
J								Assessing the quality of mobile graphical user interfaces using multi-objective optimization	SOFT COMPUTING										Mobile user interface; Aesthetic quality; Evaluation	USABILITY EVALUATION; INFORMATION; CONTEXT; LAYOUT; SYSTEMS; DESIGN; MODEL	Aesthetic defects are a violation of quality attributes that are symptoms of bad interface design programming decisions. They lead to deteriorating the perceived usability of mobile user interfaces and negatively impact the User's eXperience (UX) with the mobile app. Most existing studies relied on a subjective evaluation of aesthetic defects depending on end-users feedback, which makes the manual evaluation of mobile user interfaces human-centric, time-consuming, and error-prone. Therefore, recent studies have dedicated their effort to focus on the definition of mathematical formulas that each targets a specific structural quality of the interface. As the UX is tightly dependent on the user profile, the combination and calibration of quality attributes, formulas, and user's characteristics, when defining a defect, are not straightforward. In this context, we propose a fully automated framework which combines literature quality attributes with the user's profile to identify aesthetic defects of MUI. More precisely, we consider the mobile user interface evaluation as a multi-objective optimization problem where the goal is to maximize the number of detected violations while minimizing the detection complexity of detection rules and enhancing the interfaces overall quality in means of guidance and coherence coverage. We conducted a comparative study of several evolutionary algorithms in terms of accurately identifying aesthetic defects. We reported their performance in solving the proposed search-based multi-objective optimization problem. The results confirm the efficiency of the indicator-based evolutionary algorithm in terms of assessing the developers in detecting typical defects and also in generating the most accurate detection rules.																	1432-7643	1433-7479				MAY	2020	24	10					7685	7714		10.1007/s00500-019-04391-8													
J								Aggregation of pragmatic operators to support probabilistic linguistic multi-criteria group decision-making problems	SOFT COMPUTING										Probabilistic linguistic term sets; Heronian mean operator; Centered OWA operator; Power average operator; EDAS method	TERM SETS; PREDICTION	In this paper, a multi-attribute group decision-making method based on aggregation operators is presented to solve the decision-making problems which the evaluation values take the form of probabilistic linguistic terms sets (PLTSs). Firstly, some properties of the PLTS are defined, such as the concept and the linguistic terms transformation function, the existing comparison methods and the proposed score function and distance. Secondly, some novel operators are proposed by combining the Heronian mean operator with the centered OWA operator and the power average operator under probabilistic linguistic environment, such as the probabilistic linguistic weighted centered order weighted generalized Heronian mean operator and the probabilistic linguistic weighted power generalized Heronian mean operator. Thirdly, the model of deriving the criteria weight is put forward based on the ideology of deviation maximizing and customized individual attitudinal. Furthermore, based on the proposed aggregation operators and EDAS method, a scientific group decision-making procedure is put forward under probabilistic linguistic environment. Finally, an illustrative example is also given to demonstrate the feasibility and practicality of the proposed method.																	1432-7643	1433-7479				MAY	2020	24	10					7735	7755		10.1007/s00500-019-04393-6													
J								Multi-view reconstructive preserving embedding for dimension reduction	SOFT COMPUTING										Multi-view; Dimension reduction; Multi-view reconstructive preserving embedding; Linear reconstruction	RECOGNITION	With the development of feature extraction technique, one sample always can be represented by multiple features which are located in different high-dimensional spaces. Because multiple features can reflect one same sample from various perspectives, there must be compatible and complementary information among the multiple views. Therefore, it's natural to learn information from multiple views to obtain better performance. However, most multi-view dimension reduction methods cannot handle multiple features from nonlinear space with high dimensions. To address this problem, we propose a novel multi-view dimension reduction method named multi-view reconstructive preserving embedding (MRPE) in this paper. MRPE reconstructs each sample by utilizing its k nearest neighbors. The similarities between each sample and its neighbors are mapped into lower-dimensional space in order to preserve the underlying neighborhood structure of the original manifold. MRPE fully exploits correlations between each sample and its neighbors from multiple views by linear reconstruction. Furthermore, MRPE constructs an optimization problem and derives an iterative procedure to obtain the low-dimensional embedding. Various evaluations based on the applications of document classification, face recognition and image retrieval demonstrate the effectiveness of our proposed approach on multi-view dimension reduction.																	1432-7643	1433-7479				MAY	2020	24	10					7769	7780		10.1007/s00500-019-04395-4													
J								Firms' pricing strategies under different decision sequences in dual-format online retailing	SOFT COMPUTING										Channel strategy; Dual-format retailing; Pricing timing; Consumer value; Game theory	SUPPLY-CHAIN; CHANNEL SELECTION; COMPETITION; COORDINATION; MARKETPLACE; MANUFACTURER	E-commerce platforms are increasingly adopting a dual-format retailing mode, not only acting as a reseller, but also providing manufacturers a platform (i.e., marketplace) to access consumers. Based on platforms' offers, manufacturers can choose to operate either dual-format or single-format retailing. To figure out this problem, we first investigate channel selecting strategy for a manufacturer based on consumer value and further consider the impacts of power structure and pricing timing on firms' optimal pricing policies under dual-format retailing. Our findings suggest the manufacturer prefers to operate marketplace channel when utility discount factor is sufficiently high, while operate dual-format retailing channels when utility discount factor is moderate. An interesting observation is that more market power does not always create higher profit. Under some certain conditions, the e-commerce platform could obtain maximum profit when his rival is the leader, so does the manufacturer. In addition, we also explore leader's optimal pricing timing under different power structures. If the manufacturer is the leader, she should either let platform pricing early or set her own retail price early, depending on estimated utility discount and given platform fee. Being the leader, the e-commerce platform shouldn't set his retail price early in any case, which is consistent with "last mover" advantage.																	1432-7643	1433-7479				MAY	2020	24	10					7811	7826		10.1007/s00500-019-04399-0													
J								Computerized grading of brain tumors supplemented by artificial intelligence	SOFT COMPUTING										Magnetic resonance imaging; Medical image segmentation; Brain tumor; Artificial bee colony; Support vector machine	SEGMENTATION	For effective diagnosis of health conditions, there is a need to process medical images to obtain meaningful information. The diagnosis of brain tumors begins with magnetic resonance imaging (or MRI) scan. This is followed by segmentation of the medical images so obtained which can prove cumbersome if it were to be performed manually. Determining the best approach to do segmentation remains challenge among multiple computerized approaches. This paper combines both the identification and classification of tumors from the MRI results and is backed by a cloud-based framework to provision the same. The phase of extraction of features includes the utilization of a Hadoop framework and Gabor filter along with variations in terms of orientation and scale. Artificial bee colony algorithm and support vector machine classifier have been used to designate the degree of optimal features and categorize the same. The grading of brain tumors from MRI images can be fulfilled by the aforementioned approach. The said approach is believed to deliver promising results in terms of accuracy, which has also been verified experimentally.																	1432-7643	1433-7479				MAY	2020	24	10					7827	7833		10.1007/s00500-019-04403-7													
J								A Hybrid E-Learning Recommendation Approach Based on Learners' Influence Propagation	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Electronic learning; Recommender systems; Uncertainty; Collaboration; Hafnium; Computational modeling; Data models; Personalized e-learning; adaptive and intelligent educational systems; hybrid recommendation; influence model; self-organization; recommender system	SYSTEM; STRATEGY; TRUST; MODEL	In e-learning recommender systems, interpersonal information between learners is very scarce, which makes it difficult to apply collaborative filtering (CF) techniques to achieve recommendations. In this study, we propose a hybrid filtering recommendation approach ($SI-IFL$SI-IFL) combining learner influence model (LIM), self-organization based (SOB) recommendation strategy, and sequential pattern mining (SPM) together for recommending learning objects (LOs) to learners. The method works as follows: (i) LIM is applied to acquire the interpersonal information by computing the influence that a learner exerts on others. LIM consists of learner similarity, knowledge credibility, and learner aggregation, meanwhile, LIM is independent of ratings. Furthermore, to address the uncertainty and fuzzy natures of learners, intuitionistic fuzzy logic (IFL) is applied to optimize the LIM. (ii) A SOB recommendation strategy is applied to recommend the optimal learner cliques for active learners by simulating the influence propagation among learners. Influence propagation means that a learner can move towards active learners, and such behaviors can stimulate the moving behaviors of his/her neighbors. This SOB recommendation approach achieves a stable structure based on distributed and bottom-up behaviors of individuals. (iii) SPM is applied to decide the final learning objects (LOs) and navigational paths based on the recommended learner cliques. The experimental results demonstrate that $SI-IFL$SI-IFL can provide personalized and diversified recommendations, and it shows promising efficiency and adaptability in e-learning scenarios.																	1041-4347	1558-2191				MAY 1	2020	32	5					827	840		10.1109/TKDE.2019.2895033													
J								A Scalable Multi-Data Sources Based Recursive Approximation Approach for Fast Error Recovery in Big Sensing Data on Cloud	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Sensors; Big Data; Cloud computing; Reliability; Complex networks; Time series analysis; Big Data; big sensing data; cloud; euclidean distance; error recovery		Big sensing data is commonly encountered from various surveillance or sensing systems. Sampling and transferring errors are commonly encountered during each stage of sensing data processing. How to recover from these errors with accuracy and efficiency is quite challenging because of high sensing data volume and unrepeatable wireless communication environment. While Cloud provides a promising platform for processing big sensing data, however scalable and accurate error recovery solutions are still need. In this paper, we propose a novel approach to achieve fast error recovery in a scalable manner on cloud. This approach is based on the prediction of a recovery replacement data by making multiple data sources based approximation. The approximation process will use coverage information carried by data units to limit the algorithm in a small cluster of sensing data instead of a whole data spectrum. Specifically, in each sensing data cluster, a Euclidean distance based approximation is proposed to calculate a time series prediction. With the calculated time series, a detected error can be recovered with a predicted data value. Through the experiment with real world meteorological data sets on cloud, we demonstrate that the proposed error recovery approach can achieve high accuracy in data approximation to replace the original data error. At the same time, with MapReduce based implementation for scalability, the experimental results also show significant efficiency on time saving.																	1041-4347	1558-2191				MAY 1	2020	32	5					841	854		10.1109/TKDE.2019.2895612													
J								Adversarial Training Towards Robust Multimedia Recommender System	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Perturbation methods; Predictive models; Multimedia systems; Visualization; Feature extraction; Recommender systems; Robustness; Multimedia recommendation; adversarial learning; personalized ranking; collaborative filtering		With the prevalence of multimedia content on the Web, developing recommender solutions that can effectively leverage the rich signal in multimedia data is in urgent need. Owing to the success of deep neural networks in representation learning, recent advances on multimedia recommendation has largely focused on exploring deep learning methods to improve the recommendation accuracy. To date, however, there has been little effort to investigate the robustness of multimedia representation and its impact on the performance of multimedia recommendation. In this paper, we shed light on the robustness of multimedia recommender system. Using the state-of-the-art recommendation framework and deep image features, we demonstrate that the overall system is not robust, such that a small (but purposeful) perturbation on the input image will severely decrease the recommendation accuracy. This implies the possible weakness of multimedia recommender system in predicting user preference, and more importantly, the potential of improvement by enhancing its robustness. To this end, we propose a novel solution named Adversarial Multimedia Recommendation (AMR), which can lead to a more robust multimedia recommender model by using adversarial learning. The idea is to train the model to defend an adversary, which adds perturbations to the target image with the purpose of decreasing the model's accuracy. We conduct experiments on two representative multimedia recommendation tasks, namely, image recommendation and visually-aware product recommendation. Extensive results verify the positive effect of adversarial learning and demonstrate the effectiveness of our AMR method. Source codes are available in https://github.com/duxy-me/AMR.																	1041-4347	1558-2191				MAY 1	2020	32	5					855	867		10.1109/TKDE.2019.2893638													
J								ASCENT: Active Supervision for Semi-Supervised Learning	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Task analysis; Clustering algorithms; Data models; Redundancy; Uncertainty; Semisupervised learning; Active learning; semi-supervised learning; iterative learning; clustering; classification; data filtering	REGRESSION	Active learning algorithms attempt to overcome the labeling bottleneck by asking queries from large collection of unlabeled examples. Existing batch mode active learning algorithms suffer from three limitations: (1) The methods that are based on similarity function or optimizing certain diversity measurement, in which may lead to suboptimal performance and produce the selected set with redundant examples. (2) The models with assumption on data are hard in finding images that are both informative and representative. (3) The problem of noise labels has been an obstacle for algorithms. In this paper, we propose a novel active learning method that makes embeddings of labeled examples to those of unlabeled ones and back via deep neural networks. The active scheme makes correct association cycles that end up at the same class from that the association was started, which considers both the informativeness and representativeness of examples, as well as being robust to the noise labels. We apply our active learning method to semi-supervised classification and clustering. The submodular function is designed to reduce the redundancy of the selected examples. Specifically, we incorporate our batch mode active scheme into the classification approaches, in which the generalization ability is improved. For semi-supervised clustering, we try to use our active scheme for constraints to make fast convergence and perform better than unsupervised clustering. Finally, we apply our active learning method to data filtering. To validate the effectiveness of the proposed algorithms, extensive experiments are conducted on diversity benchmark datasets for different tasks, i.e., classification, clustering, and data filtering, and the experimental results demonstrate consistent and substantial improvements over the state-of-the-art approaches.																	1041-4347	1558-2191				MAY 1	2020	32	5					868	882		10.1109/TKDE.2019.2897307													
J								Boosting with Lexicographic Programming: Addressing Class Imbalance without Cost Tuning	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Boosting; Tuning; Fasteners; Games; Training; Linear programming; Proposals; Boosting; imbalanced classification; lexicographic linear programming; cost set tuning; multi-criterion decision making	CLASSIFICATION; ALGORITHMS	A large amount of research effort has been dedicated to adapting boosting for imbalanced classification. However, boosting methods are yet to be satisfactorily immune to class imbalance, especially for multi-class problems. This is because most of the existing solutions for handling class imbalance rely on expensive cost set tuning for determining the proper level of compensation. We show that the assignment of weights to the component classifiers of a boosted ensemble can be thought of as a game of Tug of War between the classes in the margin space. We then demonstrate how this insight can be used to attain a good compromise between the rare and abundant classes without having to resort to cost set tuning, which has long been the norm for imbalanced classification. The solution is based on a lexicographic linear programming framework which requires two stages. Initially, class-specific component weight combinations are found so as to minimize a hinge loss individually for each of the classes. Subsequently, the final component weights are assigned so that the maximum deviation from the class-specific minimum loss values (obtained in the previous stage) is minimized. Hence, the proposal is not only restricted to two-class situations, but is also readily applicable to multi-class problems. Additionally, we also derive the dual formulation corresponding to the proposed framework. Experiments conducted on artificial and real-world imbalanced datasets as well as on challenging applications such as hyperspectral image classification and ImageNet classification establish the efficacy of the proposal.																	1041-4347	1558-2191				MAY 1	2020	32	5					883	897		10.1109/TKDE.2019.2894148													
J								Control-Flow Modeling with Declare: Behavioral Properties, Computational Complexity, and Tools	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Lenses; Computational modeling; Process control; Tools; Analytical models; NP-hard problem; Computational complexity; Declarative process modelling; linear temporal logic; declare; process mining	LANGUAGE; LTL	Declarative approaches to control-flow modeling use logic-based languages to formalize a number of constraints that valid traces must satisfy. The most noticeable example is the Declare framework based on linear temporal logic. Despite the interest that Declare has been attracting, the current knowledge about its formal properties was rather limited. The goal of this paper is to fill this gap by: (i) analyzing the behavioral properties of Declare by comparing it with the modeling capabilities of traditional procedural design approaches, in particular, block-structured processes; (ii) analyzing Declare from the computational point of view. As for the former point, we identify both the block-structured processes constructs that can be simulated in Declare and the features of Declare that can be encoded in block-structured processes. As for the latter point, we show that checking whether a given set of Declare patterns admits a satisfying trace is an ${\mathrm {NP}}$ NP -hard problem. In particular, we identify some Declare specifications whose satisfying traces are all of exponential length and some useful Declare fragments where a satisfying trace whose length is polynomially bounded is guaranteed to exist. The paper also discusses the declare2sat prototype system and the results of a thorough experimental validation.																	1041-4347	1558-2191				MAY 1	2020	32	5					898	911		10.1109/TKDE.2019.2897309													
J								Efficient Entity Resolution on Heterogeneous Records	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Erbium; Indexes; Measurement; Merging; Companies; Data integration; Transforms; Data source; data integration; entity resolution	DATA EXCHANGE; FRAMEWORK	Entity resolution (ER) is the problem of identifying and merging records that refer to the same real-world entity. In many scenarios, raw records are stored under heterogeneous environment. Specifically, the schemas of records may differ from each other. To leverage such records better, most existing work assume that schema matching and data exchange have been done to convert records under different schemas to those under a predefined schema. However, we observe that schema matching would lose information in some cases, which could be useful or even crucial to ER. To leverage sufficient information from heterogeneous sources, in this paper, we address several challenges of ER on heterogeneous records and show that none of existing similarity metrics or their transformations could be applied to find similar records under heterogeneous settings. Motivated by this, we design the similarity function and propose a novel framework to iteratively find records which refer to the same entity. Regarding efficiency, we build an index to generate candidates and accelerate similarity computation. Evaluations on real-world datasets show the effectiveness and efficiency of our methods.																	1041-4347	1558-2191				MAY 1	2020	32	5					912	926		10.1109/TKDE.2019.2898191													
J								Efficient Process Conformance Checking on the Basis of Uncertain Event-to-Activity Mappings	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Uncertainty; Adaptation models; Task analysis; Probabilistic logic; Information systems; Cryptography; Business process management; business process monitoring	BUSINESS PROCESS MODELS; SEMANTIC-INTEGRATION; AGGREGATION; PRECISION	Conformance checking enables organizations to automatically identify compliance violations based on the analysis of observed event data. A crucial requirement for conformance-checking techniques is that observed events can be mapped to normative process models used to specify allowed behavior. Without a mapping, it is not possible to determine if an observed event trace conforms to the specification or not. A considerable problem in this regard is that establishing a mapping between events and process model activities is an inherently uncertain task. Since the use of a particular mapping directly influences the conformance of an event trace to a specification, this uncertainty represents a major issue for conformance checking. To overcome this issue, we introduce a probabilistic conformance-checking technique that can deal with uncertain mappings. Our technique avoids the need to select a single mapping by taking the entire spectrum of possible mappings into account. A quantitative evaluation demonstrates that our technique can be applied on a considerable number of real-world processes where existing conformance-checking techniques fail.																	1041-4347	1558-2191				MAY 1	2020	32	5					927	940		10.1109/TKDE.2019.2897557													
J								Generalized Translation-Based Embedding of Knowledge Graph	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Knowledge graph embedding		Knowledge graphs are useful for many AI tasks but often have missing facts. To populate the graphs, knowledge graph embedding models have been developed. TransE is one of such models and the first translation-based method. TransE is well known because the principle of TransE can effectively capture the rules of a knowledge graph although it seems very simple. However, TransE has problems with its regularization and an unchangeable ratio of negative sampling. In this paper, we generalize TransE to solve these problems by proposing knowledge graph embedding on a Lie group (KGLG) and the Weighted Negative Part (WNP) method for the objective function of translation-based models. KGLG is the novel translation-based method which embeds entities and relations of a knowledge graph on any Lie group. It allows us not to employ regularization during training of the model if we choose a compact lie group for the embedding space. The WNP method is for changing the ratio of negative sampling, which enhances translation-based models. Our approach outperforms other state-of-the-art approaches such as TransE, DistMult, and ComplEx on a standard link prediction task. We show that TorusE, KGLG on a torus, is scalable to large-size knowledge graphs and faster than the original TransE.																	1041-4347	1558-2191				MAY 1	2020	32	5					941	951		10.1109/TKDE.2019.2893920													
J								Joint Label Prediction Based Semi-Supervised Adaptive Concept Factorization for Robust Data Representation	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Manifolds; Image reconstruction; Kernel; Adaptation models; Data models; Data mining; Principal component analysis; Robust discriminative data representation; semi-supervised adaptive concept factorization; joint label prediction	DIMENSIONALITY REDUCTION; MATRIX FACTORIZATION; RECOGNITION; FRAMEWORK; ONLINE; VECTOR; AREA	Constrained Concept Factorization (CCF) yields the enhanced representation ability over CF by incorporating label information as additional constraints, but it cannot classify and group unlabeled data appropriately. Minimizing the difference between the original data and its reconstruction directly can enable CCF to model a small noisy perturbation, but is not robust to gross sparse errors. Besides, CCF cannot preserve the manifold structures in new representation space explicitly, especially in an adaptive manner. In this paper, we propose a joint label prediction based Robust Semi-Supervised Adaptive Concept Factorization (RS(2)ACF) framework. To obtain robust representation, RS(2)ACF relaxes the factorization to make it simultaneously stable to small entrywise noise and robust to sparse errors. To enrich prior knowledge to enhance the discrimination, RS(2)ACF clearly uses class information of labeled data and more importantly propagates it to unlabeled data by jointly learning an explicit label indicator for unlabeled data. By the label indicator, RS(2)ACF can ensure the unlabeled data of the same predicted label to be mapped into the same class in feature space. Besides, RS(2)ACF incorporates the joint neighborhood reconstruction error over the new representations and predicted labels of both labeled and unlabeled data, so the manifold structures can be preserved explicitly and adaptively in the representation space and label space at the same time. Owing to the adaptive manner, the tricky process of determining the neighborhood size or kernel width can be avoided. Extensive results on public databases verify that our RS(2)ACF can deliver state-of-the-art data representation, compared with other related methods.																	1041-4347	1558-2191				MAY 1	2020	32	5					952	970		10.1109/TKDE.2019.2893956													
J								Joint Learning of Question Answering and Question Generation	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Task analysis; Training; Mathematical model; Knowledge discovery; Data models; Computational modeling; Collaboration; Question answering; question generation		Question answering (QA) and question generation (QG) are closely related tasks that could improve each other; however, the connection of these two tasks is not well explored in the literature. In this paper, we present two training algorithms for learning better QA and QG models through leveraging one another. The first algorithm extends Generative Adversarial Network (GAN), which selectively incorporates artificially generated instances as additional QA training data. The second algorithm is an extension of dual learning, which incorporates the probabilistic correlation of QA and QG as additional regularization in training objectives. To test the scalability of our algorithms, we conduct experiments on both document based and table based question answering tasks. Results show that both algorithms improve a QA model in terms of accuracy and QG model in terms of BLEU score. Moreover, we find that the performance of a QG model could be easily improved by a QA model via policy gradient, however, directly applying GAN that regards all the generated questions as negative instances could not improve the accuracy of the QA model. Our algorithm that selectively assigns labels to generated questions would bring a performance boost.																	1041-4347	1558-2191				MAY 1	2020	32	5					971	982		10.1109/TKDE.2019.2897773													
J								K-SPIN: Efficiently Processing Spatial Keyword Queries on Road Networks	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Roads; Indexing; Throughput; Delays; Search engines; Approximation algorithms; Road networks; points of interest search; spatio-textual queries; network Voronoi diagrams		A significant proportion of all search volume consists of local searches. As a result, search engines must be capable of finding relevant results combining both spatial proximity and textual relevance with high query throughput. We observe that existing techniques answering these spatial keyword queries use keyword aggregated indexing, which has several disadvantages on road networks. We propose K-SPIN, a versatile framework that instead uses keyword separated indexing to delay and avoid expensive operations. At first glance, this strategy appears to have impractical pre-processing costs. However, by exploiting several useful observations, we make the indexing cost not only viable but also light-weight. For example, we propose a novel $\rho$rho-Approximate Network Voronoi Diagram (NVD) with one order of magnitude less space cost than exact NVDs. By carefully exploiting features of the K-SPIN framework, our query algorithms are up to two orders of magnitude more efficient than the state-of-the-art as shown in our experimental investigation on various queries, parameter settings, and real road network and keyword datasets.																	1041-4347	1558-2191				MAY 1	2020	32	5					983	997		10.1109/TKDE.2019.2894140													
J								Semi-Supervised Deep Learning Approach for Transportation Mode Identification Using GPS Trajectory Data	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Deep learning; semi-supervised learning; convolutional neural network; convolutional autoencoder; GPS trajectory data; trip segmentation; transportation mode identification		Identification of travelers' transportation modes is a fundamental step for various problems that arise in the domain of transportation such as travel demand analysis, transport planning, and traffic management. In this paper, we aim to identify travelers' transportation modes purely based on their GPS trajectories. First, a segmentation process is developed to partition a user's trip into GPS segments with only one transportation mode. A majority of studies have proposed mode inference models based on hand-crafted features, which might be vulnerable to traffic and environmental conditions. Furthermore, the classification task in almost all models have been performed in a supervised fashion while a large amount of unlabeled GPS trajectories has remained unused. Accordingly, we propose a deep SEmi-Supervised Convolutional Autoencoder (SECA) architecture that can not only automatically extract relevant features from GPS segments but also exploit useful information in unlabeled data. The SECA integrates a convolutional-deconvolutional autoencoder and a convolutional neural network into a unified framework to concurrently perform supervised and unsupervised learning. The two components are simultaneously trained using both labeled and unlabeled GPS segments, which have already been converted into an efficient representation for the convolutional operation. An optimum schedule for varying the balancing parameters between reconstruction and classification errors are also implemented. The performance of the proposed SECA model, trip segmentation, the method for converting a raw trajectory into a new representation, the hyperparameter schedule, and the model configuration are evaluated by comparing to several baselines and alternatives for various amounts of labeled and unlabeled data. Our experimental results demonstrate the superiority of the proposed model over the state-of-the-art semi-supervised and supervised methods with respect to metrics such as accuracy and F-measure.																	1041-4347	1558-2191				MAY 1	2020	32	5					1010	1023		10.1109/TKDE.2019.2896985													
J								Trust Relationship Prediction in Alibaba E-Commerce Platform	IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING										Correlation; Feature extraction; Business; Twitter; Graphical models; Prediction algorithms; Social network; trust relationship prediction		This paper introduces how to infer trust relationships from billion-scale networked data to benefit Alibaba E-Commerce business. To effectively leverage the network correlations between labeled and unlabeled relationships to predict trust relationships, we formalize trust into multiple types and propose a graphical model to incorporate type-based dyadic and triadic correlations, namely eTrust. We also present a fast learning algorithm in order to handle billion-scale networks. Systematically, we evaluate the proposed methods on four different genres of datasets with labeled trust relationships: Alibaba, Epinions, Ciao, and Advogato. Experimental results show that the proposed methods achieve significantly better performance than several comparison methods (+1.7-32.3% by accuracy; $p<<0.01$p<<0.01, with $t$t-test). Most importantly, when handling the real large networked data with over 1,200,000,000 edges (Ali-large), our method achieves 2,000x speedup to infer trust relationships, comparing with the traditional graph learning algorithms. Finally, we have applied the inferred trust relationships to Alibaba E-commerce platform: Taobao, and achieved 2.75 percent improvement on gross merchandise volume (GMV).																	1041-4347	1558-2191				MAY 1	2020	32	5					1024	1035		10.1109/TKDE.2019.2893939													
J								Two novel ELM-based stacking deep models focused on image recognition	APPLIED INTELLIGENCE										Deep learning; Extreme learning machine (ELM); Stacking generalization philosophy; Random recursive constrained ELM ((RCELM)-C-2); Random recursive local-receptive-fields-based ELM ((RELM)-E-2-LRF); Image recognition	EXTREME LEARNING-MACHINE; REPRESENTATIONS; AUTOENCODERS	Extreme learning machine (ELM) and its variants have been widely used in the field of object recognition and other complex classification tasks. Traditional deep learning architectures like Convolutional Neural Network (CNN) are capable of extracting high-level features, which are the key for the models to make right decisions. However, traditional deep architectures are confronted with solving a tough, non-convex optimization problem, which is a time-consuming process. In this paper, we propose two hierarchical models, i.e., Random Recursive Constrained ELM ((RCELM)-C-2) and Random Recursive Local- Receptive-Fields-Based ELM ((RELM)-E-2-LRF), which are constructed by stacking with CELM or ELM-LRF, respectively. Besides, inspired by the stacking generalization philosophy, random projection and kernelization are incorporated as their constitutive elements. (RCELM)-C-2 and (RELM)-E-2-LRF not only fully inherit the merits of ELM, but also take advantage of the superiority of CELM and ELM-LRF in the field of image recognition, respectively. The essence of CELM is to constrain the weight vectors from the input layer to the hidden layer to be consistent with the directions from one class to another class, while ELM-LRF is adept at exploiting the local structures in images through many local receptive fields. In the empirical results, (RCELM)-C-2 and (RELM)-E-2-LRF demonstrate their better performance in testing accuracy on the six benchmark image recognition datasets, compared with their basic learners and other state-of-the-art algorithms. Moreover, the proposed two deep ELM models need less training time when compared with traditional Deep Neural Network (DNN) based models.																	0924-669X	1573-7497				MAY	2020	50	5					1345	1366		10.1007/s10489-019-01584-4													
J								Asymmetric response aggregation heuristics for rating prediction and recommendation	APPLIED INTELLIGENCE										Collaborative filtering; Response; Positive suggestions; Negative suggestions; Linear regression method	USER SIMILARITY MODEL; REPUTATION; ATTENTION; STRATEGY; SYSTEMS	User-based collaborative filtering is widely used in recommendation systems, which normally comprises three steps: (1) finding the nearest conceptual neighbors, (2) aggregating the neighbors' ratings to predict the ratings of unrated items, and (3) generating recommendations based on the prediction. Existing algorithms mainly focus on steps 1 and 3 but neglect subtle treatments of aggregating neighbors' suggestions in step 2. Based on the discovery of psychology that (i) users' responses to positive and negative suggestions are different, and (ii) users may respond differently from one another, this paper proposes a Personal Asymmetry Response-based Suggestions Aggregation (PARSA) algorithm, which first uses a linear regression method to learn each user's response to negative/positive suggestions from neighbors and then uses a gradient descent algorithm for optimizing them. In addition, this paper designs an Identical Asymmetry Response-based Suggestions Aggregation (IARSA) baseline algorithm, which assumes that all the users' responses to suggestions are identical as references to verify the key contribution of the heuristics employed in our PARSA algorithm that user may responses differently to positive and negative suggestions. Three sets of experiments are designed and implemented over two real-life datasets (i.e., Eachmovie and Netflix) to evaluate the performance of our algorithms. Further, in order to eliminate the influence of different similarity measures, this paper selects three kinds of similarity measures to discover neighbors. Experimental results demonstrate that most people indeed pay more attention to negative suggestions and our algorithms achieve better prediction and recommendation performances than the compared algorithms under various similarity measures.																	0924-669X	1573-7497				MAY	2020	50	5					1416	1436		10.1007/s10489-019-01594-2													
J								STDS: self-training data streams for mining limited labeled data in non-stationary environment	APPLIED INTELLIGENCE										Semi-supervised learning; Self-training; Data streams; Concept drift; Clustering algorithm	CONCEPT DRIFT; EVOLVING DATA; ENSEMBLE; FRAMEWORK	Inthis article, wefocus on the classification problem to semi-supervised learning in non-stationary environment. Semi-supervised learning is a learning task from both labeled and unlabeled data points. There are several approaches to semi-supervised learning in stationary environment which are not applicable directly for data streams. We propose a novel semi-supervised learning algorithm, named STDS. The proposed approach uses labeled and unlabeled data and employs an approach to handle the concept drift in data streams. The main challenge in semi-supervised self-training for data streams is to find a proper selection metric in order to find a set of high-confidence predictions and a proper underlying base learner. We therefore propose an ensemble approach to find a set of high-confidence predictions based on clustering algorithms and classifier predictions. We then employ the Kullback-Leibler (KL) divergence approach to measure the distribution differences between sequential chunks in order to detect the concept drift. When drift is detected, a new classifier is updated from the new set of labeled data in the current chunk; otherwise, a percentage of high-confidence newly labeled data in the current chunk is added to the labeled data in the next chunk for updating the incremental classifier based on the proposed selection metric. The results of our experiments on a number of classification benchmark datasets show that STDS outperforms the supervised and the most of other semi-supervised learning methods.																	0924-669X	1573-7497				MAY	2020	50	5					1448	1467		10.1007/s10489-019-01585-3													
J								Multi-scale affined-HOF and dimension selection for view-unconstrained action recognition	APPLIED INTELLIGENCE										Action recognition; View-invariant; Affine transform; Histogram of optical flow; Dimension selection; Voting algorithm		In this paper an action recognition method that can adaptively handle the problems of variations in camera viewpoint is introduced. Our contribution is three-fold. First, a space-sampling algorithm based on affine transform in multiple scales is proposed to yield a series of different viewpoints from a single one. A histogram of dense optical flow is then extracted over each fixed-size patch for a given generated viewpoint as a local feature descriptor. Second, a dimension selection procedure is also proposed to retain only the dimensions that have distinctive information and discard the unnecessary ones in the feature vector space. Third, to adapt to a situation in which video data in multiple viewpoints are used for training; an extended method with a voting algorithm is also introduced to increase the recognition accuracy. By conducting experiments using both simulated and realistic datasets (), the proposed method is validated. The method is found to be accurate and capable of maintaining its accuracy under a wide range of viewpoint changes. In addition, the method is less sensitive to variations in subject scale, subject position, action speed, partial occlusion, and background. The method is also validated by comparing with state-of-the-art view-invariant action recognition methods using well-known i3DPost and MuHAVi public datasets.																	0924-669X	1573-7497				MAY	2020	50	5					1468	1486		10.1007/s10489-019-01572-8													
J								Mining top-k frequent patterns from uncertain databases	APPLIED INTELLIGENCE										Pattern mining; Uncertain frequent pattern; Top-k uncertain frequent patterns	EFFICIENT ALGORITHMS; ITEMSETS; PREDICTION; BITTABLEFI; CHILDREN	Mining uncertain frequent patterns (UFPs) from uncertain databases was recently introduced, and there are various approaches to solve this problem in the last decade. However, systems are often faced with the problem of too many UFPs being discovered by the traditional approaches to this issue, and thus will spend a lot of time and resources to rank and find the most promising patterns. Therefore, this paper introduces a task named mining top-k UFPs from uncertain databases. We then propose an efficient method named TUFP (mining Top-k UFPs) to carry this out. Effective threshold raising strategies are introduced to help the proposed algorithm reduce the number of generated candidates to enhance the performance in terms of the runtime as well as memory usage. Finally, several experiments on the number of generated candidates, mining time, memory usage and scalability of TUFP and two state-of-the-art approaches (CUFP-mine and LUNA) were conducted. The performance studies show that TUFP is efficient in terms of mining time, memory usage and scalability for mining top-k UFPs.																	0924-669X	1573-7497				MAY	2020	50	5					1487	1497		10.1007/s10489-019-01622-1													
J								A multi-population differential evolution with best-random mutation strategy for large-scale global optimization	APPLIED INTELLIGENCE										Differential evolution; Large-Scale Global Optimization; The best-random mutation strategy; Multi-populations; Migration strategy	ENSEMBLE; PARAMETERS; ALGORITHM	Differential evolution (DE) is an efficient population-based search algorithm with good robustness, but it faces challenges in dealing with Large-Scale Global Optimization (LSGO). In this paper, we proposed an improved multi-population differential evolution with best-random mutation strategy (called mDE-brM). The population is divided into three sub-populations based on the fitness values, each sub-population uses different mutation strategies and control parameters, individuals share different mutation strategies and control parameters by migrating among sub-populations. A novel mutation strategy is proposed, which uses the best individual and a randomly selected individual to generate base vector. The performance of mDE-brM is evaluated on the CEC 2013 LSGO benchmark suite and compared with 5 state-of-the-art optimization techniques. The results show that, compared with other contestant algorithms, mDE-brM has a competitive performance and better efficiency in LSGO.																	0924-669X	1573-7497				MAY	2020	50	5					1510	1526		10.1007/s10489-019-01613-2													
J								Keyword extraction: Issues and methods	NATURAL LANGUAGE ENGINEERING										Information extraction; Keyword extraction; Extraction features; Extraction methods	KEYPHRASE EXTRACTION; SINGLE DOCUMENT; SEARCH	Due to the considerable growth of the volume of text documents on the Internet and in digital libraries, manual analysis of these documents is no longer feasible. Having efficient approaches to keyword extraction in order to retrieve the 'key' elements of the studied documents is now a necessity. Keyword extraction has been an active research field for many years, covering various applications in Text Mining, Information Retrieval, and Natural Language Processing, and meeting different requirements. However, it is not a unified domain of research. In spite of the existence of many approaches in the field, there is no single approach that effectively extracts keywords from different data sources. This shows the importance of having a comprehensive review, which discusses the complexity of the task and categorizes the main approaches of the field based on the features and methods of extraction that they use. This paper presents a general introduction to the field of keyword/keyphrase extraction. Unlike the existing surveys, different aspects of the problem along with the main challenges in the field are discussed. This mainly includes the unclear definition of 'keyness', complexities of targeting proper features for capturing desired keyness properties and selecting efficient extraction methods, and also the evaluation issues. By classifying a broad range of state-of-the-art approaches and analysing the benefits and drawbacks of different features and methods, we provide a clearer picture of them. This review is intended to help readers find their way around all the works related to keyword extraction and guide them in choosing or designing a method that is appropriate for the application they are targeting.																	1351-3249	1469-8110				MAY	2020	26	3					259	291	PII S1351324919000457	10.1017/S1351324919000457													
J								Learning keyphrases from corpora and knowledge models	NATURAL LANGUAGE ENGINEERING										Keyphrases extraction; Keyphrases absent from the text; Inference of keyphrases; Knowledge models	EXTRACTION	Extraction keyphrase systems traditionally use classification algorithms and do not consider the fact that part of the keyphrases may not be found in the text, reducing the accuracy of such algorithms a priori. In this work, we propose to improve the accuracy of these systems with inferential mechanisms that use a knowledge representation model, including symbolic models of knowledge bases and distributional semantics, to expand the set of keyphrase candidates to be submitted to the classification algorithm with terms that are not in the text (not-in-text terms). The basic assumption we have is that not-in-text terms have a semantic relationship with terms that are in the text. To represent this relationship, we have defined two new features to be represented as input to the classification algorithms. The first feature refers to the power of discrimination of the inferred not-in-text terms. The intuition behind this is that good candidates for a keyphrase are those that are deduced from various textual terms in a specific document and that are not often deduced in other documents. The other feature represents the descriptive strength of a not-in-text candidate. We argue that not-in-text keyphrases must have a strong semantic relationship with the text and that the power of this semantic relationship can be measured in a similar way as popular metrics like TFxIDF. The method proposed in this work was compared with state-of-the-art systems using five corpora and the results show that it has significantly improved automatic keyphrase extraction, dealing with the limitation of extracting keyphrases absent from the text.																	1351-3249	1469-8110				MAY	2020	26	3					293	318	PII S1351324919000342	10.1017/S1351324919000342													
J								Detecting light verb constructions across languages	NATURAL LANGUAGE ENGINEERING										Semantics; Machine learning; Lexicography; Multilinguality		Light verb constructions (LVCs) are verb and noun combinations in which the verb has lost its meaning to some degree and the noun is used in one of its original senses, typically denoting an event or an action. They exhibit special linguistic features, especially when regarded in a multilingual context. In this paper, we focus on the automatic detection of LVCs in raw text in four different languages, namely, English, German, Spanish, and Hungarian. First, we analyze the characteristics of LVCs from a linguistic point of view based on parallel corpus data. Then, we provide a standardized (i.e., language-independent) representation of LVCs that can be used in machine learning experiments. After, we experiment on identifying LVCs in different languages: we exploit language adaptation techniques which demonstrate that data from an additional language can be successfully employed in improving the performance of supervised LVC detection for a given language. As there are several annotated corpora from several domains in the case of English and Hungarian, we also investigate the effect of simple domain adaptation techniques to reduce the gap between domains. Furthermore, we combine domain adaptation techniques with language adaptation techniques for these two languages. Our results show that both out-domain and additional language data can improve performance. We believe that our language adaptation method may have practical implications in several fields of natural language processing, especially in machine translation.																	1351-3249	1469-8110				MAY	2020	26	3					319	348	PII S1351324919000330	10.1017/S1351324919000330													
J								Using linguistically defined specific details to detect deception across domains	NATURAL LANGUAGE ENGINEERING										Deception detection; Cross-domain; Specific details; Linguistic features	OPINIONS	Current automatic deception detection approaches tend to rely on cues that are based either on specific lexical items or on linguistically abstract features that are not necessarily motivated by the psychology of deception. Notably, while approaches relying on such features can do well when the content domain is similar for training and testing, they suffer when content changes occur. We investigate new linguistically defined features that aim to capture specific details, a psychologically motivated aspect of truthful versus deceptive language that may be diagnostic across content domains. To ascertain the potential utility of these features, we evaluate them on data sets representing a broad sample of deceptive language, including hotel reviews, opinions about emotionally charged topics, and answers to job interview questions. We additionally evaluate these features as part of a deception detection classifier. We find that these linguistically defined specific detail features are most useful for cross-domain deception detection when the training data differ significantly in content from the test data, and particularly benefit classification accuracy on deceptive documents. We discuss implications of our results for general-purpose approaches to deception detection.																	1351-3249	1469-8110				MAY	2020	26	3					349	373	PII S1351324919000408	10.1017/S1351324919000408													
J								Emerging trends: Subwords, seriously?	NATURAL LANGUAGE ENGINEERING										Subwords; Word pieces; Tokenization; Morphology; Etymology		Subwords have become very popular, but the BERTa and ERNIEb tokenizers often produce surprising results. Byte pair encoding (BPE) trains a dictionary with a simple information theoretic criterion that sidesteps the need for special treatment of unknown words. BPE is more about training (populating a dictionary of word pieces) than inference (parsing an unknown word into word pieces). The parse at inference time can be ambiguous. Which parse should we use? For example, "electroneutral" can be parsed as electron-eu-tral or electro-neutral, and "bidirectional" can be parsed as bid-ire-ction-al and bi-directional. BERT and ERNIE tend to favor the parse with more word pieces. We propose minimizing the number of word pieces. To justify our proposal, a number of criteria will be considered: sound, meaning, etc. The prefix, bi-, has the desired vowel (unlike bid) and the desired meaning (bi is Latin for two, unlike bid, which is Germanic for offer).																	1351-3249	1469-8110				MAY	2020	26	3					375	382	PII S1351324920000145	10.1017/S1351324920000145													
J								Learning robots to grasp by demonstration	ROBOTICS AND AUTONOMOUS SYSTEMS										Artificial neural networks; Machine learning; Collaborative robotics; Industrial internet of things	MANIPULATION	In recent years, we have witnessed the proliferation of so-called collaborative robots or cobots, that are designed to work safely along with human operators. These cobots typically use the "program from demonstration'' paradigm to record and replay trajectories, rather than the traditional sourcecode based programming approach. While this requires less knowledge from the operator, the basic functionality of a cobot is limited to simply replay the sequence of actions as they were recorded. In this paper, we present a system that mitigates this restriction and learns to grasp an arbitrary object from visual input using demonstrated examples. While other learning-based approaches for robotic grasping require collecting a large amount of examples, either manually or automatically harvested in a real or simulated world, our approach learns to grasp from a single demonstration with the ability to improve on accuracy using additional input samples. We demonstrate grasping of various objects with the Franka Panda collaborative robot. We show that the system is able to grasp various objects from demonstration, regardless their position and rotation in less than 5 min of training time on a NVIDIA Titan X GPU, achieving over 90% average success rate. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				MAY	2020	127								103474	10.1016/j.robot.2020.103474													
J								Human-in-the-loop optimization of wearable robots to reduce the human metabolic energy cost in physical movements	ROBOTICS AND AUTONOMOUS SYSTEMS										Wearable robotics; Human-robot interaction; Human-in-the-loop design	DYNAMIC SIMULATIONS; HUMAN WALKING; ASSISTANCE	Most designs of wearable robots are based on human biomechanical statistics, engineering experience or individual experiments. Despite great successes, few of them consider the human-robot integration and individual differences between users. Additionally, the design periods, cost and safety also need to be further improved. Learning from the natural driving mechanism of human body, we propose a general human-in-the-loop (HIL) optimization designing approach for this kind of wearable robots. Firstly, the human-robot coupling model of the personalized wearable robot and the human musculoskeletal model are established. Then, the Computed Muscle Control (CMC) tool embedded in software OpenSim and the Bayesian optimization used in machine learning are combined to find the optimal design scheme for the personalized wearable robots to reduce the human metabolic energy cost in specific physical movement. The HIL approach could not only optimize the control parameters of wearable robots, but also optimize their geometry, material and any other design parameters flexibly and effectively. An application example for the HIL approach is also provided to help designers better understand and use the HIL method proposed in this paper. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				MAY	2020	127								103495	10.1016/j.robot.2020.103495													
J								Roombots extended: Challenges in the next generation of self-reconfigurable modular robots and their application in adaptive and assistive furniture	ROBOTICS AND AUTONOMOUS SYSTEMS										Self-reconfiguring; Modular robots; Universal Gripper; Assistive furniture; Adaptive furniture	CENTRAL PATTERN GENERATORS; REAL-TIME; LOCOMOTION; TRACKING; REALIZATION; SYSTEM	This work presents a series of demonstrations of our self-reconfigurable modular robots (SRMR) "Roombots'' in the context of adaptive and assistive furniture. In literature, simulations are often ahead of what currently can be demonstrated in hardware with such systems due to significant challenges in transferring them to the real world. Here, we describe how Roombots tackled these difficulties in real hardware and focus qualitatively on selected hardware experiments rather than on quantitative measurements (in hardware and simulation) to showcase the many possibilities of an SRMR. We envision Roombots to be used in our living space and define five key tasks that such a system must possess. Consequently, we demonstrate these tasks, including self-reconfiguration with 12 modules (36 Degrees of Freedom), autonomously moving furniture, object manipulation and gripping capabilities, human-module-interaction and the development of an easy-to-use user interface. We conclude with the remaining challenges and point out possible directions of research for the future of adaptive and assistive furniture with Roombots. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				MAY	2020	127								103467	10.1016/j.robot.2020.103467													
J								Hybrid strategy based model parameter estimation of irregular-shaped underwater vehicles for predicting velocity	ROBOTICS AND AUTONOMOUS SYSTEMS										Hydrodynamic model; Parameter estimation; Underwater vehicles; Model-based velocity prediction; Hybrid strategy	PARTICLE SWARM OPTIMIZER; IDENTIFICATION	The hydrodynamic model can be used to predict velocity of underwater vehicles in still water. However, there are few economical and effective methods for estimating the hydrodynamic parameters of irregular-shaped underwater vehicles. Thus, this paper proposes a hybrid estimation strategy, which contains a rough estimation using a least squares (LS) based algorithm and a precise estimation using an improved particle swarm optimization (IPSO) algorithm. The numerical simulation and field data based tests suggest that the accuracy of the predicted velocity using the hydrodynamic parameters estimated by the IPSO-based hybrid strategy is better than two state-of-the-art algorithms. Finally, a pool experiment is conducted to verify the accuracy of the predicted horizontal velocity of the underwater vehicle. (C) 2020 Published by Elsevier B.V.																	0921-8890	1872-793X				MAY	2020	127								103480	10.1016/j.robot.2020.103480													
J								Stochastic triangular mesh mapping: A terrain mapping technique for autonomous mobile robots	ROBOTICS AND AUTONOMOUS SYSTEMS										Dense mapping; Submapping; Perception; Triangular mesh; Probabilistic graphical model; Stereo cameras; LiDAR	OCCUPANCY GRID MAPS; SIMULTANEOUS LOCALIZATION; BELIEF PROPAGATION; ENVIRONMENTS; NAVIGATION; SPACE; SLAM	For mobile robots to operate autonomously in general environments, perception is required in the form of a dense metric map. For this purpose, we present the stochastic triangular mesh (STM) mapping technique: a 2.5-D representation of the surface of the environment using a continuous mesh of triangular surface elements, where each surface element models the mean plane and roughness of the underlying surface. In contrast to existing mapping techniques, an STM map models the structure of the environment by ensuring a continuous model, while also being able to be incrementally updated with linear computational cost in the number of measurements. We reduce the effect of uncertainty in the robot pose (position and orientation) by using landmark-relative submaps. The uncertainty in the measurements and robot pose are accounted for by the use of Bayesian inference techniques during the map update. We demonstrate that an STM map can be used with sensors that generate point measurements, such as stochastic triangular mesh (LiDAR) sensors and stereo cameras. We show that an STM map is a more accurate model than the only comparable online surface mapping technique a standard elevation map - and we also provide qualitative results on practical datasets. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				MAY	2020	127								103449	10.1016/j.robot.2020.103449													
J								Development of 3UPU-I parallel sensor with six division-force limbs for measuring robotic wrist load	ROBOTICS AND AUTONOMOUS SYSTEMS										Parallel structure sensor; Division-force limb; Six-component force/torque; Performance evaluation	FORCE/TORQUE SENSOR	A 3UPU-I parallel sensor with six division-force limbs and six standard force sensors is developed for measuring robotic wrist load. Its measuring approach and performances are studied and evaluated. A prototype of the developed parallel sensor is built up and its merits are analyzed. A statics equation among the forces of the six standard force sensors and the wrist load is established, and a mapped matrix from the workload to the forces of the six standard force sensors is derived based on a 3UPU-I parallel mechanism of the developed parallel sensor. The performances of the developed parallel sensor are analyzed and evaluated by respectively varying key parameters for constructing the developed parallel sensor, and the reasonable values of the key parameters are determined. The forces of the six standard force sensors are measured by adding different workload components onto the loaded platform of the prototype. Finally, some theoretical solutions of the developed parallel sensor are solved and verified by the FE simulation solutions of the developed parallel sensor. The experimental calibration solutions of the prototype are coincident with the theoretical solutions. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				MAY	2020	127								103486	10.1016/j.robot.2020.103486													
J								DeepGoal: Learning to drive with driving intention from human control demonstration	ROBOTICS AND AUTONOMOUS SYSTEMS												Recent research on automotive driving has developed an efficient end-to-end learning mode that directly maps visual input to control commands. However, it models distinct driving variations in a single network, which increases learning complexity and is less adaptive for modular integration. In this paper, we re-investigate human's driving style and propose to learn an intermediate driving intention region to relax the difficulties in end-to-end approach. The intention region follows both road structure in image and direction towards goal in public route planner, which addresses visual variations only and figures out where to go without conventional precise localization. Then the learned visual intention is projected on vehicle local coordinate and fused with reliable obstacle perception to render a navigation score map that is widely used for motion planning. The core of the proposed system is a weakly-supervised cGAN-LSTM model trained to learn driving intention from human demonstration. The adversarial loss learns from limited demonstration data with one local planned route and enables reasoning of multi-modal behaviors with diverse routes while testing. Comprehensive experiments are conducted with real-world datasets. Results indicate the proposed paradigm can produce more consistent motion commands with human demonstration and shows better reliability and robustness to environment change. Our code is available at https://github.com/HuifangZJU/visual- navigation. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				MAY	2020	127								103477	10.1016/j.robot.2020.103477													
J								Model Predictive Control without terminal constraints or costs for holonomic mobile robots	ROBOTICS AND AUTONOMOUS SYSTEMS										Motion control; Holonomic mobile robots; Model Predictive Control; Stability analysis	MPC; STABILITY; PERFORMANCE	We investigate Model Predictive Control (MPC) schemes without stabilizing constraints or costs for the set-point stabilization of holonomic mobile robots. Herein, we ensure closed-loop asymptotic stability using the concept of cost controllability. To this end, we derive a growth bound on the finite-horizon value function in terms of the running costs evaluated at the current state, which is then used to determine a stabilizing prediction horizon. In the discrete-time setting, we additionally show that asymptotic stability holds for the shortest possible prediction horizon. Moreover, we deduce a lower bound on the MPC performance on the infinite horizon. Theoretical results are verified by numerical simulations as well as laboratory experiments of stabilizing a holonomic mobile robot to a reference set point. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				MAY	2020	127								103468	10.1016/j.robot.2020.103468													
J								Robot-Assisted Intervention for children with special needs: A comparative assessment for autism screening	ROBOTICS AND AUTONOMOUS SYSTEMS										Autism Spectrum Disorder; Autism screening; Social Assistive Robotics; Child-Robot Interaction	SPECTRUM DISORDERS; EXPLORE	Despite the increment of researches related to Social Assistive Robotics (SAR), achieving a plausible Robot-Assisted Diagnosis (RAD) for Children with Autism Spectrum Disorders (CwASD) remains a considerable challenge to the clinical and robotics community. The work of specialists regarding ASD diagnosis is hard and labor-intensive due to the condition's manifestations are inherently heterogeneous and makes the process more difficult. Besides, the aforementioned complexity may be the main reason for the slow progress in the development of SAR with diagnostic purposes. Thus, this work provides a comprehensive Robot-Assisted Intervention for CwASD showing the conditions in which a Robot-based approach can be useful to assess autism risk factors for an autism diagnosis purpose. The intervention scheme consists of an improved version of a multimodal environment for Robot-based intervention proposed in our previous work. More specifically, we compared the behavior of CwASD with that of children in a control group during a human/robot-mediated intervention while Joint Attention (JA) behaviors are elicited and analyzed. Through statistical data analysis, it was possible to identify that 17 out of 23 children of the CwASD group showed a different behavior pattern related to three characteristics of autism, which suggests that this pattern can be used to identify autism risk factors through Robot-based interventions. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				MAY	2020	127								103484	10.1016/j.robot.2020.103484													
J								Selective grasp in occluded space by all-around proximity perceptible finger	ROBOTICS AND AUTONOMOUS SYSTEMS										Proximity sensor; Robot hand; Environment map; Grasp; Shape classification	HAND	The goal of this research is to develop a "Selective Grasp'' system with which robots can grasp and identify the target object even in occluded environments. In pursuit of this goal, we first develop a robot hand on which proximity sensors are mounted all around. In addition to the development, we propose a sensor model of the robot hand. By using the sensor model, robots can estimate the distance to the object and calibrate the sensors. With our robot hand, robots can accurately recognize their surroundings without touch. Secondly, we propose an approach in which robots can memorize spatial information of surroundings by building an environment map. The building map motion is generated by a combination of manipulation primitives based on proximity sensors. Thirdly, we propose a grasp planning method and an object shape classification method based on the environment map. By these methods, robots can grasp objects and classify shapes of the objects in occluded spaces. Lastly, we conduct real robot experiments, through which we validate the effectiveness of our proposed Selective Grasp system. (C) 2020 Elsevier B.V. All rights reserved.																	0921-8890	1872-793X				MAY	2020	127								103464	10.1016/j.robot.2020.103464													
J								Impact of decision style on newsvendor ordering behaviors: mean anchoring, demand chasing and overconfidence	SOFT COMPUTING										Inventory decision; Newsvendor; Decision style; Ordering behaviors; Laboratory experiment	INDIVIDUAL-DIFFERENCES; BOUNDED RATIONALITY; INVENTORY; BIAS	Prior experimental studies have shown that individuals' actual ordering decisions significantly deviate from theoretically-proved optimums in newsvendor problems. Several ordering behaviors like mean anchoring, demand chasing, reference dependence, mental accounting and overconfidence have been demonstrated to be the main causes for such deviations. However, less attention has been focused on the impact of decision style on ordering behaviors. To address such challenging issue, we conduct a between-subjects experiment and compare decision results between different groups of individuals, who are characterized by their decision styles, i.e., rational style and experiential style, as well as their tendencies to the three typical behaviors, i.e., mean anchoring, demand chasing and overconfidence. We show that individuals with high rational style or low experiential style can make better inventory order decisions. Furthermore, decision style and profit margin conditions are demonstrated to actually affect individuals' tendencies to mean anchoring, demand chasing and overconfidence, which subsequently affects their order decisions. More importantly, compared to mean anchoring and demand chasing, overconfidence is identified as a dominated factor in affecting the order decisions. This research sheds light on how to select right inventory managers and how to improve their ordering decision performance more efficiently through recognizing the impact of decision style and their behavioral tendencies in different profit margin conditions in the newsvendor problem.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6197	6212		10.1007/s00500-018-03676-8													
J								Mapping the evaluation results between quantitative metrics and meta-synthesis from experts' judgements: evidence from the Supply Chain Management and Logistics journals ranking	SOFT COMPUTING										Journal ranking; Quantitative metrics; Experts' judgement; TOPSIS; Supply Chain Management and Logistics	BIBLIOMETRIC INDICATORS; RESEARCH PRODUCTIVITY; BUSINESS; IMPACT; QUALITY; INDEX; INSTITUTIONS; STATE	Meta-syntheses from experts' judgements and quantitative metrics are two main forms of evaluation. But they both have limitations. This paper constructs a framework for mapping the evaluation results between quantitative metrics and experts' judgements such that they may be solved. In this way, the weights of metrics in quantitative evaluation are objectively obtained, and the validity of the results can be testified. Weighted average percentile (WAP) is employed to aggregate different experts' judgements into standard WAP scores. The Technique for Order Preference by Similarity to an Ideal Solution (TOPSIS) method is used to map quantitative results into experts' judgements, while WAP scores are equal to the final closeness coefficients generated by the TOPSIS method. However, the closeness coefficients of TOPSIS rely on the weights of quantitative metrics. In this way, the mapping procedure is transformed into an optimization problem, and a genetic algorithm is introduced to search for the best weights. An academic journal ranking in the field of Supply Chain Management and Logistics (SCML) is used to test the validity obtained by mapping results. Four prominent ranking lists from Association of Business Schools, Australian Business Deans Council, German Academic Association for Business Research, and Comite National de la Recherche Scientifique were selected to represent different experts' judgements. Twelve indices including IF, Eigenfactor Score (ES), H-index, Scimago Journal Ranking, and Source Normalized Impact per Paper (SNIP) were chosen for quantitative evaluation. The results reveal that the mapping results possess high validity for the relative error of experts' judgements, the quantitative metrics are 43.4%, and the corresponding best weights are determined in the meantime. Thus, some interesting findings are concluded. First, H-index, Impact Per Publication (IPP), and SNIP play dominant roles in the SCML journal's quality evaluation. Second, all the metrics are positively correlated, although the correlation varies among metrics. For example, ES and NE are perfectly, positively correlated with each other, yet they have the lowest correlation with the other metrics. Metrics such as IF, IFWJ, 5-year IF, and IPP are highly correlated. Third, some highly correlated metrics may perform differently in quality evaluation, such as IPP and 5-year IF. Therefore, when mapping the quantitative metrics and experts' judgements, academic fields should be treated distinctively.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6227	6243		10.1007/s00500-019-03837-3													
J								Coordination of port service chain with an integrated contract	SOFT COMPUTING										Price and service competition; Port service chain; Revenue-sharing and service-cost-allocation contract; Coordination of supply chain	REVENUE-SHARING CONTRACTS; SUPPLY CHAIN; NETWORK; QUALITY; PRICE; MANUFACTURERS; MANAGEMENT; RISK	Port, carriers and many other departments are involved in the whole water transportation service system, and these departments tend to have conflicts of interest in the process of service, which results in a difficult coordination phenomenon. We propose an integrated contract that combines the revenue sharing and service cost allocation to coordinate the port service chain. We explore the effects of the contract decision variables in the different scenarios. The results show that two sharing factors exist "blind zone," but the improved contract reveals that revenue sharing and cost allocation contract combining with the fixed payment mechanism can be more effective to coordinate the port service chain.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6245	6258		10.1007/s00500-019-03839-1													
J								Robust multi-product inventory optimization under support vector clustering-based data-driven demand uncertainty set	SOFT COMPUTING										Multi-product inventory; Robust optimization; Data-driven approach; Support vector clustering; Demand uncertainty	NEWSBOY PROBLEM; NEWSVENDOR; MODEL; CONSTRAINTS; SYSTEM; PRICE	A robust multi-product inventory optimization approach is developed with an uncertainty set constructed from the available data using support vector clustering (SVC). The multi-product inventory problem is subject to demand uncertainties in a newsvendor setting with the historical demand data as the only available information. By using SVC, the uncertainty set to which the uncertain demands belong is constructed with a certain confidence in a data-driven approach. The associated robust counterpart model is then developed using the absolute robustness criterion. Through mathematical deduction, the proposed counterpart model is transformed into a tractable linear programming model which can be solved efficiently. The transformed and the original models are proved to be mathematically equivalent. Numerical studies are conducted to illustrate the effectiveness and practicality of the proposed SVC-based data-driven robust optimization approach for dealing with demand uncertainties. The results show that the robust optimization approach under the proposed SVC-based uncertainty set outperforms those under the traditional, i.e., the box and the ellipsoid, uncertainty sets. These results provide evidences that the proposed data-driven robust optimization approach can better hedge against demand uncertainties in multi-product inventory problems.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6259	6275		10.1007/s00500-019-03927-2													
J								An adaptive differential evolution with combined strategy for global numerical optimization	SOFT COMPUTING										Differential evolution; Adaptive parameter; Combined strategy; Evolutionary algorithm; Global optimization	ALGORITHM; SELECTION; MUTATION	Differential evolution (DE) is a simple yet powerful evolutionary algorithm for numerical optimization. However, the performance of DE significantly relies on its mutation operator and control parameters (scaling factor and crossover rate). In this paper, we propose a novel DE variant by introducing a series of combined strategies into DE, called CSDE. Specifically, in CSDE, to obtain a proper balance between global exploration ability and local exploitation ability, we adopt two mutation operators with different characteristics to produce the mutant vector, and provide a mechanism based on their own historical success rate to coordinate the two adopted mutation operators. Moreover, we combine a periodic function based on one modulo operation, an individual-independence macro-control function and an individual-dependence function based on individual's fitness value information to adaptively produce scaling factor and crossover rate. To verify the effectiveness of the proposed CSDE, comparison experiments contained seven other state-of-the-art DE variants are tested on a suite of 30 benchmark functions and four real-world problems. The simulation results demonstrate that CSDE achieves the best overall performance among the eight DE variants.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6277	6296		10.1007/s00500-019-03934-3													
J								A new uncertain regression model and its application	SOFT COMPUTING										Uncertainty theory; Uncertain regression model; Asymmetric triangular uncertain set	LEAST-SQUARES ESTIMATION; FUZZY	Uncertain linear regression (ULR) model based on symmetric triangular uncertain set has been studied early. This paper extends the symmetric triangular uncertain coefficients to asymmetric triangular uncertain coefficients and builds two methods for estimating the parameters of ULR model. Our aim is to minimize the differences of the uncertain membership functions between the observed and estimated values. Firstly, we propose a linear programming method, whose principle is to minimize the sum of the absolute values of the differences between left width and right width of two triangular uncertain sets for each index i. Secondly, we develop a new nonlinear programming method by maximizing the overlaps of acreage of the estimated and real triangular uncertain sets in a particular hi-cut. Then, a criterion is established to evaluate the performance of the proposed approaches. Finally, we use an example based on industrial water demand data of China to illustrate our proposed approaches which are reasonable and compare the explanatory power of the ULR model and traditional linear regression (TLR) model using the presented evaluation criteria, which shows that the performance of the ULR model is obviously better than the TLR model.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6297	6305		10.1007/s00500-019-03938-z													
J								Urban hazmat transportation with multi-factor	SOFT COMPUTING										Urban hazmat transportation; Multiple factors; Multi-level programming; Biogeography-based optimization; Pareto optimization	HAZARDOUS MATERIALS TRANSPORTATION; VEHICLE-ROUTING PROBLEM; SCHEDULING PROBLEM; RISK; NETWORK; DEPOT; OPTIMIZATION; ACCIDENTS; ROUTES; MODEL	In this paper, an urban hazmat transportation problem considering multiple factors that tangle with real-world applications (i.e., weather conditions, traffic conditions, population density, time window, link closure and half link closure) is investigated. Based on multiple depot capacitated vehicle routing problem, we provide a multi-level programming formulation for urban hazmat transportation. To obtain the Pareto optimal solution, an improved biogeography-based optimization (improved BBO) algorithm is designed, comparing with the original BBO and genetic algorithm, with both simulated numerical examples and a real-world case study, demonstrating the effectiveness of the proposed approach.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6307	6328		10.1007/s00500-019-03956-x													
J								A dynamic line generation and vehicle scheduling method for airport bus line based on multi-source big travel data	SOFT COMPUTING										Airport bus line; Demand responsive transit; Candidate stations; Station grouping; Line generation and vehicle scheduling; Public transportation demand level	BOUND ALGORITHM	Airport bus is an important public transportation mode for large international airport. To improve the bus station coverage, passenger demand compatibility and the scheduling flexibility of Beijing International Airport bus line, a dynamic line generation and vehicle scheduling method is proposed in this paper. Firstly, based on multi-source big data from the airport (including data from taxi, ride-hailing service, subway, regular bus, airport bus, etc.), we accurately extract candidate stations, which are very popular with passengers and convenient for parking and transfer, through public transportation demand level calculation, iterative clustering and POI matching. Then, the candidate stations need to be partitioned appropriately by selecting suitable features and calculating the similarity of candidate stations, so as to make the stations within each group a moderate size and have a consistent spatial orientation. Finally, a line generation and vehicle scheduling algorithm, which is compatible with multi-vehicle, high success rate of ride-sharing matching and low cost, is designed to realize accurate and rapid operation scheduling within each group according to the situation of passengers booking tickets. We have carried out experiments in Wangjing and Yayuncun, and the results show that our method can satisfy passenger demand fast and accurately.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6329	6344		10.1007/s00500-019-03987-4													
J								Constructing a multilayer network for stock market	SOFT COMPUTING										Stock network; Multilayer network; Linear relation; Nonlinear relation	PORTFOLIO OPTIMIZATION PROBLEM; GRANGER-CAUSALITY; VOLATILITY; STABILITY; CRISIS; MODEL	In this paper, we discuss the stock network construction problem under simultaneous consideration of linear and nonlinear relations between stocks. A novel method based on the conditional probability is proposed to describe the nonlinear relation between stocks. Furthermore, by considering both the linear and nonlinear relations between stocks, a multilayer network is constructed to characterize stock market, in which Pearson correlation network, Granger causality network, and our proposed nonlinear relation network are combined. Finally, several experiments are conducted to illustrate the effectiveness of the proposed approaches. The results show that the proposed multilayer network not only covers more nodes than the Pearson correlation network, but also better balances the relation between prediction accuracy and the number of predictable nodes.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6345	6361		10.1007/s00500-019-04026-y													
J								Agency models based on different measures with comparison	SOFT COMPUTING										Principal agent problem; Incomplete information; Random agency model; Fuzzy agency model; Uncertain agency model	REVENUE-SHARING CONTRACT; PRODUCT DEVELOPMENT; FUZZY; INFORMATION; RISK; INCENTIVES; IMPACT	Optimal contract regulates the expected activities of both principals and agents, and influences how the gains from the cooperation are shared between the two participators. Thus, it is necessary and wise for principals to seek for the optimal contracts during the period of negotiations. To determine the optimal contracts, there are several kinds of agency models based on different measures, such as probability measure, capacity measure, credibility measure and uncertainty measure. This paper primarily presents a comparative review of random agency model, fuzzy agency model and uncertain agency model. This comparative review is aimed at not only summarizing the structure and feature of each agency model but also guiding on how to identify the most suitable agency model for each specific principal agent problem. Motivated by this idea, these three classes of agency models are respectively investigated about their structure, feature and application, and then an empirical comparison among these models is basically carried out from several aspects, such as information diversity, decision rule and calculation. As a significant contribution, the comparative result in this paper provides the guidance for the principals on how to identify the most suitable agency model for each special principal agent problem in a certain setting.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6363	6373		10.1007/s00500-019-04044-w													
J								The risk path selection problem in uncertain network	SOFT COMPUTING										Uncertain variable; Uncertain network; Uncertain programming; Risk analysis	HAZARDOUS MATERIALS; TRANSPORT RISK; INDEX	This paper characterizes the minimum risk path selection problem in an uncertain network. Assuming the accidental losses are the uncertain variables, we first present three types of uncertain risk indexes. After that, some uncertain risk programming models are built based on the proposed risk indexes. In order to obtain the minimum risk path, we convert these uncertain programming models to their corresponding deterministic forms by the operational law of uncertain variables. At last, a numerical example is given to demonstrate the models.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6375	6383		10.1007/s00500-019-04132-x													
J								An improved diffusion model for supply chain emergency in uncertain environment	SOFT COMPUTING										Emergency; Supply chain; Uncertain information; Bass model	DISRUPTION RISKS; SERVICE LEVEL; TRANSPORTATION; DESIGN	Emergencies will bring the great threat to the stability and coordination of supply chain, such as temporary interruption of raw materials supply, strong fluctuation of demand and distorted information transmission, which will lead to the breakdown of whole supply chain and threaten the survival of enterprises in supply chain. Based on the influence factors of emergency diffusion and supply chain structure in uncertain environment, this paper studies the diffusion effect of emergency and establishes an improved Bass diffusion model. On this basis, information diffusion simulation is carried out. Finally, management suggestions are proposed on supply chain emergency diffusion in uncertain environment.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6385	6394		10.1007/s00500-019-04134-9													
J								Change points detection and parameter estimation for multivariate time series	SOFT COMPUTING										Group Lasso; Change points; VAR model	MODEL SELECTION; LASSO; REGRESSION	In this paper, we propose a method to estimate the number and locations of change points and further estimate parameters of different regions for piecewise stationary vector autoregressive models. The procedure decomposes the problem of change points detection and parameter estimation along the component series. By reformulating the change point detection problem as a variable selection one, we apply group Lasso method to estimate the change points initially. Then, from the preliminary estimate of change points, a subset is selected based on the loss functions of Lasso method and a backward elimination algorithm. Finally, we propose a Lasso + OLS method to estimate the parameters in each segmentation for high-dimensional VAR models. The consistent properties of the estimation for the number and the locations of the change points and the VAR parameters are proved. Simulation experiments and real data examples illustrate the performance of the method.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6395	6407		10.1007/s00500-019-04135-8													
J								Port collaborative development based on rough set theory	SOFT COMPUTING										Rough set; Coupling degree; Port group; Coordinated development	COOPERATION; COMPETITION	As a strategic resource for the region to participate in global economic cooperation and competition, the coordinated development of ports within the port group is of great practical significance to promote its own and regional economic development. In order to better promote the port's rational development and resource allocation, this paper uses the rough set reduction method based on information entropy to determine the attribute weight and importance of the system order parameter index and uses the port group system coupling measure model to obtain the port coupling degree. Then, different coupling states are distinguished according to the coupling degree of the port, and the level of the coordinated development of the port is described. Finally, through the empirical analysis of the three major port groups in the Bohai Rim region, the level of coordinated development of ports within the three major port groups is analyzed.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6409	6419		10.1007/s00500-019-04201-1													
J								Supply chain partnership, inter-organizational knowledge trading and enterprise innovation performance: the theoretical and empirical research in project-based supply chain	SOFT COMPUTING										Supply chain partnership; Inter-organizational knowledge trading; Enterprise innovation performance; Project-based supply chain; China	INTERNATIONAL JOINT VENTURES; RELATIONSHIP COMMITMENT; COOPERATIVE STRATEGY; INTERPERSONAL-TRUST; MANAGEMENT; ORGANIZATIONS; EXPLOITATION; ACQUISITION; MECHANISMS; FRAMEWORK	Based on relational exchange theory and transaction cost theory, a conceptual model for the relationships among supply chain partnership, inter-organizational knowledge trading and enterprise innovation performance is proposed and empirically tested using the data collected from 256 Chinese manufacturing enterprises in project-based supply chain with the structural equation model. The dimension of supply chain partnership in this model is described from shared goal, trust and relationship commitment. Inter-organizational knowledge trading is categorized into explicit knowledge trading and tacit knowledge trading. The results showed that: (1) there are significant and positive effects of shared goal and trust on explicit knowledge trading, tacit knowledge trading and enterprise innovation performance, while trust has a stronger positive effect on tacit knowledge trading than explicit knowledge trading; (2) although relationship commitment has significant and positive effects on tacit knowledge trading and enterprise innovation performance, it does not affect explicit knowledge trading significantly; (3) it is also proved that inter-organizational knowledge trading (explicit knowledge trading and tacit knowledge trading) has significant and positive effects on enterprise innovation performance; (4) the mediating effects of inter-organizational knowledge trading (explicit knowledge trading and tacit knowledge trading) are proved on the relationships between supply chain partnership (shared goal, trust and relationship commitment) and enterprise innovation performance, excluding the mediating effect of explicit knowledge trading between relationship commitment and enterprise innovation performance. The findings provide a theoretical basis for inter-organizational knowledge trading participants selecting an appropriate relational mechanism to promote knowledge trading, and these also guide the inter-organizational knowledge trading among members of project-based supply chain in practice.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6433	6444		10.1007/s00500-019-04548-5													
J								Strategy analysis of governments and new energy product manufacturers and consumers based on evolutionary game model	SOFT COMPUTING										New energy product; Regulation policy; Decision behavior; Evolutionary game	SUPPLY CHAIN ANALYSIS; DUAL-CHANNEL; COORDINATION; POLICY; FIRMS; CHINA	New energy products (NEPs) are important driving forces for promoting economic transformation and green development. The current trend of NEPs development is propelled by consumers, spurred by governmental regulations, and implemented by manufacturers. However, the previous studies rarely discussed the behavioral strategies of consumers. The relationships among consumers, manufacturers, and governments need to be researched in depth. In this paper, we focus on analyzing the impact factors of these three groups strategic selection of NEPs. In bounded rationality, by jointly considering the interactions among the customers' purchases, the manufacturers' product strategies, and the regulation policies, we develop three scenarios based on the evolutionary game model. The equilibrium results show that the production cost of NEPs and the profit from NEPs, government subsidies, and the taxation imposed on the manufacturers are the key influencing factors of the manufacturers' strategy. The critical factors affecting the consumers' purchase behavior are the benefits obtained from NEPs, government subsidies to consumers, and taxation on traditional energy products (TEPs). The influencing factors of governmental regulations include consumer satisfaction with the government, the environmental loss caused by TEPs, the cost of regulations, the coefficient of taxation, and subsidies. According to the research results, the countermeasures put forward will be more helpful for government to regulate the development of NEPs.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6445	6455		10.1007/s00500-019-04571-6													
J								The dilemma phenomenon, logistics for monetary independence policy and foreign exchange reserves	SOFT COMPUTING										Dilemma phenomenon; Foreign exchange reserves; Monetary policy independence		The development of financial integration makes a country's economic links difficult. This is especially so for emerging market countries, which are more vulnerable to economic shocks. The objective of this paper is to explore whether the emerging market countries which adopt floating exchange rate system have realized, at the same time, in the aftermath of a crisis, free movement of capital flow and independence of monetary policy. This is done by introducing foreign exchange reserves into Mundell-Fleming model to do derivation. The approach is supported by empirical research based on 20 emerging countries including China, Brazil, Poland and South Africa and others. It was found that: (1) after the crisis, the emerging market countries that implemented floating exchange rate system did not achieve monetary policy independence. Rapid accumulation of foreign exchange reserves weakened the positive effect of the increase in money supply on output; (2) As a result of holding foreign exchange reserves, the independence of monetary policy in emerging market countries has been challenged, independently of the type of exchange rate system adopted, which proves the existence of dilemma phenomenon. Finally, this study puts forward policy recommendations on the exchange rate system and capital account convertibility for China.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6457	6466		10.1007/s00500-019-04587-y													
J								Optimizing stop plan and tickets allocation for high-speed railway based on uncertainty theory	SOFT COMPUTING										High-speed railway transportation; Stop plan; Tickets allocation; Uncertainty theory; Passenger satisfaction degree; Average seated occupancy rate	OPTIMIZATION; NETWORK; MODEL	Aiming to provide a generic modeling framework for finding stop plan and tickets allocation of high-speed railway, we first propose a stop plan and tickets allocation collaborative optimization model in this paper, which is established to maximize the passenger satisfaction degree and the average seated occupancy rate. Due to the randomness and uncertainty of passenger demand, uncertain variables are set and the primal model is an uncertain model. And then, the model is transformed into equivalent deterministic model based on uncertainty theory. Because of the computational complexity of the model, especially for the large-scale real-world instances, we develop a Lagrangian relaxation (LR-based) heuristic algorithm that decomposes the primal problem into two sub-problems and thus is able to find good solutions in short time. Finally, a numerical experiment based on the operation data of high-speed railway from Beijing south Station to Shanghai Hongqiao Station is implemented to verify the effectiveness and feasibility of the proposed approaches.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6467	6482		10.1007/s00500-019-04617-9													
J								A Bayesian network model for the reliability control of fresh food e-commerce logistics systems	SOFT COMPUTING										Fresh food; Reliability; Bayesian network; e-commerce logistics	OPTIMIZATION	This paper focuses on the reliability of intelligent logistics for fresh food e-commerce. Based on the development of fresh food e-commerce, this paper analyses the factors that influence the reliability of fresh food logistics from the aspects of information technology, facilities and equipment, personnel operation and external environment. A Bayesian network is used to analyse the influence of each factor on system reliability, and the degree of importance of each factor is calculated. Based on the importance of each influential factor in fresh food e-commerce logistics systems, an intelligent logistics model for reliability control of fresh food is established. The purpose of this model is to improve the economic efficiency and the intelligent level of the fresh food e-commerce logistics system on the premise of meeting the system reliability requirements. Finally, simulation results show that the developed intelligent logistics reliability control model can significantly improve the reliability of fresh food e-commerce logistics systems, and provide practical suggestions for fresh food e-commerce enterprises.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6499	6519		10.1007/s00500-020-04666-5													
J								Conditional probability on full Lukasiewicz tribes	SOFT COMPUTING										Random experiment; Stochastic channel; Joint experiment; Fuzzified probability theory; Observable; Probability integral; Statistical map; g-joint experiment; Independence; Conditional probability; Lukasiewicz tribe		We study notions of conditional probability and stochastic dependence/independence in an upgraded probability model in which the space of events is modeled by a full Lukasiewicz tribe of all measurable functions from some measurable space into [0, 1]. Our study is based on properties of joint experiments and the notion of stochastic channel, a construct equivalent to the notion of Markov kernel between two measurable spaces. Using the notion of a degenerated stochastic channel, a channel transmitting no stochastic information between two spaces, we define an asymmetrical independence of random experiments. Finally, we define the notion of conditional probability on full Lukasiewicz tribes.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6521	6529		10.1007/s00500-020-04762-6													
J								The fractional non-equidistant grey opposite-direction model with time-varying characteristics	SOFT COMPUTING										<mml; math><mml; mrow><mml; msup><mml; mrow><mml; mtext>GOM</mml; mtext></mml; mrow><mml; mtext>r</mml; mtext></mml; msup><mml; mrow><mml; mo stretchy="false">(</mml; mo><mml; mn>1</mml; mn><mml; mo>; </mml; mo><mml; mn>1</mml; mn><mml; mo stretchy="false">)</mml; mo></mml; mrow></mml; mrow></mml; math>; documentclass[12pt]{minimal}; usepackage{amsmath}; usepackage{wasysym}; usepackage{amsfonts}; usepackage{amssymb}; usepackage{amsbsy}; usepackage{mathrsfs}; usepackage{upgreek}; setlength{; oddsidemargin}{-69pt}; begin{document}$$ {; text{GOM}}<^>{; text{r}} (1; 1) $$; end{document}<inline-graphic xlink; href="500_2020_4799_Article_IEq4; gif"; >; <mml; math><mml; mrow><mml; msup><mml; mrow><mml; mtext>NTVGOM</mml; mtext></mml; mrow><mml; mtext>r</mml; mtext></mml; msup><mml; mrow><mml; mo stretchy="false">(</mml; mo><mml; mn>1</mml; mn><mml; mo>; </mml; mo><mml; mn>1</mml; mn><mml; mo stretchy="false">)</mml; mo></mml; mrow></mml; mrow></mml; math>; documentclass[12pt]{minimal}; usepackage{amsmath}; usepackage{wasysym}; usepackage{amsfonts}; usepackage{amssymb}; usepackage{amsbsy}; usepackage{mathrsfs}; usepackage{upgreek}; setlength{; oddsidemargin}{-69pt}; begin{document}$$ {; text{NTVGOM}}<^>{; text{r}} (1; 1) $$; end{document}<inline-graphic xlink; href="500_2020_4799_Article_IEq5; gif"; >; Time-varying characteristics; Grey control parameter; Fractional order	NATURAL-GAS; ELECTRICITY CONSUMPTION; FORECASTING-MODEL; PREDICTION MODEL	The grey opposite-direction model with fractional-order accumulation (GOMr (1; 1) has been appealed and interested in non-equidistant cases. However, there exists the drawback that it does not consider the effect of time-varying factor. In other words, the fixed grey control parameter defined as a certain constant limits the prediction performance of the model. By fully studying modelling procedure of the model, the optimized non-equidistant GOMr o1; 1THORN model with time-varying characteristics is proposed in this paper, which is abbreviated as NTVGOM(T) (1; 1)THORN model. In the new model, a polynomial with time-varying characteristics is applied on grey control parameter, and the optimal fractional order could be automatically determined by minimizing the mean absolute percentage error. Then, the two empirical examples are employed to verify the effectiveness of the proposed model, and the numerical results show the proposed model has a better prediction performance.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6603	6612		10.1007/s00500-020-04799-7													
J								Optimal shape synthesis of a metallic flywheel using non-dominated sorting Jaya algorithm	SOFT COMPUTING										Flywheel; Kinetic energy; Non-dominated Jaya algorithm; von Mises stresses; Cubic B-spline	OPTIMAL-DESIGN; OPTIMIZATION	This study describes the shape synthesis of a metallic flywheel using a non-dominated sorting Jaya algorithm. Generally, the flywheel is used to store the kinetic energy in the machines. Kinetic energy is an essential parameter to measure flywheel performance and can be improved by the optimal shape of the flywheel. In order to the optimal shape of the flywheel, the multi-objective problem with the maximization of the kinetic energy and minimization of von Mises stresses is formulated under appropriate design constraints using the cubic B-spline curve. A flowchart is proposed to solve the two-point boundary value differential equation for the calculation of von Mises stress at each point between the inner and outer radii of the flywheel. The design variables are represented by the control points of the cubic B-spline curve. A posteriori approach-based algorithm as non-dominated sorting Jaya algorithm (NSJaya) is used to solve the formulated optimization problem. This algorithm is based on the concepts of crowding distance and non-dominated sorting approach and gives the optimal Pareto set. The proposed method is applied to the flywheel of the agricultural thresher. The performance of the proposed algorithm is compared with that of non-dominated sorting genetic algorithm (NSGA-II) using hyper-volume performance metric. It is found that the NSJaya algorithm gives better results compared to NSGA-II and a posteriori approach-based algorithms such as genetic algorithm (GA), particle swarm optimization (PSO), and Jaya. The optimal Pareto set for the optimal shape of the flywheel is calculated and outlined in this paper. The designer can choose any solution from the Pareto set for the optimal shape of the flywheel. ANSYS parameter design language (APDL) software is used for the validation of the von Mises stresses in the optimized shapes of the flywheel.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6623	6634		10.1007/s00500-019-04302-x													
J								Pre-production box-office success quotient forecasting	SOFT COMPUTING										Machine learning; Classification; Hollywood; Facebook; Forecasting	MOVIE REVENUES; SYSTEM; MODEL	Hollywood enjoys the position of being the biggest movie producers when it comes to global recognition among movie-making industries. Despite being the biggest movie producer, it has been facing high revenue losses lately since most of the films that it has created have failed to capture viewer's attention in the first few weeks of its release resulting in a box-office flop. It has been observed in a recent study that Hollywood is estimated to witness a loss of around 1 billion to almost 10 billion US dollars till 2020. Revenue risks have created immense pressure on movie producing stakeholders. They feel a constant pressure to come up with a formula to make a successful movie, however, to date; there are no fixed ingredients that can ensure the success of a movie. Researchers and movie producers constantly feel a need to have some expert systems which would predict the fate of the movie prior to its production with reasonable accuracy. Regardless of the difficult nature of the issue area, few researchers have created expert systems to forecast the financial success of movies using different approaches, but most of them are targeted pre-release forecasting or have low prediction accuracy. Such predictions are of a seminal nature as of their limited prediction scope, and non-ability to reduce revenue loss risk. Therefore, there is a constant demand from investors to have pre-production forecasting tools with high accuracy which can help them plan and make necessary alterations to save huge investments. In this study, we proposed eighteen new features to forecast box-office success, as soon as the quotient (director and cast) signs an agreement. This proposed forecasting time is the earliest prediction that has ever been reported in the movie forecasting literature. The decision support system ranks director and lead cast by utilizing their performances of the last 100 years (1915-2015). The processed output file is a table that ranks each director and cast into four categories based on cast experience, journalist critics, media reporting, user ratings, and revenue generated by associating movie. To produce more accurate results, learner-based feature selection is also performed to select the best subsets of features. This system is intended to be a dynamic tool, integrating further data for real-time adaptation. The system has the ability to incorporate different feature selection algorithm for the progressive improvement of movie success forecasting We demonstrate the effectiveness of extracting features and explain how they improve forecasting accuracy over existing models. The adaptive behaviour of the presented system is achieved by incorporating conceptually different machine learning classifiers, i.e. support vector machine, gradient boosting, extreme boosting classifier, and random forest. A voting system is used to make the prediction by averaging the output class-probabilities. To assess the adequacy of new features, a cross-validation test is directed. Our classification results are evaluated by using two performance measures, i.e. average per cent success rate, or within one class away from its actual prediction. The new features have achieved the most noteworthy accuracy of 85% with an expansion of a 46.43% (average per cent success rate) and 5.56% (within a class away) in comparison with other state-of-the-art feature sets.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6635	6653		10.1007/s00500-019-04303-w													
J								A hybrid model of dynamic time wrapping and hidden Markov model for forecasting and trading in crude oil market	SOFT COMPUTING										Crude oil price; Dynamic time wrapping; Direction forecasting; Simulation trading; Trading strategy	NEURAL-NETWORK MODEL; GA; PSO; EVOLUTIONARY; METHODOLOGY; PERFORMANCE; PREDICTION; TUTORIAL; ANN	In this study, a hybrid model of hidden Markov model (HMM) and dynamic time wrapping (DTW) is proposed to predict the return of crude oil price movements and trading. First, three indicators are used as inputs of HMM to determine the market state for each month; next, DTW algorithm is applied to match similar price sequences which have the same market state in historical time series, and then to calculate expected returns; Finally, it forecasts the crude oil spot price direction and executes related simulation trading. For design of the trading strategy, we adopt different parameters such as trading thresholds and position-closing thresholds for each market state, and the particle swarm optimization algorithm is applied for parameter optimization of our trading strategy. In experiments, the proposed method is applied for direction forecasting and simulation trading of WTI and Brent crude oil market. Experimental results show that the proposed method yielded the best forecasting and trading performances in average. For instance, in the WTI market, the proposed method produced a hit ratio of about 62.74% and a yield of 34.3% profit per year, and a Sharpe ratio value of 2.274. Furthermore, experimental results of the proposed method were significantly superior to other benchmark methods, demonstrating that the proposed method is not only good at direction prediction and profit making, but also return/risk ratio.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6655	6672		10.1007/s00500-019-04304-9													
J								Failure mode and effects analysis: an integrated approach based on rough set theory and prospect theory	SOFT COMPUTING										Failure mode and effects analysis; Bounded rationality; Variable precision rough number; Prospect theory; TOPSIS	EXTENDED MULTIMOORA; TOPSIS APPROACH; FUZZY; RISK; SYSTEM; FMECA; METHODOLOGY; DECISION; TODIM; QFD	Failure mode and effect analysis (FMEA), a bottom-up method, is one of risk assessment tools to eliminate or reduce failures in design and process. It has been applied to many industries due to its flexibility and effectiveness. However, the conventional FMEA considers less about the subjectivity and vagueness in the process of risk assessment and assumes that three risk factors' importance is the same. Although a lot of approaches based on fuzzy logic are proposed to deal with vague information in previous literature, they need priori assumptions leading to fixed intervals to express vagueness. In addition, most of the previous methods suppose that decision makers are totally rational without considering their psychological factors. To solve the problems, an extended technique for order performance by similarity to ideal solution (TOPSIS) is developed to improve FMEA approach, which combines the advantage of variable precision rough number in dealing with vague information and the strength of prospect theory (PT) in considering decision maker's bounded rationality. The proposed method consists of two stages: one is the determination of risk factors' weight function values; and the other is ranking risk priority of failure modes with the PT-based TOPSIS. Finally, a case study of a steam valve system is used to demonstrate the effectiveness and efficiency of the proposed method.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6673	6685		10.1007/s00500-019-04305-8													
J								Human-robot collisions detection for safe human-robot interaction using one multi-input-output neural network	SOFT COMPUTING										Coupled system; Robot collision detection; Collided link identification; Safe human-robot interaction; Coupled; uncoupled neural network; Levenberg-Marquardt	CONTACT; APPROXIMATION; ARM	In this paper, a multilayer feedforward neural network-based approach is proposed for human-robot collision detection taking safety standards into consideration. One multi-output neural network is designed and trained using data from the coupled dynamics of the manipulator with and without external contacts to detect unwanted collisions and to identify the collided link using only the intrinsic joint position and torque sensors of the manipulator. The proposed method is applied to the collaborative robots, which will be very popular in the near future, and is implemented and evaluated in 3D space motion taking into account the effect of the gravity. KUKA LWR manipulator is an example of the collaborative robots, and it is used for doing the experiments. The experimental results prove that the developed system is considerably efficient and very fast in detecting the collisions in the safe region and identifying the collided link along the entire workspace of the three-joint motion of the manipulator. Separate/uncoupled neural networks, one for each joint, are also designed and trained using the same data, and their performance is compared with the coupled one.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6687	6719		10.1007/s00500-019-04306-7													
J								A robust system for road sign detection and classification using LeNet architecture based on convolutional neural network	SOFT COMPUTING										Road signs; TSDR; Detection; Classification; Histogram of oriented gradients; Support-vector machine; Convolutional neural network; LeNet		In this paper, we are reporting a system for detection and classification of road signs. This system consists of two parts. The first part detects the road signs in real time. The second part classifies the German traffic signs (GTSRB) dataset and makes the prediction using the road signs detected in the first part to test the effectiveness. We used HOG and SVM in the detection part to detect the road signs captured by the camera. Then we used a convolutional neural network based on the LeNet model in which some modifications were added in the classification part. Our system obtains an accuracy rate of 96.85% in the detection part and 96.23% in the classification part.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6721	6733		10.1007/s00500-019-04307-6													
J								Real-time hook selection: a soft computing enabled future location protection mechanism in WSN using LTR measures and random seeding approach	SOFT COMPUTING										WSN; Location protection; Hook selection; LTR measures; QoS	PRIVACY	The problem of location protection has been well studied in recent research articles, and a number of approaches are prescribed for the development of Quality of Service in Wireless Sensor Network (WSN). However, the previous approaches failed to achieve higher performance toward location protection which led to deficiency in the quality of service of WSN. To overcome the deficiency in location protection, an efficient real-time hook selection algorithm based on Latency Throughput Retransmission measure has been presented. First, this method monitors the quality of service parameters like latency, throughput and retransmission. Based on the information collected, the method identifies the list of routes being used to perform data transmission. For each route, the method estimates the latency support, throughput support and retransmission frequency support. Based on the measures estimated, the method computes a transmission support measure to perform route selection. Similarly, the method performs hook selection based on the location of suspected adversary. The newly selected hook will be just around the eccentric of old one. The proposed algorithm improves the performance of location protection as well as the entire network.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6735	6740		10.1007/s00500-019-04309-4													
J								The new optimization algorithm for the vehicle routing problem with time windows using multi-objective discrete learnable evolution model	SOFT COMPUTING										Vehicle routing problem with time windows (VRPTW); Learnable evolution model (LEM); Multi-objective combinatorial optimization (MOCO); Strength Pareto evolutionary algorithm (SPEA)	SHORTEST-PATH PROBLEM; PARTICLE SWARM OPTIMIZATION; NEIGHBORHOOD TABU SEARCH; HETEROGENEOUS FLEET; LOCAL SEARCH; RESOURCE CONSTRAINTS; GENETIC ALGORITHMS; SCHEDULING PROBLEM; MEMETIC ALGORITHM; BRANCH	This paper presents a new multi-objective discreet learnable evolution model (MODLEM) to address the vehicle routing problem with time windows (VRPTW). Learnable evolution model (LEM) includes a machine learning algorithm, like the decision trees, that can discover the correct directions of the evolution leading to significant improvements in the fitness of the individuals. We incorporate a robust strength Pareto evolutionary algorithm in the LEM presented here to govern the multi-objective property of this approach. A new priority-based encoding scheme for chromosome representation in the LEM as well as corresponding routing scheme is introduced. To improve the quality and the diversity of the initial population, we propose a novel heuristic manner which leads to a good approximation of the Pareto fronts within a reasonable computational time. Moreover, a new heuristic operator is employed in the instantiating process to confront incomplete chromosome formation. Our proposed MODLEM is tested on the problem instances of Solomon's VRPTW benchmark. The performance of this proposed MODLEM for the VRPTW is assessed against the state-of-the-art approaches in terms of both the quality of solutions and the computational time. Experimental results and comparisons indicate the effectiveness and efficiency of our proposed intelligent routing approach.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6741	6769		10.1007/s00500-019-04312-9													
J								Performance of genetic algorithms with different selection operators for solving short-term optimized reservoir scheduling problem	SOFT COMPUTING										Gezhouba reservoir; Short-term reservoir scheduling; Genetic algorithm; Optimization; Maximum power generation	KRILL HERD ALGORITHM; HYDROPOWER GENERATION; WATER-RESOURCES; RULE CURVES; SYSTEM	The tournament operator genetic algorithm (TGA) often shows poor convergence and easily gets trapped in a local optimum when solving optimized reservoir scheduling problems. The selection operation is the most important operation determining an algorithm's convergence; therefore, this study proposes a proportional reproduction selection-based operator genetic algorithm (RGA) and a steady-state reproduction selection-based operator genetic algorithm (SGA) as alternatives to TGA. This study used TGA, RGA, and SGA to solve the maximum power generation model for the Gezhouba hydropower station, the largest runoff hydropower station in the world. Then, by using the maximum hydropower station output under a given typical runoff scenario as the optimization criterion, this study evaluated the optimized solution performance of the GA using different selection operators. The results show that TGA, SGA, and RGA can be applied to solve the short-term reservoir scheduling model. As the number of iterations increases, the hydropower station output optimized by these three GAs increases. Based on the maximum power generated by the Gezhouba hydropower station, SGA and RGA provide better results than TGA, and between SGA and RGA, the former provides better results.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6771	6785		10.1007/s00500-019-04313-8													
J								Reducing overlapped pixels: a multi-objective color thresholding approach	SOFT COMPUTING										Multi-level thresholding; Evolutionary algorithms; Multi-objective optimization; Overlapping Index	SATELLITE IMAGE SEGMENTATION; PARTICLE SWARM OPTIMIZATION; EVOLUTIONARY ALGORITHMS; MULTILEVEL; ENTROPY; KAPURS	This paper proposes a general multi-objective thresholding segmentation methodology for color images and a quality metric designed to prevent and quantify the overlapping effect of segmented images. Multi-level thresholding (MTH) has been used to segment color images in recent years; this process considers each channel as a single grayscale image and applies the MTH independently. Although this method provides competitive results, the inherent relationship among color channels is disregarded. Such approaches generate spurious classes on overlapping regions, where new colors are generated, especially on the borders of the objects. The proposed multi-objective color thresholding (MOCTH) approach performs image segmentation while preserving the relationship between image channels. MOCTH is aimed to reduce the overlapping effect on segmented color images without performing additional post-processing. To measure the overlapping classes on a thresholded color image, the overlapping index is proposed to quantify the pixels affected. The presented approach is analyzed on two color spaces (RGB and CIE L*a*b*) using three multi-objective algorithms; they are NSGA-III, SPEA-2, and MOPSO. Results provide evidence pointing out to a better segmentation from MOCTH over the traditional single-objective approaches while reducing overlapped areas on the image.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6787	6807		10.1007/s00500-019-04315-6													
J								Optimum design of shallow foundation using evolutionary algorithms	SOFT COMPUTING										Metaheuristic algorithms; Global optimization; Construction industry; Shallow footing; Evolutionary algorithms	BIOGEOGRAPHY-BASED OPTIMIZATION; PARTICLE SWARM OPTIMIZATION; DIFFERENTIAL EVOLUTION; COST OPTIMIZATION; RETAINING WALLS; SEARCH ALGORITHM; HARMONY SEARCH; FOOTINGS; BEHAVIOR; SURFACE	In the current study, the performance of three evolutionary algorithms, differential algorithm (DE), evolution strategy (ES), and biogeography-based optimization algorithm (BBO), is examined for foundation design optimization. Moreover, four recent variations of evolutionary-based algorithms [i.e., improved differential evolution algorithm based on an adaptive mutation scheme, weighted differential evolution algorithm (WDE), linear population size reduction success-history-based adaptive differential evolution algorithm, and biogeography-based optimization with covariance matrix-based migration] have been tackled for handling the current problem. The objective function is based on the cost of shallow foundation designs that satisfy ACI 318-05 requirements is formulated as the objective function. This study addresses shallow footing optimization with two attitudes, routine optimization, and sensitivity analysis. As a further study, the effect of the location of the column at the top of the foundation is examined by adding two additional design variables. Three numerical case studies are used for both routine and sensitivity analysis. Moreover, the most common evolutionary-based technique, genetic algorithm (GA), is considered as a benchmark to evaluate the proposed methods' efficiency. Based on the results, there is no algorithm which works as the most efficient solver over all the cases; while, BBO and WDE showed an acceptable performance because of satisfying records in most cases. There were several cases in which GA, DE, and ES were incapable of finding a valid solution which meets all the constraints simultaneously.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6809	6833		10.1007/s00500-019-04316-5													
J								Time series prediction based on intuitionistic fuzzy cognitive map	SOFT COMPUTING										Intuitionistic fuzzy cognitive map; Information granules; Particle swarm optimization; Intuitionistic fuzzy sets	CROWD EVACUATION; MODEL; MECHANISMS; SYSTEMS; SERVICE	Time series exist widely in either nature or society such that the research on analysis of time series has great significance. However, considering the nonlinearity and uncertainty, the prediction of time series is still an open problem. In this paper, by means of the intuitionistic fuzzy set theory, we proposed a novel time series prediction scheme based on intuitionistic fuzzy cognitive map. In the previous research, intuitionistic fuzzy cognitive map, as a kind of knowledge-based modeling tool, is mainly used in decision-making field, where concept structure and weight matrix are usually obtained from experience of experts. To tackle with the diversity of time series, the proposed algorithm constructs the conceptual structure of cognitive map and weight matrix directly from raw sequential data, which effectively enlarges the application range by reducing human participation. Moreover, in order to appropriately calculate the hesitation degree, which is the key role for the application of intuitionistic fuzzy sets, we propose a real-time adjustable hesitation degree calculation scheme. By using this proposed method, hesitation degree can be adaptively adjusted by combining Femi formula with dynamic membership degree. A number of experiments are implemented to reveal feasibility and effectiveness of the proposed schemes.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6835	6850		10.1007/s00500-019-04321-8													
J								A dividing-based many-objective evolutionary algorithm for large-scale feature selection	SOFT COMPUTING										Feature selection; MaOEAs; Decision-making	NONDOMINATED SORTING APPROACH; KRILL HERD ALGORITHM; OPTIMIZATION ALGORITHM; GENETIC ALGORITHM; CLASSIFICATION; SUPPORT; INFORMATION; MACHINE; MOEA/D	Feature selection is a critical preprocess for constructing model in computer vision and machine learning, yet it is difficult to simultaneously satisfy both reducing features' number and maintaining classification accuracy. Toward this problem, we propose dividing-based many-objective evolutionary algorithm for large-scale feature selection (DMEA-FS). Firstly, four novel objectives are established for exploring the optimal feature's subsets. Meanwhile, we design two structures of wrapper for high accuracy and filter for low computation cost in DMEA-FS. Secondly, two new recombination methods are presented for rapid convergence. Mapping-based variable dividing is presented for precise related variables. Thirdly, based on minimum Manhattan distance, a triangle-approximating decision-making is proposed for assisting users' determination with/without preference information. Numerical experiments against several state-of-the-art feature selection algorithms demonstrate that the proposed DMEA-FS outperforms its competitors in terms of both classification accuracy and metrics of features' number.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6851	6870		10.1007/s00500-019-04324-5													
J								A new formulation for prediction of the shear capacity of FRP in strengthened reinforced concrete beams	SOFT COMPUTING										FRP; Multigene genetic programming; Shear capacity; Soft computing method	KRILL HERD ALGORITHM; RC BEAMS; T-BEAMS; SEISMIC PERFORMANCE; TRANSVERSE STEEL; SHORT COLUMNS; OPTIMIZATION; BEHAVIOR; RESISTANCE; MODELS	The use of fiber-reinforced polymer (FRP) to strength the concrete beams is an efficient method in retrofitting of preexisting structures. The application of FRP sheets makes to have higher shear strength, but the common equations in determining the shear strength are no longer effective. In this paper, a new formulation is presented to predict the shear contribution of FRP in strengthened reinforced concrete beams. The formula is produced using the multigene genetic programming (MGP) machine. For this purpose, a set of experimental data is collected from the literature. The shear capacity of FRP in reinforced concrete (RC) beams is considered as the output data, while other variables are considered as the input data. MGP is trained with the experimental data and a formula is produced. The results of the proposed formula are compared with the experimental data to show the ability of the proposed formula. Also, these results are compared with those obtained from the available formulas, approximation models and published researches. Results show that the proposed formula is able to predict the shear capacity of FRP in strengthened RC beams with a higher precision than the other evaluated methods such as CIDAR, Fib.TG9.3, ACI and CSA. The mean absolute percentage error for the MGP formula was reduced about 74% in comparison with the CIDAR equations. Also, the root-mean-squared-error of the MGP formula was decreased near 71% in comparison with the Fib.TG9.3 equations.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6871	6887		10.1007/s00500-019-04325-4													
J								Dealing with small sample size problems in process industry using virtual sample generation: a Kriging-based approach	SOFT COMPUTING										Small sample size problems; Virtual sample generation; Kriging interpolation; Soft sensing modeling; High-density polyethylene	ENERGY PREDICTION; TREND-DIFFUSION; INTERPOLATION	The operational data of advanced process systems have met with explosive growth, but its fluctuations are so slight that the number of the extracted representative samples is quite limited, making it difficult to reflect the nature of the process and to establish prediction models. In this study, inspired by the process of fisherman repairing nets, a Kriging-based virtual sample generation (VSG) named Kriging-VSG is proposed to generate feasible virtual samples in data sparse regions. Then, the accuracy of prediction models is further enhanced by applying the generated virtual samples. In order to reasonably find data sparse regions, a distance-based criterion is imposed on each dimension to identify important samples with large information gaps. Similar to the process of fisherman repairing nets, a certain dimension is initially fixed at different quantiles. A dimension-wise interpolation process using Kriging is then performed on the center between important samples with large information gaps. To validate the performance of the proposed Kriging-VSG, two numerical simulations and a real-world application from a cascade reaction process for high-density polyethylene are carried out. The results indicate that the proposed Kriging-VSG outperforms other methods.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6889	6902		10.1007/s00500-019-04326-3													
J								Two-degree-of-freedom Ellsberg urn problem	SOFT COMPUTING										Urn problem; Ellsberg urn; Uncertainty theory; Chance theory	UNCERTAIN; RISK	Traditional model assumes there is only randomness existing in the urn problem. However, if the numbers of the colored balls are unknown, then they should be regarded as uncertain variable. Since a ball is drawn randomly, Ellsberg urn problem is essentially a complicated system with randomness and uncertainty. Instead of psychological experiment, this paper applies uncertainty theory and chance theory to provide a rigorous mathematical method for formulating the general case of a one-degree-of-freedom Ellsberg urn problem. Furthermore, a two-degree-of-freedom Ellsberg urn problem is proposed, and the formulation for the problem is given to deal with three unknown numbers of colored balls.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6903	6908		10.1007/s00500-019-04327-2													
J								A quantum-behaved particle swarm optimization algorithm with the flexible single-/multi-population strategy and multi-stage perturbation strategy based on the characteristics of objective function	SOFT COMPUTING										Quantum-behaved particle swarm; Characteristics of function; Single-; multi-population; Multi-stage perturbation	DIFFERENTIAL EVOLUTION; GLOBAL OPTIMIZATION; ECONOMIC-DISPATCH; GENETIC ALGORITHM; MEMETIC ALGORITHM; CONVERGENCE; SEARCH	The characteristics of objective functions have important impacts on the search process of the optimization algorithm. Many multimodal functions tend to make the algorithm fall into local optima, and the local search accuracy is usually affected by the coupling of the objective functions in different dimensions. A novel quantum-behaved particle swarm optimization algorithm with the flexible single-/multi-population strategy and the multi-stage perturbation strategy (QPSO_FM) is proposed in the present paper. This algorithm aims to adjust the optimization strategies based on the characteristics of the objective functions. The number of sub-populations is determined by the monotonicity variations of the objective functions, and two mechanisms are introduced to balance the diversity and the convergent speed for the multi-population case. The strategy of multi-stage perturbation is applied to enhance the search ability. At the first stage, the main target of the perturbation is to broaden the search range. The second stage applies the univariate perturbation (relying on the coupling degree of the objective function) to raise the local search accuracy. Performance comparisons between the proposed and existing algorithms are carried out through the experiments on the standard functions. The results show that the proposed algorithm can generally provide excellent global search ability and high local search accuracy.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6909	6956		10.1007/s00500-019-04328-1													
J								Infinite Lattice Learner: an ensemble for incremental learning	SOFT COMPUTING										Supervised learning; Incremental learning; Ensemble learning; Neural networks	LINEAR DISCRIMINANT-ANALYSIS; SUPPORT VECTOR MACHINE; REGRESSION SHRINKAGE; FACE RECOGNITION; NEURAL-NETWORKS; SELECTION	The state of the art in supervised learning has developed effective models for learning, generalizing, recognizing faces and images, time series prediction, and more. However, most of these powerful models cannot effectively learn incrementally. Infinite Lattice Learner (ILL) is an ensemble model that extends state-of-the-art machine learning methods into incremental learning models. With ILL, even batch models can learn incrementally with exceptional data retention ability. Instead of continually revisiting past instances to retain learned information, ILL allows existing methods to converge on new information without overriding previous knowledge. With ILL, models can efficiently chase a drifting function without continually revisiting a changing dataset. Models wrapped in ILL can operate in continuous real-time environments where millions of unique samples are seen every day. Big datasets too large to fit in memory, or even a single machine, can be learned in portions. ILL utilizes an infinite Cartesian grid of points with an underlying model tiled upon it. Efficient algorithms for discovering nearby points and lazy evaluation make this seemingly impossible task possible. Extensive empirical evaluation reveals impressive retention ability for all ILL models. ILL similarly proves its generalization ability on a variety of datasets from classification and regression to image recognition.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6957	6974		10.1007/s00500-019-04330-7													
J								Reliability analysis of general systems with bi-uncertain variables	SOFT COMPUTING										Uncertainty theory; Uncertainty distribution; Bi-uncertain variable; Reliability; Mean time to failure	MODEL; TIME	In this paper, the lifetimes of system components are assumed to have independent and nonidentical uncertainty distributions with uncertain parameters. The reliability functions and mean time to failure of the general systems are investigated according to the uncertainty theory. Basic models of the general systems with bi-uncertain variables are established and analyzed, including series, parallel and series-parallel systems. The explicit expressions of reliability function and mean time to failure of each model are presented. Some numerical examples are given to illustrate the applications of the developed models and perform a comparison for the models with uncertain and bi-uncertain variables.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6975	6986		10.1007/s00500-019-04331-6													
J								A new distance measure of interval-valued intuitionistic fuzzy sets and its application in decision making	SOFT COMPUTING										Interval-valued intuitionistic fuzzy set; Distance measure; Decision making; Uncertainty	SOFT SETS; BELIEF ENTROPY; SELECTION; NUMBERS; COMBINATION; ALGORITHM; OPERATORS; RULES; MODEL	Interval-valued intuitionistic fuzzy sets are widely used in multi-attribute decision-making problems to select the optimal alternative, but how to measure uncertainty is an open and significant problem. In this paper, a new distance measure of interval-valued intuitionistic fuzzy sets is proposed based on the distance of interval numbers. With the advantages of taking account of the whole number in the interval and having definite physical meaning, the proposed distance measure of interval-valued intuitionistic fuzzy sets shows superiority in measuring uncertainty and imprecision. In addition, the proposed distance measure is compared with some recent research works and classical distances through numerical examples. Graphs are drawn to visually display the variation characteristics and analyze the properties of the distance measures. The results prove that the proposed distance measure of interval-valued intuitionistic fuzzy sets outperforms other metrics in measuring uncertainty and avoiding counterintuitive cases. Some illustrative examples of multi-attribute decision making under real life are conducted, which demonstrates the strong discrimination capability and effectiveness of the proposed distance measure.																	1432-7643	1433-7479				MAY	2020	24	9			SI		6987	7003		10.1007/s00500-019-04332-5													
J								Adversarial Cross-Spectral Face Completion for NIR-VIS Face Recognition	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Heterogeneous face recognition; near infrared-visible matching; face completion; face inpainting	COUPLED DICTIONARY; REGRESSION	Near infrared-visible (NIR-VIS) heterogeneous face recognition refers to the process of matching NIR to VIS face images. Current heterogeneous methods try to extend VIS face recognition methods to the NIR spectrum by synthesizing VIS images from NIR images. However, due to the self-occlusion and sensing gap, NIR face images lose some visible lighting contents so that they are always incomplete compared to VIS face images. This paper models high-resolution heterogeneous face synthesis as a complementary combination of two components: a texture inpainting component and a pose correction component. The inpainting component synthesizes and inpaints VIS image textures from NIR image textures. The correction component maps any pose in NIR images to a frontal pose in VIS images, resulting in paired NIR and VIS textures. A warping procedure is developed to integrate the two components into an end-to-end deep network. A fine-grained discriminator and a wavelet-based discriminator are designed to improve visual quality. A novel 3D-based pose correction loss, two adversarial losses, and a pixel loss are imposed to ensure synthesis results. We demonstrate that by attaching the correction component, we can simplify heterogeneous face synthesis from one-to-many unpaired image translation to one-to-one paired image translation, and minimize the spectral and pose discrepancy during heterogeneous recognition. Extensive experimental results show that our network not only generates high-resolution VIS face images but also facilitates the accuracy improvement of heterogeneous face recognition.																	0162-8828	1939-3539				MAY 1	2020	42	5					1025	1037		10.1109/TPAMI.2019.2961900													
J								Recurrent Temporal Aggregation Framework for Deep Video Inpainting	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Video inpainting; video completion; video object removal; video caption removal; video decaptioning; video editing		Video inpainting aims to fill in spatio-temporal holes in videos with plausible content. Despite tremendous progress on deep learning-based inpainting of a single image, it is still challenging to extend these methods to video domain due to the additional time dimension. In this paper, we propose a recurrent temporal aggregation framework for fast deep video inpainting. In particular, we construct an encoder-decoder model, where the encoder takes multiple reference frames which can provide visible pixels revealed from the scene dynamics. These hints are aggregated and fed into the decoder. We apply a recurrent feedback in an auto-regressive manner to enforce temporal consistency in the video results. We propose two architectural designs based on this framework. Our first model is a blind video decaptioning network (BVDNet) that is designed to automatically remove and inpaint text overlays in videos without any mask information. Our BVDNet wins the first place in the ECCV Chalearn 2018 LAP Inpainting Competition Track 2: Video Decaptioning. Second, we propose a network for more general video inpainting (VINet) to deal with more arbitrary and larger holes. Video results demonstrate the advantage of our framework compared to state-of-the-art methods both qualitatively and quantitatively. The codes are available at https://github.com/mcahny/Deep-Video-Inpainting, and https://github.com/shwoo93/video_decaptioning.																	0162-8828	1939-3539				MAY 1	2020	42	5					1038	1052		10.1109/TPAMI.2019.2958083													
J								A Temporally-Aware Interpolation Network for Video Frame Inpainting	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Interpolation; Task analysis; Predictive models; Data models; Sun; Computer science; Video inpainting; video prediction; frame interpolation; temporal upsampling	COMPLETION; CAMERA	In this work, we explore video frame inpainting, a task that lies at the intersection of general video inpainting, frame interpolation, and video prediction. Although our problem can be addressed by applying methods from other video interpolation or extrapolation tasks, doing so fails to leverage the additional context information that our problem provides. To this end, we devise a method specifically designed for video frame inpainting that is composed of two modules: a bidirectional video prediction module and a temporally-aware frame interpolation module. The prediction module makes two intermediate predictions of the missing frames, each conditioned on the preceding and following frames respectively, using a shared convolutional LSTM-based encoder-decoder. The interpolation module blends the intermediate predictions by using time information and hidden activations from the video prediction module to resolve disagreements between the predictions. Our experiments demonstrate that our approach produces smoother and more accurate results than state-of-the-art methods for general video inpainting, frame interpolation, and video prediction.																	0162-8828	1939-3539				MAY 1	2020	42	5					1053	1068		10.1109/TPAMI.2019.2951667													
J								3D Human Pose Machines with Self-Supervised Learning	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Three-dimensional displays; Two dimensional displays; Pose estimation; Solid modeling; Task analysis; Deep learning; Feature extraction; Human pose estimation; convolutional neural networks; spatio-temporal modeling; self-supervised learning; geometric deep learning		Driven by recent computer vision and robotic applications, recovering 3D human poses has become increasingly important and attracted growing interests. In fact, completing this task is quite challenging due to the diverse appearances, viewpoints, occlusions and inherently geometric ambiguities inside monocular images. Most of the existing methods focus on designing some elaborate priors /constraints to directly regress 3D human poses based on the corresponding 2D human pose-aware features or 2D pose predictions. However, due to the insufficient 3D pose data for training and the domain gap between 2D space and 3D space, these methods have limited scalabilities for all practical scenarios (e.g., outdoor scene). Attempt to address this issue, this paper proposes a simple yet effective self-supervised correction mechanism to learn all intrinsic structures of human poses from abundant images. Specifically, the proposed mechanism involves two dual learning tasks, i.e., the 2D-to-3D pose transformation and 3D-to-2D pose projection, to serve as a bridge between 3D and 2D human poses in a type of "free" self-supervision for accurate 3D human pose estimation. The 2D-to-3D pose implies to sequentially regress intermediate 3D poses by transforming the pose representation from the 2D domain to the 3D domain under the sequence-dependent temporal context, while the 3D-to-2D pose projection contributes to refining the intermediate 3D poses by maintaining geometric consistency between the 2D projections of 3D poses and the estimated 2D poses. Therefore, these two dual learning tasks enable our model to adaptively learn from 3D human pose data and external large-scale 2D human pose data. We further apply our self-supervised correction mechanism to develop a 3D human pose machine, which jointly integrates the 2D spatial relationship, temporal smoothness of predictions and 3D geometric knowledge. Extensive evaluations on the Human3.6M and HumanEva-I benchmarks demonstrate the superior performance and efficiency of our framework over all the compared competing methods.																	0162-8828	1939-3539				MAY 1	2020	42	5					1069	1082		10.1109/TPAMI.2019.2892452													
J								Fast Cross-Validation for Kernel-Based Algorithms	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Approximation algorithms; Kernel; Training; Taylor series; Support vector machines; Upper bound; Computational modeling; Cross-validation; approximation; bouligand influence function; model selection; kernel methods	SUPPORT VECTOR MACHINES; MODEL SELECTION; ROBUSTNESS PROPERTIES; CONSISTENCY; HYPERPARAMETERS; REGULARIZATION; REGRESSION	Cross-validation (CV) is a widely adopted approach for selecting the optimal model. However, the computation of empirical cross-validation error (CVE) has high complexity due to multiple times of learner training. In this paper, we develop a novel approximation theory of CVE and present an approximate approach to CV based on the Bouligand influence function (BIF) for kernel-based algorithms. We first represent the BIF and higher order BIFs in Taylor expansions, and approximate CV via the Taylor expansions. We then derive an upper bound of the discrepancy between the original and approximate CV. Furthermore, we provide a novel computing method to calculate the BIF for general distribution, and evaluate BIF criterion for sample distribution to approximate CV. The proposed approximate CV requires training on the full data set only once and is suitable for a wide variety of kernel-based algorithms. Experimental results demonstrate that the proposed approximate CV is sound and effective.																	0162-8828	1939-3539				MAY 1	2020	42	5					1083	1096		10.1109/TPAMI.2019.2892371													
J								Heterogeneous Recommendation via Deep Low-Rank Sparse Collective Factorization	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Optimization; Numerical models; Sparse matrices; Motion pictures; Stochastic processes; Collaboration; Sports; Recommendation; cross-domain; collaborative factorization; low-rank decomposition	PERSONALIZATION; ALGORITHM	A real-world recommender usually adopts heterogeneous types of user feedbacks, for example, numerical ratings such as 5-star grades and binary ratings such as likes and dislikes. In this work, we focus on transferring knowledge from binary ratings to numerical ratings, facing a more serious data sparsity problem. Conventional Collective Factorization methods usually assume that there are shared user and item latent factors across multiple related domains, but may ignore the shared common knowledge of rating patterns. Furthermore, existing works may also fail to consider the hierarchical structures in the heterogeneous recommendation scenario (i.e., genre, sub-genre, detailed-category). To address these challenges, in this paper, we propose a novel Deep Low-rank Sparse Collective Factorization (DLSCF) framework for heterogeneous recommendation. Specifically, we adopt low-rank sparse decomposition to capture the common rating patterns in related domains while splitting the domain-specific patterns. We also factorize the model in multiple layers to capture the affiliation relation between latent categories and sub-categories. We propose both batch and Stochastic Gradient Descent (SGD) based optimization algorithms for solving DLSCF. Experimental results on MoviePilot, Netfilx, Flixter, MovieLens10M and MovieLens20M datasets demonstrate the effectiveness of the proposed algorithms, by comparing them with several state-of-the-art batch and SGD based approaches.																	0162-8828	1939-3539				MAY 1	2020	42	5					1097	1111		10.1109/TPAMI.2019.2894137													
J								Hierarchical LSTMs with Adaptive Attention for Visual Captioning	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Visualization; Feature extraction; Task analysis; Decoding; Adaptation models; Natural language processing; Video captioning; image captioning; adaptive attention; hierarchical structure		Recent progress has been made in using attention based encoder-decoder framework for image and video captioning. Most existing decoders apply the attention mechanism to every generated word including both visual words (e.g., "gun" and "shooting") and non-visual words (e.g., "the", "a"). However, these non-visual words can be easily predicted using natural language model without considering visual signals or attention. Imposing attention mechanism on non-visual words could mislead and decrease the overall performance of visual captioning. Furthermore, the hierarchy of LSTMs enables more complex representation of visual data, capturing information at different scales. Considering these issues, we propose a hierarchical LSTM with adaptive attention (hLSTMat) approach for image and video captioning. Specifically, the proposed framework utilizes the spatial or temporal attention for selecting specific regions or frames to predict the related words, while the adaptive attention is for deciding whether to depend on the visual information or the language context information. Also, a hierarchical LSTMs is designed to simultaneously consider both low-level visual information and high-level language context information to support the caption generation. We design the hLSTMat model as a general framework, and we first instantiate it for the task of video captioning. Then, we further instantiate our hLSTMarefine it and apply it to the imioning task. To demonstrate the effectiveness of our proposed framework, we test our method on both video and image captioning tasks. Experimental results show that our approach achieves the state-of-the-art performance for most of the evaluation metrics on both tasks. The effect of important components is also well exploited in the ablation study.																	0162-8828	1939-3539				MAY 1	2020	42	5					1112	1131		10.1109/TPAMI.2019.2894139													
J								Large-Scale Urban Reconstruction with Tensor Clustering and Global Boundary Refinement	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Laser radar; Three-dimensional displays; Buildings; Data mining; Solid modeling; Measurement; Tensor clustering; pointcloud segmentation; pointcloud tensor field; parameter-free clustering; LiDAR reconstruction; boundary refinement		Accurate and efficient methods for large-scale urban reconstruction are of significant importance to the computer vision and computer graphics communities. Although rapid acquisition techniques such as airborne LiDAR have been around for many years, creating a useful and functional virtual environment from such data remains difficult and labor intensive. This is due largely to the necessity in present solutions for data dependent user defined parameters. In this paper we present a new solution for automatically converting large LiDAR data pointcloud into simplified polygonal 3D models. The data is first divided into smaller components which are processed independently and concurrently to extract various metrics about the points. Next, the extracted information is converted into tensors. A robust agglomerate clustering algorithm is proposed to segment the tensors into clusters representing geospatial objects e.g., roads, buildings, etc. Unlike previous methods, the proposed tensor clustering process has no data dependencies and does not require any user-defined parameter. The required parameters are adaptively computed assuming a Weibull distribution for similarity distances. Lastly, to extract boundaries from the clusters a new multi-stage boundary refinement process is developed by reformulating this extraction as a global optimization problem. We have extensively tested our methods on several pointcloud datasets of different resolutions which exhibit significant variability in geospatial characteristics e.g., ground surface inclination, building density, etc and the results are reported. The source code for both tensor clustering and global boundary refinement will be made publicly available with the publication on the author's website.																	0162-8828	1939-3539				MAY 1	2020	42	5					1132	1145		10.1109/TPAMI.2019.2893671													
J								LCR-Net plus plus : Multi-Person 2D and 3D Pose Detection in Natural Images	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Three-dimensional displays; Two dimensional displays; Pose estimation; Proposals; Joints; Heating systems; Training data; Human 3D pose estimation; 2D pose estimation; detection; localization; classification; regression; CNN		We propose an end-to-end architecture for joint 2D and 3D human pose estimation in natural images. Key to our approach is the generation and scoring of a number of pose proposals per image, which allows us to predict 2D and 3D poses of multiple people simultaneously. Hence, our approach does not require an approximate localization of the humans for initialization. Our Localization-Classification-Regression architecture, named LCR-Net, contains 3 main components: 1) the pose proposal generator that suggests candidate poses at different locations in the image; 2) a classifier that scores the different pose proposals; and 3) a regressor that refines pose proposals both in 2D and 3D. All three stages share the convolutional feature layers and are trained jointly. The final pose estimation is obtained by integrating over neighboring pose hypotheses, which is shown to improve over a standard non maximum suppression algorithm. Our method recovers full-body 2D and 3D poses, hallucinating plausible body parts when the persons are partially occluded or truncated by the image boundary. Our approach significantly outperforms the state of the art in 3D pose estimation on Human3.6M, a controlled environment. Moreover, it shows promising results on real images for both single and multi-person subsets of the MPII 2D pose benchmark and demonstrates satisfying 3D pose results even for multi-person images.																	0162-8828	1939-3539				MAY 1	2020	42	5					1146	1161		10.1109/TPAMI.2019.2892985													
J								Light Field Super-Resolution Using a Low-Rank Prior and Deep Convolutional Neural Networks	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Spatial resolution; Cameras; Image restoration; Matrix decomposition; Sparse matrices; Light fields; Deep convolutional neural networks; light field; low-rank matrix approximation; super-resolution	RESOLUTION; REMOVAL	Light field imaging has recently known a regain of interest due to the availability of practical light field capturing systems that offer a wide range of applications in the field of computer vision. However, capturing high-resolution light fields remains technologically challenging since the increase in angular resolution is often accompanied by a significant reduction in spatial resolution. This paper describes a learning-based spatial light field super-resolution method that allows the restoration of the entire light field with consistency across all angular views. The algorithm first uses optical flow to align the light field and then reduces its angular dimension using low-rank approximation. We then consider the linearly independent columns of the resulting low-rank model as an embedding, which is restored using a deep convolutional neural network (DCNN). The super-resolved embedding is then used to reconstruct the remaining views. The original disparities are restored using inverse warping where missing pixels are approximated using a novel light field inpainting algorithm. Experimental results show that the proposed method outperforms existing light field super-resolution algorithms, achieving PSNR gains of 0.23 dB over the second best performing method. The performance is shown to be further improved using iterative back-projection as a post-processing step.																	0162-8828	1939-3539				MAY 1	2020	42	5					1162	1175		10.1109/TPAMI.2019.2893666													
J								Minimal Case Relative Pose Computation Using Ray-Point-Ray Features	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Three-dimensional displays; Transmission line matrix methods; Cameras; Pose estimation; Feature extraction; Geometry; Computer vision; Structure-from-motion; visual odometry; minimal relative pose; automatic solver generation; Grobner bases; ray-point-ray structures	CLOSED-FORM SOLUTION; EGOMOTION ESTIMATION; MOTION	Corners are popular features for relative pose computation with 2D-2D point correspondences. Stable corners may be formed by two 3D rays sharing a common starting point. We call such elements ray-point-ray (RPR) structures. Besides a local invariant keypoint given by the lines' intersection, their reprojection also defines a corner orientation and an inscribed angle in the image plane. The present paper investigates such RPR features, and aims at answering the fundamental question of what additional constraints can be formed from correspondences between RPR features in two views. In particular, we show that knowing the value of the inscribed angle between the two 3D rays poses additional constraints on the relative orientation. Using the latter enables the solution of the relative pose problem with as few as 3 correspondences across the two images. We provide a detailed analysis of all minimal cases distinguishing between 90-degree RPR-structures and structures with an arbitrary, known inscribed angle. We furthermore investigate the special cases of a known directional correspondence and planar motion, the latter being solvable with only a single RPR correspondence. We complete the exposition by outlining an image processing technique for robust RPR-feature extraction. Our results suggest high practicality in man-made environments, where 90-degree RPR-structures naturally occur.																	0162-8828	1939-3539				MAY 1	2020	42	5					1176	1190		10.1109/TPAMI.2019.2892372													
J								Multiple Kernel k-Means with Incomplete Kernels	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Kernel; Clustering algorithms; Optimization; Pattern analysis; Information technology; Prediction algorithms; Multiple kernel clustering; multiple view learning; incomplete kernel learning		Multiple kernel clustering (MKC) algorithms optimally combine a group of pre-specified base kernel matrices to improve clustering performance. However, existing MKC algorithms cannot efficiently address the situation where some rows and columns of base kernel matrices are absent. This paper proposes two simple yet effective algorithms to address this issue. Different from existing approaches where incomplete kernel matrices are first imputed and a standard MKC algorithm is applied to the imputed kernel matrices, our first algorithm integrates imputation and clustering into a unified learning procedure. Specifically, we perform multiple kernel clustering directly with the presence of incomplete kernel matrices, which are treated as auxiliary variables to be jointly optimized. Our algorithm does not require that there be at least one complete base kernel matrix over all the samples. Also, it adaptively imputes incomplete kernel matrices and combines them to best serve clustering. Moreover, we further improve this algorithm by encouraging these incomplete kernel matrices to mutually complete each other. The three-step iterative algorithm is designed to solve the resultant optimization problems. After that, we theoretically study the generalization bound of the proposed algorithms. Extensive experiments are conducted on 13 benchmark data sets to compare the proposed algorithms with existing imputation-based methods. Our algorithms consistently achieve superior performance and the improvement becomes more significant with increasing missing ratio, verifying the effectiveness and advantages of the proposed joint imputation and clustering.																	0162-8828	1939-3539				MAY 1	2020	42	5					1191	1204		10.1109/TPAMI.2019.2892416													
J								Online Meta Adaptation for Fast Video Object Segmentation	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Adaptation models; Task analysis; Object segmentation; Optical imaging; Motion segmentation; Image segmentation; Runtime; Meta learning; video object segmentation; convolutional neural networks		Conventional deep neural networks based video object segmentation (VOS) methods are dominated by heavily fine-tuning a segmentation model on the first frame of a given video, which is time-consuming and inefficient. In this paper, we propose a novel method which rapidly adapts a base segmentation model to new video sequences with only a couple of model-update iterations, without sacrificing performance. Such attractive efficiency benefits from the meta-learning paradigm which leads to a meta-segmentation model and a novel continuous learning approach which enables online adaptation of the segmentation model. Concretely, we train a meta-learner on multiple VOS tasks such that the meta model can capture their common knowledge and gains the ability to fast adapt the segmentation model to new video sequences. Furthermore, to deal with unique challenges of VOS tasks from temporal variations in the video, e.g., object motion and appearance changes, we propose a principled online adaptation approach that continuously adapts the segmentation model across video frames by exploiting temporal context effectively, providing robustness to annoying temporal variations. Integrating the meta-learner with the online adaptation approach, the proposed VOS model achieves competitive performance against the state-of-the-arts and moreover provides faster per-frame processing speed.																	0162-8828	1939-3539				MAY 1	2020	42	5					1205	1217		10.1109/TPAMI.2018.2890659													
J								Pixel Transposed Convolutional Networks	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Convolution; Semantics; Image segmentation; Kernel; Task analysis; Image generation; Analytical models; Deep learning; pixel-wise prediction; up-sampling; transposed convolution; pixel transposed convolution		Transposed convolutional layers have been widely used in a variety of deep models for up-sampling, including encoder-decoder networks for semantic segmentation and deep generative models for unsupervised learning. One of the key limitations of transposed convolutional operations is that they result in the so-called checkerboard problem. This is caused by the fact that no direct relationship exists among adjacent pixels on the output feature map. To address this problem, we propose the pixel transposed convolutional layer (PixelTCL) to establish direct relationships among adjacent pixels on the up-sampled feature map. Our method is based on a fresh interpretation of the regular transposed convolutional operation. The resulting PixelTCL can be used to replace any transposed convolutional layer in a plug-and-play manner without compromising the fully trainable capabilities of original models. The proposed PixelTCL may result in slight decrease in efficiency, but this can be overcome by an implementation trick. Experimental results on semantic segmentation demonstrate that PixelTCL can consider spatial features such as edges and shapes and yields more accurate segmentation outputs than transposed convolutional layers. When used in image generation tasks, our PixelTCL can largely overcome the checkerboard problem suffered by regular transposed convolutional operations.																	0162-8828	1939-3539				MAY 1	2020	42	5					1218	1227		10.1109/TPAMI.2019.2893965													
J								RefineNet: Multi-Path Refinement Networks for Dense Prediction	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Semantics; Estimation; Image segmentation; Task analysis; Convolution; Training; Visualization; Convolutional neural network; semantic segmentation; object parsing; human parsing; scene parsing; depth estimation; dense prediction		Recently, very deep convolutional neural networks (CNNs) have shown outstanding performance in object recognition and have also been the first choice for dense prediction problems such as semantic segmentation and depth estimation. However, repeated subsampling operations like pooling or convolution striding in deep CNNs lead to a significant decrease in the initial image resolution. Here, we present RefineNet, a generic multi-path refinement network that explicitly exploits all the information available along the down-sampling process to enable high-resolution prediction using long-range residual connections. In this way, the deeper layers that capture high-level semantic features can be directly refined using fine-grained features from earlier convolutions. The individual components of RefineNet employ residual connections following the identity mapping mindset, which allows for effective end-to-end training. Further, we introduce chained residual pooling, which captures rich background context in an efficient manner. We carry out comprehensive experiments on semantic segmentation which is a dense classification problem and achieve good performance on seven public datasets. We further apply our method for depth estimation and demonstrate the effectiveness of our method on dense regression problems.																	0162-8828	1939-3539				MAY 1	2020	42	5					1228	1242		10.1109/TPAMI.2019.2893630													
J								Shared Multi-View Data Representation for Multi-Domain Event Detection	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Data models; Dictionaries; Event detection; Social network services; Computational modeling; Task analysis; Media; Multi-domain event discovery; multimodal fusion; data representation learning	LOW-RANK; MATRIX FACTORIZATION; IMAGE; COMPLETION; ALGORITHM; MODEL; GRAPH	Internet platforms provide new ways for people to share experiences, generating massive amounts of data related to various real-world concepts. In this paper, we present an event detection framework to discover real-world events from multiple data domains, including online news media and social media. As multi-domain data possess multiple data views that are heterogeneous, initial dictionaries consisting of labeled data samples are exploited to align the multi-view data. Furthermore, a shared multi-view data representation (SMDR) model is devised, which learns underlying and intrinsic structures shared among the data views by considering the structures underlying the data, data variations, and informativeness of dictionaries. SMDR incorpvarious constraints in the objective function, including shared representation, low-rank, local invariance, reconstruction error, and dictionary independence constraints. Given the data representations achieved by SMDR, class-wise residual models are designed to discover the events underlying the data based on the reconstruction residuals. Extensive experiments conducted on two real-world event detection datasets, i.e., Multi-domain and Multi-modality Event Detection dataset, and MediaEval Social Event Detection 2014 dataset, indicating the effectiveness of the proposed approaches.																	0162-8828	1939-3539				MAY 1	2020	42	5					1243	1256		10.1109/TPAMI.2019.2893953													
J								Structured Label Inference for Visual Understanding	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Videos; Visualization; Task analysis; Hidden Markov models; Deep learning; Feature extraction; Neural networks; Computer vision; multi-label classification; image classification; video recognition; action detection; structured inference	CLASSIFICATION; RECOGNITION; DATABASE	Visual data such as images and videos contain a rich source of structured semantic labels as well as a wide range of interacting components. Visual content could be assigned with fine-grained labels describing major components, coarse-grained labels depicting high level abstractions, or a set of labels revealing attributes. Such categorization over different, interacting layers of labels evinces the potential for a graph-based encoding of label information. In this paper, we exploit this rich structure for performing graph-based inference in label space for a number of tasks: multi-label image and video classification and action detection in untrimmed videos. We consider the use of the Bidirectional Inference Neural Network (BINN) and Structured Inference Neural Network (SINN) for performing graph-based inference in label space and propose a Long Short-Term Memory (LSTM) based extension for exploiting activity progression on untrimmed videos. The methods were evaluated on (i) the Animal with Attributes (AwA), Scene Understanding (SUN) and NUS-WIDE datasets for multi-label image classification, (ii) the first two releases of the YouTube-8M large scale dataset for multi-label video classification, and (iii) the THUMOS'14 and MultiTHUMOS video datasets for action detection. Our results demonstrate the effectiveness of structured label inference in these challenging tasks, achieving significant improvements against baselines.																	0162-8828	1939-3539				MAY 1	2020	42	5					1257	1271		10.1109/TPAMI.2019.2893215													
J								Object Detection in Videos by High Quality Object Linking	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Object detection in videos; object linking	NETWORKS	Compared with object detection in static images, object detection in videos is more challenging due to degraded image qualities. An effective way to address this problem is to exploit temporal contexts by linking the same object across video to form tubelets and aggregating classification scores in the tubelets. In this paper, we focus on obtaining high quality object linking results for better classification. Unlike previous methods that link objects by checking boxes between neighboring frames, we propose to link in the same frame. To achieve this goal, we extend prior methods in following aspects: (1) a cuboid proposal network that extracts spatio-temporal candidate cuboids which bound the movement of objects; (2) a short tubelet detection network that detects short tubelets in short video segments; (3) a short tubelet linking algorithm that links temporally-overlapping short tubelets to form long tubelets. Experiments on the ImageNet VID dataset show that our method outperforms both the static image detector and the previous state of the art. In particular, our method improves results by 8.8 percent over the static image detector for fast moving objects.																	0162-8828	1939-3539				MAY 1	2020	42	5					1272	1278		10.1109/TPAMI.2019.2910529													
J								Significance of Softmax-Based Features in Comparison to Distance Metric Learning-Based Features	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Feature extraction; Measurement; Principal component analysis; Dimensionality reduction; Network architecture; Automobiles; Task analysis; Deep learning; distance metric learning; classification; retrieval		End-to-end distance metric learning (DML) has been applied to obtain features useful in many computer vision tasks. However, these DML studies have not provided equitable comparisons between features extracted from DML-based networks and softmax-based networks. In this paper, we present objective comparisons between these two approaches under the same network architecture.																	0162-8828	1939-3539				MAY 1	2020	42	5					1279	1285		10.1109/TPAMI.2019.2911075													
J								Providing a Single Ground-Truth for Illuminant Estimation for the ColorChecker Dataset	IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE										Color constancy; illuminant estimation; algorithms evaluation		The ColorChecker dataset is one of the most widely used image sets for evaluating and ranking illuminant estimation algorithms. However, this single set of images has at least 3 different sets of ground-truth (i.e., correct answers) associated with it. In the literature it is often asserted that one algorithm is better than another when the algorithms in question have been tuned and tested with the different ground-truths. In this short correspondence we present some of the background as to why the 3 existing ground-truths are different and go on to make a new single and recommended set of correct answers. Experiments reinforce the importance of this work in that we show that the total ordering of a set of algorithms may be reversed depending on whether we use the new or legacy ground-truth data.																	0162-8828	1939-3539				MAY 1	2020	42	5					1286	1287		10.1109/TPAMI.2019.2919824													
J								TS-CHIEF: a scalable and accurate forest algorithm for time series classification	DATA MINING AND KNOWLEDGE DISCOVERY										Time series; Classification; Metrics; Bag of words; Transformation; Forest; Scalable	REPRESENTATION; SIMILARITY; DISTANCE	Time Series Classification (TSC) has seen enormous progress over the last two decades. HIVE-COTE (Hierarchical Vote Collective of Transformation-based Ensembles) is the current state of the art in terms of classification accuracy. HIVE-COTE recognizes that time series data are a specific data type for which the traditional attribute-value representation, used predominantly in machine learning, fails to provide a relevant representation. HIVE-COTE combines multiple types of classifiers: each extracting information about a specific aspect of a time series, be it in the time domain, frequency domain or summarization of intervals within the series. However, HIVE-COTE (and its predecessor, FLAT-COTE) is often infeasible to run on even modest amounts of data. For instance, training HIVE-COTE on a dataset with only 1500 time series can require 8 days of CPU time. It has polynomial runtime with respect to the training set size, so this problem compounds as data quantity increases. We propose a novel TSC algorithm, TS-CHIEF (Time Series Combination of Heterogeneous and Integrated Embedding Forest), which rivals HIVE-COTE in accuracy but requires only a fraction of the runtime. TS-CHIEF constructs an ensemble classifier that integrates the most effective embeddings of time series that research has developed in the last decade. It uses tree-structured classifiers to do so efficiently. We assess TS-CHIEF on 85 datasets of the University of California Riverside (UCR) archive, where it achieves state-of-the-art accuracy with scalability and efficiency. We demonstrate that TS-CHIEF can be trained on 130 k time series in 2 days, a data quantity that is beyond the reach of any TSC algorithm with comparable accuracy.																	1384-5810	1573-756X				MAY	2020	34	3					742	775		10.1007/s10618-020-00679-8													
J								An efficient K-means clustering algorithm for tall data	DATA MINING AND KNOWLEDGE DISCOVERY										K-means problem; Lloyd's algorithm; K-means plus plus; Coresets; Unsupervised learning		The analysis of continously larger datasets is a task of major importance in a wide variety of scientific fields. Therefore, the development of efficient and parallel algorithms to perform such an analysis is a a crucial topic in unsupervised learning. Cluster analysis algorithms are a key element of exploratory data analysis and, among them, the K-means algorithm stands out as the most popular approach due to its easiness in the implementation, straightforward parallelizability and relatively low computational cost. Unfortunately, the K-means algorithm also has some drawbacks that have been extensively studied, such as its high dependency on the initial conditions, as well as to the fact that it might not scale well on massive datasets. In this article, we propose a recursive and parallel approximation to the K-means algorithm that scales well on the number of instances of the problem, without affecting the quality of the approximation. In order to achieve this, instead of analyzing the entire dataset, we work on small weighted sets of representative points that are distributed in such a way that more importance is given to those regions where it is harder to determine the correct cluster assignment of the original instances. In addition to different theoretical properties, which explain the reasoning behind the algorithm, experimental results indicate that our method outperforms the state-of-the-art in terms of the trade-off between number of distance computations and the quality of the solution obtained.																	1384-5810	1573-756X				MAY	2020	34	3					776	811		10.1007/s10618-020-00678-9													
J								Discrete-time survival forests with Hellinger distance decision trees	DATA MINING AND KNOWLEDGE DISCOVERY										Class imbalance; Discrete event times; Hellinger's distance; Random survival forests; Recursive partitioning; Survival analysis	VARIABLE SELECTION; PREDICTION; RULES	Random survival forests (RSF) are a powerful nonparametric method for building prediction models with a time-to-event outcome. RSF do not rely on the proportional hazards assumption and can be readily applied to both low- and higher-dimensional data. A remaining limitation of RSF, however, arises from the fact that the method is almost entirely focussed on continuously measured event times. This issue may become problematic in studies where time is measured on a discrete scale t=1,2,...\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$t = 1, 2, ...$$\end{document}, referring to time intervals [0,a1),[a1,a2), horizontal ellipsis \documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$[0,a_1), [a_1,a_2), \ldots $$\end{document}. In this situation, the application of methods designed for continuous time-to-event data may lead to biased estimators and inaccurate predictions if discreteness is ignored. To address this issue, we develop a RSF algorithm that is specifically designed for the analysis of (possibly right-censored) discrete event times. The algorithm is based on an ensemble of discrete-time survival trees that operate on transformed versions of the original time-to-event data using tree methods for binary classification. As the outcome variable in these trees is typically highly imbalanced, our algorithm implements a node splitting strategy based on Hellinger's distance, which is a skew-insensitive alternative to classical split criteria such as the Gini impurity. The new algorithm thus provides flexible nonparametric predictions of individual-specific discrete hazard and survival functions. Our numerical results suggest that node splitting by Hellinger's distance improves predictive performance when compared to the Gini impurity. Furthermore, discrete-time RSF improve prediction accuracy when compared to RSF approaches treating discrete event times as continuous in situations where the number of time intervals is small.																	1384-5810	1573-756X				MAY	2020	34	3					812	832		10.1007/s10618-020-00682-z													
J								Transductive LSTM for time-series prediction: An application to weather forecasting	NEURAL NETWORKS										Transductive learning; Long short-term memory; Weather forecasting	CLASSIFICATION	Long Short-Term Memory (LSTM) has shown significant performance on many real-world applications due to its ability to capture long-term dependencies. In this paper, we utilize LSTM to obtain a data-driven forecasting model for an application of weather forecasting. Moreover, we propose Transductive LSTM (T-LSTM) which exploits the local information in time-series prediction. In transductive learning, the samples in the test point vicinity are considered to have higher impact on fitting the model. In this study, a quadratic cost function is considered for the regression problem. Localizing the objective function is done by considering a weighted quadratic cost function at which point the samples in the neighborhood of the test point have larger weights. We investigate two weighting schemes based on the cosine similarity between the training samples and the test point. In order to assess the performance of the proposed method in different weather conditions, the experiments are conducted on two different time periods of a year. The results show that T-LSTM results in better performance in the prediction task. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAY	2020	125						1	9		10.1016/j.neunet.2019.12.030													
J								Modeling uncertainty-seeking behavior mediated by cholinergic influence on dopamine	NEURAL NETWORKS										Decision-making; Acetylcholine; Dopamine; Uncertainty	REWARD; NEUROMODULATION; ACETYLCHOLINE; HUMANS	Recent findings suggest that acetylcholine mediates uncertainty-seeking behaviors through its projection to dopamine neurons - another neuromodulatory system known for its major role in reinforcement learning and decision-making. In this paper, we propose a leaky-integrate-and-fire model of this mechanism. It implements a softmax-like selection with an uncertainty bonus by a cholinergic drive to dopaminergic neurons, which in turn influence synaptic currents of downstream neurons. The model is able to reproduce experimental data in two decision-making tasks. It also predicts that: (i) in the absence of cholinergic input, dopaminergic activity would not correlate with uncertainty, and that (ii) the adaptive advantage brought by the implemented uncertainty-seeking mechanism is most useful when sources of reward are not highly uncertain. Moreover, this modeling work allows us to propose novel experiments which might shed new light on the role of acetylcholine in both random and directed exploration. Overall, this study contributes to a more comprehensive understanding of the role of the cholinergic system and, in particular, its involvement in decision-making. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAY	2020	125						10	18		10.1016/j.neunet.2020.01.032													
J								Reconstruction of natural visual scenes from neural spikes with deep neural networks	NEURAL NETWORKS										Vision; Natural scenes; Neural decoding; Neural spikes; Deep learning; Artificial retina	GANGLION-CELLS; PROCESSOR; ALGORITHM; RESPONSES; MODELS; IMAGES	Neural coding is one of the central questions in systems neuroscience for understanding how the brain processes stimulus from the environment, moreover, it is also a cornerstone for designing algorithms of brain-machine interface, where decoding incoming stimulus is highly demanded for better performance of physical devices. Traditionally researchers have focused on functional magnetic resonance imaging (fMRI) data as the neural signals of interest for decoding visual scenes. However, our visual perception operates in a fast time scale of millisecond in terms of an event termed neural spike. There are few studies of decoding by using spikes. Here we fulfill this aim by developing a novel decoding framework based on deep neural networks, named spike-image decoder (SID), for reconstructing natural visual scenes, including static images and dynamic videos, from experimentally recorded spikes of a population of retinal ganglion cells. The SID is an end-to-end decoder with one end as neural spikes and the other end as images, which can be trained directly such that visual scenes are reconstructed from spikes in a highly accurate fashion. Our SID also outperforms on the reconstruction of visual stimulus compared to existing fMRI decoding models. In addition, with the aid of a spike encoder, we show that SID can be generalized to arbitrary visual scenes by using the image datasets of MNIST, CIFAR10, and CIFAR100. Furthermore, with a pre-trained SID, one can decode any dynamic videos to achieve real-time encoding and decoding of visual scenes by spikes. Altogether, our results shed new light on neuromorphic computing for artificial visual systems, such as event-based visual cameras and visual neuroprostheses. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAY	2020	125						19	30		10.1016/j.neunet.2020.01.033													
J								Event-triggered synchronization of discrete-time neural networks: A switching approach	NEURAL NETWORKS										Discrete-time neural networks; Synchronization; Event-triggered control; Switching method; Actuator saturation	INFINITY STATE ESTIMATION; SAMPLED-DATA; EXPONENTIAL SYNCHRONIZATION; CHAOTIC SYSTEMS; STABILITY; DELAY; STABILIZATION; PARAMETERS; DYNAMICS; SUBJECT	This paper investigates the event-triggered synchronization control of discrete-time neural networks. The main highlights are threefold: (1) a new event-triggered mechanism (ETM) is presented, which can be regarded as a switching between the discrete-time periodic sampled-data control and a continuous ETM; (2) a saturating controller which is equipped with two switching gains is designed to match the switching property of the proposed ETM; (3) a dedicated switching Lyapunov-Krasovskii functional is constructed, which takes the sawtooth constraints of control input into account. Based on these ingredients, the synchronization criteria are derived such that the considered error systems are locally stable. Whereafter, two co-design problems are discussed to maximize the set of admissible initial conditions and the triggering threshold, respectively. Finally, the effectiveness and advantages of the proposed method are validated by two numerical examples. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAY	2020	125						31	40		10.1016/j.neunet.2020.01.024													
J								Skeleton-based Chinese sign language recognition and generation for bidirectional communication between deaf and hearing people	NEURAL NETWORKS										CSL; Recognition; Generation; RNN; Bidirectional communication; Probability model		Chinese sign language (CSL) is one of the most widely used sign language systems in the world. As such, the automatic recognition and generation of CSL is a key technology enabling bidirectional communication between deaf and hearing people. Most previous studies have focused solely on sign language recognition (SLR), which only addresses communication in a single direction. As such, there is a need for sign language generation (SLG) to enable communication in the other direction (i.e., from hearing people to deaf people). To achieve a smoother exchange of ideas between these two groups, we propose a skeleton-based CSL recognition and generation framework based on a recurrent neural network (RNN), to support bidirectional CSL communication. This process can also be extended to other sequence-to-sequence information interactions. The core of the proposed framework is a two-level probability generative model. Compared with previous techniques, this approach offers a more flexible approximate posterior distribution, which can produce skeletal sequences of varying styles that are recognizable to humans. In addition, the proposed generation method compensated for a lack of training data. A series of experiments in bidirectional communication were conducted on the large 500 CSL dataset. The proposed algorithm achieved high recognition accuracy for both real and synthetic data, with a reduced runtime. Furthermore, the generated data improved the performance of the discriminator. These results suggest the proposed bidirectional communication framework and generation algorithm to be an effective new approach to CSL recognition. (c) 2020 Elsevier Ltd. All rights reserved.																	0893-6080	1879-2782				MAY	2020	125						41	55		10.1016/j.neunet.2020.01.030													
