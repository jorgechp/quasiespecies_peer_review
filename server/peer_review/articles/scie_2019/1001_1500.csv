PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	RP	EM	RI	OI	FU	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	D2	EA	PG	WC	SC	GA	UT	PM	OA	HC	HP	DA
J								Dynamic pricing in profit-driven task assignment: a domain-of-influence based approach	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Spatial crowdsourcing; Task assignment; Dynamic pricing; Domain-of-Influence; Psychological reward		The development of mobile Internet and sharing economy brings the prosperity of spatial crowdsourcing (SC). Pricing is a crucial step for SC platforms to solve the Profit-driven Task Assignment (PTA) problem to maximize their total profit. However, dynamic pricing is still large unexplored in PTA. In addition, existing works seek solutions without considering the uncertainty of workers' acceptance for assigned tasks in the task assignment process. To deal with these challenges, we develop a two-stage task assignment framework with dynamic pricing. Specifically, we propose a novel Domain-of-Influence based dynamic pricing algorithm, which can iteratively figure out the price that represents the balance between task demand and worker supply. Then we employ hyperbolic temporal discounting function to estimate the worker's psychological reward that indicates the acceptance or rejection of assigned task. With considering the driver's psychological reward, we adopt an optimal algorithm to achieve the optimal task assignment and propose greedy algorithms to improve the computational efficiency. Finally, we evaluate the performance using two road network datasets of Jinan and Luoyang in China. The experimental results show the effectiveness and efficiency of our proposed approaches.																	1868-8071	1868-808X															10.1007/s13042-020-01217-0		OCT 2020											
J								Ontology of anomalous processes diagnosis	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										diagnosis ontology; diagnostic methods; fuzzy scale; knowledge bases; solving problem	TYPE-2 FUZZY-SYSTEMS; KNOWLEDGE	The diagnosis is complex in domains where processes occur over time inside systems or objects, changing and affecting other processes and characteristics. A large number of knowledge-based systems have been developed to increase the efficiency of the diagnostic process. But the models of knowledge of such systems do not simultaneously take into account all types of temporal relationships: changes in the values of attributes over time, taking into account the time intervals that have passed from the moments when the process began to develop and external influences, and so forth. The article considers the universal ontology of knowledge about the diagnosis of processes in various domains as a generalization and the result of many years of experience of the authors in the creation of intelligent systems for the diagnosis of processes. The ontology allows us to formalize anomalies as developing internal processes that are not inherent in a normally functioning system. On its basis (in its terms) knowledge bases and software components for decision support systems are constructed. In the case of a need for a domain-oriented ontology, it will only be necessary to redefine some concepts and relations which are features of specific domains.																	0884-8173	1098-111X															10.1002/int.22300		OCT 2020											
J								Unsynchronized wearable sensor data analytics model for improving the performance of smart healthcare systems	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Data analytics; Healthcare system; Machine learning; Time split instances; Wearable sensor	INTERNET; FRAMEWORK; FUSION; IOT	Background A wearable sensor (WS) is a prominent technology application that senses and gathers information from a user for analyzing changes in physiological signs. Analyzing the physiological sign differences enables the better healthcare solutions. Purpose This paper introduces an unsynchronized sensor data analytics (USDA) model for the effective handling of wearable device data regardless of the time factor. Time-dependent healthcare treatments and diagnosis are the themes on which this analytics model focuses. Methods The gathered WS data is classified depending on the time factor and data frequency of occurrence. This occurrence frequency is correlatively analyzed using the diagnosis module to identify defects and to fulfill the missing sensor data consideration. Healthcare diagnoses requiring immediate responses and timely solutions for patients/end-users rely on this model for uncompromising analysis. Results The vital changes in WS data and time factors are analyzed using sophisticated machine learning methods for previous diagnosis correlation and effective accuracy. Conclusion Responsive healthcare solutions using unsynchronized WS data help to achieve better efficiency and reduce complications in assessing the performance of the healthcare systems.																	1868-5137	1868-5145															10.1007/s12652-020-02576-w		OCT 2020											
J								Minimum transmission delay algorithm for multi-hop nodes in Internet of Things based on symmetric algorithm	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Symmetric algorithm; Internet of Things; Multi-hop nodes; Gradient projection algorithm; Transmission delay algorithm		In view of the increase of network information transmission times, the transmission delay of multi hop nodes in the Internet of Things increases, resulting in the decline of network information transmission performance. In order to improve the quality and performance of information transmission, a minimum transmission delay algorithm of multi-hop nodes in the Internet of Things is designed based on symmetric algorithm. In the perception layer of the Internet of Things, based on the revocable symmetric encryption algorithm, the multi-hop nodes in the perception layer are encrypted; The link information transmission delay under the direct transmission, cooperative transmission and multi-hop transmission modes is compared, and the gradient projection algorithm is used to solve the problem of minimizing the delay of multi-hop nodes' information transmission in the Internet of Things. After calculating the projection matrix and the gradient, the iteration terminates when the steady link traffic and the delay value are obtained. The source node refers to the network node that acts as the source to send the original data packet; the source node uses this algorithm to select the transmission mode with minimum delay, and sends information to the destination node step by step. The simulation test shows that the algorithm can guarantee the security of data transmission of multi-hop nodes in the Internet of Things, and the transmission delay is the minimum.																	1868-5137	1868-5145															10.1007/s12652-020-02603-w		OCT 2020											
J								Matrix representation of optimal scale for generalized multi-scale decision table	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Rough sets; Information table; Matrix representation; Boolean operators	SELECTION	Generalized multi-scale decision table is an important model in granular computing, which can be applied to feature selection and rule extraction. The generalization effect of the model is different at different scales, so scale selection is a key to generalized multi-scale decision table. However, in existing studies, scale selection is usually based on a large number of set operations, and for different types of attributes, the model cannot be directly operated. In this paper, we first investigate the generalized multi-scale information table from the perspective of matrix. Then the matrix representation of generalized multi-scale information table is proposed. Finally, we study the properties of the matrix about the optimal scale of the coordinated and uncoordinated systems respectively, so as to give the matrix method of the scale selection. Compared with traditional methods, the matrix method presented in this paper is simple and profound, and can directly deal with different types of attributes. In addition, the matrix representation of the optimal scale has certain guiding significance for the design of the scale selection algorithm.																	1868-5137	1868-5145															10.1007/s12652-020-02588-6		OCT 2020											
J								A spatiotemporal multi-feature extraction framework with space and channel based squeeze-and-excitation blocks for human activity recognition	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Human activity recognition; Deep learning; Spatiotemporal feature; Squeeze-and-excitation blocks; Aggressive activities		Human activity recognition (HAR) is an active field in ubiquitous computing and body area network (BAN), which has been widely applied in medical care, sport and smart home. In recent years, a lot of methods based on deep learning show great performance on HAR. In consideration of the temporal and spatial dependencies of time series, the extracted features of traditional methods are not comprehensive. In this paper, we propose a new activity recognition framework based on spatiotemporal multi-feature extraction with space and channel based squeeze-and-excitation blocks (SCbSE-SMFE). The framework includes a temporal feature extraction layer composed of gated recurrent unit (GRU) blocks, a spatial feature extraction layer composed of convolutional neural networks (CNN) blocks with SCbSE blocks, a statistical feature extraction layer and an output layer. Meanwhile, regarding the actual needs for recognizing aggressive activities, we simulate the prison environment and collect an aggressive activity dataset (AAD). What's more, aiming at the characteristics of aggressive activities, a threshold-based aggressive activity detection method is proposed to reduce the computational complexity. The proposed framework is evaluated on the public dataset WISDM and the collected dataset AAD, and the results prove that the proposed SCbSE-SMFE framework can effectively improve the accuracy and distinguish similar activities better. The proposed aggressive activity detection method based on threshold can simplify the model and improve the recognition speed while ensuring the recognition accuracy.																	1868-5137	1868-5145															10.1007/s12652-020-02526-6		OCT 2020											
J								A comprehensive survey on machine learning approaches for dynamic spectrum access in cognitive radio networks	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Cognitive Radio; dynamic spectrum management; heterogeneous networks; intelligent techniques; machine learning	ARTIFICIAL-INTELLIGENCE; SELF-ORGANIZATION; BAYESIAN-APPROACH; WIRELESS NETWORKS; ENERGY EFFICIENCY; CHANNEL SELECTION; NEURAL-NETWORK; FUZZY-LOGIC; OPTIMIZATION; REINFORCEMENT	Due to exponential growth in demand for radio spectrum for wireless communication networking, the radio spectrum has become over-crowded. The fixed spectrum allocation policy of the radio spectrum leads to inefficient utilisation of the available spectrum, which diverted the attention of researchers towards different intelligent techniques to access the spectrum dynamically and efficiently. The concept of Cognitive Radio (CR) has been considered as a promising technology to solve the problem of spectrum scarcity through the utilisation of various unutilised spectrum bands. In a future network deployment, multiple radio access networks may coexist having different characteristics. Hence, it becomes a challenge for CR networks to select the optimal network out of available networks. For efficient realisation, CRs requires intelligent spectrum management techniques for Dynamic Spectrum Management (DSM). Till now, there does not exist a literature survey that addresses the spectrum management with machine learning techniques in an intelligent manner. Hence, this paper presents the detailed classification and comprehensive survey of various machine learning techniques for intelligent spectrum management with their paradigms of optimisation for cognitive radio networks. The paper also provides new directions and open issues for the research community to work further in CR networks.																	0952-813X	1362-3079															10.1080/0952813X.2020.1818291		OCT 2020											
J								A fuzzy nonlinear univariate regression model with exact predictors and fuzzy responses	SOFT COMPUTING										Fuzzy data; Nonlinear; Goodness-of-fit measure; Spline; Kernel fitting	ROBUST REGRESSION; NUMBERS; RANKING	In this paper, a fuzzy nonlinear univariate regression model with nonfuzzy predictors and fuzzy responses is proposed. For this purpose, both nonlinear parametric and nonparametric methods were utilized. The left and right spreads of unknown fuzzy smooth function were estimated via a popular kernel-based curve-fitting method, while the center was estimated using both parametric and nonparametric curve-fitting methods. In fact, two techniques were suggested and compared in terms of estimating the center of fuzzy smooth function: (1) nonparametric method (similar to the left and right spreads) and (2) parametric method adopted with a common nonlinear regression model called truncated spline regression. Each stage was separately estimated the unknown components were addressed via the conventional statistical regression methods. The proposed method managed to provide a simple and fast estimation/prediction approach for the fuzzy univariate regression analysis for any types ofLR-fuzzy numbers. Some common goodness-of-fit criteria were also employed to evaluate the performance of the proposed method. The effectiveness of the developed method was further illustrated through three numerical examples including a simulation study based on a common kernel. The proposed method was also compared with several common fuzzy linear/nonlinear regression models. The numerical evaluations indicated that the proposed parametric method for centers exhibited more accurate results as compared with the nonparametric method.																	1432-7643	1433-7479															10.1007/s00500-020-05375-9		OCT 2020											
J								Categorical structures of soft groups	SOFT COMPUTING										Soft group category; Soft monomorphism; Soft epimorphism; Soft isomorphism	SETS	In the current paper, the category of soft groups and soft group homomorphisms is constructed and it is proved that this structure satisfies the category conditions. Also, algebraic properties of some types of soft group morphisms are obtained. Finally, an application is presented as 'cube' of soft groups and soft group homomorphisms.																	1432-7643	1433-7479															10.1007/s00500-020-05362-0		OCT 2020											
J								A novel multi-objective optimization algorithm for the integrated scheduling of flexible job shops considering preventive maintenance activities and transportation processes	SOFT COMPUTING										Flexible job shop scheduling problem; Preventive maintenance activities; Transportation process; Multi-objective optimization; Multi-region division sampling strategy	BEE COLONY ALGORITHM; MATHEMATICAL-MODELS; MACHINES; SYSTEM; TIMES	Most production scheduling problems, including standard flexible job shop scheduling problems, assume that machines are continuously available. However, in most cases, due to preventive maintenance activities, machines may not be available for a certain time. Meanwhile, in the entire workshop production process, the transportation process of workpieces cannot be ignored. Therefore, the impact of transportation on the production planning should be considered in the scheduling process. To consider both preventive maintenance and transportation processes in the flexible job shop scheduling problem, this paper proposes a flexible job shop scheduling problem considering preventive maintenance activities and transportation processes and establishes a multi-objective flexible job shop scheduling model optimizing the total energy consumption and total makespan. Furthermore, a multi-region division sampling strategy-based multi-objective optimization algorithm integrated with a genetic algorithm and a differential evolution algorithm (MDSS-MOGA-DE) is proposed to solve the model. In the proposed algorithm, a multi-region division sampling strategy and two evaluation functions are utilized to improve the diversity of solutions. In addition, this paper combines a genetic operation and a differential operation to further enhance the search ability of the algorithm. The validity of the algorithm is verified by a real case. The computational results reveal that the proposed model and algorithm obtain appropriate results and have the potential to be applied to other similar problems.																	1432-7643	1433-7479															10.1007/s00500-020-05347-z		OCT 2020											
J								An integrated SMED-fuzzy FMEA model for reducing setup time	JOURNAL OF INTELLIGENT MANUFACTURING										SMED; Fuzzy FMEA; Reducing setup time; Lean production	RISK-EVALUATION; IMPROVEMENT; SYSTEM; REDUCTION	Today, the companies apply lean or customized production methods, which enable the production of different kinds of products in small quantities, to meet different customer demands. But, the increase in the product variety leads to an increase in the number of setups and thus production time. The companies aim to reduce the setup time by improving activities and by eliminating the problems causing extending setup time. Single minute exchange of die (SMED) method is the most common setup method that makes it possible to perform equipment setup operations in fewer than 10 min, i.e. several minutes expressed by a single digit. It is possible to further reduce setup times by integrating quality tools and methods into the SMED method. In this study, it is developed a novel SMED model that integrating the traditional SMED and fuzzy failure modes and effects analysis (fuzzy-FMEA) methods. Fuzzy FMEA method is used to prevent problems causing further extending setup time on setup activities. A new operation worksheet, "Setup Observation and Analysis Form" that leads the analyst in during the investigation of the machine and its set-up process, is also designed. The new approach is applied to set up a plastic injection mold for a pen manufacturing company. The setup time is reduced from 71.32 to 36.97 min, achieved a 48% improvement.																	0956-5515	1572-8145															10.1007/s10845-020-01675-x		OCT 2020											
J								A self-organized approach for scheduling semiconductor manufacturing systems	JOURNAL OF INTELLIGENT MANUFACTURING										Semiconductor manufacturing; Dynamic dispatching rule; Self-organization	CLUSTER TOOLS; TIME; ALGORITHM	In semiconductor manufacturing industry, traditional scheduling rules are not conducive to improving production capacity to autonomously adjust based on real-time status. To fill this gap, this study proposes a dynamic dispatching rule based on self-organization (DDRSO) to autogenerate optimal scheduling scheme through mechanisms of interaction, coordination and competition. Besides, an extended DDRSO is proposed to further consider hot lots and transient dynamic bottlenecks. Both DDRSO and E-DDRSO are designed from three aspects: role definition of self-organization units, negotiation mechanism among self-organization units, and decision methods. This research adopts a benchmark industrial manufacturing system to illustrate the availability of the proposed approach. Compared with heuristic dispatching strategies, DDRSO achieves improvement on MOV, TH and ODR by 4.9%, 9.06% and 20.23%, respectively. Meanwhile, E-DDRSO performs better than DDRSO under all workload levels. In addition, compared with a flexible dispatching method BPSO-SVM, E-DDRSO also obtain better performances, especially improvement on CT by 16.51%.																	0956-5515	1572-8145															10.1007/s10845-020-01678-8		OCT 2020											
J								Hashtag recommendation for short social media texts using word-embeddings and external knowledge	KNOWLEDGE AND INFORMATION SYSTEMS										Hashtag recommendation; Social media analysis; Information extraction and filtering; Semantic knowledge bases	TWITTER; IMPACT	With the rapid growth of Twitter in recent years, there has been a tremendous increase in the number of tweets generated by users. Twitter allows users to make use of hashtags to facilitate effective categorization and retrieval of tweets. Despite the usefulness of hashtags, a major fraction of tweets do not contain hashtags. Several methods have been proposed to recommend hashtags based on lexical and topical features of tweets. However, semantic features and data sparsity in tweet representation have rarely been addressed by existing methods. In this paper, we propose a novel method for hashtag recommendation that resolves the data sparseness problem by exploiting the most relevant tweet information from external knowledge sources. In addition to lexical features and topical features, the proposed method incorporates the semantic features based on word-embeddings and user influence feature based on users' influential position. To gain the advantage of various hashtag recommendation methods based on different features, our proposed method aggregates these methods using learning-to-rank and generates top-ranked hashtags. Experimental results show that the proposed method significantly outperforms the current state-of-the-art methods.																	0219-1377	0219-3116															10.1007/s10115-020-01515-7		OCT 2020											
J								Evolutionary Denoising-Based Machine Learning for Detecting Knee Disorders	NEURAL PROCESSING LETTERS										Diagnosis; Decision support system; Machine learning; Support vector machine; Knee disorder; Genetic algorithm	GROUND REACTION FORCE; GENETIC ALGORITHM; ADDUCTION MOMENT; OSTEOARTHRITIS; NETWORK	Surface electromyography (sEMG) is a non-invasive tool that can aid physiological assessment of knee disorders towards clinical interventions. Machine Learning (ML) is widely used to classify sEMG data to help with early detection of knee disorders; however, the inherent noise and the high non-linearity of sEMG signals make pattern recognition a challenging task. This study aims to partly overcome these challenges with existing ML-based classifiers by denoising sEMG signals further via an innovative two-fold evolutionary approach. A novel Genetic Algorithm-based denoising approach is applied to sEMG data to decrease the search space for pattern-related classification. Thereafter, the proposed denoising technique is coupled with an ML-based classifier to improve the discrimination between physiological and pathophysiological knee functions from sEMG data by optimising its hyperparameters too. Thus, the novel evolutionary approach serves two purposes. Firstly, it further reduces noise in sEMG signals via a new GA-based denoising technique to concurrently maximise mutual information and minimise entropy; secondly, it also enables the optimisation of the classifier's hyperparameters. The classification performance of the resulting hybrid algorithm was validated using sEMG data on 144 subjects (67 patients with knee disorders, 77 healthy subjects) and was found higher (ACC = 99.57%, 95% CI: 99.47-99.66; AUC = 1, 95% CI: 0.98-1) than that of similar ML algorithms and published studies. The hybrid algorithm achieved the highest classification performance by leveraging an evolutionary approach for effective denoising and hyperparameter optimisation, whilst retaining the lowest computational cost. Thus, the proposed evolutionary denoising ML-based classifier is deemed an accurate and reliable decision support system to aid the detection of knee disorders.																	1370-4621	1573-773X															10.1007/s11063-020-10361-1		OCT 2020											
J								Topic identification oftext-basedexpert stock comments using multi-level information fusion	EXPERT SYSTEMS										belief value; multi-level information fusion; network stock comments; text classification	PREDICTION; NEWS; MODEL	Stock investment is an important mode of asset allocation and a crucial means of financial management. How to grasp the movement of stock price and predict its trend have been the focus of investors and investment companies. Since expert stock comments contain abundant essential information for investment decisions, how to identify the topic of expert stock comments with high precision and efficiency is an important research topic. However, the existing methods usually employ single feature selection strategies for topic identification of stock comments, which may lead to low accuracy. Thus, to deal with this limitation, we propose a multi-level information fusion method to construct a topic identification system of stock comments. Specifically, wefirstlyfuse various complementary feature selection methods via a multi-view learning framework which can comprehensively represent text-based topics.In addition, regarding the decision process, we propose a fusion strategy based on belief value which can further improve the classification performance. The experimental results indicate that the proposed multi-level information fusion method is not only superior to other methods in terms of classification, it is also able to accurately capture topics of expert stock comments.																	0266-4720	1468-0394															10.1111/exsy.12641		OCT 2020											
J								Effects of age and task difficulty on postural sway, variability and complexity	ADAPTIVE BEHAVIOR										Control mechanisms; postural challenges; stability; complexity	OLDER-ADULTS; BALANCE; YOUNG; PERFORMANCE; STABILITY; DYNAMICS; CHILDREN; FRAILTY; INDEX; FALLS	This study aimed to examine the effects of age and the task difficulty on postural sway, variability and complexity. The participants were 90 able-bodied individuals including children (n = 39; age: 5.89 +/- 0.94 years), young adults (n = 30; age: 23.23 +/- 1.61 years) and older adults (n = 21; age: 64.59 +/- 5.24 years) who took part in different balance tasks that had different levels of cognitive and physical challenges. The main dependent variables were postural sway area, postural variability and postural complexity. The participants stood on a standard force plate for 10 s in each task condition, and the centre of pressure displacement was collected at 100-Hz sampling frequency. The results of this study showed that children and older adults, in the more difficult tasks, had greater sway area and complexity and less postural variability. In addition, there was a linear trend in the stability measures as the difficulty of the task was increased. In conclusion, special populations, such as children and older adults, were more sensitive to the balance changes and used active control mechanisms to minimise the risk of losing balance in more challenging conditions.																	1059-7123	1741-2633														1059712320963974	10.1177/1059712320963974		OCT 2020											
J								Artificial superintelligence and its limits: why AlphaZero cannot become a general agent	AI & SOCIETY										Artificial general intelligence; Superintelligence; Agency; The belief; desire model; Intentional action; Existential risk		An intelligent machine surpassing human intelligence across a wide set of skills has been proposed as a possible existential catastrophe (i.e., an event comparable in value to that of human extinction). Among those concerned about existential risk related to artificial intelligence (AI), it is common to assume that AI will not only be very intelligent, but also be a general agent (i.e., an agent capable of action in many different contexts). This article explores the characteristics of machine agency, and what it would mean for a machine to become a general agent. In particular, it does so by articulating some important differences between belief and desire in the context of machine agency. One such difference is that while an agent can by itself acquire new beliefs through learning, desires need to be derived from preexisting desires or acquired with the help of an external influence. Such influence could be a human programmer or natural selection. We argue that to become a general agent, a machine needsproductivedesires, or desires that can direct behavior across multiple contexts. However, productive desires cannotsui generisbe derived from non-productive desires. Thus, even though general agency in AI could in principle be created by human agents, general agency cannot be spontaneously produced by a non-general AI agent through an endogenous process (i.e. self-improvement). In conclusion, we argue that a common AI scenario, where general agency suddenly emerges in a non-general agent AI, such as DeepMind's superintelligent board game AI AlphaZero, is not plausible.																	0951-5666	1435-5655															10.1007/s00146-020-01070-3		OCT 2020											
J								A Neutrosophic VIKOR Method-Based Decision-Making with an Improved Distance Measure and Score Function: Case Study of Selection for Renewable Energy Alternatives	COGNITIVE COMPUTATION										Neutrosophic set; VIKOR; Multiple-criteria decision-making; Renewable energy alternative selection		In this study, a new score and distance function which provides more accurate results than the studies in the literature and considers the decision maker's cognition has been proposed. In order to show the effects of the new score function coefficients on neutrosophic numbers, a new representation named "4D Plot" has been proposed. VIKOR (Vlse Kriterijumska Optimizacija Kompromisno Resenje which means multicriteria optimization and compromise solution in Serbian) method, which is based on the creation of a conciliatory solution within the framework of alternatives and evaluation criteria, provides highly effective results in multiple criteria decision-making problems. Neutrosophic numbers play an active role in facilitating the decision-making process in many studies. In this context, extension of the classical VIKOR method is introduced in detail in this study. The framework of the extended neutrosophic VIKOR method has been established. The proposed method has been applied to the renewable energy alternative selection problem in Turkey and compared with different methods and different scenarios. It has been shown that the proposed method gives more effective and accurate results. Furthermore, the sensitivity and accuracy of the study are enhanced by a sensitivity analysis.We have extended the classical VIKOR method for SVN numbers with a newly generalized score function and a distance measure. As a novel and newly presentation named "The 4D Plot" has been developed and applied in this paper to SVN numbers for presenting the effect of the new score function. The renewable energy alternative selection was made in Turkey with the proposed method by the opinions of the experts and the ranking of the alternatives was obtained. A sensitivity analysis has been made to show the accuracy and validity of the study. With the proposed distance measure and score function, it is shown that more accurate results can be found in multiple criteria decision-making problems. The model will provide solutions for many soft computing problems such as engineering and economy by finding more effective and accurate results in the neutrosophic number space according to the preference of decision makers. As a future work, the presented functions and framework can be adapted to different MCDM problems.																	1866-9956	1866-9964															10.1007/s12559-020-09765-x		OCT 2020											
J								Information Processing Capacity of Spin-Based Quantum Reservoir Computing Systems	COGNITIVE COMPUTATION										Machine learning; Quantum reservoir computing; Information Processing Capacity		The dynamical behavior of complex quantum systems can be harnessed for information processing. With this aim, quantum reservoir computing (QRC) with Ising spin networks was recently introduced as a quantum version of classical reservoir computing. In turn, reservoir computing is a neuro-inspired machine learning technique that consists in exploiting dynamical systems to solve nonlinear and temporal tasks. We characterize the performance of the spin-based QRC model with the Information Processing Capacity (IPC), which allows to quantify the computational capabilities of a dynamical system beyond specific tasks. The influence on the IPC of the input injection frequency, time multiplexing, and different measured observables encompassing local spin measurements as well as correlations is addressed. We find conditions for an optimum input driving and provide different alternatives for the choice of the output variables used for the readout. This work establishes a clear picture of the computational capabilities of a quantum network of spins for reservoir computing. Our results pave the way to future research on QRC both from the theoretical and experimental points of view.																	1866-9956	1866-9964															10.1007/s12559-020-09772-y		OCT 2020											
J								Intelligent IoT-based large-scale inverse planning system considering postmodulation factors	COMPLEX & INTELLIGENT SYSTEMS										IoT-based planning; Fluence map; Gantry angle; Tongue-groove effects; Tongue-groove effects; Partial differential equation; Total monitor units	MODULATED RADIATION-THERAPY; IMRT; CANCER; VOLUME; STRATEGY; HEAD	The model and algorithm of intensity-modulated radiotherapy (IMRT) are updated increasingly quickly, but the hardware upgrade of primary hospitals often lags behind. The new generation of intelligent precise radiotherapy platforms provides users with intelligent medical consortium services using big data, artificial intelligence and industrial Internet of Things technology. This technology can ensure that under the real-time guidance of a professional medical consortium, primary hospitals can realize rapid large-scale reverse planning design and can more accurately consider many factors of postprocessing. Although large-scale healthcare systems, such as volumetric-modulated arc therapy and other accurate radiotherapy technologies, have developed rapidly, the development of step-and-shoot-mode IMRT technology is still very important for developing countries. For software, in addition to the conformity of the dose distribution, the modulation speed, convenience and stability of the later dose delivery should also be considered in inverse planning. Therefore, this paper analyzes the main problems in conventional IMRT inverse planning, including the smoothing of the fluence map, the selection of the gantry angle and the dose leakage of tongue-groove effects. To address these issues, a novel Intelligent IoT-based large-scale inverse planning strategy with the key factors of the postmodulation is developed, and a detailed flow chart is also provided. The scheme consists of two steps. The first step is to obtain a relatively optimal combination of gantry angles by considering the dose distribution requirements and constraints and the modulation requirements and constraints. The second step is to optimize the intensity map, to smooth the map based on prior knowledge according to the determined angles, and to obtain the final modulation scheme according to the relevant objectives and constraints of the map decomposition (leaf sequencing). In an experiment, we calculate and validate the clinical head and neck case. Because of the special gantry angle selection, the angle combination is optimized from the initial equivalent distribution to adapt to the target area and protect the nontarget area. The value of the objective function varies greatly after the optimization, especially in the target area, and the target value decreases by approximately 10%. On this basis, we smooth the fluence map by a partial differential equation with prior knowledge and a minimization of the total number of monitor units. It is also shown from the objective function value that the target value is essentially unchanged for the target area, while for the nontarget area, the value decreases by 16%, which is very impressive.																	2199-4536	2198-6053															10.1007/s40747-020-00207-7		OCT 2020											
J								A survey of community detection methods in multilayer networks	DATA MINING AND KNOWLEDGE DISCOVERY										Community detection; Multilayer network; Temporal network; Multiplex network; Multilevel network	NONNEGATIVE MATRIX FACTORIZATION; SPREADING PROCESSES; FUNCTIONAL MODULES; CENTRALITY; DYNAMICS; IDENTIFICATION; BRAINMAP; INTERNET; MODELS; GRAPHS	Community detection is one of the most popular researches in a variety of complex systems, ranging from biology to sociology. In recent years, there's an increasing focus on the rapid development of more complicated networks, namely multilayer networks. Communities in a single-layer network are groups of nodes that are more strongly connected among themselves than the others, while in multilayer networks, a group of well-connected nodes are shared in multiple layers. Most traditional algorithms can rarely perform well on a multilayer network without modifications. Thus, in this paper, we offer overall comparisons of existing works and analyze several representative algorithms, providing a comprehensive understanding of community detection methods in multilayer networks. The comparison results indicate that the promoting of algorithm efficiency and the extending for general multilayer networks are also expected in the forthcoming studies.																	1384-5810	1573-756X															10.1007/s10618-020-00716-6		OCT 2020											
J								An algorithm for solving FEVM problem based on SPSO algorithm	EVOLUTIONARY INTELLIGENCE										SPSO; Fuzzy expected value model; Multi-layer BP artificial neural networks; Fuzzy simulation	PARTICLE SWARM OPTIMIZATION	To solve fuzzy expected value model problem that widely exists in fuzzy programming field, a new hybrid intelligence algorithm based on Stochastic Particle Swarm Optimization (SPSO) algorithm is put forward in this article. Fuzzy simulation is used to get training samples for multi-layer (BP) Back Propagation artificial neural networks and multi-layer BP artificial neural networks is used to approximate fuzzy expected value function when SPSO algorithm is used to find the optimal value, the trained multi-layer BP artificial neural networks is used to calculate fuzzy expected function's fitness value and detail steps are designed. Compared with the hybrid intelligence algorithm based on the classical Genetic Algorithm, the proposed algorithm overcomes some defects, such as taking a long time, computing complexity, easy being immersed in local extremum. The results are justified with the help of two numerical illustrations for fuzzy expected value model problem, the effectiveness of our new approach is demonstrated and it has determinate practical value.																	1864-5909	1864-5917															10.1007/s12065-020-00483-9		OCT 2020											
J								Blockchain-based secure information sharing for supply chain management: Optimization assisted data sanitization process	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										blockchain; crosspoint-based update; data sanitization and restoration; optimal key selection; privacy preservation; supply chain network	TECHNOLOGY; LOGISTICS; SYSTEM	Currently, the furious competitiveness in global markets and speedy improvements in information technology lead to shorter product life cycles, lesser transportation capabilities, and increased demands as well. In most business scenarios, the supply chain network is becoming one of the most vital focusing areas. In the supply chain network, blockchain technology is a promising solution for secure information sharing. However, it is a bit critical in maintaining security at each level of the blockchain and hence the public-private-key cryptography is more commonly preferred. This study intends to construct a new privacy preservation model in the field of supply chain networks based on blockchain technology by undergoing three major phases, namely, (a) data sanitization, (b) key generation, and (c) restoration. Further, the sensitive fields in the original data are selected during the data sanitization phase, and in the key generation phase; the optimal key is generated to hide the selected sensitive fields. The hidden data with the secured key is transferred from the source (manufacturer) to destination (vendor) in the supply chain network via the blockchain. The restoration process takes place in the receiver side with the help of the same key. Among all these data flow methods, the optimal key selection is the critical issue that needs to be overridden to make the data transmission secured. As a novelty, a new optimization algorithm referred to as Whale with New Crosspoint-based Update (WNU), which is the advanced version of Whale Optimization Algorithm (WOA), is developed here to select the optimal key. Finally, the proposed WNU model is analyzed in terms of Hiding Failure (HF) rate, Information Preservation (IP) Rate, and False Rule generation (FR), and Degree of Modification (DM). The proposed secured information sharing in supply chain management (SCM) with blockchain technology will be validated by comparing it over the traditional models in terms of security as well.																	0884-8173	1098-111X															10.1002/int.22299		OCT 2020											
J								Evaluating time series forecasting models: an empirical study on performance estimation methods	MACHINE LEARNING										Performance estimation; Model selection; Cross validation; Time series; Forecasting	CROSS-VALIDATION; SELECTION	Performance estimation aims at estimating the loss that a predictive model will incur on unseen data. This process is a fundamental stage in any machine learning project. In this paper we study the application of these methods to time series forecasting tasks. For independent and identically distributed data the most common approach is cross-validation. However, the dependency among observations in time series raises some caveats about the most appropriate way to estimate performance in this type of data. Currently, there is no consensual approach. We contribute to the literature by presenting an extensive empirical study which compares different performance estimation methods for time series forecasting tasks. These methods include variants of cross-validation, out-of-sample (holdout), and prequential approaches. Two case studies are analysed: One with 174 real-world time series and another with three synthetic time series. Results show noticeable differences in the performance estimation methods in the two scenarios. In particular, empirical experiments suggest that blocked cross-validation can be applied to stationary time series. However, when the time series are non-stationary, the most accurate estimates are produced by out-of-sample methods, particularly the holdout approach repeated in multiple testing periods.																	0885-6125	1573-0565															10.1007/s10994-020-05910-7		OCT 2020											
J								Multi-label feature ranking with ensemble methods	MACHINE LEARNING										Feature ranking; Multi-label classification; Ensemble-based methods; Predictive clustering trees	CLASSIFICATION; RELIEFF	In this paper, we propose three ensemble-based feature ranking scores for multi-label classification (MLC), which is a generalisation of multi-class classification where the classes are not mutually exclusive. Each of the scores (Symbolic, Genie3 and Random forest) can be computed from three different ensembles of predictive clustering trees:Bagging,Random forestandExtra trees. We extensively evaluate the proposed scores on 24 benchmark MLC problems, using 15 standard MLC evaluation measures. We determine the ranking quality saturation points in terms of the ensemble sizes, for each ranking-ensemble pair, and show that quality rankings can be computed really efficiently (typically 10 or 50 trees suffice). We also show that the proposed feature rankings are relevant and determine the most appropriate ensemble method for every feature ranking score. We empirically prove that the proposed feature ranking scores outperform current state-of-the-art methods in the quality of the rankings (for the majority of the evaluation measures), and in time efficiency. Finally, we determine the best performing feature ranking scores. Taking into account the quality of the rankings first and-in the case of ties-time efficiency, we identify the Genie3 feature ranking score as the optimal one.																	0885-6125	1573-0565															10.1007/s10994-020-05908-1		OCT 2020											
J								An efficient memetic genetic programming framework for symbolic regression	MEMETIC COMPUTING										Evolutionary computation; Genetic programming; Symbolic regression	OPTIMIZATION; ALGORITHM; CROSSOVER; SELECTION; MUTATION	Background Symbolic regression is one of the most common applications of genetic programming (GP), which is a popular evolutionary algorithm in automatic computer program generation. Despite existing success of GP on symbolic regression, the accuracy and efficiency of GP can still be improved especially on complicated symbolic regression problems, enabling GP to be applied to more fields. Purpose This paper proposes a novel memetic GP framework to improve the accuracy and search efficiency of GP on complicated symbolic regression problems. The proposed framework consists of two components: feature construction and feature combination. The first component focuses on constructing diverse features. The second component aims to filter redundant features and linearly combines these independent features. Methods The first component (feature construction) focuses on constructing polynomial features derived from polynomial functions, and evolves features by a GP solver. In addition, a gradient-based nonlinear least squares algorithm named Levenberg-Marquardt (LM) is embedded in the second component (feature combination) to locally adjust the weights of independent features. A filtering mechanism is put forward to discard redundant features in the second component. Hence, the polynomial features and evolved features can work together in the framework to improve the performance of GP. Results Experimental results demonstrate that the proposed framework offers enhanced performance compared with several state-of-the-art algorithms in terms of accuracy and search efficiency on nine benchmark regression problems and three real-world regression problems. Conclusion In this study, a novel memetic genetic programming framework is proposed to improve the performance of GP on symbolic regression. Experimental results demonstrate that the proposed framework can improve the accuracy and search efficiency of GP on complicated symbolic regression problems compared with four state-of-the-art algorithms.																	1865-9284	1865-9292															10.1007/s12293-020-00311-8		OCT 2020											
J								Medical image fusion method based on dense block and deep convolutional generative adversarial network	NEURAL COMPUTING & APPLICATIONS										Medical image fusion; Deep convolutional GAN; Dense block; Encoder-decoder; Loss function	QUALITY ASSESSMENT; INFORMATION; TRANSFORM	Medical image fusion techniques can further improve the accuracy and time efficiency of clinical diagnosis by obtaining comprehensive salient features and detail information from medical images of different modalities. We propose a novel medical image fusion algorithm based on deep convolutional generative adversarial network and dense block models, which is used to generate fusion images with rich information. Specifically, this network architecture integrates two modules: an image generator module based on dense block and encoder-decoder and a discriminator module. In this paper, we use the encoder network to extract the image features, process the features using fusion rule based on the Lmax norm, and use it as the input of the decoder network to obtain the final fusion image. This method can overcome the weaknesses of the active layer measurement by manual design in the traditional methods and can process the information of the intermediate layer according to the dense blocks to avoid the loss of information. Besides, this paper uses detail loss and structural similarity loss to construct the loss function, which is used to improve the extraction ability of target information and edge detail information related to images. Experiments on the public clinical diagnostic medical image dataset show that the proposed algorithm not only has excellent detail preserve characteristics but also can suppress the artificial effects. The experiment results are better than other comparison methods in different types of evaluation.																	0941-0643	1433-3058															10.1007/s00521-020-05421-5		OCT 2020											
J								Pedestrian Detection Based on Light-Weighted Separable Convolution for Advanced Driver Assistance Systems	NEURAL PROCESSING LETTERS										Pedestrian detection; Advanced driver assistance systems; Convolutional neural networks; Lightweight convolution blocks		The growth in the number of vehicles in the world makes it hard to safely share the environment with pedestrians. Pedestrian's safety is an important task that needs to be granted in the traffic environment. New cars are equipped with advanced driver assistance systems (ADAS) with a variety of applications. Pedestrian detection application is one of the most important applications for an ADAS that needs to be enhanced. In this paper, we propose a pedestrian detection system to be implemented in an ADAS. The proposed system is based on convolutional neural networks thanks to its performance when solving computer vision applications. On the other side, the proposed system ensures real-time processing and high detection performance. The proposed system will be designed by tacking the advantage of building lightweight convolution blocks and model compression techniques to ensure an embedded implementation. Those blocks will guarantee high precision and fast processing speed. To train and evaluate the proposed system, we used the Caltech dataset. The evaluation of the proposed system resulted in 87% of mean average precision and an inference speed of 35 frames per second.																	1370-4621	1573-773X															10.1007/s11063-020-10367-9		OCT 2020											
J								Adaptive Synchronization of Complex Dynamical Networks via Distributed Pinning Impulsive Control	NEURAL PROCESSING LETTERS										Exponential synchronization; Distributed impulsive control; Pinning adaptive control; Time-varying delay; Complex networks	NEURAL-NETWORKS; STABILITY	In this paper, exponential synchronization of a class of nonlinearly coupled complex dynamical networks with time-varying delay is investigated. A novel distributed controller combined with pinning impulsive method is designed by selecting the systems with largest norms of errors to be controlled at every impulsive instant. By introducing the adaptive control protocol into the negative feedback controller, suitable control gains are obtained and therefore, the control cost is efficiently saved. Based on the Lyapunov stability theory and some mathematical techniques, some novel leader-following synchronization criteria are derived. Furthermore, with consideration of time-varying impulsive effects, the obtained results are extended to a more complicated situation. Finally, two numerical examples are performed to illustrate the effectiveness of the theoretical analysis.																	1370-4621	1573-773X															10.1007/s11063-020-10373-x		OCT 2020											
J								A Convolutional Neural Network Framework for Accurate Skin Cancer Detection	NEURAL PROCESSING LETTERS										Image processing; Deep learning; Classification; Skin cancer; Melanoma	IMAGES	Skin diseases have become a challenge in medical diagnosis due to visual similarities. Although melanoma is the best-known type of skin cancer, there are other pathologies that are the cause of many death in recent years. The lack of large datasets is one of the main difficulties to develop a reliable automatic classification system. This paper presents a deep learning framework for skin cancer detection. Transfer learning was applied to five state-of-art convolutional neural networks to create both a plain and a hierarchical (with 2 levels) classifiers that are capable to distinguish between seven types of moles. The HAM10000 dataset, a large collection of dermatoscopic images, were used for experiments, with the help of data augmentation techniques to improve performance. Results demonstrate that the DenseNet201 network is suitable for this task, achieving high classification accuracies and F-measures with lower false negatives. The plain model performed better than the 2-levels model, although the first level, i.e. a binary classification, between nevi and non-nevi yielded the best outcomes.																	1370-4621	1573-773X															10.1007/s11063-020-10364-y		OCT 2020											
J								Triangular approximation of intuitionistic fuzzy numbers on multi-criteria decision making problem	SOFT COMPUTING										Intuitionistic fuzzy number; Generalized parabolic intuitionistic fuzzy numbers; Distance function; Weighting function	NEAREST INTERVAL APPROXIMATION; TRAPEZOIDAL APPROXIMATIONS; GENERAL CONDITION; ALGORITHMS; UNIQUENESS; EXISTENCE; RANKING	Most of the engineering applications depend on incomplete and imprecise information which are represented by nonlinear mathematical functions for defining membership and nonmembership functions in intuitionistic fuzzy setup. Shuyang Li and Hongxing Li have studied the approximation of conventional intuitionistic fuzzy numbers in which the membership function is bounded and nonmembership function is unbounded in nature. The concepts of intuitionistic fuzzy sets (IFSs) and interval valued intuitionistic fuzzy sets (IVIFSs) have bounded membership function and bounded nonmembership function. As the generalization of both IFSs and IVIFSs, the notion of intuitionistic fuzzy numbers of new type with bounded membership function and bounded nonmembership function has been introduced by Lakshmana et al. In this paper, the procedure for weighted triangular approximation of intuitionistic fuzzy number of new type is provided with some suitable illustrations. Moreover, some useful properties of the triangular approximation on intuitionistic fuzzy numbers have also been discussed.																	1432-7643	1433-7479															10.1007/s00500-020-05346-0		OCT 2020											
J								An evidence combination approach based on fuzzy discounting	SOFT COMPUTING										Evidence theory; Evidence combination; Fuzzy discounting; Evidence distance; Discriminability measure		In evidence theory, Dempster's rule of combination is the most commonly applied method to aggregate bodies of evidence obtained from different sources to make a decision. However, when multiple independent bodies of evidence with conflict are aggregated by Dempster's rule of combination, the counterintuitive results can be generated. Evidence discounting is proved to be an efficient way to eliminate the counterintuitive combination results. Following the discounting ideas, a new combination approach based on fuzzy discounting is put forward. Both the conflict between bodies of evidence and the uncertainty of a body of evidence itself are taken into account to determine the discounting factors. Jousselme's evidence distance is used to represent conflict between bodies of evidence, and discriminability measure is defined to represent uncertainty of a body of evidence itself. Consider that both the evidence distance and the discriminability measure are semantically fuzzy. Thus, fuzzy membership functions are defined to describe both of them, and a fuzzy reasoning rule base is constructed to derive the discounting factors. Numerical examples indicate that this new combination approach proposed can achieve fast convergence speed and is robust to disturbing evidences, i.e., it is an effective method to process conflicting evidences combination.																	1432-7643	1433-7479															10.1007/s00500-020-05359-9		OCT 2020											
J								An extended ACO-based mobile sink path determination in wireless sensor networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Wireless sensor networks; Mobile sink path determination; Ant colony optimization; Network lifetime; Energy-hole problem	ROUTING ALGORITHM; LIFETIME	In wireless sensor networks (WSNs), a mobile sink accumulate the data instead of routing directly to the sink to avoid the hotspot problem. In this process, it traverses a predetermined path by visiting a set of nodes called the rendezvous point (RP), and all the non-rendezvous points can transmit their data to the closest RP. Identifying the best collection of RPs and determining the mobile sink traveling path will decrease data loss and improve network performance. However, choosing a set of RPs and the route between them is a challenging task. It is more complicated in the event-driven applications due to the uneven data rate of SNs. In this context, we propose an extended ant colony optimization (ACO)-based mobile sink path construction for event-driven WSNs. In this, the best set of the RPs and the efficient mobile sink traveling path between them is determined. In addition to this, the RPs re-selection mechanism also adopted for balancing the energy between the nodes. After that, the virtual RPs are introduced to minimize the data transmissions between the sensor nodes and RPs. This process will improve WSNs' performance in terms of reducing data losses while increasing network lifetime. The improved performance of the extended ACO-MSPD over existing is confirmed through simulation tests.																	1868-5137	1868-5145															10.1007/s12652-020-02595-7		OCT 2020											
J								Predicting the energy consumption in software defined wireless sensor networks: a probabilistic Markov model approach	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Software defined wireless sensor networks; Energy consumption prediction; Markov model; Performance evaluation; Internet of things	ALGORITHM; IOT	The smart world is connecting all universe more than ever thought possible, benefiting from the significant advances of the Internet of Things (IoT) applications using wireless sensor networks (WSN) as the core technology. A challenging issue in the IoT paradigm is the heterogeneity in different parts of the network. The network developers need to use resources belonging to different platforms for their applications, and the software defined network (SDN) approach is a mainly considered solution. In this paper, a software defined wireless sensor network (SDWSN) with an energy predictor model (SDWSN-EPM) based on the Markov probabilistic model is proposed to reduce the energy consumption and the network latency. The energy consumption rate (ECR) of the sensor nodes is modeled using the Markov model and the states of the sensor nodes. The ECR is used by the SDN controller to predict the residual energy level of the nodes and consequently, the energy consumption of the network. The cumulative distribution functions (CDF) of the delay, power consumption, and the network lifetime in both SDWSN and SDWSN-EPM schemes are compared. The results confirm that the SDWSN-EPM model significantly improves the performance of the sensor networks.																	1868-5137	1868-5145															10.1007/s12652-020-02599-3		OCT 2020											
J								Grasshopper optimization algorithm tuned maximum power point tracking for solar photovoltaic systems	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Photovoltaic systems; Maximum power point tracking; Perturb and observe; Particle swarm optimization; Grasshopper optimization algorithm	PARTICLE SWARM OPTIMIZATION; MPPT ALGORITHM; OSCILLATION; PSO	Solar Photovoltaic (PV) system is an excellent renewable energy solution in today's scenario. Harvesting maximum power from the solar PV system under dynamic meteorological conditions is a challenging task. Numerous bio-inspired Maximum Power Point Tracking (MPPT) strategies have been proposed in the literature. The conventional methods of MPPT control are easy and simple to implement, but has drawbacks such as steady state oscillations and inability to track the maximum power under swiftly varying irradiances and partial shading conditions. This paper proposes a Grasshopper Optimization Algorithm (GOA) tuned MPPT technique with the objective of obtaining optimal duty cycle,D, to control a DC-DC boost converter. The efficacy of the proposed system under start up transients, line disturbances, load disturbances, servo conditions and partial shading conditions are evaluated and compared with the conventional Perturb and Observe (P&O) based MPPT and the familiar Particle Swarm Optimization (PSO) based MPPT algorithm using MATLAB Simulink platform. It is observed that the proposed GOA tuned MPPT technique gives good steady state and dynamic response compared to P&O and PSO based MPPT algorithms, verified in terms of rise time, settling time, percentage maximum overshoot, Integral Squared Error and Integral Absolute Error.																	1868-5137	1868-5145															10.1007/s12652-020-02593-9		OCT 2020											
J								Bayesian network for integrated circuit testing probe card fault diagnosis and troubleshooting to empower Industry 3.5 smart production and an empirical study	JOURNAL OF INTELLIGENT MANUFACTURING										Industry 3; 5; Bayesian network; Advanced quality control; Fault diagnosis; UNISON decision framework	CONVOLUTIONAL NEURAL-NETWORK; MANUFACTURING BIG DATA; UNISON FRAMEWORK; SEMICONDUCTOR; SYSTEM; YIELD; CLASSIFICATION; PROBABILITY; EXTRACTION; MAP	Probe card that serves as the carrier of die and the transmitter of information is an indispensable test interface for integrated circuit testing. The probe card extracts the electrical signal of chip and sends it to the prober to screen defectives. In the process of interface transmission, signal disturbance or attenuation will lead to functional errors, and the output result will be different from the expected one to the screen of selected known good dies for integrated circuit packaging and final test. If abnormal situations happen with probe cards, the engineers will eliminate potential fault causes by trial and error method according to domain knowledge and personal experience for troubleshooting. As semiconductor industry is continuously migrating with shrinking feature size, the diagnosing and troubleshooting procedure for probe card is exponentially complicated and time-consuming. To enhance data integrity of circuit probe testing, this study aims to develop a Bayesian network for probe card fault diagnosis and troubleshooting via the integrated data-driven solutions considering potential rules derived from domain knowledge and manufacturing big data to empower Industry 3.5 smart production. An empirical study is conducted in a leading semiconductor testing company for validation. The experiment results show that the proposed approach can improve one-shot probability from 0.13 to 0.36 to improve one-shot success chance in troubleshooting process. The expected shooting times for probe faults have been reduced from 11 times to 3.96 times on average to save 63.97% of fault troubleshooting efforts in testing process. The proposed approach can provide effective suggestions to shorten troubleshooting time for yield improvement and subsequent packaging cost reduction to empower flexible decision-making for smart production. The results have shown practical viability of proposed approach for Industry 3.5. Indeed, the developed solutions have been implemented in real settings.																	0956-5515	1572-8145															10.1007/s10845-020-01680-0		OCT 2020											
J								A novelx-shaped binary particle swarm optimization	SOFT COMPUTING										Binary particle swarm optimization (BPSO); Transfer function; S-shaped; V-shaped and linear transfer functions; x-shaped transfer function	ARTIFICIAL BEE COLONY; DRAGONFLY OPTIMIZATION; ALGORITHM; PSO; SELECTION; EVOLUTIONARY; FEATURES	Definitive optimization algorithms are not able to solve high-dimensional optimization problems because the search space grows exponentially with the problem size and an exhaustive search will be impractical. Therefore, approximate algorithms are applied to solve them. A category of approximate algorithms are meta-heuristic algorithms. They have shown an acceptable efficiency to solve these problems. Among them, particle swarm optimization (PSO) is one of the well-known swarm intelligence algorithms to optimize continuous problems. A transfer function is applied in this algorithm to convert the continuous search space to the binary one. The role of the transfer function in binary PSO (BPSO) is very important to enhance its performance. Several transfer functions have been proposed for BPSO such as S-shaped, V-shaped, linear and other transfer functions. However, BPSO algorithm can sometimes find local optima or show slow convergence speed in some problems because of using the velocity of PSO and these transfer functions. In this study, a novel transfer function calledx-shaped BPSO (XBPSO) is proposed to increase exploration and exploitation of BPSO in the binary search space. The transfer function uses two functions and improved rules to generate a new binary solution. The proposed method has been run on 33 benchmark instances of the 0-1 multidimensional knapsack problem (MKP), two discrete maximization functions and 23 minimization functions. The results have been compared with some well-known BPSO and discrete meta-heuristic algorithms. The results showed thatx-shaped transfer function considerably increased the solution accuracy and convergence speed in BPSO algorithm. The average error of compared algorithms on all 0-1 MKP benchmark instances indicated that XBPSO has the minimum error of 8.9%. Also, the mean absolute error (MAE) obtained by XBPSO on two discrete maximization functions is 0.45. Moreover, the proposed transfer function provides superior solutions in 18 functions from 23 minimization functions.																	1432-7643	1433-7479															10.1007/s00500-020-05360-2		OCT 2020											
J								Proportional-integral-derivative optimization algorithm for double-fed induction generator with the maximum wind power tracking technique	SOFT COMPUTING										Proportional-integral-derivative optimization algorithm; Basic mathematical optimization problems; Controller parameters optimization problems	GREY WOLF OPTIMIZER; SWARM OPTIMIZATION; FEATURE-SELECTION; POINT TRACKING; SYSTEMS; SEARCH; MPPT	This paper proposes a novel control-centric optimization algorithm, i.e., proportional-integral-derivative optimization algorithm. The proposed optimization algorithm is inspired by the conventional proportional-integral-derivative controller. The proposed optimization algorithm consists of two types of controllers, i.e., explorative controllers with variable parameters and exploitative controllers with fixed parameters. In the exploration process of the approach, multiple explorative controllers with variable parameters move toward the global optimal solution domain. In the exploitation process of the approach, multiple exploitative controllers with fixed parameters moving toward to local optimal solution. The case studies results obtained by the proposed proportional-integral-derivative optimization algorithm under a total of eight basic mathematical optimization problems and the parameters optimization problem of the double-fed induction generator with the maximum wind power tracking technique show that the proportional-integral-derivative optimization algorithm can explore and exploit the global optimal solution effectively.																	1432-7643	1433-7479															10.1007/s00500-020-05365-x		OCT 2020											
J								An ensemble framework for interpretable malicious code detection	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										feature extraction; knowledge graph; machine learning; malicious code; malware detection	ANDROID MALWARE DETECTION; NETWORKS	Malicious code is an ever-growing security threats to computer systems and networks, while malware detection provides effective defense against malicious codes. In this paper, a brief overview is presented on currently prevalent methods to detect malicious codes, including signature-based methods, behavioral-based detection and machine learning (ML) based ones. More specifically, the potentially effective malicious features are summarized and the novel methods using ML are deeply discussed. Furthermore, an ensemble interpretable framework is explored for automatic and efficient malicious code detection. Based on the knowledge graph of malware, the novel framework inclines to achieve robust malware detection even confronted with unseen malicious codes. Finally, both advantages and disadvantages are discussed and experimental results are outlined to verify the effectiveness of the novel methods.																	0884-8173	1098-111X															10.1002/int.22310		OCT 2020											
J								Integration of mechanistic immunological knowledge into a machine learning pipeline improves predictions	NATURE MACHINE INTELLIGENCE											TOLL-LIKE RECEPTORS; MASS CYTOMETRY; FLOW-CYTOMETRY; ELASTIC NET; IMMUNE; CELLS; REGRESSION; PERIODONTITIS; RECOGNITION; ACTIVATION	Recent advances have increased the dimensionality and complexity of immunological data. The authors developed a machine learning approach to incorporate prior immunological knowledge and applied it on clinical examples and a simulation study. The approach may be useful for high-dimensional datasets in clinical settings where the cohort size is limited. The dense network of interconnected cellular signalling responses that are quantifiable in peripheral immune cells provides a wealth of actionable immunological insights. Although high-throughput single-cell profiling techniques, including polychromatic flow and mass cytometry, have matured to a point that enables detailed immune profiling of patients in numerous clinical settings, the limited cohort size and high dimensionality of data increase the possibility of false-positive discoveries and model overfitting. We introduce a generalizable machine learning platform, the immunological Elastic-Net (iEN), which incorporates immunological knowledge directly into the predictive models. Importantly, the algorithm maintains the exploratory nature of the high-dimensional dataset, allowing for the inclusion of immune features with strong predictive capabilities even if not consistent with prior knowledge. In three independent studies our method demonstrates improved predictions for clinically relevant outcomes from mass cytometry data generated from whole blood, as well as a large simulated dataset. The iEN is available under an open-source licence.																		2522-5839															10.1038/s42256-020-00232-8		OCT 2020											
J								Enabling the future of colonoscopy with intelligent and autonomous magnetic manipulation	NATURE MACHINE INTELLIGENCE											LOCALIZATION; ACTUATION	Magnetic endoscopes have the potential to improve access, reduce patient discomfort and enhance safety. While navigation of magnetic endoscopes can be challenging for the operator, a new approach by Martin, Scaglioni and colleagues explores how to reduce this burden by offering different levels of autonomy in robotic colonoscopy. Early diagnosis of colorectal cancer substantially improves survival. However, over half of cases are diagnosed late due to the demand for colonoscopy-the 'gold standard' for screening-exceeding capacity. Colonoscopy is limited by the outdated design of conventional endoscopes, which are associated with high complexity of use, cost and pain. Magnetic endoscopes are a promising alternative and overcome the drawbacks of pain and cost, but they struggle to reach the translational stage as magnetic manipulation is complex and unintuitive. In this work, we use machine vision to develop intelligent and autonomous control of a magnetic endoscope, enabling non-expert users to effectively perform magnetic colonoscopy in vivo. We combine the use of robotics, computer vision and advanced control to offer an intuitive and effective endoscopic system. Moreover, we define the characteristics required to achieve autonomy in robotic endoscopy. The paradigm described here can be adopted in a variety of applications where navigation in unstructured environments is required, such as catheters, pancreatic endoscopy, bronchoscopy and gastroscopy. This work brings alternative endoscopic technologies closer to the translational stage, increasing the availability of early-stage cancer treatments.																		2522-5839															10.1038/s42256-020-00231-9		OCT 2020											
J								Accelerating evidence-informed decision-making for the Sustainable Development Goals using machine learning	NATURE MACHINE INTELLIGENCE											SYSTEMATIC REVIEWS; SCIENCE	The United Nations Sustainable Development Goal 2 (SDG 2) is to achieve zero hunger by 2030. We have designed Persephone, a machine learning model, to support a diverse volunteer network of 77 researchers from 23 countries engaged in creating interdisciplinary evidence syntheses in support of SDG 2. Such evidence syntheses, whatever the specific topic, assess original studies to determine the effectiveness of interventions. By gathering and summarizing current evidence and providing objective recommendations they can be valuable aids to decision-makers. However, they are time-consuming; estimates range from 18 months to three years to produce a single review. Persephone analysed 500,000 unstructured text summaries from prominent sources of agricultural research, determining with 90% accuracy the subset of studies that would eventually be selected by expert researchers. We demonstrate that machine learning models can be invaluable in placing evidence into the hands of policymakers. Evidence syntheses produced from the scientific literature are important tools for policymakers. Producing such evidence syntheses can be highly time- and labour-consuming but machine learning models can help as already demonstrated in the health and medical sciences. This Perspective describes a machine learning-based framework specifically designed to support evidence syntheses in the area of agricultural research, for tackling the UN Sustainable Development Goal 2: zero hunger by 2030.																		2522-5839															10.1038/s42256-020-00235-5		OCT 2020											
J								Partially disentangled latent relations for multi-label deep learning	NEURAL COMPUTING & APPLICATIONS										Disentangled latent relations; Diffusion; Self-attention; Feature representation; Multi-label learning	CLASSIFICATION	For multi-label learning, the specific features are extracted from the instances under the supervised of class label is meaningful, and the "purified" feature representation can also be shared with other features during learning process. Besides, it is essential to distinguish the inter-instance relations in input space and inter-label correlation relations in the output space on the multi-label datasets, which is conducive to improve the performance of the multi-label algorithm. However, most current multi-label algorithms aim to capture the mapping between instances and labels, while ignoring the information about instance relations and label correlations in the multi-label data structure. Motivated by these issues, we leverage the deep network to learn the special feature representations for multi-label components without abandoning overlapped features which may belong to other multi-label components. Meanwhile, the Euclidean matrices are leveraged to construct the diagonal matrix for the diffusion function, obtaining the new class latent representation by graph-based diffusion method preserve the inter-instance relations; it ensures that similar features have similar label sets. Further, considering that the contributions of these feature representation are different and have distinct influences on the final multi-label prediction results, the self-attention mechanism is introduced to fusion the other label-specific instance features to build the new joint feature representation, which derives dynamic weights for multi-label prediction. Finally, experimental results on the real data sets show promising wide availability for our approach.																	0941-0643	1433-3058															10.1007/s00521-020-05381-w		OCT 2020											
J								Spatiotemporal attention enhanced features fusion network for action recognition	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Action recognition; Three-stream; Spatiotemporal attention; Features fusion	SPATIAL-TEMPORAL ATTENTION	In recent years, action recognition has become a popular and challenging task in computer vision. Nowadays, two-stream networks with appearance stream and motion stream can make judgment jointly and get excellent action classification results. But many of these networks fused the features or scores simply, and the characteristics in different streams were not utilized effectively. Meanwhile, the spatial context and temporal information were not fully utilized and processed in some networks. In this paper, a novel three-stream network spatiotemporal attention enhanced features fusion network for action recognition is proposed. Firstly, features fusion stream which includes multi-level features fusion blocks, is designed to train the two streams jointly and complement the two-stream network. Secondly, we model the channel features obtained by spatial context to enhance the ability to extract useful spatial semantic features at different levels. Thirdly, a temporal attention module which can model the temporal information makes the extracted temporal features more representative. A large number of experiments are performed on UCF101 dataset and HMDB51 dataset, which verify the effectiveness of our proposed network for action recognition.																	1868-8071	1868-808X															10.1007/s13042-020-01204-5		OCT 2020											
J								A unified linear convergence analysis ofk-SVD	MEMETIC COMPUTING										Eigenvector computation; k-SVD; Riemannian optimization; Convergence analysis	ALGORITHMS	Eigenvector computation, e.g.,k-SVD for finding top-ksingular subspaces, is often of central importance to many scientific and engineering tasks. There has been resurgent interest recently in analyzing relevant methods in terms of singular value gap dependence. Particularly, when the gap vanishes, the convergence ofk-SVD is considered to be capped by a gap-free sub-linear rate. We argue in this work both theoretically and empirically that this is not necessarily the case, refreshing our understanding on this significant problem. Specifically, we leverage the recently proposed structured gap in a careful analysis to establish a unified linear convergence ofk-SVD to one of the ground-truth solutions, regardless of what target matrix and how large target rankkare given. Theoretical results are evaluated and verified by experiments on synthetic or real data.																	1865-9284	1865-9292															10.1007/s12293-020-00315-4		OCT 2020											
J								FUSI-CAD: Coronavirus (COVID-19) diagnosis based on the fusion of CNNs and handcrafted features	PEERJ COMPUTER SCIENCE										Computer-aided diagnosis (CAD); Convolution neural networks (CNN); Coronavirus (COVID-19); Discrete wavelet transform (DWT); Grey level co-occurrence matrix (GLCM)	ADAPTIVE HISTOGRAM EQUALIZATION; INFORMATION	The precise and rapid diagnosis of coronavirus (COVID-19) at the very primary stage helps doctors to manage patients in high workload conditions. In addition, it prevents the spread of this pandemic virus. Computer-aided diagnosis (CAD) based on artificial intelligence (AI) techniques can be used to distinguish between COVID-19 and non-COVID-19 from the computed tomography (CT) imaging. Furthermore, the CAD systems are capable of delivering an accurate faster COVID-19 diagnosis, which consequently saves time for the disease control and provides an efficient diagnosis compared to laboratory tests. In this study, a novel CAD system called FUSI-CAD based on AI techniques is proposed. Almost all the methods in the literature are based on individual convolutional neural networks (CNN). Consequently, the FUSI-CAD system is based on the fusion of multiple different CNN architectures with three handcrafted features including statistical features and textural analysis features such as discrete wavelet transform (DWT), and the grey level co-occurrence matrix (GLCM) which were not previously utilized in coronavirus diagnosis. The SARS-CoV-2 CT-scan dataset is used to test the performance of the proposed FUSI-CAD. The results show that the proposed system could accurately differentiate between COVID-19 and non-COVID-19 images, as the accuracy achieved is 99%. Additionally, the system proved to be reliable as well. This is because the sensitivity, specificity, and precision attained to 99%. In addition, the diagnostics odds ratio (DOR) is 100. Furthermore, the results are compared with recent related studies based on the same dataset. The comparison verifies the competence of the proposed FUSI-CAD over the other related CAD systems. Thus, the novel FUSI-CAD system can be employed in real diagnostic scenarios for achieving accurate testing for COVID-19 and avoiding human misdiagnosis that might exist due to human fatigue. It can also reduce the time and exertion made by the radiologists during the examination process.																	2376-5992					OCT 12	2020									e306	10.7717/peerj-cs.306													
J								Deformable Kernel Networks for Joint Image Filtering	INTERNATIONAL JOURNAL OF COMPUTER VISION										Joint filtering; Convolutional neural networks; Depth map upsampling; Cross-modality image restoration; Texture removal; Semantic segmentation		Joint image filters are used to transfer structural details from a guidance picture used as a prior to a target image, in tasks such as enhancing spatial resolution and suppressing noise. Previous methods based on convolutional neural networks (CNNs) combine nonlinear activations of spatially-invariant kernels to estimate structural details and regress the filtering result. In this paper, we instead learn explicitly sparse and spatially-variant kernels. We propose a CNN architecture and its efficient implementation, called the deformable kernel network (DKN), that outputs sets of neighbors and the corresponding weights adaptively for each pixel. The filtering result is then computed as a weighted average. We also propose a fast version of DKN that runs about seventeen times faster for an image of size 640x480. We demonstrate the effectiveness and flexibility of our models on the tasks of depth map upsampling, saliency map upsampling, cross-modality image restoration, texture removal, and semantic segmentation. In particular, we show that the weighted averaging process with sparsely sampled 3 x 3 kernels outperforms the state of the art by a significant margin in all cases.																	0920-5691	1573-1405															10.1007/s11263-020-01386-z		OCT 2020											
J								Perceptual bias and technical metapictures: critical machine vision as a humanities challenge	AI & SOCIETY										Machine learning; Computer vision; Bias; Interpretability; Perception		In many critical investigations of machine vision, the focus lies almost exclusively on dataset bias and on fixing datasets by introducing more and more diverse sets of images. We propose that machine vision systems are inherently biased not only because they rely on biased datasets but also because theirperceptual topology, their specific way of representing the visual world, gives rise to a new class of bias that we callperceptual bias. Concretely, we define perceptual topology as the set of those inductive biases in machine vision systems that determine its capability to represent the visual world. Perceptual bias, then, describes the difference between the assumed "ways of seeing" of a machine vision system, our reasonable expectations regarding its way of representing the visual world, and its actual perceptual topology. We show how perceptual bias affects the interpretability of machine vision systems in particular, by means of a close reading of a visualization technique called "feature visualization". We conclude that dataset bias and perceptual bias both need to be considered in the critical analysis of machine vision systems and propose to understand critical machine vision as an important transdisciplinary challenge, situated at the interface of computer science and visual studies/Bildwissenschaft.																	0951-5666	1435-5655															10.1007/s00146-020-01058-z		OCT 2020											
J								Designing ethical artifacts has resulted in creative design Empirical studies on the effect of an ethical design support tool	AI & SOCIETY										Creativity support; Design theory; Ethics; Ethical design; User study; Design science		Ethical aspects in engineering design have become increasingly important in recent years. A typical example is the recent rise of artificial intelligence (AI) ethics. This paper applies user studies of a design support tool to empirically verify that our ethical framework improves the creativity of an engineer's design activity. The design support tool provides an environment for the promotion of ethical design perspectives and description. The experiments focus on two functionalities: semi-automatic generation and scenario path recommendation. These functions are designed around the application of ethical design theory, which extends the hierarchical representation of artifacts. Doing this enables users to reconsider their themes at the highest level of the hierarchy and to apply a wider conceptual space of design solutions. For example, by reconsidering the positions of their research themes in the space of the representation field, users can semi-automatically edit them and identify focal areas. Using the scenario path recommendation, designers can update their research themes after considering the ethical impacts of those themes on stakeholders. Both functions are realized by exploiting a knowledge base of ethical and technological discourses. Finally, the ethical design theory is updated based on some unexpected results of our user studies with regards to the cyclic relationship among theory, tools (i.e., experimental equipment), and observed data. For example, temporal dimensional aspects were confirmed as important.																	0951-5666	1435-5655															10.1007/s00146-020-01043-6		OCT 2020											
J								Survey of pedestrian detection with occlusion	COMPLEX & INTELLIGENT SYSTEMS										Occlusion pedestrian detection; Neural network; Artificial features; Deep learning	TRACKING; FEATURES; VISION	Pedestrian detection is widely applied in surveillance, autonomous robotic navigation, and automotive safety. However, there are many occlusion problems in real life. This paper summarizes the research progress of pedestrian detection technology with occlusion. First, according to different occlusion, it can be divided into two categories: inter-class occlusion and intra-class occlusion. Second, it summarizes the traditional method and deep learning method to deal with occlusion. Furthermore, the main ideas and core problems of each method model are analyzed and discussed. Finally, the paper gives an outlook on the problems to be solved in the future development of pedestrian detection technology with occlusion.																	2199-4536	2198-6053															10.1007/s40747-020-00206-8		OCT 2020											
J								Metaheuristics on time series clustering problem: theoretical and empirical evaluation	EVOLUTIONARY INTELLIGENCE										Time series clustering; TsC problem; Metaheuristics; Theoretical; Empirical evaluation	FUZZY C-MEANS; GENETIC ALGORITHM	Considering the literature and the importance of using of the metaheuristic techniques in time series data mining tasks, especially time series clustering (TsC), it seems lack of a comparative study of such techniques in terms of efficiency for TsC problem. Hence, we try to offer the possibility of theoretical and empirical evaluation of metaheuristic techniques in TsC problem. In fact, we follow two main goals by performing this theoretical and empirical evaluation. These goals include: at the first, we would like to prove the effective role of metaheuristic techniques to enhance the efficiency of TsC algorithms, eliminate some challenges of such algorithms (e.g., sensitivity to initialization of some primary parameters), and indicates the popularity of their use in TsC problems during the last years because of their characteristics. Second, we would like to offer the possibility of a comparison between some metaheuristic techniques to analyze the percentage of their effectiveness for enhancing the accuracy of clustering results in TsC problems. The comparative analysis of results of the empirical evaluation for ten standard time series data sets collected from the UCR time series data sets repository show three keys conclusions: (1) generally, metaheuristic techniques have provided reasonable results and significant improvement in terms of efficiency in TsC problem in all experiments due to the main characteristic of metaheuristics that is finding an approximate solution more quickly. But, it is concluded that fuzzy metaheuristic techniques based on population solution class (e.g., FATPSO and FPSO) to provide better results versus single solution class in TsC algorithms. (2) It is deducted that consuming maximum computational time presented by fuzzy metaheuristic techniques in comparison to other optimization techniques in the performed experiments. (3) The paper has proposed that a hybrid of fuzzy metaheuristics (e.g., FATPSO) and the base TsC algorithms can be considered as a more adequate choice to present better results in terms of accuracy in different application areas.																	1864-5909	1864-5917															10.1007/s12065-020-00511-8		OCT 2020											
J								Feature reduction using SVM-RFE technique to detect autism spectrum disorder	EVOLUTIONARY INTELLIGENCE										Autism spectrum disorder (ASD); IBk (K-nearest neighbor); Naive Bayes; Recursive feature elimination (RFE); LibSVM; SVMAttributeEval	CHILDREN	Autism Spectrum Disorder (ASD) is a developmental disorder characterized by difficulties in social interaction, communication, and restricted or repetitive patterns of thought and behaviour. Diagnosing ASD is important since it is a life long condition and early diagnosis of ASD has a great deal of importance in terms of controlling the disease. This research work focuses on the analysis of the features that are vital in diagnosing the symptoms of ASD in an individual and to help in the early identification of ASD. The autism dataset for this research work is taken from the UCI repository. The proposed method, SVMAttributeEval, assigns feature weight to the features and the features are ranked based on their importance. The recursive Feature Elimination method is applied and the performance of the classification algorithms LibSVM, IBk, and Naive Bayes for the reduced feature subsets selected by the wrapper method is measured. The empirical results show an improvement in the accuracy of the classifiers on the removal of the least significant features with feature reduction of 60% achieved against the original feature set. The performance of the classification algorithms has significantly improved for the reduced feature subset of ASD. The LibSVM classification algorithm achieves 93.26% accuracy, IBk (92.3%), and Naive Bayes (91.34%) for the selected feature subset as compared to the values achieved for the whole feature set.																	1864-5909	1864-5917															10.1007/s12065-020-00498-2		OCT 2020											
J								Modeling information diffusion in online social networks using a modified forest-fire model	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Information diffusion; Forest-fire model; Nature-inspired algorithm; Online social networks; Twitter		Information dissemination has changed rapidly in recent years with the emergence of social media which provides online platforms for people worldwide to share their thoughts, activities, emotions, and build social relationships. Hence, modeling information diffusion has become an important area of research in the field of network analysis. It involves the mathematical modeling of the movement of information and study the information spread pattern. In this paper, we attempt to model information propagation in online social networks using a nature-inspired approach based on a modified forest-fire model. A slight spark can start a wildfire in a forest, and the spread of this fire depends on vegetation, weather, and topography, which may act as fuel. On similar lines, we labeled users who haven't joined the network yet asEmpty, existing users asTree, and information asFire. The spread of information across online social networks depends upon users-followers relationships, the significance of the topic, and other such features. We introduce a novelBurntstate to the traditional forest-fire model to represent non-spreaders in the network. We validate our method on six real-world data-sets extracted from Twitter and conclude that the proposed model performs reasonably well in predicting information diffusion.																	0925-9902	1573-7675															10.1007/s10844-020-00623-8		OCT 2020											
J								Driver distraction detection using capsule network	NEURAL COMPUTING & APPLICATIONS										Driver distraction; CapsNet; Dynamic routing; Posture classification		With the onset of the new technological age, the distractions caused due to handheld devices have been a major cause of traffic accidents as they affect the decision-making capabilities of the driver and give them less time to react to difficult situations. Often drivers try to multitask which reduces their reaction time leading to accidents which could have been easily avoided if they had been attentive. As such problems are related to the driver's negligence toward safety, a possible solution is to monitor driver's behavior and notify if they are distracted. We propose a CapsNet-based approach for detecting the distracted driver which is a novel approach. The proposed method scores perform well on the real-world environment inputs when compared to other famous methods used for the same. Our proposed methods get high scores for all the most commonly used metrics for classification. On the testing set, the proposed method gets an accuracy of 0.90, 0.92 as precision score, 0.90 as recall score and 0.91 as F-measure.																	0941-0643	1433-3058															10.1007/s00521-020-05390-9		OCT 2020											
J								Construction site layout planning and safety management using fuzzy-based bee colony optimization model	NEURAL COMPUTING & APPLICATIONS										CSLP; FBCO; Cost facility; Safety facility; Noise pollution; Temporary facilities; Scenarios	NOISE-POLLUTION; RISK-ASSESSMENT; BOTTOM ASH; PREDICTION; ALGORITHM; MOTION; PERFORMANCE; STRENGTH; PSO	The construction site layout planning is an activity that establishes the temporary facility location, thereby enhancing the efficiency of the construction. Recently, safety enhancement plays a significant role in construction site layout planning. This paper aims in developing an optimal construction site layout planning by fulfilling three main objectives, namely minimum cost facility, minimum risk of safety facility and minimum noise pollution. In addition to this, this paper also proposes a fuzzy-based bee colony optimization (FBCO) algorithm for tuning rho and tau parameters so as to obtain a feasible and optimal solution. Also, this FBCO algorithm is employed to solve the construction site layout problem by satisfying the multi-objective function. Then, an FBCO-CLP approach is employed in obtaining a final optimal construction site layout plan. Moreover, the performance analysis of FBCO employs five benchmark functions to examine the effectiveness of the system for numerous aspects. Finally, a case study utilizes a residential building for verifying the proposed approach containing 13 temporary facilities. The experimental result reveals that the proposed FBCO-CSLP approach provides an optimal layout plan with minimum cost, noise pollution and safety risk facilities. Moreover, the comparative analysis is done by comparing the proposed approach with various other approaches such as ANN, GA, PSO, ABC and DE for three different types of facilities, namely cost facility, safety facility risk and noise facility. The experimental analysis reveals that the proposed approach provides better performances when compared with all other approaches.																	0941-0643	1433-3058															10.1007/s00521-020-05361-0		OCT 2020											
J								A hybrid deep learning-based fruit classification using attention model and convolution autoencoder	COMPLEX & INTELLIGENT SYSTEMS										Fruit classification; DenseNet; CBAM; Convolutional neural networks	OPTIMIZATION; OBJECTS	Image recognition supports several applications, for instance, facial recognition, image classification, and achieving accurate fruit and vegetable classification is very important in fresh supply chain, factories, supermarkets, and other fields. In this paper, we develop a hybrid deep learning-based fruit image classification framework, named attention-based densely connected convolutional networks with convolution autoencoder (CAE-ADN), which uses a convolution autoencoder to pre-train the images and uses an attention-based DenseNet to extract the features of image. In the first part of the framework, an unsupervised method with a set of images is applied to pre-train the greedy layer-wised CAE. We use CAE structure to initialize a set of weights and bias of ADN. In the second part of the framework, the supervised ADN with the ground truth is implemented. The final part of the framework makes a prediction of the category of fruits. We use two fruit datasets to test the effectiveness of the model, experimental results show the effectiveness of the framework, and the framework can improve the efficiency of fruit sorting, which can reduce costs of fresh supply chain, factories, supermarkets, etc.																	2199-4536	2198-6053															10.1007/s40747-020-00192-x		OCT 2020											
J								A machine learning approach for optimizing heuristic decision-making in Web Ontology Language reasoners	COMPUTATIONAL INTELLIGENCE										description logic reasoning; machine learning; OWL reasoning; tableau optimization techniques		Description logics (DLs) are formalisms for representing knowledge bases of application domains. The Web Ontology Language (OWL) is a syntactic variant of a very expressive DL. OWL reasoners can infer implied information from OWL ontologies. The performance of OWL reasoners can be severely affected by situations that require decision-making over many alternatives. Such a nondeterministic behavior is often controlled by heuristics that are based on insufficient information. This article proposes a novel OWL reasoning approach that applies machine learning (ML) to implement pragmatic and optimal decision-making strategies in such situations. Disjunctions occurring in ontologies are one source of nondeterministic actions in reasoners. We propose two ML-based approaches to reduce the nondeterminism caused by dealing with disjunctions. The first approach is restricted to propositional DL while the second one can deal with standard DL. Both approaches speed up our ML-based reasoner by up to two orders of magnitude in comparison to the non-ML reasoner. Another source of nondeterministic actions is the order in which tableau rules should be applied. On average, our ML-based approach achieves a speedup of two orders of magnitude when compared to the most expensive rule ordering of the non-ML reasoner.																	0824-7935	1467-8640															10.1111/coin.12404		OCT 2020											
J								Psychological stimulation for anxious states detection based on EEG-related features	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Electroencephalogram; Stress and anxiety detection; Psychological stimulation; Feature extraction; Feature selection	EMOTION RECOGNITION; ASYMMETRY; ARTIFACTS; MACHINE; ANXIETY; STRESS; MODEL	Anxiety affects human capabilities and behavior as much as it affects productivity and quality of life. It is considered to be the main cause of depression and suicide. Anxious states are detectable by specialists by virtue of their acquired cognition and skills. There is a need for non-invasive reliable techniques that performs the complex task of anxiety detection. In our study, we investigate the impact of different parameters, notably: trial duration, feature type, feature combination and anxiety levels number. The system is evaluated using our own database containing recorded Electroencephalogram (EEG) signals from 23 participants during anxiety elicitation by means of face-to-face psychological stimuli. EEG signals were captured with an Emotiv Epoc headset as a cost-efficient wireless wearable equipment. Two labeling methods are used and results are presented accordingly. Our findings showed that anxiety is well elicited in 1 s. For Self Assessment Manikan SAM-based detection, Stacked Sparse Autoencoder (SSAE) with different type of features achieves 83.50% and 74.60% for 2 and 4 anxiety levels detection, respectively. Results are improved using the Hamilton-based method. We obtained a rate of 86.7% for 4 levels detection using SSAE. The presented results prove the benefits of the use of a low-cost EEG headset instead of medical non-wireless devices and create a starting point for new researches in the field of anxiety detection.																	1868-5137	1868-5145															10.1007/s12652-020-02586-8		OCT 2020											
J								Extracting Classification Rules from Artificial Neural Network Trained with Discretized Inputs	NEURAL PROCESSING LETTERS										Neural network; Rule extraction; Multiobjective genetic algorithm; Discretized attributes	SYMBOLIC INTERPRETATION; GENETIC ALGORITHMS; SELECTION	Rule extraction from artificial neural networks remains important task in complex diseases such as diabetes and breast cancer where the rules should be accurate and comprehensible. The quality of rules is improved by the improvement of the network classification accuracy which is done by the discretization of input attributes. In this paper, we developed a rule extraction algorithm based on multiobjective genetic algorithms and association rules mining to extract highly accurate and comprehensible classification rules from ANN's that have been trained using the discretization of the continuous attributes. The data pre-processing provides very good improvement of the ANN accuracy and consequently leads to improve the performance of the classification rules in terms of fidelity and coverage. The results show that our algorithm is very suitable for medical decision making, so an excellent average accuracy of 94.73 has been achieved for the Pima dataset and 99.36 for the breast cancer dataset.																	1370-4621	1573-773X															10.1007/s11063-020-10357-x		OCT 2020											
J								Cancer miRNA biomarkers classification using a new representation algorithm and evolutionary deep learning	SOFT COMPUTING										MicroRNA; Meta-label; Super-class; Intelligent reasoning; Feature selection; Optimized convolutional neural network	FEATURE-SELECTION; OPTIMIZATION ALGORITHM; RECEPTIVE-FIELDS; MICRORNAS; DIAGNOSIS; STRATEGY; KERNEL	The diagnosis of cancer is presently undergoing a change of paradigm for the diagnostic panel using molecular biomarkers. MicroRNA (miRNA) is one of the most important genomic datasets presenting the genome sequences. Since several studies have shown the relationship between miRNAs and cancers, data mining and machine learning methods can be incorporated to extract a large amount of knowledge from cancer genomic datasets. However, previous research works on the identification of cancers from miRNAs have made it possible to diagnose cancer, and the accuracy of some classes is not quite satisfactory. Therefore, this research is aimed at promoting a super-class (meta-label) approach and deep learning in a three-phase method to diagnose cancers from miRNAs. The steps in the first phase of the proposed method, named Representation learning, are partitioning data into super-classes, meta-data creation and super-classes classification. This phase helps data to be split into some subsets to improve classification accuracy. In other words, the first phase groups labels based on the separability of classes into a meta-label, and then a multi-label learner is built to predict these meta-labels. In the second phase, a feature selection to reduce the dimensions of the problem is applied to each super-class to help to focus the attention of an induction algorithm in those features that are more important to predict the target concept. In the third phase of the proposed method, an evolutionary deep neural network for the classification of labels in each super-class is performed. The last two phases are done separately for each subset in which five super-classes and subsequently five deep neural networks are trained. The experimental results reveal that the proposed method achieved more efficient results than 19 recent machine learning methods. Despite the fact that evaluating the dataset which consists of 29 types of cancers provides a more complicated situation for the convolutional neural network to be learned, the performance of the method is noticeably better than other existing methods. The other success which can be considered here is a significant reduction in running time comparing to other methods.																	1432-7643	1433-7479															10.1007/s00500-020-05366-w		OCT 2020											
J								Artificial Bee Colony algorithm based on Dominance (ABCD) for a hybrid gene selection method	KNOWLEDGE-BASED SYSTEMS										Gene selection; Multi-objective optimization; Artificial bee colony algorithm; Cancer; Support vector machine; Analytic hierarchy process	B-CELL LYMPHOMA; OPTIMIZATION; PROGRESSION; CANDIDATE; PATHWAY; TARGET; PTPRJ	In cancer research, it is important to classify tissue samples in different classes (normal, tumour, tumour type, etc.). Gene selection purpose is to find the minimum number of genes that can predict sample classes with efficacy. This work is focused on the gene selection problem by introducing a new hybrid method. This new method combines a first step of gene filtering with an optimization algorithm in a second step to find the best subset of genes for the classification task. The first step uses the Analytic Hierarchy Process, in which five ranking methods are used to select the most relevant genes in the dataset. In this way, this gene filtering reduces the number of genes to manage. Regarding the second step, the gene selection can be divided into two objectives: minimizing the number of selected genes and maximizing the classification accuracy. Therefore, we have used a multi-objective optimization approach. More exactly, an Artificial Bee Colony based on Dominance (ABCD) algorithm has been proposed for this second step. Our approach has been tested with eleven real cancer datasets and the results have been compared with several multi-objective methods proposed in the scientific literature. Our results show a high accuracy in the classification task with a small subset of genes. Also, to prove the relevance of our proposal, a biological analysis has been developed on the genes selected. The conclusions of this biological analysis are positive, because the selected genes are closely linked to the cancer dataset they belong to. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				OCT 12	2020	205								106323	10.1016/j.knosys.2020.106323													
J								Interactive online learning for graph matching using active strategies	KNOWLEDGE-BASED SYSTEMS										Online learning; Active learning; Human interaction; Graph matching; Costs functions	COOPERATIVE POSE ESTIMATION; EDIT DISTANCE; COMPUTATION; ASSIGNMENT; ALGORITHMS; OPTIMALITY; MODELS; COSTS	In some pattern recognition applications, objects are represented by attributed graphs, in which nodes represent local parts of the objects and edges represent relationships between these local parts. In this framework, the comparison between objects is performed through the distance between attributed graphs. Usually, this distance is a linear equation defined by some cost functions on the nodes and on the edges of both attributed graphs. In this paper, we present an online, active and interactive method for learning these cost functions, which works as follows. Graphs are provided to the learning algorithm by pairs in a sequential order (online). Then, a correspondence between them is computed, and there is a strategy that, given the current pair of graphs and the computed correspondence, proposes which node-to-node mapping would most contribute to the learning process (active). Finally, the human can correct some node-to-node mappings if the human thinks they are wrong (interactive). This is the first learning method applied to graph matching that has the following two features: Being an online method and being active and interactive. These properties make our method useful in the cases that data does not arrive at once and when the human can interact on the system. Thus, given some human interactions the method would have to tend to gradually increase its accuracy. The results show that with few interactions, we achieve better results than the offline learning state of the art methods that are currently available. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				OCT 12	2020	205								106275	10.1016/j.knosys.2020.106275													
J								EEG-based emotion recognition using an end-to-end regional-asymmetric convolutional neural network	KNOWLEDGE-BASED SYSTEMS										Emotion recognition; End-to-end; Regional; Asymmetric	DEPRESSION	Emotion recognition based on electroencephalography (EEG) is of great important in the field of Human-Computer Interaction (HCI), which has received extensive attention in recent years. Most traditional methods focus on extracting features in time domain and frequency domain. The spatial information from adjacent channels and symmetric channels is often ignored. To better learn spatial representation, in this paper, we propose an end-to-end Regional-Asymmetric Convolutional Neural Network (RACNN) for emotion recognition, which consists of temporal, regional and asymmetric feature extractors. Specifically, continuous ID convolution layers are employed in temporal feature extractor to learn time-frequency representations. Then, regional feature extractor consists of two 2D convolution layers to capture regional information among physically adjacent channels. Meanwhile, we propose an Asymmetric Differential Layer (ADL) in asymmetric feature extractor by taking the asymmetry property of emotion responses into account, which can capture the discriminative information between left and right hemispheres of the brain. To evaluate our model, we conduct extensive experiments on two publicly available datasets, i.e., DEAP and DREAMER. The proposed model can obtain recognition accuracies over 95% for valence and arousal classification tasks on both datasets, significantly outperforming the state-of-the-art methods. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				OCT 12	2020	205								106243	10.1016/j.knosys.2020.106243													
J								Phase space elliptic density feature for epileptic EEG signals classification using metaheuristic optimization method	KNOWLEDGE-BASED SYSTEMS										Phase space; Feature extraction; Electroencephalogram signal; Epilepsy; Classification	EMPIRICAL MODE DECOMPOSITION; WAVELET TRANSFORM; SEIZURE DETECTION; AUTOMATIC DETECTION; STATISTICAL FEATURES; APPROXIMATE ENTROPY; NEURAL-NETWORKS; IDENTIFICATION; RECONSTRUCTION; REPRESENTATION	The electroencephalography (EEG), which is a method for monitoring the brain signals, is a common method used to diagnose the epileptic seizures. In this study, some features are presented for the classification of the brain signals. These features are based on the texture and structure of the brain signals in the phase space representation (PSR). Due to the resonance property, the data are elliptical in the phase space. Therefore, the mentioned features are based on the calculations of the data density in the ellipses. In the first method of feature extraction, the radius values of the ellipses are assumed based on the normal distribution feature. In the two other methods of the presented features, the radius values of the assumed ellipses are calculated by the optimizer. These methods of feature extraction are based on the incremental ellipses and intersecting ellipses, respectively. The density of the data in the assumed ellipses is given to the k-nearest neighbor as a feature to classify the epileptic seizure and seizure-free EEG signals. The intended method was implemented and investigated on two databases of the Bonn university of Germany and the neurology and sleep center of New Delhi. The results indicate that the proposed features are strong tools for separation and diagnosis of this type of signal and have higher accuracy compared to the other classic and updated methods. The extraction speed of the presented features was higher in the test phase compared to the other methods. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				OCT 12	2020	205								106276	10.1016/j.knosys.2020.106276													
J								Clustering and supervised response for XACML policy evaluation and management	KNOWLEDGE-BASED SYSTEMS										Clustering algorithm; Large-scale policy sets; Policy Decision Point (PDP); Supervised learning; XACML	ACCESS-CONTROL; FRAMEWORK	To meet the increasingly complex requirements in access control using XACML (eXtensible Access Control Markup Language), it is necessary for a policy decision engine to deal with large-scale policy sets and intensively abundant requests efficiently. A practical policy evaluation engine, namely CSRM, is proposed to tackle this problem. The PDP (Policy Decision Point) in traditional policy decision engines is replaced by a new component ESPDP (Efficient Searching Policy Decision Point). CK-means algorithm is studied in this paper to perform clustering among all policies in a policy set. ESPDP is adopted to construct a virtual mapping table on the basis of the result of the CK-means algorithm. The virtual mapping table stores the relationship between subject attributes and policies, such that the irrelevant polices are excluded when rule search is carried out. Besides, the rules in every policy are merged according to particular principles, thus saving storage space and greatly speeding up rule search. When responding to intensive requests, a supervised response method is applied to determine an optimal rule search order by analyzing the response to the requests in a short period. The experimental results on four practical datasets demonstrate that our proposed CSRM outperforms some classic and state-of-the-art methods when dealing with large-scale policy sets. With high practicality and wide applicability, CSRM effectively eliminates the bottlenecks of improving PDP evaluation performance, and can respond to requests efficiently when handling large-scale policy sets. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				OCT 12	2020	205								106312	10.1016/j.knosys.2020.106312													
J								Learning sparse features with lightweight ScatterNet for small sample training	KNOWLEDGE-BASED SYSTEMS										Lightweight; ScatterNet; Sparse features; Learnable filters; Hybrid architecture		Convolutional neural networks (CNNs) have recently achieved impressive performances in image processing tasks such as image classification and object recognition. However, CNNs typically have a large number of parameters, leading to their requirement of a large number of training samples to extract spatial features. To address these limitations, we propose a lightweight ScatterNet with the learnable weight matrix and sparse transformation such as scale transformation and translation to learn sparse filters. This filter based on ScatterNet uses He initialization algorithm and learns from input images which are viewed as two-directional sequential data in the initial stage of model training. A Strip-Recurrent module sweeps both horizontally and vertically across the image to compress feature matrices. Then, ScatterNet decomposes the above feature matrices as a learned mixture of different harmonic functions to integrate the spectral analysis into CNNs. Finally, we combine the sequential and spectral features to build our hybrid architectures to complete image classification and segmentation. These architectures can obtain good classification accuracy on both small and large training datasets. Our proposed method is evaluated at both layer and network levels on five widely-used benchmark datasets: MNIST, CIFAR-10, CIFAR-100, Small NORB and Tiny ImageNet. We also study other small sample problems such as medical image segmentation and image classification based on few-shot learning. Experiments show that our proposed layer and hybrid model achieves better accuracy for small sample training. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				OCT 12	2020	205								106315	10.1016/j.knosys.2020.106315													
J								A proactive decision support system for predicting traffic crash events: A critical analysis of imbalanced class distribution	KNOWLEDGE-BASED SYSTEMS										Crash prediction; Proactive decision support system; Machine learning; Class-imbalance; Driving simulator	NEURAL-NETWORKS; TIME; COLLISION; CLASSIFICATION; WEATHER; FRAMEWORK; IDENTIFICATION; PERFORMANCE; REGRESSION; GEOMETRY	Real-time crash prediction plays a key role in enhancing traffic safety as well as mitigating disruptions to road users. The further improvements of predictability require the systemic analysis of crash likelihood within the driver-vehicle-environment triptych. This study presents a proactive decision support system that can predict crash events based on vehicle kinematics, driver inputs, roadway geometric features and real-time weather data. Modeling approaches that rely on Random forest, Support Vector Machine and Multilayer Perceptron machine learning techniques were applied to establish efficient crash predictions. Moreover, crash events are generally unexpected and occur rarely, thus classification results can yield deceivingly high prediction performance which are usually driven by the majority class at the expense of having poor performance on the crucial minority class. Therefore, this paper attempts to add to the current knowledge by investigating crash likelihood based on compared different data balancing techniques to improve the predictive performance through three balancing techniques: over-sampling, under-sampling and synthetic minority over-sampling (SMOTE). The highest performances have been acquired using SMOTE strategy as MLP achieved a 94.5% precision, 94.2% f1-score, 93.7% AUC and 95.3% recall, while SVM achieved a 91.5% g-mean. Furthermore, results indicated that more than 62% of total crashes have been reported in downhills and curved downhills, and 44% of all crash instances have been reported during both snow and rain weather patterns. Overall, the findings highlighted the significance of the explanatory variables associated with potential crash events and can suggest to decision-makers a safe and credible system for enhancing traffic safety. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				OCT 12	2020	205								106314	10.1016/j.knosys.2020.106314													
J								An edge creation history retrieval based method to predict links in social networks	KNOWLEDGE-BASED SYSTEMS										Online social networks; Data mining; Graph mining; Link prediction		Link prediction is a graph mining task that aims to foretell whether pairs of non-linked nodes will connect in the future. It has many useful applications in social networks such as friend recommendation, identification of future collaborations between authors in co-authorship networks, discovery of hidden groups of terrorists and criminals, among others. In general, the state-of-the-art link prediction methods consider topological data extracted from the current state (i.e., the most recent and available snapshot) of a network. They do not take into account information that describes how the network's topology was at the moments when the existing edges were created. Hence, those methods take the chance to disregard information about the circumstances that may have influenced the appearance of old edges, and that could be useful to predict the creation of new ones. Thus, this study raises and evaluates the hypothesis that recovering such data may contribute to improving link prediction. This hypothesis is justified since those data enrich the description of the application's context with examples that represent exactly the kind of event to be foreseen: the creation of new connections. To this end, this paper proposes a new link prediction method that is based on edge creation history retrieval. Results from experiments with twenty scenarios of four real co-authorship social networks presented statistical evidence that indicates the effectiveness of the proposed method and confirms the raised hypothesis. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				OCT 12	2020	205								106268	10.1016/j.knosys.2020.106268													
J								Fusion with distance-aware selection strategy for dandelion algorithm	KNOWLEDGE-BASED SYSTEMS										Dandelion algorithm; Selection strategy; Position distance; Fusion pool; Overall reward	OPTIMIZATION ALGORITHM; MODEL	The Dandelion Algorithm (DA) is a recently proposed swarm intelligent optimization algorithm which is inspired by the process of dandelion sowing. In DA, the selection strategy (SS) is of key importance which is responsible for choosing the dandelions to enter the next generation. However, current SS in DA only considers the function fitness value, which cannot maintain the diversity of the population and can easily fall into local optimal solutions prematurely. To address this problem, a distance-aware selection strategy (DSS) is proposed by jointly considering the function fitness value and the individual position distance. In addition, inspired by ensemble learning, a fusion based adaptive selection strategy (FSS) is proposed to further improve the performance of DA. Specifically, a fusion pool including several famous SSs and our proposed DSS is established. And in each iteration, the best SS in terms of overall reward is selected. Experimental results show that the proposed algorithms are powerful in terms of accuracy in the 28 standard functions from the CEC2O13. For DSS, it outperforms several famous SSs in DA. For FSS, it is superior or competitive to 11 participating algorithms benchmarked on CEC2O13 and five recently proposed algorithms improved by new selection strategy. Since FSS is a universal framework, it can be applied to other swarm intelligence algorithms with selection strategy. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				OCT 12	2020	205								106282	10.1016/j.knosys.2020.106282													
J								Adaptive and efficient high-order rating distance optimization model with slack variable	KNOWLEDGE-BASED SYSTEMS										Recommender systems; Slack variable; High-order rating distance; Newton method		The classical high-order rating distance model which aims to minimize not only (i) the difference between the estimated and real ratings of the same user-item pair (i.e., the first-order rating distance), but also (ii) the difference between the estimated and real rating difference of the same user across different items (i.e., the second-order rating distance), and use stochastic gradient descent to solve this convex optimization problem in recommender systems, has good performance in prediction accuracy. However, when the manually set parameter for the second-order rating difference is fixed, this model will not converge as the size of dataset increasing, and its performance on efficiency is slow compared with the matrix factorization method. Aiming at improving such model's adaptability and efficiency, we propose an improved high-order rating distance model with omitting rules based on slack variable, in which the static parameter used to balance the first-order rating distance and the second-order rating distance is replaced by a data-scale sensitive function. We choose Newton method to solve the convex recommendation optimization problem defined in this paper instead of stochastic gradient descent. Our model not only achieves the adaptability by eliminating several static parameters for module balancing, reduces the computation complexity, but also accelerates the optimization function convergence speed. We provide solid theoretical support and conduct comprehensive experiments on four real-world datasets. Experimental results show the proposed model has good performance in terms of prediction accuracy and efficiency. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				OCT 12	2020	205								106228	10.1016/j.knosys.2020.106228													
J								Spatio-temporal feature fusion for dynamic taxi route recommendation via deep reinforcement learning	KNOWLEDGE-BASED SYSTEMS										Spatio-temporal feature fusion; Sequential decision making; Taxi route recommendation; Deep reinforcement learning; Transportation	NETWORKS; GAME; GO	Dynamic taxi route recommendation aims at recommending cruising routes to vacant taxis such that they can quickly find and pick up new passengers. Given citizens' giant but unbalancing riding demand and the very limited taxis in a city, dynamic taxi route recommendation is essential for its ability to alleviate the waiting time of passengers and increase the earning of taxi drivers. Thus, in this paper we study the dynamic taxi route recommendation problem as a sequential decision-making problem and we design an effective two-step method to tackle it. First, we propose to consider and extract multiple real-time spatio-temporal features, which are related with the easiness degree of vacant taxis picking up new passengers. Second, we design an adaptive deep reinforcement learning method, which learns a carefully designed deep policy network to better fuse the extracted spatio-temporal features such that effective route recommendation can be done. Extensive experiments using real-world data from San Francisco and New York are conducted. Comparing with the state-of-the-arts, our method can increase at least 15.8% of average earning for taxi drivers and reduce at least 29.6% of average waiting time for passengers. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				OCT 12	2020	205								106302	10.1016/j.knosys.2020.106302													
J								Double-level adversarial domain adaptation network for intelligent fault diagnosis	KNOWLEDGE-BASED SYSTEMS										Domain adaptation; Intelligent diagnosis; Domain-level alignment; Class-level alignment; Machine	CONVOLUTIONAL NEURAL-NETWORK; ENCODER	Deep neural networks have been widely studied in the field of mechanical fault diagnosis with the rapidity of intelligent manufacturing and industrial big data, however, attractive performance gains usually come from a premise that source training data and target test data have the same distribution. Unfortunately, this assumption is generally untenable in practice due to changeable working conditions and complex industrial environment. To address this issue, a double-level adversarial domain adaptation network (DL-ADAN) is presented for cross-domain fault diagnosis, which is able to bridge the divergences between the source and target domains. Specifically, the proposed diagnostic framework is composed of a feature extractor based on deep convolutional network, a domain discriminator and two label classifiers, which conducts two minimax adversarial games. In the first adversarial stream, the feature extractor and domain discriminator game with each other to achieve domain-level alignment from a global perspective. On the other line, the extractor and two classifiers are against each other to conduct class-level alignment, in which Wasserstein discrepancy is used to detect outlier target samples. As a result, the extractor can learn transferable discriminative features for accurate fault diagnosis. Extensive diagnostic experiments are constructed for performance analysis and several state of the art diagnostic methods are selected for comparative study. The comprehensive results demonstrate the effectiveness and superiority of the proposed method. (C) 2020 Published by Elsevier B.V.																	0950-7051	1872-7409				OCT 12	2020	205								106236	10.1016/j.knosys.2020.106236													
J								Fast hyperparameter tuning using Bayesian optimization with directional derivatives	KNOWLEDGE-BASED SYSTEMS										Bayesian optimization; Gaussian process; Hyperparameter tuning		In this paper we develop a Bayesian optimization based hyperparameter tuning framework inspired by statistical learning theory for classifiers. We utilize two key facts from PAC learning theory; the generalization bound will be higher for a small subset of data compared to the whole, and the highest accuracy for a small subset of data can be achieved with a simple model. We initially tune the hyperparameters on a small subset of training data using Bayesian optimization. While tuning the hyperparameters on the whole training data, we leverage the insights from the learning theory to seek more complex models. We realize this by using directional derivative signs strategically placed in the hyperparameter search space to seek a more complex model than the one obtained with small data. We demonstrate the performance of our method on the tasks of tuning the hyperparameters of several machine learning algorithms. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				OCT 12	2020	205								106247	10.1016/j.knosys.2020.106247													
J								Multi-surrogate multi-tasking optimization of expensive problems	KNOWLEDGE-BASED SYSTEMS										Computationally expensive problems; Multi-tasking optimization; Surrogate models; Radial basis function	PARTICLE SWARM OPTIMIZATION; EVOLUTIONARY OPTIMIZATION; DIFFERENTIAL EVOLUTION; GLOBAL CONVERGENCE; ALGORITHMS; STRATEGY	Multiple surrogates can be trained in surrogate-assisted optimization of expensive problems to describe different characteristics of the real fitness landscape. It has been shown that optimization assisted by multiple surrogate models are beneficial compared to a single surrogate. Along this line of research, we propose to train two surrogate models, one global surrogate model trained using all available data, and the other one local surrogate model trained using only part of the data subsequently selected from the data sorted according to an ascending order of the objective value. Different from most existing multi-surrogate based approaches, however, we adopt the multi-tasking optimization framework to accelerate the convergence by regarding the two surrogates as two related tasks. This way, two optimal solutions found by the multi-tasking algorithm will be evaluated using the real expensive objective function, and consequently, both the global and local models will be updated. This process repeats until the allowed computational budget is exhausted. Experiments are conducted on twelve widely used benchmark problems of up to 200 dimensions to examine the performance of the proposed algorithm. Our results show that the proposed method is very competitive, has quick convergence and scales well with the increase in the number of decision variables for solving computationally expensive single-objective optimization problems. (C) 2020 Published by Elsevier B.V.																	0950-7051	1872-7409				OCT 12	2020	205								106262	10.1016/j.knosys.2020.106262													
J								Simultaneously learning feature-wise weights and local structures for multi-view subspace clustering	KNOWLEDGE-BASED SYSTEMS										Local adaptive learning; Multi-view clustering; Subspace clustering	REPRESENTATION; ALGORITHM; BLOCK	Multi-view clustering integrates multiple feature sets, which usually have a complementary relationship and can reveal distinct insights of data from different angles, to improve clustering performance. It remains challenging to productively utilize complementary information across multiple views since there is always noise in real data, and their features are highly redundant. Moreover, most existing multi-view clustering approaches only aimed at exploring the consistency of all views, but overlooked the local structure of each view. However, it is necessary to take the local structure of each view into consideration, because individual views generally present different geometric structures while admitting the same cluster structure. To ease the above issues, in this paper, a novel multi-view subspace clustering method is established by concurrently assigning weights for different features and capturing local information of data in view-specific self-representation feature spaces. In particular, a common clustering assignment regularization is adopted to explore the consistency among multiple views. An alternating iteration algorithm based on the augmented Lagrangian multiplier is also developed for optimizing the associated objective. Experiments conducted on diverse multi-view datasets manifest that the proposed method achieves state-of-the-art performance. We provide the Matlab code on https: github.com /Ekin 102003/JFLMSL. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				OCT 12	2020	205								106280	10.1016/j.knosys.2020.106280													
J								Joint subspace and discriminative learning for self-paced domain adaptation	KNOWLEDGE-BASED SYSTEMS										Subspace learning; Self-paced learning; Unsupervised domain adaptation		Unsupervised domain adaptation aims to address the problem in which the source data and target data are related but distributed differently. A widely-used two-stage strategy is to learn a domain-invariant subspace, and then train a cross-domain classifier on the resulting subspace. In this paper, we propose a single-stage domain adaption approach for joint subspace learning and discriminative learning. Specifically, a domain-invariant subspace and a cross-domain classifier are progressively learnt in a self-paced learning fashion. To avoid unlabeled target data dominating the overall loss and misleading model training, we progressively include more target data from "easy" to "complex" to optimize our model. Specifically, we propose an alternative optimization algorithm to efficiently find a reasonable solution for our task. Extensive experiments are conducted on multiple standard benchmarks to verify the effectiveness of the proposed approach. The results demonstrate that our model can outperform state-of-the-art non-deep domain adaptation methods. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				OCT 12	2020	205								106285	10.1016/j.knosys.2020.106285													
J								State representation modeling for deep reinforcement learning based recommendation	KNOWLEDGE-BASED SYSTEMS										State representation modeling; Deep reinforcement learning; Recommendation		Reinforcement learning techniques have recently been introduced to interactive recommender systems to capture the dynamic patterns of user behavior during the interaction with recommender systems and perform planning to optimize long-term performance. Most existing research work focuses on designing policy and learning algorithms of the recommender agent but seldom cares about the state representation of the environment, which is indeed essential for the recommendation decision making. In this paper, we first formulate the interactive recommender system problem with a deep reinforcement learning recommendation framework. Within this framework, we then carefully design four state representation schemes for learning the recommendation policy. Inspired by recent advances in feature interaction modeling in user response prediction, we discover that explicitly modeling user-item interactions in state representation can largely help the recommendation policy perform effective reinforcement learning. Extensive experiments on four real-world datasets are conducted under both the offline and simulated online evaluation settings. The experimental results demonstrate the proposed state representation schemes lead to better performance over the state-of-the-art methods. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				OCT 12	2020	205								106170	10.1016/j.knosys.2020.106170													
J								Agent-based modeling and simulations of terrorist attacks combined with stampedes	KNOWLEDGE-BASED SYSTEMS										Attack mechanism; Agent-based modeling; Simulations; Stampede mechanism; Perception range; Collision damage	EVACUATION; CROWD; SOCIOPHYSICS; ECONOPHYSICS; SELECTION; DECISION; BEHAVIOR; SYSTEM; IMPACT	As a global problem, the terrorism leads to high death tolls each year. During terrorist attacks, the direct death is caused by terrorists attacking civilians. However, indirect death caused by stampedes should not be underestimated. Under great panic, most civilians were running disorderly, rushing into limited number of Exits, which causes stampede injuries and deaths. To explore this dual-mechanism dynamics, we build the agent-based modeling of particle system. Civilians lose blood when attacked, which is the attack mechanism, or crashed and trampled by others civilians, which is the stampede mechanism. For all civilians, the blood variable determines the physical status of being strong, healthy, weak, injured, and dead. Five key factors, such as the perception range, the number of Exits, the density of civilians, the number of terrorists, and attack strategies, are introduced into the model. We run each simulation repeatedly for multiple times and take the averaged survival rate, attack death, and stampede death as robust outcomes. The collision damage has the phase transition effects between stampede and attack deaths. The perception range R have the peak effect on the survival rate and the trough effect on both stampede and attack deaths. It expands understandings of human behavior dynamics, and helps to predict the dynamics and outcomes in advance. The optimal perception range can be solved accordingly, to practically guide the public facility planning and regular emergency training in real-life. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				OCT 12	2020	205								106291	10.1016/j.knosys.2020.106291													
J								A novel group recommender system based on members' influence and leader impact	KNOWLEDGE-BASED SYSTEMS										Group recommender systems; Leader's impact; Members' influence; Trust; Fuzzy C-means	TRUST; PERFORMANCE; ALGORITHMS	Group recommender systems have been designed which, instead of suggesting one or more items to people individually, concurrently recommend them to a group of people who have a common interest, with a view to satisfying each of them. One of the most important issues in these systems is social relationships and the influence of individuals on each other in groups. In this article, a new method has been proposed to compute members' influence on each other based on similarity and trust. Normally in groups, there are some people called Leaders who are trusted more than other members and have a significant impact on the members. Therefore, this study has attempted to compute the leader's impact on the members' preferences. One remarkable aspect of this method is the use of a combination of fuzzy clustering and similarity measure to find users who have similar interests. Furthermore, an implicit trust metric has been formulated to improve the efficiency of the influence process and leader identification. Eventually, the proposed method which has been evaluated utilizing a MovieLens 1001k dataset showed significant results by MAE, RMSE, Precision, and a group-satisfactionmeasure compared to state-of-the-art techniques. Further, the proposed trust metric has shown better efficiency compared to some state-of-the-art methods. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				OCT 12	2020	205								106296	10.1016/j.knosys.2020.106296													
J								A reliable version of choquistic regression based on evidence theory	KNOWLEDGE-BASED SYSTEMS										Belief functions; Choquistic regression; Choquet integral; Logistic regression; Monotonic classification; Reliable classification; Epistemic uncertainty; Nonlinear models	BELIEF FUNCTIONS; CALIBRATION; CLASSIFICATION; CLASSIFIERS; PREDICTION; SETS	Choquistic regression is an elegant generalisation of logistic regression, which preserves its monotonicity whilst alleviating its linearity. However, much as logistic regression, it lacks self-awareness, that is, an ability to represent the ignorance (aka epistemic uncertainty) involved in its predictions, which is crucial in safety-critical classification problems. Recently, an extension of logistic regression was introduced to remedy this issue for this latter classifier. This extension is formalised within evidence theory and relies in particular on a sound method for statistical inference and prediction developed in this framework. In this paper, a similar extension is derived for choquistic regression. The usefulness of the obtained approach is confirmed empirically in classification problems where cautiousness in decision-making is allowed. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				OCT 12	2020	205								106252	10.1016/j.knosys.2020.106252													
J								A new COVID-19 Patients Detection Strategy (CPDS) based on hybrid feature selection and enhanced KNN classifier	KNOWLEDGE-BASED SYSTEMS										COVID-19; Classification; KNN; Feature selection	ALGORITHM	COVID-19 infection is growing in a rapid rate. Due to unavailability of specific drugs, early detection of (COVID-19) patients is essential for disease cure and control. There is a vital need to detect the disease at early stage and instantly quarantine the infected people. Many research have been going on, however, none of them introduces satisfactory results yet. In spite of its simplicity, K-Nearest Neighbor (KNN) classifier has proven high flexibility in complex classification problems. However, it can be easily trapped. In this paper, a new COVID-19 diagnose strategy is introduced, which is called COVID-19 Patients Detection Strategy (CPDS). The novelty of CPDS is concentrated in two contributions. The first is a new hybrid feature selection Methodology (HFSM), which elects the most informative features from those extracted from chest Computed Tomography (CT) images for COVID-19 patients and non COVID-19 peoples. HFSM is a hybrid methodology as it combines evidence from both wrapper and filter feature selection methods. It consists of two stages, namely; Fast Selection Stage (FS2) and Accurate Selection Stage (AS(2)). FS2 relies on filter, while AS(2) uses Genetic Algorithm (GA) as a wrapper method. As a hybrid methodology, HFSM elects the significant features for the next detection phase. The second contribution is an enhanced K-Nearest Neighbor (EKNN) classifier, which avoids the trapping problem of the traditional KNN by adding solid heuristics in choosing the neighbors of the tested item. EKNN depends on measuring the degree of both closeness and strength of each neighbor of the tested item, then elects only the qualified neighbors for classification. Accordingly, EKNN can accurately detect infected patients with the minimum time penalty based on those significant features selected by HFSM technique. Extensive experiments have been done considering the proposed detection strategy as well as recent competitive techniques on the chest CT images. Experimental results have shown that the proposed detection strategy outperforms recent techniques as it introduces the maximum accuracy rate. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				OCT 12	2020	205								106270	10.1016/j.knosys.2020.106270													
J								Community detection based on the Matthew effect	KNOWLEDGE-BASED SYSTEMS										Community detection; Matthew effect; Complex network; Cluster		Community structure exists in most real-world networks, such as social networks, smart grids, and transportation networks. Established approaches for community detection usually depend on some user-defined criteria (e.g., minimum cut, normalized cut, modularity, etc.). These criteria-based methods usually involve some optimization procedures and need to specify some parameters, which are thus time consuming and sensitive to parameters. In this paper, inspired by the Mathew effect of human society, we view a network as a social system and design a new algorithm called CDME (community detection based on the Matthew effect). Relying on the new concept, CDME has many desirable properties. It allows uncovering high-quality communities driven by dynamic CDME is also parameter free. More importantly, since CDME works in a local way and only needs to calculate the attractiveness of neighboring nodes, which lend itself to handling large-scale networks. Experiments on both synthetic and real-world data sets have demonstrated that CDME has many benefits and outperforms many state-of-the-art algorithms. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				OCT 12	2020	205								106256	10.1016/j.knosys.2020.106256													
J								A unified multi-level spectral-temporal feature learning framework for patient-specific seizure onset detection in EEG signals	KNOWLEDGE-BASED SYSTEMS										Electroencephalography (EEG); Seizure onset detection; Multi-domain feature extraction; Deep learning; Multivariate multiscale sample entropy; Common spatial pattern	WAVELET TRANSFORM; CLASSIFICATION; ALGORITHM	Epileptic seizure onset detection in electroencephalography (EEG) signals is a challenging task due to the severe variation of seizures. Recently, automatic seizure onset detection frameworks fail to fully consider both nonstationary and stochastic characteristics of EEGs in nature, which may lead to information default and further produce suboptimal recognition performance consequently. In this work, we propose a patient-specific seizure onset detection method based on fully exploration of auxiliary supplementary spectral-temporal information in EEG signals. Specifically, prior to feature extraction procedure, EEG signals are firstly decomposed into 5 groups of coefficients at different levels based on the clinical interest. Representative feature in temporal-domain, which is a translation of the nonlinear property of EEG signals, is then extracted by a combination of principal component analysis and common spatial pattern (PCA-CSP) and multivariate multiscale sample entropy (MMSE) in parallel and dimensionally reduced by a tree-based feature selection algorithm. Supplementary information in spectral-domain is further explored by the proposed unified maximum mean discrepancy autoencoder (uMMD-AE). Finally, an optimal combination of features above is identified and fed into a series of support vector machine classifiers with a decision fusion module for the intelligent recognition of epileptic EEGs. The proposed method achieves an average sensitivity, latency and false detection rate of 97.2%, 1.10s and 0.64/h respectively on Children Hospital Boston-Massachusetts Institute of Technology (CHB-MIT) Scalp EEG Database. Competitive experimental results demonstrate the efficacy of the proposed unified multi-level spectral-temporal feature learning framework in epileptic EEG recognition, validating its effectiveness in the automatic patient-specific seizure onset detection. (C) 2020 Published by Elsevier B.V.																	0950-7051	1872-7409				OCT 12	2020	205								106152	10.1016/j.knosys.2020.106152													
J								Lightweight image super-resolution with enhanced CNN	KNOWLEDGE-BASED SYSTEMS										Image super-resolution; CNN; Lightweight enhanced network; Enhancement and compression; Information refinement		Deep convolutional neural networks (CNNs) with strong expressive ability have achieved impressive performances on single image super-resolution (SISR). However, their excessive amounts of convolutions and parameters usually consume high computational cost and more memory storage for training a SR model, which limits their applications to SR with resource-constrained devices in real world. To resolve these problems, we propose a lightweight enhanced SR CNN (LESRCNN) with three successive sub-blocks, an information extraction and enhancement block (IEEB), a reconstruction block (RB) and an information refinement block (IRB). Specifically, the IEEB extracts hierarchical low-resolution (LR) features and aggregates the obtained features step-by-step to increase the memory ability of the shallow layers on deep layers for SISR. To remove redundant information obtained, a heterogeneous architecture is adopted in the IEEB. After that, the RB converts low-frequency features into highfrequency features by fusing global and local features, which is complementary with the IEEB in tackling the long-term dependency problem. Finally, the IRB uses coarse high-frequency features from the RB to learn more accurate SR features and construct a SR image. The proposed LESRCNN can obtain a high-quality image by a model for different scales. Extensive experiments demonstrate that the proposed LESRCNN outperforms state-of-the-arts on SISR in terms of qualitative and quantitative evaluation. The code of LESRCNN is accessible on https://github.com/hellloxiaotion/LERCNN. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				OCT 12	2020	205								106235	10.1016/j.knosys.2020.106235													
J								Hyper-heuristic local search for combinatorial optimisation problems	KNOWLEDGE-BASED SYSTEMS										Hyper-heuristic; Local search algorithm; Google machine reassignment; Multi-capacity bin packing	2-DIMENSIONAL VECTOR PACKING; BIN-PACKING; ALGORITHM	Local search algorithms have been successfully used for many combinatorial optimisation problems. The choice of the most suitable local search algorithm is, however, a challenging task as their performance is highly dependent on the problem characteristic. In addition, most of these algorithms require users to select appropriate internal neighbourhood structures to obtain desirable performance. No single local search algorithm can consistently perform well with a fixed setting, for different types of problems or even different instances of the same problem. To address this issue, we propose a hyper-heuristic framework which incorporates multiple local search algorithms and a pool of neighbourhood structures. This framework is novel in three respects. Firstly, a two-stage hyper-heuristic structure is designed to control the selection of a local search algorithm and its internal operators. Secondly, we propose an adaptive ranking mechanism to choose the most appropriate neighbourhood structures for the current local search algorithm. The proposed mechanism uses the entropy to evaluates the contribution of the local search in terms of quality and diversity. It adaptively adjusts the pool of candidate neighbourhood structures. Thirdly, we use a population of solutions within the proposed framework to effectively navigate different areas in the solutions search space and share solutions with local search algorithms. To ensure different solutions is allocated in different regions of the search space, we propose a distance-based strategy for population updating process that allowing solutions to share local search algorithms. We have evaluated the performance of the proposed framework using two challenging optimisation problems: Multi-Capacity Bin Packing benchmark instances and Google Machine Reassignment benchmark instances. The results show the effectiveness of the proposed framework, which outperformed state-of-the-art algorithms on several problem instances. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				OCT 12	2020	205								106264	10.1016/j.knosys.2020.106264													
J								Verify and measure the quality of rule based machine leaning	KNOWLEDGE-BASED SYSTEMS										Rule-based machine learning; Consistency; Uncertainty; Many-valued logic; Automated reasoning	MEASURING INCONSISTENCY; CLASSIFICATION; VALIDATION; INFERENCE	In recent years, explainable AI has been gaining great attention, and there is a surge of interest in studying how prediction models work and how to provide formal guarantees for the models. Rule based machine learning (RBML), which aims to automatically identify and learn a set of relational rules that collectively represent the knowledge captured by the system, are a popular class of techniques in machine learning and data mining. Since inconsistencies in the rule base learnt can have a significant, negative impact on how the system will perform and on the conclusions that it will reach, the present work addresses the issues of verification and evaluation of consistency of rule base resulted from machine learning or domain expert using the logic based automated reasoning method. The main contribution consists of two parts. The first one focused on the consistency of rule base in the classical logic sense, which can be transformed into conjunctive normal form, so the consistency of rule base learnt can be verified via a resolution based automated reasoning method. Due to the uncertainty inevitably included in the rule-base during the learning process, the more detailed work has been presented in the second part, i.e., focused on providing a formal foundation of RBML under uncertainty in order to support logical analysis, verify and measure the consistency degree of the rule-base under uncertainty based on many-valued logic automated reasoning framework and algorithms. Some examples are also provided in both parts to illustrate the feasibility and effectiveness of the present work. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				OCT 12	2020	205								106300	10.1016/j.knosys.2020.106300													
J								Deep clustering by maximizing mutual information in variational auto-encoder	KNOWLEDGE-BASED SYSTEMS										Unsupervised learning; Representation learning; Deep clustering; Variational auto-encoder; Mutual information		Unsupervised clustering, which is extensively employed in deep learning and computer vision as a fundamental technique, has attracted much attention in recent years. Deep embedding clustering often uses auto-encoders to learn representations for clustering. However, auto-encoders tend to corrupt the learning representations when simultaneously learning embedded representations and performing clustering. In this paper, we propose a Deep Clustering via Variational Auto-Encoder (DC-VAE) of mutual information maximization. First, we formulate the deep clustering problem as learning soft cluster assignments within the framework of variational auto-encoder. Second, we impose mutual information maximization on the observed data and the representations to prevent soft cluster assignments from distorting learning representations. Third, we derive a new generalization evidence lower bound objects related to several previous models and introduce parameters to balance learning informative representations and clustering. It is shown that the proposed model can significantly boost the performance of clustering by learning effective and reliable representations for downstream machine learning tasks. Through experimental results on several datasets, we demonstrate that the proposed model is competitive with existing state-of-the-arts on multiple performance metrics. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				OCT 12	2020	205								106260	10.1016/j.knosys.2020.106260													
J								Meta-knowledge dictionary learning on 1-bit response data for student knowledge diagnosis	KNOWLEDGE-BASED SYSTEMS										Dictionary learning; Latent knowledge analysis; Student knowledge based-system; Cognitive diagnosis; Student clustering and classification	DINA MODEL; MATRIX	This paper focuses on the problem of student knowledge diagnosis that is a basic task of realizing personalized education. Most traditional methods rely on the question-concept matrix empirically designed by experts. However, the expert concepts are expensive and inter-overlapping in their constructions, leading to ambiguous explanations. With the intuition that each student can master a part of the knowledge involved in all questions, in this paper, we propose a novel learning-based model for student knowledge diagnosis, dubbed Meta-knowledge Dictionary Learning (metaDL). MetaDL aims to learn a meta-knowledge dictionary from student responses, where any knowledge entity (e.g., student, question or expert concept) is a linear combination of a few atoms in the meta-knowledge dictionary. The resultant problem could be effectively solved by developing the alternating direction method of multipliers. This study has three innovations: learning independent meta-knowledges instead of traditional complex concepts, sparely representing knowledge entity instead of densely weighted representation, and interpreting expert concepts with the resulting meta-knowledges. For evaluation, the diagnosis results from metaDL are used to group students and predict responses on two public datasets and a private dataset from our institution. The experiment results show that metaDL delivers an effective student knowledge diagnosis and then results in good performances on the two applications in comparison with other methods. This technique could provide significant insights into student's knowledge state and facilitate the progress on personalized education. (C) 2020 Published by Elsevier B.V.																	0950-7051	1872-7409				OCT 12	2020	205								106290	10.1016/j.knosys.2020.106290													
J								SK-GCN: Modeling Syntax and Knowledge via Graph Convolutional Network for aspect-level sentiment classification	KNOWLEDGE-BASED SYSTEMS										Aspect-level; Sentiment analysis; Graph Convolutional Network (GCN); Commonsense knowledge graph		Aspect-level sentiment classification is a fundamental subtask of fine-grained sentiment analysis. The syntactic information and commonsense knowledge are important and useful for aspect-level sentiment classification, while only a limited number of studies have explored to incorporate them via flexible graph convolutional neural networks (GCN) for this task. In this paper, we propose a new Syntax- and Knowledge-based Graph Convolutional Network (SK-GCN) model for aspect-level sentiment classification, which leverages the syntactic dependency tree and commonsense knowledge via GCN. In particular, to enhance the representation of the sentence toward the given aspect, we develop two strategies to model the syntactic dependency tree and commonsense knowledge graph, namely SK-GCN(1) and SK-GCN(2) respectively. SK-GCN(1) models the dependency tree and knowledge graph via Syntax-based GCN (S-GCN) and Knowledge-based GCN (K-GCN) independently, and SK-GCN(2) models them jointly. We also apply pre-trained BERT to this task and obtain new state-of-the-art results. Extensive experiments on five benchmark datasets demonstrate that our approach can effectively improve the performance of aspect-level sentiment classification compared with the state-of-the-art methods. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				OCT 12	2020	205								106292	10.1016/j.knosys.2020.106292													
J								Deep Flexible Structured Spatial-Temporal Model for Taxi Capacity Prediction	KNOWLEDGE-BASED SYSTEMS										Deep learning; Traffic prediction; Spatial-temporal dependencies	NETWORKS	The prevalence of taxi-hailing applications has brought great convenience to urban travel. People can not only get a taxi anytime and anywhere, but also make an appointment for taxi in advance. Therefore, people are concerned about the current taxi capacity (i.e. the number of vacant taxis) around them. Meanwhile, they also want to know the future capacity to help them choose the appointment time to avoid congestion and plan their itinerary. However, most of the exiting studies only aim to help taxi companies to schedule traffic resources, and cannot consider future travel plans for users. In this paper, we propose the Deep Flexible Structured Spatial-Temporal Model (DFSSTM) to tackle the task. In order to explore more sufficient temporal relationship of data, DFSSTM models the temporal dynamics as three views: period, trend and closeness. Then the Siamese Spatial-Temporal Network (SSTN) is designed for each view, which introduces the Siamese architecture to capture the spatial-temporal dependencies of inflows and outflows simultaneously. Finally, DFSSTM automatically weights each view and fuses the outputs of the three views to get the final prediction. Experimental results on real-world datasets show that the proposed approach outperforms state-of-the-art methods. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				OCT 12	2020	205								106286	10.1016/j.knosys.2020.106286													
J								A hybrid Harris hawks-Nelder-Mead optimization for practical nonlinear ordinary differential equations	EVOLUTIONARY INTELLIGENCE										Harris hawks optimization; Nelder-Mead optimization; Analytical and approximate solutions; Residual function; Fourier series	ALGORITHM; DESIGN; SEARCH; FINS	Differential equations can often be seen in many fields of scientific research and engineering. Typically, finding the analytical (exact) solution is expensive task in terms of computational effort and may not an attainable task for some complex tasks. To effectively handle a wide variety of linear and nonlinear differential equations, this paper presents an approximate methodology based on hybrid Harris hawks-Nelder-Mead optimization algorithm with the aim to achieve accurate and reliable solution. The proposed methodology is introduced on basis of Fourier series expansion and Harris hawks-Nelder-Mead optimization algorithm. In this sense, the differential equation is represented as an optimization model by the means of the weighted residual function (cost function) that needed to be minimized, where the boundary and initial conditions of the differential equation are considered as the constraints of the optimization model. The practicality and efficiency of the proposed algorithm are demonstrated through six differential equations with different nature as well as four mechanical engineering differential equations. The comparison against different algorithms, by using the generational distance metric and Wilcoxon sign rank test, showed the effectiveness of the proposed algorithm.																	1864-5909	1864-5917															10.1007/s12065-020-00497-3		OCT 2020											
J								Parameter extraction of solar cell using intelligent grey wolf optimizer	EVOLUTIONARY INTELLIGENCE										Photovoltaic module; Parameter estimation; Single and double diode model; IGWO algorithm	MAXIMUM POWER POINT; ALGORITHM; MODELS; IDENTIFICATION	The focus of power producers has shifted from conventional energy sources to sustainable energy sources because of the depletion of fossil fuels and carbon emission causing global warming and climate change. Solar cells are the most prominent option to deal with these problems. The precise estimation of solar cell parameters is very much required before their installation to achieve high efficiency. In recent years applications of several optimization algorithms for parameter estimation of the solar cell have been addressed. Recently, intelligent grey wolf optimizer (IGWO), which is an advanced version of grey wolf optimizer (GWO) incorporating a sinusoidal truncated function as a bridging mechanism and opposition based learning has been introduced. The wide applicability of this variant has been examined over different conventional benchmark functions and on some real problems. This fact motivated authors to employ this variant on parameter extraction process. The main motivation behind the implementation of IGWO on solar cell parameter estimation process is the efficiency of this version to deal with complex optimization problems. To estimate the PV cell parameter values, measurement of voltage and current are considered at three important points. These are open circuit point, short circuit point and maximum power point, for two solar cell representative models i.e. single diode model and double diode model. Results of IGWO are compared with the results of other variants of GWO on these two models and for three films (Mono crystalline, poly crystalline and thin film). Results reveal that IGWO produces better results.																	1864-5909	1864-5917															10.1007/s12065-020-00499-1		OCT 2020											
J								Analysis on intelligent machine learning enabled with meta-heuristic algorithms for solar irradiance prediction	EVOLUTIONARY INTELLIGENCE										Power distribution system; Solar panel; Solar irradiance prediction; Statistical features; Optimized machine learning algorithms; Hybrid optimization	MODEL; CONSTRUCTION	The solar forecasting is an effective method to enhance the operation of an electrical system for merging a large amount of solar power generation systems and intends to expand a new empirical method to model the prediction uncertainty of the solar irradiance. The proposed model comprises three phases, such as (a) Data Acquisition, (b) Feature Extraction, and (c) Prediction. Initially, benchmark data available from local meteorological organizations are collected that includes the numerical weather forecasting data like temperature, dew point, humidity, visibility, wind speed, and other descriptive information. Once the data is collected, feature extraction is done by first-order and second-order statistical models. First Order Statistics, like mean, median, standard deviation, the maximum value of entire data, and minimum value of entire data, and Second-Order Statistics, like Kurtosis, skewness, correlation, and entropy are extracted as the features. These features are further applied to three machine learning algorithms named Multilayer Perceptron (MLP), Convolutional Neural Network (CNN), and Recurrent Neural Network (RNN). As main novelty of this paper, the number of hidden neurons of all these networks is optimized by a hybrid algorithm merging both the Deer Hunting Optimization Algorithm (DHOA) and Grey Wolf Optimization (GWO), which is named as Grey Updated DHOA (GU-DHOA). The improvement of these networks with the assistance of a hybrid meta-heuristic algorithm will be highly effective for solar irradiance prediction, overcoming the existing machine learning algorithms.																	1864-5909	1864-5917															10.1007/s12065-020-00505-6		OCT 2020											
J								Balanced Graph-based regularized semi-supervised extreme learning machine for EEG classification	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Brain-computer interface; Electroencephalogram; Semi-supervised extreme learning machine; Label-consistency graph; Sample-similarity graph	MANIFOLD	Machine learning algorithms play a critical role in electroencephalograpy (EEG)-based brain-computer interface (BCI) systems. However, collecting labeled samples for classifier training and calibration is still difficult and time-consuming, especially for patients. As a promising alternative way to address the problem, semi-supervised learning has attracted much attention by exploiting both labeled and unlabeled samples in the training process. Nowadays, semi-supervised extreme learning machine (SS-ELM) is widely used in EEG classification due to its fast training speed and good generalization performance. However, the classification performance of SS-ELM largely depends on the quality of sample graph. The graphs of most semi-supervised algorithms are constructed by the similarity between labeled and unlabeled data called manifold graph. The more similar the structural information between samples, the greater probability they belong to the same class. In this paper, the label-consistency graph (LCG) and sample-similarity graph (SSG) are combined to constrain the model output. When the SSG is not accurate enough, the weight of LCG needs to be increased, and vice versa. The weight ratio of two graphs is optimized to obtain an optimal adjacency graph, and finally the best output weight vector is achieved. To verify the effectiveness of the proposed algorithm, it was validated and compared with several existing methods on two real datasets: BCI Competition IV Dataset 2a and BCI Competition III Dataset 4a. Experimental results show that our algorithm has achieved the promising results, especially when the number of labeled samples is small.																	1868-8071	1868-808X															10.1007/s13042-020-01209-0		OCT 2020											
J								Beyond traditional approaches: a partial directed coherence with graph theory-based mental load assessment using EEG modality	NEURAL COMPUTING & APPLICATIONS										Mental load assessment; CONNECTIVITY APPROACH; Electroencephalography; Partial directed coherence; Graph theory	COGNITIVE-LOAD; BRAIN NETWORKS; FUNCTIONAL CONNECTIVITY; WORKED EXAMPLES; TASK; CHALLENGES	Brain connectivity-based methods are efficient and reliable for assessing the mental workload during high task demands as the human brain is functionally interconnected during any psychological task. On the other hand, the graph theory approach is a mathematical study that draws the pairwise relationships between objects. This paper covers the deployment of graph theory concepts on the brain connectivity methods to find the complex underlying behaviors of the brain in the simplest way. Furthermore, in this work, mental workload assessments on multimedia animations were performed using a brain connectivity approach based on partial directed coherence (PDC) with graph theory analysis. Electroencephalography (EEG) data were collected from 34 adult participants at baseline and during multimedia learning tasks. The results revealed that the EEG-based connectivity approach with graph theory offers more promising results than the traditional feature extraction techniques. The connectivity approach achieved an accuracy of 85.77% in comparison with the 78.50% accuracy achieved by the existing feature extraction techniques. It is concluded that the proposed PDC method with graph theory network analysis is a better solution for cognitive load assessment during any cognitive task.																	0941-0643	1433-3058															10.1007/s00521-020-05408-2		OCT 2020											
J								Cooperative co-evolutionary comprehensive learning particle swarm optimizer for formulation design of explosive simulant	MEMETIC COMPUTING										Explosive simulant; Formulation design; Particle swarm optimizer; Cooperative co-evolutionary	COEVOLUTION	Generally, the actual explosive is not suitable for the training of security personnel due to its danger. Hence, it is significant to create the simulant as similar as possible to the real explosive, where the difficulties are derived from finding safe compounds from the compound database and their related proportion. In this paper, a cooperative co-evolutionary comprehensive learning particle swarm optimizer is proposed to obtain the formulation design of explosive simulant. To be specific, the proposed algorithm employs particle swarm optimization as the optimizer and creates two cooperative populations focusing on finding compounds and their proportions, respectively. Moreover, a comprehensive cooperative strategy is designed to improve the solution diversity and thus enhance the search performance. To the best of our knowledge, this is the first attempt to employ evolutionary algorithm to design explosive simulant formulation. Comprehensive experiments are conducted on several typical explosives and results demonstrate the superiority of the proposed algorithm in comparison to other algorithms.																	1865-9284	1865-9292															10.1007/s12293-020-00314-5		OCT 2020											
J								An experimental approach to evaluate machine learning models for the estimation of load distribution on suspension bridge usingFBGsensors andIoT	COMPUTATIONAL INTELLIGENCE										FBG Sensor; Internet of Things; K-Nearest Neighbor; Random Forest; Structural Health Monitoring	RANDOM FOREST; FIBER; SENSORS; STRAIN; IOT	Most of the tragedies on any bridge structure have been the cause of high-density crowd behavior as a response to trampling as well as the crushing scenario. Therefore, it is most important to monitor such unforeseen situations by sensing the load imposed on the bridge structures. This scenario may arise where crowd movement is huge on these types of bridges. Similarly, the fiber Bragg grating (FBG) is a promising technology for structural health monitoring applications. In this work, an Internet of Things based FBG optical sensing scheme is proposed to monitor real-time strain distribution throughout the bridge structures and localization of load imposed on the structure from a central control room. A suspension bridge model is designed by referring to a real bridge scenario and these FBG sensors are deployed to validate the proposed machine learning models. In this article, the performances of two machine learning strategies are discussed for the accurate estimation of load and its position by acquiring high sensitive FBG sensors signals at a very high data rate. The algorithms include K-nearest neighbor (KNN) and random forest (RF); which are applied on each sensing data source, and then validated using a prototype suspension bridge model integrated with three FBG sensors (1532 nm, 1538 nm, and1541 nm) on a single optical fiber cable.																	0824-7935	1467-8640															10.1111/coin.12406		OCT 2020											
J								Learning region-guided scale-aware feature selection for object detection	NEURAL COMPUTING & APPLICATIONS										Scale variation; Object detection; RoI Pyramid; Scale-aware feature selective		Scale variation is one of the major challenges in object detection task. Modern region-based object detection architectures often adopt Feature Pyramid Network (FPN) as feature extraction neck to achieve multi-scale feature representation in solving scale variation problem. However, due to the rough feature selection strategy in Region of Interest (RoI) feature extraction step, these methods might not perform well on object detection under strong scale variation. In this work, we are motivated by the limitations of current FPN-based two-stage object detectors and then present a novel module, namely scale-aware feature selective (SAFS) module, that flexibly and adaptively selects feature levels in two-stage object detectors. Specifically, we firstly build the RoI Pyramid in standard FPN structure to extract RoI features from various scale levels. Next, in order to achieve scale-aware mechanism for solving scale variation issue, we develop a novel weighting gate function containing one set of trainable parameters to automatically learn the fusion weight for each RoI feature level, which relieves the limitation of hard feature selection strategy guided by online instance size. Outputs from the RoI features with the learned weights are fused for classification and bounding box regression. Furthermore, we design a multi-level SAFS architecture to obtain different types of RoI feature combinations that ensures our method is more robust to various instance scales. Experimental results show that our SAFS module is very compatible with most of two-stage object detectors and could achieve state-of-the-art results with Average Precision of 48.3 on COCOtest-devand other popular object detection benchmarks. Our code will be made publicly available.																	0941-0643	1433-3058															10.1007/s00521-020-05400-w		OCT 2020											
J								Data mining technology of computer testing system for intelligent machining	NEURAL COMPUTING & APPLICATIONS										Data mining; Test system; Sequential pattern mining; Cluster analysis		This paper aims at how to turn the stored data into useful information, dig out the deep information of process operation, and improve the process monitoring ability by using this information. Based on the research topic of large-range linear accelerometer detection technology and data mining, this paper improves the traditional data mining methods to different degrees according to the characteristics of different detection objects, and puts forward some new detection data analysis and processing and fault diagnosis and prediction methods. Through the analysis of the density-based algorithm and the grid-based algorithm, this paper proposes an equivalent rule of dense cell recognition and density-based object search and thus proposes a grid-based and density-based clustering algorithm CLIGRID. This algorithm achieves fast clustering by clustering in stages and selecting seed objects to extend the class, thus reducing the number of regional queries and I/O overhead. Based on the DBSCAN algorithm of parameter selection difficult and hard to find the problem of large cluster density difference, put forward the multidimensional value of worshiping improved algorithms of DBSCAN algorithm using grid map density distribution of density matrix, automatically determine the density level classification, through a multidimensional hierarchical clustering process with multiple density more careful in the level of clustering results, solved by using global epsilon value clustering quality deterioration. The comparison results show that the efficiency of CLIGRID algorithm is significantly higher than DBSCAN algorithm, and the execution time of the algorithm is basically linear with the number of data points. Experiments show that when the data volume is less than 10,000, the efficiency of the algorithm is slightly lower than DBSCAN algorithm, because the representative core unit occupies a large proportion (64%) in all the dense units, while the seed objects in the representative core unit participate in the operation in the two stages of clustering, which increases the running time. With the increase in the data set and the decrease in the proportion of core elements (26%), the effect of grid clustering on the improvement of algorithm efficiency gradually becomes apparent.																	0941-0643	1433-3058															10.1007/s00521-020-05369-6		OCT 2020											
J								A novel audio watermarking scheme using ensemble-based watermark detector and discrete wavelet transform	NEURAL COMPUTING & APPLICATIONS										Audio watermarking; Discrete wavelet transform; Support vector machine; K-nearest neighbor; Machine learning	HIGH-CAPACITY; ROBUST; DCT; DECOMPOSITION; TRANSPARENT; SYNERGY	Most existing extraction techniques in audio watermarking use conventional techniques in which some sets of special rules based on reverse embedding rules are used for watermark extraction and have many weaknesses, like very low robustness to destructive attacks. To overcome this problem, the use of machine learning-based methods has increased in recent years in this field. The disadvantage of these methods is the high reliance on a unique classifier and lack of proper efficiency when achieving high capacity, which is a major challenge in audio watermarking. The main purpose of this paper is to present a method that covers the weak points of conventional methods and simple intelligent methods and improves system performance using a synergistic combination of discrete wavelet transform (DWT) and ensemble-intelligent extraction approach by proposed combination of trained machine learning classifiers. For the embedding operation in the proposed method, the DWT and the difference in energy levels obtained through DWT coefficients are used. In the extraction section, three methods are used in parallel: (a) the trained support vector machine (SVM) classifier with RBF kernel, (b) trained SVM classifier with quadratic kernel and (c) the trained K-nearest neighbor classifier; finally, the majority function is used to vote and make a final decision to create an intelligent-based watermark detector. A training set is required to train the classifiers, whose bit sequence is generated by a proposed 5-bit linear-feedback shift register. The results of various experiments indicate that this ensemble method has achieved the appropriate imperceptibility and high capacity, along with higher robustness compared to conventional techniques and individual learning classifiers.																	0941-0643	1433-3058															10.1007/s00521-020-05389-2		OCT 2020											
J								Convex clustering method for compositional data modeling	SOFT COMPUTING										Compositional data analysis; Aitchison geometry; Convex clustering; Alternating direction method of multipliers (ADMM)	ALGORITHM; CRITERIA	Compositional data refer to a vector with parts that are positive and subject to a constant-sum constraint. Examples of compositional data in the real world include a vector with each entry representing the weight of a stock in an investment portfolio, or the relative concentration of air pollutants in the environment. In this study, we developed a Convex Clustering approach for grouping Compositional data. Convex clustering is desirable because it provides a global optimal solution given its convex relaxations of hierarchical clustering. However, when directly applied to compositions, the clustering result offers little interpretability because it ignores the unit-sum constraint of compositional data. In this study, we discuss the clustering of compositional variables in the Aitchison framework with an isometric log-ratio (ilr) transformation. The objective optimization function is formulated as a combination of a L-2-norm loss term and a L-1-norm regularization term and is then efficiently solved using the alternating direction method of multipliers. Based on the numerical simulation results, the accuracy of clustering ilr-transformed data is higher than the accuracy of directly clustering untransformed compositional data. To demonstrate its practical use in real applications, the proposed method is also tested on several real-world datasets.																	1432-7643	1433-7479															10.1007/s00500-020-05355-z		OCT 2020											
J								On argument acceptability change towards legal interpretation dynamics	ARTIFICIAL INTELLIGENCE AND LAW										Legal interpretation; Belief revision; Argumentation dynamics; Legal dynamics	REVISION; LOGIC; FRAMEWORK	We propose a formal theory built upon an abstract argumentation framework for handling argumentation dynamics. To that end, we analyze the acceptability dynamics of arguments through the proposal of two different kinds of sets of arguments which are somehow responsible for the acceptability/rejection of a given argument. We develop a study of the consequences of breaking the construction of such sets towards the acceptance of an analyzed argument. This brings about the proposal of a novel change operation which allows to determine which arguments should be removed from the framework so that another particular argument becomes accepted. Finally, the proposed model is formalized in the light of the theory of belief revision by characterizing the corresponding operations through constructive definition and an axiomatic characterization, connecting them through the corresponding representation theorem. The theoretical proposal constitutes the fundamentals for a system implementation in many dynamic domains of application. In particular, we show its application for handling the dynamics of legal interpretation. In that sense, this proposal constitutes a fundamental approach and theoretical justification to handle the dynamics of legal arguments through changes of interpretative canons. We show a possible concretisation of our abstract theory for the legal domain by analysing a real legal case from the Argentinean jurisprudence. Such a system would be capable of suggesting alternative critical points in the current state of affairs of a legal case towards pursuing a specific goal for which the case is being investigated.																	0924-8463	1572-8382															10.1007/s10506-020-09277-x		OCT 2020											
J								An ILM-cosine transform-based improved approach to image encryption	COMPLEX & INTELLIGENT SYSTEMS										CT-ILM; Random order substitution; Image encryption; CC; MSE	MAP; CRYPTANALYSIS	The chaos-based cryptography techniques are used widely to protect digital information from intruders. The chaotic systems have some of special features that make them suitable for the purpose of encryption. These systems are highly unpredictable and are highly sensitive or responsive to the initial conditions, also known as butterfly effect. This sensitive dependence on initial conditions make these systems to exhibit an intricate dynamical behaviour. However, this dynamical behaviour is not much complex in simple one-dimensional chaotic maps. Hence, it becomes easy for an intruder to predict the contents of the message being sent. The proposed work in this paper introduces an improved method for encrypting images, which uses cosine transformation of 3-D Intertwining Logistic Map (ILM). The proposed approach has been split into three major parts. In the first part, Secure Hash Function-256 (SHA-256) is used with cosine transformed ILM (CT-ILM) to generate the chaotic sequence. This chaotic sequence is used by high-efficiency scrambling to reduce the correlations between the adjacent pixels of the image. In the second part, the image is rotated to move all the pixels away from their original position. In the third part, random order substitution is applied to change the value of image pixels. The effectiveness of the proposed method has been tested on a number of standard parameters such as correlation coefficient, Entropy and Unified average change in intensity. The proposed approach has also been tested for decryption parameters like mean square error and peak signal to noise ratio. It can easily be observed from the obtained results that the proposed method of image encryption is more secure and time efficient than some earlier proposed techniques. The approach works for both color and grey scale images.																	2199-4536	2198-6053															10.1007/s40747-020-00201-z		OCT 2020											
J								Gas outburst prediction model using rough set and support vector machine	EVOLUTIONARY INTELLIGENCE										Gas outburst; Rough set theory; Support vector machine; Particle swarm optimization; Prediction		This paper is concerned with the problem of gas outburst prediction in coal mine working face. To predict the gas outburst accurately, this paper uses the rough set theory (RS) and support vector machine (SVM) to establish the prediction model. Firstly, based on the analysis of influencing factors of gas outburst, 10 factors including coal thickness variations, geological structures and gas change are selected as the influencing factors. By using the attribute reduction algorithm to eliminate redundant information, the gas outburst influencing factors as input to the prediction model are reduced from 10 to 6 in decision table. Secondly, by applying the particle swarm optimization (PSO) algorithm to optimize penalty parameter and kernel function of SVM and improve the generalization performance of model, the nonlinear relationship between main influencing factors and intensity of gas outburst is established. Finally, 60 sets of data of Jiulishan Coal Mine in Henan are used as training and testing samples to verify the proposed prediction model, and the discriminant results is compared with that of RBF model and SVM model. The results show that the prediction accuracy of the proposed model is 93%, which is improved compared with the other two models. The RS-PSOSVM model can reduce data redundancy, avoid the model to fall into the local extremum, and can predict the risk level of gas outburst effectively.																	1864-5909	1864-5917															10.1007/s12065-020-00507-4		OCT 2020											
J								DSVD-autoencoder: A scalable distributed privacy-preserving method for one-class classification	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										autoencoder; big data; distributed learning; neural network; one-class classification; privacy-preserving; singular value decomposition	AUTOASSOCIATIVE NEURAL-NETWORKS; NOVELTY DETECTION; ALGORITHM; MACHINE; SVD	One-class classification has gained interest as a solution to certain kinds of problems typical in a wide variety of real environments like anomaly or novelty detection. Autoencoder is the type of neural network that has been widely applied in these one-class problems. In the Big Data era, new challenges have arisen, mainly related with the data volume. Another main concern derives from Privacy issues when data is distributed and cannot be shared among locations. These two conditions make many of the classic and brilliant methods not applicable. In this paper, we present distributed singular value decomposition (DSVD-autoencoder), a method for autoencoders that allows learning in distributed scenarios without sharing raw data. Additionally, to guarantee privacy, it is noniterative and hyperparameter-free, two interesting characteristics when dealing with Big Data. In comparison with the state of the art, results demonstrate that DSVD-autoencoder provides a highly competitive solution to deal with very large data sets by reducing training from several hours to seconds while maintaining good accuracy.																	0884-8173	1098-111X															10.1002/int.22296		OCT 2020											
J								Adversarially regularized medication recommendation model with multi-hop memory network	KNOWLEDGE AND INFORMATION SYSTEMS										Medication recommendation; Drug-drug interactions; Memory networks; Adversarially regularization		Medication recommendation is attracting enormous attention due to its promise in effectively prescribing medicines and improving the survival rate of patients. Among all challenges, drug-drug interactions (DDI) related to undesired duplication, antagonism, or alternation between drugs could lead to fatal side effects. Previous researches usually provide models with DDI knowledge to achieve DDI reduction. However, the mixed use of patients with different DDI rates places stringent requirements on the generalization performance of models. In pursuit of a more effective method, we propose the adversarially regularized model for medication recommendation (ARMR). Specifically, ARMR firstly models temporal information from medical records to obtain patient representations and builds a key-value memory network based on information from historical admissions. Then, ARMR carries out multi-hop reading on the memory network to recommend medications. Meanwhile, ARMR uses a GAN model to adversarially regulate the distribution of patient representations by matching the distribution to a desired Gaussian distribution to achieve DDI reduction. Comparative evaluations between ARMR and baselines show that ARMR outperforms all baselines in terms of medication recommendation, achieving DDI reduction regardless of numbers of DDI types being considered.																	0219-1377	0219-3116															10.1007/s10115-020-01513-9		OCT 2020											
J								Mental imagery classification using one-dimensional convolutional neural network for target selection in single-channel BCI-controlled mobile robot	NEURAL COMPUTING & APPLICATIONS										Brain-computer interface (BCI); Convolutional neural network (CNN); Electroencephalogram (EEG); Mental imagery	EEG; RECOGNITION; INTERFACE; SYSTEM	This paper introduces the use of the one-dimensional convolutional neural network (1D-CNN) for end-to-end EEG decoding with application towards a BCI system with a shared control scheme. In general, subjects wearing a single-channel EEG electrode located atF(8)(10-20 international standards) were required to perform mental tasks by mentally visualising the rotation of a star and mind relaxation at a specific time and by robot orientation. The visualisation of a rotating star suggests that the mobile robot is currently oriented towards a target, thus enabling target selection. We showed that proposed classifier obtained the best accuracy of 92.09% in classifying the subject's performing mental rotation task or mental relaxation when compared with conventional classification methods such as support vector machine-75.69%,Kth-nearest neighbour-65.50% and linear discriminant analysis-65.20%. Furthermore, different from conventional methods, the use of 1D-CNN enables end-to-end learning, that is the automatic decoding of EEG signals without requiring feature selection or extraction. To validate that the proposed classifier performs better than conventional methods, the extracted kernel weights of proposed 1D-CNN filters were visualised as a temporal plot, and spectral analysis was performed on the extracted weights. The obtained results confirmed that the proposed 1D-CNN was able to generate filters that resemble the EEG wave patterns of different frequencies and spectral analysis confirmed that the filters exploited information from multiple frequency bands (such as alpha band and beta band) that are often associated with a heightened mental state when performing mental tasks.																	0941-0643	1433-3058															10.1007/s00521-020-05393-6		OCT 2020											
J								An intelligent and blind dual color image watermarking for authentication and copyright protection	APPLIED INTELLIGENCE										Digital image watermarking; Discrete wavelet transform (DWT); Human visual system (HVS); Singular values decomposition (SVD); Particle swarm optimization (PSO)	DIFFERENTIAL EVOLUTION; DIGITAL WATERMARKING; SVD; SCHEME; ROBUST; ALGORITHM; DCT; DWT; TRANSFORM; SYSTEM	In this paper, a blind dual watermarking scheme for color images is proposed by embedding an invisible robust watermark to protect copyright, as well as a fragile watermark is embedded for image authentication. For the purpose of copyright protection, the robust watermark is embedded into the blue channel of RGB color space based on DWT, HVS and SVD domains using a specialized PSO optimization to balance the trade-off between robustness and imperceptibility. In addition, the robust watermarking capacity in SVD is doubled by inserting two robust watermark bits into each selected blocks and the robust watermark can be extracted blindly. For the purpose of authentication, a fragile watermark is embedded into all channels of RGB color space using a new way to manipulate the diagonal singular values. The authenticity of a suspected image can be verified in the absence of original watermark and host images. The combination of robust and fragile watermarking in the proposed scheme provides a suitable mechanism to protect valuable and original color images. According to the experimental and comparative results, the proposed scheme provides superior outcomes with high robustness, imperceptibility, and capacity along with a good accuracy rate in locating the tampered area of an image.																	0924-669X	1573-7497															10.1007/s10489-020-01903-0		OCT 2020											
J								Speeding up distributed pseudo-tree optimization procedures with cross edge consistency to solve DCOPs	APPLIED INTELLIGENCE										Multiagent system; Distributed problem solving; Distributed constraint optimization; DPOP	CONSTRAINT OPTIMIZATION; BREAKOUT; SEARCH; DPOP	The Distributed Pseudo-tree Optimization Procedure (DPOP) is a well-known message passing algorithm that provides optimal solutions to Distributed Constraint Optimization Problems (DCOPs) in cooperative multi-agent systems. However, the traditional DCOP formulation does not consider constraints that must be satisfied (hard constraints), rather it concentrates only on constraints that place no restriction on satisfaction (soft constraints). This is a serious shortcoming as many real-world applications involve both types of constraints. Traditional DPOP algorithms are not able to benefit from the existence of hard constraints, where an additional calculation is required to handle such constraints. This results in longer runtimes. Thus scalability remains an issue. Additionally, in the standard DPOP, the agents are arranged as a Depth First Search (DFS) pseudo-tree, but recent work has shown that the construction of pseudo-trees in this way often leads to chain-like communication structures that greatly impair the algorithm's performance. To address these issues, we develop an algorithm that speeds up the DPOP algorithm by reducing the size of the messages exchanged and increases parallelism in the pseudo tree. For this purpose, initially, we improve the path for exchanging messages. Next, we introduce a new form of constraint propagation, which we call cross-edge consistency. Our theoretical evaluation shows that our proposed algorithm is complete and correct. In empirical evaluations, our algorithm achieves a significant reduction in the runtime, ranging from 4% to 96%, compared to the state-of-the-art.																	0924-669X	1573-7497															10.1007/s10489-020-01860-8		OCT 2020											
J								Automated inspection in robotic additive manufacturing using deep learning for layer deformation detection	JOURNAL OF INTELLIGENT MANUFACTURING										Deep learning; Semantic segmentation; Automated inspection; Material extrusion	CONSTRUCTION; CONCRETE	In this paper, an automated layer defect detection system for construction 3D printing is proposed. Initially, a step-by-step procedure is implemented to develop a deep convolutional neural network that receives images as input and is able to distinguish concrete layers from other surrounding objects through semantic pixel-wise segmentation. Using data augmentation techniques, 1M labeled images are generated and used to train and test the CNN model. Then, a defect detection module is developed which is able to detect deformations in the printed concrete layers extracted from the images using the CNN model. The evaluation results based on metrics such as accuracy, F1 score, and miss rate verify the acceptable performance of the developed system.																	0956-5515	1572-8145															10.1007/s10845-020-01684-w		OCT 2020											
J								Wind turbine blade structural state evaluation by hybrid object detector relying on deep learning models	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Wind turbine; Surveillance drones; Renewable energy; YOLOv3	DAMAGE DETECTION; ALGORITHM; IMPROVE	Surveillance drones are remarkable devices for monitoring, as they have strong spatial and remote sensing capabilities. The prompt detection of peripheral damage to the blades of wind turbines is necessary to reduce downtime and prevent the potential failure of wind farms. Computer vision breakthroughs with deep learning have developed and been refined over time, mainly using convolution neural networks. From this perspective, we suggest a deep learning model for monitoring and diagnosing the blade health of wind turbines based on images captured by surveillance drones. The main limitations of standard monitoring devices are their poor detection accuracy and lack of real-time performance, making it complex to obtain the attributes of blades from aerial images. Based on the foregoing, this study introduces a method for increasing detection accuracy when carrying out operations in real time using You Only Look at Once version 3 (YOLOv3). We train and evaluate three deep learning models on the wind turbine image dataset. We find that many aerial images are unclear because of blurred motion. As avoiding such low-resolution images for training can affect accuracy, we use a super-resolution convolution neural network to reconstruct a blurred picture as a high-resolution one. The computational results demonstrate that YOLOv3 outperforms traditional models in terms of both accuracy and handling time.																	1868-5137	1868-5145															10.1007/s12652-020-02587-7		OCT 2020											
J								Nengo and Low-Power AI Hardware for Robust, Embedded Neurorobotics	FRONTIERS IN NEUROROBOTICS										Nengo; neuromorphic; neurorobotic; spiking neural networks; robotic control; adaptive control; embedded robotics		In this paper we demonstrate how the Nengo neural modeling and simulation libraries enable users to quickly develop robotic perception and action neural networks for simulation on neuromorphic hardware using tools they are already familiar with, such as Keras and Python. We identify four primary challenges in building robust, embedded neurorobotic systems, including: (1) developing infrastructure for interfacing with the environment and sensors; (2) processing task specific sensory signals; (3) generating robust, explainable control signals; and (4) compiling neural networks to run on target hardware. Nengo helps to address these challenges by: (1) providing the NengoInterfaces library, which defines a simple but powerful API for users to interact with simulations and hardware; (2) providing the NengoDL library, which lets users use the Keras and TensorFlow API to develop Nengo models; (3) implementing the Neural Engineering Framework, which provides white-box methods for implementing known functions and circuits; and (4) providing multiple backend libraries, such as NengoLoihi, that enable users to compile the same model to different hardware. We present two examples using Nengo to develop neural networks that run on CPUs and GPUs as well as Intel's neuromorphic chip, Loihi, to demonstrate two variations on this workflow. The first example is an implementation of an end-to-end spiking neural network in Nengo that controls a rover simulated in Mujoco. The network integrates a deep convolutional network that processes visual input from cameras mounted on the rover to track a target, and a control system implementing steering and drive functions in connection weights to guide the rover to the target. The second example uses Nengo as a smaller component in a system that has addressed some but not all of those challenges. Specifically it is used to augment a force-based operational space controller with neural adaptive control to improve performance during a reaching task using a real-world Kinova Jaco(2)robotic arm. The code and implementation details are provided(1), with the intent of enabling other researchers to build and run their own neurorobotic systems.																	1662-5218					OCT 9	2020	14								568359	10.3389/fnbot.2020.568359													
J								Lightweight densely connected residual network for human pose estimation	JOURNAL OF REAL-TIME IMAGE PROCESSING										Human pose estimation; Densely connected residual module; Pyramid fusion; High-resolution net	PICTORIAL STRUCTURES	Most existing methods pay much attention to how to improve the accuracy of human pose estimation results. They usually ignore what the size of their model is. However, besides accuracy, real-time and speed are also important. In this paper, a new module named Densely Connected Residual Module is presented to effectively decrease the number of parameters in our network. We introduce our module into the backbone of High-Resolution Net. In addition, we change direct addition fusion into pyramid fusion at the end of the network. No need for ImageNet pre-training sharply decreases the total time of our training processes. We do our experiments over two benchmark datasets: the COCO keypoint detection dataset and the MPII Human Pose dataset. As a result, we achieve a decrease on number of parameters and calculated amount, respectively by around 72% and 14%, making our network more lightweight than High-Resolution Net. During testing process, our model can predict an image at a speed of 25 ms per image, which also achieves real-time fundamentally. The code has been available at. https://github.com/consistent1997/LDCRN.																	1861-8200	1861-8219															10.1007/s11554-020-01025-3		OCT 2020											
J								S4-SLAM: A real-time 3D LIDAR SLAM system for ground/watersurface multi-scene outdoor applications	AUTONOMOUS ROBOTS										Multi-scene mapping; LIDAR odometry; SLAM; Loop detection; Scan matching	REGISTRATION; ROBUST	For outdoor ground/watersurface multi-scene applications with sparse feature points, high moving speed and high dynamic noises, a real-time 3D LIDAR SLAM system (S4-SLAM) for unmanned vehicles/ships is proposed in this paper, which is composed of the odometry function in front-end and the loop closure function in back-end. Firstly, linear interpolation is used to eliminate the motion distortion caused by robot motions in the data pre-processing step. Two nodes are constructed in the odometry function: the localization node combines the improved Super4PCS with the standard ICP to realize a coarse-to-fine scan matching and outputs the location information of the robot at a high frequency (5 Hz); the correction node introduces a local map with dynamic voxel grid storage structure, which can accelerate the NDT(Normal Distributions Transform) matching process between key-frames and the local map, and then corrects the localization node at a low frequency (1 Hz) to obtain more accurate location information. In the loop closure function, a location-based loop detection approach is introduced and the overlap rate of point clouds is used to verify the loops, so that the global optimization can be carried out to obtain high-precision trajectory and map estimates. The proposed method has been extensively evaluated on the KITTI odometry benchmark and also tested in real-life campus and harbor environments. The results show that our method has low dependence on GPS/INS, high positioning accuracy (with the global drift under 1%) and good environmental robustness.																	0929-5593	1573-7527															10.1007/s10514-020-09948-3		OCT 2020											
J								Uncertain inference network in evidential reasoning	EVOLUTIONARY INTELLIGENCE										Evidential reasoning; Uncertain inference network; Uncertain set; Bayesian network	PROBABILISTIC INFERENCE; ALGORITHM; IMPACT; LOGIC; LAW	As a dominant method in evidential reasoning, Bayesian network has been proved powerful in discrete fields. Although Bayesian network performs reliable in continuous variables and interval estimations, it relies on discretizing continuous variables or building an approximate model to conduct, which causes information loss and accuracy reduction. In order to bridge this gap, this paper introduces two inference rules combined with four inference rules proposed by other scholars. Then we propose a concept of uncertain inference network that consists of six basic structures matching inference rules to represent relationships and logic connection among the evidence. Evidence is represented by uncertain sets that can apply to continuous variables using membership functions to represent vague concepts. Furthermore, a numeric experiment for a forensic investigation of fire incidents is given to compare the results of uncertain inference network and Bayesian network. We found three merits in the case study. First, an uncertain inference network has simpler data access for each node because Bayesian network depends on conditional probability tables while uncertain inference network only relies on membership function. Second, an uncertain inference network has a more wide application because it can perform continuous variables with certain mathematical formulas without discretizing or approximating. Third, an uncertain inference network has a more accurate result because Bayesian network gives a point estimation with a 0-1 value while uncertain inference network conducts an interval estimation with a range value.																	1864-5909	1864-5917															10.1007/s12065-020-00485-7		OCT 2020											
J								When algorithm selection meets Bi-linear Learning to Rank: accuracy and inference time trade off with candidates expansion	INTERNATIONAL JOURNAL OF DATA SCIENCE AND ANALYTICS										Algorithm selection; Bi-linear Learning to Rank; Multi-object evaluation; Candidates expansion		Algorithm selection (AS) tasks are dedicated to find the optimal algorithm for an unseen problem instance. With the knowledge of problem instances' meta-features and algorithms' landmark performances, Machine Learning (ML) approaches are applied to solve AS problems. However, the standard training process of benchmark ML approaches in AS either needs to train the models specifically for every algorithm or relies on the sparse one-hot encoding as the algorithms' representation. To escape these intermediate steps and form the mapping function directly, we borrow the learning to rank framework from Recommender System (RS) and embed the bi-linear factorization to model the algorithms' performances in AS. This Bi-linear Learning to Rank (BLR) has proven to work with competence in some AS scenarios and thus is also proposed as a benchmark approach. Thinking from the evaluation perspective in the modern AS challenges, precisely predicting the performance is usually the measuring goal. Though approaches' inference time also needs to be counted for the running time cost calculation, it's always overlooked in the evaluation process. The multi-objective evaluation metric Adjusted Ratio of Root Ratios (A3R) is therefore advocated in this paper to balance the trade-off between the accuracy and inference time in AS. Concerning A3R, BLR outperforms other benchmarks when expanding the candidates range toTOP3. The better effect of this candidates expansion results from the cumulative optimum performance during the AS process. We take the further step in the experimentation to represent the advantage of suchTOPKexpansion, and illustrate that such expansion can be considered as the supplement for the convention ofTOP1 selection during the evaluation process.																	2364-415X	2364-4168															10.1007/s41060-020-00229-x		OCT 2020											
J								Intuitionistic fuzzy c-means clustering algorithm based on a novel weighted proximity measure and genetic algorithm	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Intuitionistic fuzzy c-means (IFCM) algorithm; Intuitionistic fuzzification; Similarity measure; Correlation coefficient; Genetic algorithm (GA)	SIMILARITY MEASURE; SETS; MODEL	In the era of big data, the research on clustering technologies is a popular topic because they can discover the structure of complex data sets with minimal prior knowledge. Among the existing soft clustering technologies, as an extension of fuzzy c-means (FCM) algorithm, the intuitionistic FCM (IFCM) algorithm has been widely used due to its superiority in reducing the effects of outliers/noise and improving the clustering accuracy. In the existing IFCM algorithm, the measurement of proximity degree between a pair of objects and the determination of parameters are two critical problems, which have considerable effects on the clustering results. Therefore, we propose an improved IFCM clustering technique in this paper. Firstly, a novel weighted proximity measure, which aggregates weighted similarity and correlation measures, is proposed to evaluate not only the closeness degree but also the linear relationship between two objects. Subsequently, genetic algorithms are utilized for identifying the optimal parameters. Lastly, experiments on the proposed IFCM technique are conducted on synthetic and UCI data sets. Comparisons with other approaches in cluster evaluation indexes indicate the effectiveness and superiority of our method.																	1868-8071	1868-808X															10.1007/s13042-020-01206-3		OCT 2020											
J								The neural network collocation method for solving partial differential equations	NEURAL COMPUTING & APPLICATIONS										PDE; Collocation; Meshfree; Basis function	DATA APPROXIMATION SCHEME; MULTIQUADRICS; ALGORITHM	This paper presents a meshfree collocation method that uses deep learning to determine the basis functions as well as their corresponding weights. This method is shown to be able to approximate elliptic, parabolic, and hyperbolic partial differential equations for both forced and unforced systems, as well as linear and nonlinear partial differential equations. By training a homogeneous network and particular network separately, new forcing functions are able to be approximated quickly without the burden of retraining the full network. The network is demonstrated on several numerical examples including a nonlinear elasticity problem. In addition to providing meshfree approximations to strong form partial differential equations directly, this technique could also provide a foundation for deep learning methods to be used as preconditioners to traditional methods, where the deep learning method will get close to a solution and traditional solvers can finish the solution.																	0941-0643	1433-3058															10.1007/s00521-020-05340-5		OCT 2020											
J								Three-dimensional CNN-inspired deep learning architecture for Yoga pose recognition in the real-world environment	NEURAL COMPUTING & APPLICATIONS										Deep learning; 3D CNN; C3D; Yoga pose recognition; Human action recognition		Existing techniques for Yoga pose recognition build classifiers based on sophisticated handcrafted features computed from the raw inputs captured in a controlled environment. These techniques often fail in complex real-world situations and thus, pose limitations on the practical applicability of existing Yoga pose recognition systems. This paper presents an alternative computationally efficient approach for Yoga pose recognition in complex real-world environments using deep learning. To this end, a Yoga pose dataset was created with the participation of 27 individual (8 males and 19 females), which consists of ten Yoga poses, namely Malasana, Ananda Balasana, Janu Sirsasana, Anjaneyasana, Tadasana, Kumbhakasana, Hasta Uttanasana, Paschimottanasana, Uttanasana, and Dandasana. To capture the videos, we used smartphone cameras having 4 K resolution and 30 fps frame rate. For the recognition of Yoga poses in real time, a three-dimensional convolutional neural network (3D CNN) architecture is designed and implemented. The designed architecture is a modified version of the C3D architecture initially introduced for the recognition of human actions. In the proposed modified C3D architecture, the computationally intensive fully connected layers are pruned, and supplementary layers such as the batch normalization and average pooling were introduced for computational efficiency. To the best of our knowledge, this is among the first studies, which utilized the inherent spatial-temporal relationship among Yoga poses for their recognition. The designed 3D CNN architecture achieved test recognition accuracy of 91.15% on the in-house prepared Yoga pose dataset consisting of ten Yoga poses. Furthermore, on the publicly available dataset, the designed architecture achieved competitive test recognition accuracy of 99.39%, along with multifold improvement in the execution speed compared to the existing state-of-the-art technique. To promote further study, we will make the in-house created Yoga pose dataset publicly available to the research community.																	0941-0643	1433-3058															10.1007/s00521-020-05405-5		OCT 2020											
J								An integration of enhanced social force and crowd control models for high-density crowd simulation	NEURAL COMPUTING & APPLICATIONS										High-density crowd simulation; Crowd simulation; Crowd control models; Social force model	EVACUATION; DYNAMICS	Social force model is one of the well-known approaches that can successfully simulate pedestrians' movements realistically. However, it is not suitable to simulate high-density crowd movement realistically due to the model having only three basic crowd characteristics which are goal, attraction, and repulsion. Therefore, it does not satisfy the high-density crowd condition which is complex yet unique, due to its capacity, density, and various demographic backgrounds of the agents. Thus, this research proposes a model that improves the social force model by introducing four new characteristics which are gender, walking speed, intention outlook, and grouping to make simulations more realistic. Besides, the high-density crowd introduces irregular behaviours in the crowd flow, which is stopping motion within the crowd. To handle these scenarios, another model has been proposed that controls each agent with two different states: walking and stopping. Furthermore, the stopping behaviour was categorized into a slow stop and sudden stop. Both of these proposed models were integrated to form a high-density crowd simulation framework. The framework has been validated by using the comparison method and fundamental diagram method. Based on the simulation of 45,000 agents, it shows that the proposed framework has a more accurate average walking speed (0.36 m/s) compared to the conventional social force model (0.61 m/s). Both of these results are compared to the real-world data which is 0.3267 m/s. The findings of this research will contribute to the simulation activities of pedestrians in a highly dense population.																	0941-0643	1433-3058															10.1007/s00521-020-05385-6		OCT 2020											
J								Convolutional neural network-based models for diagnosis of breast cancer	NEURAL COMPUTING & APPLICATIONS										Deep learning; Convolutional neural network; Transfer learning; Breast cancer	CLASSIFICATION	Breast cancer is the most prevailing cancer in the world and each year affecting millions of women. It is also the cause of largest number of deaths in women dying in cancers. During the last few years, researchers are proposing different convolutional neural network models in order to facilitate diagnostic process of breast cancer. Convolutional neural networks are showing promising results to classify cancers using image datasets. There is still a lack of standard models which can claim the best model because of unavailability of large datasets that can be used for models' training and validation. Hence, researchers are now focusing on leveraging the transfer learning approach using pre-trained models as feature extractors that are trained over millions of different images. With this motivation, this paper considers eight different fine-tuned pre-trained models to observe how these models classify breast cancers applying on ultrasound images. We also propose a shallow custom convolutional neural network that outperforms the pre-trained models with respect to different performance metrics. The proposed model shows 100% accuracy and achieves 1.0 AUC score, whereas the best pre-trained model shows 92% accuracy and 0.972 AUC score. In order to avoid biasness, the model is trained using the fivefold cross validation technique. Moreover, the model is faster in training than the pre-trained models and requires a small number of trainable parameters. The Grad-CAM heat map visualization technique also shows how perfectly the proposed model extracts important features to classify breast cancers.																	0941-0643	1433-3058															10.1007/s00521-020-05394-5		OCT 2020											
J								Deep learning based detection and analysis of COVID-19 on chest X-ray images	APPLIED INTELLIGENCE										Covid-19; XCeption; Inception net 3; Deep-learning; ResNeXt; Chest X-ray images; CNN	ACUTE RESPIRATORY SYNDROME; CT; INFECTION; PNEUMONIA; CHINA	Covid-19 is a rapidly spreading viral disease that infects not only humans, but animals are also infected because of this disease. The daily life of human beings, their health, and the economy of a country are affected due to this deadly viral disease. Covid-19 is a common spreading disease, and till now, not a single country can prepare a vaccine for COVID-19. A clinical study of COVID-19 infected patients has shown that these types of patients are mostly infected from a lung infection after coming in contact with this disease. Chest x-ray (i.e., radiography) and chest CT are a more effective imaging technique for diagnosing lunge related problems. Still, a substantial chest x-ray is a lower cost process in comparison to chest CT. Deep learning is the most successful technique of machine learning, which provides useful analysis to study a large amount of chest x-ray images that can critically impact on screening of Covid-19. In this work, we have taken the PA view of chest x-ray scans for covid-19 affected patients as well as healthy patients. After cleaning up the images and applying data augmentation, we have used deep learning-based CNN models and compared their performance. We have compared Inception V3, Xception, and ResNeXt models and examined their accuracy. To analyze the model performance, 6432 chest x-ray scans samples have been collected from the Kaggle repository, out of which 5467 were used for training and 965 for validation. In result analysis, the Xception model gives the highest accuracy (i.e., 97.97%) for detecting Chest X-rays images as compared to other models. This work only focuses on possible methods of classifying covid-19 infected patients and does not claim any medical accuracy.																	0924-669X	1573-7497															10.1007/s10489-020-01902-1		OCT 2020											
J								Feature selection by using chaotic cuckoo optimization algorithm with levy flight, opposition-based learning and disruption operator	SOFT COMPUTING										Feature selection; High-dimensional data; Cuckoo optimization algorithm; Chaotic theory; Levy flight; Disruption operator; Opposition-based learning	SUPERVISED FEATURE-SELECTION; ARTIFICIAL BEE COLONY; SEARCH ALGORITHM; HYBRID; CLASSIFICATION	Feature selection, which plays an important role in high-dimensional data analysis, is drawing increasing attention recently. Finding the most relevant and important features for classifications are one of the most important tasks of data mining and machine learning, since all of the datasets have irrelevant features that affect accuracy rate and slow down the classifier. Feature selection is an optimization process, which improves the accuracy rate of data classification and reduces the number of selected features. Applying too many features both requires a large memory capacity and leads to a slow execution speed. Feature selection algorithms are often responsible to decide which features should be selected to be used during a classification algorithm. Traditional algorithms seemed to be inefficient due to the complexity of dimensions of the problem, thus evolutionary algorithms were used to improve the problem solving process. The algorithm proposed in this paper, chaotic cuckoo optimization algorithm with levy flight, disruption operator and opposition-based learning (CCOALFDO), is applied to select the optimal feature subspace for classification. It reduces the randomization in selecting features and avoids getting stuck in local optimum solutions which lead to a more interesting feature subset. Extensive experiments are conducted on 20 high-dimensional datasets to demonstrate the effectiveness and efficiency of the proposed method. The results showed the superiority of the proposed method to state-of-the-art methods in terms of classification accuracy rate. In addition, they prove the ability of the CCOALFDO in selecting the most relevant features for classification tasks. Thus, it is a reasonable solution in handling noise and avoiding serious negative impacts on the classification accuracy rate in real world datasets.																	1432-7643	1433-7479															10.1007/s00500-020-05349-x		OCT 2020											
J								A cyber physical system for managing customer relations	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Brand sincerity; Customer relationship management model; Finite automaton; Machine intelligence; Mathematical models of persuasion; Social internet of things	INTEGRATION	Maintaining customer-company relationship is very important from the perspective of business. It helps to gain new customers as well as retain existing customers, even the dissatisfied ones, through SIoT based informal and humble counseling. Full form of SIoT is social internet of things where smart objects communicate via social networks to extract required information and propagate suitable ones to identify potential customers and counsel them online. These objects are equipped with machine intelligence so that they can make friends in social networks, collaborate with each other to achieve a common goal, update relationship status and level of trustworthiness between them. Using social network connectivity different groups can be created, where satisfied and brand sincere customers can support company-engaged smart objects to convince or persuade prospective buyers. Convincing also includes the provision of humble and honest apologizing for poor quality customer service which the concerned person might have experienced earlier from company of the object. This may be difficult for a human being because of his underlying nature or characteristics like ego, anger etc. but becomes extremely easy for smart objects because they are devoid of human emotions and enabled by machine intelligence only. Research has proved that behavioral optimization techniques are capable of significantly improving a company's sales record. Therefore, we can emphatically say that SIoT can be of great help to increase sales of a company. In this article, we propose: a complete model of communication for SIoT-CRM, design the company object as a finite automaton, mathematical models of persuasion with and without support of brand sincere customers.																	1868-5137	1868-5145															10.1007/s12652-020-02583-x		OCT 2020											
J								An efficient on-demand charging schedule method in rechargeable sensor networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Wireless energy charging; Sensor lifetime; Mobile charger; Multi-node energy charging; Charging schedule; Sugeno FIS; Wireless rechargeable sensor network	WIRELESS POWER TRANSFER; FUZZY-LOGIC; MOBILE CHARGER; OPTIMIZATION; MAXIMIZATION; ALGORITHMS; FRAMEWORK	Nowadays, wireless energy charging (WEC) is emerging as a promising technology for improving the lifetime of sensors in wireless rechargeable sensor networks (WRSNs). Using WEC, a mobile charger (MC) reliably supplies electric energy to the sensors. However, finding an efficient charging schedule forMCto charge the sensors is one of the most challenging issues. The charging schedule depends on remaining energy, geographical and temporal constraints, etc. Therefore, in this article, a novel efficient charging algorithm is proposed, such that the lifetime of the sensors in WRSN are increased. The proposed algorithm uses a multi-nodeMCthat can charge multiple sensors at the same time. In this algorithm, the charging requests of the low-energy sensors are received by theMC. Then, a reduced number of visiting points are determined for theMCto visit them. The visiting points are within the charging range of one or more requesting sensors. Thereafter, an efficient charging schedule is determined using an adaptive fuzzy model. Sugeno-fizzy inference method (S-FIS) is being used as a fuzzy model. It takes remaining energy, node density, and distance toMC, as network inputs for making real-time decisions while scheduling. Through simulation experiments, it is finally shown that the proposed scheme has higher charging performance comparing to base-line charging schemes in terms of survival ratio, energy utilization efficiency, and average charging latency. In addition, ANOVA tests are conducted to verify the reported results.																	1868-5137	1868-5145															10.1007/s12652-020-02539-1		OCT 2020											
J								A study on modeling vehicles mobility with MLC for enhancing vehicle-to-vehicle connectivity in VANET	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Mobility model; Connectivity analysis; VANET; Mandatory lane changing; Recurrent neural network	NETWORKS	In Vehicular ad hoc networks (VANETs), vehicle-to-vehicle (V2V) is a significant mode of communication in which vehicles communicate with other moving vehicles with the aid of wireless transceivers. Due to the rapid mobility of vehicles, network connectivity over VANETs is frequently unstable, especially in sparse highways. This paper analyzes V2V connectivity dynamics by designing the microscopic mobility and lane changing decision model using an adaptive cursive control mechanism and recurrent neural network. Extensive simulators like SUMO and NS2 analyze the validity of this proposed model. The proposed analytical model provides a framework for examining the impact of mobility dependent metrics such as vehicle velocity, acceleration/deceleration, safety gap, vehicle arrival rate, vehicle density and network metric data delivery rate for characterizing the VANET connectivity of the proposed network. The simulation results synchronized those of the proposed model, which illustrated that the developed analytical model of this work is effective.																	1868-5137	1868-5145															10.1007/s12652-020-02559-x		OCT 2020											
J								Real-time traffic distribution prediction protocol (TDPP) for vehicular networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										TDPP; Traffic prediction; VANETs; Road network	CLASSIFICATION; MODEL	Over downtown and highway road scenarios several applications have been proposed to enhance the quality of driving trips there. Safety, efficiency and entertainment services are provided to vehicles through several advanced technologies. Many of these applications require accurate investigation of the traffic characteristics and distributions over the area of interest in order to successfully provide the targeted services. To mention a few, path recommendation protocols, traffic light scheduling algorithms and driving assistance techniques need specific, detailed and accurate traffic distribution reports regarding the investigated area of interest. Several traffic prediction and evaluation protocols have been proposed in the literature using historical, visual and wireless connecting technologies to gather the real-time basic traffic data. Accuracy, delay, bandwidth consumption and high cost required equipments are the main challenges of the previous protocols. In this paper, we aim to propose a real-time traffic distribution prediction protocol (TDPP) using the vehicular network technology. The proposed protocol aims to produce accurate traffic evaluation and distribution of the investigated area of interest based on gathering the basic traffic data of some traveling vehicles there. From the experimental results we can infer that the TDPP protocol provides more accurate traffic evaluation in the case that only a few vehicles are equipped with wireless transceivers. Moreover, it requires less bandwidth and time to evaluate the traffic characteristics compared to traditional protocols in this field, since it only processes the basic traffic data of the small selected set of vehicles there.																	1868-5137	1868-5145															10.1007/s12652-020-02585-9		OCT 2020											
J								An experimental study of graph-based semi-supervised classification with additional node information	KNOWLEDGE AND INFORMATION SYSTEMS										Network data analysis; Semi-supervised classification; Link analysis; Graph mining; Multi-view learning	MAXIMUM-ENTROPY AGGREGATION; FRAMEWORK; CLASSIFIERS; FUSION; MODEL	The volume of data generated by internet and social networks is increasing every day, and there is a clear need for efficient ways of extracting useful information from them. As this information can take different forms, it is important to use all the available data representations for prediction; this is often referred to multi-view learning. In this paper, we consider semi-supervised classification using both regular, plain, tabular, data and structural information coming from a network structure (feature-rich networks). Sixteen techniques are compared and can be divided in three families: the first one uses only the plain features to fit a classification model, the second uses only the network structure, and the last combines both information sources. These three settings are investigated on 10 real-world datasets. Furthermore, network embedding and well-known autocorrelation indicators from spatial statistics are also studied. Possible applications are automatic classification of web pages or other linked documents, of nodes in a social network, or of proteins in a biological complex system, to name a few. Based on our findings, we draw some general conclusions and advice to tackle this particular classification task: it is clearly observed that some dataset labelings can be better explained by their graph structure or by their features set.																	0219-1377	0219-3116				NOV	2020	62	11					4337	4371		10.1007/s10115-020-01500-0		OCT 2020											
J								Arabic (Indian) digit handwritten recognition using recurrent transfer deep architecture	SOFT COMPUTING										Arabic (Indian) handwritten recognition; Deep supervised learning; LSTM; Transfer learning		The rapid volume of digit texts and images motivates researchers to build solid and efficient prediction models to recognize such media. The Arabic language is considered one of the difficult languages regarding the way of writing characters and digits. Recent research focuses on such language for building predictive approaches to recognize written materials. The Arabic (Indian) digit recognition task has been a challenging task and has gained more attention from researchers who build optimal predictive models from historical images that are used in many applications. However, transfer learning approaches exploit deep pre-trained models that could be re-used for similar tasks. So, in this paper, we propose an adapted deep hybrid transfer model developed using two well-known pre-trained convolutional neural networks (CNN) models. These are further adapted by adding recurrent neural networks especially long short-term memory (LSTM) architectures to detect Arabic (Indian) Handwritten Digits (AHD). The CNN model learns the relevant features of Arabic (Indian) digits, while the sequence learning process in the LSTM layers extracts long-term dependence features. The experimental results, using popular datasets, show significant performance obtained by the adapted transfer models with accuracy reached up to 98.92% as well as with precision and recall values at most cases reached to 100% with statisticalttest usingp-value (p <= 0.05) compared to baseline methods.																	1432-7643	1433-7479															10.1007/s00500-020-05368-8		OCT 2020											
J								Combination of individual and group patterns for time-sensitive purchase recommendation	INTERNATIONAL JOURNAL OF DATA SCIENCE AND ANALYTICS										Point processes; Transactional data; Mixture models; Recommendation; Machine learning		Due to the availability of large amounts of data, recommender systems have quickly gained popularity in the banking sphere. However, time-sensitive recommender systems, which take into account the temporal behavior and the recurrent activities of users to predict the expected time and category of next purchase, are still an active field of research. Many researchers tend to use population-level features or their low-rank approximations because the client's purchase history is very sparse with few observations for some time intervals and product categories. But such approaches inevitably lead to a loss of accuracy. In this paper, we present a generative model of client spending based on the temporal point processes framework. The model is built in the way, to bring more individuality for the clients' purchase behavior which takes into account individual purchase histories of clients. We also tackle the problem of poor statistics for people with a low transactional activity using effective intensity function parameterizations, and several other techniques such as smoothing daily intensity levels and taking into account population-level purchase rates for clients with a small number of transactions. The model is highly interpretable, and its training time scales linearly to millions of transactions and cubically to hundreds of thousands of users. Different temporal-process models were tested, and our model with all the incorporated modifications has shown the best results in terms of both error of time prediction and the accuracy of category prediction.																	2364-415X	2364-4168															10.1007/s41060-020-00233-1		OCT 2020											
J								Event-Based Robotic Grasping Detection With Neuromorphic Vision Sensor and Event-Grasping Dataset	FRONTIERS IN NEUROROBOTICS										neuromorphic vision sensor; SMP filter; event-grasping dataset; grasping detection; deep learning		Robotic grasping plays an important role in the field of robotics. The current state-of-the-art robotic grasping detection systems are usually built on the conventional vision, such as the RGB-D camera. Compared to traditional frame-based computer vision, neuromorphic vision is a small and young community of research. Currently, there are limited event-based datasets due to the troublesome annotation of the asynchronous event stream. Annotating large scale vision datasets often takes lots of computation resources, especially when it comes to troublesome data for video-level annotation. In this work, we consider the problem of detecting robotic grasps in a moving camera view of a scene containing objects. To obtain more agile robotic perception, a neuromorphic vision sensor (Dynamic and Active-pixel Vision Sensor, DAVIS) attaching to the robot gripper is introduced to explore the potential usage in grasping detection. We construct a robotic grasping dataset namedEvent-Graspingdataset with 91 objects. A spatial-temporal mixed particle filter (SMP Filter) is proposed to track the LED-based grasp rectangles, which enables video-level annotation of a single grasp rectangle per object. As LEDs blink at high frequency, theEvent-Graspingdataset is annotated at a high frequency of 1 kHz. Based on theEvent-Graspingdataset, we develop a deep neural network for grasping detection that considers the angle learning problem as classification instead of regression. The method performs high detection accuracy on ourEvent-Graspingdataset with 93% precision at an object-wise level split. This work provides a large-scale and well-annotated dataset and promotes the neuromorphic vision applications in agile robot.																	1662-5218					OCT 8	2020	14								51	10.3389/fnbot.2020.00051													
J								Nested variational autoencoder for topic modelling on microtexts with word vectors	EXPERT SYSTEMS										microtext; neural network; topic modelling; variational autoencoder; word embedding		Most of the information on the Internet is represented in the form ofmicrotexts, which are short text snippets such as news headlines or tweets. These sources of information are abundant, and mining these data could uncover meaningful insights. Topic modelling is one of the popular methods to extract knowledge from a collection of documents; however, conventional topic models such as latent Dirichlet allocation (LDA) are unable to perform well on short documents, mostly due to the scarcity of word co-occurrence statistics embedded in the data. The objective of our research is to create a topic model that can achieve great performances on microtexts while requiring a small runtime for scalability to large datasets. To solve the lack of information of microtexts, we allow our method to take advantage of word embeddings for additional knowledge of relationships between words. For speed and scalability, we apply autoencoding variational Bayes, an algorithm that can perform efficient black-box inference in probabilistic models. The result of our work is a novel topic model called thenested variational autoencoder, which is a distribution that takes into account word vectors and is parameterized by a neural network architecture. For optimization, the model is trained to approximate the posterior distribution of the original LDA model. Experiments show the improvements of our model on microtexts as well as its runtime advantage.																	0266-4720	1468-0394														e12639	10.1111/exsy.12639		OCT 2020											
J								FR-KDE: A Hybrid Fuzzy Rule-Based Information Fusion Method with its Application in Biomedical Classification	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Fuzzy rule; Dempster-Shafer evidence theory; Granular computing; Linguistic model; Belief function; KDE; Decision-making	PERSPECTIVE; NEGATION; SYSTEM; NUMBER	Granular computing (GrC) is an essential tool to solve human real problem since the information granules is close to human perception schemes. In GrC, both classification accuracy and interpretability play significant roles. Fuzzy rule (FR) based classification systems are effective methods solving this problem. However, the accuracy of FR may be decreased when solving some complex application. In this paper, a novel model called FR-KDE integrating the FR and kernel density estimation (KDE) in the framework of Dempster-Shafer evidence theory is proposed to deal with the classification problem. By fusing the result of FR and KDE via the Dempster's combination rule, it can reduce the uncertainty of FR and obtain better accuracy. To illustrate the effect of the FR-KDE approach, it is applied to the medical data classification problem. Experimentally, the results demonstrate that the FR-KDE method is effective in handling biomedical data classification problems.																	1562-2479	2199-3211															10.1007/s40815-020-00957-z		OCT 2020											
J								A novel fuzzy hybrid neutrosophic decision-making approach for the resilient supplier selection problem	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										Dombi aggregators; fuzzy decision-making; MABAC; neutrosophic sets; resilient supply chain	AGGREGATION OPERATORS; CHAIN RESILIENCE; UNCERTAINTY; AHP; RISK; QFD	The objectives of this study are to mitigate the risk and disturbances to the supply chain, to offer required models for resolving the complex issues that arise, and to maintain the stability of the support system. Also, the uncertain conditions in a supply chain force decision-makers and experts to adopt a fuzzy-based evaluation platform to ensure secure and reliable consequences. The current study proposed a fuzzy neutrosophic decision-making approach for supplier evaluation and selection. The model is composed of a new weight aggregator that uses pairwise comparison, which has not been reported to date. The model uses a Dombi aggregator that is more qualified than other aggregators. The Dombit-conorms andt-norms have the same properties as those of the general t-conorm andt-norm, which can enhance the flexibility of the information aggregation process via the adjustment of a parameter. A decision-making environment with uncertain conditions and multiple factors is supposed. We applied this approach in a construction company to analyse the suppliers in a resilient supply chain management (RSCM) system using a MABAC (multiattribute border approximation area comparison) tool. The accuracy of the proposed model was examined via sensitivity analysis tests. This study proposes a novel fuzzy-neutrosophic-based approach for resilient supplier selection. The main contributions of this study work are the design, implementation and analysis of a multiattribute evaluation system with respect to fuzzy neutrosophic values. In this evaluation system, a new pairwise comparison is conducted with trapezoidal neutrosophic linguistic variables to determine the importance weights of supplier criteria. Typically, the provision of opinions regarding the qualitative performances of suppliers is a difficult and confusing responsibility for experts and supplier evaluators. Therefore, the proposed approach overcomes this problem by utilizing a pairwise comparison by neutrosophic values and proposes original Dombi aggregation operators for dealing with fuzzy neutrosophic sets.																	0884-8173	1098-111X				DEC	2020	35	12					1934	1986		10.1002/int.22279		OCT 2020											
J								Internet of things with bio-inspired co-evolutionary deep-convolution neural-network approach for detecting road cracks in smart transportation	NEURAL COMPUTING & APPLICATIONS										Internet of things (IoT); Deep learning network; Smart mobile sensors; Smart transportation; Road cracks; Deep-convolution networks	SEMANTIC SEGMENTATION; IMMUNE; EDGE	Internet of things (IoT) primarily aims to realize valuable services such as smart homes, smart buildings, and smart transport. To this end, smart applications are faced with different challenges, particularly in the implementation of a smart transportation system, which requires maximum road and travel safety. Road crack detection has been extensively studied and presented with various solutions; however, these approaches are limited by the inhomogeneity in the crack intensity and background complexity, such as a shadow with similar intensity and pavement contrast, which are known obstacles in the accurate prediction of road cracks. To overcome these issues, an IoT system with a bio-inspired deep learning approach was introduced herein for accurate road crack detection. In the proposed approach, transportation images are first collected using a smart mobile sensor, then processed by a bio-inspired self-learning co-evolutionary deep-convolution neural network. The optimized neural networks provide the required framework by analyzing the collected images to detect cracks more accurately. The efficiency of the proposed system was confirmed in different metrics, including the per-pixel accuracy (99.04%), Jaccard index (98.42%), loss error rate (0.03), precision (99.25%), recall (99.24%), and prediction accuracy (99.72%) metrics.																	0941-0643	1433-3058															10.1007/s00521-020-05401-9		OCT 2020											
J								Deep learning for procedural content generation	NEURAL COMPUTING & APPLICATIONS										Procedural content generation; Game design; Deep learning; Machine learning; Computational and artificial intelligence	NEURAL-NETWORKS; AI	Procedural content generation in video games has a long history. Existing procedural content generation methods, such as search-based, solver-based, rule-based and grammar-based methods have been applied to various content types such as levels, maps, character models, and textures. A research field centered on content generation in games has existed for more than a decade. More recently, deep learning has powered a remarkable range of inventions in content production, which are applicable to games. While some cutting-edge deep learning methods are applied on their own, others are applied in combination with more traditional methods, or in an interactive setting. This article surveys the various deep learning methods that have been applied to generate game content directly or indirectly, discusses deep learning methods that could be used for content generation purposes but are rarely used today, and envisages some limitations and potential future directions of deep learning for procedural content generation.																	0941-0643	1433-3058															10.1007/s00521-020-05383-8		OCT 2020											
J								Detection of weather images by using spiking neural networks of deep learning models	NEURAL COMPUTING & APPLICATIONS										Spiking neural network; Deep networks; Weather images; Feature extraction and combination		The transmission of weather information of a location at certain time intervals affects the living conditions of the people there directly or indirectly. According to weather information, people shape their behavior in daily life. Besides, agricultural activities are carried out according to the weather conditions. Considering the importance of this subject, it is possible to make weather predictions based on the weather images in today's technology exploiting the computer systems. However, the recent mention of the name of artificial intelligence technology in every field has made it compulsory for computer systems to benefit from this technology. The dataset used in the study has four classes: cloudy, rain, shine, and sunrise. In the study, GoogLeNet and VGG-16 models and the spiking neural network (SNN) were used together. The features extracted from GoogLeNet and VGG-16 models were combined and given to the SNNs as the input. As a result, the SNNs contributed to the success of classification with the proposed approach. The classification accuracy rates of cloudy, rain, shine, and sunrise classes were 98.48%, 97.58%, 97%, and 98.48%, respectively, together with SNN. Also, the use of SNNs in combination with deep learning models to obtain a successful result is proved in this study.																	0941-0643	1433-3058															10.1007/s00521-020-05388-3		OCT 2020											
J								A novel online sequential extreme learning machine with L-2,L-1-norm regularization for prediction problems	APPLIED INTELLIGENCE										ELM; L2; 1-norm; LR21-ELM; LR21OS-ELM; Prediction; OS-ELM	ABSOLUTE ERROR MAE; NEURAL-NETWORKS; CLASSIFICATION; REGRESSION; ALGORITHM; MODELS; RMSE; GAP	In today's world, data is produced at a very high speed and used in a large number of prediction problems. Therefore, the sequential nature of learning algorithms is in demand for batch learning algorithms. This paper presents a novel online sequential algorithm for extreme learning machine withl(2,1)-norm regularization (LR21OS-ELM) to handle the real-time sequential data. Wang et al. have given ELM withl(2,1)-norm based regularization namely LR21-ELM. This method is a batch processing model which takes data in a single chunk. So, whenever a new chunk of data arrives the model has to be retrained which takes a lot of time and memory. The proposed sequential algorithm does not require building a new model each time data arrives. This will update the previous model with new data that will save time and memory. Thel(2,1)-norm regularization is a structural sparse-inducing norm which is integrated with an online sequential learning algorithm to diminish the complexity of the learning model by eliminating the redundant neurons of OS-ELM model. This paper proposes an iterative bi-objective optimization algorithm to solvel(2,1)norm-based minimization problem and to handle the real time sequential data. The proposed model can learn sequentially arriving data in the form of chunks where chunk size can be fixed or varying. The experimental study has been conducted on several benchmark datasets collected from different research domains to prove the generalization ability of the proposed algorithm. The obtained results show that LR21OS-ELM combines the advantages ofl(2,1)-norm regularization and online sequential learning of data and improves the prediction performance of the system.																	0924-669X	1573-7497															10.1007/s10489-020-01890-2		OCT 2020											
J								A two-level computer vision-based information processing method for improving the performance of human-machine interaction-aided applications	COMPLEX & INTELLIGENT SYSTEMS										Computer vision; Classification learning; Human-machine interaction; Information processing; Regression analysis	OBJECT DETECTION; VEHICLE	The computer vision (CV) paradigm is introduced to improve the computational and processing system efficiencies through visual inputs. These visual inputs are processed using sophisticated techniques for improving the reliability of human-machine interactions (HMIs). The processing of visual inputs requires multi-level data computations for achieving application-specific reliability. Therefore, in this paper, a two-level visual information processing (2LVIP) method is introduced to meet the reliability requirements of HMI applications. The 2LVIP method is used for handling both structured and unstructured data through classification learning to extract the maximum gain from the inputs. The introduced method identifies the gain-related features on its first level and optimizes the features to improve information gain. In the second level, the error is reduced through a regression process to stabilize the precision to meet the HMI application demands. The two levels are interoperable and fully connected to achieve better gain and precision through the reduction in information processing errors. The analysis results show that the proposed method achieves 9.42% higher information gain and a 6.51% smaller error under different classification instances compared with conventional methods.																	2199-4536	2198-6053															10.1007/s40747-020-00208-6		OCT 2020											
J								An improved simulated annealing algorithm based on residual network for permutation flow shop scheduling	COMPLEX & INTELLIGENT SYSTEMS										Permutation flow shop scheduling; Improved simulated annealing algorithm; Residual networks; TA benchmark	DIFFERENTIAL EVOLUTION ALGORITHM; ITERATED GREEDY ALGORITHM; GENETIC ALGORITHM; OPTIMIZATION; CLASSIFICATION	The permutation flow shop scheduling problem (PFSP), which is one of the most important scheduling types, is widespread in the modern industries. With the increase of scheduling scale, the difficulty and computation time of solving the problem will increase exponentially. Adding the knowledge to intelligent algorithms is a good way to solve the complex and difficult scheduling problems in reasonable time. To deal with the complex PFSPs, this paper proposes an improved simulated annealing (SA) algorithm based on residual network (SARes). First, this paper defines the neighborhood of the PFSP and divides its key blocks. Second, the Residual Network (ResNet) is used to extract and train the features of key blocks. And, the trained parameters are stored in the SA algorithm to improve its performance. Afterwards, some key operators, including the initial temperature setting and temperature attenuation function of SA algorithm, are also modified. After every new solution is generated, the parameters trained by the ResNet are used for fast ergodic search until the local optimal solution found in the current neighborhood. Finally, the most famous benchmarks including part of TA benchmark are selected to verify the performance of the proposed SARes algorithm, and the comparisons with the-state-of-art methods are also conducted. The experimental results show that the proposed method has achieved good results by comparing with other algorithms. This paper also conducts experiments on network structure design, algorithm parameter selection, CPU time and other problems, and verifies the advantages of SARes algorithm from the aspects of stability and efficiency.																	2199-4536	2198-6053															10.1007/s40747-020-00205-9		OCT 2020											
J								A novel disruption based symbiotic organisms search to solve economic dispatch	EVOLUTIONARY INTELLIGENCE										Disruption operator; Economic dispatch; Evolutionary technique; Symbiotic organisms search; Valve point effect; Prohibited operating zones	PARTICLE SWARM OPTIMIZATION; LEARNING BASED OPTIMIZATION; GREY WOLF OPTIMIZATION; DIFFERENTIAL EVOLUTION; TABU SEARCH; ALGORITHM	Economic dispatch (ED) is one of the vital prospects in the energy management system for determining the optimal power generation distribution among several committed power generating units. The objective of ED is to generate the electric power to meet the load demand by minimizing the total operating cost subjected to various equality and inequality constraints. This ED problem is generally solved using conventional techniques such as a gradient method by assuming the objective function as linear and convex. However, in general, the ED problem is constrained, non-convex, large scale, multimodal and highly nonlinear optimization problem. In order to overcome this drawback, various heuristic techniques have been introduced in the literature. Symbiotic organism search (SOS) is one of the emerging nature-inspired meta-heuristic algorithms. Due to simple in implementation and free from algorithm-specific control parameters, the SOS algorithm has gained a reputation among the researchers. The SOS technique mimics the biological interaction between two distinct species to survive and proliferate in the ecosystem. To evade the suboptimal solution and to enhance the exploration and exploitation of the SOS algorithm, the disruption operator is integrated with the basic SOS to form a novel disruption based SOS (DSOS) algorithm. The disruption strategy is originated from astrophysics and has the ability to shift between exploration and exploitation during the process of finding an optimum solution. To illustrate the effectiveness of the proposed method, it is applied on various test systems such as 3-unit, 5-unit, 6-unit, 13-unit, 20-unit, 38-unit, and 110-unit systems to solve different ED problems, namely, without transmission losses (TL), with transmission losses, by simultaneously considering valve point loading (VPL) effect along with TL, by simultaneously considering the effect of ramp rate limits (RRLs), prohibited operating zones (POZs) and TLs, and dynamic economic load dispatch by considering RRLs, POZs, VPL, and TL. The comparison of DSOS based simulation results with other methods reveal the efficiency and robustness of the proposed method to provide minimum total generation cost in all the cases with better convergence rate and less computational time.																	1864-5909	1864-5917															10.1007/s12065-020-00506-5		OCT 2020											
J								HOTA: A Higher Order Metric for Evaluating Multi-object Tracking	INTERNATIONAL JOURNAL OF COMPUTER VISION										Multi-object tracking; Evaluation metrics; Visual tracking	PERFORMANCE-MEASURES; MULTITARGET; ALGORITHM; FILTERS; TARGET	Multi-object tracking (MOT) has been notoriously difficult to evaluate. Previous metrics overemphasize the importance of either detection or association. To address this, we present a novel MOT evaluation metric, higher order tracking accuracy (HOTA), which explicitly balances the effect of performing accurate detection, association and localization into a single unified metric for comparing trackers. HOTA decomposes into a family of sub-metrics which are able to evaluate each of five basic error types separately, which enables clear analysis of tracking performance. We evaluate the effectiveness of HOTA on the MOTChallenge benchmark, and show that it is able to capture important aspects of MOT performance not previously taken into account by established metrics. Furthermore, we show HOTA scores better align with human visual evaluation of tracking performance.																	0920-5691	1573-1405															10.1007/s11263-020-01375-2		OCT 2020											
J								Energy-efficient routing optimization algorithm in WBANs for patient monitoring	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										WBAN; Energy; Health monitoring; Genetic algorithm; Dead nodes		With the recent technological innovations for measuring the physiological characteristics in the human body, Wireless Body Area Networks (WBANs) have received much attention from the industry and academics. One of the feasible solutions provided by today's WBAN is the continuous health monitoring in which sensors planted in various parts of the body, which measure and send information about physiological health status to a sink. The energy constraint WBAN has to perform these measurements with minimum energy consumptions of the nodes, maintaining the durable health monitoring process. This paper uses the meta-heuristic Genetic Algorithm (GA) to select the best routing path by calculating distances between the nodes under multiple scenarios, in contrast to the available direct distance optimization method. This study considers the use of energy by sensor nodes, number of rounds, number of sensors, the position of the deployed sensors and distance between the sensors. The comprehensive results show that direct distance optimization method drops more packets, i.e. 12,000 as compared to 8000 packets by the genetic algorithm when 8000 rounds were executed. The proposed optimization also outperforms the previous approach in terms of the number of dead nodes, which results in saving the energy to increase the lifetime of the WBAN significantly.																	1868-5137	1868-5145															10.1007/s12652-020-02541-7		OCT 2020											
J								Local search for the maximumk-plex problem	JOURNAL OF HEURISTICS										NP-complete; Heuristic; Local search; k-plex; Large graphs		The maximumk-plex problem is an important, computationally complex graph based problem. In this study an effectivek-plex local search (KLS) is presented for solving this problem on a wide range of graph types. KLS uses data structures suitable for the graph being analysed and has mechanisms for preventing search cycling and promoting search diversity. State of the art results were obtained on 121 dense graphs and 61 large real-life (sparse) graphs. Comparisons with three recent algorithms on the more difficult graphs show that KLS performed better or as well as in 93% of 332 significantk-plex problem instances investigated achieving either larger averagek-plex sizes (including some new results) or, when these were equivalent, lower CPU requirements.																	1381-1231	1572-9397															10.1007/s10732-020-09459-5		OCT 2020											
J								A portable HCI system-oriented EEG feature extraction and channel selection for emotion recognition	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										channel selection; emotion recognition; feature extraction; human-computer interaction; relief	PERMUTATION ENTROPY; NEURAL-NETWORKS; RELIEFF	Emotion recognition has become an important component of human-computer interaction systems. Research on emotion recognition based on electroencephalogram (EEG) signals are mostly conducted by the analysis of all channels' EEG signals. Although some progresses are achieved, there are still several challenges such as high dimensions, correlation between different features and feature redundancy in the realistic experimental process. These challenges have hindered the applications of emotion recognition to portable human-computer interaction systems (or devices). This paper explores how to find out the most effective EEG features and channels for emotion recognition so as to only collect data as less as possible. First, discriminative features of EEG signals from different dimensionalities are extracted for emotion classification, including the first difference, multiscale permutation entropy, Higuchi fractal dimension, and discrete wavelet transform. Second, relief algorithm and floating generalized sequential backward selection algorithm are integrated as a novel channel selection method. Then, support vector machine is employed to classify the emotions for verifying the performance of the channel selection method and extracted features. At last, experimental results demonstrate that the optimal channel set, which are mostly located at the frontal, has extremely high similarity on the self-collected data set and the public data set and the average classification accuracy is achieved up to 91.31% with the selected 10-channel EEG signals. The findings are valuable for the practical EEG-based emotion recognition systems.																	0884-8173	1098-111X															10.1002/int.22295		OCT 2020											
J								Deep Neural Learning Adaptive Sequential Monte Carlo for Automatic Image and Speech Recognition	APPLIED COMPUTATIONAL INTELLIGENCE AND SOFT COMPUTING											SOFT COMPUTING METHODS; OPTIMIZATION; ALGORITHM; NETWORK	To enhance the performance of image classification and speech recognition, the optimizer is considered an important factor for achieving high accuracy. The state-of-the-art optimizer can perform to serve in applications that may not require very high accuracy, yet the demand for high-precision image classification and speech recognition is increasing. This study implements an adaptive method for applying the particle filter technique with a gradient descent optimizer to improve model learning performance. Using a pretrained model helps reduce the computational time to deploy an image classification model and uses a simple deep convolutional neural network for speech recognition. The applied method results in a higher speech recognition accuracy score-89.693% for the test dataset-than the conventional method, which reaches 89.325%. The applied method also performs well on the image classification task, reaching an accuracy of 89.860% on the test dataset, better than the conventional method, which has an accuracy of 89.644%. Despite a slight difference in accuracy, the applied optimizer performs well in this dataset overall.																	1687-9724	1687-9732				OCT 7	2020	2020								8866259	10.1155/2020/8866259													
J								Image Matching Across Wide Baselines: From Paper to Practice	INTERNATIONAL JOURNAL OF COMPUTER VISION										Benchmark; Dataset; Stereo; Structure from motion; Local features; 3D reconstruction	PERFORMANCE	We introduce a comprehensive benchmark for local features and robust estimation algorithms, focusing on the downstream task-the accuracy of the reconstructed camera pose-as our primary metric. Our pipeline's modular structure allows easy integration, configuration, and combination of different methods and heuristics. This is demonstrated by embedding dozens of popular algorithms and evaluating them, from seminal works to the cutting edge of machine learning research. We show that with proper settings, classical solutions may still outperform the perceived state of the art. Besides establishing the actual state of the art, the conducted experiments reveal unexpected properties of structure from motion pipelines that can help improve their performance, for both algorithmic and learned methods. Data and code are online (https://github.com/ubcvision/image-matching-benchmark), providing an easy-to-use and flexible framework for the benchmarking of local features and robust estimation methods, both alongside and against top-performing methods. This work provides a basis for the Image Matching Challenge (https://image-matching-challenge.github.io).																	0920-5691	1573-1405															10.1007/s11263-020-01385-0		OCT 2020											
J								Left-to-Right Asymmetry and Early Association in Korean	JOURNAL OF LOGIC LANGUAGE AND INFORMATION										Incrementality; Case particle; Prosody; Structural routines; Dynamic Syntax; Pragmatic; Syntax	LANGUAGE	This paper shows Korean speakers' strong preference for incremental structure building based on the following core phenomena: (1) left-right asymmetry; (2) pre-verbal structure building and a strong preference for early association. This paper argues that these phenomena reflect the procedural aspects of linguistic competence, which are difficult to explain within non-procedural grammar formalisms. Based on these observations, I argue for the necessity of a grammar formalism that adopts left-to-right incrementality as a core property of the syntactic architecture. In particular, I aim to show the role of (1) constructive particles; (2) prosody; and (3) structural routines in incremental Korean structure building. Though the nature of this discussion is theory-neutral, in order to formalise this idea I will adopt Dynamic Syntax [DS: Kempson et al. (Dynamic syntax: the flow of language understanding, Blackwell, Oxford, 2001); Cann et al. (The dynamics of language. Elsevier, Oxford, 2005)] in this paper.																	0925-8531	1572-9583															10.1007/s10849-020-09318-3		OCT 2020											
J								Spherical separation with infinitely far center	SOFT COMPUTING										Spherical separation; Classification; Grossone	CONIC SEPARATION; GROSSONE; CLASSIFICATION; SETS; METHODOLOGY	We tackle the problem of separating two finite sets of samples by means of a spherical surface, focusing on the case where the center of the sphere is fixed. Such approach reduces to the minimization of a convex and nonsmooth function of just one variable (the radius), revealing very effective in terms of computational time. In particular, we analyze the case where the center of the sphere is selected far from both the two sets, embedding the grossone idea and obtaining a kind of linear separation. Some numerical results are presented on classical binary data sets drawn from the literature.																	1432-7643	1433-7479															10.1007/s00500-020-05352-2		OCT 2020											
J								BAT optimization based Retinal artery vein classification	SOFT COMPUTING										Retinal blood vessel; Artery; Vein; Diabetic retinopathy; Random forest; BAT optimization; Feature set	VESSEL SEGMENTATION; BLOOD-VESSELS; IMAGES; ALGORITHM	The investigation of artery vein changes over time is considered to be the significant diagnosis process of retinal diseases like diabetic retinopathy. The diagnosis includes the characteristics analysis of artery vein vessels, changes in its tortuosity level and artery vein ratio; hence, it is important to classify the artery and vein in a better way. Computer-aided diagnosis requires the automated classification of retinal artery and vein for diagnosing the progression of diseases. In this paper, a supervised classification with Bat algorithm is proposed to discriminate the artery and vein vessels in the retinal fundus images. A novel feature vector space, including both additive colour space as well as luminous chromaticity model colour space, is constructed. BAT algorithm is applied to select the feature group which improve the classification accuracy and also to reduce the dimensionality of feature space. The proposed method is developed and analyzed using the publicly available databases DRIVE, IOSTAR and STARE.																	1432-7643	1433-7479															10.1007/s00500-020-05339-z		OCT 2020											
J								Using deep learning approach and IoT architecture to build the intelligent music recommendation system	SOFT COMPUTING										Artificial intelligence; Internet of Things; Intelligent Internet of Things; Music system; Deep learning	INTERNET	First, the local feature extraction of the scale-invariant feature transformation algorithm, the classification excellence of the support vector machine, and the performance of the deep learning-based Fast-RCNN algorithm in the multi-scale feature extraction are analyzed and explained to design an intelligent background music system based on deep learning and Internet of Things (IoT) technology. Then, the intelligent background music system is applied to the Intelligent Home. On this basis, a feature extraction algorithm based on the middle-level feature structure is proposed, which extracts the underlying features of the scene images. Afterward, the critical functional components of the intelligent background music system are explained. Based on the actual operations, an intelligent background music system is designed based on deep learning and IoT. The results show that the recognition rate of indoor scenarios by the middle-level feature construction-based feature extraction algorithm is the highest, which is about 87.6%. The Gabor feature algorithm classifies and identifies the scenarios, and its recognition rate is always around 20%. In the bathroom, the recognition effect of the saliency map feature algorithm is similar to that of the middle-level feature construction-based feature extraction algorithm; however, in the bedroom, the recognition effect of the middle-level feature construction-based feature extraction algorithm is significantly better due to problems such as the lighting and room orientation. The effects of middle-level feature construction-based feature extraction algorithm on the classification and recognition of indoor scenarios are sound. In contrast, the proposed feature extraction algorithm based on deep learning has an optimal effect. The designed and implemented intelligent background music system is stable and effective, which provides a new idea and a new theoretical basis for the future research of intelligent background music system.																	1432-7643	1433-7479															10.1007/s00500-020-05364-y		OCT 2020											
J								Echo State Networks and Long Short-Term Memory for Continuous Gesture Recognition: a Comparative Study	COGNITIVE COMPUTATION										Continuous gesture recognition; Echo state networks; Long Short-Term Memory	RESERVOIR	Recent developments of sensors that allow tracking of human movements and gestures enable rapid progress of applications in domains like medical rehabilitation or robotic control. Especially the inertial measurement unit (IMU) is an excellent device for real-time scenarios as it rapidly delivers data input. Therefore, a computational model must be able to learn gesture sequences in a fast yet robust way. We recently introduced an echo state network (ESN) framework for continuous gesture recognition (Tietz et al.,2019) including novel approaches for gesture spotting, i.e., the automatic detection of the start and end phase of a gesture. Although our results showed good classification performance, we identified significant factors which also negatively impact the performance like subgestures and gesture variability. To address these issues, we include experiments with Long Short-Term Memory (LSTM) networks, which is a state-of-the-art model for sequence processing, to compare the obtained results with our framework and to evaluate their robustness regarding pitfalls in the recognition process. In this study, we analyze the two conceptually different approaches processing continuous, variable-length gesture sequences, which shows interesting results comparing the distinct gesture accomplishments. In addition, our results demonstrate that our ESN framework achieves comparably good performance as the LSTM network but has significantly lower training times. We conclude from the present work that ESNs are viable models for continuous gesture recognition delivering reasonable performance for applications requiring real-time performance as in robotic or rehabilitation tasks. From our discussion of this comparative study, we suggest prospective improvements on both the experimental and network architecture level.																	1866-9956	1866-9964															10.1007/s12559-020-09754-0		OCT 2020											
J								Compliance and application tests of usability guidelines about giving information quickly and comprehensibly	COMPLEX & INTELLIGENT SYSTEMS										Information; Quickly; Comprehensibly; Guidelines; Recommendations; Web usability	DESIGN; PEOPLE	The objective of this publication is to analyze compliance with the web usability guidelines onGiving information quickly and comprehensibly. The behavior of 20 IT engineers without experience in web usability is analyzed to collect data on the application and compliance of each of the studied guidelines. The objectives are: (1) make a list of recommendations on the presentation of information and possible actions in a quickly understandable way. And highlight the most forgotten guidelines or the worst followed by web developers to think about the importance of offering specific training in this field. (2) Know the most important guidelines according to the participants themselves. To obtain the results, user tests are performed that evaluate the most ignored and applied guidelines. And its correct compliance is studied, since the participants do not have experience in web usability. Besides, interviews are conducted to find out which are the guidelines that they consider most important. It is expected to know if there are guidelines that apply intuitively and why. It is also intended to know if this innate application is helpful or compliance is wrong and needs specific training.																	2199-4536	2198-6053															10.1007/s40747-020-00198-5		OCT 2020											
J								Bi-direction Direct RGB-D Visual Odometry	APPLIED ARTIFICIAL INTELLIGENCE											PHOTOMETRIC CALIBRATION	Direct visual odometry (DVO) is an important vision task which aims to obtain the camera motion via minimizing the photometric error across the different correlated images. However, the previous research onDVOrarely considered the motion bias and only calculated using single direction, therefore potentially ignoring useful information compared with leveraging diverse directions. We assume that jointly considering forward and backward calculation can improve the accuracy of pose estimation. To verify our assumption and solid this contribution, in this paper, we test various combination of direct dense methods, including different error metrics,e.g., (intensity, gradient magnitude), alignment strategies (Forward-Compositional, Inverse-Compositional), and calculation directions (forward, backward, and bi-direction). We further study the issue of motion bias in RGB-D visual odometry and propose four strategy options to improve pose estimation accuracy,e.g., joint bi-direction estimation; two stage bi-direction estimation; transform average with weights; and transform fusion with covariance. We demonstrate the effectiveness and efficiency of our proposed algorithms across a range of popular datasets,e.g., TUM RGB-D and ICL-NUIM, in which we achieve an impressive performance through comparing with state of the art methods and provide benefits for existing RGB-D visual odometry and visual SLAM systems.																	0883-9514	1087-6545															10.1080/08839514.2020.1824093		OCT 2020											
J								Defending Against Child Death: Deeplearning-baseddiagnosis method for abnormal identification of fetus ultrasound Images	COMPUTATIONAL INTELLIGENCE										convolutional neural network; deep learning; Down syndrome analysis; fetus diagnosis; nuchal translucency	FETAL ULTRASOUND; PERFORMANCE; WEIGHT	One of the most important industries which protect human from various diseases is the medical industry. Child death is a crucial concern that needs to concentrate on "save the children." Abnormality of a child can be obtained by diagnosing the prenatal by ultrasound system within a specific period for providing better treatment to do "save the children.". This article aimed to diagnose the (prenatal) ultrasound-images by design and implement a novel framework named Defending Against Child Death (DACD). The existing method is a semiautomatic method where it used convolutional neural network (CNN) algorithm for classifying ultrasound images. Real-time medical industry requires a fully automatic method for classifying the ultrasound images to save the human. Hence this article, includes deep learning by implementing five convolutional neural network architectures in an order where it learns, estimate, and confirms the fetus parameters. All the layers in the convolutional neural network extract and classify the different number of features in the ultrasound images automatically and provide a result. The increased number of hidden layers in the CNN can extract even the hidden features of the images. The extracted features are classified automatically and improve the accuracy of disease detection. To segment the fetus abdomen, U-Net architecture is included in the CNN with Hough transformation. The experiment is carried out using the CNN toolbox in MATLAB and the outcomes are verified. The performance of the DACD is assessed by comparing the results with the earlier researches. From the experimental results, it is obtained that the accuracy of DACD is 99.7%, which is higher than the results obtained from the existing machine learning approach.																	0824-7935	1467-8640															10.1111/coin.12394		OCT 2020											
J								Wavelet Interval Type-2 Fuzzy Quad-Function-Link Brain Emotional Control Algorithm for the Synchronization of 3D Nonlinear Chaotic Systems	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Interval type-2 fuzzy system; Quad-function-link network; Brain emotional control algorithm; 3D chaotic systems	NEURAL-NETWORK; DESIGN; MODEL; IDENTIFICATION	This study provides a novel quad-function-link network to adjust the lower and upper weights of a wavelet interval type-2 fuzzy brain emotional structure to increase the response and performance for the synchronization of 3D nonlinear chaotic systems. The proposed control system is a hybrid method that comprises a new wavelet interval type-2 fuzzy quad-function-link brain emotional controller and a robust controller. It contains a fuzzy inference system and three substructures with five layers. The substructures are an amygdala, a prefrontal cortex, and a novel quad-function-link network that can adjust the weights efficiently for the amygdala and prefrontal cortex networks to achieve the synchronization of the master-slave systems well with reduced tracking errors. Then, a Lyapunov stability function is employed to provide the adaptive laws, and they are effectively used online to adjust the system parameters. Finally, simulation studies of two 3D nonlinear chaotic systems are used to verify the superiority and advantage of the proposed algorithm.																	1562-2479	2199-3211															10.1007/s40815-020-00941-7		OCT 2020											
J								A Multitasking Genetic Algorithm for Mamdani Fuzzy System with Fully Overlapping Triangle Membership Functions	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Multitasking genetic fuzzy system; Evolutionary multitasking; Multifactorial optimization; Mamdani fuzzy system; Fully overlapping triangle membership functions	EVOLUTIONARY MULTITASKING; OPTIMIZATION	Evolutionary multitasking is an emerging subject in the field of evolutionary computation. By adopting methods to effectively discover and implicitly transfer useful genetic materials from one task to another, it can process multiple optimization tasks simultaneously using one evolutionary calculation. Inspired by the idea of evolutionary multitasking, it can be also used in optimization problems of fuzzy systems (FSs). By exchanging optimization experience and knowledge between different FSs, it is expected to enhance the speed and efficiency of FS optimization and be applied to FS optimization tasks with higher requirement for running time and accuracy of results. Moreover, using the experience and knowledge of simple FSs optimization tasks to facilitate the optimization of complex FSs, it can resolve high time consuming and high cost that triggered by large, complex FSs optimization problems and improve the feasibility of its application in large complex fuzzy control optimization problems. Different from the general multi-task learning, the multi-task learning of FS optimization has its own features. Consequently, based on the thought of evolutionary multitasking and the traits of multi-task learning of FS optimization, a general framework of multitasking genetic fuzzy system (MTGFS) is proposed to effectively solve the multi-task optimization problems of fuzzy systems. A multitasking evolutionary optimization algorithm for Mamdani fuzzy systems with fully overlapping triangle membership functions (FOTMF-M-MTGFS) is also designed and implemented. Comparative studies with genetic fuzzy system (GFS), a single-task optimization algorithm of FSs, indicate that the evolution speed and result of the MTGFS are superior than GFS on average.																	1562-2479	2199-3211															10.1007/s40815-020-00954-2		OCT 2020											
J								Online Identity Crisis Identity Issues in Online Communities	MINDS AND MACHINES										Identity; Online Communities; Virtual Cognitive Niches; Filter Bubble; Double Mutual Anticipation; Bad Faith; Affordance	ANA	How have online communities affected the ways their users construct, view, and define their identity? In this paper, we will approach this issue by considering two philosophical sets of problems related to personal identity: the "Characterization Question" and the "Self-Other Relations Question." Since these queries have traditionally brought out different problems around the concept of identity, here we aim at rethinking them in the framework of online communities. To do so, we will adopt an externalist and cognitive point of view on online communities, describing them as virtual cognitive niches. We will evaluate and agree with the Attachment Theory of Identity, arguing that there is continuity between offline and online identity and that usually the latter contributes to the alteration of the former. Finally, we will discuss ways users can enact self-reflection on online frameworks, considering the impact of the Filter Bubble and the condition of Bad Faith.																	0924-6495	1572-8641															10.1007/s11023-020-09542-7		OCT 2020											
J								Clover plot: Versatile visualization in nonparametric classification	STATISTICAL ANALYSIS AND DATA MINING										bagdistance; classification; dimension reduction; halfspace depth; illumination; Mahalanobis distance; statistical depth function; visualization	DATA DEPTH	A versatile visualization tool that aggregates several classification techniques is proposed and studied. In the clover plot we combine four complementary classifiers-the depth-depth plot, the bagdistance plot, an approach based on the illumination, and the classical diagnostic plot based on Mahalanobis distances. An interactiveRimplementation of this tool, including several model examples of its use, is freely available online.																	1932-1864	1932-1872															10.1002/sam.11481		OCT 2020											
J								Ranking of Z-numbers based on value and ambiguity at levels of decision making	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										ambiguity; decision levels; ranking; value	FUZZY RISK ANALYSIS; INTERVAL TYPE-2; SIMILARITY MEASURE; STRATEGIES; SYSTEMS; UTILITY; DESIGN; MODEL	The concept of Z-number, is very new in the literature, proposed by Zadeh in 2011. The Z-numberZ=(A,B)is a pair of fuzzy numbers where the first component represents the restriction and the second component represents the certainty of the first component. Further, its application is evident in decision-making problems, risk assessment, linear programming problems, and so forth. Hence, under such circumstances ranking of Z-numbers need an utmost attention, as such, a few methods of ranking Z-numbers are proposed by various researcher. However, in many situations the existing methods depict drawbacks and limitations as discussed in this paper. Hence, a new method of ranking Z-numbers is essential for an appropriate decision-making. In this paper, a new method of ranking Z-numbers based on the concept of value and ambiguity at levels of decision-making have been proposed. The method seems to deliver a reasonable decision and proper ranking of Z-numbers of various types. A few numerical examples are discussed which show the out-performance of the proposed method.																	0884-8173	1098-111X															10.1002/int.22301		OCT 2020											
J								A time controlling neural network for time-varying QP solving with application to kinematics of mobile manipulators	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										convergent time; quadratic programming; time controlling neural network; trajectory control	QUADRATIC-PROGRAMMING PROBLEMS; CONVERGENCE	To obtain the solution for time-varying quadratic programming (QP), a time controlling neural network (TCNN) is presented and discussed. The traditional recurrent neural networks provide a prospect for real-time calculations and repeatable trajectory control of the mobile manipulators due to its high executing processing and nonlinear disposal ability. However, the convergent time is still a considerable point for the solution of a dynamic system dealing with synchronism and robustness. In this note, a TCNN model by incorporating an initial rectified term is applied to solve the online calculation problems and the convergent time can be controlled in advance. Theoretical analyses on stability, prespecified time and convergence are rigorously clarified. Finally, effectiveness and precision of the TCNN model for the solution of a QP example have been verified. In addition, a repetitive trajectory planning for a three-wheel manipulator is introduced to demonstrate the superiority of the TCNN.																	0884-8173	1098-111X															10.1002/int.22304		OCT 2020											
J								Data security sharing model based on privacy protection for blockchain-enabled industrial Internet of Things	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										blockchain; data sharing; information security; IIoT; privacy protection	GROUP KEY AGREEMENT	With the widespread application of Industrial Internet of Things (IIoT) technology in the industry, the security threats are also increasing. To ensure the safe sharing of resources in IIoT, this paper proposes a data security sharing model based on privacy protection (DSS-PP) for blockchain-enabled IIoT. Compared with previous works, DSS-PP has obvious advantages in several important aspects: (1) In the process of identity authentication, it protects users' personal information by using authentication technology with hidden attributes; (2) the encrypted shared resources are stored in off-chain database of the blockchain, while only the ciphertext index information is stored in the block. It reduces the storage load of the blockchain; (3) it uses blockchain logging technology to trace and account for illegal access. Under the hardness assumption of Inverse Computational Diffe-Hellman (ICDH) problem, this model is proven to be correct and safe. Through the analysis of performance, DSS-PP has better performance than the referred works.																	0884-8173	1098-111X															10.1002/int.22293		OCT 2020											
J								Video object detection for autonomous driving: Motion-aid feature calibration	NEUROCOMPUTING										Video object detection; Autonomous driving; Motion estimation		This paper proposes an end-to-end deep learning framework, termed as motion-aid feature calibration network (MFCN), for video object detection. The key idea is to leverage on the temporal coherence of video features while considering their motion patterns as captured by optical flow. To boost detection accuracy, the framework aggregates the calibrated features both at pixel and instance levels across frames to achieve improved robustness despite appearance variations. The aggregation and calibration are efficiently and adaptively conducted based on an integrated optical flow network. Meanwhile, the entire architecture of the proposed method is end-to-end, thus significantly improving its training and inference efficiency when compared to multi-stage methods for video object detection. Evaluations on KITTI and ImageNet VID indicate that MFCN can improve the results of a strong still-image detector by 11.2% and 7.31% respectively. MFCN also outperforms other competitive video object detectors and achieves a better trade-off between accuracy and runtime speed, demonstrating its potential for use in autonomous driving systems. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				OCT 7	2020	409						1	11		10.1016/j.neucom.2020.05.027													
J								Asymmetric multi-stage CNNs for small-scale pedestrian detection	NEUROCOMPUTING										Pedestrian detection; Deep learning; Convolution kernels		A critical bottleneck in pedestrian detection is the detection of small-scale pedestrians, which have low contrast and blurry shapes in images and videos. Considered that the body shape of a pedestrian is always rectangular (the height is greater than the width), we propose an asymmetric multi-stage network (AMS-Net) for small-scale pedestrian detection. The proposed method has two main advantages. (1) It considers the asymmetry of a pedestrian's body shape in pedestrian detection. The rectangular anchors are used to generate various rectangular proposals that have a height greater than the width. In addition, asymmetric rectangular convolution kernels are adopted for capturing the compact features of the pedestrian body. (2) The proposed AMS-Net gradually rejects the non-pedestrian boxes according to coarse-to-fine fea-tures in a three-stage framework. The proposed AMS-Net significantly improves the performance of pedestrian detection on the Far subset of the Caltech testing set (the miss rate decreases from 60.79% to 51.36%). It also achieves competitive performance on the INRIA, ETH, KITTI and CityPersons benchmarks. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				OCT 7	2020	409						12	26		10.1016/j.neucom.2020.05.019													
J								Exploring firms' innovation capabilities through learning systems	NEUROCOMPUTING										Machine learning; Innovation input capability; Collaborative innovation capability; Innovation performance; XGBoost; GBDT	MANAGEMENT	In this study, several machine learning-based experimental methods are used to analyse firms' research and development (R&D)-related activities and predict their technological innovation performance. Using unbalanced panel data from the CSMAR database for all listed firms in China from 2008 to 2018, we analyse the firms' basic information, R&D investment, patent application and authorization activity, financial status, and human capital. We use a logistic regression model, decision tree model, three weak classifiers random forest model, XGBoost model, and two weak classifiers gradient boosting decision tree (GBDT) model to integrate strong classifiers separately. A comparison of the results produced using the different models shows that the performance of the XGBoost model is better than that of the other models in terms of net profit, total sales revenue, and the number of invention patent applications as a proportion of the total number of patent applications. However, the performance of the GBDT model is significantly better than that of the other models in terms of the number of patent applications per 100,000 yuan of R&D expenditure. The results of this study can help scholars to accurately predict the innovation performance of firms and help business managers to make better decisions to improve the innovation performance of their firms in the current era of rapid technological change. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				OCT 7	2020	409						27	34		10.1016/j.neucom.2020.03.100													
J								Wasserstein distance based deep adversarial transfer learning for intelligent fault diagnosis with unlabeled or insufficient labeled data	NEUROCOMPUTING										Transfer learning; Domain adaptation; Wasserstein distance; Convolutional neural networks; Intelligent fault diagnosis		Intelligent fault diagnosis is one critical topic of maintenance solution for mechanical systems. Deep learning models, such as convolutional neural networks (CNNs), have been successfully applied to fault diagnosis tasks and achieved promising results. However, one is that two datasets (in source and target domains) of similar tasks are with different feature distributions because of different operational conditions; another one is that insufficient or unlabeled data in real industry applications (target domains) limit the adaptability of the source domain well-defined models. To solve the above problems, the concept of transfer learning should be adopted for domain adaptation, in the meantime, a network performs both supervised and unsupervised learning is required. Inspired by Wasserstein distance of optimal transport, in this paper, we propose a novel Wasserstein Distance-based Deep Transfer Learning (WD-DTL) network for both supervised and unsupervised fault diagnosis tasks. WD-DTL learns domain feature representations (generated by a CNN based feature extractor) and minimizes distributions between the source and target domains through an adversarial training process. The effectiveness of the proposed WD-DTL is verified through 16 different transfer tasks. Results show that WD-DTL achieves the highest diagnostic accuracies when compared to the existing Maximum Mean Discrepancy and CNN networks in almost all transfer tasks. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				OCT 7	2020	409						35	45		10.1016/j.neucom.2020.05.040													
J								Deep multilevel similarity hashing with fine-grained features for multi-label image retrieval	NEUROCOMPUTING										Multi-label; Image retrieval; Deep hashing; Fine-grained features; Contrastive loss		For multi-label image retrieval based on deep hashing, the ultimate challenge is to map from the original image to binary space while preserving high-level semantic similarity. Recently, many supervised deep hashing approaches for multi-label image retrieval have been proposed to generate high-quality binary codes. However, most such methods are only introduced to learn simple similarity based on these image characteristics, therein ignoring complex multilevel semantic similarity with fine-grained features. In this paper, we propose a framework named deep hashing with fine-grained feature learning (DH-FFL) to preserve complex multilevel semantic similarity between multi-label image pairs. In this proposed model, compact bilinear pooling convolutional neural networks (CNNs) with normalization are adopted to extract fine-grained feature descriptors. In addition, a novel multilevel contrastive loss is designed to preserve multilevel semantic similarity by introducing a zero-loss parameter. Moreover, a multi-label classification loss is used to maintain the unique semantic structure of each image and maximize the distinguishing ability of binary codes. Comprehensive experiments on three benchmark datasets show that the proposed DH-FFL achieves promising performance compared with other state-of-the-art multi-label image retrieval applications. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				OCT 7	2020	409						46	59		10.1016/j.neucom.2020.04.125													
J								Deep transfer neural network using hybrid representations of domain discrepancy	NEUROCOMPUTING										Hybrid representations; Transfer neural networks; Adversarial training; MMD; Domain confusion; Viewpoint estimation	KERNEL	Transfer neural networks have been successfully applied in many domain adaptation tasks. The initiative of most of the current transfer networks, essentially, is optimizing a single distance metric between the source domain and target domain, while few studies integrate multiple metrics for training transfer networks. In this paper, we propose an architecture of transfer neural network equipped with hybrid representations of domain discrepancy, which could incorporate the advantages of different types of metrics as well as compensate their imperfections. In our architecture, the Maximum Mean Discrepancy (MMD) and H-divergence based domain adaptations are combined for simultaneous distribution alignment and domain confusion. Through extensive experiments, we find that the proposed method is able to achieve compelling transfer performance across the datasets with domain discrepancy from small scale to large scale. Especially, the proposed method can be promisingly used to predict the viewpoint of 3D-printed workpiece even trained without labels of real images. The visualization of learned features and adapted distributions by our transfer network highlights that the proposed approach could effectively learn the similar features between two domains and deal with a wide range of transfer tasks. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				OCT 7	2020	409						60	73		10.1016/j.neucom.2020.05.020													
J								Incorporating uncertainties in student response modeling by loss function regularization	NEUROCOMPUTING										Uncertainty; Deep learning; Recurrent neural network; Student response modeling; Knowledge tracing		Most existing research works involving deep learning focus on performance improvement by developing new architectures or regularizers. However, in this paper we study the modeling of uncertainty in recurrent networks for the application of student response modeling, more specifically, knowledge tracing. Knowledge tracing is an application of time series machine learning. It consists of inferring the mastery level of a skill for a student as they navigate a question bank, thus adjusting curriculum for efficient learning. Deep Knowledge Tracing (DKT) takes the deep learning approach for knowledge tracing and has achieved better results than models like Bayesian Knowledge Tracing (BKT) and Performance Factor Analysis (PFA). However, the opaqueness of these deep knowledge tracing models also brings some criticisms. Providing an uncertainty score for each prediction helps mitigate this opaqueness. To investigate uncertainty modeling in DKT, we first examine a popular way of modeling data dependent uncertainties using Monte Carlo and show how it is insufficient to model variance in data. Second, we show how to incorporate sensible uncertainties by explicitly regularizing the cross entropy loss function. Third, we evaluate our method both in three different real datasets and in a more controlled way using synthetic data. Using synthetic data allows us to quantitatively understand the generated uncertainties. The results show that our method provides comparable results with standard deep knowledge tracing models as well as meaningful prediction uncertainties. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				OCT 7	2020	409						74	82		10.1016/j.neucom.2020.05.035													
J								An unsupervised image segmentation method combining graph clustering and high-level feature representation	NEUROCOMPUTING										Segmentation; Unsupervised; Autoencoder architecture; Clustering		Image segmentation is one of the most important assignments in computer vision. In this paper, we present an unsupervised segmentation method that combines graph-based clustering and high-level semantic features. We over-segment the given image into a collection of superpixels. Various low-level features assemble a descriptor of each superpixel. Besides the intrinsic image features such as color, texture and gradient, we add image saliency into the low-level visual features as prior knowledge of human perception. Instead of using the low-level features directly, we design a graph-based method to segment the image by clustering the high-level semantic features learned from a neural network. We test the proposed method on two well-known datasets. The experimental evaluation validates that our approach can provide consistent and meaningful segmentation. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				OCT 7	2020	409						83	92		10.1016/j.neucom.2020.05.073													
J								A non-specialized ensemble classifier using multi-objective optimization	NEUROCOMPUTING										Ensemble classification; Multi-objective optimization; Genetic algorithm; Multiple classifiers; Classifier selection; Diversity; Double-fault measure	NONNEGATIVE MATRIX FACTORIZATION; PARTICLE SWARM OPTIMIZATION; FEATURE-SELECTION; DIVERSITY; REGRESSION; FRAMEWORK; PERFORMANCE; ACCURACY; SPARSITY; FUSION	Ensemble classification algorithms are often designed for data with certain properties, such as imbalanced class labels, a large number of attributes, or continuous data. While high-performing, these algorithms sacrifice performance when applied to data outside the targeted domain. We propose a non-specific ensemble classification algorithm that uses multi-objective optimization instead of relying on heuristics and fragile user-defined parameters. Only two user-defined parameters are included, with both being found to have large windows of values that produce statistically indistinguishable results, indicating the low level of expertise required from the user to achieve good results. Additionally, when given a large initial set of trained base-classifiers, we demonstrate that a multi-objective genetic algorithm aiming to optimize prediction accuracy and diversity will prefer particular types of classifiers over others. The total number of chosen classifiers is also surprisingly small - only 10.14 classifiers on average, out of an initial pool of 900. This occurs without any explicit preference for small ensembles of classifiers. Even with these small ensembles, significantly lower empirical classification error is achieved compared to the current state-of-the-art. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				OCT 7	2020	409						93	102		10.1016/j.neucom.2020.05.029													
J								Supervised learning in spiking neural networks with synaptic delay-weight plasticity	NEUROCOMPUTING										Spiking neurons; Spiking neural networks; Supervised learning; Synaptic plasticity; Synaptic weight; Synaptic delay	ALGORITHM; CLASSIFICATION; RESUME	Spiking neurons encode information through their spiking temporal patterns. Although the precise spike-timing based encoding scheme has long been recognised, the exact mechanism that underlies the learning of such precise spike-timing in the brain remains an open question. Most of the existing learning methods for spiking neurons are based on synaptic weight adjustment. However, biological evidences suggest that synaptic delays can also be modulated to play an important role in the learning process. This paper investigates the viability of integrating synaptic delay plasticity into supervised learning and proposes a novel learning method that adjusts both the synaptic delays and weights of the learning neurons to make them fire precisely timed spikes, that is referred to as synaptic delay-weight plasticity. Remote Supervised Method (ReSuMe) and Perceptron Based Spiking Neuron Learning Rule (PBSNLR), two representative supervised learning methods, are studied to illustrate how the synaptic delay-weight plasticity works. The performance of the proposed learning method is thoroughly evaluated on synthetic data and is further demonstrated on real-world classification tasks. The experiments show that the synaptic delay-weight learning method outperforms the traditional synaptic weight learning methods in many ways. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				OCT 7	2020	409						103	118		10.1016/j.neucom.2020.03.079													
J								Stochastic region pooling: Make attention more expressive	NEUROCOMPUTING										Convolutional network; Channel-wise attention; Pooling; Channel descriptor	NEURAL-NETWORKS; IMAGE	Global Average Pooling (GAP) is used by default on the channel-wise attention mechanism to extract channel descriptors. However, the simple global aggregation method of GAP is easy to make the channel descriptors have homogeneity, which weakens the detail distinction between feature maps, thus affecting the performance of the attention mechanism. In this work, we propose a novel method for channel-wise attention network, called Stochastic Region Pooling (SRP), which makes the channel descriptors more representative and diversity by encouraging the feature map to have more or wider important feature responses. Also, SRP is the general method for the attention mechanisms without any additional parameters or computation. It can be widely applied to attention networks without modifying the network structure. Experimental results on image recognition datasets including CIAFR-10/100, ImageNet and three Fine-grained datasets (CUB-200-2011, Stanford Cars and Stanford Dogs) show that SRP brings the significant improvements of the performance over efficient CNNs and achieves the state-of-the-art results. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				OCT 7	2020	409						119	130		10.1016/j.neucom.2020.05.049													
J								Discriminant sub-dictionary learning with adaptive multiscale superpixel representation for hyperspectral image classification	NEUROCOMPUTING										Dictionary learning; Hyperspectral image classification; Sparse representation; Discriminative sub-dictionary; Adaptive multiscale superpixel	SPARSE REPRESENTATION; FACE RECOGNITION; K-SVD	Sparse representation and dictionary learning have been successfully applied in hyperspectral image classification. Generally, it is more effective to learn the sub-dictionary for each class and utilize multiple scale strategy. However, the sub-dictionary may only consider the within-class information and ignore the discriminative information. For multiscale sparse representation, the shape of the regions would not adaptively change according to context structure. This paper proposes a discriminative sub-dictionary learning algorithm and an adaptive multiscale superpixel classification strategy under sparse representation framework for hyperspectral image classification. In dictionary learning stage, the global constraint term can ensure the learnt sub-dictionaries contain more discriminative information. In sparse representation stage, the adaptive multiscale superpixel strategy can exploit the spatial context according to the structural information. The combination of the two strategies can further explore the spatial and spectral information in hyperspectral image. Experiments on four hyperspectral image datasets prove that the proposed method has a significant improvement in both quantitative and qualitative analysis comparing with state-of-the-art algorithms. Moreover, the proposed algorithm can produce satisfactory results for the imbalanced data problem involved in hyperspectral image. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				OCT 7	2020	409						131	145		10.1016/j.neucom.2020.05.082													
J								Quasi-synchronization of neural networks with diffusion effects via intermittent control of regional division	NEUROCOMPUTING										Intermittent control; Lyapunov function; Synchronization; Region division	DELAYS; STABILITY	In this paper, we investigate the quasi-synchronization issue of an array of Diffusion Effects Neural Networks (DENNs) with time-varying coupling strength. Two kinds of quasi-synchronization scenarios are discussed. One is self-synchronization and the other is tracking-synchronization. In order to lower the communication loads and reduce the control resource consumption, we propose a new control scheme called intermittent control of reginal division to make the synchronization error within a bounded scope. In this novel control scheme, the non-negative real region is divided into three subregions in advance and an auxiliary Lyapunov function is imported to decide the work time and the rest time by its relationship with three subregions. Moreover, by using rigorous mathematic analysis, we obtain sufficient conditions for the two kinds of quasi-synchronization modes of DENNs under the as-proposed control scheme. Nonidentical term in the network is viewed as external disturbance about to be compensated by the controller. The result shows that the upper bound of synchronization error will be progressively regulated by the parameters in chosen divided region. Several numerical examples are also presented to verify the validity of our theory. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				OCT 7	2020	409						146	156		10.1016/j.neucom.2020.05.037													
J								OSVFuseNet: Online Signature Verification by feature fusion and depth-wise separable convolution based deep learning	NEUROCOMPUTING										Online Signature Verification; Depth-wise separable convolutional neural networks; Few shot learning; Autoencoders	SELECTION; SYSTEM	Online Signature Verification (OSV) techniques have been deployed in production systems for decades, yet training the model for efficient classification of the test signature from fewer training signature samples is still an open challenge. The advancements in Convolutional Neural Networks (CNNs) enormously boosted the effectiveness of OSV systems. However, learning subtle and discriminating representations from few training samples to classify the genuineness of test signature has not been explored fully. In this paper, a Convolution Autoencoder (CAE) is used to obtain high-level feature representations from the input signature and these high level features are fused with handcrafted features to constitute a hybrid feature set. The hybrid set of features is presented as an input to an Online Signature Verification framework made up of Depth-wise Separable Convolutional Neural Network (DWSCNN). DWSCNN effectively learn deep feature representations with fewer training samples and parameters than traditional CNNs resulting in a light weight OSV framework. Thorough experimental analysis on three benchmark datasets MCYT-100 (DB1), SVC-2004-Task2 and SUSIG-Visual corpus confirm that the proposed hybrid fusion of feature set and DWSCNN based OSV framework achieve higher classification accuracies and outperforms many contemporary and state-of-the art OSV models. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				OCT 7	2020	409						157	172		10.1016/j.neucom.2020.05.072													
J								A deep domain adaption model with multi-task networks for planetary gearbox fault diagnosis	NEUROCOMPUTING										Domain adaption; Intelligent fault diagnosis; Planetary gearbox; Convolutional neural network; Convolutional autoencoder	CONVOLUTIONAL NEURAL-NETWORK; ROTATING MACHINERY; BEARINGS; TOOL	The planetary gearbox plays an important role in many advanced electromechanical mechanisms. The mechanical fault is a major factor that threatens the service performance of a planetary gearbox. Deep learning (DL) algorithms have been widely used to identify faults and health status of industrial equipment. However, owing to diversified equipment structures, variable working conditions and disparate data acquisition, the service performance of DL-based methods may degrade significantly when applied to different industrial sites. Domain adaptation emerges as a promising idea that aims to transfer knowledge from a source domain to a different but related target domain. In this paper, we introduce a novel deep domain-adaptive multi-task learning model Y-Net, which is exploited to enable domain-adaptive diagnosis of faults in planetary gearboxes. The SE-Res modules are utilized to reduce the redundancy of the model and improving the separability of deep features. Furthermore, a soft joint maximum mean discrepancy (SJMMD) is introduced to link the two pipeline in order to reduce both the marginal and conditional distribution discrepancy of the learned features, with the enhancement of auxiliary soft labels. The domain adaption between different planetary gearbox under variant operating condition is realized by the Y-Net. Experiments demonstrate the superiority of the proposed SJMMD over conventional maximum mean discrepancy, especially when the datasets of different domains suffer different imbalances. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				OCT 7	2020	409						173	190		10.1016/j.neucom.2020.05.064													
J								CompactNet: learning a compact space for face presentation attack detection	NEUROCOMPUTING										Face PAD; Biometrics; Compact space; Deep learning	SPOOFING DETECTION; LIVENESS DETECTION; IMAGE QUALITY	Face presentation attack detection (PAD) has become a clear and present threat for face recognition systems and many countermeasures have been proposed to mitigate it. In these countermeasures, some of them use the features directly extracted from well-known color spaces (e.g., RGB, HSV and YCbCr) to distinguish the fake face images from the genuine ("live") ones. However, the existing color spaces have been originally designed for displaying the visual content of images or videos with high fidelity and are not well suited for directly discriminating the live and fake face images. Therefore, in this paper, we propose a deep-learning system, called CompactNet, for learning a compact space tailored for face PAD. More specifically, the proposed CompactNet does not directly extract the features in existing color spaces, but inputs the color face image into a layer-by-layer progressive space generator. Then, under the optimization of the "points-to-center" triplet loss function, the generator learns a compact space with small intra-class distance, large inter-class distance and a safe interval between different classes. Finally, the feature of the image in compact space is extracted by a pre-trained feature extractor and used for image classification. Reported experiments on three publicly available face PAD databases, namely, the Replay-Attack, the OULU-NPU and the HKBU-MARs V1, show that CompactNet separates very well the two classes of fake and genuine faces and significantly outperforms the state-of-the-art methods for PAD. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				OCT 7	2020	409						191	207		10.1016/j.neucom.2020.05.017													
J								Leader-following consensus control of position-constrained multiple Euler-Lagrange systems with unknown control directions	NEUROCOMPUTING										Leader-following consensus control; Barrier Lyapunov control; Nussbaum gain control; Adaptive control; Multiple Euler-Lagrange systems	NONLINEAR MULTIAGENT SYSTEMS; BARRIER LYAPUNOV FUNCTIONS; UNCERTAIN MECHANICAL SYSTEMS; TRACKING CONTROL; ADAPTIVE CONSENSUS; COOPERATIVE CONTROL; SENSOR NETWORKS; ALGORITHMS; FEEDBACK; AGENTS	This paper studies the distributed leader-following consensus control problems of position-constrained multiple Euler-Lagrange Systems (ELSs) with unknown control directions. The objective of our discussion is to develop a consensus protocol despite the existence of position constraints and unknown control directions in each agent. Most of the existing works related to state constraints are for single systems and rare results are concerned about multi-agent systems. With the help of the barrier Lyapunov function, Nussbaum gain adjusting technique, and adaptive control methodology, a novel leader-following consensus algorithm is successfully designed to make sure the convergence to zero of the consensus errors and the boundedness of the signals in the closed-loop system. Moreover, the position constraints for each agent are never violated for all time during the control process. The effectiveness of the main result is illustrated by a practical mechanical system. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				OCT 7	2020	409						208	216		10.1016/j.neucom.2020.05.058													
J								Truncated prediction-based distributed consensus control of linear multi-agent systems with discontinuous communication and input delay	NEUROCOMPUTING										Distributed consensus control; Event-triggered control; Input delay; Directed graph; Uniform quantization	LEADER-FOLLOWING CONSENSUS; EVENT-BASED CONSENSUS; FAILURES; FEEDBACK; SUBJECT; AGENTS	In this paper, distributed consensus control protocols are given for linear multi-agent systems with input delay under direct communication graphs. Compared with the multi-agent systems under continuous communication, there are more challenges in the consensus problems of multi-agent systems under discontinuous communication. Furthermore, the consensus of multi-agent systems with control input delay and discontinuous communication are much more similar to the practical engineering situation, which further increases the difficulties of consensus control. Based on these discussions, two consensus problems, including leader following case and leaderless case, are studied in this paper. A prediction of the system's state information over the delay time is approximated by the zero input solution of the agent system model. We also propose a new discontinuous communication mode, which combines event triggered mechanism and a uniform quantization method. Unlike the existing discontinuous communication method, our proposed control protocols ensure that each agent decides when to transport its state information to its neighbors such that continuous communication between any two agents is no longer needed. In addition, through the non-smooth analysis of the closed-loop systems, the uniformly bounded consensus results are guaranteed, and the bound of the consensus error can be adjusted by choosing appropriate parameters. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				OCT 7	2020	409						217	230		10.1016/j.neucom.2020.05.048													
J								RoSANE: Robust and scalable attributed network embedding for sparse networks	NEUROCOMPUTING										Attributed Network Embedding; Sparse networks; Ball-tree K-nearest neighbors; Random walks; Skip-gram model		Attributed networks can better describe the real-world complex systems where the interaction or relationship between entities can be represented as networks and the auxiliary information can be represented as node attributes. Attributed Network Embedding (ANE) is attracting much attention. It utilizes network topology and node attributes to jointly learn enhanced low-dimensional node embeddings so as to facilitate various downstream inference tasks. However, the existing ANE methods cannot effectively embed attributed sparse networks which are important real-world scenarios, and/or are not scalable to large-scale networks. To tackle these challenges, we first integrate network topology and node attributes to reconstruct an enriched denser network, and then learn node embeddings upon the denser network. In above two steps, the techniques such as Ball-tree K-Nearest Neighbors and random walks based Skip-Gram model are adopted to guarantee the scalability, which is demonstrated via theoretical complexity analysis. The extensive empirical studies show the effectiveness and efficiency of the proposed method, as well as its robustness to different networks or the same network with different sparsities. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				OCT 7	2020	409						231	243		10.1016/j.neucom.2020.05.080													
J								A survey on U-shaped networks in medical image segmentations	NEUROCOMPUTING										U-shaped network; Convolutional neural networks; Medical image segmentation; Extended mechanism	WHITE-MATTER HYPERINTENSITIES; STROKE LESION SEGMENTATION; NEURAL-NETWORK; AUTOMATIC SEGMENTATION; MR-IMAGES; DEEP; CLASSIFICATION; REPERFUSION; DIFFUSION; DROPOUT	The U-shaped network is one of the end-to-end convolutional neural networks (CNNs). In electron microscope segmentation of ISBI challenge 2012, the concise architecture and outstanding performance of the U-shaped network are impressive. Then, a variety of segmentation models based on this architecture have been proposed for medical image segmentations. We present a comprehensive literature review of U-shaped networks applied to medical image segmentation tasks, focusing on the architectures, extended mechanisms and application areas in these studies. The aim of this survey is twofold. First, we report the different extended U-shaped networks, discuss main state-of-the-art extended mechanisms, including residual mechanism, dense mechanism, dilated mechanism, attention mechanism, multi-module mechanism, and ensemble mechanism, analyze their pros and cons. Second, this survey provides the overview of studies in main application areas of U-shaped networks, including brain tumor, stroke, white matter hyperintensities (WMHs), eye, cardiac, liver, musculoskeletal, skin cancer, and neuronal pathology. Finally, we summarize the current U-shaped networks, point out the open challenges and directions for future research. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				OCT 7	2020	409						244	258		10.1016/j.neucom.2020.05.070													
J								Cost-conscious mutual information maximization for improving collective interpretation of multi-layered neural networks	NEUROCOMPUTING											BLACK-BOX; FEATURE-SELECTION; MODELS; INPUT; RULES; REPRESENTATIONS; ORGANIZATION; STRATEGY; ENTROPY; MAPS	The present paper aims to improve the collective interpretation realized by compressing multi-layered neural networks and to make the interpretation as natural and stable as possible. We collectively interpret the final representations by maximizing mutual information between inputs and neurons, expecting that mutual information maximization can disentangle complex features into simpler ones. However, we have had difficulty in increasing mutual information and in obtaining interpretable features for several data sets. By examining closely the processes of information maximization, we found that, in addition to the information maximization, we need to consider the cost associated with this information maximization. Thus, we try to maximize not simply mutual information but the ratio of mutual information to the cost, and this method can be called ''cost-conscious mutual information maximization." The cost-conscious method aims to extend Linsker's maximum information preservation principle to a variety of data sets by more directly taking into account the cost associated with the process of information maximization. The method was applied to two data sets: the artificial and symmetric data set and the credit default data set. First, by using the symmetric data set injected with random noises, the cost-conscious information maximization method could extract the symmetric property almost perfectly against the random noises. In the experimental results on the credit default data set, the present method could make it possible to interpret the final results the most naturally, showing why and how the credit default could occur very naturally. The experimental results show that the neural networks can be used to interpret data sets more naturally than the conventional methods such as the logistic regression analysis. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				OCT 7	2020	409						259	274		10.1016/j.neucom.2020.04.127													
J								Learning local discriminative representations via extreme learning machine for machine fault diagnosis	NEUROCOMPUTING										Extreme learning machine; Autoencoder; Discriminative information; Local geometry; Machine fault diagnosis	DIMENSIONALITY; FRAMEWORK; SCHEME	Recently, learning data representations have been investigated to reduce the dependences of human intervention and improve the performance of machine fault diagnosis. However, most of the representation learning methods are computationally intensive due to complex training procedures. Extreme learning machine is well-known for its fast training speed and strong generalization ability. It also has been applied to learn data representations for clustering and classification tasks. In this paper, a local discriminant preserving extreme learning machine autoencoder (LDELM-AE) is proposed to learn data representations with the local geometry and local discriminant exploited from the input data. Specifically, LDELM-AE utilizes two graphs to enhance the within-class compactness and between-class separability, respectively. Furthermore, the hierarchical representations can be obtained by stacking several LDELM-AEs. On several benchmark datasets, the proposed method demonstrates better classification accuracies than the state-of-the-art methods. Moreover, the proposed method has been used to diagnostic the rotary machine faults and achieves the diagnostic accuracy of 99.96%, which proves the proposed method is an efficient tool to diagnose machine faults. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				OCT 7	2020	409						275	285		10.1016/j.neucom.2020.05.021													
J								Neural network based tracking control for an elastic joint robot with input constraint via actor-critic design	NEUROCOMPUTING										Elastic joint robot; Neural networks; Actor-critic design; Adaptive control; Input constraint	SYSTEMS	In this paper, adaptive control with actor-critic design is proposed for an elastic joint robot to cope with the tracking problems. In the critic network, a cost function is used to judge the control performance. Since it is unmeasurable and cannot be calculated, a neural network is utilized to approximate it. In the actor network, based on the critic results as the reinforcement signal, another neural network is used to cope with the system uncertainties and generate the control input. Furthermore, input constraint is imposed on the elastic joint robot to guarantee the normal operation. A high gain observer is also adopted to estimate the unmeasurable variables. With the given adaptive laws of the critic neural network and actor neural network, through the Lyapunov direct method, the stability and convergence of the closed-loop system are achieved and the tracking errors are ultimately uniformly bounded. In addition, simulations are also conducted on a two-link elastic joint robot to illustrate the feasibility and effectiveness of the proposed adaptive constrained actor-critic learning based control. (C) 2020 Published by Elsevier B.V.																	0925-2312	1872-8286				OCT 7	2020	409						286	295		10.1016/j.neucom.2020.05.067													
J								Composite NNs learning full-state tracking control for robotic manipulator with joints flexibility	NEUROCOMPUTING										Flexible-joint manipulator; Composite NNs learning; Time-varying disturbances; TSMDOB; Prediction error	SYNCHRONIZATION CONTROL; BACKSTEPPING CONTROL; CONTROL DESIGN; SYSTEMS	In this paper, the tracking control problem of flexible-joint manipulator (FJM) system subjected to system uncertainties and time-varying external disturbances is addressed. Firstly, a novel composite neural networks learning (CNNL) control framework that blends the advantage of neural networks (NNs) and terminal sliding mode disturbance observer (TSMDOB) is proposed. Furthermore, based on the backstepping theory, a full-state CNNL tracking control scheme is developed for the FJM system. Then the link-side performance is enhanced without using high-order derivatives of the link states. NNs are employed to approximate the system uncertainties of FJM and TSMDOB is mainly used to deal with the time-varying external disturbances. Significantly, the system convergence speed and accuracy are improved with added prediction errors under a composite framework. Finally, to validate the effectiveness of the proposed method, numerous simulation results on 2-link flexible-joint robotic manipulator are provided. In comparison to the other state-of-the-art approaches, the proposed control strategy possesses several advantages. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				OCT 7	2020	409						296	305		10.1016/j.neucom.2020.04.116													
J								Toward effective mobile encrypted traffic classification through deep learning	NEUROCOMPUTING										Android apps; Deep learning; Encrypted traffic; iOS apps; Machine learning; Mobile apps; Privacy; Traffic classification	NETWORK; INTERNET	Traffic Classification (TC), consisting in how to infer applications generating network traffic, is currently the enabler for valuable profiling information, other than being the workhorse for service differentiation/blocking. Further, TC is fostered by the blooming of mobile (mostly encrypted) traffic volumes, fueled by the huge adoption of hand-held devices. While researchers and network operators still rely on machine learning to pursue accurate inference, we envision Deep Learning (DL) paradigm as the stepping stone toward the design of practical (and effective) mobile traffic classifiers based on automatically-extracted features, able to operate with encrypted traffic, and reflecting complex traffic patterns. In this context, the paper contribution is fourfold. First, it provides a taxonomy of the key network traffic analysis subjects where DL is foreseen as attractive. Secondly, it delves into the non-trivial adoption of DL to mobile TC, surfacing potential gains. Thirdly, to capitalize such gains, it proposes and validates a general framework for DL-based encrypted TC. Two concrete instances originating from our framework are then experimentally evaluated on three mobile datasets of human users' activity. Lastly, our framework is leveraged to point to future research perspectives. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				OCT 7	2020	409						306	315		10.1016/j.neucom.2020.05.036													
J								Controllability of discrete-time multi-agent systems based on absolute protocol with time-delays	NEUROCOMPUTING										MASs; Controllability; Absolute protocol; Time-delays	STRUCTURAL CONTROLLABILITY; OBSERVABILITY	This paper investigates complete controllability and structural controllability of discrete-time multi-agent systems (MASs) under a leader-follower framework with time-delays based on absolute protocol, respectively, where both a single time-delay and multiple time-delays are investigated. By using the equivalent augmented systems without time-delays, some new graph-theoretic and algebraic characterizations are respectively built for complete controllability and structural controllability depending on communication flows of MASs. In addition, the simulation results are given. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				OCT 7	2020	409						316	328		10.1016/j.neucom.2020.05.046													
J								Joint multi-level attentional model for emotion detection and emotion-cause pair extraction	NEUROCOMPUTING										Information extraction; Emotion detection; Emotion cause extraction; Neural networks		Emotion detection (ED) and emotion-cause pair extraction (ECPE) have drawn extensive research interests due to their wide applications in real-world scenarios. However, existing work fails to capture the implicit connection between two tasks. This limits the performances of these tasks. To address this issue, we propose a novel joint framework to take full advantage of the clause-level and word-level information about ED and ECPE tasks. Specifically, we explore a multi-level attentional module to model the relationship between two clauses in an emotion-cause pair. Results on two benchmark datasets show that our proposed model achieves the current best performance, outperforming the previous methods and strong neural baselines by a large margin. Our code is available at https://github.com/tomsonsgs/ LVE- joint- MANN-master. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				OCT 7	2020	409						329	340		10.1016/j.neucom.2020.03.105													
J								Infrared facial expression recognition via Gaussian-based label distribution learning in the dark illumination environment for human emotion detection	NEUROCOMPUTING										Facial expression recognition; Convolutional neural network; Near-infrared; Correlation emotion label distribution; Human emotion detection		Facial expression recognition task as a crucial step for emotion recognition remains an open challenge that due to individual expression correlation/ambiguity. In this paper, to tackle these challenges, a novel model with the correlation emotion label distribution learning is proposed for near-infrared (NIR) facial expression recognition which associates multiple emotions with each expression depend on the similarity of expressions. Firstly, the similarities of the seven basic expressions are calculated, and then guide the correlation emotion label distribution by predicting the latent label probability distribution of the expression. Furthermore, the proposed model can be learned in an end-to-end manner via a constructed convolutional neural network to classify the six basic facial expressions. Experimental results on Oulu_CASIA database demonstrate that the proposed method has achieved the superior performance on NIR expression recognition. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				OCT 7	2020	409						341	350		10.1016/j.neucom.2020.05.081													
J								STBNN: Hardware-friendly spatio-temporal binary neural network with high pattern recognition accuracy	NEUROCOMPUTING										Binary neural networks; Hardware-friendly; Spatio-temporal coding; Spiking neural networks	MODEL	In recent years, the weight binarized neural network (BNN) technology has made great progress. However, neural networks with binarized inputs and binarized weights suffer from low accuracy in pattern recognition or inefficiency in hardware implementation. This work proposes a spatio-temporal binary neural network (STBNN) to solve this problem. STBNN has binary network input/output, binary neuron input/output, and binarized weights, and it integrates the computationally expensive batch normalization (BN) operation widely used in previous BNNs into the neuron threshold. STBNN can largely save computing resources and storage space while maintaining high accuracy (e.g., 98.0% on the MNIST test set). Using binary input (0 or 1) and binarized weight (+/- 1), the product of input and weight can be realized by a 1-bit Signed AND operation instead of multiplication operation in hardware implementation, thus significantly reducing computing resources, memory requirements, and power consumption. The results show that compared with a 32-bit multi-layer perceptron (MLP)-based hardware design, the STBNN-based hardware design typically reduces these three indicators by 84.2%, 96.4%, and 96.7%, respectively. This work provides an effective method to construct hardware-friendly neural network models and a guide for designing an extremely hardware-saving neural network processor. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				OCT 7	2020	409						351	360		10.1016/j.neucom.2020.06.084													
J								Lateral refinement network for contour detection	NEUROCOMPUTING										Contour detection; Lateral refinement network; Multi-scale integration		There has been significant progress in contour detection with the development of convolutional neural networks. To improve the detection performance of the object contour in the complex scenes, we proposed a novel lateral refinement network (LRNet) that extracts feature information from multiple refinement modules. In addition, we modified LRNet to train an effective contour detector, called lateral refinement contour (LRC). LRNet expands the generic refinement architecture and has different refinement levels that make the refinement network deeper to extract richer convolutional features. The low-level refinement modules explicitly exploit information available along the side output of the down-sampling process, and the high-level refinement modules attempt to fuse the low-level output features. The proposed method improves the performance of image-to-image predictions by deep-stacking all the meaningful refinement levels in a holistic manner. Using the pre-trained VGG16 network and Resnet network as backbones to train LRNet, we achieved state-of-the-art performance on several available datasets. The experimental results on the BSDS dataset are ODS = 0.816 and ODS = 0.820, and the experimental results on the NYUD-v2 dataset are ODS = 0.761 and ODS = 0.760. Especially, the results on BSDS500 surpassed the human-level performance under stricter criteria. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				OCT 7	2020	409						361	371		10.1016/j.neucom.2020.06.069													
J								Cooperative attack tolerant tracking control for multi-agent system with a resilient switching scheme	NEUROCOMPUTING										Multi-agent system; Sensor attacks; Cooperative attack tolerant tracking control; Resilient switching scheme; Networked multi-axis motion control system	LEADER-FOLLOWING CONSENSUS; EVENT-TRIGGERED CONSENSUS; SUBJECT	This paper studies the cooperative attack tolerant tracking control problem for multi-agent systems under sensor attacks. A group of distributed adaptive state observers with a resilient switching scheme is proposed to reconstruct the states, then the tracking control strategy is designed based on the estimates. It is shown that both of the estimation and the attack tolerance control performances are ensured despite of multiple sensor attacks and the disturbances. Finally, the experimental results of networked multi-axis motion control show the effectiveness of the proposed method. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				OCT 7	2020	409						372	380		10.1016/j.neucom.2020.06.087													
J								Efficient hyperparameter optimization through model-based reinforcement learning	NEUROCOMPUTING										Hyperparameter optimization; Machine learning; Reinforcement learning		Hyperparameter tuning is critical for the performance of machine learning algorithms. However, a noticeable limitation is the high computational cost of algorithm evaluation for complex models or for large datasets, which makes the tuning process highly inefficient. In this paper, we propose a novel model-based method for efficient hyperparameter optimization. Firstly, we frame this optimization process as a reinforcement learning problem and then employ an agent to tune hyperparameters sequentially. In addition, a model that learns how to evaluate an algorithm is used to speed up the training. However, model inaccuracy is further exacerbated by long-term use, resulting in collapse performance. We propose a novel method for controlling the model use by measuring the impact of the model on the policy and limiting it to a proper range. Thus, the horizon of the model use can be dynamically adjusted. We apply the proposed method to tune the hyperparameters of the extreme gradient boosting and convolutional neural networks on 101 tasks. The experimental results verify that the proposed method achieves the highest accuracy on 86.1% of the tasks, compared with other state-of-the-art methods and the average ranking of runtime is significant lower than all methods by using the predictive model. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				OCT 7	2020	409						381	393		10.1016/j.neucom.2020.06.064													
J								Building and optimization of 3D semantic map based on Lidar and camera fusion	NEUROCOMPUTING										Lidar SLAM; Semantic segmentation; Semantic map; Higher-order CRFs		When considering the robot application of the complex scenarios, the traditional geometric maps are insufficient because of the lack of interactions with the environment. In this paper, a three-dimensional (3D) semantic map with large-scale and accurate integrating Lidar and camera information is presented to achieve real-time road scenes. Firstly, simultaneous localization and mapping (SLAM) is performed to locate the robot position with the multi-sensor fusion of the Lidar and inertial measurement unit (IMU), and the map of the surrounding scenes is constructed while the robot is moving. Moreover, a convolutional neural networks (CNNs)-based semantic segmentation of images is employed to develop the semantic map of the environment. Following the synchronization of the time and space, the sensor fusion of Lidar and camera are used to generate the semantic labeled frame of point clouds and then create a semantic map in term of the posture. Besides, improving the capacity of classification, a higher-order 3D full connection conditional random fields (CRFs) method is utilized to optimize the semantic map. Finally, extensive experiment results evaluated on the KITTI dataset have illustrated the effectiveness of the proposed method. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				OCT 7	2020	409						394	407		10.1016/j.neucom.2020.06.004													
J								Hierarchical convolutional neural network via hierarchical cluster validity based visual tree learning	NEUROCOMPUTING										Hierarchical cluster validity index; Hierarchical convolutional neural network; Visual tree; Hierarchical classification	CLASSIFICATION	In multi-category classification task, some categories have strong inter-categories similarity, while others do not. Therefore, it is unreasonable to treat all these categories equally. One possible way is to organize all categories into a hierarchical structure and train a hierarchical classifier based on it. The general convolutional neural networks (CNN) can be seen as a flat classifier on hierarchical feature representations. Therefore, it is natural to combine the hierarchical structure and deep neural networks. However, for hierarchical classification, one open issue is how to build a reasonable hierarchical structure which characterizes the inter-relations between categories. An effective approach is to utilize hierarchical clustering to build a visual tree structure, but the critical issue is how to determine the number of clusters in hierarchical clustering. In this paper, a hierarchical cluster validity index (HCVI) is developed for supporting visual tree learning. Before clustering of each level begins, we will measure the impact of different numbers of clusters on visual tree building and select the most suitable number of clusters. Based on this visual tree, a hierarchical convolutional neural network (HCNN) can be trained for achieving more discriminative capability. Our experimental results have demonstrated that the proposed hierarchical cluster validity index (HCVI) can guide the building of a more reasonable visual tree structure, so that the hierarchical convolutional neural network can achieve better results on classification accuracy. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				OCT 7	2020	409						408	419		10.1016/j.neucom.2020.05.095													
J								Crowd counting via scale-communicative aggregation networks	NEUROCOMPUTING										Crowd counting; Scale-communicative; Aggregation networks		Crowd counting is a fundamental computer vision task that draws increasing attention in recent years, due to its wide applications in commercial activities and public securities. Despite much process has been achieved by applying the resurgent neural networks in this task, critical challenges still lie in tremendous variation of crowd scales, together with other issues like background clutters and occlusions, making the crowd appearances hard to model. To address these challenges, we propose a scale-communicative aggregation network (SCANet) for crowd counting. Our model is characterized by three aspects: (i) It contains different streams of convolutional neural networks (CNNs), where each stream consumes an individual scaled version of the input image and communicates complementarily to produce a high-resolution density map. (ii) Each CNN stream obtains robust feature presentation via our proposed multi-scale feature encoders (MSFEs) with dilated convolutional layers, and skip connections are adopted to exploit multi-stage feature aggregation. (iii) A multi-scale structural similarity metric along with Euclidean distance is introduced for optimizing the quality of generated density maps. Extensive experiments and comparisons on several crowd counting benchmarks demonstrate the effectiveness of our proposed method. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				OCT 7	2020	409						420	430		10.1016/j.neucom.2020.05.042													
J								Robust low rank representation via feature and sample scaling	NEUROCOMPUTING										Low rank representation; Feature and sample scaling; Cosine similarity metric; Bilinear factorization	MATRIX FACTORIZATION; FACE RECOGNITION; APPROXIMATION; ALGORITHMS	Low-rank representation (LRR) is a very competitive technique in various real-world applications for its powerful capability in discovering latent structure of noisy or corrupted data set. However, traditional low-rank models treat each data point and feature equally so that noisy data cannot be detected and suppressed effectively and have obvious deterioration in performance, especially in heavy noisy scenario. In this paper, to address this problem, we develop a method of feature and sample scaling for low rank representation. The importance of data points and their features in both feature and sample spaces are considered, as such, clean data points and noisy data points and their features can be distinguished. In addition, based on the observation that noisy data points are usually deviated far away from the principal projection of the data set, a cosine similarity metric between data vector and the principal projection vector is developed to measure the importance of each sample. Applying our method into two classical low rank models such as Low Rank Representation (LRR) and Bilinear Factorization (BF), we can learn better low-rank structure of clean data, while the outliers or missing data being suppressed. Extensive experimental results on ORL, COIL20 and video surveillance, demonstrate that our proposed method can outperform state-of-the-art low rank methods in image clustering tasks with various levels of corruptions, especially in a heavy noisy scenario. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				OCT 7	2020	409						431	442		10.1016/j.neucom.2020.06.065													
J								A survey on parallel clustering algorithms for Big Data	ARTIFICIAL INTELLIGENCE REVIEW										Algorithms; Big Data; Clustering; Data mining; DBSCAN; FPGA; GPU; k-means; MapReduce; MPI; Multi-cores CPU; Spark	GPU; DBSCAN	Data clustering is one of the most studied data mining tasks. It aims, through various methods, to discover previously unknown groups within the data sets. In the past years, considerable progress has been made in this field leading to the development of innovative and promising clustering algorithms. These traditional clustering algorithms present some serious issues in connection with the speed-up, the throughput, and the scalability. Thus, they can no longer be directly used in the context of Big Data, where data are mainly characterized by their volume, velocity, and variety. In order to overcome their limitations, the research today is heading to the parallel computing concept by giving rise to the so-called parallel clustering algorithms. This paper presents an overview of the latest parallel clustering algorithms categorized according to the computing platforms used to handle the Big Data, namely, the horizontal and vertical scaling platforms. The former category includes peer-to-peer networks, MapReduce, and Spark platforms, while the latter category includes Multi-core processors, Graphics Processing Unit, and Field Programmable Gate Arrays platforms. In addition, it includes a comparison of the performance of the reviewed algorithms based on some common criteria of clustering validation in the Big Data context. Therefore, it provides the reader with an overall vision of the current parallel clustering techniques.																	0269-2821	1573-7462															10.1007/s10462-020-09918-2		OCT 2020											
J								Evidential evolving C-means clustering method based on artificial bee colony algorithm with variable strings and interactive evaluation mode	FUZZY OPTIMIZATION AND DECISION MAKING										Belief functions; Evidential clustering; Soft clustering; Evolutionary computation; Artificial bee colony		The Evidential C-Means algorithm provides a global treatment of ambiguity and uncertainty in memberships when partitioning attribute data, but still requires the number of clusters to be fixed as a priori, like most existing clustering methods do. However, the users usually do not know the exact number of clusters in advance, particularly in practical engineering. To relax this requirement, this paper proposes an Evidential Evolving C-Means (E2CM) clustering method in the framework of evolutionary computation: cluster centers are encoded in a population of variable strings (or particles) to search the optimal number and locations of clusters simultaneously. To perform such joint optimization problem well, an artificial bee colony algorithm with variable strings and interactive evaluation mode is proposed. It will be shown that the E2CM can automatically create appropriate credal partitions by just requiring an upper bound of the cluster number rather than the exact one. More interestingly, there are no restrictions on this upper bound from the theoretic point of view. Some numerical experiments and a practical application in thermal power engineering validate our conclusions.																	1568-4539	1573-2908															10.1007/s10700-020-09344-7		OCT 2020											
J								A New Model to Distinguish Railhead Defects Based on Set-Membership Type-2 Fuzzy Logic System	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Type-2 fuzzy logic systems; Set-membership; Computational complexity reduction; Adaptive algorithms	FAULT CLASSIFICATION; COMPONENTS	This paper focuses on the new model for the classification of railhead defects, through images acquired by a rail inspection vehicle. In this regard, we discuss the use of set-membership concept, derived from the adaptive filter theory, into the training procedure of an upper and lower singleton type-2 fuzzy logic system, aiming to reduce computational complexity and to increase the convergence speed. The performance is based on the data set composed of images provided by a Brazilian railway company, which covers the four possible railhead defects (cracking, flaking, head-check and spalling) and the normal condition of the railhead. Additionally, we apply different levels of additive white Gaussian noise in the images in order to challenge the proposed model. Finally, we discuss performance analysis in terms of convergence speed, computational complexity reduction, and classification ratio. The reported results show that the proposal achieved improved convergence speed, slightly higher classification ratio and remarkable computation complexity reduction when we limit the number of epochs for training, which may be required under real-time constraint or low computational resource availability.																	1562-2479	2199-3211															10.1007/s40815-020-00945-3		OCT 2020											
J								A hybrid many-objective competitive swarm optimization algorithm for large-scale multirobot task allocation problem	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Multi-robot task allocation (MRTA) problem; Many-objective optimization; Competitive swarm optimization (CSO)	MULTIOBJECTIVE CUCKOO SEARCH; BAT ALGORITHM; SYSTEMS; DECOMPOSITION; ASSIGNMENT; DOMINANCE; SELECTION; DISTANCE; TREE	Large-scale multi-robot task allocation (MRTA) problem is an important part of intelligent logistics scheduling. And the load capacity of robot and picking station are important factors affecting the MRTA problem. In this paper, the MRTA problem is built as a many-objective optimization model with four objectives, which takes the load capacity of single robot, single picking station, all robots and all picking stations into account. To solve the model, a hybrid many-objective competitive swarm optimization (HMaCSO) algorithm is designed. The novel selection method employing two different measurement mechanisms will form the mating selection operation. Then the population will be updated by employing the competitive swarm optimization strategy. Meanwhile, the environment selection will play a role in choosing the excellent solution. To prove the superiority of our approach, there are two series of experiments are carried out. On the one hand, our approach is compared with other five famous many-objective algorithms on benchmark problem. On the other hand, the involved algorithms are applied in solving large-scale MRTA problem. Simulation results prove that the performance of our approach is superior than other algorithms.																	1868-8071	1868-808X															10.1007/s13042-020-01213-4		OCT 2020											
J								A novel BNMF-DNN based speech reconstruction method for speech quality evaluation under complex environments	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Speech quality evaluation; Non-intrusive; Bayesian NMF; Deep neural network	NMF; FEATURES	Speech quality evaluation (SQE) under complex noisy environment is important for audio processing systems and quality of service. Recently, the non-intrusive SQE is getting more and more attentive due to its efficient and ease of use. However, non-intrusive SQEs are expected to be underperformed the intrusive ones since it has no prior knowledge of the clean speech. In this paper, a novel quasi-clean speech reconstruction method for non-intrusive SQE is proposed. The method incorporates Bayesian NMF (BNMF) with deep neural network (DNN), which takes the advantages of both NMF and DNN. BNMF is utilized to calculate the basic spectro-temporal matrixes of target speech, and the obtained matrices are integrated into the DNN model as an individual layer. Then DNN is trained to learn the complex mapping between the target source and the mixture signal, and reconstruct the magnitude spectrograms of the quasi-clean speech. Finally, the reconstructed speech is regarded as the reference of the perceptual model to estimate the Mean opinion score of the tested noisy sample. The experiment results show that the proposed method outperforms the comparative non-intrusive SQE algorithms under challenging conditions in terms of objective measurement.																	1868-8071	1868-808X															10.1007/s13042-020-01214-3		OCT 2020											
J								A novel quantum-inspired solution for high-performance energy-efficient data acquisition from IoT networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Quantum computing; Internet of things (IoT); Network optimization; Quantum computing inspired optimization (QCIO)	EVOLUTIONARY ALGORITHM; THINGS IOT; INTERNET; INDUSTRY	Accuracy in IoT data acquisition has been an indispensable need to meet the increasing demand for time-sensitive data analysis, and real-time decision-making. Conspicuously, this study proposes a Quantum Computing inspired technique of temporal space optimization for real-time big IoT applications. For this purpose, quantification of IoT sensors is performed in terms of Sensors of Interest (SoI) and Degree of Aptness (DoA) measure to minimize IoT sensor-space in real-time. The proposed methodology incorporates quantum computing-based formalization of IoT sensor parameters to present a Quantum-Temporal Minimization Algorithm. Moreover, 2 key performance indicators in terms of Data Similarity Analysis and Energy Efficiency are estimated for optimized efficacy. To evaluate the presented technique, numerous simulations are performed in real-time scenario of vehicular traffic determination over 1km of Regional National Highway using 70 WiSense nodes comprising of noise sensors, vibration sensors, and Raspberry Pi device. Acquired data comprising of 28,586 segments are stored in the Amazon EC2 cloud database for evaluation. The performance enhancement is estimated based on comparative analysis with several state-of-the-art optimization techniques. Results registered depict that significant improvements are registered for the presented technique in terms of temporal effectiveness, and performance parameters like Accuracy, Correlation Analysis, and Reliability.																	1868-5137	1868-5145															10.1007/s12652-020-02494-x		OCT 2020											
J								A novel algorithm for global optimization: Rat Swarm Optimizer	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Optimization; Metaheuristics; Swarm-intelligence; Benchmark test functions; Engineering design problems	HARMONY SEARCH ALGORITHM; SPOTTED HYENA OPTIMIZER; DESIGN; EXPLORATION/EXPLOITATION; MODEL	This paper presents a novel bio-inspired optimization algorithm called Rat Swarm Optimizer (RSO) for solving the challenging optimization problems. The main inspiration of this optimizer is the chasing and attacking behaviors of rats in nature. This paper mathematically models these behaviors and benchmarks on a set of 38 test problems to ensure its applicability on different regions of search space. The RSO algorithm is compared with eight well-known optimization algorithms to validate its performance. It is then employed on six real-life constrained engineering design problems. The convergence and computational analysis are also investigated to test exploration, exploitation, and local optima avoidance of proposed algorithm. The experimental results reveal that the proposed RSO algorithm is highly effective in solving real world optimization problems as compared to other well-known optimization algorithms. Note that the source codes of the proposed technique are available at: http://www.dhimangaurav.com.																	1868-5137	1868-5145															10.1007/s12652-020-02580-0		OCT 2020											
J								An elderly health monitoring system based on biological and behavioral indicators in internet of things	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Internet of things; Health monitoring system; Smart elderly care; Data mining	FRAILTY	Advancement of sensor technologies has conducted to the rapid evolution of platforms, tools and approaches such as Internet of Things (IoT) for developing behavioral and physiological monitoring systems. Nowadays, According to growing number of elderlies living alone without their relatives scattered over the wide geographical areas, it is significantly essential to track their health function status continuously. In this paper, an IoT-based health monitoring system is proposed to check vital signs and detect biological and behavioral changes via smart elderly care technologies. It provides a health monitoring system for the involved medical teams to continuously monitor and assess a disabled or elderly's behavioral activity as well as the biological parameters, applying sensor technology through the IoT devices. In this approach, vital data is collected via IoT monitoring objects and then, data analysis is carried out through different machine learning methods such as Decision Tree (J48), Sequential Minimal Optimization (SMO), Multi-Layer Perceptron (MLP) and Naive Bayes (NB) classifiers for detecting the level of probable risks of elderly's physiological and behavioral changes. The experimental results confirm that the SMO, MLP and NB classifiers meet approximately close performance considering the accuracy, precision, recall, and f-score factors. However, the J48 method shows the highest performance for health function status predicting in our scenario with 99%, of accuracy and precision, 100% of recall and 97% of f-score. Moreover, the J48 performs with the lowest execution time in comparison to the other applied classifiers.																	1868-5137	1868-5145															10.1007/s12652-020-02579-7		OCT 2020											
J								Parsing argued opinion structure in Twitter content	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Argued opinion; Opinion mining; Argumentation; Twitter messages	ARGUMENTATION; FRAMEWORK	In this paper, we address the opinion argumentation mining issue from Twitter data with the objective of further analyzing Twitter users' preferences and motivations. After introducing the argued opinion definition and its different elements, we propose an argued opinion mining system calledTOMASwhere we present an end-to-end approach to parse the structure of the argued opinion in order to identify its elements. Our suggested system consists of four consecutive sub-tasks, namely: (1) opinion-topic detection, (2) argumentative opinions identification, (3) argument components detection, and (4) argumentative relation recognition. The proposed system optimizes the argued opinion structure using different classification models. The experimental study is conducted on the MC2 Lab CLEF2017 tweets corpus while considering various comparative baselines. We highlight that our system significantly outperforms the majority baselines and significantly outperforms challenging existing approaches.																	0925-9902	1573-7675															10.1007/s10844-020-00620-x		OCT 2020											
J								An evaluation of k-means as a local search operator in hybrid memetic group search optimization for data clustering	NATURAL COMPUTING										Data clustering; Evolutionary algorithms; Group search optimization; K-means	PARTICLE SWARM OPTIMIZATION; DIFFERENTIAL EVOLUTION; ALGORITHM; COLONY	Cluster analysis is one important field in pattern recognition and machine learning, consisting in an attempt to distribute a set of data patterns into groups, considering only the inner properties of those data. One of the most popular techniques for data clustering is the K-Means algorithm, due to its simplicity and easy implementation. But K-Means is strongly dependent on the initial point of the search, what may lead to suboptima (local optima) solutions. In the past few decades, Evolutionary Algorithms (EAs), like Group Search Optimization (GSO), have been adapted to the context of cluster analysis, given their global search capabilities and flexibility to deal with hard optimization problems. However, given their stochastic nature, EAs may be slower to converge in comparison to traditional clustering models (like K-Means). In this work, three hybrid memetic approaches between K-Means and GSO are presented, named FMKGSO, MKGSO and TMKGSO, in such a way that the global search capabilities of GSO are combined with the fast local search performances of K-Means. The degree of influence of K-Means on the behavior of GSO method is evaluated by a set of experiments considering both real-world problems and synthetic data sets, using five clustering metrics to access how good and robust the proposed hybrid memetic models are.																	1567-7818	1572-9796															10.1007/s11047-020-09809-z		OCT 2020											
J								Medical image segmentation using customized U-Net with adaptive activation functions	NEURAL COMPUTING & APPLICATIONS										Adaptive activation function; U-Net; Customized network; Medical segmentation; Retinal images	RETINAL VESSEL SEGMENTATION; BLOOD-VESSELS; REMOVAL; NETWORK	Since medical imaging is a fundamental step in clinical diagnosis and treatment, medical image processing is an attractive field for researchers. Among the different applications of medical image processing, this paper focuses on the segmentation task using a customized deep convolutional neural network (CNN). The proposed network is developed based on the idea of improving a deep network performance and speeding up its learning process while using less parameters. Using the famous U-Net architecture which has proven its effectiveness in the segmentation field, the customization is done here by applying adaptive activation functions. In the proposed network, the U-Net complexity is reduced about 200 times by alleviating some parameters to accelerate the learning process. However, the important modification is the use of adaptive activation functions where each convolution layer learns its own data-adaptive activation function as a linear combination of 16 well-known basic functions. This modification successfully compensates the accuracy drop caused by parameter alleviation and also makes the model capable to be tuned with small amount of training data. Conducting several experiments on five famous retinal image datasets, namely DRIVE, STARE, CHASE, HRF, and ARIA, the proposed customized U-Net achieved 96%, 97%, 96%, 97%, and 95% accuracy in segmenting blood vessels, respectively. The proposed network also showed 98% accuracy on ISIC skin lesion dataset for segmenting the lesion area. The obtained results obviously confirm the high performance of the proposed customized network compared to the previous successful researches in handling the medical segmentation task. They also light the hope that many famous deep networks can benefit from these types of customization to become efficient compact models with the ability to handle lack of sufficient data.																	0941-0643	1433-3058															10.1007/s00521-020-05396-3		OCT 2020											
J								Adaptive composite neural network disturbance observer-based dynamic surface control for electrically driven robotic manipulators	NEURAL COMPUTING & APPLICATIONS										Neural network; Disturbance observer; Dynamic surface control; Electrically driven; Robotic manipulator; Backstepping control	SLIDING MODE CONTROL; ROBUST-CONTROL; TRACKING CONTROL; SYSTEMS	This paper presents an adaptive backstepping control scheme for electrically driven robotic manipulator (EDRM) system with uncertainties and external disturbances by using neural network disturbance observer (NNDO) and dynamic surface control (DSC) design technique. NNDO is employed to estimate the uncertainties and external disturbances such that the priori information of the unknown dynamics will not be needed. To overcome the problem of "explosion of complexity" inherent in the backstepping design method, the DSC technique is integrated into the adaptive backstepping control design framework, where the NNDOs with adaptive composite law are utilized to compensate the uncertainties and external disturbances of EDRM. Based on the Lyapunov stability theory, it can be proven that the closed-loop system is stable in the sense that all the variables are guaranteed to be uniformly ultimately bounded. The results of simulation and experimental tests demonstrate the approximation capability of NNDO and the effectiveness of the proposed adaptive DSC scheme.																	0941-0643	1433-3058															10.1007/s00521-020-05391-8		OCT 2020											
J								Optimized gene selection and classification of cancer from microarray gene expression data using deep learning	NEURAL COMPUTING & APPLICATIONS										Microarray data; Deep learning; Laplacian score (LS); Convolutional neural network (CNN)	MACHINE; HYBRID	Cancer is the major leading reason of death around the world. However, the early identification and prediction of a cancer type is very critical for patient's health. Recently, microarray gene expression data was utilized for efficient and early diagnosis of cancer. Previous work shows that microarray data has two major issues which are high dimensionality and small sample size. Several researchers have analyzed and evaluated the cancer classification problem using different statistical and machine learning-based approaches but there are still some issues with these approaches that make cancer classification a nontrivial task. Such as, the inability of certain machine learning algorithms to use unstructured data has limited their utility in the cancer classification process. Convolutional neural networks are proven to very suitable to analyze variety of unstructured data. This ability allowed the deep learning algorithms to play a vibrant part in early detection of cancer through data classification. In this research, a hybrid deep learning model based on Laplacian Score-Convolutional Neural Network (LS-CNN) is employed for the classification of given cancer's data. The performance of the proposed system was evaluated on 10 different benchmark datasets using various performance measurement metrics such as accuracy and confusion matrix. The experimental results conclude that proposed LS-CNN model outperformed compared to traditional machine learning and recently used deep learning approaches.																	0941-0643	1433-3058															10.1007/s00521-020-05367-8		OCT 2020											
J								Constructing accuracy and diversity ensemble using Pareto-based multi-objective learning for evolving data streams	NEURAL COMPUTING & APPLICATIONS										Data streams; Concept drift; Ensemble learning; Diversity; Classifier selection; Multi-objective optimization	STATISTICAL COMPARISONS; CONCEPT DRIFT; MAJORITY	Ensemble learning is one of the most frequently used techniques for handling concept drift, which is the greatest challenge for learning high-performance models from big evolving data streams. In this paper, a Pareto-based multi-objective optimization technique is introduced to learn high-performance base classifiers. Based on this technique, a multi-objective evolutionary ensemble learning scheme, named Pareto-optimal ensemble for a better accuracy and diversity (PAD), is proposed. The approach aims to enhance the generalization ability of ensemble in evolving data stream environment by balancing the accuracy and diversity of ensemble members. In addition, an adaptive window change detection mechanism is designed for tracking different kinds of drifts constantly. Extensive experiments show that PAD is capable of adapting to dynamic change environments effectively and efficiently in achieving better performance.																	0941-0643	1433-3058															10.1007/s00521-020-05386-5		OCT 2020											
J								Multiple ACO-based method for solving dynamic MSMD traffic routing problem in connected vehicles	NEURAL COMPUTING & APPLICATIONS										Ant colony optimization; Dynamic traffic routing; IoV; MSMD	ANT COLONY OPTIMIZATION; SWARM INTELLIGENCE; INTERNET; FRAMEWORK; CONSENSUS; SYSTEM; THINGS	In this study, we focus on dynamic traffic routing of connected vehicles with various origins and destinations; this is referred to as a multi-source multi-destination traffic routing problem. Ant colony optimization (ACO)-based routing method, together with the idea of coloring ants, is proposed to solve the defined problem in a distributed manner. Using the concept of coloring ants, traffic flows of connected vehicles to different destinations can be distinguished. To evaluate the performance of the proposed method, we perform simulations on the multi-agent NetLogo platform. The simulation results indicate that the ACO-based routing method outperforms the shortest path-based routing method (i.e., given the same simulation period, the average travel time decreases by 8% on average and by 11% in the best case, whereas the total number of arrived vehicles increases by 13% on average and by 23% in the best case).																	0941-0643	1433-3058															10.1007/s00521-020-05402-8		OCT 2020											
J								A three-stage shearlet-based algorithm for vessel segmentation in medical imaging	PATTERN ANALYSIS AND APPLICATIONS										Vessel segmentation; Medical images; Shearlets; Contrast stretching	MRA IMAGES; TEXTURE; EXTRACTION	Dictionaries are known tools used in different branches of image processing like edge detection, inpainting and, etc. Segmentation is the task of extracting an object as the part of a particular image. The common drawback of different segmentation methods is that they perform the extraction task incompletely. Tasks like edge detection, denoising and smoothing, as the parts of segmentation, can be done through applying the dictionaries. In this paper, we propose three new contrast stretching function. Based on one of the stretching functions and shearlets as a dictionary, we improved the previous version of a method that has been used in binary segmentation for magnetic resonance angiography images (MRI). We also introduce a three-stage binary image segmentation algorithm for vessel segmentation in MRI images. There are some disadvantages in recent proposed methods when dealing with extracting vessels of medical images. Our algorithm does the task with a more accurate extraction in detecting vessels having low intensity and weak edges in MRI.																	1433-7541	1433-755X															10.1007/s10044-020-00915-3		OCT 2020											
J								An ensemble of fingerprint matching algorithms based on cylinder codes and mtriplets for latent fingerprint identification	PATTERN ANALYSIS AND APPLICATIONS										Latent fingerprint identification; Match-score fusion; Fingerprint matching algorithm		Automatic latent fingerprint identification is beneficial during forensic investigations. Usually, latent fingerprint identification algorithms are used to find a subset of similar fingerprints from those previously captured on databases, which are finally examined by latent examiners. Yet, the identification rate achieved by latent fingerprint identification algorithms is far from those obtained by latent examiners. One approach for improving identification rates is the fusion of the match scores computed with fingerprint matching algorithms using a supervised classification algorithm. This approach fuses the results provided by different lower-level algorithms to improve them. Thus, we propose a fusion of fingerprint matching algorithms using a supervised classifier. Our proposal starts with two different local matching algorithms. We substitute their global matching algorithms with another independent of the local matching, creating two lower-level algorithms for fingerprint matching. Then, we combine the output of these lower-level algorithms using a supervised classifier. Our proposal achieves higher identification rates than each lower-level algorithm and their fusion using traditional approaches for most of the rank values and reference databases. Moreover, our fusion algorithm reaches a Rank-1 identification rate of 74.03% and 71.32% matching the 258 samples in the NIST SD27 database against 29,257 and 100,000 references, the two largest reference databases employed in our experiments.																	1433-7541	1433-755X															10.1007/s10044-020-00911-7		OCT 2020											
J								A multiple ant colony system with random variable neighborhood descent for the dynamic vehicle routing problem with time windows	SOFT COMPUTING										Dynamic vehicle routing; Time windows; Ant colony optimization; Random variable neighborhood descent	TABU SEARCH; ALGORITHMS; SINGLE	This paper proposes a framework to solve the dynamic vehicle routing problem with time windows. This problem involves determining the minimum cost routes of a homogeneous fleet for meeting the demand for a set of customers within time windows. In addition, new customers can be assigned to vehicles during the execution of the routes. A framework is based on two phases: a priori where the routes are obtained for the known customers using static routing, and a posteriori where routes are re-optimized repeatedly during the planning horizon either continuously or periodically. The framework was validated using seven algorithm variants based on insertion heuristic, ant colony optimization, variable neighborhood descent, and random variable neighborhood descent, which were adapted to solve a posteriori phase. The best algorithm is a hybrid version that combines an improved version of the multiple ant colony systems with a random variable neighborhood descent. Computational results show that most of the algorithms are competitive regarding the state of the art with the best results in the objective of minimizing the number of unserved customers.																	1432-7643	1433-7479															10.1007/s00500-020-05350-4		OCT 2020											
J								Automatic camera calibration by landmarks on rigid objects	MACHINE VISION AND APPLICATIONS										Camera calibration; Optimization; Surveillance	POSE	This article presents a new method for automatic calibration of surveillance cameras. We are dealing with traffic surveillance, and therefore, the camera is calibrated by observing vehicles; however, other rigid objects can be used instead. The proposed method is usingkeypointsorlandmarksautomatically detected on the observed objects by a convolutional neural network. By using fine-grained recognition of the vehicles (calibration objects), and by knowing the 3D positions of the landmarks for the (very limited) set of known objects, the extracted keypoints are used for calibration of the camera, resulting in internal (focal length) and external (rotation, translation) parameters and scene scale of the surveillance camera. We collected a dataset in two parking lots and equipped it with a calibration ground truth by measuring multiple distances in the ground plane. This dataset seems to be more accurate than the existing comparable data (GT calibration error reduced from 4.62 % to 0.99 %). Also, the experiments show that our method overcomes the best existing alternative in terms of accuracy (error reduced from 6.56 % to 4.03%) and our solution is also more flexible in terms of viewpoint change and other.																	0932-8092	1432-1769				OCT 6	2020	32	1							2	10.1007/s00138-020-01125-x													
J								Iterative transfer learning with neural network for clustering and cell type classification in single-cell RNA-seq analysis	NATURE MACHINE INTELLIGENCE											TRANSCRIPTOMES; IDENTITY; REVEAL	Clustering and cell type classification are important steps in single-cell RNA-seq (scRNA-seq) analysis. As more and more scRNA-seq data are becoming available, supervised cell type classification methods that utilize external well-annotated source data start to gain popularity over unsupervised clustering algorithms; however, the performance of existing supervised methods is highly dependent on source data quality and they often have limited accuracy to classify cell types that are missing in the source data. We developed ItClust to overcome these limitations, a transfer learning algorithm that borrows ideas from supervised cell type classification algorithms, but also leverages information in target data to ensure sensitivity in classifying cells that are only present in the target data. Through extensive evaluations using data from different species and tissues generated with diverse scRNA-seq protocols, we show that ItClust considerably improves clustering and cell type classification accuracy over popular unsupervised clustering and supervised cell type classification algorithms.																		2522-5839															10.1038/s42256-020-00233-7		OCT 2020											
J								Real-time implementation of fabric defect detection based on variational automatic encoder with structure similarity	JOURNAL OF REAL-TIME IMAGE PROCESSING										Fabric defects; Variational automatic encoder; Structural similarity; Real-time; Deep learning	CLASSIFICATION; INSPECTION	Automatic detection of fabric defects based on machine vision is an important topic in the quality control of cotton textile factories. There are many kinds of defects in fabric production, it is very difficult to classify the defects automatically. In recent years, deep learning image processing technology based on a convolutional neural network (CNN) can train and extract features of the target image automatically. Since a large number of defect samples cannot be collected completely, we compared unsupervised learning algorithms based on CNN, including auto encoder (AE), variational automatic encoder (VAE), and generative adversarial networks (GAN). Because of the large amount of calculation and the difficulty of training in GAN, we chose AE and VAE codec networks and then introduced mean structural similarity (MSSIM) as network training loss function to improve the performance that only used L-p-distance loss function for image brightness comparison. After training finished, the authors used the trained model to obtain target defects from SSIM residual maps between input and reconstruct images. According to the evaluation results, we finally implemented a fabric defect detection system based on VAE on Jetson TX2 from Nvidia Corporation, USA. The optimized algorithm can meet the real-time requirements of the project and realize its popularization and application.																	1861-8200	1861-8219															10.1007/s11554-020-01023-5		OCT 2020											
J								A deep reinforcement learning-based algorithm for reliability-aware multi-domain service deployment in smart ecosystems	NEURAL COMPUTING & APPLICATIONS										Multi-domain orchestration; Service function chaining; Service reliability; QoS embedding; Multi-attribute embedding	FRAMEWORK; EFFICIENT; NETWORKS; ENVIRONMENT	The transition towards full network virtualization will see services for smart ecosystems including smart metering, healthcare and transportation among others, being deployed as Service Function Chains (SFCs) comprised of an ordered set of virtual network functions. However, since such services are usually deployed in remote cloud networks, the SFCs may transcend multiple domains belonging to different Infrastructure Providers (InPs), possibly with differing policies regarding billing and Quality-of-service (QoS) guarantees. Therefore, efficiently allocating the exhaustible network resources to the different SFCs while meeting the stringent requirements of the services such as delay and QoS among others, remains a complex challenge, especially under limited information disclosure by the InPs. In this work, we formulate the SFC deployment problem across multiple domains focusing on delay constraints, and propose a framework for SFC orchestration which adheres to the privacy requirements of the InPs. Then, we propose a reinforcement learning (RL)-based algorithm for partitioning the SFC request across the different InPs while considering service reliability across the participating InPs. Such RL-based algorithms have the intelligence to infer undisclosed InP information from historical data obtained from past experiences. Simulation results, considering both online and offline scenarios, reveal that the proposed algorithm results in up to 10% improvement in terms of acceptance ratio and provisioning cost compared to the benchmark algorithms, with up to more than 90% saving in execution time for large networks. In addition, the paper proposes an enhancement to a state-of-the-art algorithm which results in up to 5% improvement in terms of provisioning cost.																	0941-0643	1433-3058															10.1007/s00521-020-05372-x		OCT 2020											
J								Machine learning for KPIs prediction: a case study of the overall equipment effectiveness within the automotive industry	SOFT COMPUTING										Machine learning; Key performance indicators; Overall equipment effectiveness; Prediction; Improvement	MARKOVIAN JUMP SYSTEMS; FAULT-DETECTION	Key performance indicators are tools for management, decision support and forecasting; they reflect the strategy and vision of the company in terms of objectives and allow to always staying in step with the expectations of the stakeholders. Accurate forecasting of the indicators allows decisions to be reoriented to ensure performance optimization while reducing both cost and effort. This paper aims to apply different machine learning methods, namely support vector regression, optimized support vector regression (using genetic algorithm), random forest, extreme gradient boosting and deep learning to predict the overall equipment effectiveness as a case study. We will make use of several configurations of the listed models in order to provide a wide field of comparison. The data used to train our models were provided by an automotive cable production industry. The result shows that the configuration in which we used cross-validation technique, and we performed a duly splitting of data, provides predictor models with the better performances.																	1432-7643	1433-7479															10.1007/s00500-020-05348-y		OCT 2020											
J								Application of computer information technology in college physical education using fuzzy evaluation theory	COMPUTATIONAL INTELLIGENCE										computer information technology; fuzzy evaluation theory; ICT; physical education	STUDENTS; ATTITUDES	The educational sector faces a new dimension that is dominated by lifelong learning and is affected by the technical, social, and cultural changes. This pattern represents the need to improve the teaching methods for physical education and sports science. The use of computers and other information technology to increase the effectiveness of the teaching process is a modern method. This paper aims to illustrate the use of information and communication technologies (ICT) in physical education and sports. In our field, the gradual computerization results can be summed up in the following aspects: education software, design, and planning activities, recording outcomes, motion monitoring, video analysis, comparison of performance and synchronizing, measurements at distance and time and the evaluation of the activity. Although physical education and sports are practical activities, specialists can make use of modern teaching technologies. In this paper, the system of curriculum assessment for physical education has been analyzed and researched in computer assessment. The first section introduced the method of assessment of the physical education program. The second phase of the paper represents a teaching model of the physical education mathematical model utilizing the Comprehensive Adaptive Fuzzy Evaluation Theory has been proposed. A new level is the modernization of physics education with the artificial intelligence computer education system built in this paper. The experimental results have high performance in detecting the physical activity of college students.																	0824-7935	1467-8640															10.1111/coin.12352		OCT 2020											
J								Biometrics and quality of life of lymphoma patients: A longitudinalmixed-modelapproach	EXPERT SYSTEMS										haemato-oncological diseases; health-related quality of life; heart rate variability; longitudinal analysis; mixed-effect regression models; physiological indicators; wearable smart sensors	CANCER-RELATED FATIGUE; HEART-RATE-VARIABILITY	Knowledge Engineering has become essential in the fields of Medical and Health Care with emphasis for helping citizens to improve their health and quality of life. This includes individual methods and techniques in health-related knowledge acquisition and representation and their application in the construction of intelligent systems capable of using the acquired information to improve the patients' health and/or quality of life. Haemato-oncological diseases can provide significant disability and suffering, with severe symptoms and psychological distress. They can create difficulties in fulfilling professional, family and social roles, affecting an individual's quality of life. Health related quality of life (HRQoL) is a subjective concept but there is also an objective component related to physiological indicators. Some of these physiological indicators can be easily assessed by wearable technology such heart rate variability (HRV). This paper introduces an intelligent system to assess, in real-time, potential HRV indices, that can predict HRQoL in lymphoma patients throughout chemotherapy treatment and to account the individuals' variability. The system is based on wearable technology and intelligent processing of the patients' biometric information to assess some quality of life related parameters. A longitudinal study was conducted among 16 lymphoma patients using this intelligent system. Mixed-effect regression models were performed to investigate predictors for and time effects on HRQoL. There were no significant changes in all HRQoL domains over time. Some quality of life domains revealed similar time trends as HRV indices. These HRV indices also have a significant effect on the domains of quality of life.																	0266-4720	1468-0394														e12640	10.1111/exsy.12640		OCT 2020											
J								Analyzing and forecastingCOVID-19 pandemic in the Kingdom of Saudi Arabia usingARIMAandSIRmodels	COMPUTATIONAL INTELLIGENCE										ARIMA model; COVID-19; forecasting; Saudi Arabia; SIR model	COVID-19; EPIDEMIC	The novel coronavirus COVID-19 is spreading all across the globe. By June 29, 2020, the World Health Organization announced that the number of cases worldwide had reached 9 994 206 and resulted in more than 499 024 deaths. The earliest case of COVID-19 in the Kingdom of Saudi Arabia (KSA) was registered on March 2 in 2020. Since then, the number of infections as per the outcome of the tests increased gradually on a daily basis. The KSA has 182 493 cases, with 124 755 recoveries and 1551 deaths on June 29, 2020. There have been significant efforts to develop models that forecast the risks, parameters, and impacts of this epidemic. These models can aid in controlling and preventing the outbreak of these infections. In this regard, this article details the extent to which the infection cases, prevalence, and recovery rate of this pandemic are in the country and the predictions that can be made using the past and current data. The well-known classical SIR model was applied to predict the highest number of cases that may be realized and the flattening of the curve afterward. On the other hand, the ARIMA model was used to predict the prevalence cases. Results of the SIR model indicate that the repatriation plan reduced the estimated reproduction number. The results further affirm that the containment technique used by Saudi Arabia to curb the spread of the disease was efficient. Moreover, using the results, close interaction between people, despite the current measures remains a great risk factor to the spread of the disease. This may force the government to take even more stringent measures. By validating the performance of the applied models, ARIMA proved to be a good forecasting method from current data. The past data and the forecasted data, as per the ARIMA model provided high correlation, showing that there were minimum errors.																	0824-7935	1467-8640															10.1111/coin.12407		OCT 2020											
J								A modified butterfly optimization algorithm: An adaptive algorithm for global optimization and the support vector machine	EXPERT SYSTEMS										adaptive; butterfly optimization algorithm; support vector machine; transition segment of the tubular belt conveyor	GENETIC ALGORITHM; METAHEURISTIC ALGORITHM; SEARCH ALGORITHM; BAT ALGORITHM; WATER CYCLE; GREY WOLF; ANT LION; DESIGN; MODEL	A modified adaptive butterfly optimization algorithm is established with the aim of addressing the "early search blindness" and the relatively poor adaptability of the sensory modality. A normal-distribution-based model and a Weibull-distribution-based adaptive model of sensory modalities are respectively proposed for the global search process and iteration process. Among them, the Weibull-distribution-based adaptive model of sensory modalities is mainly manifested as the c value, that is, the adaptive change trend based on the Weibull model. The performance of the modified butterfly optimization algorithm is validated using a 14-benchmark test function and compared with performances of some latest algorithms. The experimental results indicate that the modified algorithm performs competitively in terms of accuracy and stability. Following the experiment, the modified algorithm is further tested by running a support-vector-machine prediction model based on engineering data of a pipe belt conveyor's flat-pipe/pipe-flat transition segment. The results of the modified algorithm are then compared with test-run outcomes of the back-propagation prediction model and KCV-SVM model. The results show that the prediction error is well within 10%, demonstrating the method's competence as a reliable reference for future designs of pipe belt conveyors.																	0266-4720	1468-0394														e12642	10.1111/exsy.12642		OCT 2020											
J								CloTAS protocol: CloudIoT available services protocol through autonomic computing against distributed denial of services attacks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										CloudIoT paradigm; S(2)aaS model; Autonomic computing; Diagnosable; Self-healing; DDoS attacks	INTERNET; THINGS; SYSTEMS	In the reliability and security challenges for the Internet of Things (IoT) systems, managing unpredictable events and controlling the abnormal situation automatically are provisioned by the integration of self-healing properties. Despite this, in our knowledge, this integration is not used to repair from malicious behaviors and potential distributed denial of service (DDoS) device attacks, especially in a collaborative way to maximize the quality of service parameters (availability). For that, we propose a diagnosable distributed protocol, in a hybrid-IoT system designed based on Service-Oriented Architecture (SOA) for IoT middleware in the things-oriented vision. The proposed protocol identifies and re-locates the denied service using the contextual recovery or the Sensing as a Service model (S(2)aaS) in the Cloud-IoT platform, as a backup to ensure the service availability. On the basis of this idea, this paper defines the system states under self-healing (in collaboration state) and self-protection autonomic-control loops. Besides, the closure (safety) and the convergence (liveness) properties will be defined and proved satisfied by the proposed protocol. Furthermore, to highlight the interest of the protocol, we present an application in the context of the smart-building (Lab) as an IoT solution.																	1868-5137	1868-5145															10.1007/s12652-020-02556-0		OCT 2020											
J								Badminton match outcome prediction model using Naive Bayes and Feature Weighting technique	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Match outcome prediction; Machine learning; CHIRP classifier; NB-CBFW; Hyper Pipes	BANKRUPTCY PREDICTION; MACHINE; ALGORITHM	The recent growth in the field of data mining and machine learning has remitted into more recognition of outcome prediction and classification. However, the application of these techniques in the field of sports is still unexplored. This paper presents the implementation of data mining and machine learning in sports particularly. Here, machine learning based algorithm to predict the outcome of the badminton tournament has been proposed. We have employed three classifiers, Naive Bayes with Correlation Based Feature Weighting (NB-CBFW), Composite Hypercubes on Iterated Random Projections (CHIRP) and Hyper Pipes to predict the outcome of Australian Open 2019, Malaysian Open 2019, German Open 2019 and Singapore Open 2019 Badminton tournaments. The outcome prediction is measured in terms of Accuracy, Root Mean Square Error (RMSE), True Positive Rate (TPR), True Negative Rate (TNR), Positive Predicted Value (PPV), Negative Predicted Value (NPV) and Receiver Operating Characteristics (ROC). After implementing the classifiers, it has been observed that NB-CBFW shows excellent accuracy in match outcome prediction as compared to CHIRP and Hyper Pipes.																	1868-5137	1868-5145															10.1007/s12652-020-02578-8		OCT 2020											
J								Random offset minimization in low frequency front-end amplifiers using swarm intelligence based techniques	EVOLUTIONARY INTELLIGENCE										Random offset voltage; Mismatch; Pelgrom's model; Swarm based algorithms; Optimization; Monte Carlo analysis	PARTICLE SWARM; DIFFERENTIAL EVOLUTION; OPTIMIZATION ALGORITHM; ANALOG CIRCUIT; DESIGN; TUTORIAL; MISMATCH	Random offset in amplifiers arises mainly due to random variations i.e. inherent mismatch in transistor parameters and greatly impacts its overall design specifications, especially in case of low frequency application. It must be minimized for high precision in the amplifier's performance. A new approach for minimization of random offset voltage in amplifiers has been proposed through the use of swarm intelligence based optimization algorithms due to their derivative free nature and easy search mechanism. The approach involves firstly, modelling the random offset voltage due to mismatch between transistors parameters based on Pelgrom's model and then minimizing the formulated model subjected to design constraints using swarm intelligence based algorithms. Two case studies are considered, firstly, a high swing Folded Cascode Operational Transconductance Amplifier (OTA) and secondly, a Recycling Folded Cascode (RFC) OTA. Comparative analysis have been performed by recording best, worst and mean data for 2500 function evaluations and also using statistical analysis such as Friedmann's test and Mann-Whitney's U test. The results indicate that the Hybrid Whale Particle Swarm Optimization (HWPSO) algorithm outperforms the other state of the art algorithms by giving a minimum random offset voltage of 7.2 mV and 1.452 mV with a mean rank of 1.55 and 1.75 for the 1st and 2nd case studies respectively. Validation of HWPSO results have been done by performing simulations and Monte Carlo Analysis for the two amplifiers in Cadence Virtuoso, which are found to be in close agreement with the algorithmic results.																	1864-5909	1864-5917															10.1007/s12065-020-00495-5		OCT 2020											
J								Dual local learning regularized nonnegative matrix factorization and its semi-supervised extension for clustering	NEURAL COMPUTING & APPLICATIONS										Data representation; NMF; Geometric structure; Manifold; Dual local learning regularization; Hard constraint	DIMENSIONALITY REDUCTION	Nonnegative matrix factorization (NMF) has received considerable attention in data representation due to its strong interpretability. However, traditional NMF methods neglect the discriminative information and geometric structure of both the data space and the feature space, simultaneously. In this paper, we propose a dual local learning regularized nonnegative matrix factorization (DLLNMF) method, which not only considers the geometric structure of both the data manifold and the feature manifold, simultaneously, but also takes advantage of the discriminative information of both the data space and the feature space. To make full use of the partial label information among samples, we further propose its semi-supervised extension, called dual local learning regularized nonnegative matrix factorization with label constraint (DLLNMF-LC), which imposes the label information as a hard constraint without additional parameters. Experimental results on some benchmark datasets have demonstrated the effectiveness of our proposed methods.																	0941-0643	1433-3058															10.1007/s00521-020-05392-7		OCT 2020											
J								Multivariate classification of Southern Brazilian table wines	JOURNAL OF CHEMOMETRICS										exploratory analysis; multivariate analysis; physicochemical data; wine	ARTIFICIAL NEURAL-NETWORKS; MICROEXTRACTION-GAS CHROMATOGRAPHY; STRAWBERRY AROMA; RED WINES	In this work, we evaluated the performance of several classifiers (supervised Kohonen self-organizing maps (KSOMs), soft independent modelling of class analogy (SIMCA), k-nearest neighbors (kNN), and partial least squares with discriminant analysis (PLS-DA) in the multiclass classification of Southern Brazilian table wines based on their physicochemical data. We also employed an unsupervised KSOM for the exploratory analysis of our data and compared its performance to that of PCA in this same task. All methods tested here presented a non-error rate (NER) and accuracy equal to or higher than 67% in the classification of the samples, having PLS-DA achieved an NER of 86% in classifying the samples from the test set and accuracy of 83% in classifying the samples from the training set. However, the best overall classification performance (when classification performances in training, cross-validation, and test sets are taken into account) in terms of NER and accuracy was that of SIMCA. Regarding the unsupervised analysis of the data, principal component analysis (PCA) provided a better separation of the samples and more convenient visualization of relationships between variables, and between variables and samples, than unsupervised KSOMs.																	0886-9383	1099-128X														e3302	10.1002/cem.3302		OCT 2020											
J								Adaptive online sequential extreme learning machine for dynamic modeling	SOFT COMPUTING										Extreme learning machine; Online sequential extreme learning machine; Data saturation problem; Approximate linear dependence; Hybrid forgetting mechanism		Extreme learning machine (ELM) is an emerging machine learning algorithm for training single-hidden-layer feedforward networks (SLFNs). The salient features of ELM are that its hidden layer parameters can be generated randomly, and only the corresponding output weights are determined analytically through the least-square manner, so it is easier to be implemented with faster learning speed and better generalization performance. As the online version of ELM, online sequential ELM (OS-ELM) can deal with the sequentially coming data one by one or chunk by chunk with fixed or varying chunk size. However, OS-ELM cannot function well in dealing with dynamic modeling problems due to the data saturation problem. In order to tackle this issue, in this paper, we propose a novel OS-ELM, named adaptive OS-ELM (AOS-ELM), for enhancing the generalization performance and dynamic tracking capability of OS-ELM for modeling problems in nonstationary environments. The proposed AOS-ELM can efficiently reduce the negative effects of the data saturation problem, in which approximate linear dependence (ALD) and a modified hybrid forgetting mechanism (HFM) are adopted to filter the useless new data and alleviate the impacts of the outdated data, respectively. The performance of AOS-ELM is verified using selected benchmark datasets and a real-world application, i.e., device-free localization (DFL), by comparing it with classic ELM, OS-ELM, FOS-ELM, and DU-OS-ELM. Experimental results demonstrate that AOS-ELM can achieve better performance.																	1432-7643	1433-7479															10.1007/s00500-020-05289-6		OCT 2020											
J								Feature-transfer network and local background suppression for microaneurysm detection	MACHINE VISION AND APPLICATIONS										Feature-transfer network; Local background suppression; Feature distance; Microaneurysm detection	AUTOMATIC DETECTION; SYSTEM; CARE	Microaneurysm (MA) is the earliest lesion of diabetic retinopathy (DR). Accurate detection of MA is helpful for the early diagnosis of DR. In this paper, an efficient approach is proposed to detect MA, based on feature-transfer network and local background suppression. In order to reduce noise, a feature-distance-based algorithm is proposed to suppress local background. The similarity matrix of feature distances is calculated to measure the difference between background noise and retinal objects. Moreover, a feature-transfer network is proposed to detect MAs with imbalanced data. For each training process, the optimized weights and bias are transferred to the next training, until the optimal network is generated. Experimental results demonstrate that the proposed approach can accurately detect subtle MAs surrounded by complex background. Furthermore, the sensitivity values on the public datasets are up to 98.3%, 100%, 99.3%, 100%, 96.5%, respectively. The proposed approach outperforms the state-of-the-arts, in terms of the competition performance measure score.																	0932-8092	1432-1769				OCT 4	2020	32	1							1	10.1007/s00138-020-01119-9													
J								High utility itemset mining using dolphin echolocation optimization	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Data mining; Genetic algorithm (GA); High utility itemset mining (HUIM); Dolphin echolocation optimization (DEO) algorithm; High utility itemsets (HUIs)	ALGORITHM; SEARCH	The critical issue identified in recent times is the high utility itemset mining (HUIM). It may be used for showing products that are profitably utilizing considering the factors of profit and quality as opposed to the frequent itemset (FIM) or the association rule (ARM) mining. There are numerous high utility itemset mining (HUIs) algorithms, mostly designed for handling the exponential search space to discover the HUIs at the time the number of the distinct items and the database that was very large. For this purpose, a meta-heuristic algorithm is designed for HUIs mining, working based on the genetic algorithm (GA) and the Dolphin echolocation optimization (DEO). The intended purpose of this evolutionary computation (EC) techniques on the DEO, here only fewer parameters are required to be compared to the approaches that are based on the GA. As the traditional DEO mechanism had been found for handling this continuous problem, an efficient algorithm based on the DEO called the high-utility itemset mining-DEO (HUIM-DEO) was proposed. To prove that the algorithm proposed was able to outperform the other heuristic algorithms to mine the HUIs for the time taken for execution, the number of HUIs discovered, and their convergence.																	1868-5137	1868-5145															10.1007/s12652-020-02571-1		OCT 2020											
J								An efficient evolutionary algorithm with a nearest neighbor search technique for clustering analysis	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cluster analysis; Clustering; Nearest neighbor search; Evolutionary algorithms; Optimization algorithms; Nature-inspired algorithms	GENETIC ALGORITHM; OPTIMIZATION	Evolutionary algorithms have shown their powerful capabilities in different machine learning problems including clustering which is a growing area of research nowadays. In this paper, we propose an efficient clustering technique based on the evolution behavior of genetic algorithm and an advanced variant of nearest neighbor search technique based on assignment and election mechanisms. The goal of the proposed algorithm is to improve the quality of clustering results by finding a solution that maximizes the separation between different clusters and maximizes the cohesion between data points in the same cluster. Our proposed algorithm which we refer to as "EvoNP" is tested with 15 well-known data sets using 5 well-known external evaluation measures and is compared with 7 well-regarded clustering algorithms . The experiments are conducted in two phases: evaluation of the best fitness function for the algorithm and evaluation of the algorithm against other clustering algorithms. The results show that the proposed algorithm works well with silhouette coefficient fitness function and outperforms the other algorithms for the majority of the data sets. The source code of EvoNP is available at. http://evo-ml.com/evonp/.																	1868-5137	1868-5145															10.1007/s12652-020-02570-2		OCT 2020											
J								Deep anomaly detection in expressway based on edge computing and deep learning	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Edge computing; Deep learning; Intelligent monitoring; Anomaly detection; AlexNet network	VEHICLE DETECTION; MODEL; NETWORK; IMAGES	In order to improve the real-time efficiency of expressway operation monitoring and management, the anomaly detection in intelligent monitoring network of expressway based on edge computing and deep learning is studied. The video data collected by the camera equipment in the intelligent monitoring network structure of the expressway is transmitted to the edge processing server for screening and then sent to the convolutional neural network. The convolutional neural network uses the multi-scale optical flow histogram method to preprocess the video data after the edge calculation to generate the training sample set and send it to the AlexNet model for feature extraction. SVM classifier model is used to train the feature data set and input the features of the test samples into the trained SVM classifier model to realize the anomaly detection in the intelligent monitoring network of expressway. The research method is used to detect the anomaly in an intelligent monitoring network of an expressway. The experimental results show that the method has better detection effect. The miss rate has reduced by 20.34% and 40.76% on average compared with machine learning method and small block learning method, respectively. The false positive rate has reduced by 27.67% and 21.77%, and the detection time is greatly shortened.																	1868-5137	1868-5145															10.1007/s12652-020-02574-y		OCT 2020											
J								Image dehazing based on dark channel spatial stimuli gradient model and image morphology	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Image dehazing; Edge preservation; Dark channel prior; Spatial stimuli gradient model	CONTRAST ENHANCEMENT; VISION	Image dehazing has become a critical problem to cater to as it has several parameters that need to be addressed. Real color, contrast, and illumination are the major parameters that are to be restored in the dehazed image. Different scenarios distort these parameters in different ways so it is difficult to restore the original image. Many approaches are used in the literature to cater to these problems but suffer from low contrast, faded color, and weak edges. This article introduces an effective technique, which is named as dark channel spatial stimuli gradient model (DCSSGM) that performs well for the aforementioned problems. The DCSSGM technique applies the dark channel prior (DCP) and spatial stimuli gradient sketch model (SSGSM) on each color channel to eliminate the haze from the image and to restore true edges. SSGSM is responsible to restore robust edges in an image using the perceived brightness and calculations based on neighborhood similarity. Morphology is applied to the resultant image to receive sharp true edges. The final restored image is the output of dynamic histogram equalization (DHE) which restores the contrast of the image. The evaluation analysis qualitatively and quantitatively concludes that the DCSSGM technique outperforms other state-of-the-art image dehazing techniques.																	1868-5137	1868-5145															10.1007/s12652-020-02581-z		OCT 2020											
J								Automatic Tongue Delineation from MRI Images with a Convolutional Neural Network Approach	APPLIED ARTIFICIAL INTELLIGENCE											REAL-TIME MRI; TRACKING; SEGMENTATION; ARTICULATORS; RESOLUTION	Tongue contour extraction from real-time magnetic resonance images is a nontrivial task due to the presence of artifacts manifesting in form of blurring or ghostly contours. In this work, we present results of automatic tongue delineation achieved by means of U-Net auto-encoder convolutional neural network. We present both intra- and inter-subject validation. We used real-time magnetic resonance images and manually annotated 1-pixel wide contours as inputs. Predicted probability maps were post-processed in order to obtain 1-pixel wide tongue contours. The results are very good and slightly outperform published results on automatic tongue segmentation.																	0883-9514	1087-6545															10.1080/08839514.2020.1824090		OCT 2020											
J								Enterprise AI Canvas Integrating Artificial Intelligence into Business	APPLIED ARTIFICIAL INTELLIGENCE												Artificial Intelligence (AI) and Machine Learning have enormous potential to transform businesses and disrupt entire industry sectors. However, companies wishing to integrate algorithmic decisions into their organization face multiple challenges: They have to identify use-cases in which artificial intelligence can create value, as well as decisions that can be supported or executed automatically. Furthermore, the organization will need to be transformed to be able to integrate AI-based systems into their human work-force. In addition, the more technical aspects of the underlying machine learning model have to be discussed in terms of how they impact the various units of a business: Where do the relevant data come from, which constraints have to be considered, how is the quality of the data and the prediction evaluated? The Enterprise AI canvas is designed to bring data scientist and business expert together to discuss and define all relevant aspects which need to be clarified in order to integrate AI-based systems into a digital enterprise. It consists of two parts, where part one focuses on the business view and organizational aspects, whereas part two focuses on the underlying machine learning model and the data it uses.																	0883-9514	1087-6545															10.1080/08839514.2020.1826146		OCT 2020											
J								An Effective Way to Large-Scale Robot-Path-Planning Using a Hybrid Approach of Pre-Clustering and Greedy Heuristic	APPLIED ARTIFICIAL INTELLIGENCE											TRAVELING SALESMAN PROBLEM	Robot-path-planning seeks the shortest path to optimize the motion cost for robots. In robot-path-planning, the computational time will significantly increase if the moving targets rise largely, also known as the large-scale TSP. Hence, the current algorithms for the shortest path planning may be ineffective in the large-scale TSP. Aimed at the real-time applications that a robot must achieve as many goals as possible within limited time and the computational time of a robot has to be short enough to provide the next moving signal in time. Otherwise, the robot will be trapped into the idle status. This work proposes a hybrid approach, called the pre-clustering greedy heuristic, to tackle the reduction of computational time cost and achieve the near-optimal solutions. The proposed algorithm demonstrates how to lower the computational time cost drastically via smaller data of a sub-group, divided byk-means clustering, and the intra-cluster path planning. An algorithm is also developed to construct the nearest connections between any two unconnected clusters, ensuring the inter-cluster tour is the shortest. As a result, by utilizing the proposed heuristic, the computational time is significantly reduced and the path length is more efficient than the benchmark algorithms, while the input data grow up to a large scale. In applications, the proposed work can be applied practically to the path planning with large-scale moving targets, for example, the employment for the ball-collecting robot in a court.																	0883-9514	1087-6545															10.1080/08839514.2020.1824094		OCT 2020											
J								An enhanced particle swarm optimization algorithm to solve probabilistic load flow problem in a micro-grid	APPLIED INTELLIGENCE										Enhanced particle swarm optimization (EPSO); Micro-grid; Optimization; Probabilistic load flow (PLF); And renewable energy resources	OPTIMAL POWER-FLOW; GLOBAL OPTIMIZATION; GA ALGORITHM; SEARCH; WIND; COMPUTATION; PERFORMANCE; STRATEGIES; SYSTEMS	This paper presents an Enhanced PSO (EPSO) method to address the weaknesses associated with the traditional PSO such as high sensitivity to the initial conditions, fast convergence, decline of solutions' variety, and trapping in the local optimum. For this, several strategies are suggested including segmentation of search space, modification of solution's updating rule, accepting poor solutions using an intelligent probabilistic function, searching in the regions with poor solutions, gradually removing the regions with poor solutions, and focusing the solutions on the local search after removing all regions with poor solutions. The performance of the EPSO was investigated using the 30 CEC 2014 test functions, 30 CEC 2017 test functions, 10 standard optimization algorithms, and a challenging optimization problem called Probabilistic Load Flow (PLF) in a distribution network. The results of the Wilcoxon signed-rank test on the 2014 and 2017 test functions revealed the superiority of the EPSO over nearly 80% of cases compared to the other algorithms. The obtained results of solving 10 benchmark functions by the proposed EPSO and the other six improved PSO algorithms indicated advantages of the proposed EPSO compared to the other algorithms in finding the optimal value. Meanwhile, the proposed EPSO took the average time of 0.66 s to solve the 10 test functions; it was the shortest time compared to other improved PSO algorithms. According to the results of implementing the EPSO algorithm and other algorithms with random agents for 61 times, in more than 65% of the test functions, the proposed EPSO could find the global optimal solution in a shorter time than the other algorithms. The results of solving the probabilistic load flow problem indicated 89% similarity of the results of the proposed EPSO to those of the most accurate method i.e. Monte Carlo Simulation (MCS). Comparison of the obtained results with the other algorithms as well as outcomes of several improved versions of the PSO indicated the competitive proficiency of the proposed EPSO in various optimization circumstances.																	0924-669X	1573-7497															10.1007/s10489-020-01872-4		OCT 2020											
J								Cost-effective workflow scheduling approach on cloud under deadline constraint using firefly algorithm	APPLIED INTELLIGENCE										Deadline constraint; Workflow scheduling; Scientific workflows; Firefly	SCIENTIFIC WORKFLOWS	Cloud computing, a novel and promising methodology in the distributed computing domain, provides a pay-per-use framework to solve large-scale scientific and business workflow applications. These workflow applications have a constraint that each of them must completed within the limited time (deadline constraint). Therefore, scheduling a workflow with deadline constraints is increasingly becoming a crucial research issue. However, many analytical reviews on scheduling problems reveal that existing solutions fail to provide cost-effective solutions and they do not consider the parameters like CPU performance variation, delay in acquisition and termination of Virtual Machines (VMs). This paper presents a Cost-Effective Firefly based Algorithm (CEFA) to solve workflow scheduling problems that can occur in an Infrastructure as a Service (IaaS) platform. The proposed CEFA uses a novel method for problem encoding, population initialization and fitness evaluation with an objective to provide cost-effective and optimized workflow execution within the time limit. The performance of the proposed CEFA is compared with the state-of-the-art algorithms such as IaaS Cloud-Partial Critical Path (IC-PCP), Particle Swarm Optimization (PSO), Robustness-Cost-Time (RCT), Robustness-Time-Cost (RTC), and Regressive Whale Optimization (RWO). Our experimental results demonstrate that the proposed CEFA outperforms current state-of-the-art heuristics with the criteria of achieving the deadline constraint and minimizing the cost of execution.																	0924-669X	1573-7497															10.1007/s10489-020-01875-1		OCT 2020											
J								Improved TODIM method for intuitionistic fuzzy MAGDM based on cumulative prospect theory and its application on stock investment selection	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Multiple attribute group decision making (MAGDM); Intuitionistic fuzzy sets; TODIM; Cumulative prospect theory (CPT); Stock investment selection	ATTRIBUTE DECISION-MAKING; AGGREGATION OPERATORS; SIMILARITY MEASURES; RANKING PRODUCTS; RISK-ASSESSMENT; TOPSIS METHOD; DESTINATION; APPRAISAL; WEIGHTS; NUMBERS	The stock investment selection could be regarded as a classical multiple attribute group decision making (MAGDM) issue. The intuitionistic fuzzy sets (IFSs) can fully describes the uncertain information for stock investment selection. Furthermore, the classical TODIM method based on the cumulative prospect theory (CPT-TODIM) is built, which is a selectable method in reflecting the DMs' psychological behavior. Thus, in this paper, the intuitionistic fuzzy CPT-TODIM (IF-CPT-TODIM) method is proposed for MAGDM issue. At the same time, it is enhancing rationality to get the weight information of attributes by using the CRITIC method under IFSs. And focusing on hot issues in contemporary society, this article applies the discussed method for stock investment selection and demonstrates for stock investment selection based on the proposed method. Finally, through comparing the outcome of comparative analysis, we conclude that this improved approach is acceptable.																	1868-8071	1868-808X															10.1007/s13042-020-01208-1		OCT 2020											
J								Representing unstructured text semantics for reasoning purpose	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Text analysis; Open information extraction; Rule-based inference; Semantic role labeling; Semantic representation	FRAMEWORK; DESIGN	To interpret a natural language text using a machine, we need to convert its semantics into structured information. In the field of Natural Language Processing, multiple tasks have been designed and developed to interpret the semantics of an unstructured text, and change words into meanings. However, there are some challenges in directly using the output of these tasks in subsequent applications such as logical inference. There has been a growing interest in building and enhancing state-of-the-art semantic representation systems in recent years. However, most of these systems involve supervised models that benefit from manually annotated data, which is not accessible for a wide range of languages. This paper presents a new framework for modeling text in order to extract its information, and through an inference system, obtain new information that is not explicitly stated in the text, but could be logically inferred. This framework is based on Open Information Extraction and Semantic Web techniques for machine reading. We translate the text into a machine-readable representation by using Semantic Types Identification and Question-based Semantic Role Labeling, which could be used in low-resource languages. We integrate the extracted information into the background knowledge by using existing Semantic Web standards. The proposed framework could increase generalization of labelling and reduce ambiguities, therefore, it is an appropriate solution for preparing text for reasoning systems.																	0925-9902	1573-7675															10.1007/s10844-020-00621-w		OCT 2020											
J								Linguistic summarization to support supply network decisions	JOURNAL OF INTELLIGENT MANUFACTURING										Supply network design; Linguistic summarization; Fuzzy set theory; Decision support	BIG DATA ANALYTICS; CHAIN NETWORK; FUZZY; REPRESENTATION; PERSPECTIVE; MANAGEMENT; DISCOVERY; DESIGN; LEVEL	A supply chain network architecture is a key element of designing and modeling a supply chain to better understand the cost and time associated with the distribution of products with available resources and market locations. Due to the large size of combinations for product design and supplier choices; descriptive, predictive and prescriptive analytics are needed to design, control and then improve a supply chain network. Current study is the first instance in the supply network management field using linguistic summarization (LS), a descriptive analytics tool generating natural language-based summaries of raw data with the help of fuzzy sets. This study has developed a LS method for revealing information from a realistic complex network of a bike supply chain, and it produces network description phrases by using fuzzy set theory to model linguistic/textual terms. The truth degree of generated summaries is calculated by fuzzy cardinality-based methods instead of scalar cardinality-based methods to overcome inherent disadvantages. The results of the study are interpreted in two ways: word clouds are used for single objective cases, and list of sentences that exceed a threshold value are used for bi-objective cases. LS-based findings, explanations and strategic decisions are directed at decision support to increase supply network performance, efficiency and sustainability.																	0956-5515	1572-8145															10.1007/s10845-020-01677-9		OCT 2020											
J								Assembly quality evaluation for linear axis of machine tool using data-driven modeling approach	JOURNAL OF INTELLIGENT MANUFACTURING										Assembly quality evaluation; Linear axis of machine tool; Data-driven; Variable selection; SMOTE; GA-optimized multi-class SVM	SUPPORT VECTOR MACHINE; ROUGH SET-THEORY; FAULT-DIAGNOSIS; MANUFACTURING PROCESS; GEOMETRIC ERRORS; MOTION ERROR; SELECTION; PREDICTION; ALGORITHM; CLASSIFICATION	During the batch assembly analysis of linear axis of machine tool, assembly quality evaluation is crucial to reduce assembly quality fluctuations and improve efficiency. This study presented a data-driven modeling approach for evaluating assembly quality of linear axis based on normalized mutual information and random sampling with replacement (NMI-RSWR) variable selection method, synthetic minority over-sampling technique (SMOTE), and genetic algorithm (GA)-optimized multi-class support vector machine (SVM). First, a variable selection method named NMI-RSWR was proposed to select key assembly parameters which affected assembly quality of linear axis. Then, a hybrid method based on SMOTE and GA-optimized multi-class SVM was presented to construct assembly quality evaluation model. In this method, Class imbalance problem was solved by using SMOTE, and parameters optimization problem was solved by using GA. Finally, the assembly-related data from the batch assembly of x-axis of a three-axis vertical machining center were collected to validate the proposed method. The results indicate that the proposed NMI-RSWR approach has capacity for selecting the highly related assembly parameters with assembly quality of linear axis, and the proposed data-driven modeling approach is effective for assembly quality evaluation of linear axis.																	0956-5515	1572-8145															10.1007/s10845-020-01666-y		OCT 2020											
J								Control of chaotic two-predator one-prey model with single state control signals	JOURNAL OF INTELLIGENT MANUFACTURING										Two-predator one-prey model; Chaotic Samardzija-Greller system; Lotka-Volterra system; Lyapunov based nonlinear control; Sliding mode control; Chaos control	IMPULSIVE CONTROL; FEEDBACK-CONTROL; SYSTEM; SYNCHRONIZATION	In this paper, the complex control dynamics of a predator-prey Lotka-Volterra chaotic system are studied. The main purpose is to control the chaotic trajectories of two-predator one-prey system which was introduced by Samardzija and Greller (Bull Math Biol 50(5):465-491. 10.1007/BF02458847, 1988). Lyapunov based nonlinear control and sliding mode control methods are used. The other purpose of this paper is to present the sliding mode control performances under different sliding surface choices. Based on the sliding mode control and Lyapunov stability theory, four alternative sliding surfaces are constructed to stabilize the chaotic two-predator one-prey model to its zero equilibrium point. The focused control signals realize the control from only one state which provides simplicity in implementation. Numerical simulations are demonstrated to validate the theoretical analyses and compare the effectiveness of proposed controllers for the chaotic Samardzija-Greller system.																	0956-5515	1572-8145															10.1007/s10845-020-01676-w		OCT 2020											
J								Real-time underwater image resolution enhancement using super-resolution with deep convolutional neural networks	JOURNAL OF REAL-TIME IMAGE PROCESSING										Underwater images; Real-time; Quality enhancement; Color correction; Super-resolution; Convolutional neural network	COLOR CORRECTION; CHANNEL	In this paper, a two-step image enhancement is presented. In the first step, color correction and underwater image quality enhancement are conducted if there are artifacts such as darkening, hazing and fogging. In the second step, the image resolution optimized in the previous step is enhanced using the convolutional neural network (CNN) with deep learning capability. The main reason behind the adoption of this two-step technique, which includes image quality enhancement and super-resolution, is the need for a robust strategy to visually improve underwater images at different depths and under diverse artifact conditions. The effectiveness and robustness of the real-time algorithm are satisfactory for various underwater images under different conditions, and several experiments have been undertaken for the two datasets of images. In both stages and for each of image datasets, the mean square error (MSE), peak signal to noise ratio (PSNR), and structural similarity (SSIM) evaluation measures were fulfilled. In addition, the low computational complexity and suitable outputs were obtained for different artifacts that represented divergent depths of water to achieve a real-time system. The super-resolution in the proposed structure for medium layers can offer a proper response. For this reason, time is also one of the major factors reported in the research. Applying this model to underwater imagery systems will yield more accurate and detailed information.																	1861-8200	1861-8219															10.1007/s11554-020-01024-4		OCT 2020											
J								Cauchy with whale optimizer based eagle strategy for multi-level color hematology image segmentation	NEURAL COMPUTING & APPLICATIONS										Hematology; Image segmentation; Entropy; Swarm-intelligence; Eagle strategy; Optimization	CROSS-ENTROPY; ALGORITHM; SCHEME; CLASSIFICATION	Pathological color image segmentation is an exigent procedure due to the existence of imperceptibly correlated, and indistinct multiple regions of concern. Multi-level thresholding has been introduced as one of the most significant image segmentation procedures for pathological analysis. However, finding an optimal set of threshold values is an extremely time-consuming task, and crucially depends on the objective function criterion. In order to solve these problems, this paper presents a multi-level hematology color image thresholding approach with the assistance of a two-stage strategy called Eagle Strategy coupled with Whale Optimization Algorithm (ES-WOA), analyzing the performance over five well-known objective functions, namely; Kapur's entropy, Fuzzy entropy, Tsallis' entropy, Otsu's method, and Cross entropy. A rigorous comparative study is performed among classical WOA and existing eagle strategy based optimization algorithms, considering a set of hematology color images, and common performance indexes evaluated by each objective function tested. Experimental results indicate that proposed ES-WOA in combination with Tsallis' entropy outperforms the rest of tested algorithms in terms of computational effort, image segmentation quality, and robustness. For example, ES-WOA with Tsallis' produces segmented images with average Peak Signal-to-Noise Ratio (PSNR) values 16.0371, 17.9975, 21.1353, and 23.0759 for threshold values 2, 3, 4, and 5, respectively, which are superior to other tested methods. Additionally, the numerical results are statistically validated using a nonparametric approach to eliminate the random effect in the obtained results.																	0941-0643	1433-3058															10.1007/s00521-020-05368-7		OCT 2020											
J								Convert index trading to option strategies via LSTM architecture	NEURAL COMPUTING & APPLICATIONS										Long short-term memory; Big data; Options; Futures; Kelly criterion; Trading strategy	PREDICTING STOCK; NEURAL-NETWORKS	In the past, most strategies were mainly designed to focus on stocks or futures as the trading target. However, due to the enormous number of companies in the market, it is not easy to select a set of stocks or futures for investment. By investigating each company's financial situation and the trend of the overall financial market, people can invest precisely in the market and choose to go long or short. Moreover, how to determine the position size of the transaction is also a problematic issue. In the past, many money management theories were based on the Kelly criterion. And they put a certain percentage of their total funds into the market for trading. Nonetheless, three massive problems cannot be overcome. First, futures are leveraged transactions, and extra funds must be deposited as margin. It causes that the position size is hard to be estimated by the Kelly criterion. The second point is that the trading strategy is difficult to determine the winning rate in the financial market and cannot be brought into the Kelly criterion to calculate the optimal fraction. Last, the financial data are always massive. A big data technique should be applied to resolve this issue and enhance the performance of the framework to reveal knowledge in the financial data. Therefore, in this paper, a concept of converting the original futures trading strategy into options trading is proposed. An LSTM (long short-term memory)-based framework is proposed to predict the profit probability of the original futures strategy and convert the corresponding daily take-profit and stop-loss points according to the delta value of the options. Finally, the proposed framework brings the results into the Kelly criterion to get the optimal fraction of options trading. The final research results show that options trading is closer to the optimal fraction calculated by the Kelly criterion than futures trading. If the original futures trading strategy can profit, the benefits after converting to options trading can be further superior.																	0941-0643	1433-3058															10.1007/s00521-020-05377-6		OCT 2020											
J								Exploratory Data Analysis and Foreground Detection with the Growing Hierarchical Neural Forest	NEURAL PROCESSING LETTERS										Self-organization; Clustering; Text mining; Image segmentation	SELF-ORGANIZING MAP; BACKGROUND SUBTRACTION; FEATURES; TREE	In this paper, a new self-organizing artificial neural network called growing hierarchical neural forest (GHNF) is proposed. The GHNF is a hierarchical model based on the growing neural forest, which is a tree-based model that learns a set of trees (forest) instead of a general graph so that the forest can grow in size. This way, the GHNF faces three important limitations regarding the self-organizing map: fixed size, fixed topology, and lack of hierarchical representation for input data. Hence, the GHNF is especially amenable to datasets containing clusters where each cluster has a hierarchical structure since each tree of the GHNF forest can adapt to one of the clusters. Experimental results show the goodness of our proposal in terms of self-organization and clustering capabilities. In particular, it has been applied to text mining of tweets as a typical exploratory data analysis application, where a hierarchical representation of concepts present in tweets has been obtained. Moreover, it has been applied to foreground detection in video sequences, outperforming several methods specialized in foreground detection.																	1370-4621	1573-773X															10.1007/s11063-020-10360-2		OCT 2020											
J								Robust Two-Dimensional Linear Discriminant Analysis via Information Divergence	NEURAL PROCESSING LETTERS										Discriminant analysis; Information divergence; L21 norm; Adaptive weighted scheme	PRINCIPAL COMPONENT ANALYSIS; FACE RECOGNITION; LP-NORM; P-NORM; REPRESENTATION	Due to the complexities of collected data that may contain outliers and noises, many variants of LDA and 2DLDA have been proposed to address this problem and have shown that the improved methods produce much better performance than classical LDA and 2DLDA. In this paper we propose a novel two-dimensional linear discriminant analysis method via information divergence. The proposed method applies the weighted L21 norm to learn a robust projection matrix in the image space. In the proposed model, we introduce the weights into the within-class scatter and the total scatter simultaneously, and learn the weights by imposing information divergence on the objective functions. To handle the proposed model, we resort to Dinkelbach's extended algorithm to solve the proposed ratio minimization problem. Considering the characteristics of the subproblems, we exploit an equivalent representation of subproblems which can be solved by alternating optimization techniques where each block of variables has good optimization properties. The proposed model not only overcomes the small-sample-size problem, but also suppresses outliers by an adaptively weighted scheme with the guidance of information divergences. The experiments on several image data sets demonstrate that the classification performance of the proposed method is superior to that of some existing methods in the presence of outliers.																	1370-4621	1573-773X															10.1007/s11063-020-10359-9		OCT 2020											
J								The network-untangling problem: from interactions to activity timelines	DATA MINING AND KNOWLEDGE DISCOVERY										Temporal networks; Complex networks; Timeline reconstruction; Vertex cover; Linear programming; 2-SAT	APPROXIMATION; ALGORITHMS; EVOLUTION	In this paper we study a problem of determining when entities are active based on their interactions with each other. We consider a set of entities V and a sequence of time-stamped edges E among the entities. Each edge (u, v, t) is an element of E denotes an interaction between entities u and v at time t. We assume an activity model where each entity is active during at most k time intervals. An interaction (u, v, t) can be explained if at least one of u or v are active at time t. Our goal is to reconstruct the activity intervals for all entities in the network, so as to explain the observed interactions. This problem, the network-untangling problem, can be applied to discover event timelines from complex entity interactions. We provide two formulations of the network-untangling problem: (i) minimizing the total interval length over all entities (sum version), and (ii) minimizing the maximum interval length (max version). We study separately the two problems for k = 1 and k > 1 activity intervals per entity. For the case k = 1, we show that the sum problem is NP-hard, while the max problem can be solved optimally in linear time. For the sum problem we provide efficient algorithms motivated by realistic assumptions. For the case of k > 1, we show that both formulations are inapproximable. However, wepropose efficient algorithms based on alternative optimization. We complement our study with an evaluation on synthetic and real-world datasets, which demonstrates the validity of our concepts and the good performance of our algorithms.																	1384-5810	1573-756X															10.1007/s10618-020-00717-5		OCT 2020											
J								A new globally adaptive k-nearest neighbor classifier based on local mean optimization	SOFT COMPUTING										k-nearest neighbors; Pattern classification; Globally adaptive nearest neighbors; Local mean optimization	ALGORITHMS; RULE	The k-nearest neighbor (KNN) rule is a simple and effective nonparametric classification algorithm in pattern classification. However, it suffers from several problems such as sensitivity to outliers and inaccurate classification decision rule. Thus, a local mean-based k-nearest neighbor classifier (LMKNN) was proposed to address these problems, which assigns the query sample with a class label based on the closest local mean vector among all classes. It is proven that the LMKNN classifier achieves better classification performance and is more robust to outliers than the classical KNN classifier. Nonetheless, the unreliable nearest neighbor selection rule and single local mean vector strategy in LMKNN classifier severely have negative effect on its classification performance. Considering these problems in LMKNN, we propose a globally adaptive k-nearest neighbor classifier based on local mean optimization, which utilizes the globally adaptive nearest neighbor selection strategy and the implementation of local mean optimization to obtain more convincing and reliable local mean vectors. The corresponding experimental results conducted on twenty real-world datasets demonstrated that the proposed classifier achieves better classification performance and is less sensitive to the neighborhood size k compared with other improved KNN-based classification methods.																	1432-7643	1433-7479															10.1007/s00500-020-05311-x		OCT 2020											
J								Two-stage hybrid learning techniques for bankruptcy prediction	STATISTICAL ANALYSIS AND DATA MINING										bankruptcy prediction; data mining; hybrid learning; machine learning	FINANCIAL DISTRESS; GENETIC ALGORITHM; FEATURE-SELECTION; CLASSIFICATION; FAILURE; CLASSIFIERS; ENSEMBLES; ADABOOST; IMPROVE; MODELS	Many machine learning-based techniques have been used for the prediction of bankruptcy. They can be divided into single, ensemble, and hybrid learning techniques. This paper focuses on a two-stage hybrid learning approach for bankruptcy prediction where, in the first stage, a clustering algorithm is used to perform the instance selection task in order to filter out a certain number of unrepresentative training data. The clustering results output from the first stage are used with a classification algorithm to construct the prediction model. The results of experiments based on five different country datasets show that the best support vector machine (SVM) classifier performance is obtained using instance selection by affinity propagation (AP) and k-means individually. Moreover, we also find that although the best AP/k-means and SVM combination is dataset dependent, the criteria for selecting representative training data are specific. This should become a guideline for developing bankruptcy prediction systems based on the hybrid learning approach.																	1932-1864	1932-1872															10.1002/sam.11482		OCT 2020											
J								Rapid self-localization of robot based on omnidirectional vision technology	MACHINE VISION AND APPLICATIONS										Omnidirectional vision system; Soccer robot; Self-localization; Homography	SOCCER ROBOTS; CALIBRATION; CAMERAS; IMPLEMENTATION; NAVIGATION; OBJECTS; SYSTEM; ROBUST; FIELD	In this paper, we propose a self-localization method for a soccer robot using an omnidirectional camera. Based on the projective geometry of the omnidirectional visual system, the image distortion from the original omnidirectional image can be completely corrected, so the robot can quickly localize itself on the playing field. First, we transform the distorted omnidirectional image to a distortion-free unwrapped image of the soccer field by projective geometry. The obtained image makes the sequent field recognizable and the self-localization of the robot more convenient and accurate. Then, by geometric invariants, the correspondence between the unwrapped image and the model of the playing field is constructed. Next, the homography theory is applied to get the precise location and orientation of the robot. The simulation and experimental results show that the proposed method can quickly and accurately determine the position and azimuth of the soccer robot and the distance between two objects on the playing field.																	0932-8092	1432-1769				OCT 3	2020	31	7-8							74	10.1007/s00138-020-01129-7													
J								The Path Planning of Mobile Robot by Neural Networks and Hierarchical Reinforcement Learning	FRONTIERS IN NEUROROBOTICS										neural network; hierarchical reinforcement learning; mobile robot; path planning; fusion algorithm	VIRTUAL-REALITY	Existing mobile robots cannot complete some functions. To solve these problems, which include autonomous learning in path planning, the slow convergence of path planning, and planned paths that are not smooth, it is possible to utilize neural networks to enable to the robot to perceive the environment and perform feature extraction, which enables them to have a fitness of environment to state action function. By mapping the current state of these actions through Hierarchical Reinforcement Learning (HRL), the needs of mobile robots are met. It is possible to construct a path planning model for mobile robots based on neural networks and HRL. In this article, the proposed algorithm is compared with different algorithms in path planning. It underwent a performance evaluation to obtain an optimal learning algorithm system. The optimal algorithm system was tested in different environments and scenarios to obtain optimal learning conditions, thereby verifying the effectiveness of the proposed algorithm. Deep Deterministic Policy Gradient (DDPG), a path planning algorithm for mobile robots based on neural networks and hierarchical reinforcement learning, performed better in all aspects than other algorithms. Specifically, when compared with Double Deep Q-Learning (DDQN), DDPG has a shorter path planning time and a reduced number of path steps. When introducing an influence value, this algorithm shortens the convergence time by 91% compared with the Q-learning algorithm and improves the smoothness of the planned path by 79%. The algorithm has a good generalization effect in different scenarios. These results have significance for research on guiding, the precise positioning, and path planning of mobile robots.																	1662-5218					OCT 2	2020	14								63	10.3389/fnbot.2020.00063													
J								Synthetic CT images for semi-sequential detection and segmentation of lung nodules	APPLIED INTELLIGENCE										Nodule segmentation; Deep learning; Synthetic CT images; Semi-sequential segmentation	PULMONARY NODULES; CLASSIFICATION	Accurately detecting and segmenting lung nodules from CT images play a critical role in the earlier diagnosis of lung cancer and thus have attracted much interest from the research community. However, due to the irregular shapes of nodules, and the low-intensity contrast between the nodules and other lung areas, precisely segmenting nodules from lung CT images is a very challenging task. In this paper, we propose a highly effective and robust solution to this problem by innovatively utilizing the changes of nodule shapes over continuous slices (inter-slice changes) and develop a deep learning based end-to-end system. Different from the existing 2.5D or 3D methods that attempt to explore the inter-slice features, we propose to create a novel synthetic image to depict the unique changing pattern of nodules between slices in distinctive colour patterns. Based on the new synthetic images, we then adopt the deep learning based image segmentation techniques and develop a modified U-Net architecture to learn the unique color patterns formed by nodules. With our proposed approach, the detection and segmentation of nodules can be achieved simultaneously with an accuracy significantly higher than the state of the arts by 10% without introducing high computation cost. By taking advantage of inter-slice information and form the proposed synthetic image, the task of lung nodule segmentation is done more accurately and effectively.																	0924-669X	1573-7497															10.1007/s10489-020-01914-x		OCT 2020											
J								Deep learning of individual aesthetics	NEURAL COMPUTING & APPLICATIONS										Evolutionary art; Aesthetics; Aesthetic measure; Convolutional neural networks; Dimension reduction; Morphogenesis	APPRECIATION; MODEL	Accurate evaluation of human aesthetic preferences represents a major challenge for creative evolutionary and generative systems research. Prior work has tended to focus on feature measures of the artefact, such as symmetry, complexity and coherence. However, research models from psychology suggest that human aesthetic experiences encapsulate factors beyond the artefact, making accurate computational models very difficult to design. The interactive genetic algorithm circumvents the problem through human-in-the-loop, subjective evaluation of aesthetics, but is limited due to user fatigue and small population sizes. In this paper, we look at how recent advances in deep learning can assist in automating personal aesthetic judgement. Using a leading artist's computer art dataset, we investigate the relationship between image measures, such as complexity, and human aesthetic evaluation. We use dimension reduction methods to visualise both genotype and phenotype space in order to support the exploration of new territory in a generative system. Convolutional neural networks trained on the artist's prior aesthetic evaluations are used to suggest new possibilities similar or between known high-quality genotype-phenotype mappings. We integrate this classification and discovery system into a software tool for evolving complex generative art and design.																	0941-0643	1433-3058															10.1007/s00521-020-05376-7		OCT 2020											
J								Convolutional neural network-bagged decision tree: a hybrid approach to reduce electric vehicle's driver's range anxiety by estimating energy consumption in real-time	SOFT COMPUTING										Driving range anxiety; Energy consumption estimation; Electric vehicle; Convolutional neural network; Bagged decision tree; Microscopic driving parameters	MODEL; SIMULATION; BEHAVIOR; SPEED; STATE; ROAD	To overcome range anxiety problem of electric vehicles (EVs), an accurate real-time energy consumption estimation is necessary, which can be used to provide the EV's driver with information about the remaining range in real time. A hybrid CNN-BDT approach has been developed, in which convolutional neural network (CNN) is used to provide an energy consumption estimate considering the effect of temperature, wind speed, battery's SOC, auxiliary loads, road elevation, vehicle speed and acceleration. Further, bagged decision tree (BDT) is used to fine-tune the estimate. Unlike existing techniques, the proposed approach does not require internal vehicle parameters from manufacturer and can easily learn complex patterns even from noisy data. The comparison results with existing techniques show that the developed approach provides better estimates with least mean absolute energy deviation of 0.14.																	1432-7643	1433-7479															10.1007/s00500-020-05310-y		OCT 2020											
J								Minimax models for capacitatedp-center problem in uncertain environment	FUZZY OPTIMIZATION AND DECISION MAKING										Uncertain programming; Minimax model; P-center problem; Discrete location; Uncertainty theory	P-CENTER PROBLEM	The capacitatedp-center problem is concerned with how to selectplocations for facility centers and assign demand points to them such that the maximum distance between a demand point and its nearest center is minimized. This paper focuses on the capacitatedp-center problem in an uncertain environment, in which demands and distances are regarded as uncertain variables. Consequently, two minimax models with uncertain parameters are formulated, and their crisp equivalences are investigated. Additionally, a hybrid algorithm based on the 99-method, a genetic algorithm and a tabu search algorithm is designed to solve the models. Finally, some numerical examples are presented to unveil the applications of the models and algorithm.																	1568-4539	1573-2908															10.1007/s10700-020-09343-8		OCT 2020											
J								A fast and fully distributed method for region-based image segmentation Fast distributed region-based image segmentation	JOURNAL OF REAL-TIME IMAGE PROCESSING										Image segmentation; Region growing; Pixel labeling; Distributed computing; Real-time computer vision systems	EDGE-DETECTION; MULTIAGENT SYSTEM; RANGE	Distributed and parallel computing techniques allow fast image processing, namely when these techniques are applied at the low and the medium level of a vision system. In this paper, a collective and distributed method for image segmentation is introduced and evaluated. The method is modeled as a multi-agent system, where the agents aim to collectively produce a region-based segmentation. Each agent starts searching for an acceptable region seed by randomly jumping within the image. Next, it performs a region growing around its position. Thus, several agents find themselves within the same homogeneous region and are organized in a graph where two agents are connected if they are within the same region. So, a unifying of the labels in a same region is collaboratively performed by the agents themselves. The proposed method was experimented on real range images from the ABW dataset and the Object Segmentation Database (OSD) one, and the obtained results were compared to those of some well-referenced methods from the literature. The evaluation results show that the proposed method provides fast and accurate image segmentation, allowing it to be deployed for real-time vision systems.																	1861-8200	1861-8219															10.1007/s11554-020-01021-7		OCT 2020											
J								Limitations of the Recall Capabilities in Delay-Based Reservoir Computing Systems	COGNITIVE COMPUTATION										Lasers; Reservoir computing; Nonlinear dynamics	PREDICTION PERFORMANCE; LASER SUBJECT; FEEDBACK	We analyse the memory capacity of a delay-based reservoir computer with a Hopf normal form as nonlinearity and numerically compute the linear as well as the higher order recall capabilities. A possible physical realization could be a laser with external cavity, for which the information is fed via electrical injection. A task-independent quantification of the computational capability of the reservoir system is done via a complete orthonormal set of basis functions. Our results suggest that even for constant readout dimension the total memory capacity is dependent on the ratio between the information input period, also called the clock cycle, and the time delay in the system. Optimal performance is found for a time delay about 1.6 times the clock cycle.																	1866-9956	1866-9964															10.1007/s12559-020-09733-5		OCT 2020											
J								Optimal control for uncertain discrete-time singular systems under expected value criterion	FUZZY OPTIMIZATION AND DECISION MAKING										Optimal control; Uncertain singular systems; Expected value; Recurrence equation	NONLINEAR-SYSTEMS	Optimal control problems governed by two different types of uncertain discrete-time singular systems are investigated under expected value criterion. The objective function including uncertain variables is optimized with the help of expected value method provided that the singular systems are both regular and impulse-free. At first, based on the principle of dynamic programming, a recurrence equation is derived to simplify an optimal control model for a class of uncertain discrete-time singular systems. After that, according to uncertainty theory and the recurrence equation, two kinds of optimal control problems subject to an uncertain linear singular system and an uncertain singular system with quadratic input variables are considered in order, and the optimal solutions are both presented by accurate expressions. A numerical example and a dynamic input-output model are settled to illustrate the effectiveness of the results obtained.																	1568-4539	1573-2908															10.1007/s10700-020-09346-5		OCT 2020											
J								An Intelligent Web Caching System for Improving the Performance of a Web-Based Information Retrieval System	INTERNATIONAL JOURNAL ON SEMANTIC WEB AND INFORMATION SYSTEMS										Access Log; Bit Hit Rate; Byte Hit Rate; Caching; Internet; Preloading; Replacement Policy; Response Time; Web	NETWORK; SCHEME	With an increasing number of web users, the data traffic generated by these users generates tremendous network traffic which takes a long time to connect with the web server. The main reason is, the distance between the client making requests and the servers responding to those requests. The use of the CDN (content delivery network) is one of the strategies for minimizing latency. But, it incurs additional cost. Alternatively, web caching and preloading are the most viable approaches to this issue. It is therefore decided to introduce a novel web caching strategy called optimized popularity-aware modified least frequently used (PMLFU) policy for information retrieval based on users' past access history and their trends analysis. It helps to enhance the proxy-driven web caching system by analyzing user access requests and caching the most popular web pages driven on their preferences. Experimental results show that the proposed systems can significantly reduce the user delay in accessing the web page. The performance of the proposed system is measured using IRCACHE data sets in real time.																	1552-6283	1552-6291				OCT-DEC	2020	16	4					26	44		10.4018/IJSWIS.2020100102													
J								Concept Map Information Content Enhancement Using Joint Word Embedding and Latent Document Structure	INTERNATIONAL JOURNAL ON SEMANTIC WEB AND INFORMATION SYSTEMS										Concept Graph; Concept Map Mining; Concept Maps Summarization; Enhanced Concept Map Generation; Joint Word Embedding; Knowledge Extraction and Representation		The concept map (CM) can be enhanced by extracting precise propositions, representing compactly, adding useful features that increase the information content (IC). To enhance the IC with domain knowledge of the document, an automatic enhanced CM generation using word embedding based concept and relation representation along with organization using latent semantic structure is proposed. To improve the concept significance, precise identification of similar items, clustering topically associated concepts, and hierarchical clustering of semantically related concepts are carried out. This augments the IC of the CM with additional information and generates CM with concise and informative content. The joint word embedding based on various contexts is utilized to determine distributional features critical for these enhancements. Summarization of the ECM to visualize the document summary is used to illustrate its resourcefulness. The work is evaluated using ACL anthology, Genia, and CRAFT dataset, and the information gain is approximately three times more in comparison with general CM.																	1552-6283	1552-6291				OCT-DEC	2020	16	4					45	60		10.4018/IJSWIS.2020100103													
J								Intelligent Extraction of a Knowledge Ontology From Global Patents: The Case of Smart Retailing Technology Mining	INTERNATIONAL JOURNAL ON SEMANTIC WEB AND INFORMATION SYSTEMS										Clustering; Intellectual Property (IP); Latent Dirichlet Allocation (LDA); Ontology Schema; Patent Mining	CLUSTER-ANALYSIS	The growth of global patents increased over the last decade as enterprises and inventors sought greater protection of their intellectual property (IP) rights. Global patents represent state-of-the-art knowledge for given domains. This research develops a hierarchical Latent Dirichlet Allocation (LDA)-based approach as a computational intelligent method to discover topics and form a top-down ontology, a semantic schema, representing the collective patent knowledge. To validate the knowledge extraction, 1,546 smart retailing patents collected from the Derwent Innovation platform from 2011 and 2016 are used to build the domain ontology schema. The patent set focuses on in-use, globally established, and non-disputed IP covering payment, user experience, and information integration for smart retailing. The clustering and LDA-based ontology system automatically build the knowledge map, which identifies the technology trends and the technology gaps enabling the development of competitive R&D and management strategies.																	1552-6283	1552-6291				OCT-DEC	2020	16	4					61	80		10.4018/IJSWIS.2020100104													
J								Tag's Depth-Based Expert Profiling Using a Topic Modeling Technique	INTERNATIONAL JOURNAL ON SEMANTIC WEB AND INFORMATION SYSTEMS										Expert Profiling; Expertise; Finding; Folksonomy; Information Retrieval; LDA; Personomy; Social Tagging; Tags; Topic Modeling	LANGUAGE MODELS; RANKING	Expert finding and expert profiling are two important tasks for organizations, researchers, and work seekers. This importance can also be seen in online communities especially with the explosion of social networks. Expert finding on one hand addresses the task of finding the right person with the appropriate knowledge or skills. Expert profiling on the other hand gives a concise and meaningful description of a candidate expert. This paper focuses on what social tagging can bring to improve expert finding and profiling. A novel expertise indicator that models and assesses an expert based on the expert's tagging activities is proposed. First, tags are used as interest indicator to build candidate's profiles; then, Latent Dirichlet Allocation algorithm (LDA) is used to construct the tags distribution over topics by exploiting the tag's semantic characteristics. Topics of interest are then filtered using tag's depth. The latter is finally used as the expertise indicator. Experiments performed on the stack overflow dataset show the accuracy of the proposed approach.																	1552-6283	1552-6291				OCT-DEC	2020	16	4					81	99		10.4018/IJSWIS.2020100105													
J								Weak supervision for generating pixel-level annotations in scene text segmentation	PATTERN RECOGNITION LETTERS										Image analysis; Image processing; Neural nets; Artificial intelligence	COMPETITION	Providing pixel-level supervisions for scene text segmentation is inherently difficult and costly, so that only few small datasets are available for this task. To face the scarcity of training data, previous approaches based on Convolutional Neural Networks (CNNs) rely on the use of a synthetic dataset for pretraining. However, synthetic data cannot reproduce the complexity and variability of natural images. In this work, we propose to use a weakly supervised learning approach to reduce the domain-shift between synthetic and real data. Leveraging the bounding-box supervision of the COCO-Text and the MLT datasets, we generate weak pixel-level supervisions of real images. In particular, the COCO-Text-Segmentation (COCO_TS) and the MLT-Segmentation (MLT_S) datasets are created and released. These two datasets are used to train a CNN, the Segmentation Multiscale Attention Network (SMANet), which is specifically designed to face some peculiarities of the scene text segmentation task. The SMANet is trained end-to-end on the proposed datasets, and the experiments show that COCO_TS and MLT_S are a valid alternative to synthetic images, allowing to use only a fraction of the training samples, with a significant improvement in performance. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						1	7		10.1016/j.patrec.2020.06.023													
J								A PSO-based algorithm for mining association rules using a guided exploration strategy	PATTERN RECOGNITION LETTERS										PSO Algorithm; Association Rules; Metaheuristic algorithm		Association rule mining is one of the most important and active research areas in data mining. In the literature, several association rule miners have been proposed; among them, those based on particle swarm optimization (PSO) have reported the best results. However, these algorithms tend to prematurely fall into local solutions, avoiding a wide exploration that could produce even better results. In this paper, an algorithm based on PSO, called PSO-GES, for mining association rules using a Guided Exploration Strategy is introduced. Our experiments, over real-world transactional databases, show that our proposed algorithm mines better quality association rules than the most recent PSO-based algorithms for mining association rules of the state of the art. (C) 2020 Published by Elsevier B.V.																	0167-8655	1872-7344				OCT	2020	138						8	15		10.1016/j.patrec.2020.05.006													
J								Multi-lingual scene text detection and language identification	PATTERN RECOGNITION LETTERS										Multi-lingual; Scene Text; MSER; SWT; Deep learning; GAN; CNN	COMPETITION	Scene text analysis is a field of research that poses challenges to researchers owing to the background complexities, image quality, text orientation, text size, etc. The problem gets more complex when the image contains multi-lingual texts. Most scene text detection techniques approach the problem as either a feature-based or deep learning-based problem. In this work, an end-to-end system is proposed for scene text detection, localization and language identification to combine feature-based and deep learning-based approaches. The model uses Maximally Stable Extremal Regions and Stroke Width Transform for generating text proposals, followed by proposal refinement using Generative Adversarial Network. Finally, a Convolution Neural Network based model is used for language identification of the detected scene texts. Experiments have been conducted on standard datasets like KAIST, COCO, CTW1500, CVSI and ICDAR along with an in-house multi-lingual Indic scene text dataset for which the proposed model achieves satisfactory results. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						16	22		10.1016/j.patrec.2020.06.024													
J								Pattern recognition and artificial intelligence techniques for cultural heritage	PATTERN RECOGNITION LETTERS										Pattern recognition; Cultural heritage; Artificial Intelligence	HISTORICAL DOCUMENTS; CLASSIFICATION; TRANSCRIPTION; RESTORATION; NETWORK; SYSTEM; ART	This paper is the editorial of the virtual special issue (VSI) "Pattern recognition and artificial intelligence techniques for cultural heritage", of which the authors of this paper have been the guest editors. It aims to bring together the work of experts from the fields of pattern recognition and artificial intelligence and that of cultural heritage. This multidisciplinary subject covers a wide spectrum spanning from the study of the cultural heritage to the development of tools based on PR/AI techniques for cultural heritage analysis, reconstruction and understanding. The papers included in this special issue allowed us to highlight the advances on this subject from a wide-angle perspective, as well as to stimulate new theoretical and applied researches for better characterizing the state of the art in this domain. (C) 2020 Published by Elsevier B.V.																	0167-8655	1872-7344				OCT	2020	138						23	29		10.1016/j.patrec.2020.06.018													
J								On the use of a full stack hardware/software infrastructure for sensor data fusion and fault prediction in industry 4.0	PATTERN RECOGNITION LETTERS										Industry4.0; Deep learning; Data fusion; IIoT industrial testbed	IOT	Aspects related to prognostics are becoming a crucial part in the industrial sector. In this sense, Industry 4.0 is considered as a new paradigm that leverages on the IoT to propose increasingly more solutions to provide an estimate on the working conditions of an industrial plant. However, in context like the industrial sector where the number and heterogeneity of sensors can be very large, and the time requirements are very stringent, emerges the challenge to design effective infrastructures to interact with these complex systems. In this paper, we propose a full stack hardware/software infrastructure to collect, manage, and analyze the data gathered from a set of heterogeneous sensors attached to a real scale replica industrial plant available in our laboratory. On top of the proposed infrastructure we designed and implemented a fault prediction algorithm which exploits sensors data fusion with the aim to assess the working conditions of the industrial plant. The result section shows the obtained results in terms of accuracy from testing our proposed model and provides a comparison with a traditional Deep Neural Network (DNN) topology. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						30	37		10.1016/j.patrec.2020.06.028													
J								Discriminative sampling via deep reinforcement learning for kinship verification	PATTERN RECOGNITION LETTERS										Kinship verification; Deep learning; Reinforcement learning; Sampling; Biometrics	FACE; RECOGNITION	In this paper, we propose a discriminative sampling method to select most effective negative samples via deep reinforcement learning for kinship verification. Unlike most existing facial kinship verification methods which focus on extracting effective features with the random sampling strategy, we develop a deep reinforcement learning method to select samples which are more suitable for learning discriminative features, so that the overall performance can be improved. Specifically, our method uses two subnetworks to achieve the kinship verification task: one DQN-based sampling network to filter the negative samples, and one multi-layer convolutional network to verify the kin relationship. Experimental results on the KinFaceW-I and KinFaceW-II datasets show the superiority of our proposed approach over the state-of-the-arts. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						38	43		10.1016/j.patrec.2020.06.019													
J								Prudence when assuming normality: An advice for machine learning practitioners	PATTERN RECOGNITION LETTERS										Bayes classifier; Multinormal distribution; Central limit theorem; Classification; Binormal model	PARTIAL AREA; ASSESSING CLASSIFIERS; SAMPLE-SIZE	In a binary classification problem the feature vector (predictor) is the input to a scoring function that produces a decision value (score), which is compared to a particular chosen threshold to provide a final class prediction (output). Although the normal assumption of the scoring function is important in many applications, sometimes it is severely violated even under the simple multinormal assumption of the feature vector. This article proves this result mathematically with a counterexample to provide an advice for practitioners to avoid blind assumptions of normality. On the other hand, the article provides a set of experiments that illustrate some of the expected and well-behaved results of the Area Under the ROC curve (AUC) under the multinormal assumption of the feature vector. Therefore, the message of the article is not to avoid the normal assumption of either the input feature vector or the output scoring function; however, a prudence is needed when adopting either of both. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						44	50		10.1016/j.patrec.2020.06.026													
J								mDixon-based synthetic CT generation via transfer and patch learning	PATTERN RECOGNITION LETTERS										Synthetic CT; mDixon MR; Abdomen; Transfer learning; Patch learning	ZERO-ECHO-TIME; MRI-ONLY RADIOTHERAPY; ATTENUATION CORRECTION; PSEUDO-CT; CLINICAL-EVALUATION; PET/MRI; PET/CT; CLASSIFICATION; SEGMENTATION; SYSTEMS	We propose a practicable method for generating synthetic CT images from modified Dixon (mDixon) MR data for the challenging body section of the abdomen and extending into the pelvis. Attenuation correction is necessary to make quantitatively accurate PET but is problematic withPET/MR scanning as MR data lack the information of photon attenuation. Multiple methods were proposed to generate synthetic CT from MR images. However, due to the challenge to distinguish bone and air in MR signals, most existing methods require advanced MR sequences that entail long acquisition time and have limited availablity. To address this problem, we propose a voxel-oriented method for synthetic CT generation using both the transfer and patch learning (SCG-TPL). The overall framework of SCG-TPL includes three stages. Stage I extracts seven-dimensional texture features from mDixon MR images using the weighted convolutional sum; Stage II enlists the knowledge-leveraged transfer fuzzy c-means (KL-TFCM) clustering as well as the patch learning-oriented semi-supervised LapSVM classification to train multiple candidate fourtissue-type-identifiers (FM); Stage III synthesizes CT for new patients' mDixon images using the candidate FITIs and voting principle. The significance of our method is threefold: (1) As the global model for patch learning, guiding by the referenced knowledge, KL-TFCM can credibly initialize MR data with overcoming the individual diversity. As the local complement, LapSVM can adaptively model each patch with low time and labor costs. (2) Jointly using the transfer KL-TFCM clustering and patch learning-oriented LapSVM classification, SCG-TPL is able to output accurate synthetic CT in the abdomen. (3) SCG-TPL synthesizes CT only using easily-obtainable mDixon MR images, which greatly facilitates its clinical practicability. Experimental studies on ten subjects' mDixon MR data verified the superiority of our proposed method. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						51	59		10.1016/j.patrec.2020.06.017													
J								Memetic algorithm for multivariate time-series segmentation	PATTERN RECOGNITION LETTERS										Time series segmentation; Multivariate data; Memetic algorithm	CLASSIFICATION; OPTIMIZATION	In recent years, analyzing time-series data has become an ever important research topic due to an increased number of temporal datasets in science and engineering. Segmentation is widely used in time-series data analysis because it provides a more compact representation by dividing the series into segments. Segmentation approaches based on genetic algorithms have been proposed to extract segments and patterns with a given objective, such as a low rate of change or periodicity from time-series data. However, they may not be effective in obtaining the precise solution because they perform global search. In this study, we propose a memetic algorithm for multivariate time-series segmentation. For efficient local refinement, we calculate a likelihood-based score for all time points and use it in the evolutionary process. Experiments demonstrate that the proposed method is superior to conventional segmentation methods. The source code of the proposed method can be downloaded from https://github.com/hlim-kist/ma_mts (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						60	67		10.1016/j.patrec.2020.06.022													
J								Dynamics of soil surface temperature with unmanned aerial systems	PATTERN RECOGNITION LETTERS										Thermographic imaging; Unmanned aerial systems; Soil surface temperature; Remote sensing	EMISSIVITY	Thermographies are a source of rich information, valuable in precision agriculture tasks such as crop stress assessment, plant disease analysis, and soil moisture evaluation. Traditionally, practitioners obtain soil temperature from the ground or using satellites and other airborne methods, which are costly and offer limited spatial and temporal resolution. In this paper, we introduce a method to measure soil surface temperature dynamics with the use of an unmanned aerial system (UAS). In our approach, we fuse information from thermal and multispectral cameras with ambient variables to generate estimates for soil temperature using computational intelligence models. Using the images, we produce a spatial reconstruction using structure from motion (SfM). After the multimodal registration of the resulting geo-referenced orthomosaics, we characterize the dynamics of the soil surface temperature using the differences between consecutively captured temperature orthomosaics. In our results, we are capable of estimating soil surface temperature from a UAS flying at 30 m AGL with a RMSE of 3.24 degrees C +/- 0.3 and 1.77 degrees C +/- 0.2, at one standard deviation, for two test fields with average ground sampling distances below 6.0 cm/pixel, using a Random Forest regressor. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						68	74		10.1016/j.patrec.2020.07.003													
J								On the expected behaviour of noise regularised deep neural networks as Gaussian processes	PATTERN RECOGNITION LETTERS										Neural networks; Gaussian processes; Signal propagation; Noise regularisation		Recent work has established the equivalence between deep neural networks and Gaussian processes (GPs), resulting in so-called neural network Gaussian processes (NNGPs). The behaviour of these models depends on the initialisation of the corresponding network. In this work, we consider the impact of noise regularisation (e.g. dropout) on NNGPs, and relate their behaviour to signal propagation theory in noise regularised deep neural networks. For ReLU activations, we find that the best performing NNGPs have kernel parameters that correspond to a recently proposed initialisation scheme for noise regularised ReLU networks. In addition, we show how the noise influences the covariance matrix of the NNGP, producing a stronger prior towards simple functions away from the training points. We verify our theoretical findings with experiments on MNIST and CIFAR-10 as well as on synthetic data. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						75	81		10.1016/j.patrec.2020.06.027													
J								Improved ASD classification using dynamic functional connectivity and multi-task feature selection	PATTERN RECOGNITION LETTERS										ASD Classification; Resting state functional MRI; Dynamic functional connectivity; Multi-task feature selection; Multi-kernel learning	MILD COGNITIVE IMPAIRMENT; AUTISM SPECTRUM DISORDER; ALZHEIMERS-DISEASE; BRAIN CONNECTIVITY; NETWORK; REGRESSION	Accurate diagnosis of autism spectrum disorder (ASD), which is a neurodevelopmental disorder and often accompanied by abnormal social skills, communication skills, interests and behavior patterns, has always been a challenging task in clinical practice. Recent studies have shown great potential for using fMRI data to distinguish ASD from typical control (TC). However, it has always been a challenging problem to extract which features from fMRI data and how to combine these different types of features to achieve improved ASD/TC classification performance. To address this problem, in this study we propose an improved ASD/TC classification framework based on dynamic functional connectivity (DFC) and multi-task feature selection. Our proposed ASD/TC classification framework is evaluated on 871 subjects with fMRI data from the Autism Brain Imaging Data Exchange I (ABIDE I) via a 10-fold cross validation strategy. Experimental results show that our proposed method achieves an accuracy of 76.8% and an area under the receiver operating characteristic curve (AUC) of 0.81 for ASD/TC classification. In addition, compared with some existing state-of-the-art methods, our proposed method achieves better accuracy and AUC for ASD/TC classification. Overall, our proposed ASD/TC classification framework is effective and promising for automatic diagnosis of ASD in clinical practice. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						82	87		10.1016/j.patrec.2020.07.005													
J								Quantification of malaria parasitaemia using trainable semantic segmentation and capsnet	PATTERN RECOGNITION LETTERS										Malaria parasite; Pixel-based segmentation; Capsule network; Feature extraction; Blood cell classification		Malaria is a life-threatening mosquito (Anopheles)-borne blood disease caused by the plasmodium parasite. Microscopic examination of peripheral blood smears by experts helps to identify parasites precisely. The manual assessment technique is a tedious and time-consuming process. The present study focuses on developing a hybrid screening algorithm for automated identification and classification of malaria parasite-infected red blood cells (RBCs). Initially, a semantic blood cell segmentation method is adopted where a supervised classifier-regulated pixel-based segmentation is adopted to segment individual RBC present in an image. In pixel-based classification, foreground (RBCs) and background regions are considered, a pixel-based large feature dataset is generated, and an artificial neural network (ANN) classifier is trained. The trained model generates a probability map of an image which is later post-processed by Graph-cut and Marker-controlled Watershed method for developing cropped RBC image set. The proposed segmentation method achieves 99.1% accuracy. Finally, a trained modified Capsule Network (CapsNet) model is used for classification of segmented blood cells to identify the species and stages of the parasites. Here, two specific parasite species viz., Plasmodium vivax and Plasmodium falciparum with stages are considered for classification. The performance of the proposed two-steps hybrid malaria screening is promising and the training and testing on local and benchmark dataset with respect to ground truth yield 98.7% accuracy. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						88	94		10.1016/j.patrec.2020.07.002													
J								If dropout limits trainable depth, does critical initialisation still matter? A large-scale statistical analysis on ReLU networks	PATTERN RECOGNITION LETTERS										Neural networks; Critical initialisation; Signal propagation; Randomised control trial	MULTIPLE; CLASSIFIERS	Recent work in signal propagation theory has shown that dropout limits the depth to which information can propagate through a neural network. In this paper, we investigate the effect of initialisation on training speed and generalisation for ReLU networks within this depth limit. We ask the following research question: given that critical initialisation is crucial for training at large depth, if dropout limits the depth at which networks are trainable, does initialising critically still matter? We conduct a large-scale controlled experiment, and perform a statistical analysis of over 12000 trained networks. We find that (1) trainable networks show no statistically significant difference in performance over a wide range of noncritical initialisations; (2) for initialisations that show a statistically significant difference, the net effect on performance is small; (3) only extreme initialisations (very small or very large) perform worse than criticality. These findings also apply to standard ReLU networks of moderate depth as a special case of zero dropout. Our results therefore suggest that, in the shallow-to-moderate depth setting, critical initialisation provides zero performance gains when compared to off-critical initialisations and that searching for off-critical initialisations that might improve training speed or generalisation, is likely to be a fruitless endeavour. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						95	105		10.1016/j.patrec.2020.06.025													
J								A general model to define the substitution, insertion and deletion graph edit costs based on an embedded space	PATTERN RECOGNITION LETTERS										Graph edit distance; Learning edit costs; Multivariate Gaussian distribution; Neural network	COMPUTATION; OPTIMALITY	The paper presents a method to learn the substitution, deletion and insertion costs on nodes and edges applied to the graph edit distance. We model the learning strategy as a general model and then we concretise it in two different architectures: the first architecture is based on a neural network and the second architecture is based on a multivariate normal distribution, which have been previously trained. The insertion, deletion and substitution costs on nodes and edges are defined as functions that depend on the output of the machine learning architecture. Other machine learning architectures have been presented in the literature to define the graph edit distance costs. Nevertheless, the main feature of our method is that the insertion, deletion and substitution costs are learned together, training the same machine learning and generating only one model. Thus, these costs are influenced one to another, achieving a higher accuracy in the pattern recognition stage than previous methods. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						115	122		10.1016/j.patrec.2020.07.010													
J								SAFFO: A SIFT based approach for digital anastylosis for fresco recOnstruction	PATTERN RECOGNITION LETTERS										SIFT features; Fresco Reconstruction; DAFNE challenge		Anastylosis is an archaeological technique which focuses on the reconstruction of collapsed building and destroyed artworks, starting from the original pieces. Many digital approaches have been developed in the last decade, mainly based on 2D and 3D analysis of the structure of the fragments. These techniques aim at supporting the priceless work of the involved operators, mainly in the decision processes and in the resolution of positioning ambiguities. Techniques acting with this scope lie in the field of the digital anastylosis. In this paper we present SAFFO, a digital approach to 2D reconstruction of frescoes, based on the extraction of SIFT features from a painting. The approach appears to be very robust to false positives, resulting optimal in scenarios involving fragment sets containing spurious elements. The experiments have been performed on the DAFNE (Digital Anastylosis for Fresco challeNgE) dataset, which gathers more than 30 2D artworks and provides several tessellation for each. For its robustness against spurious fragments, SAFFO won the third place in the rank list of DAFNE Challenge 2019. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						123	129		10.1016/j.patrec.2020.07.008													
J								tau-SS3: A text classifier with dynamic n-grams for early risk detection over text streams	PATTERN RECOGNITION LETTERS										Early text classification; Dynamic word n-grams; Incremental classification; SS3; Explainability; Trie; Digital tree	DEPRESSION	A recently introduced classifier, called SS3, has shown to be well suited to deal with early risk detection (ERD) problems on text streams. It obtained state-of-the-art performance on early depression and anorexia detection on Reddit in the CLEF's eRisk open tasks. SS3 was designed to deal with ERD problems naturally since: it supports incremental training and classification over text streams, and it can visually explain its rationale. However, SS3 processes the input using a bag-of-word model lacking the ability to recognize important word sequences. This aspect could negatively affect the classification performance and also reduces the descriptiveness of visual explanations. In the standard document classification field, it is very common to use word n-grams to try to overcome some of these limitations. Unfortunately, when working with text streams, using n-grams is not trivial since the system must learn and recognize which n-grams are important "on the fly". This paper introduces tau-SS3, an extension of SS3 that allows it to recognize useful patterns over text streams dynamically. We evaluated our model in the eRisk 2017 and 2018 tasks on early depression and anorexia detection. Experimental results suggest that tau-SS3 is able to improve both current results and the richness of visual explanations. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						130	137		10.1016/j.patrec.2020.07.001													
J								Axis bound registration of pan-tilt RGB-D scans for fast and accurate reconstruction	PATTERN RECOGNITION LETTERS										Pan-tilt camera; Rotation axis calibration; Servo control; Point cloud registration		A fast and accurate algorithm is presented to register scans from an RGB-D camera, which rotates and scans the entire scene in an automated fashion on a pan-tilt platform. The proposed algorithm, Axis Bound Registration, exploits the movement of the camera that is bound by the two rotation axes of pan-tilt servos, so as to realize fast and accurate registration of acquired point clouds. The rotation parameters, including the rotation axes, pan-tilt transformations and the servo control mechanism, are calibrated beforehand. Subsequently, fast global registration can be performed during online operation with transformation matrices formed by the calibrated rotation axes and angles. In local registration, features are extracted and matched between two scenes. For robust registration, false-positive correspondences are rejected based on the distances of pre-oriented keypoint pairs, namely the circle of deviation constraint. Then, a more accurate registration can be achieved by minimizing the residual distances between correspondence pairs, while estimated transformations are bound to the rotation axes. Results of comparative experiments validate that the proposed method outperforms state-of-the-art algorithms of various approaches based on camera calibration, global registration, and simultaneous-localization-and-mapping in terms of root-mean-square error and computation time. (C) 2020 The Authors. Published by Elsevier B.V.																	0167-8655	1872-7344				OCT	2020	138						138	145		10.1016/j.patrec.2020.07.016													
J								Transcoding across 3D shape representations for unsupervised learning of 3D shape feature	PATTERN RECOGNITION LETTERS										Unsupervised feature learning; 3D shape retrieval and classification; Deep learning; Autoencoder		Unsupervised learning of 3D shape feature is a challenging yet important problem for organizing a large collection of 3D shape models that do not have annotations. Recently proposed neural network-based approaches attempt to learn meaningful 3D shape feature by autoencoding a single 3D shape representation such as voxel, 3D point set, or multiview 2D images. However, using single shape representation isn't sufficient in training an effective 3D shape feature extractor, as none of existing shape representation can fully describe geometry of 3D shapes by itself. In this paper, we propose to use transcoding across multiple 3D shape representations as the unsupervised method to obtain expressive 3D shape feature. A neural network called Shape Auto-Transcoder (SAT) learns to extract 3D shape features via cross-prediction of multiple heterogeneous 3D shape representations. Architecture and training objective of SAT are carefully designed to obtain effective feature embedding. Experimental evaluation using 3D model retrieval and 3D model classification scenarios demonstrates high accuracy as well as compactness of the proposed 3D shape feature. The code of SAT is available at https://github.com/takahikof/ShapeAutoTranscoder. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						146	154		10.1016/j.patrec.2020.07.012													
J								On minimum spanning tree streaming for hierarchical segmentation	PATTERN RECOGNITION LETTERS										Minimum spanning tree; Streaming processing; Hierarchical segmentation; Mathematical morphology		The minimum spanning tree (MST) is one the most popular data structure used to extract hierarchical information from images. This work addresses MST construction in streaming for images. First, we focus on the problem of computing a MST of the union of two graphs with a non-empty intersection. Then we show how our solution can be applied to streaming images. The proposed solution relies on the decomposition of the data in two parts. One stable that does not change in the future. This can be stocked or used for further treatments. The other unstable needs further information before becoming stable. The correctness of proposed algorithm has been proven and confirmed in the case of morphological segmentation of remote sensing images. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						155	162		10.1016/j.patrec.2020.07.006													
J								Encoding multi-granularity structural information for joint Chinese word segmentation and POS tagging	PATTERN RECOGNITION LETTERS										Chinese word segmentation; POS tagging; Joint model; Lattice model; Graph model	MODEL	Recent studies show that the joint Chinese word segmentation and POS tagging can enhance the mutual interaction and yield better performances for two tasks. However, existing joint methods fail to effectively take the advantage of the multiple granularity of information, e.g., character, word and subword, which has been proven prominently useful. In this paper, we propose to improve the joint tasks by leveraging such multi-granularity of information, by exploiting the lattice-LSTM and Convolutional Network (GCN) models for effectively encoding the graph information. On five benchmark datasets our proposed model shows highly competitive performances, achieving the new state-of-the-art results in the literature. Further analysis reveals that the multi-granularity information can relieve the out-of-vocabulary and the long-range dependency issues. Also the GCN structure is more effective for encoding the multi-granularity graph information, compared with the lattice structure. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						163	169		10.1016/j.patrec.2020.07.017													
J								Pedestrian attribute recognition based on multiple time steps attention	PATTERN RECOGNITION LETTERS										Pedestrian attribute; Attention network; Intelligent surveillance		Pedestrian Attribute Recognition (PAR) plays an important role in intelligent video surveillance. This paper tackles two severe challenges in it i.e., complex relations between images and attributes, and imbalanced distribution of pedestrian attributes. Specifically, a new multiple time steps attention mechanism is pro-posed to boost the modeling of the relations. Different from existing attention approaches that only focus on the current and previous time steps, it also exploits the knowledge of next time step. By adaptively capturing the knowledge of multiple time steps, more contextual knowledge is exploited. Meanwhile, to alleviate the challenge of imbalanced distribution of pedestrian attributes, a focal balance loss function is developed by increasing the cost of those attributes difficult to recognize. The proposed framework is dubbed as MTA-Net, which is demonstrated to be effective on two benchmark datasets, i.e., PETA and RAP. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						170	176		10.1016/j.patrec.2020.07.018													
J								MinReduct: A new algorithm for computing the shortest reducts	PATTERN RECOGNITION LETTERS										Rough sets; Shortest reducts; Binary cumulative operations	ROUGH SET REDUCTS	This paper deals with the problem of computing the shortest reducts of a decision system. The shortest reducts are useful for attribute reduction in classification problems and data size reduction. Unfortunately, finding all the shortest reducts is an NP-hard problem. There are some algorithms reported in the literature to overcome the complexity of computing the shortest reducts. However, most of these algorithms relay on costly operations for candidate evaluation. In this paper, we propose a new algorithm for computing all the shortest reducts; based on binary cumulative operations over a pair-wise comparison matrix, and a fast candidate evaluation process. Binary cumulative operations save computation time by avoiding repetitive calculations. Furthermore, unlike other algorithms reported in the literature, our candidate evaluation process relays on low-cost operations which reduce the runtime in most cases. Our experiments over synthetic and real-world decision systems show that our proposal is faster than state of the art algorithms in most decision systems. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						177	184		10.1016/j.patrec.2020.07.004													
J								Deep k-Means: Jointly clustering with k-Means and learning representations	PATTERN RECOGNITION LETTERS										Deep clustering; k-Means; Deep learning; Clustering		We study in this paper the problem of jointly clustering and learning representations. As several previous studies have shown, learning representations that are both faithful to the data to be clustered and adapted to the clustering algorithm can lead to better clustering performance, all the more so that the two tasks are performed jointly. We propose here such an approach for k-Means clustering based on a continuous reparametrization of the objective function that leads to a truly joint solution. The behavior of our approach is illustrated on various datasets showing its efficacy in learning representations for objects while clustering them. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						185	192		10.1016/j.patrec.2020.07.028													
J								Community detection in complex network based on APT method	PATTERN RECOGNITION LETTERS										Complex network; Community detection; Dimension reduction; Manifold learning; Affinity propagation	AFFINITY PROPAGATION; ALGORITHM; MODEL	Community detection is a significant methodology in network science. Traditional methods show limitations in dealing with multi-scale and high-dimensional complex data. As one of the most popular unsupervised algorithms, affinity propagation algorithm (AP) has been widely applied in community detection. However, its negative Euclidean similarity and inflexible parameter may lead to high time or memory consumption and excessive detection. Thus, this article presents a novel affinity propagation algorithm in t-distribution (APT), integrated with manifold learning, for detecting community structure. In APT algorithm, the data is compressed by dimensionality reduction, and joint probability is applied to construct the similarity matrix. Further, based on optimized modularity, parameters are adjusted to improve the accuracy. Experiments show that APT has better adaptability and universality than AP algorithm. In contrast to other mainstream algorithms, our algorithm can extract more meaningful communities from multi-scale and high-dimensional networks. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						193	200		10.1016/j.patrec.2020.07.021													
J								Editorial - Virtual Special Issue: "Hierarchical Representations: New Results and Challenges for Image Analysis"	PATTERN RECOGNITION LETTERS											COMPONENT TREES; SEGMENTATION	This editorial introduces the Virtual Special Issue: "Hierarchical Representations: New Results and Challenges for Image Analysis" that was handled between May 2019 and June 2020. (C) 2020 Published by Elsevier B.V.																	0167-8655	1872-7344				OCT	2020	138						201	203		10.1016/j.patrec.2020.07.019													
J								Probabilistic graph-based valuation model for measuring the relative patent value in a valuation scenario	PATTERN RECOGNITION LETTERS										Patent value; Probabilistic graphical model	INDICATORS; INDEX	With the intense competition of global intellectual property, the increasing number of patents promotes the potential of patent transactions. However,an unknown patent value degrades the patent transaction rate. Automatic patent valuation faces some challenges, including the following: (1) how to represent a valuation object, (2) how to construct the valuation scenario, and (3) how to generate and measure the patent value. To solve the above issues, we propose a probabilistic graph-based patent valuation model. In the model, the textual parts are combined with some structured parts of patents to represent a valuation object. A heterogeneous association network is constructed as the valuation scenario. Thereafter, a patent valuation model is formed by a generative process, which is represented by a probabilistic graphical model.The patent value distribution is learned by making inference using the valuation model. We evaluate our model by comparing it with state-of-the-art models on patent data sets. The results show that our model outperforms other models in the evaluation measurements. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						204	210		10.1016/j.patrec.2020.07.023													
J								MAGAN: A masked autoencoder generative adversarial network for processing missing IoT sequence data	PATTERN RECOGNITION LETTERS										Missing data; Time series data; Sensor data; GAN; Deep learning	IMPUTATION	Missing sequence data prevent local data from reflecting the overall distribution of a sample, hindering data analysis. The problem of missing data during actual production is a serious issue and results in a high defect rate, low dimensionality, and high noise level. In this study, a Masked Generative Adversarial Network (MAGAN) model is proposed that is less affected by the data loss rate than a baseline comparison model, and at an 80% missing data rate, the model can still better reflect the distribution of real data. MAGAN shows better results than a traditional processing method for dealing with missing data. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						211	216		10.1016/j.patrec.2020.07.025													
J								Adversarial learning based attentional scene text recognizer	PATTERN RECOGNITION LETTERS										Scene text recognition; Generative adversarial network; Image rectification	NETWORK	In this paper, we propose an adversarial learning based attentional scene text recognizer to solve the distortion problem of scene text image. We choose a rectification module which can rectify images in both horizontal and vertical directions, and use a recognizer based on the attention mechanism. Through the adversarial learning of the rectification network and the recognition network, we iteratively improve the rectification effect and the recognition performance. The entire network is trained with weak supervision, so only images and corresponding text labels are needed. Our method achieves high performance for both regular and irregular scene text images, and the experimental results tested on multiple benchmarks prove that our method achieves the performance of state-of-the-art. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						217	222		10.1016/j.patrec.2020.07.027													
J								Semantic scene segmentation in unstructured environment with modified DeepLabV3+	PATTERN RECOGNITION LETTERS										Semantic Segmentation; Convolutional Neural Network(CNN); Xception; MobileNetV2		Semantic scene segmentation has become a key application in computer vision and is an essential part of intelligent transportation systems for complete scene understanding of the surrounding environment. While several methods based on deep fully Convolutional Neural Network (CNN) have been emerging, there are two main challenges: (i) They mainly focus on improvement of the accuracy than efficiency. (ii) They assume structured driving environment like in USA and Europe. While most of the current works focus on the well structured driving environment, we focus our research on India Driving Dataset (IDD) which contains data from unstructured traffic scenario. In this paper, we propose modifications in the DeepLabV3+ framework by using lower atrous rates in Atrous Spatial Pyramid Pooling (ASPP) module for dense traffic prediction. We propose to use dilated Xception network as the backbone for feature extraction. A lightweight segmentation framework is also presented by exploring the effectiveness of MobileNetV2 architecture, which achieves competitively high accuracy and is much smaller than other state-of-art architectures. The performance is evaluated in terms of mean Intersection over Union (mIoU) on 26 fine grained classes of IDD. Our proposed model with 24 M parameters achieves 68.41 mIoU on test set and efficient mobile model achieves mIoU of 61.6 by reducing the parameters to 2.2 M only. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						223	229		10.1016/j.patrec.2020.07.029													
J								Multi-view clustering by exploring complex mapping relationship between views	PATTERN RECOGNITION LETTERS										Multi-view clustering; Complex mapping relationship; Partial multi-view clustering; Non-negative matrix factorization (NMF)		Almost all of the existing methods assume that the samples between different views have a strict oneto-one relationship whether it is for complete multi-view data or for partial multi-view data. In this paper, we refer to the neglected many-to-many relationship between cross-view samples as the complex mapping relationship between views. To address this issue, we propose a resultful Complex Mapping Multi-View Clustering (CMMVC) method by exploring the complex mapping relationship between views. We firstly construct a complex mapping relationship matrix for each pair of views by using the nearest neighbor relationship between cross-view samples. Then the complex mapping relationship matrix is introduced into the framework of multi-view clustering based on non-negative matrix factorization to guide multi-view information fusion in order to obtain more compact representation of multi-view data space. Finally, we give the objective function of CMMVC and an effective optimization scheme. The experimental results demonstrate the advantages of the proposed CMMVC method on multi-view clustering tasks by mining the complex mapping relationship between different views. (C) 2020 Published by Elsevier B.V.																	0167-8655	1872-7344				OCT	2020	138						230	236		10.1016/j.patrec.2020.07.031													
J								Analysis of spatiotemporal influence patterns of toxic gas monitoring concentrations in an urban drainage network based on IoT and GIS	PATTERN RECOGNITION LETTERS										Internet of things; Sewage pipe network; Spatiotemporal distribution; Smart city; Geographic information system		Urban underground pipelines have complex structures, long service lives, and are susceptible to illegal interference, corrosion, and external force damage. Therefore, they are a constant security risk that seriously threaten the public security of the city. Due to the complexity of the underground environment, lack of various monitoring technologies, high cost, backwardness of emergency technology research, incongruity of safety management, and the transport of flammable, explosive, toxic, and harmful hazardous sources to densely populated areas, the boundary between industrial, residential, and living areas has become increasingly blurred, causing a major threat to public security, people's lives, industrial production, and social stability. Traditional underground pipeline accident prevention and control technology is currently unable to meet the increasing demands of public security. Combining pipeline accident prevention and control with internet of things and artificial intelligence technology can achieve urban disaster prevention, and therefore is of great interest to researchers. Herein, the research status of underground pipeline accident prevention and control technology is summarized, and an analysis of the advantages of applying big data for risk factor monitoring, risk assessment, risk early warning, and emergency decision-making technology is discussed. Further, the application difficulties and difficulties regarding big data technology in underground pipeline accident prevention and control and their potential solutions are detailed. Based on the internet of things data, spatiotemporal model mining, and Geographic Information System (GIS), we analyze the distribution and influencing factors of harmful gases in the urban underground sewage pipe network of Chongqing City, and explore the influence of smart city developments on harmful gases in the urban underground sewage pipe network. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						237	246		10.1016/j.patrec.2020.07.022													
J								A comprehensive security analysis of match-in-database fingerprint biometric system	PATTERN RECOGNITION LETTERS										Biometric system; Security; Attacks; Fingerprint; Threat model	ATTACKS; VULNERABILITY; IMAGE	The match-in-database fingerprint biometric system has emerged as the most widely used human identification and authentication method in public and private sectors due to its high efficiency, low-cost, and ease of use. This paper provides an exhaustive insight into the security aspect of such systems. We propose a comprehensive threat model depicting sixteen vulnerable attack points, which can act as a reference model while designing a new biometric application. We provide a comparison between the existing and proposed threat model. This article pinpoints all probable attacks at each vulnerable location and suggests possible mitigation techniques to thwart such attack attempts. We investigate the attacks on such systems, present a threat model, and categorize them into eight classes while specifying the risk factor associated with each class. This facilitates any new attack on such a system to be classified into one of these eight classes. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						247	266		10.1016/j.patrec.2020.07.024													
J								RefineU-Net: Improved U-Net with progressive global feedbacks and residual attention guided local refinement for medical image segmentation	PATTERN RECOGNITION LETTERS										U-Net; Medical image segmentation; Progressive global feedbacks; Local refinement; Residual attention gate		Motivated by the recent advances in medical image segmentation using a fully convolutional network (FCN) called U-Net and its modified variants, we propose a novel improved FCN architecture called RefineU-Net. The proposed RefineU-Net consists of three modules: encoding module (EM), global refinement module (GRM) and local refinement module (LRM). EM is backboned by pretrained VGG-16 using ImageNet. GRM is proposed to generate intermediate layers in the skip connections in U-Net. It progressively upsamples the top side output of EM and fuses the resulted upsampled features with the side outputs of EM at each resolution level. Such fused features combine the global context information in shallow layers and the semantic information in deep layers for global refinement. Subsequently, to facilitate local refinement, LRM is proposed using residual attention gate (RAG) to generate discriminative attentive features to be concatenated with the decoded features in the expansive path of U-Net. Three modules are trained jointly in an end-to-end manner thereby both global and local refinement are performed complementarily. Extensive experiments conducted on four public datasets of polyp and skin lesion segmentation show the superiority of the proposed RefineU-Net to multiple state-of-the-art related methods. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						267	275		10.1016/j.patrec.2020.07.013													
J								Controlling information capacity of binary neural network	PATTERN RECOGNITION LETTERS										Deep learning; Binary neural network; Information theory; Shannon entropy		Despite the growing popularity of deep learning technologies, high memory requirements and power consumption are essentially limiting their application in mobile and IoT areas. While binary convolutional networks can alleviate these problems, the limited bitwidth of weights is often leading to significant degradation of prediction accuracy. In this paper, we present a method for training binary networks that maintains a stable predefined level of their information capacity throughout the training process by applying Shannon entropy based penalty to convolutional filters. The results of experiments conducted on the SVHN, CIFAR and ImageNet datasets demonstrate that the proposed approach can statistically significantly improve the accuracy of binary networks. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						276	281		10.1016/j.patrec.2020.07.033													
J								Person re-identification for smart cities: State-of-the-art and the path ahead	PATTERN RECOGNITION LETTERS										Person re-identification; Automated surveillance; People analysis; Smart city applications		One of the indispensable pillars of a smart city is its surveillance infrastructure, and it requires smart techniques to analyze the videos acquired from the surveillance cameras. Person re-identification (PRId) is one of the fundamental tasks in automated visual surveillance, and it has been an area of extensive research spanning the past decade. PRId aims at finding a person who has previously been seen or identified using some unique descriptor of the person. This survey comprises a broad spectrum of PRId methods spanning from traditional to deep-learning, being analyzed and compared. This survey also discusses various PRId frameworks based on machine learning and deep learning. This study emphasizes the challenges in building PRId systems for the benefits of smart cities and presents a critical overview of recent progress and the state-of-the-art approaches to solving some significant challenges of existing PRId systems. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						282	289		10.1016/j.patrec.2020.07.030													
J								Partial attention and multi-attribute learning for vehicle re-identification	PATTERN RECOGNITION LETTERS										Vehicle re-identification; Keypoint detection; Multi-branch network	PERSON REIDENTIFICATION	Intelligent monitoring systems are in increasing need with the rapid growth of traffic nowadays. Vehicle re-identification has vital applications in digital forensics to track suspected vehicles in camera network. It is very challenging to learn discriminative information because of violent changes including the illumination and the viewpoint when a vehicle appears in different cameras, which will lead to the difficulty in distinguishing different vehicles and confusing the same vehicle. To improve the discrimination and the robustness of vehicle re-identification, we propose a partial attention and multi-attribute learning network. Focusing on the local areas which contain abundant discriminative information, we employ partial attention based on vehicle keypoint detection model. Moreover, because the color and the model of a vehicle are relatively stable in different viewpoints, we employ the branch networks to extract multi-attribute features which will improve the robustness. To validate our approach, experiments are carried out on VeRi and VehicleID datasets, and results show that the proposed method achieves higher accuracy compared with other methods. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						290	297		10.1016/j.patrec.2020.07.034													
J								Robust and precise isotropic scaling registration algorithm using bi-directional distance and correntropy	PATTERN RECOGNITION LETTERS										Scaling Registration; Bi-Directional Distance; Correntropy; Iterative Closest Point	AFFINE REGISTRATION; ICP ALGORITHM; POINT SETS	In orthodontics, a patient is collected a lot of 3D oral cavity data, including oral cavity gypsum and scan data sets. To accurately measure the patient's tooth movement, this paper proposes a robust and precise isotropic scaling registration algorithm using bi-directional distance and correntropy. Firstly, because the oral cavity gypsum data sets have a lot of gypsum tumors and bubbles, which can cause the accuracy of registration results to decrease. Then, we introduce the correntropy into the traditional scaling registration model. Secondly, since unconstrained scaling registration is an ill-posed problem, bi-directional distance is used to enhance the robustness. In this way, a registration model using bi-directional distance and correntropy is established. In order to solve this problem, this paper proposes a new registration algorithm with iterative closest point. Moreover, the convergence of the algorithm is proved theoretically. Finally, the proposed algorithm is tested on the orthodontic database, and our experimental results demonstrate that our algorithm performs robust and high accuracy. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						298	304		10.1016/j.patrec.2020.07.026													
J								Semantically-guided low-light image enhancement	PATTERN RECOGNITION LETTERS										Image enhancement; Low light; Semantic information; Simplified retinex model	CONTRAST ENHANCEMENT; SEGMENTATION	Recently, extensive research efforts have been made on low-light image enhancement. Many novel models have been proposed, such as the ones based on the Retinex theory, multiple exposure fusion, and deep neural networks. However, current models do not directly consider the semantic information in the modeling process. As a result, they tend to introduce more artifacts, such as boosted noises and unnatural visual appearances. To address this issue, we propose a fusion-based low-light enhancement model that explicitly harnesses the scene semantics into the enhancement process. In constructing the fusion map, the image regions with a specific semantic category is firstly extracted via semantic segmentation. Then, they are further combined and refined jointly with an illumination-aware map estimated from the scene illumination. Guided by the semantic information, our model is able to intentionally enhance a part of the dark regions, which therefore generates enhanced results with more natural appearances and less artifacts. In experiments, we first validate our model with some empirical studies, including parameter sensitivity and segmentation error tolerance. Then we compare our model with several state-of-the-art low-light enhancement methods, which further shows the effectiveness and advantage of our model. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						308	314		10.1016/j.patrec.2020.07.041													
J								High-quality depth up-sampling via a supervised classification guided MRF model	PATTERN RECOGNITION LETTERS										Markov random fields; K-means clustering; K-NN; Supervised classification; Gradient descent algorithm	MAP SUPERRESOLUTION; ALGORITHM; VIDEO	In this paper, a Supervised Classification assisted Markov Random Field (SC-MRF) model is proposed for generating high-quality up-sampled depth maps. The proposed model aims to reduce depth bleeding and depth confusion artifacts that can be produced at boundary regions of the up-sampled depth maps. In the proposed model, segmentation of low-resolution (LR) depth map is first used to supervise the classification of corresponding high-resolution (HR) color image. With this supervised classification, not only can the depth edges be retained, but redundant textures in the HR color image can be omitted. The classification result is then introduced into the design of a MRF energy function, and the final up-sampled depth map is obtained by optimizing this energy function with the gradient descent algorithm. For simplicity, classical K-means clustering is adopted to segment the LR depth map into several classes, and a feature-based K-nearest neighbour (K-NN) method is utilized for the supervised classification. With the proposed SC-MRF model, interaction between depths of different classes will be strongly suppressed, meaning depth edges are well preserved. Comparisons with the state-of-the-art demonstrate the strong performance of the proposed method both visually and by quantitative evaluation. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						315	320		10.1016/j.patrec.2020.07.011													
J								A Supervised Filter Feature Selection method for mixed data based on Spectral Feature Selection and Information-theory redundancy analysis	PATTERN RECOGNITION LETTERS										Supervised feature selection; Mixed data; Filter feature subset selection; Redundancy analysis	EFFICIENT FEATURE-SELECTION; MUTUAL INFORMATION; ALGORITHM; RELEVANCE	Spectral analysis and Information-theory are two powerful and successful frameworks for feature selection in supervised classification problems. However, most of the methods developed under these frameworks have been introduced for handling exclusively numerical or non- numerical data. In this paper, we propose a supervised filter feature selection method that combines Spectral Feature Selection and Information-theory based redundancy analysis for selecting relevant and non-redundant features in supervised mixed datasets; i.e., datasets where the objects are described simultaneously by both, numerical and non-numerical features. To demonstrate the effectiveness of our proposed supervised filter feature selection method, we conducted several experiments on 40 public real-world datasets. Additionally, we compare our method against relevant state-of-the-art supervised filter methods for numerical, nonnumerical, and mixed data. From this comparison, our method, in general, obtains better results than the results obtained by the other evaluated filter feature selection methods. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						321	328		10.1016/j.patrec.2020.07.039													
J								Weighted hybrid fusion with rank consistency	PATTERN RECOGNITION LETTERS										Semi-supervised learning; Multi-view learning; Information fusion; Hybrid fusion; Rank consistency	MATRIX COMPLETION; VECTOR	This paper proposes a weighted hybrid multi-view fusion method for the semi-supervised classification problem. Instead of getting access to the features from different views directly, this method utilizes the square losses of the multi-view classifiers to exploit the between-view relationship, which preserves the privacy of data. Considering the different prediction capability of classifiers on multiple views, an objective function with the constraint of rank consistency is constructed to weight view-specific learners adaptively, where the constraint makes each view-specific learner improve its performance by exploring the predicted results of other learners. Furthermore, an iterative algorithm based on the Variant Alternating Splitting Augmented Lagrangian Method (VASALM) and the quadratic programming method is developed to optimize the objective function. Experimental results on different real-world datasets demonstrate the effectiveness of the proposed method for multi-view learning. The experiments also analyze parameter sensitivity and convergency of the optimization algorithm. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						329	335		10.1016/j.patrec.2020.07.037													
J								Vanishing region loss for crowd density estimation	PATTERN RECOGNITION LETTERS										Crowd counting; Crowd density estimation; Perspective distortion; Crowd analysis; Convolutional neural networks; Auxiliary loss		Crowd density estimation is a crucial component in surveillance systems to construct safe and efficient urban environments. Due to perspective distortion, individuals in crowd scenes diminish in size as they converge toward the vanishing point. Hence, there are significant visual variations in individuals' size and appearance, which may lead to inaccurate estimations of crowd counts. This paper proposes an intuitive and effective loss function for the error estimation of crowd counts, particularly in vanishing crowd regions. Specifically, estimation errors in vanishing crowd regions are used to refine and generate network filters that are adaptive toward perspective distortion during network training. Extensive experiments on the challenging UCF-QNRF, WorldExpo, and ShanghaiTech benchmark datasets demonstrate the effectiveness of our novel loss function for training a network to achieve accurate crowd density estimation, particularly in the presence of perspective distortion. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						336	345		10.1016/j.patrec.2020.08.001													
J								TGC20ReId: A dataset for sport event re-identification in the wild	PATTERN RECOGNITION LETTERS										Sport; Re-identification; Dataset		Person re-identification (Re-ID) is the task of retrieving a person of interest taken from different cameras or from the same camera in different occasions. To address this challenging task, a large amount of labelled data is required both for testing and for learning. Such high quality annotated data is still rare for many Re-ID applications. In this paper, we introduce a novel dataset to evaluate Re-ID methods in complex real-world scenarios. In this case, we will be using a sporting event as the scenario. For this aim, participants in a 128 km night and day course were captured in five different recording points along the track and later manually identified and annotated. The dataset is evaluated using state-of-the-art techniques and wide array of experimental setups are considered, such as: day vs. night, motion blur, changes in clothing, etc. The evaluation suggests that the features present in the proposed dataset (time difference, unrestricted weather and illumination capture conditions, and the possibility of clothes changes between probe and gallery) pose a challenging scenario for future development in the Re-ID problem. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						355	361		10.1016/j.patrec.2020.08.003													
J								An exact algorithm for time-dependent variational inference for the dynamic stochastic block model	PATTERN RECOGNITION LETTERS										Adjusted rand index; Expectation-maximization algorithm; Longitudinal data	BLOCKMODELS; LIKELIHOOD; PREDICTION	An exact algorithm for estimating the dynamic stochastic block model is proposed. This model assumes a hidden Markov chain for the evolution of the social behavior of a group of individuals at repeated time occasions and may be used to assign these individuals to the latent blocks in a dynamic fashion. For the estimation of this model, the proposed exact algorithm maximizes the target function introduced by Matias and Miele [7]. This function is derived from a variational approximation of the model log-likelihood, based on the assumption that the latent variables identifying the blocks are a posteriori independent across individuals, but not across time occasions. A simulation study is performed to compare the exact algorithm with the approximate maximization algorithm proposed by Matias and Miele [7]. Results show that there is a certain advantage of the first in terms of dynamic assignment of individuals to the latent blocks in comparison to the true blocking structure, as measured by the adjusted Rand index. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						362	369		10.1016/j.patrec.2020.07.014													
J								Real-MFF: A large realistic multi-focus image dataset with ground truth	PATTERN RECOGNITION LETTERS										Image fusion; Multi-focus images; Multi-focus dataset; Deep learning	FUSION	Multi-focus image fusion, a technique to generate an all-in-focus image from two or more partially-focused source images, can benefit many computer vision tasks. However, currently there is no large and realistic dataset to perform convincing evaluation and comparison of algorithms in multi-focus image fusion. Moreover, it is difficult to train a deep neural network for multi-focus image fusion without a suitable dataset. In this letter, we introduce a large and realistic multi-focus dataset called Real-MFF, which contains 710 pairs of source images with corresponding ground truth images. The dataset is generated by light field images, and both the source images and the ground truth images are realistic. To serve as both a well-established benchmark for existing multi-focus image fusion algorithms and an appropriate training dataset for future development of deep-learning-based methods, the dataset contains a variety of scenes, including buildings, plants, humans, shopping malls, squares and so on. We also evaluate 10 typical multi-focus algorithms on this dataset for the purpose of illustration. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						370	377		10.1016/j.patrec.2020.08.002													
J								Learning label correlations for multi-label image recognition with graph networks	PATTERN RECOGNITION LETTERS										Multi-label image recognition; Graph convolutional networks; Label correlation graph; Sparse correlation constraint		Multi-label image recognition is a task that predicts a set of object labels in an image. As the objects co-occur in the physical world, it is desirable to model label dependencies. Previous existing methods resort to either recurrent networks or pre-defined label correlation graphs for this purpose. In this paper, instead of using a pre-defined graph which is inflexible and may be sub-optimal for multi-label classification, we propose the A-GCN, which leverages the popular Graph Convolutional Networks with an Adaptive label correlation graph to model label dependencies. Specifically, we introduce a plug-and-play Label Graph (LG) module to learn label correlations with word embeddings, and then utilize traditional GCN to map this graph into label-dependent object classifiers which are further applied to image features. The basic LG module incorporates two 1 x 1 convolutional layers and uses the dot product to generate label graphs. In addition, we propose a sparse correlation constraint to enhance the LG module, and also explore different LG architectures. We validate our method on two diverse multi-label datasets: MS-COCO and Fashion550K. Experimental results show that our A-GCN significantly improves baseline methods and achieves performance superior or comparable to the state of the art. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						378	384		10.1016/j.patrec.2020.07.040													
J								Automated diagnosis of multi-class brain abnormalities using MRI images: A deep convolutional neural network based method	PATTERN RECOGNITION LETTERS										Magnetic resonance imaging; Deep learning; Brain disease; CNN; Transfer learning	CLASSIFICATION	Automated detection of multi-class brain abnormalities through magnetic resonance imaging (MRI) has received much attention due to its clinical significance and therefore has become an active area of research over the years. The earlier automated schemes often followed traditional machine learning paradigms, in which the proper choice of features and classifiers has remained a major concern. Therefore, deep learning algorithms have been profoundly applied in various medical imaging applications. In this paper, a deep convolutional neural network (CNN) based automated approach is designed for the diagnosis of multi-class brain abnormalities. The proposed CNN model comprises five layers with learnable parameters: four convolutional layers and one fully-connected layer. The objective of designing such a custom deep network is to achieve greater classification performance with reduced number of parameters. The proposed model is evaluated on two benchmark multi-class brain MRI datasets namely, MD1 and MD-2. The model achieved a classification accuracy of 100.00% and 97.50% on MD-1 and MD-2 datasets respectively. Moreover, four pre-trained CNN models based on the transfer learning approach have been tested over the same datasets. The comparative analysis with existing schemes indicates the superiority of the proposed method. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						385	391		10.1016/j.patrec.2020.04.018													
J								A simple deep learning based image illumination correction method for paintings	PATTERN RECOGNITION LETTERS										Deep learning algorithm; Low space-time complexity; Image enhancement; RGB color image; High range variation		Image illumination correction has been a long standing topic for research in the Computer Vision problem. However, all previous literature on this topic has either been statistical in nature in the sense that a specified algorithm has been developed for approaching a particular case of illumination normalization, or involves extremely complex deep learning methods for illumination correction of either one of over illuminated or under illuminated images. We present here a very simple deep learning based image illumination correction architecture which works on color images of paintings irrespective of whether they are under or over illuminated. We have tested the results using a synthetic database as well as on real world painting images of diverse nature. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						392	396		10.1016/j.patrec.2020.08.013													
J								User personality prediction based on topic preference and sentiment analysis using LSTM model	PATTERN RECOGNITION LETTERS										Attention-based LSTM; LDA; Big-Five		Based on the original text information, this paper converts the users' theme preferences and text sentiment features into attention information and combines different forms with the LSTM (Long Short-Term Memory) model to predict the personality characteristics of social network users. Finally, the experimental results of multiple groups' show that the Attention-based LSTM model proposed in the paper can achieve better results than the currently popular methods in the recognition of user personality traits and that the model has good generalization ability. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						397	402		10.1016/j.patrec.2020.07.035													
J								Person re-identification based on multi-scale constraint network	PATTERN RECOGNITION LETTERS										Multi-scale; Person Re-ID; TriHard loss		Combining features of different scales to learn a more discriminative model is an essential solution for person re-identification (Re-ID) tasks. Most existing multi-scale methods are based on the fusion of features from different scales, which cannot exploit information throughly at each scale and cause gradient chaos in optimizing. To address this problem, in this paper we propose an end-to-end multi-scale constraint network(MSCN) to capture detailed information from multiple scales which can independently train each scale and integrate the features of each scale for prediction. In order to retain more information at different scales, we uniformly divide the feature maps into several parts, and vary the number of parts in different scales, then concatenate all the parts in each scale as the entire feature for training. We use both classification loss and metric loss to optimize the network from different aspects. Extensive experiments on three datasets demonstrate that our method achieves very competitive performance. Especially on the CUHK03 dataset, our approach achieves the state-of-the-art results outperforming the current best method by 2.4%/2.0% in Rank-1/mAP. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						403	409		10.1016/j.patrec.2020.08.012													
J								Deep supervised feature selection for social relationship recognition	PATTERN RECOGNITION LETTERS										Social relationship recognition; Feature selection; Deep learning	MODELS	Social relationships link everyone in human society. Exploring social relationships in still images promotes researches of behaviors or characteristics among persons. Previous literature has discovered that face and body attributes can provide effective semantic information for social relationship recognition. However, they ignore that attributes contribute much differently to the recognition accuracy, and these multi-source attributes may contain redundancies and noises. This work aims to promote social relationship recognition accuracy by abstracting multi-source attribute features more efficiently. To this end, we propose a novel Deep Supervised Feature Selection (DSFS) framework to recognize social relationships in photos, which fuses the deep learning algorithm with l(2,1)-norm to learn a discriminative feature subset from multi-source features by leveraging the face and body attributes. Experimental results on PIPA-relation dataset qualitatively demonstrate the effectiveness of the proposed DSFS framework. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						410	416		10.1016/j.patrec.2020.08.005													
J								Fast and efficient reconstruction of digitized frescoes	PATTERN RECOGNITION LETTERS										Cultural heritage; Image reconstruction; Features matching	HISTOGRAM	Virtually recomposing destroyed frescoes is of great importance for heritage conservation. Given a digitized fresco image and a digitized set of fragments, such a problem is challenging due to the potentially large number of fragments, their irregular shape, uniqueness and non-overlapping constraints, the possible absence of fragments and the possible presence of small, homogeneous, eroded and/or spurious fragments. To cope with these specific features, we propose in this paper a fast and efficient non-dense approach benefiting from previous developments in pattern matching. Preliminary experiments led on simulations exhibit a mean accuracy above 90% with a mean translation error of less than 4 pixels and a mean orientation error of about 1 degree. An analysis of fresco and fragment features impacting the algorithm is also provided. Compared to a dense approach and the recent DeepMatch approach, the proposed one remains competitive both in running time and accuracy. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						417	423		10.1016/j.patrec.2020.08.006													
J								Weighted-capsule routing via a fuzzy gaussian model	PATTERN RECOGNITION LETTERS										Capsule network; Deep learning; Image classification; Weighted capsule fuzzy gaussian model; Pose loss		Capsule network (CapsNet) is a novel architecture that takes into account the hierarchical pose relationships between object parts, which had achieved desirable results on image classification. EM-Routing (EM-R) used in CapsNet is the process of assigning child capsules (parts) to each parent capsule (objects) based on a level of agreement, which is similar to the fuzzy clustering process. However, CapsNet still struggles with backgrounds and the presence of noise. In this paper, a new routing algorithm based on a weighted capsule fuzzy gaussian model (WCFGM-R) and a pose loss function are proposed. The proposed algorithm aims to prohibit atypical child capsules from contaminating the parent capsules by incorporating the activations of capsules in a lower layer as weights that play the role of precision. The pose loss provides the best inter-class separation and improves the ability of pattern classification. Indeed, the experimental analyses demonstrate that CapsNet with WCFGM-R outperforms the CapsNet with EM-R in which it shows excellent results on three datasets (MNIST-bg-img, MNIST-bg-rnd, and CIFAR10). (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						424	430		10.1016/j.patrec.2020.08.009													
J								Rotation-invariant NCC for 2D color matching of arbitrary shaped fragments of a fresco	PATTERN RECOGNITION LETTERS										Texture template matching; Normalized cross correlation (NCC); Rotation-invariant color NCC; Arbitrary curvilinear shapes of 2D textures matching; Recognition of spurious fragments by color and/or overlap		The proposed RINCCAS method, an abbreviation of the paper title, was originally developed to participate in DAFNE (Digital Anastylosis of Frescos challeNgE) race, June July2019. The method consists of two phases. Phase 1 extends the classic Normalized Cross Correlation (NCC) for template matching of arbitrary curvilinear 2D shapes of fragments that are assumed belonging to a fresco as perceived in a photograph. For this purpose, each fragment is approximated by one (up to several, but not overlapping) Maximal & Axes-Collinear Inner Rectangles (MACIRs). The extension also includes rotation invariance and vector compatibility of NCC in respect to the three (RGB) color channels. The high positioning accuracy makes it possible to identify eventual/existing spurious fragments in Phase 2 of RINCCAS, as follows - by HSV scheme for color differences, and by accurate recognition of overlaps among the fragments. The first phase is 'log-cubically' complex in speed, estimated on the average size of the MACIRs of the fragments. For some DAFNE tasks, the 1st phase of RINCCAS requires high computational resources (HPC), while a conventional PC is sufficient for its 2nd phase, even in the case of multiple interactive optimization of the ratio between true and spurious fragments. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						431	438		10.1016/j.patrec.2020.08.010													
J								On the design of biometric-based user authentication protocol in smart city environment	PATTERN RECOGNITION LETTERS										Smart city; User authentication; Key agreement; Biometrics; Security; AVISPA; MIRACL	KEY AGREEMENT PROTOCOL; SECURE; SCHEME; MANAGEMENT; CITIES; HEALTH	Among the security services, like authentication, access control, key management and intrusion detection, user authentication is very much needed for a smart city environment because an external authorized user may require the real time data to be accessed directly from the deployed Internet of Things (IoT) enabled smart devices. Using the established session key between the user and an access smart device though mutual authentication and key agreement process, the real time data can be securely accessed. To deal with this issue, we propose a new user authentication scheme in smart city environment using three factors of a legal registered user (mobile device, password and biometrics). The proposed scheme is shown to be robust against a number of potential attacks needed in an IoT-based smart city deployment. The simulation study for formal security verification using the widely-accepted "Automated Validation of Internet Security Protocols and Applications (AVISPA)" tool demonstrates that the proposed scheme is also secure. Furthermore, experiments on various cryptographic primitives have been carried out using "MIRACL Cryptographic SDK: Multiprecision Integer and Rational Arithmetic Cryptographic Library" under both server and Raspberry PI 3 settings. Finally, a comprehensive comparative analysis shows the effectiveness and better security of the proposed scheme as compared with other state of art user authentication schemes. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						439	446		10.1016/j.patrec.2020.08.017													
J								Assessing similarity in handwritten texts	PATTERN RECOGNITION LETTERS										Computers and graphics; Image processing; Handwriting similarity; Character shape comparison		Today, people rely almost full time on digital texts. It is not surprising that handwriting earned a special status, and solutions to mimic real handwriting became attractive. A particular field called handwriting synthesis generates renderings of text which resemble natural writing but are synthesized from actual handwriting samples. The main idea behind samples' current solutions is to collect enough samples to capture a given subject's writing style, and therefore be able to reproduce it in new texts, with natural variability. Nevertheless, the question remains of how much input variability is enough to represent specific handwriting. In this paper, we address sample acquisition for handwriting synthesis. We conducted a study comparing written text similarity between two sets of samples, one using augmented pangrams (with a total of 473 characters) and the other using general texts (with 1586 characters). Our results show that the samples collected with pangrams are statistically equivalent in variation with samples collected using general texts, with many benefits, particularly the shorter time needed to collect the samples. We also made our data collection publicly available, providing a valuable original resource for future research. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						447	454		10.1016/j.patrec.2020.08.011													
J								LieToMe: Preliminary study on hand gestures for deception detection via Fisher-LSTM	PATTERN RECOGNITION LETTERS										Deception detection; Hand gestures; Fisher vector encoding	SIGN-LANGUAGE; RECOGNITION; IMPACT; LIE	The ability to discern lies, more broadly known as deception detection, is an invaluable skill that can strongly influence the outcome of relevant situations such as court trials and police interrogatories. Several devices currently exist and are being used (e.g., magnetic resonance and polygraphs) to ease those tasks; although, due to the subject awareness of such tools, their effectiveness can be compromised by the person intentional behavioural changes. Thus, alternative ways to discriminate lies without using physical devices, could become critical assets for the aforementioned situations, especially in ever improving smart cities environments. In this letter, we present an unorthodox deception detection approach, based on hand gestures found in RGB videos of famous trials. The proposed system first extrapolates hands skeletons from the RGB sequences, then computes meaningful features which are summarized into Fisher Vectors (FVs), and finally feeds this representation to a Long-Short Term Memory (LSTM) network, defined Fisher-LSTM, to try and discern if a lie is being told. In the experimental results, we show how the FV representation can help a LSTM network grasp hand gestures characteristics that could otherwise be missed. What is more, the devised Fisher-LSTM, due to its real-time computation, can be employed in smart environments as an alternative lie detector in situations requiring an immediate response, such as the aforementioned law enforcement examples. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						455	461		10.1016/j.patrec.2020.08.014													
J								Community enhanced graph convolutional networks	PATTERN RECOGNITION LETTERS										Graph representation learning; Community structure; Graph convolutional networks	FRAMEWORK	Graph representation learning is a key technology for processing graph-structured data. Graph convolutional networks (GCNs), as a type of currently emerging and commonly used model for graph representation learning, have achieved significant performance improvement. However, GCNs acquire node representations mainly through aggregating their neighbor information, largely ignoring the community structure which is one of the most important feature of the graph. In this paper, we propose a novel method called Community Enhanced Graph Convolutional Networks (CE-GCN), which integrates both neighborhood and community information to learn node representations. Specifically, the neighborhood information of nodes is aggregated by a graph convolutional network. The community information of nodes is calculated by a modularity constraint. Finally, we incorporate the modularity constraint into the graph convolutional network, and then form a unified model framework. Experimental results on five real-world network datasets demonstrate that CE-GCN significantly outperforms state-of-the-art methods. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						462	468		10.1016/j.patrec.2020.08.015													
J								Cross-modal retrieval via label category supervised matrix factorization hashing	PATTERN RECOGNITION LETTERS										Cross-modal retrieval; Matrix factorization; Hash		Due to the emergence and development of big data, cross-modal hash retrieval has become progressively more important in large-scale multi-modal retrieval tasks depending on its accuracy and efficiency. It completes the retrieval task in a common low-dimensional space by finding a common semantic space for heterogeneous data of different modalities. Recently, many works have concentrated on supervised cross-modal hashing and achieved higher retrieval accuracy. However, there are still many challenges in how to maintain the local geometric structure of the original space in the public space and how to use the supervision information efficiently. To deal with such issues, this paper proposes a hash retrieval method that incorporates supervised information based on matrix factorization (LCSMFH) by maintain the inter-modal and the intra-modal similarity in the original space and make the most of the label information to improve the retrieval task effect. Through experiments on two benchmark data sets, our method is more effective and outperforms state-of-the-art methods. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						469	475		10.1016/j.patrec.2020.08.007													
J								Blockchain-based anomaly detection of electricity consumption in smart grids	PATTERN RECOGNITION LETTERS										Smart grids; Electricity consumption; Anomaly detection; Blockchain	ANALYTICS	The big data generated by Industry 4.0 is expected to increase 20-fold in the next ten years and it has raised various challenges in Industrial Wireless Sensor Networks (IWSNs). Among these challenges, detecting different types of anomalies of industrial electricity consumption in an accurate and timely manner is a priority. If not handled properly, these anomalies could lead to serious consequences, such as irregular fire and paralyzed power system components. While existing anomaly detection techniques may be efficient for old systems, they are now faced with big transmitted data. Therefore, it is important to design new methods that can detect the electricity consumption anomaly and carry out appropriate actions. In this article, we first review several existing work on anomaly detection schemes, and then introduce the system and monitoring models. Then, we present a new framework that aims to detect electricity consumption anomalies accurately and timely using sensor processing, smart meter readings, machine learning and blockchain. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						476	482		10.1016/j.patrec.2020.07.020													
J								Iris presentation attack detection: Where are we now?	PATTERN RECOGNITION LETTERS										Biometrics; Iris presentation attack detection; Security	RECOGNITION	As the popularity of iris recognition systems increases, the importance of effective security measures against presentation attacks becomes paramount. This work presents an overview of the most important advances in the area of iris presentation attack detection published in the recent two years. Newly-released, publicly-available datasets for development and evaluation of iris presentation attack detection are discussed. Recent literature can be seen to be broken into three categories: traditional "hand-crafted" feature extraction and classification, deep learning-based solutions, and hybrid approaches fusing both methodologies. Conclusions of modern approaches underscore the difficulty of this task. Finally, commentary on possible directions for future research is provided. (C) 2020 Published by Elsevier B.V.																	0167-8655	1872-7344				OCT	2020	138						483	489		10.1016/j.patrec.2020.08.018													
J								A two-stage regularization framework for heterogeneous event networks	PATTERN RECOGNITION LETTERS										Event analysis; Regularization frameworks; Graph-based learning	SCIENCE	Event analysis from news and social networks is a promising way to understand complex social phenomena. Each event consists of different components, which indicate what happened, when, where, and the people and organizations involved. Heterogeneous networks are useful for modeling large event datasets, where we map different types of objects (e.g. events and their components), as well as the different relationships between objects. Such networks enable the identification of related events, in which users label some events in categories and then use the network's topological structure to find other events of interest. Although this process can be automated, there is a lack of machine learning methods to properly handle event classification from heterogeneous networks. In this paper, we present the framework named Heterogeneous Event Network Regularization in Two-stages (HENR2). The first stage of HENR2 aims to learn the importance level of each relationship between events and their components. In the second stage, the regularization process considers the importance levels of each relationship to propagate labels on the network. Thus, the classification process is improved by considering the domain characteristics of the event dataset, such as temporal seasonality and geographical distribution. In both stages, our approach also deals with noisy data through parameters that define the confidence level of labeled events during label propagation. Experimental results involving twelve event networks from different application domains show that our proposal outperforms existing regularization frameworks. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						490	496		10.1016/j.patrec.2020.08.019													
J								A new focus detection criterion in holograms of planktonic organisms	PATTERN RECOGNITION LETTERS										Digital holography; Holographic reconstruction; Focus detection; Image quality; Plankton	NUMERICAL RECONSTRUCTION; DIGITAL HOLOGRAPHY; PARTICLE DISTRIBUTIONS; PHASE; MICROSCOPY; COEFFICIENT; TRANSFORM; ALGORITHM; OBJECTS; PLANE	With the increased use of digital holography to observe biological events in three dimensions the need for autofocusing becomes essential. We introduced a modified image quality evaluation technique as a focus criterion in the reconstruction of digital holograms. The method employed was initially proposed by [Wang et al. (2008)] to evaluate image inpainting quality from three aspects: luminance, definition, and gradient similarity. We considered these criteria for automatic focusing, individually and in combination. Our numerical simulation and experimental results obtained from 96 holograms of planktonic organisms show that the combined approach provided more accurate results compared to the individual metrics. We propose the use of such combined criterion to implement autofocusing algorithms for serial holograms, such as from observations of performances of plankton behavior. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						497	506		10.1016/j.patrec.2020.08.004													
J								Distributed discrete-time event-triggered algorithm for economic dispatch problem	PATTERN RECOGNITION LETTERS										Distributed discrete-time algorithm; Economic dispatch problem; Convex optimization; Equality constraint; Event-triggered communication	OPTIMIZATION	In this paper, a distributed discrete-time event-triggered algorithm is proposed to deal with the economic dispatch problem with equality constraint. The communication can be reduced and the energy of systems can be saved by adopting event-triggered communication mechanism. The Zeno behavior is naturally excluded based on discrete iteration scheme. Moreover, the proposed algorithm has been proved to be exponentially convergent with the aid of convex optimization and Lyapunov stability theory that the optimal value is obtained with discrete exponential convergence rate. Finally, the effectiveness of the proposed algorithm is verified via a numerical example. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						507	512		10.1016/j.patrec.2020.08.016													
J								Towards practical implementations of person re-identification from full video frames	PATTERN RECOGNITION LETTERS										Security application; Person re-identification; Pedestrian detection	PEDESTRIAN DETECTION	With the major adoption of automation for cities security, person re-identification (Re-ID) has been extensively studied recently. In this paper, we argue that the current way of studying person re-identification, i.e. by trying to re-identify a person within already detected and pre-cropped images of people, is not sufficient to implement practical security applications, where the inputs to the system are the full frames of the video streams. To support this claim, we introduce the Full Frame Person Re-ID setting (FF-PRID) and define specific metrics to evaluate FF-PRID implementations. To improve robustness, we also formalize the hybrid human-machine collaboration framework, which is inherent to any Re-ID security applications. To demonstrate the importance of considering the FF-PRID setting, we build an experiment showing that combining a good people detection network with a good Re-ID model does not necessarily produce good results for the final application. This underlines a failure of the current formulation in assessing the quality of a Re-ID model and justifies the use of different metrics. We hope that this work will motivate the research community to consider the full problem in order to develop algorithms that are better suited to real-world scenarios. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						513	519		10.1016/j.patrec.2020.08.023													
J								CAN-GAN: Conditioned-attention normalized GAN for face age synthesis	PATTERN RECOGNITION LETTERS										GAN; Face aging synthesis; Normalization layer		This work aims to freely translate an input face to an aging face with robust identity preservation, satisfying aging effect and authentic visual appearance. Witnessing the success of GAN in image synthesis, researchers employ GAN to address the problem of face aging synthesis. However, most GAN-based methods hold that the aging changing of all facial regions is equal, which ignores the fact that different facial regions have distinct aging speeds and aging patterns. To this end, we propose a novel Conditioned-Attention Normalization GAN (CAN-GAN) for age synthesis by leveraging the aging difference between two age groups to capture facial aging regions with different attention factors. In particular, a new Conditioned-Attention Normalization (CAN) layer is designed to enhance the aging-relevant information of face, while smoothing the aging-irrelevant information of face by attention map. Since different facial attributes contribute to the discrimination of age groups with divers degrees, we further present a Contribution-Aware Age Classifier (CAAC) that finely measures the importance of face vector's elements in terms of the age classification. Qualitative and quantitative experiments on several commonly-used datasets show the advance of CAN-GAN compared with the other competitive methods. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						520	526		10.1016/j.patrec.2020.08.021													
J								CtrlFaceNet: Framework for geometric-driven face image synthesis	PATTERN RECOGNITION LETTERS										Generative adversarial networks; Self-supervised; Image editing		In this work, we introduce a novel framework based on Generative Adversarial Networks to control the pose, expression and facial features of a given face image using another face image. It can then be used for data augmentation, pose invariant face identification, face verification, and lightweight image editing. Generating new realistic face images with controllable poses, facial features, and expressions is a challenging generative learning problem due to skin tone variations, the identity preservation problem, necessity to deal with unseen large poses, and the absence of ground truth images in the training process. We make the following contributions. First, we present a network, CtrlFaceNet that can control a source face image while preserving the identity and skin tone. Second, we introduce a method for training the framework in fully self-supervised mode using a large-scale dataset of unconstrained face images. Third, we show that the style loss function can be used to preserve the skin tone of the source image. The experimental results show that our approach outperforms all other baselines. Furthermore, to the best of our knowledge, we are the first to train such a model using large-scale dataset of unconstrained face images. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						527	533		10.1016/j.patrec.2020.08.026													
J								EBIT: Weakly-supervised image translation with edge and boundary enhancement	PATTERN RECOGNITION LETTERS										Weakly-supervised; GAN; Canny; Silhouette; Disentanglement		In this paper, targeting image translation between the thermal and visible domains, we propose a novel framework to enhance the edge and boundary features during translation. We tackle the unsupervised training task where sample image pairs from two domains are randomly chosen so the image content does not match, but we also take advantage of paired feature maps during the feature disentanglement process. This can be considered to be weakly-supervised training. First, since thermal images usually have vague edges, we propose to apply a Canny operator to strengthen the edge features of the thermal images. Next, to define the correct object boundaries, we extract the boundary features from silhouette masks which are paired with the visible domain images. Then we disentangle the features of both domains into a domain-shared latent space and a domain-exclusive latent space. In the domain-shared latent space, the edge and boundary features extracted from the thermal and visible domains respectively act as domain-shared information which is used to render the edges and define the boundaries of the translated images. In the domain-exclusive latent space, domain-exclusive information such as colour is used to render the colour of objects of the translated images. In addition, we propose a pixel-wise adversarial loss rather than more traditional ones. The experimental results show that the proposed method has the ability to render realistic edge and colour features within the correct object boundaries and outperforms several state-of-the-art methods. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						534	539		10.1016/j.patrec.2020.08.025													
J								Joint multi-scale discrimination and region segmentation for person re-ID	PATTERN RECOGNITION LETTERS										Person re-identification; Pattern recognition; Multi-scale deep network; Semantic segmentation; Supervised learning		Most existing person re-identification methods are mainly based on human part partition with horizontal stripes or human body semantic segmentation. In this paper, we propose a method called MDRS (Multiscale Discriminative network with Region Segmentation) to integrate multi-scale discriminative feature learning, horizontal stripe partition and semantic segmentation in a single framework, in which multiscale horizontal stripe partition and usage of both global and local features make the framework be robust to human pose variation, occlusion and background clutter, and semantic segmentation boosts the performance of person identification via shared multi-scale feature extraction. MDRS is trained end-to-end with a multi-task learning strategy that considers three tasks simultaneously: person identification, triplet prediction and pixel-wise semantic segmentation. Comprehensive experiments confirm that our approach exceeds many methods and robustly achieves excellent performances on mainstream evaluation datasets including Market-1501, DukeMTMC-reid and CUHK03. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						540	547		10.1016/j.patrec.2020.08.022													
J								Intelligent medical heterogeneous big data set balanced clustering using deep learning	PATTERN RECOGNITION LETTERS										Deep learning; Smart medical; Heterogeneous big data; Deep neural network; Data balanced clustering	ALGORITHM; OPTIMIZATION	In order to address the clustering problem of intelligent medical data, the data sets were not preprocessed using the traditional method, leading to a large amount of calculation, low efficiency, and large data cluster center offset distance. We proposed a balanced clustering algorithm for intelligent medical heterogeneous big data set using deep learning. Firstly, a deep neural network model based on incremental updating was constructed, and adaptive training and adjustment were made according to data scale, and the multi-layer feature learning of heterogeneous big data sets of intelligent medical care. Secondly, under-sampling preprocessing was carried out on the data set so that the data of the heterogeneous big data set was in a balanced state, and on this basis, clustering calculation of the heterogeneous big data was conducted. Then, the clustering center was set according to the kernel density estimation results, and the data cluster center was updated iteratively until convergence by combining the data features obtained from deep learning and euclidean distance calculation, so as to complete the balanced clustering of the heterogeneous big data set of intelligent medical treatment. The results show that the proposed algorithm has the advantages of small data cluster center offset distance, short clustering time, low energy consumption, high Macro-F1 value and NMI value, and the accuracy of clustering can be as high as 95%, the calculational cost is low, which has certain advantages. (C) 2020 Elsevier Ltd. All rights reserved. (c) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						548	555		10.1016/j.patrec.2020.08.027													
J								Attention-aware invertible hashing network with skip connections	PATTERN RECOGNITION LETTERS										Image retrieval; Deep hashing; Invertible network	LEARNING BINARY-CODES	In recent years, Convolutional Neural Networks (CNNs) have shown promising performance on image hashing retrieval. However, due to the information-discarded nature of CNN, some meaningful information can not be further extracted into a deep level and embedded into hash codes. To solve the problem, this study attempts to design an invertible CNN feature extractor to fully maintain input information meanwhile having well generalization ability. Specifically, we propose a novel Attention-Aware Invertible Hashing Network with Skip Connection (AIHN-SC) for image retrieval. Represented by an invertible feature, the hash code can be learned and generated from image characteristics preserving all input information. For achieving favourable generalization ability in our invertible architecture, we present a novel spatial attention mechanism to highlight regions involving semantic information. In addition, we introduce two kinds of skip connection, i.e. hierarchical and residual connections, which aim to provide richer knowledges for hash code learning and ease our training process. Extensive experiments on benchmark datasets demonstrate the effectiveness of our proposed AIHN-SC and show the significant performance in image retrieval against the state-of-the-arts. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						556	562		10.1016/j.patrec.2020.09.002													
J								On the application of convex transforms to metric search	PATTERN RECOGNITION LETTERS										Sorting and searching; Similarity search; Convex transform; Efficient search		Scalable similarity search in metric spaces relies on using the mathematical properties of the space in order to allow efficient querying. Most important in this context is the triangle inequality property, which can allow the majority of individual similarity comparisons to be avoided for a given query. However many important metric spaces, typically those with high dimensionality, are not amenable to such techniques. In the past convex transforms have been studied as a pragmatic mechanism which can overcome this effect; however the problem with this approach is that the metric properties may be lost, leading to loss of accuracy. Here, we study the underlying properties of such transforms and their effect on metric indexing mechanisms. We show there are some spaces where certain transforms may be applied without loss of accuracy, and further spaces where we can understand the engineering tradeoffs between accuracy and efficiency. We back these observations with experimental analysis. To highlight the value of the approach, we show three large spaces deriving from practical domains whose dimensionality prevents normal indexing techniques, but where the transforms applied give scalable access with a relatively small loss of accuracy. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						563	570		10.1016/j.patrec.2020.08.008													
J								Robust least squares one-class support vector machine	PATTERN RECOGNITION LETTERS										One-class support vector machine; Least square one-class support vector machine; Correntropy; One-class classification	CLASSIFIERS; SIGNAL	In comparison with the conventional one-class support vector machine (OCSVM), least squares OCSVM (LS-OCSVM) can describe similarity between a new-coming sample and training set more accurately. However, LS-OCSVM is very sensitive to outliers in training set. The main reason lies that the values of square error function for outliers are relatively large, which makes LS-OCSVM put more emphasis on these outliers. To enhance the robustness of LS-OCSVM against outliers, a novel robust LS-OCSVM based on correntropy loss function is proposed. As a result, the unbounded convex square loss function of LS-OCSVM is substituted by a bounded nonconvex correntropy loss function. Experimental results on synthetic and benchmark data sets show that robust LS-OCSVM possesses better anti-outlier and generalization abilities in comparison with its related approaches. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						571	578		10.1016/j.patrec.2020.09.005													
J								Nonparametric maximum likelihood estimation using neural networks	PATTERN RECOGNITION LETTERS										Neural network; Maximum likelihood estimation; Nonparametric; Probability density function	DENSITY-ESTIMATION	Estimation of probability density functions is an essential component of various applications. Nonparametric techniques have been widely used for this task owing to the difficulty in parameterization of data. In particular, certain kernel density estimation methods have been developed. However, they are either incapable of maximum likelihood estimation or require the maintenance of a training set to process new patterns. In this study, a new approach, called the nonparametric maximum likelihood neural network (MLNN), is proposed. This is a nonparametric method, relying on maximum likelihood and neural network. It is compact in form and does not require the maintenance of training patterns. Theoretical and experimental analyses demonstrate the efficacy of the proposed approach. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						580	586		10.1016/j.patrec.2020.09.006													
J								Deep shape constrained network for robust face alignment	PATTERN RECOGNITION LETTERS										Face alignment; Facial landmarks; Deep learning		Recently, face alignment plays a significant role in lots of computer vision tasks. Due to the various head poses, diverse expressions and partial occlusions, face alignment has still been a great challenge. In this paper, we propose a new Deep Shape Constrained Network (DSCN) for robust face alignment. Specifically, our DSCN consists of a Shape Initialization Network (SIN) and a series of Shape Refinement Networks (SRN) in a cascaded way. SIN takes the holistic facial image as the input and generates promising preliminary facial landmarks for SRN. Each SRN consists of two task-specific streams: shape stream aiming to learn the constraint of global face shape, and the texture stream used to extract robust face feature. Finally, the network automatically learns to combine these two streams in an early fusion approach. Experimental results show that the proposed method outperforms state-of-the-art approaches on 300-W dataset, which consists of LFPW, HELEN and AFW. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						587	593		10.1016/j.patrec.2020.09.008													
J								Fused 3-D spectral-spatial deep neural networks and spectral clustering for hyperspectral image classification	PATTERN RECOGNITION LETTERS										Hyperspectral image classification; Dimensionality reduction; Convolutional Neural Network (CNN); Band clustering; Feature extraction	CONSTRAINED BAND SELECTION	Recently, classification and dimensionality reduction (DR) have become important issues of hyperspectral image (HSI) analysis. Especially, HSI classification is a challenging task due to the high-dimensional feature space, with a large number of spectral bands, and a low number of labeled samples. In this paper, we propose a new HSI classification approach, which is called fused 3-D spectral-spatial deep neural networks for hyperspectral image classification. We propose an unsupervised band selection method to avoid the problem of redundancy between spectral bands and automatically find a set of groups Ck each one containing similar spectral bands. Moreover, the model uses the different groups of selected bands to extract spectral-spatial features in order to improve the classification rate. Each group is associated with a 3-D CNN model, which are then fused to improve the precision of classification. The main advantage of the proposed method is to keep the initial spectral-spatial features by automatically selecting relevant spectral bands, which improves the classification of HSI using a low number of labeled samples. Experiments on two real HSIs, Indian Pines and Salinas datasets, are performed to demonstrate the effectiveness of the proposed method. Results show that the proposed method reaches competitive good performances, and achieves better classification rates compared to various state-of-the-art techniques. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						594	600		10.1016/j.patrec.2020.08.020													
J								Spatial probabilistic distribution map-based two-channel 3D U-net for visual pathway segmentation	PATTERN RECOGNITION LETTERS											IMAGES; MODEL; ORGANS	Precise segmentation of the visual pathway is significant in preoperative planning to prevent the surgeon from touching it during the operation. Manual segmentation is time consuming and tedious. Thus, automatic segmentation strategies are necessary to assist clinical evaluation. However, the low contrast and blurred boundary between the target and the background in the image make automatic segmentation a challenging problem. This paper proposed a spatial probabilistic distribution map (SPDM)-based two-channel 3D U-Net to make shape and position prior information available for deep learning. First, an atlas calculated by group-wise registration was used to register each training volume image for deformation field determination. Second, the deformation field was used to transform the label of the corresponding training image to the template space, and then all the warped labels were summed up to create an SPDM. Third, the region of interest of the image and SPDM were sent to the network to predict the final segmentation. The proposed method was evaluated and compared against a conventional 3D U-Net on two datasets. Experimental results indicated that our method overcame the problem of low contrast and achieved better performance than previous methods. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						601	607		10.1016/j.patrec.2020.09.003													
J								Effective schizophrenia recognition using discriminative eye movement features and model-metric based features	PATTERN RECOGNITION LETTERS										Eye movement; Schizophrenia recognition; Discriminative feature; Fixation prediction; Classification	DISORDERS; ATTENTION; TRACKING; DEFICITS; TESTS	Eye movement abnormalities have been effective biomarkers that provide the possibility of distinguishing patients with schizophrenia from healthy controls. The existing methods for measuring eye movement abnormalities mostly focus on synchronic parameters, such as fixation duration and saccade amplitude, which can be directly obtained from eye movement data, while lack of considering more thorough features. In this paper, to better characterize eye-tracking dysfunction, we create a dataset containing 100 images with eye movement data of 40 patients and 30 healthy controls via a free-viewing task, and propose two types of features for effective schizophrenia recognition, i.e. the hand-crafted discriminative eye movement features and the model-metric based features via utilizing the computational models of fixation prediction and the metrics of evaluating their prediction performance. Using the proposed features, two commonly used classifiers including support vector machine and random forest have been trained for classification between patients and controls. Experimental results demonstrate the effectiveness of the proposed features for improving classification performance, and the potential that our method can serve as an alternative and promising approach for the computer-aided diagnosis of schizophrenia. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						608	616		10.1016/j.patrec.2020.09.017													
J								Depth occlusion perception feature analysis for person re-identification	PATTERN RECOGNITION LETTERS										Person re-identification; Pose estimation; Occlusion perception; Self-attention mechanism; Deep neural network	SIMILARITY	Person re-identification (ReID) has achieved significant improvement under the setting of matching two holistic person images. However, persons are easily occluded by the various objects and other persons in real-world scenarios, making Person ReID a challenging task. In this paper, we propose a novel method named Pose-Driven Visibility Model (PDVM) to effectively solve the degradation of recognition performance caused by occlusion. Firstly, we extract non-occluded human body features through pose estimation, pay attention to the salient features of non-human parts through self-attention mechanism, and obtains the final feature representation after the combination. Secondly, we more accurately locate person body parts by utilizing the detected human keypoints in different occlusion situations, effectively reducing the impact of unalignment and realizing better matching for persons. We implement extensive experiments on Occluded-DukeMTMC and Partial-REID. Our proposed method achieves state of the art performances which reaches 53.0% Rank-1 accuracy on Occluded-DukeMTMC dataset and ablation analysis also verify the effectiveness of our method. 2020 Elsevier Ltd. All rights reserved (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						617	623		10.1016/j.patrec.2020.09.009													
J								Greedy AutoAugment	PATTERN RECOGNITION LETTERS										AutoAugment; Augmentation; ANN; Neural networks; Vision; Classification		A major problem in data augmentation is to ensure that the generated new samples cover the search space. This is a challenging problem and requires exploration for data augmentation policies to ensure their effectiveness in covering the search space. In this paper, we propose Greedy AutoAugment as a highly efficient search algorithm to find the best augmentation policies. We use a greedy approach to reduce the exponential growth of the number of possible trials to linear growth. The Greedy Search also helps us to lead the search towards the sub-policies with better results, which eventually helps to increase the accuracy. The proposed method can be used as a reliable addition to the current artifitial neural networks. Our experiments on four datasets (Tiny ImageNet, CIFAR-10, CIFAR-100, and SVHN) show that Greedy AutoAugment provides better accuracy, while using 360 times fewer computational resources. Published by Elsevier B.V.																	0167-8655	1872-7344				OCT	2020	138						624	630		10.1016/j.patrec.2020.08.024													
J								DAFNE: A dataset of fresco fragments for digital anastlylosis	PATTERN RECOGNITION LETTERS										Image dataset; Image fragmentation; Image reconstruction; Cultural heritage; Frescoes	IMAGE; RECOVERY	Restoring artworks seriously damaged or completely destroyed is a challenging task. In particular, the reconstruction of frescoes has to deal with problems such as very small fragments, irregular shapes and missing pieces. Several attempts have been done to develop new techniques for helping restorers in the matching process, starting from traditional image processing methods to the more recent deep learning approaches. However, as often happens in the Cultural Heritage field, the availability of labeled data to test new strategies is limited, and publicly available datasets contain only few samples. For this reason, in this paper we introduce DAFNE, a large dataset that includes hundreds of thousands of images of fresco fragments artificially generated to guarantee a high variability in terms of shapes and dimensions. Fragments have been obtained starting from 62 images of famous frescoes of various artists and historical periods, in order to consider different artistic styles, subjects and colors. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						631	637		10.1016/j.patrec.2020.09.015													
J								Learnable pooling weights for facial expression recognition	PATTERN RECOGNITION LETTERS										Facial expression recognition; Deep learning; Kernel methods		Pooling layers are spatial down-sampling layers used in convolutional neural networks (CNN) to gradually downscale the feature map, increase the receptive field size and reduce the number of the parameters in the model. The use of pooling layers leads to less computing complexity and memory consumption reduction but also introduces invariance to certain filter distortions which may induce subtle detail loss. This behaviour is undesired for some fine-grained recognition tasks such as facial expression recognition (FER) which highly relies on specific regional distortion detection. In this paper, we introduce a more filter distortion aware pooling layer based on kernel functions. The proposed pooling reduces the feature map dimensions while keeping track of the majority of the information fed to the next layer instead of ignoring part of them. The experiments on RAF, FER2013 and ExpW databases demonstrate the benefits of such layer and show that our model achieves competitive results with respect to the state-of-the-art approaches. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						644	650		10.1016/j.patrec.2020.09.001													
J								Robust discriminant analysis using multi-directional projection pursuit	PATTERN RECOGNITION LETTERS										Classification; Dimension reduction; Optimal scores; Projection pursuit; Robustness		While linear discriminant analysis (LDA) is a widely used classification method, it is highly affected by outliers which commonly occur in various real datasets. Therefore, several robust LDA methods have been proposed. However, they either rely on robust estimation of the sample means and covariance matrix which may have noninvertible Hessians or can only handle binary classes or low dimensional cases. The proposed robust discriminant analysis is a multi-directional projection-pursuit approach which can classify multiple classes without estimating the covariance or Hessian matrix and work for high dimensional cases. The weight function effectively gives smaller weights to the points more deviant from the class center. The discriminant vectors and scoring vectors are solved by the proposed iterative algorithm. It inherits good properties of the weight function and multi-directional projection pursuit for reducing the influence of outliers on estimating the discriminant directions and producing robust classification which is less sensitive to outliers. We show that when a weight function is appropriately chosen, then the influence function is bounded and discriminant vectors and scoring vectors are both consistent as the percentage of outliers goes to zero. The experimental results show that the robust optimal scoring discriminant analysis is effective and efficient. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						651	656		10.1016/j.patrec.2020.09.013													
J								Pattern recognition and beyond: Alfredo Petrosino's scientific results	PATTERN RECOGNITION LETTERS										Computer vision; Deep learning; Fuzzy logic; High-performance computing; Machine learning; Neuro-fuzzy; Neural network; Soft and granular computing	BACKGROUND SUBTRACTION; NEURAL-NETWORKS; IMAGE; TRACKING; SPACE; RETRIEVAL; ALGORITHM; MOTIFS; SETS	We summarize the main scientific contributions of our friend and colleague Alfredo Petrosino, full professor in computer science at the University of Naples Parthenope, Italy. They mainly cover topics in high-performance computing, neural network models, soft and granular computing, computer vision, and machine learning. We also highlight how most of his research activity lays the foundation for biometry and its applications. (C) 2020 Elsevier B.V. All rights reserved.																	0167-8655	1872-7344				OCT	2020	138						659	669		10.1016/j.patrec.2020.07.032													
J								On complex archaeologies: conceptualizing social complexity and its potential for archaeology	ADAPTIVE BEHAVIOR										Social complexity; complex systems thinking; archaeological theory; material complexity; community formation	ORIGINS	This article surveys a number of approaches in complex systems thinking and their relevance for applications in the field of archaeology. It focuses in particular on the fundamental role of social interactions and information transmission as constituent elements for the development of organizational complexity on a community level. Given the impossibility of direct observations of these constituent interactions and practices, it is then outlined how this theoretical model can be applied on the material remains found in the archaeological record. It is discussed how material surroundings - including architectural structures and material objects - are used to shape and structure social interactions and practices in various ways. It is shown how complex organizational structures develop through underlying mechanisms of change such as diversification, connectivity and standardization, and how these can be applied in archaeological case studies. The presented framework will thus show how structures of social organization and development of social complexity can be inferred from the archaeological record.																	1059-7123	1741-2633				OCT	2020	28	5			SI		323	328		10.1177/1059712319826539													
J								A Generalized Heterogeneous Type-2 Fuzzy Classifier and Its Industrial Application	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Uncertainty; Fuzzy logic; Fuzzy neural networks; Support vector machines; Computational modeling; Artificial neural networks; Fuzzy neural network (FNN); fuzzy rule-base classifier; heterogeneous type-2 fuzzy system; online learning	INTERVAL TYPE-2; NEURAL-NETWORK; LOGIC SYSTEMS; PREDICTION; IDENTIFICATION; OPTIMIZATION; SETS	Recently, evolving fuzzy systems have been proved to be effective in dealing with real-time data streams. However, their fixed structures are not flexible enough to address the structural variations triggered by the changing operating conditions or system states in complex industrial environments. A novel generalized heterogeneous interval type-2 (IT2) fuzzy classifier, named as GHIT2Class, is proposed in this paper, which is built upon a multivariable IT2 fuzzy neural network. To fully reflect the industrial data characteristics of uncertainty, this paper proposes an approach of constructing the uncertainty footprint with ellipsoidal rotation. A rule pruning method based on error and incentive intensity dynamic adjustment mechanism is reported in the process of modeling, and a corresponding rule recall mechanism is designed to avoid rules of catastrophic forgetting. In addition, the simultaneous update of the upper and lower bounds of IT2 fuzzy consequent parameters is designed to relieve the computing overhead of the fuzzy systems. The performance of the proposed GHIT2Class is experimentally validated by a number of synthetic datasets and industry study cases by using state-of-the-art comparative classifiers, where the proposed approach outperforms the others in achieving the best tradeoff between accuracy and simplicity.																	1063-6706	1941-0034				OCT	2020	28	10					2287	2301		10.1109/TFUZZ.2019.2930492													
J								ADONiS-Adaptive Online Nonsingleton Fuzzy Logic Systems	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Uncertainty; Frequency selective surfaces; Fuzzy logic; Estimation; Noise level; Fuzzy sets; Signal to noise ratio; Adaptive input; firing strength; nonsingleton fuzzy logic system (NSFLS); noise estimation; online learning	NOISE-LEVEL ESTIMATION; SIGNAL	Nonsingleton fuzzy logic systems (NSFLSs) have the potential to capture and handle input noise within the design of input fuzzy sets (FSs). In this article, we propose an online learning method that utilizes a sequence of observations to continuously update the input FSs of an NSFLS, thus providing an improved capacity to deal with variations in the level of input-affecting noise, common in real-world applications. The method removes the requirement for both a priori knowledge of noise levels and relying on offline training procedures to define input FS parameters. To the best of our knowledge, the proposed ADaptive, ONline Nonsingleton (ADONiS) fuzzy logic system (FLS) framework represents the first end-to-end framework to adaptively configure nonsingleton input FSs. The latter is achieved through online uncertainty detection applied to a sliding window of observations. Since real-world environments are influenced by a broad range of noise sources, which can vary greatly in magnitude over time, the proposed technique for combining online determination of noise levels with associated adaptation of input FSs provides an efficient and effective solution which elegantly models input uncertainty in the FLS's input FSs, without requiring changes in any other part (e.g., antecedents, rules or consequents) of the FLS. In this article, two common chaotic time series (Mackey-Glass, Lorenz) are used to perform prediction experiments to demonstrate and evaluate the proposed framework. Results indicate that the proposed adaptive NSFLS framework provides significant advantages, particularly in environments that include high variation in noise levels, which are common in real-world applications.																	1063-6706	1941-0034				OCT	2020	28	10					2302	2312		10.1109/TFUZZ.2019.2933787													
J								A Generalization of the Choquet Integral Defined in Terms of the Mobius Transform	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Aggregation function; Choquet integral; fuzzy measure; Mobius transform	AGGREGATION FUNCTIONS; RESPECT	In this article, we propose a generalization of the Choquet integral, starting fromits definition in terms of the Mobius transform. We modify the product on R considered in the Lovasz extension form of the Choquet integral into a function F, and we discuss the properties of this new functional. For a fixed n, a complete description of all F yielding an n-ary aggregation function with a fixed diagonal section, independent of the considered fuzzy measure, is given, and several particular examples are presented. Finally, all functionsF yielding an aggregation function, independent of the number n of inputs and of the considered fuzzy measure, are characterized, and related aggregation functions are shown to be just the Choquet integrals over the distorted inputs.																	1063-6706	1941-0034				OCT	2020	28	10					2313	2319		10.1109/TFUZZ.2019.2933803													
J								Adaptive Fuzzy Nonsmooth Backstepping Output-Feedback Control for Hypersonic Vehicles With Finite-Time Convergence	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Backstepping; Observers; Convergence; Vehicle dynamics; Control systems; Fuzzy logic; Mathematical model; Adaptive fuzzy control; finite-time control; fixed-time state observer; hypersonic vehicle; nonsmooth backstepping	UNCERTAIN NONLINEAR-SYSTEMS; DYNAMIC SURFACE CONTROL; SLIDING MODE CONTROL; CONTROL DESIGN; OBSERVER; STABILIZATION	It is commonly believed that uncertainties are obstacles to the tracking performances of flexible air-breathing hypersonic vehicles (FAHVs). In addition, the estimation of unmeasured states in an FAHV makes this issue much more complicated. To deal with these difficulties, this article explores a novel adaptive fuzzy nonsmooth backstepping output-feedback control scheme for the FAHV with closed-loop finite-time convergence. First, to approximate the unknown dynamics of the FAHV, some function approximators are constructed by utilizing an interval type-2 (IT2) fuzzy logic system. On this basis, a fixed-time convergent adaptive IT2 fuzzy observer is designed to estimate the unmeasured flight path angle and the unmeasured angle of attack of the FAHV accurately and rapidly. Consequently, the estimation errors of the designed observer are convergent in a fixed time independent of their initial estimation errors. Furthermore, based on the estimated states, velocity and altitude tracking controllers are designed by using a finite-time adaptive IT2 fuzzy nonsmooth backstepping control technique, which avoids the problem of "explosion of complexity" in traditional backstepping methods. Subsequently, rigorous Lyapunov stability analysis is conducted to show the finite-time convergence of the closed-loop signals of the FAHV control system. Finally, the robustness and superiority of the proposed control scheme is validated by several simulations in representative scenarios.																	1063-6706	1941-0034				OCT	2020	28	10					2320	2334		10.1109/TFUZZ.2019.2934934													
J								Multiobjective Fault-Tolerant Control for Fuzzy Switched Systems With Persistent Dwell Time and Its Application in Electric Circuits	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Switches; Output feedback; Actuators; Fuzzy systems; Switched systems; Stability analysis; Extended dissipativity; output feedback control; persistent dwell-time (PDT); Takagi-Sugeno (T-S) fuzzy model	H-INFINITY CONTROL; SINGULARLY PERTURBED SYSTEMS; OUTPUT-FEEDBACK CONTROL; LINEAR-SYSTEMS; STABILITY; DESIGN	This article concentrates on the output feedback controller design problem for discrete-time nonlinear switched systems with actuator faults. The Takagi-Sugeno fuzzy model is adopted to approximate the nonlinearity of the plant with a set of local linear models. The persistent dwell-time (DT) switching law, which is more general than DT or average DT switching, is introduced to govern the switching among subsystems. In order to alleviate the effects of actuator failures on system stability and performance, a synthesized fault-tolerant output feedback controller ensuring various performance requirements is designed. Intensive attention is focused on establishing sufficient conditions, which can guarantee the exponential mean-square stability as well as the prescribed extended dissipativity property of the closed-loop system. By virtue of the Lyapunov stability theory and appropriate matrix transformation methods, the desired controller gains can be obtained by solving a convex optimization problem. The developed method is finally applied to address the control issue of a tunnel diode circuit system model to illustrate its efficiency and applicability.																	1063-6706	1941-0034				OCT	2020	28	10					2335	2347		10.1109/TFUZZ.2019.2935685													
J								Incremental Missing-Data Imputation for Evolving Fuzzy Granular Prediction	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Adaptation models; Data models; Predictive models; Machine learning; Genetic algorithms; Prediction algorithms; Sensor phenomena and characterization; Data stream; evolving intelligence; fuzzy system; incremental learning; missing-data imputation	DATA STREAMS; VALUES; IDENTIFICATION; REGRESSION; MODELS	Missing values are common in real-world data stream applications. This article proposes a modified evolving granular fuzzy-rule-based model for function approximation and time-series prediction in an online context, where values may be missing. The fuzzy model is equipped with an incremental learning algorithm that simultaneously imputes missing data and adapts model parameters and structure over time. The evolving fuzzy granular predictor (eFGP) handles single and multiple missing values on data samples by developing reduced-term consequent polynomials and utilizing time-varying granules. Missing at random (MAR) and missing completely at random (MCAR) values in nonstationary data streams are approached. Experiments to predict monthly weather conditions, the number of bikes hired on a daily basis, and the sound pressure on an airfoil from incomplete data streams show the usefulness of eFGP models. Results were compared with those of state-of-the-art fuzzy and neuro-fuzzy evolving modeling methods. A statistical hypothesis test shows that eFGP outperforms other evolving intelligent methods in online MAR and MCAR settings, regardless of the application.																	1063-6706	1941-0034				OCT	2020	28	10					2348	2362		10.1109/TFUZZ.2019.2935688													
J								Adaptive Fuzzy Inverse Optimal Control for Uncertain Strict-Feedback Nonlinear Systems	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Optimal control; Adaptive systems; Fuzzy logic; Nonlinear systems; Approximation error; Backstepping; Adaptive fuzzy control; inverse optimal control; strict-feedback; uncertain nonlinear systems	DISCRETE-TIME-SYSTEMS; SMALL-GAIN APPROACH; TRACKING CONTROL; DESIGN; INPUT	This article first investigates the adaptive fuzzy inverse optimal control design problem for a class of uncertain strict-feedback nonlinear systems. Fuzzy logic systems are utilized to identify the unknown nonlinear dynamics, and then, an equivalent system and an auxiliary system are established. Based on the auxiliary system and using backstepping recursive design algorithm, an adaptive fuzzy inverse optimal scheme, associating with a meaningful objective functional, is developed. It is proved that the presented adaptive fuzzy inverse optimal control scheme can guarantee that the considered system is input-to-state stabilizable and also achieves the goal of inverse optimality with respect to the cost functional. Finally, the simulation studies and comparisons via two examples are provided to confirm the validity of the developed control strategy.																	1063-6706	1941-0034				OCT	2020	28	10					2363	2374		10.1109/TFUZZ.2019.2935693													
J								Fault Detection for Fuzzy Semi-Markov Jump Systems Based on Interval Type-2 Fuzzy Approach	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Fault detection; fuzzy semi-Markov jump systems (FSMJSs); interval type-2 (IT2) fuzzy method; sensor saturation	WAVELET-NEURAL-NETWORK; STABILITY ANALYSIS; DESIGN; ALGORITHM; TRACKING; OBSERVER	This article studies the fault detection problem for continuous-time fuzzy semi-Markov jump systems (FSMJSs) by employing an interval type-2 (IT2) fuzzy approach. First, the continuous-time FSMJSs model is designed and the parameter uncertainty is addressed by the IT2 fuzzy approach, where the characteristic of sensor saturation is taken into account in the control system. Second, the IT2 fuzzy semi-Markov mode-dependent filter is constructed, which is employed to deal with the fault detection problem. Then, by using the Lyapunov theory, it can be guaranteed that the constructed fault detection model based on this filter and IT2 FSMJSs is stochastically stable with H8 performance. Moreover, the quantization strategy is applied to the fault detection plant to dispose of the problem of limited network bandwidth. Compared with the existing literature, the differences mainly lie in two aspects, one is that the IT2 fuzzy method is utilized for FSMJSs to tackle the parameter uncertainty of system, and the other is to detect the fault signal of IT2 FSMJSs by using the fault detection system that is constructed based on the IT2 fuzzy semi-Markovmode-dependent filter and IT2 FSMJSs. Finally, two simulation examples are provided to illustrate the effectiveness and the usefulness of the proposed theoretical method.																	1063-6706	1941-0034				OCT	2020	28	10					2375	2388		10.1109/TFUZZ.2019.2936333													
J								Adaptive Fuzzy Event-Triggered Control for Leader-Following Consensus of High-Order Nonlinear Systems	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Multi-agent systems; Control systems; Synchronization; Nonlinear dynamical systems; Topology; Adaptive fuzzy control; event-triggered control; high-order nonlinear system; leader-following consensus	LINEAR MULTIAGENT SYSTEMS; OUTPUT CONSENSUS; TRACKING CONTROL; NETWORK; SYNCHRONIZATION	This article is concerned with the adaptive fuzzy event-triggered control for leader-following consensus of high-order nonlinear systems under directed communication topologies. Utilizing fuzzy logic systems to model the uncertain dynamics of the followers, a novel adaptive fuzzy event-triggered control law is presented, and a distributed event-trigger condition is proposed, simultaneously. The developed adaptive fuzzy event-triggered control is carried out by the violation of the designed event-trigger condition, which can realize ultimate state synchronization with bounded tracking errors. Moreover, the data transmissions and the frequency of the control law updates can be significantly reduced compared with the control law with fixed sampling period. In addition, it is proved that the Zeno behavior is excluded in the proposed control scheme. Finally, a simulation example is provided to illustrate the obtained theoretical results.																	1063-6706	1941-0034				OCT	2020	28	10					2389	2400		10.1109/TFUZZ.2019.2936359													
J								Fuzzy Observer-Based Repetitive Tracking Control for Nonlinear Systems	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Nonlinear systems; Control systems; Observers; Symmetric matrices; Linear matrix inequalities; Stability analysis; Linear systems; Fuzzy observer-based control; fuzzy repetitive control; fuzzy Lyapunov functional; linear matrix inequality (LMI)	OUTPUT-FEEDBACK CONTROL; NETWORKED CONTROL; CONTROL DESIGN; SERVO SYSTEM; STABILITY CONDITIONS; LYAPUNOV FUNCTION; MIMO SYSTEMS; MODEL	This article is concerned with the periodic tracking control problem for nonlinear systems. First, the Takagi-Sugeno (T-S) fuzzy model is employed to describe the nonlinear control systems. Second, considering the partly unmeasurable states of the system, a novel fuzzy observer-based repetitive controller, which is the mixed controller of the fuzzy observer-based controller and the fuzzy repetitive controller, is designed to deal with the periodic tracking control problem. To reduce the conservatism and increase the feasible solution space of the stabilization conditions, a new fuzzy relaxed matrix technique is developed by introducing some relaxed matrices in the derivative of the fuzzy normalized membership function. Then, the fuzzy Lyapunov functional with an additional separation parameter and the augmented fuzzy matrix technique (the interactions of fuzzy observer subsystems) are proposed such that the delay-dependent stability condition of the closed-loop system in the form of linear matrix inequality is obtained with less conservatism. It is worth noting that due to introducing an additional parameter in the fuzzy Lyapunov functional, the fuzzy controller and fuzzy observer can be separately designed, which largely enhances the flexibility of design with low computational complexity. Finally, three examples are provided to illustrate the effectiveness and less conservatism of the proposed method.																	1063-6706	1941-0034				OCT	2020	28	10					2401	2415		10.1109/TFUZZ.2019.2936808													
J								Adaptive Fuzzy Prescribed Performance Control of Nontriangular Structure Nonlinear Systems	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Nonlinear systems; Adaptive systems; Control design; Backstepping; Fuzzy logic; Process control; Adaptive fuzzy control; backstepping design technique; nontriangular structure nonlinear systems; prescribed performance control	DYNAMIC SURFACE CONTROL; OUTPUT-FEEDBACK CONTROL; NEURAL-NETWORK CONTROL; CONTROL DESIGN	In this article, a new n-step fuzzy adaptive output tracking prescribed performance control problem is investigated for a class of nontriangular structure nonlinear systems. In the control design process, the mean value theorem is used to separate the virtual state variables needed for the control design, and the implicit function theorem is exploited to assert the existence of the desired continuous control. The fuzzy logic systems are used to identify the unknown nonlinear functions and ideal controller, respectively. By constructing a novel iterative Lyapunov function, a new n-step adaptive backstepping control design algorithm is established. The prominent characteristics of the proposed adaptive fuzzy backstepping control design algorithm are as follows: one is that it can ensure the closed-loop control system is the semiglobally uniformly ultimately bounded and the tracking error can converge within the prescribed performance bounds. The other is that it solves the controller design problem for the nontriangular nonlinear systems that the previous adaptive backstepping design techniques cannot deal with. Two examples are provided to show the effectiveness of the presented control method.																	1063-6706	1941-0034				OCT	2020	28	10					2416	2426		10.1109/TFUZZ.2019.2937046													
J								Uncertain Data Modeling Based on Evolving Ellipsoidal Fuzzy Information Granules	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Takagi-Sugeno model; Data models; Resource management; Numerical models; Performance analysis; Feature extraction; Prediction algorithms; Clustering; ellipsoidal information granules (IGs); interpretability; Takagi-Sugeno (T-S) systems; time-series forecasting; uncertainty	PRINCIPLE; FUNDAMENTALS	Dealing with uncertain data requires effective methods to properly describe their real meaning in terms of a tradeoff between interpretability and generality on the process of knowledge formation based on data abstraction. This article proposes an online granulation process based on evolving ellipsoidal fuzzy information granules (EEFIG) and the principle of justifiable granularity (PJG) for data streams parameterization. The granulation process consists in the information granule development taking into consideration the data stream with a simplified optimal granularity allocation. In the sequel, an evolving Takagi-Sugeno fuzzy model based on the ellipsoidal granules is proposed for data reconstruction and one-step ahead prediction from past data numerical evidence. Experimental studies concerning clustering, data granulation, and time-series forecasting are performed to illustrate the effectiveness of the proposed method.																	1063-6706	1941-0034				OCT	2020	28	10					2427	2436		10.1109/TFUZZ.2019.2937052													
J								Noncooperative and Cooperative Strategy Designs for Nonlinear Stochastic Jump Diffusion Systems With External Disturbance: T-S Fuzzy Approach	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Game theory; Hamilton-Jacobi-Isaacs inequality (HJII); multiobjective optimization problem (MOP); nonlinear stochastic system; Takagi-Sugeno (T-S) fuzzy model	DIFFERENTIAL-GAMES; STABILITY ANALYSIS; COORDINATION; ALGORITHM	In this article, we consider the multiplayer H-infinity noncooperative and cooperative game strategy designs for a class of stochastic jump diffusion systems with external disturbance. To attenuate the effect from the competitive strategies of other players and unpredictable external disturbance on the desired target tracking performance of each player, a multiplayer H-infinity noncooperative game strategy design problem is proposed and formulated as an equivalent multiobjective optimization problem (MOP) with a Nash equilibrium solution. Also, the multiplayer H-infinity cooperative game strategy design problem is discussed and formulated as an equivalent single-objective optimization problem. To overcome the difficulties in solving multiple Hamilton-Jacobi-Issacs inequalities (HJIIs) for noncooperative and cooperative game strategy designs, the Takagi-Sugeno fuzzy model is introduced to approximate the nonlinear stochastic system and HJII could be transformed into a set of linear matrix inequalities (LMIs). Besides, an LMI-constrained multiobjective evolution algorithm is developed to efficiently solve the MOP of a noncooperative multiplayer H8 stochastic game strategy design problem. A financial market with multiple investors is provided as a simulation example to demonstrate the effectiveness of the proposed noncooperative and cooperative game investment strategies.																	1063-6706	1941-0034				OCT	2020	28	10					2437	2451		10.1109/TFUZZ.2019.2939956													
J								Sampled-Data Fuzzy Control With Guaranteed Cost for Nonlinear Parabolic PDE Systems via Static Output Feedback	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Mathematical model; Output feedback; Fuzzy control; Sensors; Symmetric matrices; Nonlinear systems; Actuators; Fuzzy control (FC); guaranteed cost; linear matrix inequality (LMI); partial differential equation (PDE); sampled-data (SD) control; static output feedback	DISTRIBUTED-PARAMETER SYSTEMS; OBSERVER-BASED CONTROL; H-INFINITY CONTROL; CHAOTIC SYSTEMS; CONTROL DESIGN; STABILIZATION; STABILITY	This article introduces a sampled-data (SD) static output feedback fuzzy control (FC) with guaranteed cost for nonlinear parabolic partial differential equation (PDE) systems. First, a Takagi-Sugeno (T-S) fuzzy parabolic PDE model is employed to represent the nonlinear PDE system. Second, with the aid of the T-S fuzzy PDE model, a SD FC design with guaranteed cost under spatially averaged measurements is developed in the formulation of linear matrix inequalities by utilizing a time-dependent Lyapunov functional and inequality techniques, which can stabilize exponentially the PDE system while providing an optimized upper bound on the cost function. The membership functions of the proposed controller are determined by the measurement output and independent of the fuzzy PDE plant model. Finally, simulation results are presented to control the diffusion equation and the FitzHugh-Nagumo equation for demonstrating the effectiveness of the proposed method.																	1063-6706	1941-0034				OCT	2020	28	10					2452	2465		10.1109/TFUZZ.2019.2939961													
J								Consensus Reaching With Time Constraints and Minimum Adjustments in Group With Bounded Confidence Effects	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Numerical models; Time factors; Artificial intelligence; Computer science; Analytical models; Decision making; Informatics; Bounded confidence; consensus; group decision making (GDM); minimum adjustments; opinion dynamics (OD); time constraints	GROUP DECISION-MAKING; OPINION DYNAMICS; SOCIAL NETWORK; COST; MODEL; FRAMEWORK; CONTEXT	In the bounded confidence model, it is widely known that individuals rely on the opinions of their close friends or people with similar interests. Meanwhile, the decision maker always hopes that the opinions of individuals can reach a consensus in a required time. Therefore, with this idea in mind, this article develops a consensus reaching model with time constraints and minimum adjustments in a group with bounded confidence effects. In the proposed consensus approach, the minimum adjustments rule is used to modify the initial opinions of individuals with bounded confidence, which can further influence the opinion evolutions of individuals to reach a consensus in a required time. The properties of the model are studied, and detailed numerical examples and comparative simulation analysis are provided to justify its feasibility.																	1063-6706	1941-0034				OCT	2020	28	10					2466	2479		10.1109/TFUZZ.2019.2939970													
J								A Refined Fuzzy Min-Max Neural Network With New Learning Procedures for Pattern Classification	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Pattern classification; Fuzzy neural networks; Distortion; Fuzzy logic; Neurons; Artificial neural networks; Fuzzy min-max model; hyperbox structure; neural network learning; online learning; pattern classification	RULE; OPTIMIZATION; SELECTION	The fuzzy min-max (FMM) neural network stands as a useful model for solving pattern classification problems. FMM has many important features, such as online learning and one-pass learning. It, however, has certain limitations, especially in its learning algorithm, which consists of the expansion, overlap test, and contraction procedures. This article proposes a refined fuzzy min-max (RFMM) neural network with new procedures for tackling the key limitations of FMM. RFMM has a number of contributions. First, a new expansion procedure for overcoming the problems of overlap leniency and irregularity of hyperbox expansion is introduced. It avoids the overlap cases between hyperboxes from different classes, reducing the number of overlap cases to one (containment case). Second, a new formula that simplifies the original rules in the overlap test is proposed. It has two important features: (i) identifying the overlap leniency problem during the expansion procedure; (ii) activating the contraction procedure to eliminate the containment case. Third, a new contraction procedure for overcoming the data distortion problem and providing more accurate decision boundaries for the contracted hyperboxes is proposed. Fourth, a new prediction strategy that combines both membership function and distance measure to prevent any possible random decision-making during the test stage is proposed. The performance of RFMM is evaluated with the UCI benchmark datasets. The results demonstrate the effectiveness of the proposed modifications in making RFMM a useful model for solving pattern classification problems, as compared with other existing FMM and non-FMM classifiers.																	1063-6706	1941-0034				OCT	2020	28	10					2480	2494		10.1109/TFUZZ.2019.2939975													
J								Fuzzy Removing Redundancy Restricted Boltzmann Machine: Improving Learning Speed and Classification Accuracy	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Feature extraction; Support vector machines; Data mining; Redundancy; Deep learning; Robustness; Image recognition; Deep learning; feature extraction; fuzzy removing redundancy restricted Boltzmann machine (F3RBM); restricted Boltzmann machine (RBM); support vector machine (SVM)	FEATURE-EXTRACTION; NEURAL-NETWORK; SYSTEM	To improve the feature extraction ability and shorten the learning time, fuzzy removing redundancy restricted Boltzmann machine (F3RBM) is developed. The features extracted by F3RBM with unsupervised learning are imported into support vector machine (SVM) to establish F3RBM-SVM model, which achieves fast and high-precision automatic classification of different samples. To expand the feature extraction capability of restricted Boltzmann machine (RBM), the deterministic parameters of control model are replaced by fuzzy numbers in view of the superiority of fuzzy idea and the redundancy removal mechanism is introduced. Comparing the feature similarity of hidden units with the threshold value, if the similarity is greater than the threshold value, they are considered to be redundant units with the same features. The redundant units are removed to achieve further dimension reduction. Finally, the learning speed, feature extraction ability, and classification accuracy of different models are compared in MINIST handwritten dataset, Fashion MNIST dataset, and Olivetti Face dataset. The experimental results show that the feature extraction capability of FRBM and F3RBM is better than that of RBM. When there are a large number of hidden units, the learning speed of F3RBM is obviously faster than that of FRBM. The features extracted from F3RBM are imported into the SVM to build F3RBM-SVM model, which improves the classification accuracy and learning speed than general classifier. When adding other noises, F3RBM-SVM has better robustness than other models.																	1063-6706	1941-0034				OCT	2020	28	10					2495	2509		10.1109/TFUZZ.2019.2940415													
J								Computing With Comparative Linguistic Expressions and Symbolic Translation for Decision Making: ELICIT Information	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Linguistics; Computational modeling; Proposals; Decision making; Uncertainty; Semantics; Cognition; Computing with words; comparative linguistic expressions; decision making (DM); hesitant fuzzy linguistic term set; symbolic translation	RANKING FUZZY NUMBERS; AGGREGATION OPERATORS; TERM SETS; REPRESENTATION MODEL; PREFERENCE RELATIONS; WORDS	Many real-world decision making (DM) problems present changing contexts in which uncertainty or vagueness appear. Such uncertainty has been often modeled based on the linguistic information by using single linguistic terms. Dealing with linguistic information in DM demands processes of computing with words whose main characteristic is to emulate human beings reasoning processes to obtain linguistic outputs from linguistic inputs. However, often single linguistic terms are limited or do not express properly the expert's knowledge, being necessary to elaborate richer linguistic expressions easy to understand and able to express greater amount of knowledge, as it is the case of the comparative linguistic expressions based on hesitant fuzzy linguistic terms sets. Nevertheless, current computational models for comparative linguistic expressions present limitations both from understandability and precision points of view. The 2-tuple linguistic representation model stands out in these aspects because of its accuracy and interpretability dealing with linguistic terms, both related to the use of the symbolic translation, although 2-tuple linguistic values are still limited by the use of single linguistic terms. Therefore, the aim of this article is to present a new fuzzy linguistic representation model for comparative linguistic expressions that takes advantage of the goodness of the 2-tuple linguistic representation model and improve the interpretability and accuracy of the results in computing with words processes, resulting the so-called extended comparative linguistic expressions with symbolic translation. Taking into account the proposed model, a new computing with words approach is presented and then applied to a DM case study to show its performance and advantages in a real case by comparing with other linguistic decision approaches.																	1063-6706	1941-0034				OCT	2020	28	10					2510	2522		10.1109/TFUZZ.2019.2940424													
J								Nonfragile Retarded Sampled-Data Switched Control of T-S Fuzzy Systems and Its Applications	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Fuzzy systems; Symmetric matrices; Switches; Stability analysis; Linear matrix inequalities; Delays; Fuzzy membership functions (FMFs); linear matrix inequalities (LMIs); nonfragile retarded sampled-data control; Takagi-Sugeno (T-S) fuzzy system	KRASOVSKII FUNCTIONAL-APPROACH; CHAOTIC SYSTEMS; FEEDBACK CONTROL; STABILIZATION; STABILITY; SYNCHRONIZATION	This article investigates the nonfragile retarded fuzzy sampled-data control design of Takagi-Sugeno (T-S) fuzzy system. When compared to the traditional sampled-data control scheme, a constant signal transmission delay is involved with uncertainties in control gain parameters, which exploited for the first time to tackle the stabilization analysis of the closed-loop T-S fuzzy systems based on the switched control idea. By constructing a fuzzy membership function (FMF)-dependent Lyapunov-Krasovskii functional (LKF) and utilizing the time-derivative information of the FMF, the sufficient conditions are carried out in terms of linear matrix inequalities (LMIs) such that the resultant T-S fuzzy systems can achieve the globally asymptotically stable under the designed control. Then, the desired control gain is determined by solving the obtained LMIs. Finally, the numerical simulations are demonstrated to show the advantage and applicability of the derived sufficient conditions.																	1063-6706	1941-0034				OCT	2020	28	10					2523	2532		10.1109/TFUZZ.2019.2940432													
J								Lower Triangle Factor-Based Fault Estimation and Fault Tolerant Control for Fuzzy Systems	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Observers; Circuit faults; Output feedback; Fuzzy systems; Estimation error; State feedback; Convergence; fault estimation; fault tolerant control (FTC); lower triangle factor-based estimation observer (LTFEO); Takagi-Sugeno (T-S) fuzzy systems	LINEAR-SYSTEMS; ACTUATOR FAULT; DELAY; IDENTIFICATION; OBSERVER; DESIGN	This article focuses on the problems of observer-based fault estimation and dynamic output feedback fault tolerant control (FTC) for a class of nonlinear fuzzy systems with faults. A novel lower triangle factor-based estimation observer (LTFEO) is proposed for the first time. The designed LTFEO can reduce to the traditional robust estimation observer (REO) and is superior to the adaptive estimation observer (AEO) in the relevant literature, furthermore, the designed LTFEO does not contain the output derivatives. The initial estimation accuracy is improved under the designed estimation observer. An iterative estimation algorithm based on the iterative LTFEOs is given, by which, the obtained mean sequence of state and fault estimation errors can converge to zeros (vector) and the proof is given in theoretical. Then, a new dynamic output feedback FTC is designed to stabilize the faulty system, where the output feedback gains can be determined by state feedback gains and the computational difficulty is decreased. Less conservative stability conditions for the estimation error dynamics and the closed-loop system are given in terms of linear matrix inequalities (LMIs). A tunnel diode circuit system is applied to test the effectiveness and merits of the proposed methods.																	1063-6706	1941-0034				OCT	2020	28	10					2533	2542		10.1109/TFUZZ.2019.2941168													
J								Adaptive Type-2 Fuzzy Neural-Network Control for Teleoperation Systems With Delay and Uncertainties	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Uncertainty; Robots; Adaptation models; Delays; Data collection; Adaptive systems; Artificial neural networks; Adaptive control; neural network (NN); time delay; teleoperation systems; type-2 fuzzy (T2F) models; uncertainties	INTERVAL TYPE-2; LOGIC SYSTEMS; BILATERAL TELEOPERATION; DESIGN; SETS; MANIPULATORS; ROBOT	Interacting with human operators, remote environment, and communication networks, teleoperation systems are considerably suffering from complexities and uncertainties. Managing these is of paramount importance for safe and smooth performance of teleoperation systems. Among the countless solutions developed by researchers, type-2 fuzzy (T2F) algorithms have shown an outstanding performance in modeling complex systems and tackling uncertainties. Moreover, artificial neural networks (NNs) are well known for their adaptive learning potentials. This article proposes an adaptive interval type-2 fuzzy neural-network control scheme for teleoperation systems with time-varying delays and uncertainties. The T2F models are developed based on the experimental data collected from a teleoperation setup over a local computer network. However, the resulted controller is evaluated on an intercontinental communication network through the Internet between Australia and Scotland. Moreover, the slave robot and the remote workspace are completely different and unforeseen. Stability and performance of the proposed control is analyzed by Lyapunov-Krasovskii method. Comprehensive comparative studies demonstrate that the proposed controller outperforms traditional techniques in experimental evaluations.																	1063-6706	1941-0034				OCT	2020	28	10					2543	2554		10.1109/TFUZZ.2019.2941173													
J								A Representable Uninorm-Based Intuitionistic Fuzzy Analytic Hierarchy Process	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Mathematical model; Computational modeling; Analytic hierarchy process; Indexes; Analytical models; Fuzzy sets; Acceptability; intuitionistic fuzzy analytic hierarchy process; intuitionistic fuzzy preference relation; multiplicative consistency; representable uninorm	GROUP DECISION-MAKING; RECIPROCAL PREFERENCE RELATIONS; MULTIPLICATIVE CONSISTENCY; MODELS; RANKING; WEIGHTS; VALUES; AHP	Intuitionistic fuzzy preference relations (IFPRs) have been exposed to be an appropriate and effective preference representation framework in an analytic hierarchy process (AHP) with vagueness and hesitancy. This article focuses mainly on obtaining an intuitionistic fuzzy extension of Tanino's multiplicative consistency and deriving an analytic solution of normalized intuitionistic fuzzy weights (NIFWs) from IFPRs as well as checking acceptability of IFPRs. This article first introduces two indices to, respectively, measure hesitancy of intuitionistic fuzzy judgments and hesitancy of IFPRs, and illustrates that any existing multiplicative consistency model of IFPRs is not an actual intuitionistic fuzzy extension of Tanino's multiplicative consistency. A conjunctive-representable cross-ratio uninorm-based functional equation is then developed to define multiplicative consistency of IFPRs and a consistency index is devised to measure the inconsistency degree of an IFPR. This article establishes a representable uninorm-based transformation method for consistent IFPRs and intuitionistic fuzzy weights, and proposes a new framework of NIFWs. Based on the transformation method and the row hesitancy distribution of an IFPR, a logarithmic least square model is constructed and its analytic solution is found by applying the Lagrange multiplier method to its equivalent least square model. This article puts forward a novel acceptability checking method by taking both acceptable consistency and acceptable hesitancy into consideration. A representable uninorm-based fusion method is presented to aggregate local NIFWs into global intuitionistic fuzzy weights and a representable uninorm-based likelihood formula is given and used to compare and rank intuitionistic fuzzy weights in the proposed intuitionistic fuzzy AHP. Six numerical examples including an outstanding Ph.D. student selection problem are provided to illustrate and validate the obtained results.																	1063-6706	1941-0034				OCT	2020	28	10					2555	2569		10.1109/TFUZZ.2019.2941174													
J								On the Functional Equivalence of TSK Fuzzy Systems to Neural Networks, Mixture of Experts, CART, and Stacking Ensemble Regression	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Fuzzy systems; Neural networks; Machine learning; Stacking; Machine learning algorithms; Fuzzy sets; Regression tree analysis; CART; ensemble regression; fuzzy systems; mixture of experts; neural networks; stacking	UNIVERSAL APPROXIMATION; DECISION TREES; CONTROLLERS; ALGORITHM; RULES; TIME	Fuzzy systems have achieved great success in numerous applications. However, there are still many challenges in designing an optimal fuzzy system, e.g., how to efficiently optimize its parameters, how to balance the trade-off between cooperations and competitions among the rules, how to overcome the curse of dimensionality, how to increase its generalization ability, etc. Literature has shown that by making appropriate connections between fuzzy systems and other machine learning approaches, good practices from other domains may be used to improve the fuzzy systems, and vice versa. This article gives an overview on the functional equivalence between Takagi-Sugeno-Kang fuzzy systems and four classic machine learning approaches-neural networks, mixture of experts, classification and regression trees, and stacking ensemble regression-for regression problems. We also point out some promising new research directions, inspired by the functional equivalence, that could lead to solutions to the aforementioned problems. To our knowledge, this is so far the most comprehensive overview on the connections between fuzzy systems and other popular machine learning approaches, and hopefully will stimulate more hybridization between different machine learning algorithms.																	1063-6706	1941-0034				OCT	2020	28	10					2570	2580		10.1109/TFUZZ.2019.2941697													
J								Boundary Static Output Feedback Control for Nonlinear Stochastic Parabolic Partial Differential Systems via Fuzzy-Model-Based Approach	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Stochastic processes; Mathematical model; Hilbert space; Control theory; Stability; Biological system modeling; Actuators; Boundary control; mean square exponential stability; stochastic partial differential equations (SPDEs); Takagi-Sugeno (T-S) fuzzy model	ROBUST STABILIZATION DESIGN; WAVE; EQUATIONS; STABILITY	This article investigates a fuzzy boundary control problem of a class of stochastic nonlinear systems modeled by the Ito-type parabolic stochastic partial differential equation (SPDE). Initially, a Takagi-Sugeno fuzzy SPDE model is proposed to accurately represent the nonlinear SPDE system. Then, on the basis of the infinite-dimensional infinitesimal operator, a fuzzy boundary static output feedback controller is developed in terms of a set of linear matrix inequalities to locally exponentially stabilize the resulting system in the mean square sense. By using an approximation argument and constructing a Lyapunov function for the mild solution, the local mean square exponential stability of the closed-loop system is proved. Meanwhile, the closed-loop well-posedness analysis is also given by virtue of the semigroup theory. Finally, a simulation study on a Belousov-Zhabotinsky reaction-diffusion system with random parameter variation is presented to illustrate the effectiveness of the proposed method.																	1063-6706	1941-0034				OCT	2020	28	10					2581	2591		10.1109/TFUZZ.2019.2941698													
J								Adaptive Linearizing Fuzzy Control of Delayed MIMO Nonstrict-Feedback Systems	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Delay effects; Fuzzy logic; Control design; MIMO communication; Stability analysis; Task analysis; Adaptive linearizing; dynamic surface control (DSC); direct compensation; fuzzy control; multiswitching; nonstrict feedback; time delays	DYNAMIC SURFACE CONTROL; NEURAL-NETWORK CONTROL; NONLINEAR-SYSTEMS; TRACKING CONTROL; APPROXIMATION	The multiswitching algorithm has demonstrated its ability in achieving the global tracking stability and preventing the control singularity for strict-feedback systems. However, its direct application to the nonstrict-feedback systems is prohibited due to the serious algebraic-loop problem. Via exploring the ability of the dynamic surface control for conquering such a difficulty, the direct-compensation approach ensured the semiglobally uniformly ultimately bounded stability for nonstrict-feedback systems with unity control coefficients so far. This article aims to integrate these two methods to tackle a more challenging task of controlling multiinput-multioutput nonstrict-feedback systems with unknown control-affine functions and time-varying delays. Comparing with the existing schemes in the literature, the proposed design exhibits the following distinct features: 1) the control-affine functions and the delayed functions can depend on the whole state vector; 2) the time delays can be incoherent among different states; and 3) the globally uniformly ultimately bounded stability is achieved without incurring the control singularity, explosion of complexity, and the algebraic-loop problems simultaneously.																	1063-6706	1941-0034				OCT	2020	28	10					2592	2604		10.1109/TFUZZ.2019.2945233													
J								Neural Network Based Adaptive SMO Design for T-S Fuzzy Descriptor Systems	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Neural networks; Symmetric matrices; Adaptive systems; Observers; Time-varying systems; Delays; Biological system modeling; Adaptive sliding-mode observer (SMO); restricted equivalent transformation (RET); radial basis function (RBF) neural network; Takagi-Sugeno (T-S) fuzzy descriptor systems; time-varying delay	SLIDING-MODE CONTROL; H-INFINITY CONTROL; OBSERVER DESIGN; SINGULAR SYSTEMS; RECONSTRUCTION; STABILIZATION; STATE; DELAY	This article is concerned with the problem of sliding-mode observer (SMO) design for Takagi-Sugeno (T-S) fuzzy descriptor systems with time-varying delay. First, based on the restricted equivalent transformation, a new restricted equivalent form (REF) of descriptor systems is proposed. Under the new REF, an integral-type sliding surface is constructed for the error system. Then, a sufficient condition is established in terms of linear matrix inequality, which guarantees the admissibility of the sliding-mode dynamics. Furthermore, an radial basis function (RBF) neural network based adaptive sliding-mode control (SMC) strategy is adopted such that the reachability condition can be ensured. By utilizing the RBF neural network to approximate the unknown nonlinearity, many restricted conditions, which are required in most existing results about SMC for T-S fuzzy systems, can be removed. Finally, simulation examples are presented to show the effectiveness of our results.																	1063-6706	1941-0034				OCT	2020	28	10					2605	2618		10.1109/TFUZZ.2019.2945238													
J								Adaptive Fuzzy Fault-Tolerant Control of Uncertain Euler-Lagrange Systems With Process Faults	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Uncertainty; Adaptive systems; Fault tolerance; Fault tolerant systems; Process control; Observers; Adaptive fuzzy control (AFC); Euler-Lagrange systems; fault-tolerant control (FTC); fuzzy systems; process faults	NONLINEAR-SYSTEMS; DESIGN; TRACKING; OBSERVER; SUBJECT; VEHICLE	This article is concerned with the fault-tolerant control (FTC) problem for Euler-Lagrange systems subject to process faults that cause changes in the system dynamics. The possibly time-varying and discontinuous changes bring out a challenge for fuzzy approximation. To conquer this obstacle, a novel adaptive fuzzy FTC strategy is proposed in this article. First, a constraint-handling scheme is designed to ensure zero overshoot for certain signals. In this way, the resulting unknown term, composed of the signs of sliding surfaces, the bounds of faults, and model uncertainties, is both continuous and within a fixed form. Subsequently, adaptive fuzzy systems are adopted to approximate this unknown term for compensation. It is proved that the tracking performance and the boundedness of signals in the closed-loop system are guaranteed by the developed method. The simulation results further illustrate the established theoretical findings.																	1063-6706	1941-0034				OCT	2020	28	10					2619	2630		10.1109/TFUZZ.2019.2945256													
J								Extended Dissipativity and Control Synthesis of Interval Type-2 Fuzzy Systems via Line-Integral Lyapunov Function	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Lyapunov methods; Linear matrix inequalities; Fuzzy systems; Fuzzy sets; Asymptotic stability; Stability criteria; Extended dissipativity; fuzzy system; interval type-2 (IT2) fuzzy set; line-integral Lyapunov function	STABILIZATION; FEEDBACK; DESIGN	This article addresses the problems of the stability and extended dissipativity analysis and control synthesis for interval type-2 fuzzy systems. A sufficient condition of asymptotic stability and extended dissipativity of the systems under consideration is established by using line-integral Lyapunov function. This condition obtained is more general than the one which is based on quadratic Lyapunov function. Calculating the control gains in light of the obtained condition is more complicated and challenging, since the matrix inequalities in the condition are of nonlinear form with respect to some matrix variables. Also, change of variable as well as the existing matrix decoupling approach cannot be used directly to handle the nonlinear problem. By utilizing cone complementarity linearization algorithm and a new matrix decoupling method, the state feedback controller can be developed by converting the nonlinear matrix inequalities into a quadratic optimization problem with linear inequality constraints. Four simulation examples are provided to show the effectiveness of the proposed approach.																	1063-6706	1941-0034				OCT	2020	28	10					2631	2644		10.1109/TFUZZ.2019.2945258													
J								On the Relationship Between the Crescent Method and SUOWA Operators	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Open wireless architecture; Games; Closed-form solutions; Additives; Presses; Capacity planning; Indexes; Choquet integral; ordered weighted averaging (OWA) operators; semiuninorm-based OWA (SUOWA) operators; the Crescent Method; weighted means	WEIGHTED MEANS; AGGREGATION	Different families of functions have been proposed in the literature with the purpose of simultaneously generalizing weighted means and ordered weighted averaging (OWA) operators [see, for instance, weighted OWA and semiuninorm-based OWA (SUOWA) operators]. Recently, Jin et al. have introduced in the literature a new procedure called the Crescent Method to melt additive capacities with those of OWA operators. The main aim of this article is to establish a relationship between the Crescent Method and SUOWA operators. From this relationship, we give closed-form expressions for some well-known indices, such as the Shapley values, the veto and favor indices, and the k-conjunctiveness and k-disjunctiveness indices.																	1063-6706	1941-0034				OCT	2020	28	10					2645	2650		10.1109/TFUZZ.2019.2934937													
J								Least Squares Estimation in Uncertain Differential Equations	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Differential equations; Mathematical model; Uncertainty; Parameter estimation; Stochastic processes; Measurement uncertainty; Estimation; Parameter estimation; uncertain differential equation; uncertainty theory	STABILITY; PARAMETERS; MODEL	Uncertain differential equations are a type of differential equations driven by Liu processes. How to estimate the parameters in an uncertain differential equation based on the observed data is a crucial problem in the real applications of these equations. By means of the least squares estimation, this article proposes a principle of minimum noise as an approach to the problem. Following this principle, the estimates of the parameters in some special types of uncertain differential equations are derived, which are represented as functions of the observed data. In addition, some numerical experiments are performed to illustrate the principle.																	1063-6706	1941-0034				OCT	2020	28	10					2651	2655		10.1109/TFUZZ.2019.2939984													
J								Event-Triggered Sliding-Mode Control for a Class of T-S Fuzzy Systems	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Fuzzy systems; Switches; Uncertainty; Symmetric matrices; Sliding mode control; Linear systems; Fans; Event-triggered sliding-mode control (ETSMC); linear switching function; Takagi-Sugeno (T-S) fuzzy system	FAULT-TOLERANT CONTROL; DESCRIPTOR SYSTEMS; QLPV MODELS; DESIGN; STABILITY; REPRESENTATION; FEASIBILITY	An event-triggered sliding-mode control problem is investigated for a class of multiple-input Takagi-Sugeno (T-S) fuzzy systems via designing a linear switching function. An assumption that all local linear systems share a common input matrix is removed. Then, a novel event-triggered sliding-mode controller with asynchronous premise variables is designed, which can ensure that the trajectory of multiple-input T-S fuzzy systems can be driven onto the region near the sliding surface after finite time. A sufficient condition is established to guarantee the stability of sliding motion, and the sliding-mode controller gain is obtained by solving a set of linear matrix inequalities. The positive lower bound of the interexecution time can be ensured, which means that there is no Zeno phenomenon. In the end, the advantages and effectiveness of the theoretical results are illustrated by two examples.																	1063-6706	1941-0034				OCT	2020	28	10					2656	2664		10.1109/TFUZZ.2019.2940867													
J								T2-ETS-IE: A Type-2 Evolutionary Takagi-Sugeno Fuzzy Inference System With the Information Entropy-Based Pruning Technique	IEEE TRANSACTIONS ON FUZZY SYSTEMS										Fuzzy systems; Mathematical model; Information entropy; Uncertainty; Data models; Takagi-Sugeno model; Evolutionary Takagi-Sugeno (ETS) fuzzy system; information entropy; learning-from-scratch; Type-2 fuzzy system	MODEL IDENTIFICATION; LOGIC SYSTEMS	We introduce a new nonlinear system identification technique, leveraging the benefits of the Type-2 Evolutionary Takagi-Sugeno (T2-ETS) fuzzy system. The major advantage of our proposed system identification technique is mainly due to its ability to learn-from-scratch while accommodating the footprint-of-uncertainties (FoUs). To support its mission to achieve a reasonably high prediction accuracy for uncertain nonlinear dynamic systems, we also introduce a new type reduction method to convert Type-2 fuzzy systems into their Type-1 counterparts. As a part of its efficient pruning strategy, the proposed system incorporates the concept of information entropy to avoid over fitting, which is a highly undesirable issue in modeling. We demonstrate the effectiveness of our system identification technique in achieving a delicate balance between minimizing the complexity of the acquired fuzzy model and maximizing the prediction accuracy. To highlight the efficacy of our algorithm, we employ a set of challenging pH neutralization data, known for its substantial nonlinearity, in addition to the dynamics of a nonlinear mechanical system. We conclude our research by conducting a rigorous comparative study to quantify the relative merits of our proposed technique with respect to the previous ETS algorithm (as its predecessor), the well-known KM-type reduction technique, and the higher-order discrete transfer functions, widely implemented in most conventional mathematical modeling techniques.																	1063-6706	1941-0034				OCT	2020	28	10					2665	2672		10.1109/TFUZZ.2019.2943813													
J								Robust Fuzzy Clustering Algorithms for Change-Point Regression Models	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										Change-point regression models; Change-point; Outliers; Robust estimation; RoFCP procedure	MAXIMUM-LIKELIHOOD ESTIMATOR; SERIES	This article presents a robust fuzzy procedure for estimating change-point regression models. We propose incorporating the fuzzy change-point algorithm with the M-estimation technique for robust estimations. The fuzzy c partitions concept is embedded into the change-point regression model so the fuzzy c-regressions and fuzzy c-means clustering can be employed to obtain the estimates of change-points and regression parameters. The M estimation with a robust criterion is used to make the estimators robust to the presence of outliers and heavy-tailed distributions. We create two robust algorithms named FCH and FCT by using Huber's and Tukey's functions as the robust criterion respectively. Extensive experiments with numerical and real examples are provided for demonstrating the effectiveness and the superiority of the proposed algorithms. The experimental results show the proposed algorithms are resistant to atypical observations and outperform the existing methods. The proposed FCH and FCT are generally comparable but FCT performs better in the presence of extremely high leverage outliers and heavy-tailed distributions. Real data applications show the practical usefulness of the proposed method.																	0218-4885	1793-6411				OCT	2020	28	5					701	725		10.1142/S0218488520500300													
J								A General Cipher for Individual Data Anonymization	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										Privacy-preserving data publishing; statistical disclosure control; permutation paradigm; permutation matrices; power means; cipher	PERMUTATION PARADIGM; DISCLOSURE RISK	Over the years, the literature on individual data anonymization has burgeoned in many directions. While such diversity should be praised, it does not come without some difficulties. Currently, the task of selecting the optimal analytical environment is complicated by the multitude of available choices and the fact that the performance of any method is generally dependent of the data properties. In light of these issues, the contribution of this paper is twofold. First, based on recent insights from the literature and inspired by cryptography, it proposes a new anonymization method that shows that the task of anonymization can ultimately rely only on ranks permutations. As a result, the method offers a new way to practice data anonymization by performing it ex-ante and independently of the distributional features of the data instead of being engaged, as it is currently the case in the literature, in several ex-post evaluations and iterations to reach the protection and information properties sought after. Second, the method establishes a conceptual connection across the field, as it can mimic all the currently existing tools. To make the method operational, this paper proposes also the introduction of permutation menus in data anonymization, where recently developed universal measures of disclosure risk and information loss are used ex-ante for the calibration of permutation keys. To justify the relevance of their uses, a theoretical characterization of these measures is also proposed.																	0218-4885	1793-6411				OCT	2020	28	5					727	756		10.1142/S0218488520500312													
J								Uncertainty-Aware Dissimilarity Measures for Interval-Valued Fuzzy Sets	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										Dissimilarity; interval-valued sets; order on interval-valued fuzzy sets	SIMILARITY MEASURE; DISTANCE; ENTROPY	Dissimilarities are a very usual way to compare two fuzzy sets and also two interval-valued fuzzy sets. In both cases, the dissimilarity between two sets is a number. In this work, we introduce a generalization of the notion of dissimilarity for interval-valued fuzzy sets such that it assumes values on the set of subintervals instead of the set of numbers. This seems to be more realistic taking into account the available information. We also investigate its relationship with the classical notions of dissimilarity between fuzzy sets and we obtain that the new class is richer than the existing one.																	0218-4885	1793-6411				OCT	2020	28	5					757	768		10.1142/S0218488520500324													
J								Cross-Validation for the Uncertain Chapman-Richards Growth Model with Imprecise Observations	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										Regression analysis; cross-validation; Chapman-Richards growth model; uncertainty theory; imprecise observations	LINEAR-REGRESSION ANALYSIS; FUZZY; SELECTION	Regression analysis estimates the relationships among variables which has been widely used in growth curves, and cross-validation as a model selection method assesses the generalization ability of regression models. Classical methods assume that the observation values of variables are precise numbers while in many cases data are imprecisely collected. So this paper explores the Chapman-Richards growth model which is one of the widely used growth models with imprecise observations under the framework of uncertainty theory. The least squares estimates of unknown parameters in this model are given. Moreover, cross-validation with imprecise observations is proposed. Furthermore, estimates of the expected value and variance of the uncertain error using residuals are given. In addition, ways to predict the value of response variable with new observed values of predictor variables are discussed. Finally, a numerical example illustrates our approach.																	0218-4885	1793-6411				OCT	2020	28	5					769	783		10.1142/S0218488520500336													
J								MPE Computation in Bayesian Networks Using Mini-Bucket and Probability Trees Approximation	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										Bayesian networks; most probable explanation; deletion algorithm; mini-bucket; probability trees	PENNILESS PROPAGATION; GENERAL SCHEME; FRAMEWORK	Given a set of uncertain discrete variables with a joint probability distribution and a set of observations for some of them, the most probable explanation is a set or configuration of values for non-observed variables maximizing the conditional probability of these variables given the observations. This is a hard problem which can be solved by a deletion algorithm with max marginalization, having a complexity similar to the one of computing conditional probabilities. When this approach is unfeasible, an alternative is to carry out an approximate deletion algorithm, which can be used to guide the search of the most probable explanation, by using A* or branch and bound (the approximate+search approach). The most common approximation procedure has been the mini-bucket approach. In this paper it is shown that the use of probability trees as representation of potentials with a pruning of branches with similar values can improve the performance of this procedure. This is corroborated with an experimental study in which computation times are compared using randomly generated and benchmark Bayesian networks from UAI competitions.																	0218-4885	1793-6411				OCT	2020	28	5					785	805		10.1142/S0218488520500348													
J								On Generating of t-Norms and t-Conorms on Bounded Lattices	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										Bounded lattice; T-norm; T-conorm; Ordinal sum	TRIANGULAR NORMS; ORDINAL SUMS; UNINORMS	In this paper, we study t-norms and t-conorms on bounded lattices. We propose new methods for generating these operators, applicable on any bounded lattice M by use of the presence of a t-norm on [0(M), k] and a t-conorm on [k, 1(M)] for an element k is an element of M/{0(M), 1(M)}. In addition, some corresponding examples are provided for well understanding the structure of new t-norms and t-conorms.																	0218-4885	1793-6411				OCT	2020	28	5					807	835		10.1142/S021848852050035X													
J								Modal Interval Probability: Application to Bonus-Malus Systems	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										Modal intervals; interval probability; Markov chain; Bonus-Malus		Classical intervals have been a very useful tool to analyze uncertain and imprecise models, in spite of operative and interpretative shortcomings. The recent introduction of modal intervals helps to overcome those limitations. In this paper, we apply modal intervals to the field of probability, including properties and axioms that form a theoretical framework applied to the Markovian analysis of Bonus-Malus systems in car insurance. We assume that the number of claims is a Poisson distribution and in order to include uncertainty in the model, the claim frequency is defined as a modal interval; therefore, the transition probabilities are modal interval probabilities. Finally, the model is exemplified through application to two different types of Bonus-Malus systems, and the attainment of uncertain long-run premiums expressed as modal intervals.																	0218-4885	1793-6411				OCT	2020	28	5					837	851		10.1142/S0218488520500361													
J								Distance-Based Consistency Measure and Priority Weights of Best-Worst Multi-Criteria Decision Making Method	INTERNATIONAL JOURNAL OF UNCERTAINTY FUZZINESS AND KNOWLEDGE-BASED SYSTEMS										Multi-criteria decision making (MCDM); best-worst method (BWM); consistency measure; priority weights	SUPPLIER SELECTION; SCALING METHOD; QUALITY; BWM	Best-Worst method (BWM) is a new multi-criteria decision-making method based on pairwise comparisons, but only the comparisons concerning the best and the worst alternatives or criteria. This method shows some significant advantages in the simplicity with a less requirement of comparison data and reliability with better consistency. This paper proposes a new consistency measure method based on the distance of the vectors of reference comparisons in BWM because the difference of the preference in two vectors directly affects the reliability of results. Through the establishment of the threshold of consistency ratio, we supplement the definition of satisfactory consistency of the comparisons in BWM. With comparisons satisfying the acceptable consistency, we use linear programming models to find all possible priority weights between the preferences given by decision maker and derive interval weights. For comparisons with unacceptable consistency, another approach is presented to find the interval weights meeting the consistent requirement. At last, several examples are used to illustrate the details of process.																	0218-4885	1793-6411				OCT	2020	28	5					853	878		10.1142/S0218488520500373													
J								Improved recognition of bacterial species using novel fractional-order orthogonal descriptors	APPLIED SOFT COMPUTING										Bacterial species; Recognition; Salp swarm algorithm; Teaching-based global optimization; Fractional-order orthogonal moments	COLOR IMAGES REPRESENTATION; AUTOMATIC IDENTIFICATION; ZERNIKE MOMENTS; OPTIMIZATION; CLASSIFICATION; INVARIANTS; SYSTEM; SET	Detection and distinguishing between different species of bacteria using experimental microbiology is an expensive, time-consuming, and risky process. Automatic computer-based methods for accurate detection and classification of bacteria species significantly reduce the cost, time, and avoiding scientists the risk of infection. This paper presents a novel computer-based approach for highly accurate recognition of bacterial species. The proposed method consists of two main stages. First, a novel set of fractional-order orthogonal moments proposed to extract the fine features from the color images of bacteria. Second, a new method for feature selection, SSATLBO, is proposed. In this method, the teaching-based learning optimization (TLBO) as local operators is used to improve the exploitation ability of the Salp Swarm Algorithm (SSA) to avoid the local point. The proposed detection and classification method tested by using the DIBaS dataset (Digital Image of Bacterial Species), which includes 660 images with 33 various genera and classes of bacteria. The proposed method achieved a bacterial species recognition rate, 98.68%. The obtained results ensure the superiority of the proposed method over the traditional SSA and TLBO methods and the other Metaheuristic methods. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106504	10.1016/j.asoc.2020.106504													
J								HSMA_WOA: A hybrid novel Slime mould algorithm with whale optimization algorithm for tackling the image segmentation problem of chest X-ray images	APPLIED SOFT COMPUTING										Image segmentation problem; Slime mould algorithm (SMA); Whale optimization algorithm; Kapur's entropy; X-ray images; COVID-19	MULTILEVEL; ENTROPY	Recently, a novel virus called COVID-19 has pervasive worldwide, starting from China and moving to all the world to eliminate a lot of persons. Many attempts have been experimented to identify the infection with COVID-19. The X-ray images were one of the attempts to detect the influence of COVID-19 on the infected persons from involving those experiments. According to the X-ray analysis, bilateral pulmonary parenchymal ground-glass and consolidative pulmonary opacities can be caused by COVID-19 - sometimes with a rounded morphology and a peripheral lung distribution. But unfortunately, the specification or if the person infected with COVID-19 or not is so hard under the X-ray images. X-ray images could be classified using the machine learning techniques to specify if the person infected severely, mild, or not infected. To improve the classification accuracy of the machine learning, the region of interest within the image that contains the features of COVID-19 must be extracted. This problem is called the image segmentation problem (ISP). Many techniques have been proposed to overcome ISP. The most commonly used technique due to its simplicity, speed, and accuracy are threshold-based segmentation. This paper proposes a new hybrid approach based on the thresholding technique to overcome ISP for COVID-19 chest X-ray images by integrating a novel meta-heuristic algorithm known as a slime mold algorithm (SMA) with the whale optimization algorithm to maximize the Kapur's entropy. The performance of integrated SMA has been evaluated on 12 chest X-ray images with threshold levels up to 30 and compared with five algorithms: Lshade algorithm, whale optimization algorithm (WOA), FireFly algorithm (FFA), Harris-hawks algorithm (HHA), salp swarm algorithms (SSA), and the standard SMA. The experimental results demonstrate that the proposed algorithm outperforms SMA under Kapur's entropy for all the metrics used and the standard SMA could perform better than the other algorithms in the comparison under all the metrics. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106642	10.1016/j.asoc.2020.106642													
J								Prioritization based Taxonomy of Cloud-based Outsource Software Development Challenges: Fuzzy AHP analysis	APPLIED SOFT COMPUTING										Cloud-based outsource software development (COSD); Fuzzy analytical hierarchy process (FAHP); Challenges	SYSTEMATIC LITERATURE; PROCESS IMPROVEMENT; SUCCESS FACTORS; FRAMEWORK; DRIVEN; MODEL	Cloud-Based Outsource Software Development (COSD) is a new methodology adopted by organizations to develop software using teams of knowledge workers located across the globe using cloud computing services. However, there is a lack of understanding of challenges associated with successful execution of COSD projects. The objective of this study is to identify and prioritize the challenges that influence COSD projects. First, we conducted a Systematic Literature Review (SLR) and identified 21 challenges that impact COSD projects. Next, a questionnaire survey was developed based on the SLR findings to collect feedback from industry practitioners. Finally, we applied the Fuzzy Analytical Hierarchy Process (FAHP) to rank the identified challenges for COSD projects. We also present a prioritization-based taxonomy of the identified challenges which will help practitioners to focus on the critical areas for successful implementation of COSD projects. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106557	10.1016/j.asoc.2020.106557													
J								Learning to rank developers for bug report assignment	APPLIED SOFT COMPUTING										Bug report; Bug assignment; Learning to rank; Software quality; Mining software repositories	LOCALIZATION	Bug assignment is a burden for projects receiving many bug reports. To automate the process of assigning bug reports to the appropriate developers, several studies have relied on combining natural language processing and information retrieval techniques to extract two categories of features. One of these categories targets developers who have fixed similar bugs before, and the other determines developers working on source files similar to the description of the bug. Commit messages represent another rich source for profiling developer expertise as the language used in commit messages is closer to that used in bug reports. In this work, we propose a more enhanced profiling of developers through their commits, which are captured in a new set of features that we combine with features used in previous studies. More precisely, we propose an adaptive ranking approach that takes as input a given bug report and ranks the top developers who are most suitable to fix it. This approach learns from the history of previously fixed bugs to profile developers in terms of their expertise. With respect to a given bug report, the ranking score of each developer is computed as a weighted combination of an array of features encoding domain knowledge, where the weights are trained automatically on previously solved bug reports using a learning-to-rank technique. Our model was evaluated using around 22,000 bug reports, exported from four large scale open-source Java projects. Results show that our model significantly outperformed two recent state-of-the-art methods in recommending the suitable developer to handle a certain bug report. Specifically, the percentage of recommending a developer within the top 5 ranked developers correctly was over 80% for both the Eclipse UI Platform and Birt projects. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106667	10.1016/j.asoc.2020.106667													
J								Hyper-heuristics using multi-armed bandit models for multi-objective optimization	APPLIED SOFT COMPUTING										Multi-objective optimization; Hyper-heuristics; Multi-armed bandit	LOCAL SEARCH ALGORITHM; MEMETIC ALGORITHMS; GENETIC ALGORITHM; CLASSIFICATION; DECOMPOSITION; TARDINESS	In this work, we explore different multi-armed bandit-based hyper-heuristics applied to the multi-objective permutation flow shop problem. It is a scheduling problem which has been extensively studied due to its relevance for industrial engineering. Three multi-armed bandit basic formulations are used in the hyper-heuristic selection mechanism: (i) classic, (ii) restless, and (iii) contextual. All the three approaches are considered online selection perturbative hyper-heuristics as they are designed to choose the low level heuristic (crossover and mutation operators) that should be applied to modify each solution during the search process. Performances of the proposed approaches combined with MOEA/DD (Multi-objective Evolutionary Algorithm based on Dominance and Decomposition) are evaluated using the hypervolume indicator and nonparametric statistical tests on a set of 220 problem instances with two and three objectives. The best proposed approach (contextual with a linear realizability assumption) is compared with a stand-alone version of MOEA/DD using the best considered crossover and mutation operator. It is also compared with three state-of-the-art multi-objective algorithms. Results show that this best approach is able to outperform MOEA/DD in scenarios with two and three objectives and by encompassing both, Pareto and decomposition strategies, is competitive with the state of the art in those scenarios. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106520	10.1016/j.asoc.2020.106520													
J								Application of hybrid multi-objective moth flame optimization technique for optimal performance of hybrid micro-grid system	APPLIED SOFT COMPUTING										HMGS; HMOMFO algorithm; Multi-objective; PEE; LPSP	RENEWABLE ENERGY SYSTEM; POWER-SYSTEM; RURAL ELECTRIFICATION; LEVY FLIGHT; ALGORITHM; STORAGE	The research work carried out here deals with the application of a Hybrid Micro-Grid System (HMGS), which includes solar/wind/battery storage/diesel generator applied to three different parts of India. For a better analysis of all the three cases, an efficient and recent metaheuristic optimization method named hybrid multi-objective moth flame optimization (HMOMFO) technique has been used in MATLAB. The aim is to find better candidate solutions for which particle swarm optimization (PSO) technique and levy flight method are integrated, with the moth flame optimization (MFO) algorithm Moreover a new enhanced differential evolution algorithm (EDE) with self-adjustable parameters has been integrated with the second phase of the hybrid algorithm to enhance the searching and exploitation capabilities of the proposed algorithm. Here, for an initial load of 15 households, simulation and statistical results show that HMOMFO proves to be successful in terms of minimizing the price of electrical energy (PEE). Results further assure that minimum values of loss of power supply probability (LPSP) for Durgapur, Hospet, and Tirunelveli, are obtained using HMOMFO, with fewer iterations. The results also contain optimum photovoltaic (PV) power, battery bank performance in terms of autonomy days (AD), the optimum number of wind turbine generators (WT), and diesel generators (DG). The results demonstrate the mastery and effectiveness of the proposed HMOMFO against three area hybrid micro-grid systems. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106487	10.1016/j.asoc.2020.106487													
J								Colonoscopy contrast-enhanced by intuitionistic fuzzy soft sets for polyp cancer localization	APPLIED SOFT COMPUTING										Visual enhancement; Colonoscopy image; Polyp cancer; Fuzzy set; Intuitionistic fuzzy set; Fuzzy soft set; Fuzzy entropy	IMAGE QUALITY ASSESSMENT; HISTOGRAM EQUALIZATION	Medical images often suffer from low contrast, irregular gray-level spacing and contain a lot of uncertainties due to constraints of imaging devices and environment (various lighting conditions) when capturing images. In order to achieve any clinical-diagnosis method for medical imaging with better comprehensibility, image contrast enhancement algorithms would be appropriate to improve the visual quality of medical images. In this paper, an automated image enhancement method is presented for colonoscopy images based on the intuitionistic fuzzy soft set. The fuzzy soft set is used to model the intuitionistic fuzzy soft image matrix based on a set of soft features of the colonoscopy images. The technique decomposes the fuzzy image into multiple blocks and estimates a soft-score based on an adaptive soft parametric hesitancy map by using the hesitant entropy for each block to quantify the uncertainties. In the processing stage, an adaptive intensity modification process is done for each block according to its soft-score. These scores are accurately addressed the gray-level ambiguities in colonoscopy images that lead to better results. Finally, the enhanced image achieved by performing a defuzzification together with all unprocessed blocks. Qualitative and quantitative assessments demonstrate that the proposed method improves image contrast and region-of-interest of polyps in colonogram. Experimental results on enhancing a large CVC-Clinic-DB and ASU-Mayo clinic colonoscopy benchmark datasets show that the proposed method outperforms the state-of-the-art medical image enhancement methods. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106492	10.1016/j.asoc.2020.106492													
J								Adolescent Identity Search Algorithm (AISA): A novel metaheuristic approach for solving optimization problems	APPLIED SOFT COMPUTING										Optimization; Metaheuristics; IIR system identification; Inverse kinematics problem; AISA	NUMERICAL FUNCTION OPTIMIZATION; SYSTEM-IDENTIFICATION; GLOBAL OPTIMIZATION; SWARM; DESIGN; EVOLUTIONARY; COLONY; MODEL; LINKS	This paper proposes a novel population-based metaheuristic optimization algorithm, called Adolescent Identity Search Algorithm (AISA), which is inspired by the process of identity development/search of adolescents. AISA simulates the identity formation behavior of adolescents in the peer group. This behavior is modeled mathematically to solve optimization problems. The proposed algorithm is evaluated on thirty-nine well-known unimodal, multimodal, fixed-dimensional multimodal, composite and CEC 2019 benchmark functions to test exploration, exploitation, local optima avoidance, and convergence properties. The results are verified by an extensive comparative study with thirteen state-of-art metaheuristic algorithms. Furthermore, AISA has been used to solve IIR system identification and inverse kinematics problem of a seven Degrees Of Freedom (7-DOF) robot manipulator considered as the real-life engineering applications. The overall optimization results demonstrate that AISA possesses a strong and robust capability to produce superior performance over other competitor metaheuristic algorithms in solving various complex numerical optimization problems. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106503	10.1016/j.asoc.2020.106503													
J								Efficient well placement optimization coupling hybrid objective function with particle swarm optimization algorithm	APPLIED SOFT COMPUTING										Well placement optimization; Hybrid objective function; Particle swarm optimization; Computational time reduction	JOINT OPTIMIZATION; SEARCH ALGORITHM	Well placement optimization is a critical part of the oil field development planning which aims to find the optimal locations of wells to maximize a traditional objective function (TOF), e.g., cumulative oil production (COP) or the net present value (NPV). However, the optimization process can be quite time-consuming since it requires iterative evaluations of the objective function and each evaluation requires one simulation run of the fluid flow in the discretized time domain and space domain. This paper examined the consistency between productivity potential value (PPV) and cumulative oil production (COP), and proposed to use PPV as the objective function, whose evaluation does not require simulation run, to improve computational efficiency. However, since PPV is a static measure of a reservoir, we use PPV only in early iterations of the well placement optimization followed by TOF in later iterations in order to capture reservoir dynamics. The use of PPV and TOF in a sequential manner is referred to as a hybrid objective function (HOF). In this work, a naturally parallelizable optimization algorithm, particle swarm optimization (PSO), where simulation runs can be conducted in batches is used as the optimizer. The effectiveness of the proposed procedure is validated based on three numerical examples including a 2D model, the PUNQ-S3 model and the Egg model. Results demonstrate the well placement optimization strategy using HOF finds comparable COP within much less simulation runs compared to the optimization using TOF. In summary, well placement optimization with the objective function defined as PPV in the first 25% iterations and TOF in the following 75% iterations is the best combination. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106511	10.1016/j.asoc.2020.106511													
J								Finding Longest Common Subsequences: New anytime A* search results	APPLIED SOFT COMPUTING										Longest common subsequence problem; Hybrid metaheuristic; A* search; Beam search; Anytime column search	SIMILARITY MEASURES; BEAM SEARCH; ALGORITHM; SEQUENCES	The Longest Common Subsequence (LCS) problem aims at finding a longest string that is a subsequence of each string from a given set of input strings. This problem has applications, in particular, in the context of bioinformatics, where strings represent DNA or protein sequences. Existing approaches include numerous heuristics, but only a few exact approaches, limited to rather small problem instances. Adopting various aspects from leading heuristics for the LCS, we first propose an exact A* search approach, which performs well in comparison to earlier exact approaches in the context of small instances. On the basis of A* search we then develop two hybrid A*-based algorithms in which classical A* iterations are alternated with beam search and anytime column search, respectively. A key feature to guide the heuristic search in these approaches is the usage of an approximate expected length calculation for the LCS of uniform random strings. Even for large problem instances these anytime A* variants yield reasonable solutions early during the search and improve on them over time. Moreover, they terminate with proven optimality if enough time and memory is given. Furthermore, they yield upper bounds and, thus, quality guarantees when terminated early. We comprehensively evaluate the proposed methods using most of the available benchmark sets from the literature and compare to the current state-of-the-art methods. In particular, our algorithms are able to obtain new best results for 82 out of 117 instance groups. Moreover, in most cases they also provide significantly smaller optimality gaps than other anytime algorithms. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106499	10.1016/j.asoc.2020.106499													
J								Bi-directional Long Short-Term Memory model to analyze psychological effects on gamers	APPLIED SOFT COMPUTING										Emotion; Games; Deep learning; Long Short-Term Memory model; Electroencephalography	COMMON SPATIAL-PATTERNS; VIDEO GAME; BEHAVIOR; REAL; DESENSITIZATION; CLASSIFICATION; LSTM	With the increasing popularity of android gaming applications on smart phones, detection of emotional states of hard-core gamers become the interest of study among psychologists. Although there exist a few interesting research works on the impact of video games over the child and adult group, most of them are only able to throw light on psychological aspects associated with the said cognitive task. The real-time detection of emotional states of the player while playing video game is still an unexplored area of research. The present work feels the void by proposing a novel scheme of detecting the emotional changes of human subject from their electroencephalographic (EEG) signal acquired during their engagement in playing video games. The problem is formulated in the settings of pattern classification, which involves four main steps: Data collection, pre-processing and artifact removal, feature extraction and classification. The novelty of the work lies in extracting the emotional content with a high recognition rate from the acquired EEG response using a deep learning algorithm. The primary contribution of the paper lies in efficient usage of a novel phase-sensitive Common Spatial Pattern algorithm for feature extraction and design of an attention-based Bi-directional Long Short-Term Memory (Bi-LSTM) network for classifying the emotional states of a video-game player into five classes: happiness, sadness, surprise, anger and neutral. Moreover, the scarcity of labeled data in EEG-based brain-computer interfacing (BCI) tasks is a serious issue while understanding the performance capabilities of the data-driven deep-learning models. Therefore, the present work also makes an attempt to handle the scarcity in the dimension of the extracted feature using a novel feature augmentation algorithm before feeding the feature-vector to the proposed Bi-LSTM network. Experiments undertaken yield productive and conclusive results that validate the efficacy of the proposed framework with the accuracy rate of 88.71%. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106573	10.1016/j.asoc.2020.106573													
J								Optimization of varying-parameter drilling for multi-hole parts using metaheuristic algorithm coupled with self-adaptive penalty method	APPLIED SOFT COMPUTING										Nonlinear constraint; PSO; Drilling; Drill wear	NI-BASED SUPERALLOY; SURFACE INTEGRITY; OBJECTIVE OPTIMIZATION; NEURAL-NETWORK; TOOL LIFE; WEAR; COMPOSITE; ROUGHNESS; FREQUENCY; SIGNALS	Multi-hole parts made of difficult-to-cut materials like discs, blisks and casings are common and require high surface quality in aero engines. In workshops, a large number of holes in these parts are drilled successively in one process to ensure their positional accuracy. Due to the fast time-varying drill wear, the surface roughness of the holes is unstable and difficult to be satisfied. To this end, this paper presents a varying-parameter drilling (VPD) method to improve machining efficiency and hole surface roughness for multi-hole parts made of Ni-based superalloy. This method uses varying cutting parameters for each hole to adapt to the varying drill wear. The main issue of this method lies in an optimization problem in which the optimal sequence of cutting parameters need to be found, with the objective of the processing time and the constraint of the hole surface roughness. As the cutting parameter sequence has a high dimension and the surface roughness of all the holes must be guaranteed, the challenge of this optimization problem is the strict constraint with a complicated non-linear boundary of the feasible zone. To address the convergence difficulty of the searching algorithm, a soft computing method based on particle swarm optimization (PSO) algorithm with a self-adaptive penalty method (SAPM) is applied. The hole surface roughness is predicted with a radial basis function (RBF) neural network. Different types of drill wear comprising flank wear, crater wear, chisel wear and outer corner wear are considered, and the grey relational analysis (GRA) is employed to select the input drill wear parameters to the network. The PSO algorithm coupled with the SAPM is used to search the global optimal solution of the optimization problem. It is found that the satisfied solutions can be searched in all the three trials with the proposed algorithm, even though the proportion of feasible solutions is severely fluctuant during the searching process. The drilling experiment confirm that, when compared with the fixed-parameter drilling, the proposed VPD and the soft computing method for solving the optimization problem can effectively improve machining efficiency and surface quality for drilling Ni-based superalloy multi-hole parts. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106489	10.1016/j.asoc.2020.106489													
J								A fuzzy additive regression model with exact predictors and fuzzy responses	APPLIED SOFT COMPUTING										Goodness-of-fit measure; Fuzzy nonparametric; Kernel function; Fuzzy response; Non-fuzzy predictor; Cross-validation	POLYNOMIAL REGRESSION; LEAST-SQUARES; ASYMPTOTIC PROPERTIES; ALGORITHM; OPTIMIZATION; MACHINE	Fuzzy regression analysis is aimed at modeling the relationship between a set of fuzzy responses and a set of non-fuzzy/fuzzy predictors. However, compared to parametric methods, nonparametric regression often provides a very flexible approach to exploring the relationship between a response and the associated predictors without specifying a parametric model. In this paper, a novel fuzzy additive regression model with non-fuzzy predictors and fuzzy responses was proposed. For this purpose, a back-fitting stepwise regression approach with kernel smoothing was introduced to estimate a fuzzy smooth function corresponding to each predictor. An extended cross-validation criterion was also utilized to evaluate the unknown bandwidths. Some common goodness-of-fit criteria were employed to evaluate the performance of the proposed method. Effectiveness of the developed method was demonstrated through four numerical examples including two simulation studies based on three common kernels. The proposed method was further compared with several conventional fuzzy linear/nonlinear regression models, clearly indicating superior accuracy of the proposed model over other methods. Thus, it can be successfully applied to improve the prediction accuracy and interpretability of the fuzzy regression models for real-life applications in the context of intelligence systems. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106507	10.1016/j.asoc.2020.106507													
J								Decision-making in cognitive paradoxes with contextuality and quantum formalism	APPLIED SOFT COMPUTING										Cognition; Contextuality; Kolmogorov probability theories; Paradox and Quantum probability theories	CONJUNCTION FALLACY; PROBABILITY JUDGMENT; MODEL; COMBINATIONS; SPACE	Contextuality in human cognition can be defined as the existence of thoughts that are locally consistent and globally inconsistent. Absence of contextuality in the representational theories that are used for modelling cognitive processes results in inconsistencies between expected and actual decisions. These inconsistencies are usually regarded as paradoxes. However, recent literature reports that the quantum formalism is successful in explaining these paradoxes. In this article, we show the remarkable connection between contextuality, cognitive paradoxes and quantum formalism with a focus on decision-making. In this regard, we first analyse the paradoxes of human cognition from contextuality perspective as they share the pattern of local consistency and global inconsistency. This analysis is carried out using the proposed two-step contextuality analysis framework using the theories such as contextuality-by-Default and State Context Property formalism. Subsequently, this analysis provides an understanding on the dynamics of thoughts that has led to the paradoxical decisions which varies from the theoretical decisions. On the other hand, the success of quantum approach to cognitive paradoxes is due to their support for contextuality which is facilitated by the natural quantum characteristics. In this regard, we also narrate on how natural quantum characteristics facilitate contextuality with an example on decision-making. As a result, this article brings out the importance of contextuality in cognitive paradoxes and quantum formalism. Inferences obtained from this analysis emphasize on how and why quantum formalism is suitable for cognitive paradoxes. Our contribution stands out from the literature due to its comprehensive nature. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106521	10.1016/j.asoc.2020.106521													
J								Cluster validity index for irregular clustering results	APPLIED SOFT COMPUTING										Cluster validity index; Hard clustering; Irregular clustering results; Backbone	COMPARING FUZZY; VALIDATION; FIND	Different clustering algorithms with different parameter settings can produce various partitions on the input data. Without the priori knowledge, it is difficult for users to select the proper clustering algorithm and the parameters for the specific data in advance. Therefore, the cluster validity index (CVI) is crucial to help select the best partition that fits the underlying structure of the data. However, most existing CVIs (including some recent ones that designed for complex partitions) have strong assumptions. They only work well for partitions where clusters are spherically distributed, with similar sizes and densities, and with large separation distances. In complicated situations where irregular clustering results (i.e., clustering results having clusters in arbitrary shapes, different sizes and densities, and with small separation distances) exist, they usually fail to find the best fitting partition. Focusing on the insufficiencies of the existing CVIs (designed for hard clustering), this paper presents a new index that helps to find the best partition produced by hard clustering algorithms when irregular clustering results exist. The proposed index uses the density changes inside a cluster, and the density changes from the inner to the inter-cluster regions of the cluster to evaluate its quality. Experiments are implemented on both real and synthetic datasets. 11 other CVIs including some well known as well as recently proposed ones are also tested for comparison. Experimental results are given to demonstrate the effectiveness of the new index. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106583	10.1016/j.asoc.2020.106583													
J								Multi-point shortest path planning based on an Improved Discrete Bat Algorithm	APPLIED SOFT COMPUTING										Multi-point path planning; Bat algorithm; Floyd-Warshall algorithm; Incomplete connected graph	TRAVELING SALESMAN PROBLEM; COLONY; TSP	Multi-point shortest path planning problem is a typical problems in discrete optimization. The bat algorithm is a nature-inspired metaheuristic optimization algorithm that is used in a wide range of fields. However, there is one problem with the BA, which is easy to premature. To solve multi-point shortest path planning problem, an improved discrete bat algorithm (IDBA) is proposed in this paper. In this algorithm, the Floyd-Warshall algorithm is first used to transform an incomplete connected graph into a complete graph whose vertex set consists of a start point and necessary points. Then the algorithm simulates the bats' foraging and obstacle avoidance process to find the shortest path in the complete graph to satisfy the constraints. Finally, the path is transferred to the original incomplete graph to get the solution. In order to overcome the premature phenomenon of a discrete bat algorithm, the modified neighborhood operator is proposed. To prove the effectiveness of our method, we compared its performance in 26 instances with the results obtained by three different algorithms: DBA, IBA and GSA-ACS-PSOT. We also performed a sensitivity analysis on the parameters. The results indicate that the improved bat algorithm outperforms all the other alternatives in most cases. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106498	10.1016/j.asoc.2020.106498													
J								Comprehensive system based on a DNN and LSTM for predicting sinter composition	APPLIED SOFT COMPUTING										Chemical composition prediction; Deep neural network; Long short-term memory network; Data preprocessing; Sintering production	GRANULE SIZE DISTRIBUTION; NEURAL-NETWORKS; FAULT-DIAGNOSIS; MODEL; OPTIMIZATION; SIMULATION; REDUCTION; QUALITY	Because of the lag in sinter composition detection, fluctuations in production conditions are not conducive to making timely adjustments to sintering. In this paper, the characteristics of sintering production data are studied, and three core conclusions are drawn. We find that these data have (1) noise, (2) high dimensionality, and (3) time correlation. Based on these findings, an integrated model based on a deep neural network (DNN) and a long short-term memory (LSTM) network is proposed; using a DNN and an LSTM network solves the problem of developing a system model according to the given input and output data to predict the chemical composition of sinter. Specifically, first, we use a box plot and an isolated forest (iForest) algorithm to detect and filter noise in the data preparation stage and then propose using key feature selection and the Pearson correlation coefficient to reduce the high dimensionality of the data. Then, both an online component monitoring model based on a DNN and an advance component prediction model based on an LSTM are proposed to help the field operators to control the change of the sinter composition in real time. The results of many experiments show that the proposed method performs better than the current methods: the goodness of fit (R-2) score of the better model is above 0.92, and both the mean square error (MSE) and mean absolute error (MAE) are approximately zero. The deep neural network-based method proposed in this paper is more suitable for the online monitoring and advance prediction of sinter components. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106574	10.1016/j.asoc.2020.106574													
J								Parallel and distributed architecture of genetic algorithm on Apache Hadoop and Spark	APPLIED SOFT COMPUTING										Genetic algorithm; Parallel and distributed computing; Traveling salesman problems; Apache Hadoop; Apache Spark	2-MACHINE FLOWSHOP; OPTIMIZATION; SUBJECT; MODELS	The genetic algorithm (GA), one of the best-known metaheuristic algorithms, has been extensively utilized in various fields of management science, operational research, and industrial engineering. The efficiency of GAs in solving large-scale optimization problems would be enhanced if the iterative processes required by the genetic operators can be implemented in a parallel and distributed computing architecture. Apache Hadoop has recently been one of the most popular systems for distributed storage and parallel processing of big data. By integrating the GA highly into Apache Hadoop, this study proposes an advanced GA parallel and distributed computing architecture that achieves the effectiveness and efficiency of GA evolution. Characterized by the sophisticated mechanism of dispatching the GA core operators into Apache Hadoop, the developed computing framework fits well with the cloud computing model. The presented GA parallelization architecture outperforms the state-of-the-art reference architectures according to the computational experiments where the testing instances of traveling salesman problems are employed. Our numerical experiments also demonstrate that the proposed architecture can readily be extended to Apache Spark. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106497	10.1016/j.asoc.2020.106497													
J								Hybrid attribute conditional adversarial denoising autoencoder for zero-shot classification of mechanical intelligent fault diagnosis	APPLIED SOFT COMPUTING										Fault diagnosis; Autoencoder; Generative adversarial network; Zero-shot classification; Shipborne antenna	NEURAL-NETWORKS	Data-based intelligent fault diagnosis method is a research hotspot in modern mechanical systems. However, due to practical limitations, fault samples under all working conditions cannot be obtained, which would cause the data-based model lack of particular training data, resulting in unsatisfied testing performance. Therefore, zero-shot classification of mechanical intelligent fault diagnosis is a very practical work. Inspired by the zero-shot learning method, hybrid attribute conditional adversarial denoising autoencoder (CADAE), which uses hybrid attribute as condition, is proposed to solve the zero-shot classification problem. CADAE consists of three network modules: an encoder, a generator and a discriminator. The discriminator is applied to control the data distribution of hidden layer encoded by the encoder, and we add hybrid attribute condition into hidden layer to control the reconstruction process of generator. Finally, the generator module of the trained CADAE would be used to generate samples to train a classifier for missing classes. The proposed method is verified with three datasets under different data missing conditions. The results show that the proposed method could effectively solve the zero-shot classification problem with high classification accuracy exceeds 95%. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106577	10.1016/j.asoc.2020.106577													
J								A phase change material selection using the interval-valued target-based BWM-CoCoMULTIMOORA approach: A case-study on interior building applications	APPLIED SOFT COMPUTING										Phase change material (PCM); Material selection; Interval numbers; Best-worst method (BWM); Target-based COmbined COmpromise SOlution (T-coCoSo); Target-based multi-objective optimization of ratio analysis plus the full multiplicative form (T-MULTIMOORA)	THERMAL-ENERGY STORAGE; VIKOR METHOD; EQUIPMENT SELECTION; MULTIMOORA METHOD; PCM; COMBINATION; SYSTEM; MODEL	The move towards sustainable development and energy efficient solutions in the built environment has led to develop innovative solutions, particularly in the area of indoor environmental comfort in buildings. Phase change materials (PCMs) are a potential approach to target building energy use reduction. PCMs are smart materials that have been commercialized and used in buildings for effective regulation of surface and indoor air temperature fluctuations and peak energy reductions. Appropriate PCM selection is the critical step in PCM system design that determines the full effectiveness and applicability of PCM integrated building applications. Optimal selection of PCMs can be effectively executed with the support of multiple attribute decision making (MADM) approaches. Although, the traditional MADM methods have usually focused on beneficial and non-beneficial attributes, in real-world and practical problems, decision-makers tend to determine the rank of an optimal alternative based on the target values of their attributes. In response to the knowledge gap of an existing practical and functional PCM selection solution, this study proposed a hybrid and novel target-based MADM approach that combines the best-worst method (BWM) with COmbined COmpromise SOlution (CoCoSo) and multi-objective optimization of ratio analysis plus the full multiplicative form (MULTIMOORA) with an interval-valued structure called the IV-T-BWM-CoCoMULTIMOORA approach. A case study is examined to select the optimal PCM for interior building surface applications based on a case-specific construction project in Toronto, Canada. Two separate scenarios are considered, one based on thermophysical specifications and one based on managerial preferences. The connection between both technical and managerial criteria was demonstrated to clearly affect the decision-making process to take into account both thermophysical properties of PCM alternatives in the context of risk factors and design considerations. Nevertheless, the influence of the technical parameters was shown to be dominant in the final selection of the best case scenario. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106508	10.1016/j.asoc.2020.106508													
J								Grey-Lotka-Volterra model for the competition and cooperation between third-party online payment systems and online banking in China	APPLIED SOFT COMPUTING										Third-party online payment; Lotka-Volterra model; Grey direct modelling; Competition and cooperation		In China, online banking and third-party online payment systems have a special relationship in which cooperation and competition co-exist. Quantifying the degree of competition and collaboration between them and predicting their future development trends will help to achieve a win-win situation. This paper uses the Lotka-Volterra model to quantitatively analyse and predict the impact of commercial banks' online payment systems on the development of third-party online payment systems. The predator is an online bank, and the prey is a third online payment system. In the modelling process, the continuous model is discretized by using the grey theory. To improve modelling accuracy, a background coefficient is introduced. The parameters of the model are estimated by the least-squares method, and the Grey-Lotka-Volterra model for fitting the relationship between third- party payment and bank competition and cooperation is obtained. This paper empirically analyses quarterly transaction volume data for online banking and third payment. The results show that before 2011, the relationship between the two is more inhibition than cooperation, and the inhibition of online banking on third-party payment transactions is more significant than the third payment reaction. Only by strengthening cooperation can they win-win; the growth rate of third-party online payment transactions is more durable than that of online banking, but the market share is lower than that of banks; after 2011, the inhibition of third-party payment by banks is more significant than that of online banking. Compared with the period before 2011, this use decreased from inhibition to cooperation, and the growth of the overall turnover of the two plays a role in improving their development. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106501	10.1016/j.asoc.2020.106501													
J								Training set selection and swarm intelligence for enhanced integration in multiple classifier systems	APPLIED SOFT COMPUTING										Combination rule; Swarm intelligence; Optimization; Data reduction; Classifier integration; Multiple classifier systems; Big data; Machine learning; Classifier ensemble	OPTIMIZATION; ENSEMBLE; ALGORITHM; FUSION	Multiple classifier systems (MCSs) constitute one of the most competitive paradigms for obtaining more accurate predictions in the field of machine learning. Systems of this type should be designed efficiently in all of their stages, from data preprocessing to multioutput decision fusion. In this article, we present a framework for utilizing the power of instance selection methods and the search capabilities of swarm intelligence to train learning models and to aggregate their decisions. The process consists of three steps: First, the essence of the complete training data set is captured in a reduced set via the application of intelligent data sampling. Second, the reduced set is used to train a group of heterogeneous classifiers using bagging and distance-based feature sampling. Finally, swarm intelligence techniques are applied to identify a pattern among multiple decisions to enhance the fusion process by assigning class-specific weights for each classifier. The proposed methodology yielded competitive results in experiments that were conducted on 25 benchmark datasets. The Matthews correlation coefficient (MCC) is regarded as the objective to be maximized by various nature-inspired metaheuristics, which include the moth-flame optimization algorithm (MFO), the grey wolf optimizer (GWO) and the whale optimization algorithm (WOA). (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106568	10.1016/j.asoc.2020.106568													
J								Modeling and control of nonlinear systems using an Adaptive LAMDA approach	APPLIED SOFT COMPUTING										LAMDA; Intelligent control; Online learning; Adaptive fuzzy models	FUZZY-LOGIC CONTROL; LEARNING ALGORITHM; NEURAL-NETWORKS; INVERSE CONTROL; DESIGN; ANFIS; IDENTIFICATION; OPTIMIZATION; DYNAMICS; ROBOT	This paper presents a soft computing technique for modeling and control of nonlinear systems using the online learning criteria. In order to obtain an accurate modeling, and therefore a controller with good performance, a method based on the fundamentals of the artificial intelligence algorithm, called LAMDA (Learning Algorithm for Multivariate Data Analysis), is proposed, with a modification of its structure and learning method that allows the creation of an adaptive approach. The novelty of this proposal is that for the first time LAMDA is used for fuzzy modeling and control of complex systems, which is a great advantage if the mathematical model is not available, partially known, or variable. The adaptive LAMDA consists of a training stage to establish initial parameters for the controller, and the application stage in which the control strategy is computed and updated using an online learning that evaluates the closed-loop system. We validate the method in several control tasks: (1) Regulation of mixing tank with variable dead-time (slow variable dynamics), (2) Regulation of a Heating, Ventilation and Air-Conditioning (HVAC) system (multivariable slow nonlinear dynamics), and (3) trajectory tracking of a mobile robot (multivariable fast nonlinear dynamics). The results of these experiments are analyzed and compared with other soft computing control techniques, demonstrating that the proposed method is able to perform an accurate control through the proposed learning technique. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106571	10.1016/j.asoc.2020.106571													
J								Modified Social Group Optimization-a meta-heuristic algorithm to solve short-term hydrothermal scheduling	APPLIED SOFT COMPUTING										Meta-heuristic algorithm; Benchmark functions; Fitness function evaluations; Social Group Optimization; Hydrothermal scheduling problem	PARTICLE SWARM; DIFFERENTIAL EVOLUTION; GLOBAL OPTIMIZATION; SEARCH ALGORITHM; DYNAMICS	Social Group Optimization (SGO), developed by Satapathy et al. in the year 2016, is a class of metaheuristic optimization inspired by social behavior. It has two phases: improving phase and acquiring phase. In the improving phase, each individual improves its knowledge by interacting with the best person/solution and in acquiring phase, the individuals interact with randomly selected individuals and the best person simultaneously to acquire knowledge. Modified Social Group Optimization (MSGO) is the improved version of SGO, where the acquiring phase is modified. A self-awareness probability factor is added in the acquiring phase, which enhances the learning capability of an individual from the best-learned person in the societal setup. It is observed that this modification has improved both exploration and exploitation abilities in comparison with the conventional SGO. To analyze the performance of the MSGO, an exhaustive performance comparison is made with GA, PSO, DE, ABC, and a few newer algorithms of the years 2010-2019. The results are tabulated in six experiments. Later, MSGO is applied to solve the short-term hydrothermal scheduling (HTS) problem. The central objective of the HTS problem is to ascertain the optimal plan of action for hydro and thermal generation minimizing the fuel cost of thermal plants and, at the same time satisfying various operational and physical constraints. The valve point loading effect related to the thermal power plants, transmission loss, and other constraints lead HTS as a complex non-linear, non-convex, and non-smooth optimization problem. Simulation results clearly show that the MSGO method is capable of obtaining a better solution. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106524	10.1016/j.asoc.2020.106524													
J								Centroid of polygonal fuzzy sets	APPLIED SOFT COMPUTING										Polygonal fuzzy set; Interval type-2 fuzzy set; General type-2 fuzzy set; Centroid; EKM algorithm	REDUCTION METHOD; COLLAPSING METHOD; FLOW ALGORITHM; DEFUZZIFICATION	In real applications, calculation of centroid, as a common defuzzification method, requires discretization of the fuzzy sets which makes the calculation on one hand time-consuming and on the other hand approximated. Particularly, centroid calculation for higher order fuzzy sets, is a bottleneck toward their real use. In this paper, we introduce polygonal fuzzy sets of type-1, interval type-2, and general type-2. Then we propose a closed form formula for exact and fast calculation of the centroid of polygonal type1 fuzzy sets defined on continuous domain without the need of discretization. With the integration of the proposed method with EKM algorithm - that is a well-known algorithm for calculating the centroid of interval type-2 fuzzy sets - a fast and precise method for calculating the centroid of polygonal interval type-2 fuzzy sets and polygonal type-2 fuzzy sets on discrete and continuous domain are presented. It is shown that the proposed algorithms calculate the centroids of polygonal fuzzy sets faster and more accurately. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106519	10.1016/j.asoc.2020.106519													
J								Regenerative and combinatorial random variable based particle swarm optimization towards optimal transmission switching	APPLIED SOFT COMPUTING										Combinatorial problem; Meta-heuristic method; Micro-PSO; Mixed-integer non-linear problem; Optimal transmission switching; Regenerating constraint handling strategy	VOLTAGE SECURITY; REACTIVE POWER; RECONFIGURATION; HEURISTICS; ALGORITHMS	This paper describes the development of a meta-heuristic based algorithm to solve the optimal transmission switching (OTS) problem. The approach solves the mixed-integer non-linear problem considering simultaneous optimization of transmission topology and generation dispatch. The algorithm is the first ever application of particle swarm optimization (PSO) to model OTS and operates using combined real and binary (CRB) variables to solve a weighted sum of interdependent multiple objective functions. The unique stochastic generation principle of combinatorial variables is based on a blend of both uniform and biased Gaussian probability distribution functions. The required binary values of swarms are generated from the continuous values of the random Gaussian distribution with the application of the Heaviside function. All the transmission lines are considered as potential switch variables the operations of which are limited by different aspects. The algorithm can tackle the complex optimization problem with many constraints of varying difficulty. The randomness in CRB variables and unpredictability in switching may generate infeasible particle(s) resulting in island formation. The algorithm also proposes regeneration of these infeasible particle(s) by modifying them to a feasible one to avoid this islanding as well as to have stable switching possibilities. The improvement in the computational efficiency of the algorithm is proposed with the adoption of Micro-PSO for small load deviations. A wide range of unique solutions are obtained based on the preferences of the system operator. This OTS algorithm is tested using the IEEE 57 bus and the IEEE 118 bus system and encouraging results are obtained. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106529	10.1016/j.asoc.2020.106529													
J								Symplectic incremental matrix machine and its application in roller bearing condition monitoring	APPLIED SOFT COMPUTING										Symplectic incremental matrix machine; Incremental proximal descent; Roller bearing condition monitoring; Symplectic geometry similarity transformation	FAULT-DIAGNOSIS; SIMULATION; REGRESSION	For roller bearing condition monitoring, the collected signals have complex internal structure, which can be naturally represented as matrices. Support matrix machine (SMM), as a new classifier with matrices as inputs, makes full use of the correlation between rows and columns of matrices and achieves ideal classification results. Unfortunately, SMM have ignored the issue of redundant features, which seriously affects the operational efficiency and recognition accuracy of algorithm. In this paper, we introduce symplectic geometry, l(1)-norm and incremental proximal descent (IPD) to SMM, and symplectic incremental matrix machine (SIMM) is proposed. In SIMM, through symplectic geometry similarity transformation, the de-noising symplectic geometry coefficient matrix is obtained, and the noise robustness of SMM method is therefore improved. Moreover, l(1)-norm is used to constrain the objective function, which can weaken the influence of redundant features, and thus greatly improving the recognition accuracy of SMM. Meanwhile, we use IPD to solve the objective function, which can obviously enhance the algorithm efficiency under the constant recognition rates. The experimental results of two kinds of roller bearings show that the proposed method has a good effectiveness in roller bearing condition monitoring, and the achieved recognition rate can reach 3%-25% much higher than those of the traditional recognition methods in 5-cross validation. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106566	10.1016/j.asoc.2020.106566													
J								A new approach to generate diversified clusters for small data sets	APPLIED SOFT COMPUTING										Data mining; Clustering; Homogeneous; Heterogeneous; Diversified clusters	BIG DATA; SYSTEM	Clustering is a common data mining technique whose main principle states that the samples within a cluster are similar to one another and dissimilar to those in other clusters. This means that samples in the same cluster possess high homogeneity, while different clusters possess high heterogeneity. However, a user may require a result of diversified clustering. Compared to traditional clustering methods, the aim of diversified clustering is to make samples of the same cluster possess high heterogeneity, and different clusters possess high homogeneity. Diversified clustering can be practically applied to aspects of our daily lives such as normal class grouping, student grouping in learning, cluster sampling, balanced diets and assignment of jobs. Nevertheless, our survey of related papers in the research field of data mining found that there has been no proposed research for diversified clustering. In this paper, we formal define the problem of diversified clustering and propose a new method to solve this problem. Experimental results showed that our method can generate good diversified clustering. However, our method is currently only appropriate for small data sets since the execution time of our method increases quickly as the number of diversified clusters increases. We also hope this paper will garner interest in more research on effective methods to generate diversified clusters for use in data mining. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106564	10.1016/j.asoc.2020.106564													
J								Similar case matching with explicit knowledge-enhanced text representation	APPLIED SOFT COMPUTING										Similar case matching; Multi-task learning; Legal document; BERT; Text matching	NETWORKS	Similar case matching is one of the important practical applications of text matching, which is a crucial issue in natural language processing. For a well-structured natural language document, the sentence-based representation and explicit knowledge elements should be effectively integrated so as to fit the original structure of the text, when constructing the document representation. In this paper, we propose a multi-task learning framework with "de- and re-construction", which leverages the extraction of sub-tasks based on sentence-level knowledge elements to enhance the representation at the document level, to improve the performance of the model in the main task of similar case matching. Extensive experiments have proved that our proposed model outperforms methods like latent dirichlet allocation (LDA), LSTM-RNN, BERT, etc, which only focus on constructing document representations. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106514	10.1016/j.asoc.2020.106514													
J								2-stage modified random forest model for credit risk assessment of P2P network lending to "Three Rurals" borrowers	APPLIED SOFT COMPUTING										P2P network lending; "Three Rurals" borrowers; Credit risk evaluation; Random forest	GENETIC ALGORITHM; LOAN EVALUATION; DECISION TREE; COST; CLASSIFICATION; INFORMATION; PERFORMANCE; SELECTION; ENSEMBLE	With the rapid growth of the P2P online loan industry in the "Three Rurals" (agriculture, rural areas, and farmers) sector, it is imperative to manage the borrowing risk of borrowers in the rural areas. A credit risk assessment model is proposed to classify the credit worthiness of the "Three Rurals" borrowers. We select the loan data of the Pterosaur Loan platform as the research sample, and establish a 2-stage Syncretic Cost-sensitive Random Forest (SCSRF) model to evaluate the credit risk of the borrowers. From the random forest, we construct a cost relationship from the actual distribution of the data categories, introduce a weighted Mahalanobis distance using the entropy weight method in the cost function, and adopt a weighted voting for the cost-sensitive decision tree base classifier. The parameters of the SCSRF model are optimized via a grid search. We validate the SCSRF classification model against several established credit evaluation models. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106570	10.1016/j.asoc.2020.106570													
J								Use of stochastic nature-inspired population-based algorithms within an online adaptive controller for mechatronic devices	APPLIED SOFT COMPUTING										Dynamic optimization problem; Online adaptive control; Evolutionary Algorithms; Swarm Intelligence; PI-controller	EVOLUTIONARY ALGORITHMS; DIFFERENTIAL EVOLUTION; DYNAMIC OPTIMIZATION; GLOBAL OPTIMIZATION	Stochastic nature-inspired population-based algorithms are very powerful tools for solving stationary and deterministic, NP-hard optimization problems. These algorithms have rarely been applied to real-world dynamic and uncertain optimization due to their complexity. In this paper, this kind of algorithms were ported onto real hardware (i.e., the velocity controller of a one degree of freedom robot mechanism), where they were used to control the behavior of a non-linear system online. This means that the feedback response from the system must be less than 5 ms. Due to the complexity of the fitness function evaluation, a surrogate linear model was used, implemented as a single-layer artificial neural network, consisting of two phases: learning and simulation. In the first phase, the model of the nonlinear plant is learned during online operation, while in the second, the value of the fitness function needed by the optimization algorithms is predicted. Six algorithms were compared with the PI-controller in our experimental work. This were: classical evolution strategies, contemporary evolution strategies, differential evolution, self-adaptive differential evolution, particle swarm optimization, and the bat algorithm. The results showed that the algorithms outperformed PI-controller in the sense of stability, flexibility and adaptability. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106559	10.1016/j.asoc.2020.106559													
J								Multi-scale decomposition based supervised single channel deep speech enhancement	APPLIED SOFT COMPUTING										DNN; EMD; Hurst exponent; Intelligibility; Speech enhancement; Objective loss function; Spectral variance equalization	MEAN-SQUARE ERROR; PERCEPTUAL EVALUATION; NOISE-ESTIMATION; NEURAL-NETWORKS; INTELLIGIBILITY; CLASSIFICATION; ALGORITHM; REGRESSION; SEPARATION; POWER	Speech signals reaching our ears are in general contaminated by the background noise distortion which is detrimental to both speech quality and intelligibility. In this paper, we propose a nonlinear multi-scale decomposition-based deep speech enhancement method to improve the quality and intelligibility of the contaminated speech. In the proposed method, we have applied Hurst exponent-based Empirical Mode Decomposition (HEMD) to the noisy speech and obtained a set of intrinsic mode functions (IMFs) and a residual. The Deep Neural Networks (DNNs) are trained for each of the extracted IMF and residual to learn a non-linear mapping with a deep hidden structure to construct a time-frequency mask. We have formulated three deep speech enhancement structures, established on three time-frequency masks comprised of Ideal Ratio Mask (IRM), Ideal Binary Mask (IBM), and Phase Sensitive Mask (PSM). Background noise also degrades the original phase of the clean speech; therefore, introduces perceptual disturbance which leads to negative impacts on the speech quality and intelligibility. To avoid speech quality and intelligibility degradations, an iterative procedure is adopted to compensate the phase during noisy backgrounds. Nonlinear Mel-scale weighted MSE (LMW-MSE) is used as a loss function during network training, and computed the gradients which are based on the perceptually motivated nonlinear frequency scale. Usually, the output features of the conventional deep neural networks are over-smoothed which deteriorates the quality of the speech. To alleviate over-smoothness; frequency-independent spectral variance equalization is applied as a post-filtering method. The performance of the proposed deep enhancement methods is extensively evaluated and compared to the DNNs established on same time-frequency mask in various adverse noisy environments. The results have demonstrated that the proposed deep speech enhancement performed better in terms of the perceived speech quality and intelligibility. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106666	10.1016/j.asoc.2020.106666													
J								Metaheuristics for solving the vehicle routing problem with the time windows and energy consumption in cold chain logistics	APPLIED SOFT COMPUTING										Vehicle routing problem; Time windows; Energy consumption; Artificial fish swarm algorithm; Cold chain logistic	FISH SWARM ALGORITHM; LOCAL SEARCH ALGORITHM; BEE COLONY ALGORITHM; HETEROGENEOUS FLEET; NEIGHBORHOOD SEARCH; TABU SEARCH; SCHEDULING PROBLEM; OPTIMIZATION ALGORITHM; GENETIC ALGORITHM; SPLIT DELIVERIES	In this study, we consider a canonical vehicle routing problem (VRP) in the cold chain logistic system, where three special constraints are included, i.e., the dispatching time windows for each customer, different types of vehicles, and different energy consumptions and capacities for each vehicle. The objective is to minimize the total cost including the fixed cost and the energy consumptions. An improved artificial fish swarm (IAFS) algorithm is proposed, where a special encoding approach is designed to consider the problem feature with different type of vehicles. Then, improved preying and following heuristics are developed to perform the exploitation and exploration tasks. A novel customer satisfaction heuristic is embedded in the proposed algorithm, which makes the problem close to the reality. To further improve the performance of the algorithm, a right-shifting heuristic is designed to increase the customer satisfaction without increasing the energy consumption. An initialization heuristic based on the canonical Put Forward Insertion Heuristics (PFIH) is proposed to generate initial solutions with better performance. Finally, a set of realistic instances is generated to test the performance of the proposed algorithm, and after detailed experimental comparisons, the competitive performance of the proposed algorithm is verified. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106561	10.1016/j.asoc.2020.106561													
J								Wavelet neural networks based solutions for elliptic partial differential equations with improved butterfly optimization algorithm training	APPLIED SOFT COMPUTING										Wavelet neural networks; Partial differential equations; Elliptic partial differential equations; Improved butterfly optimization algorithm; Butterfly optimization algorithm	BOUNDARY-VALUE-PROBLEMS; NUMERICAL-SOLUTION; SELECTION; DYNAMICS; SYSTEMS; MODEL; FLOW	In this study, a machine learning approach based on the unsupervised version of wavelet neural networks (WNNs) is used to solve two-dimensional elliptic partial differential equations (PDEs). The design of the WNNs must be judiciously addressed, particularly, the adopted training algorithm, since it greatly influences the generalization performance and the convergence rate of WNNs. Although the gradient information of the commonly used gradient descent training algorithm in WNNs may direct the search to optimal weight solutions that minimize the error function, the learning process is slow due to the complex calculation of the partial derivatives. To date, on account of the derivative free characteristic and adaptability to respond to the complex dynamic changes of the interdependencies, numerous studies explored the potential benefit of integrating a meta-heuristic algorithm as the training algorithm of WNNs, where encouraging results are achieved. In this paper, an improved butterfly optimization algorithm (IBOA) is proposed and subsequently integrated into the training process of the WNNs. To evaluate the performance of the proposed IBOA training method, the obtained results are compared to the results of the momentum backpropagation (MBP), the particle swarm optimization (PSO) and the standard butterfly optimization algorithm (BOA) training methods. Statistical analyses of the results based on a sufficient number of independent runs validate the effectiveness of the proposed method in terms of accuracy, robustness and convergence. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106518	10.1016/j.asoc.2020.106518													
J								Monitoring agriculture areas with satellite images and deep learning	APPLIED SOFT COMPUTING										Agriculture monitoring; Spatio-temporal-spectral deep learning; Rice map; Satellite imagery mining; Landsat	NEURAL-NETWORKS; LAND-COVER; CLASSIFICATION; VEGETATION; ALGORITHM; FUSION; EXTENT; DELTA; CLOUD; SCALE	Agriculture applications rely on accurate land monitoring, especially paddy areas, for timely food security control and support actions. However, traditional monitoring requires field works or surveys performed by experts, which is costly, slow, and sparse. Agriculture monitoring systems are looking for sustainable land use monitoring solutions, starting with remote sensing on satellite data for cheap and timely paddy mapping. The aim of this study is to develop an autonomous and intelligent system built on top of imagery data streams, which is available from low-Earth orbiting satellites, to differentiate crop areas from non-crop areas. However, such agriculture mapping framework poses unique challenges for satellite image processing, including the seasonal nature of crop, the complexity of spectral channels, and adversarial conditions such as cloud and solar radiance. In this paper, we propose a novel multi-temporal high-spatial resolution classification method with an advanced spatiotemporal-spectral deep neural network to locate paddy fields at the pixel level for a whole year long and for each temporal instance. Our method is built and tested on the case study of Landsat 8 data due to its high spatial resolution. Empirical evaluations on real imagery datasets of different landscapes from 2016 to 2018 show the superior of our mapping model against the baselines with over 0.93 F1-score, the importance of each model design, the robustness against seasonal effects, and the visual mapping results. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106565	10.1016/j.asoc.2020.106565													
J								Artificial neural network based crossover for evolutionary algorithms	APPLIED SOFT COMPUTING										Evolutionary algorithms; Artificial neural networks; Recombination operators; Radial basis function network; Genetic algorithms	COMPUTATION	Recombination is a powerful way of generating new solutions in Evolutionary Algorithms. There are many ways to implement recombination. Traditional recombination operators do not use information about parents, evolutionary process, or models for variable interaction in order to find better ways to recombine solutions. Some modern recombination operators use information about parents and models for variable interaction, but they cannot always be efficiently applied. We propose to use an artificial neural network to compute the recombination mask, given two parents. Here, a radial basis function network (RBFN) is trained online using past successful recombination cases obtained during the optimization performed by the evolutionary algorithm. The RBFN crossover (RBFNX) is used together with other recombination operators (here, uniform crossover is employed). Applying RBFNX has O(N) time complexity, where N is the dimension of the optimization problem. Results of experiments with genetic algorithms, applied to two binary optimization problems, and evolution strategies, applied to continuous optimization test problems, indicate that RBFNX is generally able to improve the successful recombination rates. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106512	10.1016/j.asoc.2020.106512													
J								Lightweight speaker verification for online identification of new speakers with short segments	APPLIED SOFT COMPUTING										Speaker identification; Generic verification; Low resources		Verifying if two audio segments belong to the same speaker has been recently put forward as a flexible way to carry out speaker identification, since it does not require to be re-trained when new speakers appear on the auditory scene. Although many of the current techniques have achieved high performances, they require a considerably high amount of memory, and a specific minimum length for their input audio segments. These requirements limit the applicability of these techniques in scenarios such as service robots, internet of things and virtual assistants, where computational resources are limited and the users tend to speak in short segments. In this work we propose a BLSTM-based model that reaches a level of performance comparable to the current state of the art when using short input audio segments, while requiring a considerably less amount of memory. Further, as far as we know, a complete speaker identification system has not been reported using this verification paradigm. Thus, we present a complete online speaker identifier, based on a simple voting system, that shows that the proposed BLSTM-based model achieves a similar performance at identifying speakers online compared to the current state of the art. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106704	10.1016/j.asoc.2020.106704													
J								Ensemble probabilistic prediction approach for modeling uncertainty in crude oil price	APPLIED SOFT COMPUTING										Crude oil price; Ensemble method; Interval prediction; Uncertainty modeling; Grey wolf optimization	WIND-SPEED; ALGORITHM; INTERVALS; SYSTEM; DECOMPOSITION; REGRESSION; FORECASTS; ARIMA	The quantification of the uncertainty in crude oil price is of significance to improve the related financial decision-making. However, studies in this field have remained limited because the nonlinearity inherent in the crude oil price makes it challenging to model its uncertainty. In this paper, a novel learning system of ensemble probabilistic prediction combining five popular machine learning methods and an improved optimizer is presented to effectively model the uncertainty in crude oil price and establish the corresponding prediction interval with satisfactory reliability and resolution. An improved grey wolf optimizer based on the adaptive Cuckoo search algorithm (AGWOCS) is proposed in the learning system to integrate the prediction intervals produced by the above machine learning methods. In addition, the superiority of the proposed AGWOCS is validated based on an algorithm test, compared to three benchmark optimizers. To validate the effectiveness of the proposed learning system, the uncertainties in daily and weekly Europe Brent spot prices are modeled as a case study. The evaluation results based on the reliability, resolution, and sharpness demonstrate that the proposed learning system can yield the prediction interval with a higher quality, which has distinct advantages over eight benchmarks as a whole. The convergence and scalability of the learning system are also investigated, which reveals its feasibility. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106509	10.1016/j.asoc.2020.106509													
J								A fuzzy nonlinear programming approach for planning energy-efficient wafer fabrication factories	APPLIED SOFT COMPUTING										Energy efficiency; Power consumption; Yield; Fuzzy nonlinear programming; Wafer fab	ARTIFICIAL NEURAL-NETWORK; LINEAR-REGRESSION; DECISION-MAKING; YIELD; OPTIMIZATION; SIMULATION; CAPACITY; MODEL; CONSUMPTION; MANAGEMENT	Wafer fabrication is an energy-consuming process. Achieving energy efficiency is, therefore, crucial for wafer fabrication factories (wafer fabs). However, limited studies have focused on resolving product quality problems for improving energy efficiency. This study proposed a novel fuzzy nonlinear programming (FNLP) approach for minimizing energy wastage caused by product quality problems. In the proposed methodology, first, the process of resolving the quality problems of a product is modeled as a fuzzy yield learning process. Then, the energy saved by the yield learning process is quantified. The total power consumption is obtained by summing the power consumptions of all the products in a wafer fab. Subsequently, an FNLP model is formulated and optimized to minimize the total energy consumption in the wafer fab by optimizing the product mix. The data from a wafer fab is used to demonstrate the applicability of the proposed FNLP approach. According to the experimental results, the total power consumption at a future period can be reduced by 17.2% by optimizing the product mix. (C) 2020 Published by Elsevier B.V.																	1568-4946	1872-9681				OCT	2020	95								106506	10.1016/j.asoc.2020.106506													
J								Fast and accurate online sequential learning of respiratory motion with random convolution nodes for radiotherapy applications	APPLIED SOFT COMPUTING										Radiotherapy; Respiratory motion; Random convolution nodes; Sequential learning	REAL-TIME PREDICTION; TUMOR MOTION; TRACKING SYSTEM; COMPENSATION; ALGORITHM; MODEL	Accurate prediction of tumor motion for motion adaptive radiotherapy has been a challenge as respiration-induced motion is non-stationary in nature and often subjected to irregularities. Despite having a plethora of works for predicting this motion, their tracking capabilities are usually prone to large prediction errors due to the time-varying irregularities and intra-trace variabilities. To overcome this, prediction models are re-trained at regular intervals. This solution however demands a trade-off between the re-training interval and prediction accuracy in estimating the future tumor location. This is because re-training with small interval increases the computational requirements whereas a larger interval hampers the prediction performance. To address these issues, a prediction model that relies on random convolution nodes (RCN) governed by local receptive fields (LRFs) is proposed for respiratory motion prediction. The innate nature of LRFs extracts the features that contribute to the local-patterns as well as the non-stationary patterns in recent samples and subsequently learn them using extreme learning machine (ELM) theories. To address the re-training issue, we propose an online sequential learning framework (OS-fRCN) that can update the model parameters at regular intervals. Suitability of the proposed OS-fRCN for respiratory motion prediction is evaluated on 304 respiratory motion traces. Performance analysis conducted at four prediction horizons (in-line with the commercially available radiotherapy systems) demonstrated that the proposed OS-fRCN method requires less computational complexity and yields robust, accurate prediction performance when compared with existing prediction methods. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106528	10.1016/j.asoc.2020.106528													
J								Social image mining for fashion analysis and forecasting	APPLIED SOFT COMPUTING										Image clustering; Classification; Fashion analysis and forecasting; Social images	MEAN SHIFT; CLASSIFICATION; REGRESSION	Fashion industries need to be attentive towards the changing fashion and its upcoming market demands to grow their business, optimally. This paper describes research work involved in image mining for fashion analysis and forecasting using fashion-related images collected from the social network. A novel soft clustering technique is proposed for grouping the social fashion images. This technique is robust against uncertainty found in given images. The proposed clustering approach is compared with existing soft clustering approaches. It is found that the proposed approach performs well. Attributes of fashion items found in each cluster are analyzed through correlation, causal analysis, and fashion cycle visualization. Predictive models are applied to the clustered fashion items for style forecasting. A comparative study of predictive models is also done to find an optimal technique for various fashion items. As social visual perception is helpful for decision making, the proposed system is very useful in fashion industries to uplift their business. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106517	10.1016/j.asoc.2020.106517													
J								A multi-objective immune algorithm for intrusion feature selection	APPLIED SOFT COMPUTING										Feature selection; Immune algorithm; Multi-objective optimization; Intrusion detection	ANOMALY DETECTION; DETECTION SYSTEM; OPTIMIZATION; EVOLUTIONARY; DECISION	Feature selection plays a crucial role in classification problems, which tries to remove redundant or irrelevant features by mapping high-dimensional data to low-dimensional ones. Thus, this approach can improve the classification accuracy and reduce the computational cost to train the classification model. In this paper, we suggest an improved multi-objective immune algorithm (MOIA) for feature selection in intrusion detection. Specifically, the feature subsets for intrusion detection are treated as the individuals for immune optimization, which will select suitable feature subsets in order to reduce the dimensions of the dataset. After that, a neural network is used to train the classification model using the selected suitable feature subsets, and the output of the classification model is regarded as the target fitness value for each individual. As multiple attack types are considered in this paper, a traditional MOIA is modified by using an elite selection strategy based on the reference vectors, which can maintain the individuals with more promising performance when distinguishing more than five attack types in intrusion detection. By this way, the proposed algorithm can accelerate the convergence speed of classification, which also improves the classification accuracy. Experimental results on the NSL-KDD and UNSW-NB15 datasets validate the higher classification accuracy of the proposed algorithm. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106522	10.1016/j.asoc.2020.106522													
J								An adaptive Harris hawks optimization technique for two dimensional grey gradient based multilevel image thresholding	APPLIED SOFT COMPUTING										Computational intelligence; Soft computing; Optimal multilevel image thresholding	MAXIMUM TSALLIS ENTROPY; CUCKOO SEARCH ALGORITHM; DIFFERENTIAL EVOLUTION; SEGMENTATION; HISTOGRAM	A metaheuristic algorithm called Harris hawks optimization (HHO) is gaining its popularity among its clan and useful for optimization. In this algorithm, the prey gets completely exhausted when the escape energy is equal to zero, therefore it fails to explore further. The random operator chosen in the existing method is a wastage of search agents (Harris hawk). To overcome this issue, we propose an adaptive Harris Hawks optimization (AHHO) technique. In this work, the mutation is employed to restrict the escape energy within the range [0, 2], except for the mutation interval. Our method adaptively decides the chance of the Harris hawk would do perch along with the other family members or move to a random tall tree with the help of average fitness. The proposed AHHO algorithm is benchmarked with 23 classical test functions and 30 modern test function from CEC 2014 test suite consisting of unimodal, multimodal, hybrid and composite functions. The qualitative and quantitative analysis, which include metrics such as statistical results, convergence curves, p-value from Wilcoxon rank-sum test and Friedman mean rank. It reveals that AHHO provides good results when compared with other well-known nature-inspired algorithms. It can be used for multilevel thresholding which is an optimization problem. Recently, 2D histogram based multilevel image thresholding techniques are becoming more popular for different image processing applications. The local averaging scheme used for the construction of a 2D histogram in existing methods fails to preserve the edge information. The choice of the diagonal pixels only results in the loss of information making the earlier multilevel thresholding methods inefficient to retain the spatial correlation information. Although the computation of 2D histogram based on grey gradient information is a better way to threshold an image, it faces problems due to the presence of the edge magnitude peaks. These problems are solved by investigating an improved 2D grey gradient (I2DGG) method, a new technique is suggested in this paper to suppress high edge magnitudes. The I2DGG is a maximization problem, which requires an exhaustive search process. Therefore, AHHO is used to obtain the optimal threshold values. The result of our proposed AHHO based multilevel thresholding using the I2DGG method is obtained using all the 500 images from the Berkeley Segmentation Data set (BSDS 500). When we compare the proposed method I2DGG with 2D Tsallis entropy and 1D Tsallis entropy based multilevel thresholding, the I2DGG outperforms other methods. The experimental results are also compared with the state-of-art optimization-based multilevel thresholding methods, which shows our proposed method is beneficial to the segmentation field of image processing. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106526	10.1016/j.asoc.2020.106526													
J								Fault diagnosis of rolling bearing of wind turbines based on the Variational Mode Decomposition and Deep Convolutional Neural Networks	APPLIED SOFT COMPUTING										Deep learning; Convolutional Neural Networks; VariationaL Mode Decomposition; Fault diagnosis; Rolling bearing		Machine learning techniques have been successfully applied in intelligent fault diagnosis of rolling bearings in recent years. However, in the real world industrial application, the dissimilarity of data due to changes in the working conditions and data acquisition environment often cause a poor performance of the existing fault diagnosis methods. Consequently, to address these inadequacies, this paper developed a novel method by integrating the Convolutional Neural Networks (CNNs) with the Variational Mode Decomposition (VMD) algorithms. Named as "Variational Mode Decomposition with Deep Convolutional Neural Networks (VMD-DCNNs)", the method, in an end-to-end way, directly processes raw vibration signals without artificial experiences and manual intervention to realize the fault diagnosis of rolling bearings. In addition, the CNN technique is used to extract features from each Intrinsic Mode Function (IMF) in order to address the deficiency in extracting features from a single source and to achieve an effective and efficient fault diagnosis of rolling bearings under different environments and states. The value of parameter K of the VMD-DCNNs model is optimized by considering time complexity and generalization ability of the model. Lastly, bearing experiments are conducted to verify the superiority of the VMD-DCNNs in diagnosing fault under different conditions. The visualizations of the signals in the convolutional layer explain the reasonability in selecting the value of parameter K and they also indicate that the translational invariances in a raw IMF component have been learned by the VMD-DCNNs model. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106515	10.1016/j.asoc.2020.106515													
J								Prediction model of energy market by long short term memory with random system and complexity evaluation	APPLIED SOFT COMPUTING										Prediction neural network model; Long short term memory; Multiscale cross sample entropy; Random time effective function; Energy market	ARTIFICIAL NEURAL-NETWORKS; OIL PRICE SHOCKS; TIME-SERIES; ALGORITHM; DEMAND	Due to the frequent and violent fluctuation of energy futures prices, the investment risk of energy investors is increased. Forecasting energy futures prices has progressively become the focus of research. However, traditional prediction model only conducts forecasting based on historical data without considering the behavior of the market, resulting in poor accuracy. In this paper, the random time effective function that considers the timeliness of historical data and the random change of market environment is applied to the long short term memory model to establish a novel prediction model, which is denoted by long short term memory with random time effective function model (LSTMRT). LSTM model has the characteristics of selective memory and the internal influence of time series, which is very suitable for the prediction of price time series. Random time effective function can give different weights to historical data. Furthermore, using multiscale cross-sample entropy (MCSE) as an innovative method to reveal the performance of prediction. Finally, comparing with other models selected in this paper, error evaluations and statistical comparisons are utilized to demonstrate the advantages and superiority of the proposed model. LSTMRT model has the effect of random movement and keeps the trend fluctuation of the original nonlinear data, which makes the prediction more accurate and more credible. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106579	10.1016/j.asoc.2020.106579													
J								Target tracking strategy using deep deterministic policy gradient	APPLIED SOFT COMPUTING										Deep deterministic policy gradient; Cognitive electronic warfare; Reinforcement learning; Motion planning; Target tracking	LEVEL CONTROL; UAV; VEHICLES	To address the challenge of maintaining high robustness of target tracking in a 3D dynamic high-altitude scenario, this paper presents a method to formulate continuous strategic maneuvers for unmanned combat air vehicles (UCAVs) based on deep deterministic policy gradient (DDPG). DDPG is an efficient reinforcement learning approach that helps UCAV perform a variety of navigation tasks in real-time in a dynamic and random electronic warfare environment, and therefore possesses clear advantages over other technologies. First, create a target tracking simulator, Tracker, in the cognitive electronic warfare framework, and conduct a theoretical analysis of maneuvering bias produced by environmental observational errors. Tracker can automatically correlate the maximum physical overload with UCAV's attitude angles and desired movement commands. Second, shape the agent's behavior rewards under the inspiration of vector-based navigation to ensure that the DDPG's output is reliable. Finally, a DRL-based navigation decision framework is employed to validate the simulation for target tracking tasks in different environments and bring excellent results. In terms of behavior assessment, the agile maneuvers mastered by the agent are dissected by time segmentation of high-quality trajectories. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106490	10.1016/j.asoc.2020.106490													
J								Multiscale intelligent fault detection system based on agglomerative hierarchical clustering using stacked denoising autoencoder with temporal information	APPLIED SOFT COMPUTING										Process monitoring; Stacked denoising autoencoder; Multiscale analysis; Robust features; Temporal information	DIAGNOSIS; FRAMEWORK; PCA	Deep learning-based process monitoring has achieved remarkable progress. Generally, a deep model is empirically selected before the data features are learned. In this study, the interpretability and suitability of stacked denoising autoencoder (SDAE) in process monitoring territory are theoretically analyzed and validated. Considering that the data will show different feature representations at different scales, such as overall outline, local information, and microscopic details, this study utilizes the concept of multiscale analysis to mine the feature information of raw data deeply in different scales. The multiscale analysis is performed on the basis of agglomerative hierarchical clustering and silhouette coefficient, which makes the analysis data characteristics-based and intelligently abandons the intervention of manual prior knowledge. Then, the SDAE models are established under each scale to learn the high-order and robust features from the data with noise and fluctuation, and all monitoring results of the different scales are integrated using the Bayesian inference. Finally, given the temporal information in sequence data, the state representation of previous events is embedded into the current decision through a sliding window. The numerical process, benchmark Tennessee Eastman and real steel plate process are used to analyze the superiority of the proposed method (MSDAE-TP) over other deep learning-based monitoring methods. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106525	10.1016/j.asoc.2020.106525													
J								Nonlinear systems modelling based on self-organizing fuzzy neural network with hierarchical pruning scheme	APPLIED SOFT COMPUTING										Nonlinear system modelling; Artificial neural network; Generalized ellipsoidal basis function; Hierarchical pruning; Modified recursive least square	WASTE-WATER TREATMENT; EVOLVING FUZZY; ONLINE IDENTIFICATION; INFERENCE SYSTEM; PREDICTION; OPTIMIZATION; REGRESSION; ALGORITHM; RULES; CLASSIFICATION	In this paper, a self-organizing fuzzy neural network with hierarchical pruning scheme (SOFNN-HPS) is proposed for nonlinear systems modelling in industrial processes. In SOFNN-HPS, to strike the optimal balance between system accuracy and network complexity, an online self-organizing scheme for identifying the structure and parameters of the network simultaneously is developed. First, to enhance the characterization ability of the fuzzy rules for nonlinear systems, the asymmetric Gaussian functions that can partition the input space more flexibly are introduced as membership functions. Second, a hierarchical pruning scheme, which is designed by rule density and rule significance, is used to delete the redundant fuzzy rules while using the geometric growing criteria to generate fuzzy rules automatically, which can avoid the requirement of pre-setting the pruning threshold and prevent the mistaken deletion of significant rules. Third, an adaptive allocation strategy is adopted to set the antecedent parameters of the fuzzy rules in the learning process, which can not only adjust the region of generalized ellipsoidal basis functions for better local approximation, but also balance the accuracy of the system and the interpretability of the rule base obtained. Finally, to speed up the convergence of the estimation error, a modified recursive least square algorithm is used to update the consequent parameters of the resulting fuzzy rules online. In addition, the convergence proofs of the estimation error and the network linear parameters of SOFNN-HPS are given, and they are helpful in successfully applying the SOFNN-HPS in practical engineering. To verify the effectiveness of SOFNNHPS, two benchmark test problems and a key water quality parameter prediction experiment in the wastewater treatment process are examined. The simulation results demonstrate that the proposed SOFNN-HPS algorithm can obtain a self-organizing fuzzy neural network with compact structure and powerful generalization performance. The source codes of SOFNN-HPS and other competitors can be downloaded from https://github.com/hyitzhb/SOFN-HPS. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106516	10.1016/j.asoc.2020.106516													
J								DTOF-ANN: An Artificial Neural Network phishing detection model based on Decision Tree and Optimal Features	APPLIED SOFT COMPUTING										Phishing detection; Feature selection; Neural network; K-medoids clustering	FEATURE-SELECTION	Recently, phishing emerges as one of the biggest threats to human's daily networking environments. Phishing attackers disguise illegal URLs as normal ones to steal user's private information with the social engineering techniques, such as emails and SMS, which calls for an effective method of preventing phishing attacks to relieve the loss by them. Neural networks can be used to detect and prevent phishing attacks because of their strong active learning abilities from massive datasets and high accuracy in data classification. However, duplicate points in the public datasets and negative and useless features in the feature vectors will trap the training of the neural networks into the problem of over-fitting, which will make the trained classifier weak when detect phishing websites. This paper proposes DTOF-ANN (Decision Tree and Optimal Features based Artificial Neural Network) to tackle this shortcoming, which is a neural-network phishing detection model based on decision tree and optimal feature selection. First, the traditional K-medoids clustering algorithm is improved with an incremental selection of initial centers to remove the duplicate points from the public datasets. Then, an optimal feature selection algorithm based on the new defined feature evaluation index, decision tree and local search method is designed to prune out the negative and useless features. Finally, the optimal structure of the neural network classifier is constructed through properly adjusting parameters and trained by the selected optimal features. Experimental results have demonstrated that DTOF-ANN exhibits higher performance than many of the existing methods. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106505	10.1016/j.asoc.2020.106505													
J								Dynamic framework to mining Internet of Things for multimedia services	EXPERT SYSTEMS										Bayes network; friendship selection; IoT; link selection; multimedia services; priority; searching IoT; social IoT (SIoT)	NETWORKS	The rapid and unprecedented technological advancements are currently dominated by two technologies. At one hand, we witness the rise of the Internet of Things (IoT) as the next evolution of the Internet. At the other hand, we witness a vast spread of social networks that connects people together socially and opens the door for people to share and express ideas, thoughts, and information. IoT is overpopulated by a vast number of objects, millions of multimedia services, and interactions. Therefore, the search of the right object that can provide the specific multimedia service is considered as an important issue. The merge of these two technologies resulted in new paradigm called Social IoT (SIoT). The main idea in SIoT is that every object can mine IoT in search for certain multimedia service. We investigate the issue of friends' management in SIoT and propose a framework to manage friends' requests. The proposed framework employs several mechanisms to better manage friends' relationships. The proposed framework consists of friend selection, friendship removal, and an update module. It proposes a weight-based algorithm and Naive Bayes Classifier-based algorithm for the selection component. Moreover, a random service allocation model is proposed to construct service-specific network model. This model is then used in the simulation setup to examine the performance of different friends' management algorithms. The performance of the proposed framework is evaluated using simulation under different scenarios. The obtained simulation results show improvement over other strategies in terms of average degree of connections, average path length, local cluster coefficients, and throughput.																	0266-4720	1468-0394				OCT	2020	37	5			SI				e12404	10.1111/exsy.12404													
J								A proposal for a 360 degrees information system model for private health care organizations	EXPERT SYSTEMS										360 degrees HIS; information system model; market-oriented health care; private health care organizations	BIG DATA; EXPERIENCE; QUALITY; RECORD; MANAGEMENT; KNOWLEDGE; ADOPTION; TECHNOLOGY; ANALYTICS; BUSINESS	At a time when communication, new media, and digitalization are transversal to the whole of society, private health care organizations have the possibility of making their business processes evolve. The objective is thus to seize the benefits associated to the active use of patients' electronic health records (EHRs) as the basis for personalized health care. In order to initially validate the health care sector acceptance of a 360 degrees health care information system (HIS), focused on collecting patients' data to create the necessary knowledge for delivering personalized health care procedures and initiatives, a focus group involving a set of health-related professionals was performed. Despite recognizing the immense possibilities associated to EHR and its direct incorporation on a 360 degrees HIS, the referred professionals still highlighted their concerns relative to the maintenance of adequate security and privacy levels. With this in mind, a proposal for a 360 degrees HIS model is presented, and its main functional blocks are described with a focus on triggering patient/customer loyalty.																	0266-4720	1468-0394				OCT	2020	37	5			SI				e12420	10.1111/exsy.12420													
J								Online eigenvector transformation reflecting concept drift for improving network intrusion detection	EXPERT SYSTEMS										concept drift; eigenvalue; eigenvector; online transformation; principle component analysis	PRINCIPAL-COMPONENTS; ROBUST PCA	Currently, large data streams are constantly being generated in diverse environments, and continuous storage of the data and periodic batch-type principal component analysis (PCA) are becoming increasingly difficult. Various online PCA algorithms have been proposed to solve this problem. In this study, we propose an online PCA methodology based on online eigenvector transformation with the moving average of the data stream that can reflect concept drift. We compared the network intrusion detection performance based on online transformation of eigenvectors with that of offline methods by applying three machine learning algorithms. Both online and offline methods demonstrated excellent performance in terms of precision. However, in terms of the recall ratio, the performance of the proposed methodology with integrated online eigenvector transformation was better; thus, the F1-measure also indicated better performance. The visualization of the principal component score shows the effectiveness of our method.																	0266-4720	1468-0394				OCT	2020	37	5			SI				e12477	10.1111/exsy.12477													
J								A competitive chain-based Harris Hawks Optimizer for global optimization and multi-level image thresholding problems	APPLIED SOFT COMPUTING										Optimization; Metaheuristic; Multilevel Thresholding; Salp swarm algorithm; Harris Hawks Optimization	PARTICLE SWARM OPTIMIZATION; DIFFERENTIAL EVOLUTION; ALGORITHM; PARAMETERS; IDENTIFICATION; ENTROPY; CELLS	This paper presents an enhanced Harris Hawks Optimizer (HHO) to tackle global optimization and determine the optimal threshold values for multi-level image segmentation problems. HHO is a new swarm-based metaheuristic technique that simulates the behaviors of Harris hawks during the process of catching the rabbits. The HHO established its strong performance as a swarm-based optimization technique. However, population-based HHO still may face some limitations in dealing with more multimodal and composition problems. For example, this optimizer may be stagnated to local optima and turned to immature convergence when performing phases of exploration and exploitation. To mitigate these drawbacks, an improved HHO is proposed that considers the salp swarm algorithm (SSA) as a competitive method to enhance the balance between its exploration and exploitation trends. Firstly, a set of solutions is generated. Then, we divide those solutions into two halves, where the exploratory and exploitative phases of HHO will be applied to the first half, and the searching stages of SSA will be used to update the solutions in the second half. Thereafter, the best solutions from the union subpopulations are selected to continue the iterative process. According to the improved HHO, which is called HHOSSA, an effective multi-level image segmentation approach is also developed in this research. A comprehensive set of experiments are performed using 36 IEEE CEC 2005 benchmark functions and 11 natural gray-scale images. Extensive results and comparisons show the high ability of the SSA to improve the HHO's performance since the proposed HHOSSA achieves a more stable performance compared to HHO, SSA, and many other well-known methods. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106347	10.1016/j.asoc.2020.106347													
J								Assessment of compressive strength of Ultra-high Performance Concrete using deep machine learning techniques	APPLIED SOFT COMPUTING										Compressive strength; Ultra-High Performance Concrete; Deep machine learning; Sequential Feature selection; Neural Interpretation Diagrams	ARTIFICIAL NEURAL-NETWORK; REACTIVE POWDER CONCRETE; FEATURE-SELECTION; FATIGUE LIFE; RC BEAMS; MECHANICAL-PROPERTIES; MATERIAL EFFICIENCY; SHEAR-STRENGTH; PREDICTION; TENSILE	The compressive strength of Ultra-High Performance Concrete (UHPC) is a function of the type, property and quantities of its material constituents. Empirically capturing this relationship often requires the utilization of intelligent algorithms, such as the Artificial Neural Network (ANN), to derive a predictive model that fits into an experimental dataset. However, its black-box nature prevents researchers from mathematically describing its contents. This paper attempts to address this ambiguity by employing two deep machine learning techniques - Sequential Feature Selection (SFS) and Neural Interpretation Diagram (NID) - to identify the critical material constituents that affect the ANN. 110 UHPC compressive strength tests varying based on the material quantities were compiled into a database to train the ANN. As a result, four material constituents were selected; mainly, cement, fly ash, silica fume and water. These material constituents were then employed into the ANN to compute more accurate predictions (r(2) = 80.1% and NMSE = 0.012) than the model with all eight material constituents (r(2) = 21.5% and NMSE = 0.035). Finally, a nonlinear regression model based on the four selected material constituents was developed and a parametric study was conducted. It was concluded that the utilization of ANN with SFS and NID drastically improved the accuracy of the model, and provided valuable insights on the ANN compressive strength predictions for different UHPC mixes. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106552	10.1016/j.asoc.2020.106552													
J								An asynchronously deep reservoir computing for predicting chaotic time series	APPLIED SOFT COMPUTING										Reservoir computing; Time delay; Short-term memory; Chaotic; Time series	ECHO STATE NETWORK; MULTISTEP	Chaotic time series prediction is a research topic in both theoretical and real-life area. Its aim is to predict the future of the time series based on past observations. Reservoir computing (RC) is a promising tool widely used in time series prediction. Short-term memory (STM) is very important to model time-dependent time series by the RC approach. However, traditional RC hardly achieves sufficient STM capacity required by a complicated time series prediction task. For this reason, this paper proposes an asynchronous deep RC (ADRC), which is composed of a number of sub-reservoirs that are connected one by one in sequence. Moreover, delayed modules are inserted between every two adjacent sub-reservoirs. The sub-reservoirs in the proposed ADRC preserve the input characteristics by a relay mode and deal with them asynchronously. This makes the reservoir achieve large STM capacity and rich dynamics. The experimental results demonstrate that the proposed ADRC is prominent in modeling chaotic time series signals with high performance. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106530	10.1016/j.asoc.2020.106530													
J								Public opinion spread risk assessment model on third-party payment rough network	APPLIED SOFT COMPUTING										Third-party payment rough network; Public opinion spread; Game theory; Risk assessment	ALGORITHM	The spread of the public opinion in a third-party payment rough network may endanger the platform's economic stability. Hence, network public opinion spread risk assessment is urgently needed. Unfortunately, there has been no research on this problem. Based on research of the characteristics of the public opinion spread of a third-party payment rough network, we determined that the essence of the public opinion spread risk assessment in a network is to evaluate the important nodes and the risks of customer mining. The paper establishes a node-option model and node-search algorithm based on game theory. The game results show that managers select the customers who buy the degree nodes with the greatest weight as their public opinion customers, and they then perform risk assessment so that they can better determine the risk rating and index weights. The risk ratings and index weights can help managers control and adjust policies related to the spread of public opinion spread to prevent crises. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106532	10.1016/j.asoc.2020.106532													
J								Adaptive verifiability-driven strategy for evolutionary approximation of arithmetic circuits	APPLIED SOFT COMPUTING										Approximate computing; Energy efficiency; Circuit optimisation; Genetic programming	DESIGN; MULTIPLIERS	We present a novel approach for designing complex approximate arithmetic circuits that trade correctness for power consumption and play important role in many energy-aware applications. Our approach integrates in a unique way formal methods providing formal guarantees on the approximation error into an evolutionary circuit optimisation algorithm. The key idea is to employ a novel adaptive search strategy that drives the evolution towards promptly verifiable approximate circuits. As demonstrated in an extensive evaluation including several structurally different arithmetic circuits and target precisions, the search strategy provides superior scalability and versatility with respect to various approximation scenarios. Our approach significantly improves capabilities of the existing methods and paves a way towards an automated design process of provably-correct circuit approximations. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106466	10.1016/j.asoc.2020.106466													
J								Heart sound segmentation via Duration Long-Short Term Memory neural network	APPLIED SOFT COMPUTING										Heart sound segmentation; Long-Short Term Memory neural network; Sequence tagging	EXTRACTION; ENVELOPE; 1ST	Heart sound segmentation, which aims at detecting the first and second heart sound in phonocardiogram, is an essential step to automatically analyze heart valve diseases. Recently, the neural network-based methods have demonstrated their promising performance in segmenting the heart sound data. However, the methods also suffer from serious limitations due to the used envelope features. The reason is largely due to that the envelope features cannot effectively model the intrinsic sequential characteristic, resulting in the poor utilization of the duration information of heart cycles. In this paper, we propose a Duration Long-Short Term Memory network (Duration LSTM) to effectively address this problem by incorporating the duration features. The proposed method is investigated in the real-world phonocardiogram dataset (Massachusetts Institute of Technology heart sounds database) and compared with the two representatives of the existing state-of-the-art methods, the experimental results demonstrate that the proposed method has the promising performance on different tolerance windows. In addition, the proposed model also has some advantages in the impact of recording length and the phenomenon of the end effect. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106540	10.1016/j.asoc.2020.106540													
J								Identifying and prioritizing factors affecting in-cabin passenger comfort on high-speed rail in China: A fuzzy-based linguistic approach	APPLIED SOFT COMPUTING										Passenger comfort satisfaction; Quality function deployment; High-speed rail; Consistency checking; Consensus reaching	GROUP DECISION-MAKING; QUALITY FUNCTION DEPLOYMENT; PUBLIC TRANSPORT EVIDENCE; SERVICE QUALITY; CUSTOMER REQUIREMENTS; QFD METHODOLOGY; TERM SET; MODEL; SATISFACTION; INTEGRATION	Factors affecting customer comfort are crucial for the success of many services such as public transportation, health facilities, and so on. Therefore, the identification and prioritization of such factors is an important demand from stakeholders. This research aims to identify and prioritize the factors that affect in-cabin passenger comfort on high-speed rail (HSR) based on empirical evidence collected from China. For the identification process, the quality-management tool known as quality function deployment (QFD) to capture the voice of the customer is used to discover the most important demands as the driving influencing factors of in-cabin passenger comfort, by utilizing passengers' feedback regarding their experiences with HSR posted in social media. Such factors will be prioritized by a fuzzy linguistic group decision-making approach based on the adoption of generalized comparative linguistic expressions obtained from a questionnaire investigation on randomly selected HSR passengers, and accomplishing a decision-solving procedure that includes consistency-checking and consensus-reaching processes to achieve an effective, reliable, and agreed prioritization of the factors. The research outputs will suggest the factors of greatest concern among all HSR passenger demands. This study gives HSR operators and designers not only insights and tools for managing HSR passenger demands but also advice for refining the design and service quality of HSR in China. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106558	10.1016/j.asoc.2020.106558													
J								Human action recognition in videos based on spatiotemporal features and bag-of-poses	APPLIED SOFT COMPUTING										Human action recognition; Surveillance systems; Spatiotemporal features; Video sequences; Bag-of-poses	DESCRIPTORS	Currently, there is a large number of methods that use 2D poses to represent and recognize human action in videos. Most of these methods use information computed from raw 2D poses based on the straight line segments that form the body parts in a 2D pose model in order to extract features (e.g., angles and trajectories). In our work, we propose a new method of representing 2D poses. Instead of directly using the straight line segments, firstly, the 2D pose is converted to the parameter space in which each segment is mapped to a point. Then, from the parameter space, spatiotemporal features are extracted and encoded using a Bag-of-Poses approach, then used for human action recognition in the video. Experiments on two well-known public datasets, Weizmann and KTH, showed that the proposed method using 2D poses encoded in parameter space can improve the recognition rates, obtaining competitive accuracy rates compared to state-of-the-art methods. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106513	10.1016/j.asoc.2020.106513													
J								Two effective methods for the irregular knapsack problem	APPLIED SOFT COMPUTING										Two-dimensional Knapsack problem; Irregular shaped items; Biased random-key genetic algorithm; Variable neighborhood search; Bottom-left	VARIABLE NEIGHBORHOOD SEARCH; KEY GENETIC ALGORITHM; NESTING PROBLEMS; PROGRAMMING-MODELS; CIRCLES; NETWORK	Two methods are developed for a two-dimensional cutting problem with irregular shaped items. The concepts of inner-fit raster and no-fit raster are used to search for a feasible positioning of items on a rectangular container. The first method is a Biased Random Key Genetic Algorithm, which is a population method, while the other is a Variable Neighborhood Search, which is a single trajectory method. In the proposed methods, a solution is represented by a vector of items, and the positioning of items is achieved with three rules inspired by the bottom-left strategy. When positioning items, feasible positions can be skipped as a strategy to diversify the search and escape from local optima solutions. Numerical experiments performed on literature instances show that the methods are better than the current state-of-the-art method since they obtained equal or better solutions for all the instances. On average, the occupied area increased 6.44%, and the known optimal solution was obtained for 60% of the instances. The population-based method performed better overall, obtaining solutions with better-occupied areas. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106485	10.1016/j.asoc.2020.106485													
J								Testing of multi layered soil models based on data obtained from Finite Element Models with known soil structures using metaheuristics for parameters' determination	APPLIED SOFT COMPUTING										Grounding system; Finite Element Method; Metaheuristics; Soil models	LEARNING-BASED-OPTIMIZATION; DIFFERENTIAL EVOLUTION; ALGORITHM; FEM; SIMULATION; SYSTEMS; MUTATION; EARTH; FIELD	Grounding systems are an important part of protection systems, which protect people and devices in case of lightning strikes and defects in electro energetic systems. Grounding systems are often dimensioned using numerical models, among other numerical methods, also the Finite Element Method. Data about the soil in the surroundings of the grounding system are obtained using measurements. Soil parameters can be determined using analytical soil models. The determination of the soil models' parameters is an optimisation problem which is based on the measured data. In this paper, different soil models are tested on different data, and compared with each other. Test data are also obtained using finite element models of different soil structures, which offer better analysis of the soil models because the soil structure is known. The horizontally, vertically layered soil and inhomogeneity in the soil are modelled. Different metaheuristics are used and tested for the determination of soil parameters: A Genetic Algorithm, Differential Evolution with two different strategies, Artificial Bee Colony, Teaching-Learning Based Optimisation and a combination of Artificial Bee Colony and Teaching-Learning Based Optimisation. Analysis of the models and solving methods are made based on the test results. As a result, the appropriate soil models among those tested are selected, which are 4, 5 and 6 layered models, and appropriate methods for parameters' determination are presented, which are Artificial Bee Colony and a combination of Artificial Bee Colony and Teaching-Learning Based Optimisation. Also, analysis is made of the usefulness of the horizontally multi-layered model for differently structured soils. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106541	10.1016/j.asoc.2020.106541													
J								Journalistic transparency using CRFs to identify the reporter of newspaper articles in Spanish	APPLIED SOFT COMPUTING										Journalistic transparency; Conditional random fields; Entity extraction	NAMED ENTITY RECOGNITION; NEWS; EXTRACTION; TEXTS	Journalistic transparency rises as a key issue against the lack of credibility to which journalists are exposed, as well as the media manipulators and fake news providers. With the use of Natural Language Processing (NLP) and Machine Learning (ML), it is possible to automate the extraction of information from newspaper articles to know what the sources of information are to verify their veracity. Along with this article, we present the application of Conditional Random Fields (CRFs) for a specific type of Entity Recognition (ER) task, namely, to identify what we have called the "reporter" in newspaper articles, i.e., who or what is the provider of the information. Thus, we have created a labelled corpus for the Spanish language and trained and analysed several CRFs models with a set of specific features. The obtained results suppose a solid baseline for our goal. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106496	10.1016/j.asoc.2020.106496													
J								alpha-cut induced Fuzzy Deep Neural Network for change detection of SAR images	APPLIED SOFT COMPUTING										Change detection; alpha-cut; Fuzzy logic; Deep neural network; SAR images	UNSUPERVISED CHANGE DETECTION; INFORMATION; ALGORITHMS	Change detection (CD) is a process of identifying dissimilarities from two or more co-registered multitemporal images. In this paper, we have introduced a a-cut induced Fuzzy layer to the Deep Neural Network (alpha FDNN). Deep neural networks for change detection normally rely on the preclassified labels of the clustering. But the pre-classified labels are more coarse and ambiguous, which is not able to highlight the changed information accurately. This challenge can be addressed by encapsulating the local information and fuzzy logic into the deep neural network. This takes the advantage of enhancing the changed information and of reducing the effect of speckle noise. As the first step in change detection, a fused difference image is generated from the mean and log ratio image with the advent of Stationary Wavelet Transform (SWT). It not only eliminates the impact of speckle noise but also it has good ability to identify the trend of change thanks to the shift invariance property. Pseudo classification is performed as the next step using Fuzzy C Means (FCM) clustering. Then, we apply reformulated alpha-cut induced Fuzzy Deep Neural Network to generate the final change map which facilitates a final representation of data more suitable for the process of classification and clustering. It also results into a noteworthy improvement in the change detection result. The efficacy of the algorithm is analyzed through the parameter study. Experimental results on three Synthetic Aperture Radar (SAR) datasets demonstrate the superior performance of the proposed method compared to state-of-the art change detection methods. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106510	10.1016/j.asoc.2020.106510													
J								Cognitive visual anomaly detection with constrained latent representations for industrial inspection robot	APPLIED SOFT COMPUTING										Visual anomaly detection; Industrial inspection robot; Latent space representations; Cognitive computing		With the fast growth of intelligent manufacturing industry, developing advanced industrial inspection robots is becoming a research and application hotspot in the fields of both computer vision and robotics. This kind of industrial inspection robots is expected to automatically detect anomalous structures (e.g., defects, damages, rejects, etc.) from the images of the manufactured products. Generally, the existing visual anomaly detection (VAD) methods mainly focus on modeling the complex and high-dimensional distribution of normal data, while neglecting the specific visual properties of abnormal data since their frequency of occurrence is much less than that of the normal data. In this paper, inspired by the human cognition on extracting abstractly visual properties and to distinguish the anomaly patterns from the observed data, we propose a novel cognitive VAD method for industrial inspection robot. Specifically, we introduce a constrained latent space to mimic the cognitive ability of humans, where the abstraction learned from the observed normal and anomaly data are represented. We build our method based on a convolutional generative adversarial network and a denoising autoencoder, where the adversarial learning mechanism is adopted to establish the boundary between the normal and anomaly data. In the experiment, we evaluate our method on a real-world dataset where the images are captured for the manufactured products. The comprehensive results comparing with several recent VAD methods show that the proposed method is effective to detect the anomaly images of different categories with a high accuracy. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106539	10.1016/j.asoc.2020.106539													
J								Multi-reservoir echo state computing for solar irradiance prediction: A fast yet efficient deep learning approach	APPLIED SOFT COMPUTING										Echo state network; Multi-reservoir; Deep neural network; Solar irradiance prediction; Quantitative and qualitative analysis	RADIATION; OUTPUT; MULTIVARIATE; NETWORK; IMPACT; ONLINE; ENERGY; MODEL	Accurate solar irradiance prediction plays an important role in renewable energy systems. Based on time series analysis, a serially connected multi-reservoir echo state network (MR-ESN) is developed to predict solar irradiance. MR-ESN is a fast yet efficient approach, which makes use of the high efficiency of ESN and the advantages of deep learning. MR-ESN consists of multiple reservoirs in series, which are responsible for encoding the input signals into a richer state representation. The time series analysis is adopted to provide more appropriate input and output for MR-ESN. Various prediction horizons including one-hour-ahead and multi-hour-ahead prediction are conducted, respectively. The effect of reservoir layer number on the MR-ESN performance is explored in detail. Three internal qualitative indicators are adopted to investigate the performance differences of MR-ESN, i.e., probability distribution, correlation analysis, and principal component analysis (PCA) of network states. Simulation results demonstrate that MR-ESN outperforms than traditional ESN, backpropagation (BP) and Elman neural networks. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106481	10.1016/j.asoc.2020.106481													
J								An improved artificial bee colony algorithm for solving multi-objective low-carbon flexible job shop scheduling problem	APPLIED SOFT COMPUTING										Flexible job-shop scheduling problem; Artificial bee colony (ABC); Low-carbon; Dynamic neighborhood search; Multi-objective optimization	TOTAL WEIGHTED TARDINESS; ENERGY-CONSUMPTION; GENETIC ALGORITHM; OPTIMIZATION	Based on the analysis of multi-objective flexible job-shop scheduling problem (FJSP), a multi-objective low-carbon job-shop scheduling problem(MLFJSP) with variable processing speed constraint is proposed in this paper. The optimization objectives of MLFJSP include minimizing the makespan, total carbon emission and machine loading. Meanwhile, an improved artificial bee colony algorithm (IABC) is designed to solve the MLFJSP. The improvement of algorithm mainly includes: (1) an effective three-dimensions encoding/decoding mechanism and a mixed initialization strategy are designed to generate a better initial population; (2) special crossover operators and mutation operators were designed to increase the diversity of the population in the employed bee phase; (3)an efficient dynamic neighbor search (DNS) is applied to enhance local search capabilities in the onlooker bee phase; (4) the new food sources generation strategy was proposed to reduce the blindness in the scout bee phase. Finally, this paper carried out a series of comparative experimental studies, including the comparison before and after algorithm improvement, and the comparison between the improved algorithm with MOPSO, MODE and NSGA-II. The results demonstrate that the IABC can achieve a better performance for solving the MLFJSP. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106544	10.1016/j.asoc.2020.106544													
J								DeGAN: Mixed noise removal via generative adversarial networks	APPLIED SOFT COMPUTING										Mixed noise removal; Generative adversarial network; Joint loss function; Feature extractor network	IMPULSE NOISE; IMAGE; REDUCTION; FILTER	Restoration of images corrupted by mixed noise (e.g., additive white Gaussian noise and impulse noise) is very difficult due to the complexity of the mixed noise distribution. Various mixed noise removal models involve the preprocessing based on outlier detection. However, the performance of these models largely depends on the accuracy of pixel location detection of outliers, and artifacts and missing image details are prone to occur when the mixture noise is strong. In this paper, a new denoising model based on generative adversarial network (DeGAN) is proposed to remove mixed noise in images. The proposed model combines generator, discriminator, and feature extractor networks. Through the mutual game between the generator and discriminator networks combined with additional training from the feature extractor network, the generator network implements a direct mapping from the noisy image domain to the noise-free image domain. In addition, we design a new joint loss function to incorporate information from image features and human visual perception into the mixed noise elimination task, which further improves the image quality and the visual effect. Abundant experiments show that the performance of our model is better than the state-of-the-art mixed noise removal methods in three different types of mixed noise scenarios, and the joint loss function does improve the denoising performance. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106478	10.1016/j.asoc.2020.106478													
J								Particle swarm optimization-based strategy for detecting border-collision bifurcation points in piecewise smooth maps	APPLIED SOFT COMPUTING										Border-collision bifurcation; Bifurcation point detection; Bifurcation analysis; Initial value setup problem; Piecewise smooth maps; Particle swarm optimization (PSO)	CONVERGENCE; COMPUTATION; VALUES	A border-collision bifurcation point detection strategy based on particle swarm optimization (PSO) is proposed in this paper, and it facilitates a bifurcation point detection in piecewise smooth maps. This method is tested via detection experiments of border-collision bifurcation points in two popular piecewise smooth maps with one or more borders. The algorithm design and analysis part shows that the proposed algorithm is fairly simple, easily understandable, and easily implementable. The simulation results show that it accurately detects the border-collision parameters of the target period regardless of its stability, but requires no careful initialization or gradient information of the system. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106319	10.1016/j.asoc.2020.106319													
J								A new hybrid SSA-TA: Salp Swarm Algorithm with threshold accepting for band selection in hyperspectral images	APPLIED SOFT COMPUTING										Band selection; Salp Swarm Algorithm; Threshold Acceptance; Hybrid metaheuristic; Hyperspectral image classification	OPTIMIZATION ALGORITHM; CLASSIFICATION	Hyperspectral images classification is a primordial step to produce the Land Use maps. Unfortunately, the classification accuracy depends largely on the quality of spectral bands. Several bands are non-informative and the adjacent bands are generally highly correlated. This paper presents a novel band selection approach named SSA-TA based on Salp Swarm Algorithm (SSA) which a new metaheuristic recently developed and Threshold Acceptance(TA). The proposed approach SSA-TA is a hybrid metaheuristic used to select the relevant bands by eliminating the irrelevant and redundant bands to enhance the hyperspectral image classification. This work presents two main ideas. Firstly, we propose a hybridization model based on SSA and Threshold Acceptance (TA). The basic idea is using SSA to find the promising region and use TA to enhance the exploration of the best solution. Secondly, the fitness function is designed to take into consideration three important terms: (1) the maximization of classification accuracy rate (2) the minimization of the number of selected bands (3) the minimization of correlated bands. The performance evaluation of the proposed approach is tested on three hyperspectral images widely used on remote sensing. The proposed approach is compared to other algorithms. The experimental results demonstrate the efficiency of our approach in improving the classification accuracy rate. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106534	10.1016/j.asoc.2020.106534													
J								Hybrid SVM-CIPSO methods for optimal operation of reservoir considering unknown future condition	APPLIED SOFT COMPUTING										Optimal operation of reservoir; Prediction; Support vector machine; Improved particle swarm optimization algorithm; Artificial neural networks	PARTICLE SWARM OPTIMIZATION; SUPPORT VECTOR REGRESSION; ARTIFICIAL NEURAL-NETWORK; ANT COLONY OPTIMIZATION; WATER-RESOURCES; MONTHLY INFLOW; RIVER FLOW; ALGORITHM; MODEL; MACHINES	In this paper, new hybrid methods have been proposed to solve reservoir operation optimization problem for uncertain water inflows condition by equipping improved particle swarm optimization (IPSO) algorithm with support vector machine (SVM) method. The constrained version of IPSO algorithm (CIPSO) has been used here to improve the efficiency of the IPSO algorithm. In CIPSO algorithm, the problem constraints have been explicitly satisfied leading to smaller search space and finally smaller computational cost. Two approaches have been considered to propose the hybrid methods. In the first approach, named SVM-CIPSO1, water inflows into the dam reservoir have been predicted using SVM model and these predicted values have been used to solve reservoir operation optimization problem using CIPSO algorithm. However, in the second approach, named SVM-CIPSO2, at first, the CIPSO algorithm has been applied to solve reservoir operation optimization problem using the historical data and finally the optimal water release values have been used as input and output data to create a SVM model for predicting optimal water release from reservoir for the future condition. For comparison purpose, the ANN model has been also used to predict the water inflow or release values for the future condition and the standard form of IPSO algorithm has been also used to solve the optimization problem. Here, to evaluate the proposed approaches, the optimal water release values form Zayandehroud dam reservoir have been obtained using proposed methods and the reliability, resiliency, vulnerability and sustainability indexes have been computed. Comparison of the results indicates the capability of the proposed methods to predict the optimal water release values for future condition with acceptable accuracy. In other words, the RMSE values of SVM model for test, validation and training processes are 9.7104 (23. 56196), 11.2553 (42.69093), and 7.9556 (47.9346) MCM, respectively, which are obtained using second (first) approach. In addition, the best reliability, resiliency, vulnerability and sustainability index values are 51.95% (45.45%), 45.95% (38.10%), 3.539 (0.0041) MCM and 62.02% (55.74%), respectively, which are obtained using SVM-CIPSO2 (SVM-CIPSO1) method. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106572	10.1016/j.asoc.2020.106572													
J								A novel solution approach for multiobjective linguistic optimization problems based on the 2-tuple fuzzy linguistic representation model	APPLIED SOFT COMPUTING										Computing with words; Linguistic variable; Multi-objective linguistic optimization; Tsukamoto's fuzzy inference method; 2-tuple fuzzy linguistic approach	GROUP DECISION-MAKING; NUMERICAL SCALE; CONSENSUS; WORDS; SETS; GDM	This paper proposes a novel solution technique for the multi-objective linguistic optimization problems (MOLOPs) based on the 2-tuple fuzzy linguistic approach. The proposed approach has two main advantages. First, it can handle the MOLOPs in which the linguistic information are represented through either monotonic or non-monotonic functions. Second, for both the scenarios, it provides unique solutions in the linguistic form. On the other hand, the existing MOLOP solution approach which is based on the Tsukamoto's inference method, provides unique solutions only for those MOLOPs in which the linguistic information are expressed as monotonic functions. For the MOLOPs, in which the linguistic information are expressed as non-monotonic functions, the Tsukamoto's inference method based solution approach cannot provide unique solutions. Moreover, for both monotonic and non-monotonic cases, the Tsukamoto's inference method based solution approach cannot provide linguistic solutions, but gives only numeric solutions. We have demonstrated the applicability of the proposed MOLOP solution approach considering a case study on student's performance evaluation, and compared the results with the Tsukamoto's inference method based solution approach. It is observed that the proposed approach is capable of addressing the limitations of the Tsukamoto's inference method and hence is more suitable in solving MOLOPs. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106395	10.1016/j.asoc.2020.106395													
J								New metric learning model using statistical inference for kinship verification	APPLIED SOFT COMPUTING										Kinship verification; Metric learning; Statistical inference; Biometrics	FACE; SCALE	Kinship verification aims to predict whether there is a kin relationship between a pair of facial images. Previous representative research has proved that learning an appropriate similarity metric plays an important role in this task. Most of the related metric learning methods focus on learning a single Mahalanobis distance metric under the assumption that kin data has a unimodal distribution. Instead, in this paper, we propose a novel nonlinear multi-metric learning (NMML) method to learn a similarity measure where the pairwise difference is drawn from a mixture distribution. Particularly, the similarity measure is derived based on a statistical inference perspective, which can indicate the characteristics of a likelihood ratio that captures the subtle resemblance between identities. To make better use of multiple estimators, we further propose an ensemble NMML (ENMML) method to perform multiple estimators fusion to improve the performance of kinship verification. Extensive experiments on three publicly available kinship datasets demonstrate the feasibility and effectiveness of our proposed methods. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106569	10.1016/j.asoc.2020.106569													
J								High-performance differential evolution algorithm guided by information from individuals with potential	APPLIED SOFT COMPUTING										Differential algorithm; Potential; Parameter adaptation; Mutation strategy adaptation	MULTIPLE COMPARISONS; OPTIMIZATION; ENSEMBLE; PARAMETERS; STRATEGY; DESIGN	In the differential evolution (DE) algorithm, many adaptive methods have been studied in terms of fitness values. However, few studies exist on the information from individuals with potential, which presents a large difference in fitness values from that of previous individuals and contains much evolution information. This study proposes a high-performance DE (PDE) algorithm guided by information from individuals with potential. In PDE, all individuals are divided into individuals with potential and individuals without potential according to their improvement in fitness values. The experience learned from the generation of individuals with potential is used to guide future individuals. At each generation, the selection probability of each strategy in the strategy pool is determined by the strategy's contribution to the improvement in fitness values when generating individuals with potential. The parameters are randomly generated with two distributions, and the location parameters of the two distributions are adjusted on the basis of the improvement in fitness values of individuals with potential. Different individuals (with or without potential) may have different characteristics and evolution methods. Therefore, the generation process of individuals with potential is separated into two cases according to whether they are from previous individuals with or without potential. The study results of the two cases are applied to guide the evolution of current individuals with and without potential. The proposed algorithm is evaluated by comparing it with five advanced DE variants on CEC2005 and seven up-to-date evolutionary algorithms on CEC2014. Comparison results demonstrate the competitive performance of the proposed algorithm. The PDE is also applied to estimate the parameters of a kinetic model of p-xylene oxidation process. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106531	10.1016/j.asoc.2020.106531													
J								Pseudoinverse learning of Fuzzy Cognitive Maps for multivariate time series forecasting	APPLIED SOFT COMPUTING										Fuzzy Cognitive Maps; Learning; Time series; Forecasting	OPTIMIZATION; PREDICTION	Forecasting multivariate time series is an important problem considered in many real-world scenarios. To deal with that problem, several forecasting models have already been proposed, where Fuzzy Cognitive Maps (FCMs) are proved to be a suitable alternative. The key limitation of the existing FCM-based forecasting models is the lack of time-efficient learning algorithms. In this paper, we plug that gap by proposing a new FCM learning algorithm which is based on Moore-Penrose inverse. Moreover, we propose an innovative approach that equips FCM with long-term, multistep prediction capabilities. A huge advantage of our method is the lack of parameters which in the case of competitive approaches require laborious adjustment or tuning. The other added value of our method is the reduction of the processing time required to train FCM. The performed experiments revealed that FCM trained using our method outperforms the best FCM-based forecasting model reported in the literature. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106461	10.1016/j.asoc.2020.106461													
J								An Atanassov intuitionistic fuzzy programming method for group decision making with interval-valued Atanassov intuitionistic fuzzy preference relations	APPLIED SOFT COMPUTING										Atanassov interval-valued intuitionistic fuzzy preference relation; Group decision making; Atanassov intuitionistic fuzzy program	MULTIPLICATIVE CONSISTENCY; SETS; FRAMEWORK; MODEL; AHP	The focus of this paper is on group decision making (GDM) problems with interval-valued Atanassov intuitionistic fuzzy preference relations (IV-AIFPRs). A new consistency index of an AIFPR is introduced to check the additive consistency degree of an AIFPR. Then, an additive consistency definition and an acceptable additive consistency definition of an IV-AIFPR are respectively defined by splitting an IV-AIFPR into two AIFPRs. For several IV-AIFPRs with unacceptably additive consistency, a goal program-based approach is proposed to improve their consistency simultaneously. Employing consistency degrees of individual IV-AIFPRs, decision makers' (DMs') weights are determined objectively and applied to integrate individual IV-AIFPRs into a collective one. Further, it is proved that the collective IV-AIFPR is acceptably additive consistent if all individual IV-AIFPRs are acceptably additive consistent. To derive priority weights of alternatives, an Atanassov intuitionistic fuzzy programming model is established and solved by three approaches considering DMs' different risk attitudes. Thus, a novel method is put forward for GDM with IV-AIFPRs. A material selection example is analyzed to verify the effectiveness of the proposed method. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106556	10.1016/j.asoc.2020.106556													
J								An input shaping based active vibration control and adaptive RBF impedance control for suppressing the myospasm in upper-limb rehabilitation	APPLIED SOFT COMPUTING										Rehabilitation manipulator; Upper limb; Input shaping; Vibration control; Impedance control; Myospasm	ROBOT	During the upper limb rehabilitation training exercise, the stroke patients always suffers from the myospasm. The myospasm may last from several seconds to tens of seconds, causing the vibration of the flexible rehabilitation manipulators as well as exerting a sudden force on the end of the rehabilitation manipulator. It is harmful to both of patients and the upper limb rehabilitation manipulator. In this paper, an input shaping based active vibration control is proposed to suppress the vibration in order to guarantee the stability of the flexible upper limb rehabilitation system. In the meantime, an adaptive RBF impedance control is applied to compensate the sudden force caused by the myospasm so as to ensure the safety. The proposed method is analyzed by Lyapunov stability. Numerical simulations are performed and results show the effectiveness of the proposed method. (C) 2020 Published by Elsevier B.V.																	1568-4946	1872-9681				OCT	2020	95								106380	10.1016/j.asoc.2020.106380													
J								A flight maneuver recognition method based on multi-strategy affine canonical time warping	APPLIED SOFT COMPUTING										UCAV flight maneuver recognition; Multi-strategy LSHADE; Affine canonical time warping; Segmentation	SERIES; ALGORITHM; CLASSIFICATION; OPTIMIZATION	Maneuver recognition for unmanned combat air vehicle (UCAV) is a necessary technique for autonomous air combat. As a spatiotemporal alignment problem of multidimensional time series, the flight maneuver recognition is solved by a novel alignment measure, multi-strategy affine canonical time warping approach (MACTW) and its derivative form, which are extensions of affine canonical time warping (ACTW). MACTW makes several contributions: (1) it proposes multi-strategy success-history based adaptive differential evolution algorithm with linear population size reduction (MLSHADE) to accelerate the search of warping path of dynamic time warping (DTW); (2) it introduces affine strategy to address offset and scaling of canonical time warping (CTW), which is a combination of DTW and canonical correlation analysis (CCA); and (3) it extends ACTW based on MLSHADE to align multidimensional time series In addition, MLSHADE is a novel optimization technique that employs weighted mutation, inferior solution search, and eigen Gaussian walk strategies to improve the optimization efficiency. The experimental results on the CEC 2018 test suite illustrate the superior benefits of MLSHADE. UCAV flight maneuver recognition system which includes segmentation, preprocessing and recognition modules is modeled. The experimental results on UCR datasets and UCAV maneuver datasets including action units and long maneuver datasets illustrate the superiority of MACTW and its derivative form compared with other state-of-the-art alignment measures. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106527	10.1016/j.asoc.2020.106527													
J								A novel two-stage method for matching the technology suppliers and demanders based on prospect theory and evidence theory under intuitionistic fuzzy environment	APPLIED SOFT COMPUTING										Technology commercialization; Two-sided matching; Intuitionistic fuzzy number; Prospect theory; Evidence theory	DECISION-MAKING; RANDOM-PATHS; STABILITY; VALUATION; MARKETS	Matching the technology suppliers and demanders in the market is essential because such a matching facilitates the technology commercialization. Under the intuitionistic fuzzy environment, this paper proposes a novel two-stage matching method to match the technology suppliers and demanders by fully considering the characteristics of the real matching process. In the first stage, following the prospect theory and evidence theory, the satisfactions of the two parties with the attributes' performances are calculated. These satisfactions are further used as the basis for the ranking of cooperation preferences. In the second stage, the price satisfaction under the influence of the cooperation preferences is considered, and the final matching pairs are obtained by maximizing the price satisfaction. We also conduct a numerical example to show the application of the novel two-stage method and compare its performance with the single-stage method The results show that the two-stage method can better identify both sides' cooperation intentions and enhance their transaction satisfactions. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106553	10.1016/j.asoc.2020.106553													
J								A new grey prediction model and its application to predicting landslide displacement	APPLIED SOFT COMPUTING										Mathematical modeling; Prediction; Grey theory; Engineering application; Landslide displacement	ENERGY-CONSUMPTION; RANDOM FOREST; FAILURE; TIME; CLASSIFICATION; TREE	Landslide displacement prediction is an important part of reducing landslide hazard losses. The existing methods to predict landslide displacement are too complicated to be applied to engineering practice. The landslide displacement evolution and the grey model prediction mechanism show a good consistency. However, the existing grey prediction model also has some shortcomings including neglecting time term. A new grey prediction model called the background value optimization nonlinear grey prediction model (BNGM(1,1, t(2))) is proposed to overcome the deficiencies. BNGM(1,1, t(2)) is a univariate prediction model that incorporates the influence of the time term. The integration method is used to determine the background value, and the minimum value method is employed to obtain a constant term of time response functions. Five accuracy test methods for BNGM(1,1, t(2)) are examined. BNGM(1,1, t(2)) can show better performances than other multivariate prediction models including the recursive discrete multivariate grey prediction model. BNGM(1,1, t(2)) is applied to four typical landslide case studies. The results indicate that the BNGM(1,1, t(2)) has the best prediction accuracy. The complexity of BNGM(1,1, t(2)) is lower than the nonlinear grey Bernoulli model, the Weibull-Bernoulli grey model, and the fractional accumulation nonlinear grey Bernoulli model. The comparison of comprehensive results demonstrates that the BNGM(1,1, t(2))-based method has a wide application potential to predict landslide displacement. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106543	10.1016/j.asoc.2020.106543													
J								Predicting China's energy consumption using a novel grey Riccati model	APPLIED SOFT COMPUTING										Grey Riccati model; Energy consumption; Simulated annealing algorithm; Genetic algorithm; Optimized parameter	FORECASTING-MODEL; BASS MODEL; OPTIMIZATION; ALGORITHMS; GROWTH	This paper studies the China's oil consumption and the China's nuclear energy consumption by a grey Riccati model. The newly developed model is analysed by the trapezoidal formula of definite integrals, the theory of ordinary differential equations and the grey technique. And some special cases including the GM(1,1) model, the grey Verhulst model and the grey Bass model are all discussed. Meanwhile, the hybrid of the simulated annealing algorithm and the genetic algorithm is utilized to search optimal background values. Further, the performance of the new model is verified through some experiments. Finally, the model is applied to study China's energy consumption with original sequences from 2001 to 2018 claimed by British Petroleum Statistical Review of World Energy 2019, and the results show that the new model can obtain competitive results and better than other comparative models. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106555	10.1016/j.asoc.2020.106555													
J								Parameter optimization for nonlinear grey Bernoulli model on biomass energy consumption prediction	APPLIED SOFT COMPUTING										Box-Cox transformation; BC-NGBM(1,1) model; Biomass energy; Quantum adiabatic evolution	NATURAL-GAS CONSUMPTION; FORECASTING-MODEL; QUANTUM GATES; ALGORITHM; SPIN	Nonlinear Grey Bernoulli Model (NGBM(1,1)) and its derivative model utilize the specific power exponent function to manifest the nonlinear characteristics of the energy consumption data pattern. Because the modeling constraint conditions and the data processing mechanism are rarely considered in parameter optimization of NGBM(1,1) the aim of this paper is just to establish a novel NGBM(1,1) optimization model with constraints using Box-Cox transformation (BC-NGBM*), in which the constraint conditions of the power index in the power function transformation are discussed according to the principle of difference information and the data processing mechanism. Parameter optimization of BC-NGBM* would be solved collectively using Quantum Adiabatic Evolution (QAE) algorithm. 143 data sets from M4-competition are studied for confirming the effectiveness of BC-NGBM* with QAE algorithm Finally, using data from 2010 to 2018, BC-NGBM* is used to forecast biomass energy consumption in China, the United States, Brazil, and Germany. The proposed model demonstrates high accuracy in all cases and is efficient for short-term biomass energy consumption forecasting. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106538	10.1016/j.asoc.2020.106538													
J								Energy-efficient steelmaking-continuous casting scheduling problem with temperature constraints and its solution using a multi-objective hybrid genetic algorithm with local search	APPLIED SOFT COMPUTING										Energy-efficient scheduling; Steelmaking-continuous casting; Temperature; Multi-objective optimization; Local search	LAGRANGIAN-RELAXATION APPROACH; FLOW-SHOP; CONSUMPTION; TIME; OPTIMIZATION; TARDINESS	With the increasing energy price and the urgent demand of manufacturing enterprises for energy conservation, energy-efficient scheduling (EES) technology has been widely investigated and applied in industry and academia. The steelmaking-continuous casting (SCC) production is the main energy-consuming sector and the key process for quality control of steel manufacturing. Due to the high-temperature characteristics of SCC production, the temperature drop deriving from nonprocessing process could directly lead to energy loss and increase the energy consumption of each procedure, which has important influence on the total energy consumption. Therefore, the energy-efficient steelmaking-continuous casting scheduling problem with temperature constraints (EESCCSPT) was concerned and a multi-objective mathematical programming model was introduced to minimize the penalty of due date deviation and the extra energy consumption measured by temperature drop. Comparing to the general scheduling problem, the constraint of minimum casting superheat and the constraint of target tapping temperature generated by the high-temperature technical requirements were directly considered to ensure schedule feasibility in terms of temperature. A multi-objective hybrid genetic algorithm combined with local search (MOHGALS) was presented, in which the enhanced evolutionary mechanisms combined with the improved genetic operators and the local search were also designed. Results of computational experiments showed that MOHGALS was more feasible and effective than NSGA-II and SPEA2 on the EESCCSPT. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106554	10.1016/j.asoc.2020.106554													
J								A multi-task Faster R-CNN method for 3D vehicle detection based on a single image	APPLIED SOFT COMPUTING										Vehicle detection; CNN; Geometric constraints; Perspective projection; Autonomous driving		Vehicle detection is an important part of robot environmental perception. In this paper, a 3D vehicle detection method using a single image is proposed to generate the 3D space coordinate information of the object using monocular vision for autonomous driving. The proposed method works under the multi-task framework and integrates 2D object detection, 3D object detection, orientation estimation and key point detection into one unified deep convolution neural network (DCNN) which could be trained by end-to-end learning. Besides, our proposed method is built by modifying Fast R-CNN using multi-task learning, and thus our proposed method is named multi-task Faster R-CNN (MT-Faster R-CNN). The experiments on KITTI dataset are conducted to evaluate our proposed method and the other 3D vehicle detection methods. The experimental results demonstrate that our proposed method is competitive and could significantly assist autonomous driving. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106533	10.1016/j.asoc.2020.106533													
J								Feature selection considering Uncertainty Change Ratio of the class label	APPLIED SOFT COMPUTING										Classification; Feature selection; Information theory; Uncertainty Change Ratio	MUTUAL INFORMATION; DIFFERENTIAL EVOLUTION; CLASSIFICATION; ALGORITHMS; RELEVANCE	The topic of feature selection in high-dimensional data sets has attracted considerable attention. Feature selection can reduce the dimension of feature and improve the prediction accuracy of the classification model. Information-theoretical-based feature selection methods intend to obtain classification information regarding class labels from the already-selected feature subset as much as possible. Existing methods focus on the reduced uncertainty of class labels while ignoring the change of the remained uncertainty of class labels. In the process of feature selection, the large reduced uncertainty of class labels does not signify the few remained uncertainty of class labels when different candidate features are given. In this paper, we analyze the difference between the reduced uncertainty of class labels and the remained uncertainty of class labels and propose a new term named Uncertainty Change Ratio that considers the change of uncertainty of class labels. Finally, a novel method named Feature Selection considering Uncertainty Change Ratio (UCRFS) is proposed. To prove the classification superiority of the proposed method, UCRFS is compared to three traditional methods and four state-of-the-art methods on fourteen benchmark data sets. The experimental results demonstrate that UCRFS outperforms seven other methods in terms of average classification accuracy, AUC and Fl score. (C) 2020 Elsevier B.V. All rights reserved.																	1568-4946	1872-9681				OCT	2020	95								106537	10.1016/j.asoc.2020.106537													
J								Demographic classification through pupil analysis	IMAGE AND VISION COMPUTING										Biometrics; Pupil analysis; Gaze analysis	EYE-MOVEMENTS; ADABOOST; DIAMETER; SIZE; AGE	An area of biometrics that has recently attracted much attention is gender and age classification. Its applications can be found not only in the fields of security and surveillance, but also in the context of marketing and demographic information gathering. In addition, extracting this information from a biometric sample can help to decrease the time to identify the exact individual. In this paper, we exploit pupil size as a discriminating feature for the estimation of gender and age. Data obtained from the free observation of face images have been used to train two classifiers (Adaboost and SVM), considering both the best results produced by each classifier and their fusion through weighted means. With experiments involving more than 100 participants, we have found that pupil size can provide significant results, better than those achievable using data on fixations and gaze paths. Pupil Diameter Mean (PDM) has proved to be the best discriminating feature for both gender and age. To the best of our knowledge, there are no other studies trying to perform such a classification using pupil size only. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				OCT	2020	102								103980	10.1016/j.imavis.2020.103980													
J								Application of the best evacuation model of deep learning in the design of public structures	IMAGE AND VISION COMPUTING										Evacuation; Deep learning; VR video tracking method; YOLO-based recursive neural network model; Simulation		Evacuation behavior is an important factor which must be considered in the design of public structures. With the continuous complexity of structure, more and more factors should be considered in evacuation. The traditional design based on experience may have some limitations in practice. Based on the deep neural network model, the evacuation design simulation for subway station buildings is implemented. VR video tracking technologies such as auxiliary image data pre-training algorithm, tracking sequence pre-training algorithm, and recursive neural network model based on You Only Look Once (YOLO) arc introduced. Compared with the convolutional neural network (CNN) model, the classified data set pre-training model, and YOLO algorithm, the accuracy and training speed of the model algorithm are verified. In simulation, the Zhujiang New Town Station in Guangzhou is taken as the object. The initial evacuation test point is selected according to the structure of the subway platform, and the test personnel are selected according to the test requirements. The average evacuation time and the average satisfaction score of the testers under the influence factors such as gender, age, subway frequency, and familiarity with VR equipment, as well as under the initial starting points of different evacuation tests. The results show that the accuracy of the algorithm is lower than that of the CNN, but the training speed is faster. The accuracy of the model based on YOLO recurrent neural network is the highest. Although the training speed is 19 ms, which is higher than other models, the overall performance is the best. Differences in factors such as gender, age, frequency of subway ride, and familiarity with VR devices will result in different differences in average evacuation time and average satisfaction score. When the platform center is used as the initial evacuation point, the average evacuation time is the shortest, and the average satisfaction score of the testers is the highest. In conclusion, through VR video tracking technology, the actual situation of subway station buildings can be well simulated, and further design schemes can be made according to the simulated situation, which has practical reference significance. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				OCT	2020	102								103975	10.1016/j.imavis.2020.103975													
J								Investigating bias in deep face analysis: The KANFace dataset and empirical study	IMAGE AND VISION COMPUTING										Dataset bias; Face recognition; Age estimation; Gender recognition; Kinship verification	KINSHIP; GENDER; AGE	Deep learning-based methods have pushed the limits of the state-of-the-art in face analysis. However, despite their success, these models have raised concerns regarding their bias towards certain demographics. This bias is inflicted both by limited diversity across demographics in the training set, as well as the design of the algorithms. In this work, we investigate the demographic bias of deep learning models in face recognition, age estimation, gender recognition and kinship verification. To this end, we introduce the most comprehensive, large-scale dataset of facial images and videos to date. It consists of 40K still images and 44K sequences (14.5M video frames in total) captured in unconstrained, real-world conditions from 1,045 subjects. The data are manually annotated in terms of identity, exact age, gender and kinship. The performance of state-of-the-art models is scrutinized and demographic bias is exposed by conducting a series of experiments. Lastly, a method to debias network embeddings is introduced and tested on the proposed benchmarks. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				OCT	2020	102								103954	10.1016/j.imavis.2020.103954													
J								Implementing cascaded regression tree-based face landmarking: An in-depth overview	IMAGE AND VISION COMPUTING										Face landmarking; Cascaded regression; Regression trees; Temporal tracking	ALIGNMENT	Face landmarking, defined as the detection of fiducial points on faces, has received a lot of attention over the last two decades within the computer vision community. While research literature documents major advances using state-of-art deep convolutional neural networks, earlier cascaded regression tree-based approaches remain a relevant alternative for low-cost, low-power embedded systems. Yet, from a practical point of view, their implementation and parametrization can be a difficult and tedious process. In this paper, we provide the readers with insights and advice on how to design a successful face landmarking system using a cascade of regression trees. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				OCT	2020	102								103976	10.1016/j.imavis.2020.103976													
J								Multimodal facial biometrics recognition: Dual-stream convolutional neural networks with multi-feature fusion layers	IMAGE AND VISION COMPUTING										Multimodal facial biometrics recognition; Deep multimodal learning; Dual-stream convolutional neural network; Network fusion layers		Facial recognition for surveillance applications still remains challenging in uncontrolled environments, especially with the appearances of masks/veils and different ethnicities effects. Multimodal facial biometrics recognition becomes one of the major studies to overcome such scenarios. However, to cooperate with multimodal facial biometrics, many existing deep learning networks rely on feature concatenation or weight combination to construct a representation layer to perform its desired recognition task. This concatenation is often inefficient, as it does not effectively cooperate with the multimodal data to improve on recognition performance. Therefore, this paper proposes using multi-feature fusion layers for multi modal facial biometrics, thereby leading to significant and informative data learning in dual-stream convolutional neural networks. Specifically, this network consists of two progressive parts with distinct fusion strategies to aggregate RGB data and texture descriptors for multimodal facial biometrics. We demonstrate that the proposed network offers a discriminative feature representation and benefits from the multi-feature fusion layers for an accuracy-performance gain. We also introduce and share a new dataset for multimodal facial biometric data, namely the Ethnic-facial dataset for benchmarking. In addition, four publicly accessible datasets, namely AR. FaceScrub, IMDB_WIKI, and YouTube Face datasets are used to evaluate the proposed network. Through our experimental analysis, the proposed network outperformed several competing networks on these datasets for both recognition and verification tasks. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				OCT	2020	102								103977	10.1016/j.imavis.2020.103977													
J								Non-local attention association scheme for online multi-object tracking	IMAGE AND VISION COMPUTING										Data association; Multi-object tracking; Non-local attention		Online multi-object tracking (MOT) is a fundamental problem in video analysis and multimedia applications. The major challenge in the popular tracking-by-detection framework is knowing how to associate candidate detections results with existing tracklets. In this, we propose a non-local attention association approach and apply it to a unified online MOT framework that integrates the merits of single object tracking and data association methods. Specifically, we use non-local attention association networks (NAAN) to incorporate both spatial and temporal characteristics to associate new detections. The non-local attention mechanism generates global attention maps across space and time, enabling the network to focus on the whole tracklet information, as opposed to the local attention mechanism to overcome the problems of noisy detections, occlusion, and frequent interactions between targets. Experimental results on MOT benchmark datasets show that the proposed algorithm performs favorably against various online trackers on the basis of identity-preserving metrics. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				OCT	2020	102								103983	10.1016/j.imavis.2020.103983													
J								An attention-based deep learning model for multiple pedestrian attributes recognition	IMAGE AND VISION COMPUTING										Pedestrian attributes recognition; Multi-task learning; Visual surveillance		The automatic characterization of pedestrians in surveillance footage is a tough challenge, particularly when the data is extremely diverse with cluttered backgrounds, and subjects are captured from varying distances, under multiple poses, with partial occlusion. Having observed that the state-of-the-art performance is still unsatisfactory, this paper provides a novel solution to the problem, with two-fold contributions: 1) considering the strong semantic correlation between the different full-body attributes, we propose a multi-task deep model that uses an element-wise multiplication layer to extract more comprehensive feature representations. In practice, this layer serves as a filter to remove irrelevant background features, and is particularly important to handle complex, cluttered data; and 2) we introduce a weighted-sum term to the loss function that not only relativizes the contribution of each task but also is crucial for performance improvement in multiple-attribute inference settings. Our experiments were performed on two well-known datasets (RAP and PETA) and point for the superiority of the proposed method with respect to the state-of-the-art. The code is available at https://github.com/EhsanYaghoubi/MAN-PAR-. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				OCT	2020	102								103981	10.1016/j.imavis.2020.103981													
J								A two-stage real-time YOLOv2-based road marking detector with lightweight spatial transformation-invariant classification	IMAGE AND VISION COMPUTING										Deep learning; Road marking; Spatial transform; Real-time object detection; Object classification	BASE-LINE	In recent years, Autonomous Driving Systems (ADS) become more and more popular and reliable. Road markings are important for drivers and advanced driver assistance systems by better understanding the road environment. While the detection of road markings may suffer a lot from various illuminations, weather conditions and angles of view, most traditional road marking detection methods use fixed threshold to detect road markings, which is not robust enough to handle various situations in the real world. To deal with this problem, some deep learning-based real-time detection frameworks such as Single Shot Detector (SSD) and You Only Look Once (YOLO) are suitable for this task. However, these deep learning-based methods are data-driven even while there is no public road marking dataset. Besides, these detection frameworks usually struggle with distorted road markings and balancing between the precision and recall. We propose a two-stage YOLOv2-based network to tackle distorted road marking detection as well as to balance precision and recall. The proposed spatial transformer layer is able to handle the distorted road markings in the second stage, so as to achieve the improvement of precision. Our network is able to run at 58 FPS in a single GTX 1070 under diverse circumstances. Furthermore, we present a dataset for the public use of road marking detection tasks, which consists of 11,800 high-resolution images captured under different weather conditions. Specifically, the images are manually annotated into 13 classes with bounding boxes. We empirically demonstrate both mean average precision ( mAP) and detection speed of our system over several baseline models. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				OCT	2020	102								103978	10.1016/j.imavis.2020.103978													
J								Application of 3D laser scanning technology for image data processing in the protection of ancient building sites through deep learning	IMAGE AND VISION COMPUTING										3D laser measuring technology; Deep learning; Point cloud data; 3D model construction; Chinese traditional architecture; Architectural heritage		To study the conservation of architectural heritage, computer algorithms are used to process the artificial field measurements and reference data maps. A 3D virtual model of traditional architecture is constructed for the direct learning and feature extraction from image data through deep learning. A set of tools and processes for pixellevel image processing labeling arc proposed, and a label quality checking tool is written specially. Through the training method of transfer learning, the convergence speed and accuracy of the network are accelerated and improved. Based on the field data collection of the architectural heritage by a 3D laser scanner, the impacts of the number of stations, the number of targets, and the distance on the data scanning results are analyzed, thereby drawing the rules and principles of setting up the survey stations. While processing the point cloud data, for the redundant data and the rough difference points found in the original point cloud data, the program is written by the gross error elimination algorithm, which realizes the automatic elimination of the point cloud gross error data and provides a convenient method for data processing. The data collection, data processing, and model construction of architectural heritage are performed by the 3D laser scanner. The 3D models of traditional architecture with texture photos are obtained. The results show that through correlation error analysis and model evaluation, the requirements for the measurement of traditional architecture can be achieved. Therefore, the technology has a guiding significance for the conservation of traditional architecture, which proves that the 3D laser scanner has broad application prospects in the surveying and mapping of architectural heritage. (C) 2020 Published by Elsevier B.V.																	0262-8856	1872-8138				OCT	2020	102								103969	10.1016/j.imavis.2020.103969													
J								Adversarial sliced Wasserstein domain adaptation networks	IMAGE AND VISION COMPUTING										Transfer learning; Domain adaptation; Image classification; Adversarial learning		Domain adaptation has become a resounding success in learning a domain agnostic model that performs well on target dataset by leveraging source dataset which has related data distribution. Most of existing works aim at learning domain-invariant features across different domains, but they ignore the discriminability of learned features although it is import to improve the model's performance. This paper proposes a novel adversarial sliced Wasserstein domain adaptation network (AWDAN) that uses a shared encoder and classifier along with a domain classifier to enhance the discriminability of the domain-invariant features. AWDAN utilizes adversarial learning to learn domain-invariant features in feature space and simultaneously minimizes sliced Wasserstein distance in label space to enforce the generated features to be discriminative that guarantees the transfer performance. Meanwhile, we propose to fix the weights of the pre-trained CNN backbone to guarantee its adaptability. We provide theoretical analysis to demonstrate the efficacy of AWDAN. Experimental results show that the proposed AWDAN significantly outperforms existing domain adaptation methods on three visual domain adaptation tasks. Feature visualizations verify that AWDAN learns both domain-invariant and discriminative features, and can achieve domain agnostic feature learning. (C) 2020 Elsevier B.V. All rights reserved.																	0262-8856	1872-8138				OCT	2020	102								103974	10.1016/j.imavis.2020.103974													
J								Observer-Based Impulsive Synchronization for Neural Networks With Uncertain Exchanging Information	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Artificial neural networks; Synchronization; Observers; Neurons; Multi-agent systems; Delays; Protocols; Distributed impulsive observer; impulsive control; neural networks (NNs); synchronization; uncertain exchanging information	DYNAMICAL NETWORKS; SYSTEMS; TIME; CONSENSUS; ROBUST; STABILITY; DISCRETE; DELAYS	This article investigates synchronization for a group of discrete-time neural networks (NNs) with the uncertain exchanging information, which is caused by the uncertain connection weights among the NNs nodes, and they are transformed into a norm-bounded uncertain Laplacian matrix. Distributed impulsive observers, which possess the advantage of reducing the communication load among NNs nodes, are designed to observe the NNs state. The impulsive controller is proposed to improve the efficiency of the controller. An impulsive augmented error system (IAES) is obtained based on the matrix Kronecker product. A sufficient condition is established to ensure synchronization of the group of NNs by proving the stability of the IAES. An iterative algorithm is given to obtain a suboptimal allowed interval of the impulsive signal, and the corresponding gains of the observer and the controller are derived. The developed result is illustrated by a numerical example.																	2162-237X	2162-2388				OCT	2020	31	10					3777	3787		10.1109/TNNLS.2019.2946151													
J								State-Saturated Recursive Filter Design for Stochastic Time-Varying Nonlinear Complex Networks Under Deception Attacks	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Couplings; Upper bound; Symmetric matrices; Stochastic processes; Complex networks; Covariance matrices; Complexity theory; Complex networks (CNs); deception attacks; nonlinear time-varying systems; recursive filtering; state saturations	DATA INJECTION ATTACKS; NEURAL-NETWORKS; SYSTEMS; SECURITY	This article tackles the recursive filtering problem for a class of stochastic nonlinear time-varying complex networks (CNs) suffering from both the state saturations and the deception attacks. The nonlinear inner coupling and the state saturations are taken into account to characterize the nonlinear nature of CNs. From the defender's perspective, the randomly occurring deception attack is governed by a set of Bernoulli binary distributed white sequence with a given probability. The objective of the addressed problem is to design a state-saturated recursive filter such that, in the simultaneous presence of the state saturations and the randomly occurring deception attacks, a certain upper bound is guaranteed on the filtering error covariance, and such an upper bound is then minimized at each time instant. By employing the induction method, an upper bound on the filtering error variance is first constructed in terms of the solutions to a set of matrix difference equations. Subsequently, the filter parameters are appropriately designed to minimize such an upper bound. Finally, a numerical simulation example is provided to demonstrate the feasibility and usefulness of the proposed filtering scheme.																	2162-237X	2162-2388				OCT	2020	31	10					3788	3800		10.1109/TNNLS.2019.2946290													
J								Structured Optimal Graph-Based Clustering With Flexible Embedding	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Clustering algorithms; Bipartite graph; Laplace equations; Clustering methods; Mathematical model; Task analysis; Manifolds; Bipartite graph; coclustering; flexible embedding; unsupervised learning		In the real world, the duality of high-dimensional data is widespread. The coclustering method has been widely used because they can exploit the co-occurring structure between samples and features. In fact, most of the existing coclustering methods cluster the graphs in the original data matrix. However, these methods fail to output an affinity graph with an explicit cluster structure and still call for the postprocessing step to obtain the final clustering results. In addition, these methods are difficult to find a good projection direction to complete the clustering task on high-dimensional data. In this article, we modify the flexible manifold embedding theory and embed it into the bipartite spectral graph partition. Then, we propose a new method called structured optimal graph-based clustering with flexible embedding (SOGFE). The SOGFE method can learn an affinity graph with an optimal and explicit clustering structure and does not require any postprocessing step. Additionally, the SOGFE method can learn a suitable projection direction to map high-dimensional data to a low-dimensional subspace. We perform extensive experiments on two synthetic data sets and seven benchmark data sets. The experimental results verify the superiority, robustness, and good projection direction selection ability of our proposed method.																	2162-237X	2162-2388				OCT	2020	31	10					3801	3813		10.1109/TNNLS.2019.2946329													
J								Deep Learning Method Based on Gated Recurrent Unit and Variational Mode Decomposition for Short-Term Wind Power Interval Prediction	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Wind power generation; Logic gates; Predictive models; Prediction algorithms; Training; Data models; Deep learning; Deep learning; gated recurrent unit (GRU); variational mode decomposition (VMD); wind power interval prediction (WPIP)	MEMORY NETWORK; SPEED; CONSTRUCTION; OPTIMIZATION; MACHINE	Wind power interval prediction (WPIP) plays an increasingly important role in evaluations of the uncertainty of wind power and becomes necessary for managing and planning power systems. However, the intermittent and fluctuating characteristics of wind power mean that high-quality prediction intervals (PIs) production is a challenging problem. In this article, we propose a novel hybrid model for the WPIP based on the gated recurrent unit (GRU) neural networks and variational mode decomposition (VMD). In the hybrid model, VMD is employed to decompose complex wind power data into simplified modes. Basic GRU prediction models, comprising a GRU input layer, multiple fully connected layers, and a rank-ordered terminal layer, are then trained for each mode to produce PIs, which are combined to obtain final PIs. In addition, an adaptive optimization method based on constructed intervals (CIs) is proposed to build high-quality training labels for supervised learning with the hybrid model. Several numerical experiments were implemented to validate the effectiveness of the proposed method. The results indicate that the proposed method performs better than the traditional interval prediction models with much higher quality PIs, and it requires less training time.																	2162-237X	2162-2388				OCT	2020	31	10					3814	3827		10.1109/TNNLS.2019.2946414													
J								Compressing Deep Neural Networks With Sparse Matrix Factorization	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Sparse matrices; Matrices; Indexes; Neural networks; Task analysis; Bayes methods; Learning systems; Deep neural networks (DNNs); memory efficiency; network compression; sparse representation		Modern deep neural networks (DNNs) are usually overparameterized and composed of a large number of learnable parameters. One of a few effective solutions attempts to compress DNN models via learning sparse weights and connections. In this article, we follow this line of research and present an alternative framework of learning sparse DNNs, with the assistance of matrix factorization. We provide an underlying principle for substituting the original parameter matrices with the multiplications of highly sparse ones, which constitutes the theoretical basis of our method. Experimental results demonstrate that our method substantially outperforms previous states of the arts for compressing various DNNs, giving rich empirical evidence in support of its effectiveness. It is also worth mentioning that, unlike many other works that focus on feedforward networks like multi-layer perceptrons and convolutional neural networks only, we also evaluate our method on a series of recurrent networks in practice.																	2162-237X	2162-2388				OCT	2020	31	10					3828	3838		10.1109/TNNLS.2019.2946636													
J								Subject-Independent Brain-Computer Interfaces Based on Deep Convolutional Neural Networks	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Electroencephalography; Databases; Feature extraction; Electrodes; Brain modeling; Task analysis; Calibration; Brain-computer interface (BCI); convolutional neural networks (CNNs); deep learning (DL); electroencephalography (EEG); motor imagery (MI); subject-independent	SINGLE-TRIAL EEG; MOTOR IMAGERY; CLASSIFICATION; PERFORMANCE; PATTERNS; BCI	For a brain-computer interface (BCI) system, a calibration procedure is required for each individual user before he/she can use the BCI. This procedure requires approximately 20-30 min to collect enough data to build a reliable decoder. It is, therefore, an interesting topic to build a calibration-free, or subject-independent, BCI. In this article, we construct a large motor imagery (MI)-based electroencephalography (EEG) database and propose a subject-independent framework based on deep convolutional neural networks (CNNs). The database is composed of 54 subjects performing the left- and right-hand MI on two different days, resulting in 21 600 trials for the MI task. In our framework, we formulated the discriminative feature representation as a combination of the spectral-spatial input embedding the diversity of the EEG signals, as well as a feature representation learned from the CNN through a fusion technique that integrates a variety of discriminative brain signal patterns. To generate spectral-spatial inputs, we first consider the discriminative frequency bands in an information-theoretic observation model that measures the power of the features in two classes. From discriminative frequency bands, spectral-spatial inputs that include the unique characteristics of brain signal patterns are generated and then transformed into a covariance matrix as the input to the CNN. In the process of feature representations, spectral-spatial inputs are individually trained through the CNN and then combined by a concatenation fusion technique. In this article, we demonstrate that the classification accuracy of our subject-independent (or calibration-free) model outperforms that of subject-dependent models using various methods [common spatial pattern (CSP), common spatiospectral pattern (CSSP), filter bank CSP (FBCSP), and Bayesian spatio-spectral filter optimization (BSSFO)].																	2162-237X	2162-2388				OCT	2020	31	10					3839	3852		10.1109/TNNLS.2019.2946869													
J								A Self-Learning Immune Co-Evolutionary Network for Multiple Escaping Targets Search With Random Observable Conditions	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Immune system; Search problems; Task analysis; Reinforcement learning; Vehicle dynamics; Heuristic algorithms; Escaping target; immune co-evolutionary network; immune learning; multiple dynamic targets search; random observable condition	VEHICLE-ROUTING PROBLEM; ALGORITHM	The search for multiple escaping targets is a significant issue of cooperative control in multi-agent systems since targets consciously seek to avoid being captured. Moreover, the assumption of continuous observations in existing works is not always suitable due to the limit of measuring equipment and uncertain movement of targets. Therefore, the problem with searching for escaping targets, which can be more aptly labeled "multiple escaping-targets search with random observation conditions" (MESROC), is difficult to address by conventional methods. Inspired by machine learning and the immune response mechanism of human bodies, a self-learning immune co-evolutionary network (SLICEN) is proposed. The SLICEN consists mainly of an immune cellular network (ICN) and an immune learning algorithm (ILA). The ICN provides feasible solutions to MESROC. Different kinds of network models are introduced to work as an ICN, such as convolutional neural networks, extreme learning machines, and support vector machines. The ILA evaluates the performance of feasible solutions and selects the optimal ones to further strengthen ICN reversely. Solutions are repeatedly improved through the co-evolution of ICN and ILA. An essential distinction to conventional machine learning approaches is that SLICEN works well without training samples. Simulations and comparisons demonstrate that patterns of advanced cooperative behavior among searchers function properly. SLICEN is an efficient method for solving MESROC.																	2162-237X	2162-2388				OCT	2020	31	10					3853	3865		10.1109/TNNLS.2019.2946913													
J								A Novel Separating Hyperplane Classification Framework to Unify Nearest-Class-Model Methods for High-Dimensional Data	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Training; Data models; Support vector machines; Learning systems; Face; Security; Pattern recognition; Classification; convex cone; convex hull; dual analysis; separating hyperplane; subspace	FACE RECOGNITION; SUBSPACE	In this article, we establish a novel separating hyperplane classification (SHC) framework to unify three nearest-class-model methods for high-dimensional data: the nearest subspace method (NSM), the nearest convex hull method (NCHM), and the nearest convex cone method (NCCM). Nearest-class-model methods are an important paradigm for the classification of high-dimensional data. We first introduce the three nearest-class-model methods and then conduct dual analysis for theoretically investigating them, to understand deeply their underlying classification mechanisms. A new theorem for the dual analysis of NCCM is proposed in this article by discovering the relationship between a convex cone and its polar cone. We then establish the new SHC framework to unify the nearest-class-model methods based on the theoretical results. One important application of this new SHC framework is to help explain empirical classification results: why one class model has a better performance than others on certain data sets. Finally, we propose a new nearest-class-model method, the soft NCCM, under the novel SHC framework to solve the overlapping class model problem. For illustrative purposes, we empirically demonstrate the significance of our SHC framework and the soft NCCM through two types of typical real-world high-dimensional data: the spectroscopic data and the face image data.																	2162-237X	2162-2388				OCT	2020	31	10					3866	3876		10.1109/TNNLS.2019.2946967													
J								Maximum Correntropy Criterion-Based Robust Semisupervised Concept Factorization for Image Representation	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Data models; Image representation; Convergence; Task analysis; Principal component analysis; Analytical models; Learning systems; Concept factorization (CF); machine learning; maximum correntropy criterion (MCC); nonnegative matrix factorization (NMF); semisupervised learning	NONNEGATIVE MATRIX FACTORIZATION; OPTIMIZATION; ALGORITHMS	Concept factorization (CF) has shown its great advantage for both clustering and data representation and is particularly useful for image representation. Compared with nonnegative matrix factorization (NMF), CF can be applied to data containing negative values. However, the performance of CF method and its extensions will degenerate a lot due to the negative effects of outliers, and CF is an unsupervised method that cannot incorporate label information. In this article, we propose a novel CF method, with a novel model built based on the maximum correntropy criterion (MCC). In order to capture the local geometry information of data, our method integrates the robust adaptive embedding and CF into a unified framework. The label information is utilized in the adaptive learning process. Furthermore, an iterative strategy based on the accelerated block coordinate update is proposed. The convergence property of the proposed method is analyzed to ensure that the algorithm converges to a reliable solution. The experimental results on four real-world image data sets show that the new method can almost always filter out the negative effects of the outliers and outperform several state-of-the-art image representation methods.																	2162-237X	2162-2388				OCT	2020	31	10					3877	3891		10.1109/TNNLS.2019.2947156													
J								Automatic Analysis of Facial Expressions Based on Deep Covariance Trajectories	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Manifolds; Trajectory; Covariance matrices; Kernel; Feature extraction; Support vector machines; Face; Convolutional neural networks; covariance matrix; deep trajectory; facial expression recognition (FER); symmetric positive definite (SPD) manifold	RATE-INVARIANT ANALYSIS; REGION COVARIANCE; RECOGNITION; KERNEL	In this article, we propose a new approach for facial expression recognition (FER) using deep covariance descriptors. The solution is based on the idea of encoding local and global deep convolutional neural network (DCNN) features extracted from still images, in compact local and global covariance descriptors. The space geometry of the covariance matrices is that of symmetric positive definite (SPD) matrices. By conducting the classification of static facial expressions using a support vector machine (SVM) with a valid Gaussian kernel on the SPD manifold, we show that deep covariance descriptors are more effective than the standard classification with fully connected layers and softmax. Besides, we propose a completely new and original solution to model the temporal dynamic of facial expressions as deep trajectories on the SPD manifold. As an extension of the classification pipeline of covariance descriptors, we apply SVM with valid positive definite kernels derived from global alignment for deep covariance trajectories classification. By performing extensive experiments on the Oulu-CASIA, CK+, static facial expression in the wild (SFEW), and acted facial expressions in the wild (AFEW) data sets, we show that both the proposed static and dynamic approaches achieve the state-of-the-art performance for FER outperforming many recent approaches.																	2162-237X	2162-2388				OCT	2020	31	10					3892	3905		10.1109/TNNLS.2019.2947244													
J								Multiclass Probabilistic Classification Vector Machine	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Support vector machines; Probabilistic logic; Training; Bayes methods; Prediction algorithms; Learning systems; Acceleration; Bayesian; expectation-maximization (EM); multiclass probabilistic classification vector machine (mPCVM); multiclass; probabilistic classification vector machine (PCVM)	REGRESSION; BINARY	The probabilistic classification vector machine (PCVM) synthesizes the advantages of both the support vector machine and the relevant vector machine, delivering a sparse Bayesian solution to classification problems. However, the PCVM is currently only applicable to binary cases. Extending the PCVM to multiclass cases via heuristic voting strategies such as one-vs-rest or one-vs-one often results in a dilemma where classifiers make contradictory predictions, and those strategies might lose the benefits of probabilistic outputs. To overcome this problem, we extend the PCVM and propose a multiclass PCVM (mPCVM). Two learning algorithms, i.e., one top-down algorithm and one bottom-up algorithm, have been implemented in the mPCVM. The top-down algorithm obtains the maximum a posteriori (MAP) point estimates of the parameters based on an expectation-maximization algorithm, and the bottom-up algorithm is an incremental paradigm by maximizing the marginal likelihood. The superior performance of the mPCVMs, especially when the investigated problem has a large number of classes, is extensively evaluated on the synthetic and benchmark data sets.																	2162-237X	2162-2388				OCT	2020	31	10					3906	3919		10.1109/TNNLS.2019.2947309													
J								Pulsewidth Modulation-Based Algorithm for Spike Phase Encoding and Decoding of Time-Dependent Analog Data	IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS										Encoding; Neurons; Pulse width modulation; Decoding; Signal processing algorithms; Heuristic algorithms; Data models; Analog data; data compression; spike encoding; spike series decoding; spiking neural networks (SNNs); streaming data	NEURAL-NETWORK METHODOLOGY; LOSSLESS COMPRESSION; EEG DATA; CLASSIFICATION; COMMUNICATION; INFORMATION; PATTERNS; NEURONS; CODES	This article proposes a new spike encoding and decoding algorithm for analog data. The algorithm uses the pulsewidth modulation principles to achieve a high reconstruction accuracy of the signal, along with a high level of data compression. Two benchmark data sets are used to illustrate the method: stock index time series and human voice data. Applications of the method for spiking neural network (SNN) modeling and neuromorphic implementations are discussed. The proposed method would allow the development of new applications of SNNs as regression techniques for predictive time-series modeling.																	2162-237X	2162-2388				OCT	2020	31	10					3920	3931		10.1109/TNNLS.2019.2947380													
