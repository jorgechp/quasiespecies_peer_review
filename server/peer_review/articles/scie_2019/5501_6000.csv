PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	RP	EM	RI	OI	FU	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	D2	EA	PG	WC	SC	GA	UT	PM	OA	HC	HP	DA
J								A hybrid ridesharing algorithm based on GIS and ant colony optimization through geosocial networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Ride-sharing; Geosocial network; ACO; VCRQ; RCC5; Allen's' interval algebra	LOCATION; INFORMATION; RELEVANCY; SYSTEMS; TAXI	The increasing use of private cars in large cities is accompanied by adverse ramifications such as severe shortage of parking spaces, traffic congestion, air pollution, a high level of fuel consumption, and travel cost. Ridesharing is one of the emerging solutions that facilitate the simultaneous match of drivers and passengers with similar travel schedules. In this paper, ridesharing equals carsharing which involves a cooperative trip of at least two passengers who share an automobile and must match their itineraries. The main objective of this paper is to develop a ridesharing system based on the geosocial network to be employed in Tehran, capital of Iran. In this regard, a new hybrid approach based on GIS and ant colony is developed to provide optimal shared-routes through integrating three main procedures sequentially. First, the spatio-temporal clustering of passengers is carried out using the K-means algorithm, second spatio-temporal matching of passengers 'clusters, and drivers' has been carried out by combining Voronoi continuous range query (VCRQ), a region connected calculus (RCC5) and Allen's temporal interval algebra. Third, the optimum shared-route is found by the ant colony optimization (ACO) algorithm. The proposed hybrid model integrates metric and topological GIS-based methods with a metaheuristic algorithm. It is implemented via a bot "@Hamsafar" within the platform of a robot Telegram messenger. The proposed ridesharing application is applied with 220 passengers and 70 drivers with 61 shared trips in District # 6 of Tehran, Iran. The system are evaluated based on the statistical results, usability questionnaire, time performance, and comparison to some other metaheuristic approaches which in turn demonstrate the efficiency of the proposed algorithm.																	1868-5137	1868-5145															10.1007/s12652-020-02364-6		JUL 2020											
J								Analysis of security issues of cloud-based web applications	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Cloud architecture security; Threats; Vulnerabilities; Vulnerabilities score; Threat modelling	CHALLENGES; PROTECTION; FRAMEWORK; THREATS; SYSTEM	The Cloud computing is a powerful tool to optimize the cost in terms of hardware, controllable, utility to sharing the data, due to abovementioned features most of the organization switching their applications and services on the cloud. Cloud services offer secure and scalable services, but there is always some security problem when data have transmitted from a central storage server to a different cloud, personal and private data commitment increase risk of data confidentiality, integrity, availability, and authentication before one choose a vendor in the cloud or choose the cloud and move services in the cloud. This paper aims to resolve issues and provide the countermeasures relating to security issues in clouds based web applications. The vulnerability scores, its impact on confidentiality, integrity, availability, access complexity, and risk on assets calculated, and it observed most of the vulnerabilities identified during the scanning are related to security due to mis-configurations of web servers. The results analyzed for the vulnerability scores, vulnerabilities impact on confidentiality, integrity, availability, access complexity, and risk are analyzed description, and risk assessment. The countermeasures for each vulnerability based on the experimental results discussed.																	1868-5137	1868-5145															10.1007/s12652-020-02370-8		JUL 2020											
J								MMC-DIA: multi-metric clustering with differential interference alignment for improving small cell performance	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Clustering; Differential optimization problem; Interference alignment; Rate loss; Small cell	HETEROGENEOUS NETWORKS; RESOURCE-ALLOCATION; MAXIMIZATION; ALGORITHM; SELECTION	Interference in small cells occurs due to the interoperability of different wireless communication technologies. Uncontrolled interference defaces the increasing user density and subscriber services. Therefore, interference management is mandatory to balance user service and performance enhancements. In this paper, multi-metric clustering with differential interference alignment (MMC-DIA) for leveraging the performance of small cell users is presented. This proposed technique operates in two phases namely clustering and differential interference alignment. In the clustering process, sum-rate maximization objective based grouping of small cell users is performed to retain the efficiency of communication. In a differential IA phase, the transmitted signal is analyzed for its first and second order of assessment on the basis of transmitter-receiver communication interval. Pre-coding and cancellation matrix over the signal vectors are imposed in the periodic time intervals for improving the degree of freedom (DoF) and thereby retaining the efficiency of the system. This is applicable for both the first and second order signal derivatives to handle inter and intra cluster interference along with the objective satisfaction. The performance of the proposed technique is compared for sum-rate, spectral efficiency, and DoF with the existing methods and non-clustering method respectively. From the comparative analysis, the proposed MMC-DIA is found to improve spectral efficiency and sum rate by 6.84% and 11.18% respectively. Similarly, with respect to the varying transmit power, the proposed MMC-DIA achieves 5.85% and 6.292% better spectral efficiency and sum rate.																	1868-5137	1868-5145															10.1007/s12652-020-02387-z		JUL 2020											
J								Uniting holistic and part-based attitudes for accurate and robust deep human pose estimation	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Human pose estimation; Holistic prediction; Part-based prediction; Deep learning; Convolutional neural network		Deep learning has been utilized in many intelligent systems, including computer vision techniques. Human pose estimation is one of the popular tasks in computer vision that has benefited from modern feature learning strategies. In this regard, recent advances propose part-based approaches since pose estimation based on parts can produce more accurate results than when the human shape is considered holistically as one unbreakable, but deformable object. However, in real-word scenarios, problems like occlusion and cluttered background make difficulties in part-based methods. In this paper, we propose to unite the two attitudes of the part-based and the holistic pose predictions to make more accurate and more robust estimations. These two schemes are modeled using convolutional neural networks as regression and classification tasks in order, and are combined in three frameworks: multitasking, series, and parallel. Each of these settings has its own advantages, and the experimental results on the LSP test set demonstrate that it is essential to observe subjects, both based on parts and holistically in order to achieve more accurate and more robust estimation of human pose in challenging scenarios.																	1868-5137	1868-5145															10.1007/s12652-020-02347-7		JUL 2020											
J								An AC-DC/DC-DC hybrid multi-port embedded energy router based steady-state power flow optimizing in power system using substantial transformative energy management strategy	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Deferrable load; Hybrid energy management; Energy storage system; Substantial transformative energy management (STEM) strategy	CONVERTER	Novel and flexible interconnected architecture for micro grids and their associated energy the management strategy proposes to respond to the increase in renewable energy sources power generation. The key idea of the system is improve the renewable energy generation capacity using a multi-port DC-DC converter, the energy flow between the generation source and the micro grid system can be analyzed with the energy routers. The consumption methods can be used to compensate immediate energy shortage energy production. A multi-port based AC-DC and DC-DC converter and the, power routers are used as energy centers, are improve the stabilize energy flow between micro grids and the main grid. The power generation of the each and every renewable sources are monitored and controlled by the proposed substantial transformative energy management (STEM) strategy also the energy flow variation of the grid system is analyzed with the. This work demonstrates the role of energy routers in optimizing the efficient and flexible way of energy stabilization in grid system. This analysis model establishes the entire system, including power routers, interconnected microcircuits, and the main grid. Interconnected microcircuits were analyzed by various operating systems facilitated by power routers, and corresponding control strategies were developed. The simulation is performed on Mat lab/Simulink simulation platforms, and the results show the effectiveness especially in THD 2.52% is achieved and reliability of the control strategy for micro-grid interconnection and flexible energy flow correspondence.																	1868-5137	1868-5145															10.1007/s12652-020-02362-8		JUL 2020											
J								Towards developing an ensemble based two-level student classification model (ESCM) using advanced learning patterns and analytics	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Learning analytics; Ensemble based student classification model (ESCM); Support vector machine (SVM); Naive Bayesian (NB); J48 classifier; Bagging; Stacking; Student behaviour		In recent decade, learning analytics has gained more attention and several advanced data mining models are developed for deriving the hidden sources from educational databases. The extracted data helps the Educational Institutions or Universities to enhance the teaching methodologies of faculties and student's learning process in efficient manner. For improving the student performance and better educational results, the student data evaluations based on their learning modes are significant. With that note, the proposed model develops a new model called ensemble based two-level student classification model (ESCM) for effectively analysing and classifying the student data. With the student data pursuing technical higher education, the ESCM is performed with three traditional classification model and ensemble classifier techniques for enhancing the classification accuracy. The model utilizes support vector machine, Naive Bayesian and J48 classifier that are combined with Ensemble classification methods as modified meta classifier such as bagging and Stacking. Here, the technical higher education student data collected from SRM student database based on the feature set contains the student learning factors that support performance enhancement. The results are evaluated with the SRM student datasets and compared based on the classification accuracy and model reliability. Furthermore, the obtained results outperform the existing models. Based on the accurate predictions, special attentions and measures are taken to improve the student results and institutional reputation.																	1868-5137	1868-5145															10.1007/s12652-020-02375-3		JUL 2020											
J								Unsupervised representation learning with Minimax distance measures	MACHINE LEARNING										Representation learning; Distance measure; Computational efficiency; Minimax distances	GRAPHS	We investigate the use of Minimax distances to extract in a nonparametric way the features that capture the unknown underlying patterns and structures in the data. We develop a general-purpose and computationally efficient framework to employ Minimax distances with many machine learning methods that perform on numerical data. We study both computing the pairwise Minimax distances for all pairs of objects and as well as computing the Minimax distances of all the objects to/from a fixed (test) object. We first efficiently compute the pairwise Minimax distances between the objects, using the equivalence of Minimax distances over a graph and over a minimum spanning tree constructed on that. Then, we perform an embedding of the pairwise Minimax distances into a new vector space, such that their squared Euclidean distances in the new space equal to the pairwise Minimax distances in the original space. We also study the case of having multiple pairwise Minimax matrices, instead of a single one. Thereby, we propose an embedding via first summing up the centered matrices and then performing an eigenvalue decomposition to obtain the relevant features. In the following, we study computing Minimax distances from a fixed (test) object which can be used for instance inK-nearest neighbor search. Similar to the case of all-pair pairwise Minimax distances, we develop an efficient and general-purpose algorithm that is applicable with any arbitrary base distance measure. Moreover, we investigate in detail the edges selected by the Minimax distances and thereby explore the ability of Minimax distances in detecting outlier objects. Finally, for each setting, we perform several experiments to demonstrate the effectiveness of our framework.																	0885-6125	1573-0565															10.1007/s10994-020-05886-4		JUL 2020											
J								Node classification over bipartite graphs through projection	MACHINE LEARNING										Bipartite graphs; Two-mode networks; Node classification; Behavioral data	MODELS	Many real-world large datasets correspond to bipartite graph data settings-think for example of users rating movies or people visiting locations. Although there has been some prior work on data analysis with such bigraphs, no general network-oriented methodology has been proposed yet to perform node classification. In this paper we propose a threestage classification framework that effectively deals with the typical very large size of such datasets. The stages are: (1) top node weighting, (2) projection to a weighted unigraph, and (3) application of a relational classifier. This paper has two major contributions. Firstly, this general framework allows us to explore the design space, by applying different choices at the three stages, introducing new alternatives and mixing-and-matching to create new techniques. We present an empirical study of the predictive and run-time performances for different combinations of functions in the three stages over a large collection of bipartite datasets with sizes of up to 20 million x 30 million nodes. Secondly, thinking of classification on bigraph data in terms of the three-stage framework opens up the design space of possible solutions, where existing and novel functions can be mixed and matched, and tailored to the problem at hand. Indeed, in this work a novel, fast, accurate and comprehensible method emerges, called the SW-transformation, as one of the best-performing combinations in the empirical study.																	0885-6125	1573-0565															10.1007/s10994-020-05898-0		JUL 2020											
J								Twin labeled LDA: a supervised topic model for document classification	APPLIED INTELLIGENCE										Supervised; Topic modeling; Document classification; Hierarchical Dirichlet distributions		Recently, some statistic topic modeling approaches, e.g., Latent Dirichlet allocation (LDA), have been widely applied in the field of document classification. However, standard LDA is a completely unsupervised algorithm, and then there is growing interest in incorporating prior information into the topic modeling procedure. Some effective approaches have been developed to model different kinds of prior information, for example, observed labels, hidden labels, the correlation among labels, label frequencies; however, these methods often need heavy computing because of model complexity. In this paper, we propose a new supervised topic model for document classification problems, Twin Labeled LDA (TL-LDA), which has two sets of parallel topic modeling processes, one incorporates the prior label information by hierarchical Dirichlet distributions, the other models the grouping tags, which have prior knowledge about the label correlation; the two processes are independent from each other, so the TL-LDA can be trained efficiently by multi-thread parallel computing. Quantitative experimental results compared with state-of-the-art approaches demonstrate our model gets the best scores on both rank-based and binary prediction metrics in solving single-label classification, and gets the best scores on three metrics, i.e., One Error, Micro-F1, and Macro-F1 while multi-label classification, including non power-law and power-law datasets. The results show benefit from modeling fully prior knowledge, our model has outstanding performance and generalizability on document classification. Further comparisons with recent works also indicate the proposed model is competitive with state-of-the-art approaches.																	0924-669X	1573-7497															10.1007/s10489-020-01798-x		JUL 2020											
J								A hybrid evolutionary approach for identifying spam websites for search engine marketing	EVOLUTIONARY INTELLIGENCE										Search engine marketing; Outlier detection; Vendor selection; Bio-inspired computing; Machine learning; Supplier selection	FIREFLY ALGORITHM; OPTIMIZATION; WEB; COMPUTATION; SYSTEM; IMPACT	With the increased digital usage, web visibility has become critically essential for organizations when catering to a larger audience. This visibility on the web is directly related to web searches on search engines which is often governed by search engine optimization techniques liked link building and link farming amongst others. The current study identifies metrics for segregating websites for the purpose of link building for search engine optimization as it is important to invest resources in the right website sources. These metrics are further used for detecting websites outliers for effective optimization and subsequent search engine marketing. Two case studies of knowledge management portals from different domains are used having 1682 and 1070 websites respectively for validation of the proposed approach. The study evolutionary intelligence by proposing a k-means chaotic firefly algorithm coupled with k-nearest neighbor outlier detection for solving the problem. Factors like Page Rank, Page Authority, Domain Authority, Alexa Rank, Social Shares, Google Index and Domain Age emerge significant in the process. Further, the proposed chaotic firefly variants are compared to K-Means integrated firefly algorithm, bat algorithm and cuckoo search algorithm for accuracy and convergence showing comparable accuracy. Findings indicate that the convergence speeds are higher for proposed chaotic firefly approach for tuning absorption and attractiveness coefficients resulting in faster search for optimal cluster centroids. The proposed approach contributes both theoretically and methodologically in the domain of vendor selection for identifying genuine websites for avoiding investment on untrustworthy websites.																	1864-5909	1864-5917															10.1007/s12065-020-00461-1		JUL 2020											
J								Electroencephalogram for epileptic seizure detection using stacked bidirectional LSTM_GAP neural network	EVOLUTIONARY INTELLIGENCE										Artefacts; Deep learning; EEG; Epilepsy; GAP; LSTM; Machine learning; Seizure	EEG; CLASSIFICATION; SEGMENTATION; ENTROPY; IMAGES; TUMOR	Epilepsy seizure classification has been an ongoing research from decades. Auto seizure detection methods are developed by many researchers. Year by year new methods and new approaches are introduced by researchers to improve the performance of these automatic detection methods. Electroencephalogram (EEG) is the most widely used tool for detecting seizure as it has high resolution which makes it more feasible for analysis. In this paper we are introducing an approach based on stacked bidirectional long short term memory with global average pooling (LSTM_GAP) neural network for detecting epileptic seizure events. We use open access EEG dataset available in Bonn University Germany for the experiment. The model is evaluated based on three performance metrics, specificity, sensitivity and accuracy. It resulted with outstanding performance with 100% accuracy, sensitivity and specificity in detecting epileptic seizure events which is the highest accuracy among all the techniques available in state-of-the art literature. Another advantage of LSTM_GAP model lies in its robustness in noise. In real, the EEG recorded in hospital laboratories consists of artefacts generated from eye blink and muscle activities of the patients. When our model will be put to practical use by the neurologists, the EEG recordings consisting of artefacts will be given as input to the model for detecting seizure events. Hence we need to develop a model which is robust against noise and which can provide good results even when the EEG data consisting artefacts is given as input. Hence we extended our experiment to check the performance of the LSTM_GAP model by adding eye blink artefacts and muscle artefacts to the input EEG data. The model was able to give superior results even after adding artefacts. We were able to achieve 97.67% sensitivity, 98.83% specificity and 97.65% accuracy. Thus LSTM_GAP model gives superior results for the EEG with and without noise compared to other methods in the literature.																	1864-5909	1864-5917															10.1007/s12065-020-00459-9		JUL 2020											
J								Hierarchical Path Planning of Unmanned Surface Vehicles: A Fuzzy Artificial Potential Field Approach	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Hierarchical path planning; Unmanned surface vehicle; Adaptive genetic algorithm; Fuzzy artificial potential field	OBSTACLE AVOIDANCE; GENETIC ALGORITHM; OPTIMIZATION; NAVIGATION; TRACKING	Under unforeseen circumstances, a hierarchical path planning (HPP) scheme combining global and local tasks of an unmanned surface vehicle (USV) is proposed by devising fuzzy artificial potential field (FAPF). Within the HPP scheme, the elite reservation, diversity increment, adaptive mutation probability and adaptive genetic algorithm are incorporated to generate optimally sparse waypoints together with smooth path. To exclusively accommodate unpredictable environments, the innovative FAPF method with fuzzy decision-making is developed to remove singular dilemmas, pertaining to which the obstacles on the path make the USV hesitate. By inserting virtual returning points onto the global path, a hybrid compound combining the global and local paths is established. Finally, the simulations and comparisons in various geographical areas demonstrate the effectiveness and superiority of the proposed HPP scheme.																	1562-2479	2199-3211															10.1007/s40815-020-00912-y		JUL 2020											
J								An intelligent EGWO-SCA-CS algorithm for PSS parameter tuning under system uncertainties	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										cuckoo search algorithm; enhanced grey wolf optimization; hybrid optimization algorithm; inter-area oscillations; power system stabilizer; STATCOM	PARTICLE SWARM OPTIMIZATION; GREY WOLF OPTIMIZER; SINE-COSINE ALGORITHM; POWER-SYSTEM; STABILIZERS DESIGN; TRANSIENT STABILITY; COORDINATED DESIGN; CUCKOO SEARCH; FUZZY-LOGIC; IMPROVEMENT	This paper proposes a novel hybrid technique called enhanced grey wolf optimization-sine cosine algorithm-cuckoo search (EGWO-SCA-CS) algorithm to improve the electrical power system stability. The proposed method comprises of a popular grey wolf optimization (GWO) in an enhanced and hybrid form. It embraces the well-balanced exploration and exploitation using the cuckoo search (CS) algorithm and enhanced search capability through the sine cosine algorithm (SCA) to elude the stuck to the local optima. The proposed technique is validated with the 23 benchmark functions and compared with state-of-the-art methods. The benchmark functions consist of unimodal, multimodal function from which the best suitability of the proposed technique can be identified. The robustness analysis also presented with the proposed method through boxplot, and a detailed statistical analysis is performed for a set of 30 individual runs. From the inferences gathered from the benchmark functions, the proposed technique is applied to the stability problem of a power system, which is heavily stressed with the nonlinear variation of the load and thereby operating conditions. The dynamics of power system components have been considered for the mathematical model of a multimachine system, and multiobjective function has been framed in tuning the optimal controller parameters. The effectiveness of the proposed algorithm has been assessed by considering two case studies, namely, (i) the optimal controller parameter tuning, and (ii) the coordination of oscillation damping devices in the power system stability enhancement. In the first case study, the power system stabilizer (PSS) is considered as a controller, and a self-clearing three-phase fault is considered as the system uncertainty. In contrast, static synchronous compensator (STATCOM) and PSS are considered as controllers to be coordinated, and perturbation in the system states as uncertainty in the second case study.																	0884-8173	1098-111X				OCT	2020	35	10					1520	1569		10.1002/int.22263		JUL 2020											
J								Multi-cost Bounded Tradeoff Analysis in MDP	JOURNAL OF AUTOMATED REASONING										Markov decision process; Multi-objective verification; Pareto-optimal strategies; Cost-bounded reachability; Expected rewards; Probabilistic model checking	DECISION-PROCESSES	We provide a memory-efficient algorithm for multi-objective model checking problems on Markov decision processes (MDPs) with multiple cost structures. The key problem at hand is to check whether there exists a scheduler for a given MDP such that all objectives over cost vectors are fulfilled. We cover multi-objective reachability and expected cost objectives, and combinations thereof. We further transfer approaches for computing quantiles over single cost bounds to the multi-cost case and highlight the ensuing challenges. An empirical evaluation shows the scalability of our new approach both in terms of memory consumption and runtime. We discuss the need for more detailed visual presentations of results beyond Pareto curves and present a first visualisation approach that exploits all the available information from the algorithm to support decision makers.																	0168-7433	1573-0670				OCT	2020	64	7			SI		1483	1522		10.1007/s10817-020-09574-9		JUL 2020											
J								Deep learning applications in pulmonary medical imaging: recent updates and insights on COVID-19	MACHINE VISION AND APPLICATIONS										Coronavirus Deep Learning; Pulmonary Imaging; Medical Image Analysis; Convolutional Neural Networks	CONVOLUTIONAL NEURAL-NETWORKS; FALSE-POSITIVE REDUCTION; COMPUTER-AIDED DETECTION; LUNG NODULE; CHEST RADIOGRAPHS; ARTIFICIAL-INTELLIGENCE; AUTOMATIC DETECTION; CT IMAGES; CLASSIFICATION; TUBERCULOSIS	Shortly after deep learning algorithms were applied to Image Analysis, and more importantly to medical imaging, their applications increased significantly to become a trend. Likewise, deep learning applications (DL) on pulmonary medical images emerged to achieve remarkable advances leading to promising clinical trials. Yet, coronavirus can be the real trigger to open the route for fast integration of DL in hospitals and medical centers. This paper reviews the development of deep learning applications in medical image analysis targeting pulmonary imaging and giving insights of contributions to COVID-19. It covers more than 160 contributions and surveys in this field, all issued between February 2017 and May 2020 inclusively, highlighting various deep learning tasks such as classification, segmentation, and detection, as well as different pulmonary pathologies like airway diseases, lung cancer, COVID-19 and other infections. It summarizes and discusses the current state-of-the-art approaches in this research domain, highlighting the challenges, especially with COVID-19 pandemic current situation.																	0932-8092	1432-1769				JUL 28	2020	31	6							53	10.1007/s00138-020-01101-5													
J								Modeling of rock fragmentation by firefly optimization algorithm and boosted generalized additive model	NEURAL COMPUTING & APPLICATIONS										Rock size distribution; Rock fragmentation; FFA-BGAM; Blasting; Optimization algorithm; Hybrid technique	PEAK PARTICLE-VELOCITY; ARTIFICIAL NEURAL-NETWORK; FUZZY INFERENCE SYSTEM; GROUND VIBRATION; MIXED MODELS; PREDICTION; REGRESSION; PERFORMANCE; SIMULATION; STRENGTH	This paper proposes a new soft computing model (artificial intelligence model) for modeling rock fragmentation (i.e., the size distribution of rock (SDR)) with high accuracy, based on a boosted generalized additive model (BGAM) and a firefly algorithm (FFA), called FFA-BGAM. Accordingly, the FFA was used as a robust optimization algorithm/meta-heuristic algorithm to optimize the BGAM model. A split-desktop environment was used to analyze and calculate the size of rock from 136 images, which were captured from 136 blasts. To this end, blast designs were collected and extracted as the input parameters. Subsequently, the proposed FFA-BGAM model was evaluated and compared through previous well-developed soft computing models, such as FFA-ANN (artificial neural network), FFA-ANFIS (adaptive neuro-fuzzy inference system), support vector machine (SVM), Gaussian process regression (GPR), and k-nearest neighbors (KNN) based on three performance indicators (MAE, RMSE, andR(2)). The results indicated that the new intelligent technique (i.e., FFA-BGAM) provided the highest accuracy in predicting the SDR with an MAE of 0.920, RMSE of 1.213, andR(2)of 0.980. In contrast, the remaining models (i.e., FFA-ANN, FFA-ANFIS, SVM, GPR, and KNN) yielded lower accuracies in predicting the SDR, i.e., MAEs of 1.248, 1.661, 1.096, 1.573, 1.237; RMSEs of 1.598, 2.068, 1.402, 2.137, 1.717; andR(2)of 0.967, 0.968, 0.972, 0.940, 0.963, respectively.																	0941-0643	1433-3058															10.1007/s00521-020-05197-8		JUL 2020											
J								Change detection and convolution neural networks for fall recognition	NEURAL COMPUTING & APPLICATIONS										Deep learning; Change detection; Fall detection; Wearable devices; Assisted living	CUSUM; CLASSIFICATION; SYSTEM; TIME	Accurate fall detection is a crucial research challenge since the time delay from fall to first aid is a key factor that determines the consequences of a fall. Wearable sensors allow a reliable way for motion tracking, allowing immediate detection of high-risk falls via a machine learning framework. Toward this direction, accelerometer devices are widely used for the assessment of fall risk. Although there exist a plethora of studies under this perspective, several challenges still remain, such as dealing simultaneously with extremely demanding data management, power consumption and prediction accuracy. In this work, we propose a complete methodology based on the cooperation of deep learning for signal classification along with a lightweight control chart method for change detection. Our basic assumption is that it is possible to control computational resources by selectively allowing the operation of a relatively heavyweight, but very efficient classifier, when it is truly required. The proposed methodology was applied to real experimental data providing the reliable results that justify the original hypothesis.																	0941-0643	1433-3058															10.1007/s00521-020-05208-8		JUL 2020											
J								Unsupervised double weighted domain adaptation	NEURAL COMPUTING & APPLICATIONS										Transfer learning; Domain adaptation; Sample reweighting; Weighted distribution alignment; Structural risk minimization	GENERAL FRAMEWORK; REGULARIZATION	Domain adaptation can effectively transfer knowledge between domains with different distributions. Most existing methods use distribution alignment to mitigate the domain shift. But they typically align the marginal and conditional distributions with equal weights. This neglects the relative importance of different distribution alignments. In this paper, we propose a double weighted domain adaptation (DWDA) method, which employs new distribution alignment weighting and sample reweighting strategies. Specifically, the distribution alignment weighting strategy explores the relative importance of marginal and conditional distribution alignments, based on the maximum mean discrepancy; the sample reweighting strategy weights the source and target samples separately based onk-means clustering. The two strategies reinforce each other in the iterative optimization procedure, thus improving the overall performance. In addition, our method also considers the geometry structure preservation. The closed-form solution of the objective function is presented, and the computational complexity and convergence analysis are given. Experimental results demonstrate that DWDA outperforms state-of-the-art domain adaptation methods on several public datasets, and it also has good performance on an imbalanced dataset. Besides, DWDA is robust to a wide range of parameters. Moreover, the convergence curves show that DWDA generally converges rapidly within five iterations. We also evaluate the components of DWDA and show that each component is necessary. Finally, we compare the computational time with the methods of the recent three years to demonstrate the efficiency of DWDA.																	0941-0643	1433-3058															10.1007/s00521-020-05228-4		JUL 2020											
J								Saddle point equilibrium model for uncertain discrete systems	SOFT COMPUTING										Uncertain optimal control; Uncertain process; Optimistic value; Differential game theory; Saddle point equilibrium	DIFFERENTIAL-GAMES	Uncertainty theory is a newly founded mathematical tool for modeling subjective indeterminacy. This type of indeterminate events is described as uncertain events and measured by belief degrees. In this paper, a saddle point equilibrium game model is studied for an uncertain discrete system, where the system is disturbed by an uncertain event at each stage. Applying Bellman's dynamic programming approach, recurrence equations for this model are presented. The explicit solution of a bang-bang game model for the uncertain discrete system is obtained. Furthermore, for general cases, a hybrid intelligent algorithm is provided to approximate the solution numerically. Finally, a discrete game of duopoly is discussed to show the effectiveness of our results.																	1432-7643	1433-7479															10.1007/s00500-020-05206-x		JUL 2020											
J								Classification ofhigh-dimensionalelectroencephalography data with location selection using structuredspike-and-slabprior	STATISTICAL ANALYSIS AND DATA MINING										Bayesian variable selection; Gibbs sampling; neuroimaging data; slice sampling; spatial clustering; spatio-temporal EEG	REGRESSION	With the advent of modern technologies, it is increasingly common to deal with data of large dimensions in various scientific fields of study. In this paper, we develop a Bayesian approach for the classification of multi-subject high-dimensional electroencephalography (EEG) data. In this EEG data, we have a matrix of covariates corresponding to each subject from either the alcoholic or control group. The matrix covariates have a natural spatial correlation based on the locations of the brain, and temporal correlation as the measurements are taken over time. We employ a divide and conquer strategy by building multiple local Bayesian models at each time point separately. We incorporate the spatial structure through the structured spike-and-slab prior, which has inherent variable selection properties. The temporal structure is incorporated within the prior by learning from the local model from the previous time point. We pool the information from the local models and use a weighted average to design a prediction method. We perform simulation studies to show the efficiency of our approach and demonstrate the local Bayesian modeling with a case study on EEG data.																	1932-1864	1932-1872				OCT	2020	13	5					465	481		10.1002/sam.11477		JUL 2020											
J								Internet of things assisted condition-based support for smart manufacturing industry using learning technique	COMPUTATIONAL INTELLIGENCE										accuracy; condition-based maintenance; industries; linear four rates; nominal and continuous	REGRESSION; MACHINE	Nowadays, countless industrial IIoT contraptions and sensors are conveyed a sharp plant to gather tremendous information regarding system conditions and a computerized bodily framework for handling industrial plant's mist point of convergence by using keen assembling projects. By then, the system utilizes an array of condition-based support model (CBM) procedures to predict when devices begin to unusually work and to keep them up or supplant their fragments ahead of time to avoid assembling colossal investigator items in smart manufacturing industries. CBM experiences problems of floating ideas (ie, conveying examples of deficiencies can change extra time) and information of lop-sidedness (ie, information with issues represents a minority of all things considered). The condition-based support assisted learning technique by the group that coordinates the assorted variety of numerous classifiers provides an elite response to address these issues. Therefore, in this work the proposed work classifies offline three-organized CBM with floats of ideas and awkwardness data, using an improved Dynamic AdaBoost for preparing a group classifier and an enhanced linear four rates (LFR) methodology is used by the classifier of nominal and continuous (NC) with synthetic minority oversampling technique (SMOTE) method to tackle inconsistent information in recognizing concept floats in lop-sidedness information. The investigational results scheduled datasets by varying notches anomaly demonstration that the future strategy has a high degree of accuracy in the identifiable evidence of minority knowledge, which is over 96%.																	0824-7935	1467-8640															10.1111/coin.12319		JUL 2020											
J								Identification of predictive factors of the degree of adherence to the Mediterranean diet through machine-learning techniques	PEERJ COMPUTER SCIENCE										Feature selection; Nutritional status; Machine learning; Mediterranean diet; Support vector machines; Nutrition disorders	BASAL METABOLIC-RATE; ENERGY-INTAKE; CALF CIRCUMFERENCE; NUTRITIONAL-STATUS; PHYSICAL-ACTIVITY; BODY-IMAGE; SELECTION; OBESITY; ADULTS; MASS	Food consumption patterns have undergone changes that in recent years have resulted in serious health problems. Studies based on the evaluation of the nutritional status have determined that the adoption of a food pattern-based primarily on a Mediterranean diet (MD) has a preventive role, as well as the ability to mitigate the negative effects of certain pathologies. A group of more than 500 adults aged over 40 years from our cohort in Northwestern Spain was surveyed. Under our experimental design, 10 experiments were run with four different machine-learning algorithms and the predictive factors most relevant to the adherence of a MD were identified. A feature selection approach was explored and under a null hypothesis test, it was concluded that only 16 measures were of relevance, suggesting the strength of this observational study. Our findings indicate that the following factors have the highest predictive value in terms of the degree of adherence to the MD: basal metabolic rate, mini nutritional assessment questionnaire total score, weight, height, bone density, waist-hip ratio, smoking habits, age, EDI-OD, circumference of the arm, activity metabolism, subscapular skinfold, subscapular circumference in cm, circumference of the waist, circumference of the calf and brachial area.																	2376-5992					JUL 27	2020									e287	10.7717/peerj-cs.287													
J								Deep learning based Sequential model for malware analysis using Windows exe API Calls	PEERJ COMPUTER SCIENCE										Malware analysis; Sequential models; Network security; Long-short-term memory; Malware dataset		Malware development has seen diversity in terms of architecture and features. This advancement in the competencies of malware poses a severe threat and opens new research dimensions in malware detection. This study is focused on metamorphic malware, which is the most advanced member of the malware family. It is quite impossible for anti-virus applications using traditional signature-based methods to detect metamorphic malware, which makes it difficult to classify this type of malware accordingly. Recent research literature about malware detection and classification discusses this issue related to malware behavior. The main goal of this paper is to develop a classification method according to malware types by taking into consideration the behavior of malware. We started this research by developing a new dataset containing API calls made on the windows operating system, which represents the behavior of malicious software. The types of malicious malware included in the dataset are Adware, Backdoor, Downloader, Dropper, spyware, Trojan, Virus, and Worm. The classification method used in this study is LSTM (Long Short-Term Memory), which is a widely used classification method in sequential data. The results obtained by the classifier demonstrate accuracy up to 95% with 0.83 $F_1$-score, which is quite satisfactory. We also run our experiments with binary and multi-class malware datasets to show the classification performance of the LSTM model. Another significant contribution of this research paper is the development of a new dataset for Windows operating systems based on API calls. To the best of our knowledge, there is no such dataset available before our research. The availability of our dataset on GitHub facilitates the research community in the domain of malware detection to benefit and make a further contribution to this domain.																	2376-5992					JUL 27	2020									e285	10.7717/peerj-cs.285													
J								Automated Proof of Bell-LaPadula Security Properties	JOURNAL OF AUTOMATED REASONING										Bell-LaPadula model; Security; Automated proof; {log}; Set theory; Binary relations	THEOREM	Almost 50 years ago, D. E. Bell and L. LaPadula published the first formal model of a secure system, known today as the Bell-LaPadula (BLP) model. BLP is described as a state machine by means of first-order logic and set theory. The authors also formalize two state invariants known as security condition and *-property. Bell and LaPadula prove that all the state transitions preserve these invariants. In this paper we present a fully automated proof of the security condition and the *-property for all the model operations. The model and the proofs are coded in the {log} tool. As far as we know this is the first time such proofs are automated. Besides, we show that the {log} model is also an executable prototype. Therefore we are providing an automatically verified executable prototype of BLP.																	0168-7433	1573-0670															10.1007/s10817-020-09577-6		JUL 2020											
J								On the Borel summability method for convergence of triple sequences of Bernstein-Stancu operators of fuzzy numbers	SOFT COMPUTING										Borel summability method; Bernstein-Stancu operator; Weighted statistical convergence; Rough convergence; Fuzzy number	WEIGHTED STATISTICAL CONVERGENCE; APPROXIMATION THEOREMS; DIFFERENCE OPERATOR; A-SUMMABILITY; KOROVKIN; (P; Q)-ANALOG	We define Borel rough summable of triple sequences and discuss some fundamental results related to Borel rough summable of triple Bernstein-Stancu operators based on (p, q)-integers. Further, we study rough-weighted generalized statistical convergence method for some approximation properties for (p, q)-analogue of Bernstein-Stancu operators for triple sequence of fuzzy numbers. With the help of an example we illustrate our approximation results.																	1432-7643	1433-7479															10.1007/s00500-020-05178-y		JUL 2020											
J								General type-2 fuzzy sliding mode control for motion balance adjusting of power-line inspection robot	SOFT COMPUTING										General type-2 fuzzy system; Sliding mode control; Power-line inspection robot; Approaching law	FINITE-TIME CONTROL; ROBUST-CONTROL; NONLINEAR-SYSTEMS; CONTROL DESIGN; MECHANICAL SYSTEMS; CONTROL STRATEGY	In this paper, a novel general type-2 fuzzy sliding mode controller for motion balance adjusting of a power-line inspection robot is developed. The combination method between general type-2 fuzzy system and sliding mode control is different from the existing works. In this paper, sliding mode control is used to finish the motion balance adjusting work. The sliding mode surface is designed according to the Ackermann's formula. And the general type-2 fuzzy system is used to strengthen the anti-interference ability of the power-line inspection robot. The general type-2 fuzzy sliding mode controller is developed by substituting an element of the sliding mode control law by the output of general type-2 fuzzy system. The novel combination method makes it possible to enhance the anti-interference ability of PLI robot while achieving motion balance control. According to the simulation results, the effectiveness of the proposed controller is proved. And it can also be known that the proposed method is not limited to the GT2FS, and its IT2FS and T1FS counterpart can also strengthen the anti-interference ability in different level.																	1432-7643	1433-7479															10.1007/s00500-020-05202-1		JUL 2020											
J								A hybrid fuzzy-stochastic multi-criteria ABC inventory classification using possibilistic chance-constrained programming	SOFT COMPUTING										Multi-criteria inventory classification; Chance-constrained programming; Possibilistic programming	DATA ENVELOPMENT ANALYSIS; DECISION-MAKING; DEA APPROACH; MODEL; MULTIPERIOD; IMPROVEMENT; ALGORITHM; NUMBERS	Inventory classification is a fundamental issue in the development of inventory policy that assigns each inventory item to several classes with different levels of importance. This classification is the main determinant of a suitable inventory control policy of inventory classes. Therefore, a great deal of research is done on solving this problem. Usually, the problem of inventory classification is considered in a multi-criteria and uncertain environment. The proposed method in this paper inspired by the notion of heterogeneous decision-making problems in which decision-makers deal with different types of data. To this aim, a mathematical modeling-based approach is proposed considering different types of uncertainty in classification information. Demand information is considered to be stochastic due to its time-varying nature and cost information is considered to be fuzzy due to its cognitive ambiguity. A hybrid algorithm based on chance-constrained and possibilistic programming is proposed to solve the problems. Considering the stochastic nature of demand information, solving the proposed model using the hybrid algorithm, the classification of items to three classes of extremely important, class A, moderately important, class B, and relatively unimportant, class C, items are determined along with a minimum inventory level required to deal with the stochasticity of demands information. The proposed approach is applied to a case study of classifying 51 inventory items. The obtained results assigned 22%, 39%, and 39% of the items to A, B, and C classes, respectively.																	1432-7643	1433-7479															10.1007/s00500-020-05204-z		JUL 2020											
J								Pythagorean fuzzy interactive Hamacher power aggregation operators for assessment of express service quality with entropy weight	SOFT COMPUTING										Multiple attribute decision making; Pythagorean fuzzy sets; Aggregation operators; Interactive Hamacher operation laws	CRITERIA DECISION-MAKING; EXTENSION; TOPSIS	Reasonable and effective assessment of express service quality can help express company discover its own shortcomings and overcome them, which is crucial significant to enhance its service quality. When considering the decision assessment of express company, the key issue that emerge powerful ambiguity. Pythagorean fuzzy set as an efficient math tool can capture the indeterminacy successfully. The major focus of this manuscript is to explore various interactive Hamacher power aggregation operators for Pythagorean fuzzy numbers. Firstly, we defined novel interactive Hamacher operation, on this basis we presented some Pythagorean fuzzy interactive Hamacher power aggregation operators such as Pythagorean fuzzy interactive Hamacher power average, weighted average (PFIHPWA), ordered weighted average, Pythagorean fuzzy interactive Hamacher power geometric, weighted geometric (PFIHPWG) and ordered geometric operators,respectively. Meanwhile, we verified their general properties and specific cases as well. The salient feature of proposed operators is that they can not only reduce the impact of negative data and consider the interactions between membership and nonmembership degrees, but also provide more general results through a parameter. Secondly, we defined a Pythagorean fuzzy entropy measure, and then establish a method to determine the attribute weights. Further, based on the conceived PFIHPWA and PFIHPWG operators we explored a novel approach to manage multiple attribute decision making problems. At last, the proposed techniques are carried out in a real application concerning on the assessment of express service quality to display the applicability and effectiveness, as well as the influence of changed parameters on the results. In addition, its advantages are displayed by a systematic comparison with relevant approaches.																	1432-7643	1433-7479															10.1007/s00500-020-05193-z		JUL 2020											
J								DeepImpact: a deep learning model for whole body vibration control using impact force monitoring	NEURAL COMPUTING & APPLICATIONS										Deep learning; Machine learning; Artificial intelligence; Synthetic rubber; Whole body vibrations; Shovel dumping; Surface mining; Dump truck	SUPPORT VECTOR REGRESSION; ARTIFICIAL-INTELLIGENCE MODELS; K-NEAREST NEIGHBOR; NEURAL-NETWORKS; RISK-FACTORS; CLASSIFICATION; PREDICTION; DISTANCE; MACHINE	Large capacity shovels are matched with large capacity dump trucks for gaining economic advantage in surface mining operations. These high impact shovel loading operations (HISLO) result in large dynamic impact force at truck bed surface. This impact force generates high-frequency shockwaves which expose the operator to whole body vibrations (WBVs). These WBVs cause serious injuries and fatalities to operators in mining operations. The present work focuses on developing solution technology for minimizing impact force on truck bed surface, which is the cause of these WBVs. The proposed technology involves modifying the truck bed structural design through the addition of synthetic rubber. Detailed experiments, with the technology implementation, showed a reduction of impact force by 22.60% and 23.83%, during the first and second shovel passes, respectively, which in turn reduced the WBV levels by 25.56% and 26.95% during the first and second shovel passes, respectively, at the operator's seat. To make the smart implementation of the technology feasible, a novel state-of-the-art deep learning model, 'DeepImpact,' is designed and developed for impact force real-time monitoring during a HISLO operation. DeepImpact showed an exceptional performance, giving anR(2), RMSE, and MAE values of 0.9948, 10.750, and 6.33, respectively, during the model validation. This smart and intelligent real-time monitoring system with design and process optimization would minimize the impact force on truck surface, which in turn would reduce the level of vibration on the operator, thus leading to a safer and healthier working environment at mining sites.																	0941-0643	1433-3058															10.1007/s00521-020-05218-6		JUL 2020											
J								The monarch butterfly optimization algorithm for solving feature selection problems	NEURAL COMPUTING & APPLICATIONS										Optimization; Monarch Butterfly Optimization; Classification; K-nearest neighbor; Feature selection; Wrapper approach	NEURAL-NETWORK; DIFFERENTIAL EVOLUTION; COLONY OPTIMIZATION; MUTUAL INFORMATION; GENETIC ALGORITHM; CLASSIFICATION; DIAGNOSIS; SEARCH	Feature selection (FS) is considered to be a hard optimization problem in data mining and some artificial intelligence fields. It is a process where rather than studying all of the features of a whole dataset, some associated features of a problem are selected, the aim of which is to increase classification accuracy and reduce computational time. In this paper, a recent optimization algorithm, the monarch butterfly optimization (MBO) algorithm, is implemented with a wrapper FS method that uses the k-nearest neighbor (KNN) classifier. Experiments were implemented on 18 benchmark datasets. The results showed that, in comparison with four metaheuristic algorithms (WOASAT, ALO, GA and PSO), MBO was superior, giving a high rate of classification accuracy of, on average, 93% for all datasets as well as reducing the selection size significantly. Therefore, the use of the MBO to solve the FS problems has been proven through the results obtained to be effective and highly efficient in this field, and the results have also proven the strength of the balance between global and local search of MBO.																	0941-0643	1433-3058															10.1007/s00521-020-05210-0		JUL 2020											
J								Artificial bee colony algorithm including some components of iterated greedy algorithm for permutation flow shop scheduling problems	NEURAL COMPUTING & APPLICATIONS										Artificial bee colony; Iterated greedy; Permutation flow shop; Makespan	SWARM OPTIMIZATION ALGORITHM; GENETIC ALGORITHM; SEARCH ALGORITHM; MINIMIZING MAKESPAN; HEURISTICS; CLASSIFICATION; MINIMIZATION; FLOWSHOPS; BLOCKING; TIME	The permutation flow shop scheduling problem has been investigated by researchers for more than 40 years due to its complexity and lots of real-life examples of the problem. Many exact or approximate solution approaches have been presented for the problem. Among solution approaches in the literature, iterated greedy algorithm and its variants are the most effective solution methods for the problem. This paper proposes a hybrid solution algorithm that uses the best components such as local search operators and destruction/construction operators of the variants of iterated greedy algorithm in an artificial bee colony algorithm. An ANOVA is made for determining the most proper components of iterated greedy algorithm. Then, these components are combined with artificial bee colony algorithm. Furthermore, a design of experiment is made for determining the best parameter setting for the proposed hybrid artificial bee colony. The experimental results of the proposed algorithm compared with variants of iterated greedy algorithms in the literature show that the proposed algorithm produces better solutions that deviate less from the optimum or lower-bound solutions for permutation flow shop scheduling problems with the makespan performance criterion.																	0941-0643	1433-3058															10.1007/s00521-020-05174-1		JUL 2020											
J								Frequency-amplitude coupling: a new approach for decoding of attended features in covert visual attention task	NEURAL COMPUTING & APPLICATIONS										Electroencephalography; Cross-frequency coupling; Covert visual attention; Machine learning algorithms; Frequency-amplitude coupling	COMPUTER-AIDED DIAGNOSIS; TOP-DOWN MODULATION; WORKING-MEMORY; BRAIN OSCILLATIONS; THETA-OSCILLATIONS; PREFRONTAL CORTEX; FUNCTIONAL-ROLE; PHASE; GAMMA; SIGNALS	A method that has recently been mentioned as information encoding brain is cross-frequency coupling (CFC). It is generally assumed that CFC can play a crucial role in perception, memory, and attention. In this study, two new indices for evaluating frequency-amplitude coupling (FAC) through generalized linear model (GLM) and linear regression method were introduced and investigated along with other CFC indices. Electroencephalogram (EEG) signals were recorded during covert visual attention tasks to find out the CFC index capability so as to distinguish different states in the mentioned tasks. To this end, machine learning algorithms were used and four various types of CFC, phase-amplitude coupling (PAC), phase-phase coupling (PPC), amplitude-amplitude coupling (AAC), and frequency-amplitude coupling (FAC) in recorded signals were considered as inputs for classifiers. The results demonstrated that the proposed method used for evaluating FAC through linear regression can provide more information about the different states in two covert attention tasks using quadratic discriminant analysis (QDA) by classification performance of 94.21% and 90.54% in color and direction tasks, respectively. Also, FAC that used a GLM model and PAC had a higher performance compared with PPC and AAC in color task (90.74 and 92.24% against 83.21 and 86.22). We can conclude that CFC can encompass useful information about semantic category of stimuli in covert attention tasks and can be used as an acceptable alternative for the time-frequency features of brain signals.																	0941-0643	1433-3058															10.1007/s00521-020-05222-w		JUL 2020											
J								Data Description Through Information Granules: A Multiview Perspective	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Information granules; Multiview perspective; Clustering; Reconstruction; Classification; Prediction; Granular signature of data	PRINCIPLE; FUNDAMENTALS; DESIGN	In light of the remarkable diversity of data, arises an interesting and challenging problem of their description and concise interpretation. In a nutshell, in the proposed description pursued in this study, we consider a framework of information granules. The study develops a general scheme composed of two functional phases: (i) clustering data and features forming segments of original data and delivering a meaningful partition of data, and (ii) development of information granules. In both phases, we discuss a suite of performance indexes quantifying the quality of segments of data and the resulting information granules. Along this line, discussed are collections of information granules and their mutual relationships. A series of publicly available data sets is used in the experiments-their granular signature is quantified, and the quality of these findings is analyzed.																	1562-2479	2199-3211				SEP	2020	22	6			SI		1731	1747		10.1007/s40815-020-00903-z		JUL 2020											
J								Development of Fuzzy Exploratory Factor Analysis for Designing an E-Learning Service Quality Assessment Model	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Fuzzy exploratory factor analysis; Service quality assessment; E-learning; Critical incident technique	SYSTEMS SUCCESS; INFORMATION; PERSPECTIVES; SATISFACTION; AHP	The exploratory factor analysis (EFA) method is regarded as one of the most well-known statistical multivariate analysis methods, which is used to discover the underlying structure of a relatively large set of variables. The EFA has a wide range of applications due to its properties such as data reduction. In this paper, the fuzzy EFA (FEFA) method was developed to maintain the uncertain nature of the data related to the variables. The FEFA method is used to construct an e-learning service quality assessment model. Several assignment criteria have been classified to identify the strengths and weaknesses of an e-learning system, to provide an information system for educational institutions, and rank these information systems. The assessment measures of an e-learning service are determined from the students' and users' perspectives by exploring previous models and using open questionnaires. In addition, due to the typical uncertainty of assessment indicators, a questionnaire was designed with triangular fuzzy numbers to increase the value of the information collected from evaluating e-learning users. By implementing the developed FEFA, an e-learning assessment model was constructed with 13 latent variables including reliable infrastructure, benefits and financial support, government support, perception and knowledge, educational facilities, quality of holding classes, entrance conditions, meeting the needs of students, process of education, planning, flexibility of courses, professors' opinion, and information exchange.																	1562-2479	2199-3211				SEP	2020	22	6			SI		1772	1785		10.1007/s40815-020-00901-1		JUL 2020											
J								Non-weighted Asynchronous H-infinity Filtering for Continuous-Time Switched Fuzzy Systems	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Non-weighted asynchronous H-infinity filtering; Continuous-time switched nonlinear systems; Takagi-Sugeno (T-S) fuzzy models; Time-scheduled fuzzy multiple Lyapunov function (TSFMLF)	NONLINEAR-SYSTEMS; LYAPUNOV FUNCTION; ASYMPTOTIC STABILITY; LINEAR-SYSTEMS; DELAY; STABILIZATION; GAIN	This paper focuses on the non-weighted asynchronous H-infinity filtering problem for a class of continuoustime switched nonlinear systems. The nonlinearities of subsystems are described by Takagi-Sugeno (T-S) fuzzy models. Using the information of switching instants, the filters are designed to be time-scheduled and separately in the asynchronous and synchronous time intervals. Based on a new time-scheduled fuzzy multiple Lyapunov function (TSFMLF), sufficient conditions are achieved to guarantee the switched filtering error system is globally asymptotically stable with a non-weighted H-infinity performance. Finally, an example is presented to demonstrated the effectiveness of the theoretical results.																	1562-2479	2199-3211				SEP	2020	22	6			SI		1892	1904		10.1007/s40815-020-00873-2		JUL 2020											
J								Gauging human visual interest using multiscale entropy analysis of EEG signals	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Human-computer interaction; Electroencephalogram; Artificial neural networks; Emotion; Enjoyment; Multiscale entropy	EMOTION RECOGNITION; ENJOYMENT	Gauging human emotion can be of great benefit in many applications, such as marketing, gaming, and medicine. In this paper, we build a machine learning model that estimates the enjoyment and visual interest level of individuals experiencing museum content. The input to the model is comprised of 8-channel electroencephalogram signals, which we processed using multiscale entropy analysis to extract three features: the mean, slope of the curve, and complexity index (i.e., the area under the curve). Then, the number of features was drastically reduced using principle component analysis without a notable loss of accuracy. Multivariate analysis of variance showed that there exists a statistically significant correlation (i.e.,p<0.05) between the extracted features and the enjoyment level. Moreover, the classification model was able to predict the enjoyment level with a mean squared error of 0.1474 and an accuracy of 98.0%, which outperforms methods in the existing literature.																	1868-5137	1868-5145															10.1007/s12652-020-02381-5		JUL 2020											
J								A multi objective binary bat approach for testcase selection in object oriented testing	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Test case selection; Software testing; Genetic algorithm (GA); And multi-objective BAT algorithm	ALGORITHM	Time and resources are usually neglected areas in the life cycle of software development. So, these become the primary constraints in software testing. Optimization of a test suite is quite crucial in reducing the complexity of the testing phase and selection of the test cases by eliminating redundant data; this is critical for defining the strategies. Most of the work in literature employs single-objective optimization methods. Though these are not always efficient, these play a critical role in the selection of a test case. Test case selection is, however, non-deterministic. Selection of test cases using Parallel Programming is treated as a complex task due to the need for higher performance in Parallel Computing. Parallel Computing can be stated as a combination of Computational mechanisms and Mathematical techniques. Hence, this investigation proposes a novel BAT algorithm for multi-objective optimization. It has code coverage as well as Object-oriented testing strategies. Comparing the experimental results with the Genetic Algorithm (GA), it is observed that the proposed method has faster convergence with adequate code coverage.																	1868-5137	1868-5145															10.1007/s12652-020-02360-w		JUL 2020											
J								Geometric and decentralized approach for localization in wireless sensor network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										WSN; Localization; Mobile anchor; Spiral trajectory; Cloud of points; Geometric center; RSSI	PROTOCOLS	The main function of a sensor node is to collect data from its environment and forward it to base station. In the absence of further information concerning their locations, those data will be unnecessary. Hence, developing algorithms for localizing all nodes of wireless sensor network is extremely important. We present in this paper, a new approach to determine geographical coordinates of unknown nodes, by using mobile anchor. The mobile anchor adopts a spiral trajectory, and diffuses its position periodically during its travel. The proposed approach uses Received Signal Strength Indicator to estimate distance with all broadcast messages received from mobile anchor. To calculate position, our approach determines a cloud of points that surround the solution; these points are selected from the set of intersection points of all beacons received by unknown node, by considering some constraints. The estimated position of unknown node represents the geometric center of this cloud. The behavior of our algorithm was studied by varying some metrics; the average error was minimized compared to those proposed in literature.																	1868-5137	1868-5145															10.1007/s12652-020-02240-3		JUL 2020											
J								Enhanced security in cloud applications using emerging blockchain security algorithm	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Blockchain security; Cloud applications; Distributed and parallel application; Online payment; E-booking		Cloud computing is one of the biggest distributed platforms accomplishing with parallel processing. Cloud computing is a household buzzword. A highest portion of data transmission in entire world is using cloud. Since cloud is distributed and involved parallel processing, it is not assured that it can provide security. One of the invisible technologies changing the world is block chain (BC). Various core functions of Governments, societies, and economics using internet are highly secured due to blockchain. Blockchain is a historical record about every digital activity happen in the internet. Blockchain is a continuous collection of blocks interlinked by chains which encrypt blocks, and it cannot be modified by anyone in the world. Most of the emerging applications are suffering from intruders, anomalies, thieves and insurance companies fetching the personal information and disturbing the customer daily activities. In order to provide enhance and tighten the security level, this paper focused on designing and implementing a blockchain security for cloud applications. The entire process of the paper is simulated using JAVA and Cloud Simulator for verifying the performance. It is also evaluated by comparing the performance with the existing approaches.																	1868-5137	1868-5145															10.1007/s12652-020-02339-7		JUL 2020											
J								Prediction of atherosclerosis pathology in retinal fundal images with machine learning approaches	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Atherosclerosis; Retinal imaging; Enhanced Bayesian classifier; Retinal vascular imaging; Blood vessels		Atherosclerosis is a common cause of cardiac attack and its early detection prevents further complications. In this paper, a research concept is proposed focusing on a novel method of classification system. This method is carried out with image features derived from fundus photographs. It depends upon the arteries and vein classification process and also by the morphological appearance. Further, the proposed mixed algorithm, by using the retina fundal images, this method achieves an accuracy of detecting Atherosclerosis. In spite of the method being somewhat a hard one, of late, several methods are developed which employ advanced retinal photographic imaging techniques. These techniques involve characterizing, measuring and quantifying any variations and dissimilarities in the retinal structure. The hallmark of these methods, which have both qualitative and quantitative prediction, illustrated the allied symptoms found on cardiovascular diseases. This paper deals with providing the accurate input for the atherosclerosis detection by way of image preprocessing method. The study focuses on the reducing the disease independent variations without damaging any information related to the differences between the images of healthy and atherosclerotic eyes. The propose method enables correction of illumination in the blood vessels, by repainting them. Further, there is a normalization of the focus region for the feature extraction and classification process. Finally, Enhanced Bayesian Arithmetic Classifier (EBAC) is implemented for effective classification of the blood vessels. MATLAB software of 2014b version is employed for deriving the simulation results.																	1868-5137	1868-5145															10.1007/s12652-020-02294-3		JUL 2020											
J								Ameliorated monopole antenna with perforated ground for breast tumor detection	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Specific absorption rate (SAR); Ultra wide band antenna (UWB); Breast model; Microwave imaging	DIELECTRIC-PROPERTIES; CANCER; TISSUES; MAMMOGRAPHY; ARRAY	This letter confer an ameliorated monopole antenna for locating the tumor in breast tissue. The posited UWB antenna with a size of 35 x 30 mm provides good impedance matching, gain and radiation characteristics over the wide range of 3.1 GHz to 12.8 GHz. Homogeneous breast model was enlarged to analyze the area as well as size of the tumor in breast, The absorbed power and the SAR for average mass of 10 g of breast tissue at different frequency, different size of the tumor and different distance among the antenna and the breast model were analyzed. In order to locate the tumor, the coordinates of the maximum SAR value are discerned. The antenna and breast phantom were designed using CST studio simulator.																	1868-5137	1868-5145															10.1007/s12652-020-02380-6		JUL 2020											
J								A tabu search for the design of capacitated rooted survivable planar networks	JOURNAL OF HEURISTICS										Capacitated rooted survivable networks; Planar graphs; Tabu search		Consider a rooted directed graphGwith a subset of vertices called terminals, where each arc has a positive integer capacity and a non-negative cost. For a given positive integerk, we say thatGisk-survivable if every of its subgraphs obtained by removing at mostkarcs admits a feasible flow that routes one unit of flow from the root to every terminal. We aim at determining ak-survivable subgraph ofGof minimum total cost. We focus on the case where the input graphGis planar and propose a tabu search algorithm whose main procedure takes advantage of planar graph duality properties. In particular, we prove that it is possible to test thek-survivability of a planar graph by solving a series of shortest path problems. Experiments indicate that the proposed tabu search algorithm produces optimal solutions in a very short computing time, when these are known.																	1381-1231	1572-9397				DEC	2020	26	6					829	850		10.1007/s10732-020-09453-x		JUL 2020											
J								Motion Planning by Sampling in Subspaces of Progressively Increasing Dimension	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Motion and path planning; Redundant robots	RRT; OPTIMIZATION; CONFIGURATION; EXPLORATION; ROADMAPS	This paper introduces an enhancement to traditional sampling-based planners, resulting in efficiency increases for high-dimensional holonomic systems such as hyper-redundant manipulators, snake-like robots, and humanoids. Despite the performance advantages of modern sampling-based motion planners, solving high dimensional planning problems in near real-time remains a considerable challenge. The proposed enhancement to popular sampling-based planning algorithms is aimed at circumventing the exponential dependence on dimensionality, by progressively exploring lower dimensional volumes of the configuration space. Extensive experiments comparing the enhanced and traditional version of RRT, RRT-Connect, and Bidirectional T-RRT on both a planar hyper-redundant manipulator and the Baxter humanoid robot show significant acceleration, up to two orders of magnitude, on computing a solution. We also explore important implementation issues in the sampling process and discuss the limitations of this method.																	0921-0296	1573-0409															10.1007/s10846-020-01217-w		JUL 2020											
J								SNOWL model: social networks unification-based semantic data integration	KNOWLEDGE AND INFORMATION SYSTEMS										Social networks; Semantic data integration; Knowledge discovery; SPARQL query; RML	ONTOLOGY; WEB; DESIGN; SYSTEM; IOT	Integrating social networks data in the process of promoting business and marketing applications is widely addressed by several researchers. However, regarding the isolation between social network platforms managing such data has become a challenging task facing data scientist. In this respect, the present paper is designed to put forward a special semantic data integration approach, whereby a unified presentation and access to social networks data can be maintained. To this end, the novel SNOWL (Social Network OWL) ontology aims to provide a new social network content modeling, following the UPON Lite ontology-construction methodology. The advanced ontology is not created from scratch; it is but a continuation of some previously devised ontologies, elaborated to integrate an additional selection of newly incorporated social entities, such as content and user popularity. Additionally, and for an effective advantage of the model to be gained, a special mapping of the social networks data has been firstly implemented to the designed ontology, developed on the basis of the RML mapping language. Secondly, the SNOWL ontology is evaluated through the OOPS! Pitfall tool. Finally, a set of SPARQL-based services has also been designed on top of the SNOWL ontology in a bid to ensure a unified access to the mapped social data.																	0219-1377	0219-3116				NOV	2020	62	11					4297	4336		10.1007/s10115-020-01498-5		JUL 2020											
J								Embedding-based Silhouette community detection	MACHINE LEARNING										Node embedding; Community detection; Network analysis; Unsupervised learning	NETWORKS	Mining complex data in the form of networks is of increasing interest in many scientific disciplines. Network communities correspond to densely connected subnetworks, and often represent key functional parts of real-world systems. This paper proposes the embedding-based Silhouette community detection (SCD), an approach for detecting communities, based on clustering of network node embeddings, i.e. real valued representations of nodes derived from their neighborhoods. We investigate the performance of the proposed SCD approach on 234 synthetic networks, as well as on a real-life social network. Even though SCD is not based on any form of modularity optimization, it performs comparably or better than state-of-the-art community detection algorithms, such as the InfoMap and Louvain. Further, we demonstrate that SCD's outputs can be used along with domain ontologies in semantic subgroup discovery, yielding human-understandable explanations of communities detected in a real-life protein interaction network. Being embedding-based, SCD is widely applicable and can be tested out-of-the-box as part of many existing network learning and exploration pipelines.																	0885-6125	1573-0565															10.1007/s10994-020-05882-8		JUL 2020											
J								Robust Subspace Clustering via Latent Smooth Representation Clustering	NEURAL PROCESSING LETTERS										Subspace clustering; Smooth representation; Sparse representation; Latent subspace	LOW-RANK; SEGMENTATION; ALGORITHM	Subspace clustering aims to group high-dimensional data samples into several subspaces which they were generated. Among the existing subspace clustering methods, spectral clustering-based algorithms have attracted considerable attentions because of their predominant performances shown in many subspace clustering applications. In this paper, we proposed to apply smooth representation clustering (SMR) to the reconstruction coefficient vectors which were obtained by sparse subspace clustering (SSC). Because the reconstruction coefficient vectors could be regarded as a kind of good representations of original data samples, the proposed method could be considered as a SMR performed in a latent subspace found by SSC and hoped to achieve better performances. For solving the proposed latent smooth representation algorithm (LSMR), we presented an optimization method and also discussed the relationships between LSMR with some related algorithms. Finally, experiments conducted on several famous databases demonstrate that the proposed algorithm dominates the related algorithms.																	1370-4621	1573-773X				OCT	2020	52	2			SI		1317	1337		10.1007/s11063-020-10306-8		JUL 2020											
J								Finite/Fixed-Time Bipartite Synchronization of Coupled Delayed Neural Networks Under a Unified Discontinuous Controller	NEURAL PROCESSING LETTERS										Bipartite synchronization; Discontinuous controllers; Finite time; Fixed time	MULTIAGENT SYSTEMS; ROBUST STABILIZATION; COMPLEX NETWORKS; OUTPUT-FEEDBACK; CONSENSUS; STABILITY	This paper considers the finite-time and fixed-time bipartite synchronization (FFTBS) of coupled delayed neural networks (CDNNs) under signed graphs. For the structurally balanced or unbalanced network topology, both the goals of FFTBS of CDNNs are achieved simultaneously by a unified discontinuous controller. Some sufficient criterion are obtained to ensure the FFTBS under the new designed protocols, and the corresponding settling times are estimated as well. Finally, two simulations are established to verify the validity and effectiveness of the designs.																	1370-4621	1573-773X				OCT	2020	52	2			SI		1359	1376		10.1007/s11063-020-10308-6		JUL 2020											
J								Partial Pinning Control for the Synchronization of Fractional-Order Directed Complex Networks	NEURAL PROCESSING LETTERS										Fractional-order; Partial pinning control; DAG condensation; Layering theory; ControlRank algorithm		This paper mainly studies the synchronization problem of fractional-order directed complex networks through partial pinning control. Unlike other papers, the network studied in this paper is neither strongly connected nor contains directed spanning trees. By utilizing the directed acyclic graph condensation and Layering theory, the network is decomposed into several strong connected components and then divided into layers. It proved that all or part of nodes in the network can achieve synchronization with the pinner's trajectory, only when the root strong connected components in the upstream of these nodes are pinned and satisfy some sufficient conditions. In addition, according to the ControlRank algorithm, an optimized strategy is designed to solve the problem of the optimal selection of the pinning nodes to ensure the specific nodes of the network can achieve synchronization eventually. Meanwhile the amount of control energy cost will also be given in this paper. Finally, two simulation examples are given to verify the reliability and feasibility of the optimized algorithm.																	1370-4621	1573-773X				OCT	2020	52	2			SI		1427	1444		10.1007/s11063-020-10315-7		JUL 2020											
J								Knowledge Acquisition and Design Using Semantics and Perception: A Case Study for Autonomous Robots	NEURAL PROCESSING LETTERS										Knowledge construction; Ontologies; Robots; Neural networks	COGNITIVE ARCHITECTURES; ARTIFICIAL CURIOSITY; SYSTEM	The pervasive use of artificial intelligence and neural networks in several different research fields has noticeably improved multiple aspects of human life. The application of these techniques to machines has made them progressively more "intelligent" and able to solve tasks considered extremely complex for a human being. This technological evolution has deeply influenced the way we interact with machines. Purely symbolic artificial intelligence and techniques like ontologies, have also been successfully used in the past applied to robotics, but have also shown some limitations and failings in the knowledge construction task. In fact, the exhibited "intelligence" is rarely the result of a real autonomous decision, but it is rather hard-encoded in the machine. While a number of approaches have already been proposed in literature concerning knowledge acquisition from the surrounding environment, they are either exclusively based on low-level features or they involve solely high-level semantics-based attributes. Moreover, they often don't use a general high-level knowledge base for grounding the acquired knowledge. In this contexts, the use of semantics technologies, such as ontologies, is mostly employed for action-oriented tasks. In this article we propose an extension of a novel approach for knowledge acquisition based on a general semantic knowledge-base and the fusion of semantics and visual information by means of neural networks and ontologies. The proposed approach has been implemented on a humanoid robotic platform and the experimental results are shown and discussed.																	1370-4621	1573-773X															10.1007/s11063-020-10311-x		JUL 2020											
J								Hybrid many-objective cuckoo search algorithm withLevyand exponential distributions	MEMETIC COMPUTING										HMaOCS; Cuckoo search; Levydistribution; Exponential distribution	EVOLUTIONARY ALGORITHM; SELECTION	Hybrid many-objective cuckoo search algorithm (HMaOCS) is a newly proposed method for Many-objective optimization problems (MaOPs), and has achieved promising performance. However,Levyand Gaussian distributions used in global search manner of HMaOCS is originally proposed for optimization problems with one objective, and they are not suitable for MaOPs as illustrated in this paper. To further exploit the potential of HMaOCS, this paper investigates four different probability distributions and their six corresponding combinations. Comparison results illustrate that the combination ofLevyand Exponential distributions is able to greatly improve HMaOCS. On the basis of comparison results and analysis on both DTLZ and WFG test suites with 2, 3, 4, 6, 8 and 10 objectives, it can be concluded that HMaOCS withLevyand Exponential distributions exhibits better performance compared with most advanced algorithms.																	1865-9284	1865-9292				SEP	2020	12	3					251	265		10.1007/s12293-020-00308-3		JUL 2020											
J								A dynamical system approach for detection and reaction to human guidance in physical human-robot interaction	AUTONOMOUS ROBOTS											FORCE	A seamless interaction requires two robotic behaviors: the leader role where the robot rejects the external perturbations and focuses on the autonomous execution of the task, and the follower role where the robot ignores the task and complies with human intentional forces. The goal of this work is to provide (1) a unified robotic architecture to produce these two roles, and (2) a human-guidance detection algorithm to switch across the two roles. In the absence of human-guidance, the robot performs its task autonomously and upon detection of such guidances the robot passively follows the human motions. We employ dynamical systems to generate task-specific motion and admittance control to generate reactive motions toward the human-guidance. This structure enables the robot to reject undesirable perturbations, track the motions precisely, react to human-guidance by providing proper compliant behavior, and re-plan the motion reactively. We provide analytical investigation of our method in terms of tracking and compliant behavior. Finally, we evaluate our method experimentally using a 6-DoF manipulator.																	0929-5593	1573-7527				NOV	2020	44	8					1411	1429		10.1007/s10514-020-09934-9		JUL 2020											
J								A further study on biologically inspired feature enhancement in zero-shot learning	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Zero-shot learning; Feature enhancement; Feature transfer; Biological taxonomy	CLASSIFICATION	Most of the zero-shot learning (ZSL) algorithms currently use the pre-trained models trained on ImageNet as their feature extractor, which is considered to be an effective method to improve the feature extraction ability of the ZSL models. However, our research found that this practice is difficult to work well if the training data used by the ZSL task differs greatly from ImageNet. Although one can adapt the pre-trained models to the ZSL task with fine-tuning methods, it turns out that the extractors obtained in this way cannot be guaranteed to be friendly to the unseen classes. To solve these problems, we have further studied a biologically inspired feature enhancement framework for ZSL that we proposed earlier and re-fined its biological taxonomy-based selection method for choosing auxiliary datasets. Moreover, we have proposed a word2vec-based selection strategy as a supplement to the biologically inspired selection method for the first time and experimentally proved the inherent unity of these two methods. Extensive experimental results show that our proposed method can effectively improve the generalization ability of the ZSL model and achieve state-of-the-art results on benchmarks. We have also explained the experimental phenomena through the way of feature visualization.																	1868-8071	1868-808X															10.1007/s13042-020-01170-y		JUL 2020											
J								Defining a vibrotactile toolkit for digital musical instruments: characterizing voice coil actuators, effects of loading, and equalization of the frequency response	JOURNAL ON MULTIMODAL USER INTERFACES										Vibrotactile feedback; Characterization; Total harmonic distortion (THD); Non-linearity; Equalization; Digital musical instruments (DMIs); Haptic feedback	SERIES	The integration of vibrotactile feedback in digital music instruments (DMIs) is thought to improve the instrument's response and make it more suitable for expert musical interactions. However, given the extreme requirements of musical performances, there is a need for solutions allowing for independent control of frequency and amplitude over a wide frequency bandwidth (40-1000 Hz) and low harmonic distortion, so that flexible and high-quality vibrotactile feedback can be displayed. In this paper, we evaluate cost-effective and portable solutions that meet these requirements. We first measure the magnitude-frequency and harmonic distortion characteristics of two vibrotactile actuators, where the harmonic distortion is quantified in the form of total harmonic distortion (THD). The magnitude-frequency and THD characteristics in two unloaded cases (actuator suspended freely or placed on a sandbag) are observed to be largely identical, with minor attenuation for actuators placed on the sandbag. Loading the actuator (when placed in a DMI) brings resonant features to its magnitude-frequency characteristics, increasing the output THD and imposing a dampening effect. To equalize the system's frequency response, an autoregressive method that automatically estimates minimum-phase filter parameters is introduced, which by design, remains stable upon inversion A practical use of this method is demonstrated by implementing vibrotactile feedback in the poly vinyl chloride chassis of an unfinished DMI, thet-Stick. We finally compare the result of equalization by performing sinesweep measurements on the implementation and discuss the degree of equalization achieved using it.																	1783-7677	1783-8738				SEP	2020	14	3			SI		285	301		10.1007/s12193-020-00340-0		JUL 2020											
J								The logic induced by effect algebras	SOFT COMPUTING										Lattice effect algebra; Lattice effect implication algebra; Effect algebra; Effect implication algebra; Finite effect algebra; Gentzen system; Algebraic semantics		Effect algebras form an algebraic formalization of the logic of quantum mechanics. For lattice effect algebras E, we investigate a natural implication and prove that the implication reduct of E is term equivalent to E. Then, we present a simple axiom system in Gentzen style in order to axiomatize the logic induced by lattice effect algebras. For effect algebras which need not be lattice-ordered, we introduce a certain kind of implication which is everywhere defined but whose result need not be a single element. Then, we study effect implication algebras and prove the correspondence between these algebras and effect algebras satisfying the ascending chain condition. We present an axiom system in Gentzen style also for not necessarily lattice-ordered effect algebras and prove that it is an algebraic semantics for the logic induced by finite effect algebras.																	1432-7643	1433-7479				OCT	2020	24	19					14275	14286		10.1007/s00500-020-05188-w		JUL 2020											
J								DBN based SD-ARX model for nonlinear time series prediction and analysis	APPLIED INTELLIGENCE										Deep belief network; nonlinear time series modeling; hybrid model; stability	IDENTIFICATION; SYSTEMS	One of the main purposes of nonlinear system modeling is to design model-based controllers such as model predictive control (MPC). A group of deep belief networks (DBNs) are used to approximate the function type coefficients of a state dependent autoregressive model with exogenous variables (SD-ARX), which can represent nonlinear dynamics, and thus a DBN-based state-dependent ARX (DBN-ARX) model is obtained in this paper. The DBN-ARX model has the function approximation ability of single DBN model and the nonlinear description advantage of SD-ARX model. All parameters of the DBN-ARX model are estimated by the pre-training and fine-tuning strategies and the stability condition of the model are also discussed. The proposed DBN-ARX model is a pseudo-linear ARX model identified offline, and its function type coefficients are composed of the operating-point dependent DBNs. The usefulness of the DBN-ARX model is illustrated by modeling a continuously stirred tank reactor (CSTR) time series, Box and Jenkins data, a nonlinear process and a water tank system. The four experimental results show that the one-step-ahead and multi-step-ahead prediction accuracy of the proposed DBN-ARX model is improved comparing with the modeling results of several existing models.																	0924-669X	1573-7497															10.1007/s10489-020-01804-2		JUL 2020											
J								Consensus function based on cluster-wise two level clustering	ARTIFICIAL INTELLIGENCE REVIEW										Consensus clustering; K-means; Similarity criterion; Machine learning; Data mining	COMBINING MULTIPLE CLUSTERINGS; COMBINATION SCHEME; ENSEMBLE FRAMEWORK; SELECTION; EXTRACTION; WISDOM; MATRIX	The ensemble clustering tries to aggregate a number of basic clusterings with the aim of producing a more consistent, robust and well-performing consensus clustering result. The current paper wants to introduce an ensemble clustering method. The proposed method, called consensus function based on two level clustering (CFTLC), introduces a new consensus clustering where it makes a cluster clustering task through applying an average hierarchical clustering on a cluster-cluster similarity matrix obtained by an innovative similarity metric. By applying the average hierarchical clustering algorithm, a set of meta clusters has been attained. Considering each meta cluster as a consensus cluster in the consensus clustering output, it then assigns each data point to a meta cluster through defining an object-cluster similarity. Before doing anything, CFTLC converts the primary partitions into a binary cluster representation where the primary ensemble has been broken into a number of basic binary clusters (BC). CFTLC first combines the basic BCs with the maximum cluster-cluster similarity. This step is iterated as long as a predefined number of meta clusters are ready. At the subsequent step, it assigns each data point to exactly one meta cluster. The proposed method has been experimentally compared with the state of the art clustering algorithms in terms of accuracy and robustness.																	0269-2821	1573-7462															10.1007/s10462-020-09862-1		JUL 2020											
J								Performance evaluation of half bridge AC-AC resonant converter by using various load in domestic purpose of induction heating application	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Temperature; THD; Induction heating; Half bridge AC converter		A new 28 kHz is proposed for the generation of output power of 775 watts with a high frequency to drive a domestic induction heating application using AC convertor. The proposed system deals with the output parameters of THD, PF, and output power in the simulation. The simulation carried out for the various loaded conditions and selected with respect to various parameters like power factor, THD, temperature, time, higher power, uniform distribution of heat through the IH system. The requirement of the proposed system is to have a highly reliable and adaptable to different loaded condition. A prototype of the proposed system was implemented and verified with the system developed. Experiment was carried out with different medium (liquid) and results are measured and graphically represented. The validations of the electrical and thermal parameters are focused in the proposed and implemented system.																	1868-5137	1868-5145															10.1007/s12652-020-02374-4		JUL 2020											
J								Interpretable policy derivation for reinforcement learning based on evolutionary feature synthesis	COMPLEX & INTELLIGENT SYSTEMS										Reinforcement learning; Genetic programming; Policy derivation; Explainable machine learning		Reinforcement learning based on the deep neural network has attracted much attention and has been widely used in real-world applications. However, the black-box property limits its usage from applying in high-stake areas, such as manufacture and healthcare. To deal with this problem, some researchers resort to the interpretable control policy generation algorithm. The basic idea is to use an interpretable model, such as tree-based genetic programming, to extract policy from other black box modes, such as neural networks. Following this idea, in this paper, we try yet another form of the genetic programming technique, evolutionary feature synthesis, to extract control policy from the neural network. We also propose an evolutionary method to optimize the operator set of the control policy for each specific problem automatically. Moreover, a policy simplification strategy is also introduced. We conduct experiments on four reinforcement learning environments. The experiment results reveal that evolutionary feature synthesis can achieve better performance than tree-based genetic programming to extract policy from the neural network with comparable interpretability.																	2199-4536	2198-6053				OCT	2020	6	3					741	753		10.1007/s40747-020-00175-y		JUL 2020											
J								Clinical AI: opacity, accountability, responsibility and liability	AI & SOCIETY										Artificial intelligence; Clinical decision making; Law; Ethics; Responsibility; Accountability	CARE	The aim of this literature review was to compose a narrative review supported by a systematic approach to critically identify and examine concerns about accountability and the allocation of responsibility and legal liability as applied to the clinician and the technologist as applied the use of opaque AI-powered systems in clinical decision making. This review questions (a) if it is permissible for a clinician to use an opaque AI system (AIS) in clinical decision making and (b) if a patient was harmed as a result of using a clinician using an AIS's suggestion, how would responsibility and legal liability be allocated? Literature was systematically searched, retrieved, and reviewed from nine databases, which also included items from three clinical professional regulators, as well as relevant grey literature from governmental and non-governmental organisations. This literature was subjected to inclusion/exclusion criteria; those items found relevant to this review underwent data extraction. This review found that there are multiple concerns about opacity, accountability, responsibility and liability when considering the stakeholders of technologists and clinicians in the creation and use of AIS in clinical decision making. Accountability is challenged when the AIS used is opaque, and allocation of responsibility is somewhat unclear. Legal analysis would help stakeholders to understand their obligations and prepare should an undesirable scenario of patient harm eventuate when AIS were used.																	0951-5666	1435-5655															10.1007/s00146-020-01019-6		JUL 2020											
J								Wind disturbance rejection for unmanned aerial vehicles using acceleration feedback enhanced H-infinity method	AUTONOMOUS ROBOTS										Wind disturbance rejection; Acceleration feedback; Unmanned aerial vehicle; H-infinity control	TRAJECTORY TRACKING CONTROL; OBSERVER; POSITION; UAV	One of the most critical issues for unmanned aerial vehicle (UAV) safety and precision flight is wind disturbance. To this end, this paper presents an acceleration feedback (AF) enhanced H-infinity method for UAV flight control against wind disturbance and its application on a hex-rotor platform. The dynamics of the UAV system are decoupled into attitude control and position control loops. A hierarchical H-infinity controller is then designed for the decoupled system. Finally, an AF-enhanced method is introduced into the decoupled system without altering the original control structure. The stability of the AF-enhanced H-infinity method for the UAV system is analyzed and verified using the H-infinity theory. Two types of wind disturbances-continuous and gusty winds-are considered and analyzed for guiding the AF-enhanced controller design. The results of an experimental comparison between the H-infinity controller and the AF-enhanced H-infinity controller against wind disturbances demonstrate the robustness and effectiveness of the proposed method for wind disturbance rejection.																	0929-5593	1573-7527				SEP	2020	44	7			SI		1271	1285		10.1007/s10514-020-09935-8		JUL 2020											
J								MIDIA: exploring denoising autoencoders for missing data imputation	DATA MINING AND KNOWLEDGE DISCOVERY										Missing data imputation; Denoising autoencoder; MIDIA; Deep learning	LIKELIHOOD-BASED INFERENCE; VALUES	Due to the ubiquitous presence of missing values (MVs) in real-world datasets, the MV imputation problem, aiming to recover MVs, is an important and fundamental data preprocessing step for various data analytics and mining tasks to effectively achieve good performance. To impute MVs, a typical idea is to explore the correlations amongst the attributes of the data. However, those correlations are usually complex and thus difficult to identify. Accordingly, we develop a new deep learning model calledMIssing Data Imputation denoising Autoencoder(MIDIA) that effectively imputes the MVs in a given dataset by exploring non-linear correlations between missing values and non-missing values. Additionally, by considering various data missing patterns, we propose two effective MV imputation approaches based on the proposed MIDIA model, namely MIDIA-Sequential and MIDIA-Batch. MIDIA-Sequential imputes the MVs attribute-by-attribute sequentially by training an independent MIDIA model for each incomplete attribute. By contrast, MIDIA-Batch imputes the MVs in one batch by training a uniform MIDIA model. Finally, we evaluate the proposed approaches by experimentation in comparison with existing MV imputation algorithms. The experimental results demonstrate that both MIDIA-Sequential and MIDIA-Batch achieve significantly higher imputation accuracy compared with existing solutions, and the proposed approaches are capable of handling various data missing patterns and data types. Specifically, MIDIA-Sequential performs better than MIDIA-Batch for data with monotone missing pattern, while MIDIA-Batch performs better than MIDIA-Sequential for data with general missing pattern.																	1384-5810	1573-756X				NOV	2020	34	6					1859	1897		10.1007/s10618-020-00706-8		JUL 2020											
J								Artificial intelligence and game theory controlled autonomous UAV swarms	EVOLUTIONARY INTELLIGENCE										ai; Game theory; manet; Autonomousuav; Swarm; Bio-inspired computation	GENETIC ALGORITHM; NETWORK	Autonomous unmanned aerial vehicles (uavs) operating as a swarm can be deployed in austere environments, where cyber electromagnetic activities often require speedy and dynamic adjustments to swarm operations. Use of central controllers,uavsynchronization mechanisms or pre-planned set of actions to control a swarm in such deployments would hinder its ability to deliver expected services. We introduce artificial intelligence and game theory based flight control algorithms to be run by each autonomousuavto determine its actions in near real-time, while relying only on local spatial, temporal and electromagnetic (em) information. Eachuavusing our flight control algorithms positions itself such that the swarm maintains mobile ad-hoc network (manet) connectivity and uniform asset distribution over an area of interest. Typical tasks for swarms using our algorithms include detection, localization and tracking of mobileemtransmitters. We present a formal analysis showing that our algorithms can guide a swarm to maintain a connectedmanet, promote a uniform network spreading, while avoiding overcrowding with other swarm members. We also prove that they maintainmanetconnectivity and, at the same time, they can lead a swarm of autonomousuavs to follow or avoid anemtransmitter. Simulation experiments inopnetmodeler verify the results of formal analysis that our algorithms are capable of providing an adequate area coverage over a mobileemsource and maintainmanetconnectivity. These algorithms are good candidates for civilian and military applications that require agile responses to the changes in dynamic environments for tasks such as detection, localization and tracking mobileemtransmitters.																	1864-5909	1864-5917															10.1007/s12065-020-00456-y		JUL 2020											
J								Dimensionality reduction of hyperspectral image using signal entropy and spatial information in genetic algorithm with discrete wavelet transformation	EVOLUTIONARY INTELLIGENCE										Band selection; Hyperspectral; Genetic algorithm; Signal entropy; Spatial information; Wavelet	BAND SELECTION; FEATURE-EXTRACTION	Band selection is being performed in hyperspectral imagery as a dimensionality reduction measure to enhance the efficiency of processing and analysis of the data. In this paper, a genetic algorithm based method is proposed that uses weighted combination of signal entropy and image spatial information in the objective function. The spatial dimension, that also includes huge redundancy, has been reduced using discrete wavelet transformation to make the method more time efficient without compromising the quality of the output. The performance of the method is evaluated by classifying the hyperspectral image with selected bands and measuring the accuracy of the classified output. The proposed method is also compared with other state of the art methods and found to be more efficient in selecting information rich bands in the hyperspectral data.																	1864-5909	1864-5917															10.1007/s12065-020-00460-2		JUL 2020											
J								Desmogging of still smoggy images using a novel channel prior	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Deep transfer learning; Smog; Restoration; Oblique gradient	HISTOGRAM EQUALIZATION; CONTRAST ENHANCEMENT; MODEL; ALGORITHM; FILTER	Images captured in poor atmospheric circumstances, like smog, rainy, cloudy, fog, smoke, etc., suffer from number of problems such as poor visibility, distortion of spectral ans spatial information, etc. In this paper, we have considered images taken in smoggy environment. However, optical imaging systems provide only smoggy images, but desmogging process require atmospheric veil and transmission map information. Thus, to restore smoggy images, it is required to predict optical information of smoggy images in an efficient manner. From the extensive review, it has been found that the optical information predicted using various channel prior such as dark channel prior may provide poor results especially when images contain brighter regions, large smog gradient, textured information, etc. Therefore, in this paper, a deep transfer learning (DTL) and oblique gradient profile prior (OGPP) is utilized to approximate the optical information. To train DTL, we have obtained benchmark somggy and smog-free images. Thereafter, DTL model is trained. Smog gradient predicted using DTL is used by OGPP model to recover smog-free images. Comparative analysis prove that the proposed DTL-OGPP based restoration model performs significantly better than the competitive restoration models.																	1868-5137	1868-5145															10.1007/s12652-020-02161-1		JUL 2020											
J								Exploring interaction techniques for 360 panoramas inside a 3D reconstructed scene for mixed reality remote collaboration	JOURNAL ON MULTIMODAL USER INTERFACES										Mixed reality; Remote collaboration; 360 Panorama; 3D scene reconstruction; Interaction methods		Remote collaboration using mixed reality (MR) enables two separated workers to collaborate by sharing visual cues. A local worker can share his/her environment to the remote worker for a better contextual understanding. However, prior techniques were using either 360 video sharing or a complicated 3D reconstruction configuration. This limits the interactivity and practicality of the system. In this paper we show an interactive and easy-to-configure MR remote collaboration technique enabling a local worker to easily share his/her environment by integrating 360 panorama images into a low-cost 3D reconstructed scene as photo-bubbles and projective textures. This enables the remote worker to visit past scenes on either an immersive 360 panoramic scenery, or an interactive 3D environment. We developed a prototype and conducted a user study comparing the two modes of how 360 panorama images could be used in a remote collaboration system. Results suggested that both photo-bubbles and projective textures can provide high social presence, co-presence and low cognitive load for solving tasks while each have its advantage and limitations. For example, photo-bubbles are good for a quick navigation inside the 3D environment without depth perception while projective textures are good for spatial understanding but require physical efforts.																	1783-7677	1783-8738				DEC	2020	14	4			SI		373	385		10.1007/s12193-020-00343-x		JUL 2020											
J								The AAA ABox Abduction Solver System Description	KUNSTLICHE INTELLIGENZ										Description logics; Abduction; Implementation	OWL	AAA is a sound and complete ABox abduction solver based on the Reiter's MHS algorithm and the Pellet reasoner. It supports DL expressivity up to SROIQ (i.e., OWL 2). It supports multiple observations, and allows to specify abducibles.																	0933-1875	1610-1987															10.1007/s13218-020-00685-4		JUL 2020											
J								OCLU-NET for occlusal classification of 3D dental models	MACHINE VISION AND APPLICATIONS										Occlusion; Deep learning; Convolutional neural network; Support vector machine; Random forest regressor; K-nearest neighbours	SUPPORT VECTOR MACHINES; SIMILARITY SEARCH	With the emergence in modern dentistry, the study of dental occlusion has been a subject of major interest. The aim of the present study is to investigate the capabilities of deep learning for the classification of dental occlusion using 3D images that has an exciting impact in several fields of dental anatomy. In present work, the 3D stereolithography (STL) files depicting the dental structures are converted to 2D histograms, using Absolute Angle Shape Distribution (AAD) technique, which are used as an input to deep or machine learning models for classification of dental structures based on the similarity of their shape features. To the best of the authors' knowledge, no solution has been proposed for classification of dental occlusion using deep learning. Thus, an attempt has been made to propose a classification technique for dental occlusion. Based on the experimental analysis, it has been revealed that the deep learning-based convolutional neural network along with AAD performs better as compared to other existing machine learning techniques, with maximum accuracy of 78.95% for occlusion classification. However, the presented study is preliminary, but the experimental outcomes have demonstrated that deep learning is helpful in classifying dental occlusion and it has great application potential in the computer-assisted orthodontic treatment diagnosis.																	0932-8092	1432-1769				JUL 25	2020	31	6							52	10.1007/s00138-020-01102-4													
J								An Analysis of Activation Function Saturation in Particle Swarm Optimization Trained Neural Networks	NEURAL PROCESSING LETTERS										Feed forward neural network; Particle swarm optimization; Saturation; Activation function		The activation functions used in an artificial neural network define how nodes of the network respond to input, directly influence the shape of the error surface and play a role in the difficulty of the neural network training problem. Choice of activation functions is a significant question which must be addressed when applying a neural network to a problem. One issue which must be considered when selecting an activation function is known as activation function saturation. Saturation occurs when a bounded activation function primarily outputs values close to its boundary. Excessive saturation damages the network's ability to encode information and may prevent successful training. Common functions such as the logistic and hyperbolic tangent functions have been shown to exhibit saturation when the neural network is trained using particle swarm optimization. This study proposes a new measure of activation function saturation, evaluates the saturation behavior of eight common activation functions, and evaluates six measures of controlling activation function saturation in particle swarm optimization based neural network training. Activation functions that result in low levels of saturation are identified. For each activation function recommendations are made regarding which saturation control mechanism is most effective at reducing saturation.																	1370-4621	1573-773X				OCT	2020	52	2			SI		1123	1153		10.1007/s11063-020-10290-z		JUL 2020											
J								Kinetic-molecular theory optimization algorithm using opposition-based learning and varying accelerated motion	SOFT COMPUTING										KMTOA; Clustering behavior; Opposition-based learning; Varying accelerated motion; Exploration and exploitation	PARTICLE SWARM OPTIMIZATION	This paper proposes an improved kinetic-molecular theory optimization algorithm (OKMTOA) by analyzing the characteristics of KMTOA cluster behavior and combining the opposition-based learning strategy with varying accelerated motion in physics. The algorithm first applies different opposition-based learning strategies to the population initialization and iterative process of the algorithm. The two-stage strategy is beneficial to improving the quality of the solution set and accelerating the convergence of the algorithm. Then, based on the concept of varying accelerated motion, the acceleration formula is improved to increase the ability to escape local optimum. The experimental results show that the algorithm has good performance in solution precision, convergence speed and can be well applied to the functions with different shift values.																	1432-7643	1433-7479				SEP	2020	24	17					12709	12730		10.1007/s00500-020-05057-6		JUL 2020											
J								The use of grossone in elastic net regularization and sparse support vector machines	SOFT COMPUTING										Elastic net regularization; Grossone; Sparse support vector machines	NUMERICAL-METHODS; INFINITESIMALS; METHODOLOGY; INFINITIES	New algorithms for the numerical solution of optimization problems involving the l(0) pseudo-norm are proposed. They are designed to use a recently proposed computational methodology that is able to deal numerically with finite, infinite and infinitesimal numbers. This new methodology introduces an infinite unit of measure expressed by the numeral (1) (grossone) and indicating the number of elements of the set IN, of natural numbers. We show how the numerical system built upon (1) and the proposed approximation of the l(0) pseudo-norm in terms of (1) can be successfully used in the solution of elastic net regularization problems and sparse support vector machines classification problems.																	1432-7643	1433-7479															10.1007/s00500-020-05185-z		JUL 2020											
J								Estrogens determination through disposable pipette extraction coupled to ultraviolet spectroscopy and nonlinear pseudo-univariate calibration: Solving rank deficiency problems	JOURNAL OF CHEMOMETRICS										DPX; hormones; MCR-ALS; SVM	SUPPORT VECTOR MACHINES; INDEPENDENT COMPONENT ANALYSIS; LIQUID-LIQUID MICROEXTRACTION; CURVE-RESOLUTION; LEAST-SQUARES; ENVIRONMENTAL ESTROGENS; QUANTIFICATION; REGRESSION; CYCLODEXTRINS; CHEMOMETRICS	In this study, the disposable pipette extraction (DPX) approach was employed to prepare water samples containing the hormones 17-beta-estradiol (beta-EST), estrone (ETR), estriol (EST), and 17-alpha-ethinylestradiol (17-EE). Both multivariate and univariate experimental designs were used to optimize the DPX procedure through ultraviolet (UV) and high-performance liquid chromatography (HPLC). The determination of estrogens in water through UV spectroscopy exhibited rank deficiency, and after trying several strategies, it was possible to suggest that the rank deficiency in the spectra was solved by column augmented matrix strategy afterward complexing the hormones with beta-cyclodextrin, in which the relative concentration recovered by the multivariate curve resolution with alternating least squares (MCR-ALS) to each hormone was employed in the development of nonlinear pseudo-univariate calibration models based on support vector machine (SVM) approach. The models were validated through parameters of merit determination and by comparing the models results with those obtained by HPLC. The alternative methodology for the determination of estrogens in water through UV spectroscopy is an environmentally friendly analytical methodology employing less solvent and with high throughput. Moreover, it is not possible to determinate these hormones without using the chemometric approaches, mainly due to the rank deficiency, which needs to be broken to allow for the differentiation between the substances and later modeling.																	0886-9383	1099-128X														e3276	10.1002/cem.3276		JUL 2020											
J								A New Research Model for Higher Risk ACTIVITES Applied to the Use of Small Unmanned Aircraft for Data Gathering Operations	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										VMUTES; Unmanned Aircraft; Risk; Technology Acceptance	TECHNOLOGY ACCEPTANCE MODEL; PLANNED BEHAVIOR; INFORMATION-TECHNOLOGY; PRESERVICE TEACHERS; INTERNET BANKING; INTENTION; EXTENSION; METAANALYSIS; NETWORKS; ADOPTION																		0921-0296	1573-0409															10.1007/s10846-020-01232-x		JUL 2020											
J								Exploiting relay nodes for maximizing wireless underground sensor network lifetime	APPLIED INTELLIGENCE										Wireless underground sensor network; Relay nodes placement; Load balancing; Network lifetime; Beam search; Hopcroft-Karp algorithm	PLACEMENT; MAXIMIZATION; OPTIMIZATION	A major challenge in wireless underground sensor networks is the signal attenuation originated from multi-environment transmission between underground sensor nodes and the above-ground base station. To overcome this issue, an efficient approach is deploying a set of relay nodes aboveground, thereby reducing transmission loss by shortening transmitting distance. However, this introduces several new challenges, including load balancing and transmission loss minimization. This paper tackles the problem of deploying relay nodes to reduce transmission loss under a load balancing constraint by proposing two approximation algorithms. The first algorithm is inspired by Beam Search, combined with a new selection scheme based on Boltzmann distribution. The second algorithm aims to further improve the solutions obtained by the former by reducing the transmission loss. We observe that we can find an optimal assignment between sensor nodes and a set of the chosen relay in polynomial time by reformulating the part of the problem as a bipartite matching problem with minimum cost. Experimental results indicate that the proposed methods perform better than the other existing ones in most of our test instances while reducing the execution time.																	0924-669X	1573-7497															10.1007/s10489-020-01735-y		JUL 2020											
J								A neural network structure specified for representing and storing logical relations	NEURAL COMPUTING & APPLICATIONS										Logical representation and storage; Generative neural network structure; Logical artificial neural network; Adaptivity; Connection structure; Inhibitory link		Logical representation and reasoning is an important aspect of intelligence. Current ANN models are good at perceptual intelligence while they are not good at cognitive intelligence such as logical representation, so researchers have tried to design novel models so as to represent and store logical relations into the neural network structures, called the type of Knowledge-Based Neural Network. However, there is an ambiguous problem that the same neural network structure represents multiple logical relations. It causes the corresponding logical relations not to be read out from these neural network structures which are constructed according to them. To let logical relations stored in the format of neural network and read out from it, this paper studies the direct mapping method between logical relations and neural network structures and proposes a novel model called Probabilistic Logical Generative Neural Network, which is specified for logical relation representation by redesigning the neurons and links. It can make neurons solely for representing things while making links solely for representing logical relations between things, and thus no extra logical neurons and layers are needed. Moreover, the related construction and adjustment methods of the neural network structure are also designed making the neural network structure dynamically constructed and adjusted according to logical relations.																	0941-0643	1433-3058				SEP	2020	32	18			SI		14975	14993		10.1007/s00521-020-04852-4		JUL 2020											
J								Estimation of axial load-carrying capacity of concrete-filled steel tubes using surrogate models	NEURAL COMPUTING & APPLICATIONS										Composite steel and concrete structural members; Artificial intelligence; ANFIS; Optimization	ARTIFICIAL NEURAL-NETWORKS; TUBULAR STUB COLUMNS; PARTICLE SWARM OPTIMIZATION; CIRCULAR CFT COLUMNS; ABSOLUTE ERROR MAE; COMPRESSIVE STRENGTH; SHEAR-STRENGTH; EXPERIMENTAL BEHAVIOR; DAMAGE DETECTION; NUMERICAL-MODEL	The main objective of the present work is to estimate the load-carrying capacity of concrete-filled steel tubes (CFST) under axial compression using hybrid artificial intelligence (AI) algorithms. In particular, the adaptive neuro-fuzzy inference system (ANFIS) with various metaheuristic optimization methods, such as the biogeography-based optimization (ANFIS-BBO), the particle swarm optimization (ANFIS-PSO), and the genetic algorithm (ANFIS-GA), have been employed taking into account the variability of input parameters. Commonly used statistical criteria, such as the coefficient of determination (R-2), the a20-index, and the root mean squared error (RMSE), were utilized to evaluate and compare the effectiveness of the proposed AI models. The Monte Carlo approach was used to propagate the variability in the input space to the predicted output. The results showed that the ANFIS system, optimized by PSO, was the most effective and robust model with respect to three considered criteria (a20-index = 0.881,R-2 = 0.942 and RMSE = 185.631). Sensitivity analysis was performed, indicating that the minor axis length and thickness of the steel tube exhibited the highest contribution to the axial compression load-carrying capacity of the CFST.																	0941-0643	1433-3058															10.1007/s00521-020-05214-w		JUL 2020											
J								"SPOCU": scaled polynomial constant unit activation function	NEURAL COMPUTING & APPLICATIONS										Activation function; SPOCU; SELU; ReLU; Cancer discrimination; Percolation	MOMENTS; CANCER	We address the following problem: given a set of complex images or a large database, the numerical and computational complexity and quality of approximation for neural network may drastically differ from one activation function to another. A general novel methodology, scaled polynomial constant unit activation function "SPOCU," is introduced and shown to work satisfactorily on a variety of problems. Moreover, we show that SPOCU can overcome already introduced activation functions with good properties, e.g., SELU and ReLU, on generic problems. In order to explain the good properties of SPOCU, we provide several theoretical and practical motivations, including tissue growth model and memristive cellular nonlinear networks. We also provide estimation strategy for SPOCU parameters and its relation to generation of random type of Sierpinski carpet, related to the [pppq] model. One of the attractive properties of SPOCU is its genuine normalization of the output of layers. We illustrate SPOCU methodology on cancer discrimination, including mammary and prostate cancer and data from Wisconsin Diagnostic Breast Cancer dataset. Moreover, we compared SPOCU with SELU and ReLU on large dataset MNIST, which justifies usefulness of SPOCU by its very good performance.																	0941-0643	1433-3058															10.1007/s00521-020-05182-1		JUL 2020											
J								Intoxicated person identification using Markov chains and neural networks	NEURAL COMPUTING & APPLICATIONS										Face biometrics; Drunk identification; Markov chains; Neural networks	FACE RECOGNITION	In this work, Markov chains are used to model the statistical behavior of the pixels on the image of the forehead of a person in order to detect intoxication. It is the first time that second-order statistics are used for this purpose. The images were obtained in the thermal infrared region. Intoxication affects blood vessels activity and thus the temperature distribution on the face having a significant effect on the corresponding pixels statistics. The pixels of the forehead images are quantized to 32 Gy levels so that Markov chain models are structured using 32 states. The feature vectors used are the eigenvalues obtained from the first-order transition matrices of the Markov chain models. Since for each person a frame sequence of 50 views is acquired, a cluster of 50 vectors is formed in the 32-dimensional feature space. The feature space is firstly analyzed using projections of the clusters in 3D subspaces of the original 32D feature space. After that, the capability of a simple feed forward neural network to separate the clusters belonging to sober persons from those corresponding to intoxicated persons is investigated. A simple three-layer neural structure has a 98% vector separability success and a 100% cluster separability if the majority voting is considered. Furthermore, the classification problem is faced by excluding from the training procedure either one or five persons and using them in the testing phase. Accordingly, a neural network is trained using all except the excluded data. The obtained neural structure tested with the features of the persons in which it was not trained presents high drunk identification success if the majority voting is considered.																	0941-0643	1433-3058															10.1007/s00521-020-05219-5		JUL 2020											
J								Multi-view clustering via neighbor domain correlation learning	NEURAL COMPUTING & APPLICATIONS										Clustering; Multi-view learning; Neighbor domain correlation; Feature learning	ALGORITHM; DISCOVERY	With the development of data science, more and more data are presented in the form of multi-view. Compared with single-view feature learning, multi-view feature learning is more effective, and it has been successfully applied in many fields. Clustering is a core technology of computer science. Thus, many researchers start to study multi-view clustering. Recently, combining with multi-view feature learning techniques, some multi-view clustering methods have been presented. These methods mainly focus on the multiple features fusion, while most of them ignore the correlations among multiple views. Therefore, it cannot make full use of the advantages of multiple view features. In this paper, we propose a novel approach, named multi-view clustering via neighbor domain correlation learning (MCNDCL) approach. Specifically, MCNDCL learns a discriminant common space for multiple view features. Under the learned common space, the correlations of the consistent neighbor domain are maximized, and the correlations of specific neighbor domain are minimized at the same time. Extensive experimental results on four typical benchmarks, i.e., UCI Digits, Caltech7, BBCSport and CCV, validate the high effectiveness of our proposed approach.																	0941-0643	1433-3058															10.1007/s00521-020-05185-y		JUL 2020											
J								Integrated intelligent computing paradigm for nonlinear multi-singular third-order Emden-Fowler equation	NEURAL COMPUTING & APPLICATIONS										Nonlinear Emden-Fowler equation; Artificial neural networks; Statistical analysis; Genetic algorithms; Singular systems; Active-set algorithm; Hybrid computing	ARTIFICIAL NEURAL-NETWORK; ACTIVE-SET ALGORITHM; GENETIC ALGORITHM; DIFFERENTIAL-EQUATIONS; SERIES SOLUTION; DYNAMICS; DESIGN; SYSTEMS; STABILITY; ANALYZE	In this study, an advance computational intelligence scheme is designed and implemented to solve third-order nonlinear multiple singular systems represented with Emden-Fowler differential equation (EFDE) by exploiting the efficacy of artificial neural networks (ANNs), genetic algorithms (GAs) and active-set algorithm (ASA), i.e., ANN-GA-ASA. In the scheme, ANNs are used to discretize the EFDE for formulation of mean squared error-based fitness function. The optimization task for ANN models of nonlinear multi-singular system is performed by integrated competency GA and ASA. The efficiency of the designed ANN-GA-ASA is examined by solving five different variants of the singular model to check the effectiveness, reliability and significance. The statistical investigations are also performed to authenticate the precision, accuracy and convergence.																	0941-0643	1433-3058															10.1007/s00521-020-05187-w		JUL 2020											
J								Derivation and analysis of parallel-in-time neural ordinary differential equations	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Residual Neural Network; Neural Ordinary Differential Equations; Parareal method; Parallelism-in-time	PARAREAL	The introduction in 2015 of Residual Neural Networks (RNN) and ResNET allowed for outstanding improvements of the performance of learning algorithms for evolution problems containing a "large" number of layers. Continuous-depth RNN-like models called Neural Ordinary Differential Equations (NODE) were then introduced in 2019. The latter have a constant memory cost, and avoid thea priorispecification of the number of hidden layers. In this paper, we derive and analyze a parallel (-in-parameter and time) version of the NODE, which potentially allows for a more efficient implementation than a standard/naive parallelization of NODEs with respect to the parameters only. We expect this approach to be relevant whenever we have access to a very large number of processors, or when we are dealing with high dimensional ODE systems. Moreover, when using implicit ODE solvers, solutions to linear systems with up to cubic complexity are then required for solving nonlinear systems using for instance Newton's algorithm; as the proposed approach allows to reduce the overall number of time-steps thanks to an iterative increase of the accuracy order of the ODE system solvers, it then reduces the number of linear systems to solve, hence benefiting from a scaling effect.																	1012-2443	1573-7470				OCT	2020	88	10					1035	1059		10.1007/s10472-020-09702-6		JUL 2020											
J								The next wave: We will all be data scientists	STATISTICAL ANALYSIS AND DATA MINING										active learning; data science education; high-performance computing; learning community; undergraduate		In the next wave of educating future data scientists, we need to think of all undergraduate students, regardless of background or major, as future data scientists. We should train them in supportive, interdisciplinary environments. Starting from their first day at college, they should be given the opportunity to apply powerful tools to large data sets, using real-world problems. Partnerships with research computing, academic departments, research centers, companies, government, and nonprofits will all be necessary to fully prepare these students for the breadth of the data science workforce.																	1932-1864	1932-1872															10.1002/sam.11476		JUL 2020											
J								Global exponential stability analysis of neural networks with a time-varying delay via some state-dependent zero equations	NEUROCOMPUTING										Neural networks; Time-varying delay; Exponential stability; Lyapunov-Krasovskii functional; State-dependent zero equation	LYAPUNOV-KRASOVSKII FUNCTIONALS; ASYMPTOTIC STABILITY; CRITERIA; SYSTEMS	This paper studies the exponential stability problem of neural networks with a time-varying delay. Firstly, an augmented Lyapunov-Krasovskii functional (LKF) containing a single integral state is constructed. Then a generalized free-matrix-based integral inequality and the auxiliary function-based integral inequality combined with an extended reciprocally convex matrix inequality are used to estimate the derivative of the LKF. The novelty of this paper is that some state-dependent zero equations are introduced into before and after bounding the LKF's derivative so as to increase the freedom and reduce the conservatism. As a result, a less conservative stability criterion is derived in the form of linear matrix inequality, whose superiority is illustrated with three numerical examples. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 25	2020	399						1	7		10.1016/j.neucom.2020.02.064													
J								Fixed-time stochastic outer synchronization in double-layered multi-weighted coupling networks with adaptive chattering-free control	NEUROCOMPUTING										Fixed-time outer synchronization; Stochastic effects; Double-layered networks; Adaptive chattering-free control; Multi-weighted	COMPLEX DYNAMICAL NETWORKS; NEURAL-NETWORKS; FINITE-TIME; PINNING CONTROL; CLUSTER SYNCHRONIZATION; NONIDENTICAL NODES; STABILIZATION; STABILITY	The problem for fixed-time outer synchronization of double-layered multi-weighted coupled complex networks with stochastic effects is considered in this paper. To suppress chattering in synchronization, an adaptive chattering-free control algorithm is designed. Based upon the Lyapunov stability theory, some sufficient criteria for the adaptive stochastic outer synchronization are proposed. The designed adaptive chattering-free controller and the sufficient conditions can be applicable to not only the fixed-time stochastic synchronization of double-layered multi-weighed undirected networks, but also the fixed-time stochastic synchronization of double-layered multi-weighted directed networks. Our theoretical results indicate that the settling time is related to the size of dynamic networks, the dimension of each node and the designed adaptive controllers in the fixed-time stochastic outer synchronization. The effectiveness of our derived theoretical framework is illustrated via simulation examples. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 25	2020	399						8	17		10.1016/j.neucom.2020.02.072													
J								An online and generalized non-negativity constrained model for large-scale sparse tensor estimation on multi-GPU	NEUROCOMPUTING										Canonical polyadic decomposition; Generalized model; GPU and multi-GPU; High-dimension and sparse data; Single-thread-based model; Sparse non-negative tensor factorization; Online learning; Stream-like data merging	NONNEGATIVE MATRIX FACTORIZATION; DECOMPOSITION	Non-negative Tensor Factorization (NTF) models are effective and efficient in extracting useful knowledge from various types of probabilistic distribution with multi-way information. Current NTF models are mostly designed for problems in computer vision which involve the whole Matricized Tensor Times Khatri - RaoProduct (MTTKRP). Meanwhile, a Sparse NTF (SNTF) proposed to solve the problem of sparse Tensor Factorization (TF) can result in large-scale intermediate data. A Single-thread-based SNTF (SSNTF) model is proposed to solve the problem of non-linear computing and memory overhead caused by largescale intermediate data. However, the SSNTF is not a generalized model. Furthermore, the above methods cannot describe the stream-like data from industrial applications in mainstream processors, e.g, Graphics Processing Unit (GPU) and multi-GPU in an online way. To address these two issues, a Generalized SSNTF (GSSNTF) is proposed, which extends the works of SSNTF to the Euclidean distance, KullbackLeibler (KL)-divergence, and ItakuraSaito (IS)-divergence. The GSSNTF only involves the feature elements instead of the entire factor matrices during its update process, which can avoid the formation of large-scale intermediate matrices with convergence and accuracy promises. Furthermore, GSSNTF can merge the new data into the state-of-the-art built tree dataset for sparse tensor, and then online learning has the promise of the correct data format. At last, a model of Compute Unified Device Architecture (CUDA) parallelizing GSSNTF (CUGSSNTF) is proposed on GPU and Multi-GPU (MCUGSSNTF). Thus, CUGSSNTF has linear computing complexity and space requirement, and linear communication overhead on multi-GPU. CUGSSNTF and MCUGSSNTF are implemented on 8 P100 GPUs in this work, and the experimental results from realworld industrial data sets indicate the linear scalability and 40X speedup performances of CUGSSNTF than the state-of-the-art parallelized approachs. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 25	2020	399						18	36		10.1016/j.neucom.2020.02.068													
J								Self-accelerated Thompson sampling with near-optimal regret upper bound	NEUROCOMPUTING										Thompson sampling; Linear bandits; Online optimization	SUMS	Thompson sampling utilizes Bayesian heuristic strategy to balance the exploration-exploitation trade-off. It has been applied in a variety of practical domains and achieved great success. Despite being empirically efficient and powerful, Thompson sampling has eluded theoretical analysis. Existing analyses of Thompson sampling only provide regret upper bound of (O) over tilde (d(3/2) root T) for linear contextual bandits, which is worse than the information-theoretic lower bound by a factor of root d. In this paper, we design and analyze self-accelerated Thompson Sampling algorithm for the stochastic contextual multi-armed bandit problem. Our analysis establishes that the regret upper bound of self-accelerated Thompson sampling is (O) over tilde (d root T), which is the best upper bound achieved by any efficient contextual bandit algorithm in infinite action space. Our experiment on simulated data and real-world dataset shows that self-accelerated Thompson sampling outperforms standard Thompson sampling in both convergence rate and prediction accuracy. (C) 2020 Published by Elsevier B.V.																	0925-2312	1872-8286				JUL 25	2020	399						37	47		10.1016/j.neucom.2020.01.086													
J								Local preserving logistic I-Relief for semi-supervised feature selection	NEUROCOMPUTING										Logistic I-Relief; Multi-class semi-supervised learning; Feature selection; Laplacian regularization; Local structural information	DIMENSIONALITY REDUCTION; VARIABLE SELECTION; ALGORITHMS	Logistic I-Relief (LIR), a supervised feature selection method, has been successfully extended for semi-supervised feature selection. However, exiting semi-supervised methods based on LIR are incapable of maintaining the local structure of data. In this paper, we propose a local preserving logistic I-Relief (LPLIR) algorithm for multi-class semi-supervised feature selection. The goal of LPLIR is to maximize the expected margin of the given labeled data and remain the local structural information of all given data, where the local structural information is represented by a local preserving regularization. Additionally, extensive experiments conducted on real-world datasets demonstrate that LPLIR is superior to the existing state-of-the-art semi-supervised feature selection methods. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 25	2020	399						48	64		10.1016/j.neucom.2020.02.098													
J								Multilayer deep features with multiple kernel learning for action recognition	NEUROCOMPUTING										Multilayer features; Multiple kernel learning; Action recognition		In accurate action recognition, discriminative human-region representation as auxiliary information is critical for fusing multiple visual clues in a video and further improving the recognition performance. To this end, in this paper we propose integrating a novel representation named multilayer deep features (MDF) of both the human region and whole image area into an extended region-aware multiple kernel learning (ER-MKL) framework. To be specific, we first exploit the human cues with the help of the off-the-shelf semantic segmentation models. Then more powerful representations MDF are constructed by concatenating activations at the last convolutional layer and fully connected layer. Finally, the proposed framework termed ER-MKL is presented to learn a robust classifier for fusing human-region MDF and whole-region MDF. In addition to combining multiple kernels derived from features of heterogeneous image regions, ER-MKL also considers the sets of pre-learned classifiers and incorporates prior knowledge of different regions. Extensive evaluations on the JHMDB and UCF Sports datasets validate the effectiveness and the superiority of our proposed approach. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 25	2020	399						65	74		10.1016/j.neucom.2020.02.096													
J								Adaptive blind deconvolution using generalized cross-validation with generalized l(p)/l(q) norm regularization	NEUROCOMPUTING										Blind deconvolution; Generalized cross-validation; Regularization parameters; Sparse signal processing	SPARSE REPRESENTATION	Blind image deconvolution is a typically ill-conditioned inverse problem that requires additional information to constrain the solution space. The purpose of this paper is to investigate the characteristics of generalized l(p)/l(q) norm on the derivatives of nature images and present a novel efficient method for blind image deconvolution with generalized l(p)/l(q) norm (0 < p <= 1, p < q). Firstly we analyze the mathematical characteristics of generalized l(p)/l(q) norm base on the BSDS dataset, then apply generalized l(p)/l(q) normbased prior model in gradient space to estimate the blur kernel. Due to the complexity of optimization model, we develop an alternating gradient descent method to solve the generalized l(p)/l(q) norm-based model which can achieve high recovery quality. Specifically, the selection strategy of regularization parameters is given by using generalized cross-validation method, and these parameters can be updated in alternating minimization steps. Our preliminary experiments show that the proposed method can achieve state-of-the-art results. (C) 2020 ElsevierB.V. All rights reserved.																	0925-2312	1872-8286				JUL 25	2020	399						75	85		10.1016/j.neucom.2020.02.091													
J								Adaptive finite-time fuzzy control of full-state constrained high-order nonlinear systems without feasibility conditions and its application	NEUROCOMPUTING										High-order nonlinear systems; Full-state constraints; Adaptive finite-time fuzzy control; Feasibility conditions	BARRIER LYAPUNOV FUNCTIONS; OUTPUT-FEEDBACK STABILIZATION; TRACKING CONTROL; HOMOGENEOUS DOMINATION; STABILITY	This paper investigates adaptive finite-time fuzzy control for full-state constrained high-order nonlinear systems. Fuzzy logic systems are employed to relax growth assumptions imposed on unknown system nonlinearities. By integrating a nonlinear state-dependent transformation into control design, full-state constraints can be handled without imposing feasibility conditions on virtual controllers. It is rigorously proved that fuzzy approximation is valid based on a compact set, full-state constraints aren't violated for all time. Besides, the solution of the closed-loop system is semi-global practical finite-time stable, and the tracking error converges to an adjustable compact set around the origin in finite-time. Two examples show the advantages of this control scheme. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 25	2020	399						86	95		10.1016/j.neucom.2020.02.089													
J								Unified non-uniform scale adaptive sampling model for quality assessment of natural scene and screen content images	NEUROCOMPUTING										Image quality assessment (IQA); Adaptive resolution scaling; Natural scene image; Screen content image; Non-uniform scale	EYE-MOVEMENTS; INFORMATION; PERCEPTION; SIMILARITY; ATTENTION; TIME	Among many multimedia scenarios, digital images are mainly composed of natural scene images (NSIs) and screen content images (SCIs). NSIs are always captured by cameras from the real world while SCIs are typically the mixture of pictures and text. Traditional full-reference image quality assessment (IQA) metrics are designed for natural scene images, and they cannot work well on SCIs, because they do not take account of characteristics of different visual content/regions between NSIs and SCIs. Inspired by early eye movement study that human visual system samples the world non-completely or non-uniformly, we observe that human brain perceive different image content at different scales. Given these discoveries, we propose a non-uniform scale adaptive sampling model (NUSM) which dedicates to preprocess the input images for IQA methods, enabling traditional NSI IQA models to predict the quality of SCIs effectively. The proposed sampling model applies different sample scales into different regions to generate pre-sampled image pairs, then send to existing IQAs to evaluate the quality of images. The experimental results demonstrate that our model has achieved a promising performance on both NSIs and SCIs. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 25	2020	399						96	106		10.1016/j.neucom.2020.02.084													
J								An oversampling framework for imbalanced classification based on Laplacian eigenmaps	NEUROCOMPUTING										Imbalanced data; Oversampling; SMOTE; Laplacian eigenmaps	SMOTE	Imbalanced classification is a challenging problem in machine learning and data mining. Oversampling methods, such as the Synthetic Minority Oversampling Technique (SMOTE), generate synthetic data to achieve data balance for imbalanced classification. However, such kind of oversampling methods generates unnecessary noise when the data are not well separated. On the other hand, there are many applications with inadequate training data and vast testing data, making the imbalanced classification much more challenging. In this paper, we propose a novel oversampling framework to achieve the following two objectives. (1) Improving the classification results of the SMOTE based oversampling methods; (2) Making the SMOTE based oversampling methods applicable when the training data are inadequate. The proposed framework utilizes the Laplacian eigenmaps to find an optimal dimensional space, where the data are well separated and the generation of noise by SMOTE based oversampling methods can be avoided. The construction of graph Laplacian not only explores the useful information from the unlabeled testing data to facilitate imbalanced learning, but also makes the learning process incremental. Experimental results on several benchmark datasets demonstrate the effectiveness of the proposed framework. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 25	2020	399						107	116		10.1016/j.neucom.2020.02.081													
J								Joint discriminative attributes and similarity embddings modeling for zero-shot recognition	NEUROCOMPUTING										Discriminative attribute learning; Visual similarity embedding; Semantic similarity embedding; Zero-shot recognition	CLASSIFICATION	Zero-shot recognition (ZSR) is an appealing technique when addressing novel categories without any training data. However, existing methods face two critical limitations: 1) most existing works directly exploit user-defined attributes to transfer knowledge across classes, yet the importance to learn discriminative attributes is neglected; 2) previous studies typically consider the semantic similarity embedding, while there are limited research efforts on exploring the visual similarity embedding. In this paper, we propose a novel approach for ZSR to address the above issues. Specifically, we develop a simple yet effective strategy to learn discriminative attributes via regressing from labels, which yields superior performance in the semantic similarity space. Meanwhile, to our best knowledge, we are among the first to consider the visual similarity embedding, which fully explores the discriminative information of visual features to benefit the final performance. Moreover, we combine the visual and semantic similarity embeddings, which enriches the information in the joint similarity space and greatly reduces the gap between visual features and semantic attributes. Extensive experiments on five benchmarks show that our method achieves state-of-the-art results on both conventional and generalized zero-shot recognition tasks. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 25	2020	399						117	128		10.1016/j.neucom.2020.02.077													
J								Estimating stochastic linear combination of non-linear regressions efficiently and scalably	NEUROCOMPUTING										Linear regression; Gradient descent; Random projection	DIMENSION REDUCTION	Recently, many machine learning and statistical models such as non-linear regressions, the Single Index, Multi-index, Varying Coefficient Index Models and Two-layer Neural Networks can be reduced to or be seen as a special case of a new model which is called the Stochastic Linear Combination of Non-linear Regressions model. However, due to the high non-convexity of the problem, there is no previous work study how to estimate the model. In this paper, we provide the first study on how to estimate the model efficiently and scalably. Specifically, we first show that with some mild assumptions, if the variate vector x is multivariate Gaussian, then there is an algorithm whose output vectors have l(2)-norm estimation errors of O(root p/n) with high probability, where p is the dimension of x and n is the number of samples. The key idea of the proof is based on an observation motived by the Stein's lemma. Then we extend our result to the case where x is bounded and sub-Gaussian using the zero-bias transformation, which could be seen as a generalization of the classic Stein's lemma. We also show that with some additional assumptions there is an algorithm whose output vectors have l(infinity)-norm estimation errors of O(1 root p + root p/n) with high probability. We also provide a concrete example to show that there exists some link function which satisfies the previous assumptions. Finally, for both Gaussian and sub-Gaussian cases we propose a faster sub-sampling based algorithm and show that when the sub-sample sizes are large enough then the estimation errors will not be sacrificed by too much. Experiments for both cases support our theoretical results. To the best of our knowledge, this is the first work that studies and provides theoretical guarantees for the stochastic linear combination of non-linear regressions model. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 25	2020	399						129	140		10.1016/j.neucom.2020.02.074													
J								The NN-Stacking: Feature weighted linear stacking through neural networks	NEUROCOMPUTING										Meta-learning; Neural networks; Model stacking; Model selection		Stacking methods improve the prediction performance of regression models. A simple way to stack base regressions estimators is by combining them linearly, as done by Breiman [1]. Even though this approach is useful from an interpretative perspective, it often does not lead to high predictive power. We propose the NN-Stacking method (NNS), which generalizes Breiman's method by allowing the linear parameters to vary with input features. This improvement enables NNS to take advantage of the fact that distinct base models often perform better at different regions of the feature space. Our method uses neural networks to estimate the stacking coefficients. We show that while our approach keeps the interpretative features of Breiman's method at a local level, it leads to better predictive power, especially in datasets with large sample sizes. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 25	2020	399						141	152		10.1016/j.neucom.2020.02.073													
J								Left ventricle landmark localization and identification in cardiac MRI by deep metric learning-assisted CNN regression	NEUROCOMPUTING										MRI cardiac images; Landmark localization and identification; Distance metric learning; Triplet network; CNN regression	ANATOMICAL STRUCTURES; REGISTRATION; FORESTS; CONFIGURATION; SEGMENTATION; EFFICIENT; ROBUST; MODEL; PARTS	Accurate left ventricle landmark localization in cardiac MRI plays a vital role in computer-aided diagnosis of heart disease. Typical classification models hardly deal with artifacts and low discrimination of landmark regions by using only local image information, while regression models suffer from ambiguity between random samples and label, also the imbalance of samples. To overcome this limitation, this paper proposes a left ventricle landmark localization and identification method in cardiac MRI based on deep distance metric learning and CNN (convolutional neural network) regression. The method includes sample generation and regression stages, where super-pixel over-segmentation and unsupervised deep metric learning are integrated to extract the embedding information of local images, then a dual-channel salient sample mining module is designed by integrating specific super-pixel patches and grid patches extracted by the embedded triplet network. The weights of this triplet network are fed to build the CNN regression model, and the salient samples are employed to predict the landmark coordinate point cloud clusters. Furthermore, the centroid of each point cloud cluster is refined as landmark by Mean Shift iteration. The proposed method and close related methods were thoroughly evaluated on the CAP (cardiac atlas project) data set, and experimental results show that the proposed method achieves the competitive accuracy and outperforms the state-of-the-art classification and regression-based models. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 25	2020	399						153	170		10.1016/j.neucom.2020.02.069													
J								Multi-level supervised hashing with deep features for efficient image retrieval	NEUROCOMPUTING										Multi-table mechanism; Multi-level deep feature; Image retrieval; Structural and semantic similarity		Image hashing based on deep convolutional neural networks (CNN), deep hashing, has acquired breakthrough in image retrieval. Although deep features from various CNN layers have various levels of information, most of the existing deep hashing methods extract the feature vector only from the output of the penultimate fully-connected layer, focusing primarily on semantic information whilst ignoring detailed structure information. This calls for research on multi-level hashing, utilizing multi-level features to exploit different levels of CNN characteristics. To fill this gap, a novel image hashing method, Multi-Level Supervised Hashing with deep feature (MLSH), is proposed in this paper to further exploit multiple levels of deep image features. It uses a multiple-hash-table mechanism to integrate multi-level features extracted from an individual deep convolutional neural network. It takes advantage of the complementarity among multi-level features from various layers of a single deep network. High-level features reveal the semantic content of the image, while low-level features provide the structural information that is missing in high-level features. Instead of simple concatenation, several hash tables are trained individually using different levels of features from different layers, which are then integrated for efficient image retrieval. The method has been systematically evaluated through experiments on three image databases, including CIFAR-10, MNIST and NUSWIDE, and has thus been demonstrated to set a new state of the art in image hashing, outperforming several state-of-the-art hashing methods. Furthermore, the recall and precision can be balanced and improved simultaneously. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 25	2020	399						171	182		10.1016/j.neucom.2020.02.046													
J								Combination of fractional FLANN filters for solving the Van der Pol-Duffing oscillator	NEUROCOMPUTING										FLANN; Nonlinear identification; Van der Pol-Duffing oscillator; Fractional calculus	NEURAL-NETWORK; CONVEX COMBINATION; ALGORITHM; LMS; STRATEGIES; REGRESSION; GRADIENT; SYSTEMS	Functional link artificial neural network (FLANN) has received much attention due to its wide applicability. The Van der Pol-Duffing oscillator (VdPDO)-based nonlinear systems, which own complex dynamical behaviors, identification of such nonlinear model is vital. This paper exploits the nonlocality of fractional calculus, aiming to enhance the identification accuracy of the VdPDO-based nonlinear systems. The proposed combined FEM-LMS (CFEM-LMS) algorithm, which is based on the FLANN structure, convexly combines the least mean square (LMS) algorithm and the newly proposed fractional-order error modified LMS (FEM-LMS) algorithm. The CFEM-LMS algorithm has improved performance and can dynamically adapt to the nonlinearity of the system. As an added contribution, a novel mixing parameter adaptation criterion is proposed for performance improvement. Extensive simulation results in the context of VdPDO-based nonlinear system identification demonstrate the superiority of the proposed algorithm as compare to state-of-the-art approaches. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 25	2020	399						183	192		10.1016/j.neucom.2020.02.022													
J								Robust adaptive neuronal controller for exoskeletons with sliding-mode	NEUROCOMPUTING										Adaptive control; Sliding mode; Neural network; Exoskeleton	DESIGN; ROBOT	A robust neural adaptive integral sliding mode control approach is proposed in the present paper for non-linear exoskeleton systems. The proposed control technique is composed of two parts: an adaptive neural network controller and an adaptive integral terminal sliding mode controller. The adaptive laws are developed to estimate unknown parameters and ensure asymptotic stability of the closed-loop system. Only classical system's properties are supposed to be known, such as the bounds on some parameters. The unknown dynamics of the system are estimated on line by the neural network control part. The proposed adaptive control strategy is designed to ensure the reaching of the sliding surface with enhanced tracking performance. The singularity problem of the terminal sliding mode approach is overcomed without adding any constraint. The closed-loop stability of the system in the sense of Lyapunov is demonstrated. The effectiveness of the proposed approach is tested in real time application with healthy human subjects by performing passive arm movements using a 2-DOF upper limb exoskeleton. (c) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 25	2020	399						317	330		10.1016/j.neucom.2020.02.088													
J								Fast adaptive neighbors clustering via embedded clustering	NEUROCOMPUTING										Fast clustering; Large-scale data; Anchor-based graph embedded; Adaptive neighbor method	NONNEGATIVE MATRIX FACTORIZATION; CLASSIFICATION	Recently, spectral clustering (SC) has been gaining more and more attention due to its excellent performance in unsupervised learning. However, the computational complexity of the SC is high. Also, the adjacency graph matrix of the SC is ofen constructed by the Gaussian kernel, so the clustering result is sensitive to the kernel parameter sigma. Since most large-scale datasets are high-dimensional and sparse, it is a great challenge to apply the SC to these data. Therefore, a fast adaptive neighbor clustering method based on the embedded clustering (FANCEC) is proposed. First, m anchors are selected from raw data. Next, a bipartite graph matrix Z connecting the raw data and anchors is constructed in a parameter-free manner. Then, the graph embedded data are obtained from raw data by the singular value decomposition (SVD) method. The graph embedded data extracts and combines valid information from raw data while discarding the redundant information. After that, m anchors are selected from graph embedded data, and the adjacency matrix S is initialized. Finally, the adaptive neighbor strategy is used to update matrix S until optimal function convergences. The clustering result of the FANCEC can be obtained directly without the post-processing that is required in the k-means method. The experimental results show that the proposed FANCEC can reduce time-consumption for large-scale data and obtain a good comprehensive clustering effect compared with the traditional SC methods. (c) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 25	2020	399						331	341		10.1016/j.neucom.2020.02.087													
J								ALSTM: An attention-based long short-term memory framework for knowledge base reasoning	NEUROCOMPUTING										Knowledge base; LSTM; Attention; Memory; Logical rule; Deep learning	DEEP NEURAL-NETWORKS; GAME; GO	Knowledge Graphs (KGs) have been applied to various application scenarios including Web searching, Q&A, recommendation system, natural language processing and so on. However, the vast majority of Knowledge Bases (KBs) are incomplete, necessitating a demand for KB completion (KBC). Methods of KBC used in the mainstream current knowledge base include the latent factor model, the random walk model and recent popular methods based on reinforcement learning, which performs well in their respective areas of expertise. Recurrent neural network (RNN) and its variants model temporal data by remembering information for long periods, however, whether they also have the ability to use the information they have already remembered to achieve complex reasoning in the knowledge graph. In this paper, we produce a novel framework (ALSTM) based on the Attention mechanism and Long Short-Term Memory (LSTM), which associates structure learning with parameter learning of first-order logical rules in an end-to-end differentiable neural networks model. In this framework, we designed a memory system and employed a multi-head dot product attention (MHDPA) to interact and update the memories embedded in the memory system for reasoning purposes. This is also consistent with the process of human cognition and reasoning, looking for enlightenment for the future in historical memory. In addition, we explored the use of inductive bias in deep learning to facilitate learning of entities, relations, and rules. Experiments establish the efficiency and effectiveness of our model and show that our method achieves better performance in tasks which include fact prediction and link prediction than baseline models on several benchmark datasets such as WN18RR, FB15K-237 and NELL-995. (c) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 25	2020	399						342	351		10.1016/j.neucom.2020.02.065													
J								QMCM: Minimizing Vapnik's bound on the VC dimension	NEUROCOMPUTING										VC dimension; Minimal complexity machine; Model complexity; Classification; Feature selection	SUPPORT VECTOR MACHINES; COMPLEXITY; MARGIN; SVM	The recently proposed Minimal Complexity Machine (MCM) learns a hyperplane classifier by minimizing a bound on the Vapnik-Chervonenkis (VC) dimension. Both the linear and kernel versions of the MCM solve a linear programming problem, in order to minimize a bound on the VC dimension. This paper proposes a new quadratic programming formulation, termed as the Quadratic MCM (QMCM), that minimizes a tighter bound on the VC dimension. We present two variants of the QMCM, that differ in the norm of the error vector being minimized. We also explore a scalable variant of the QMCM for large datasets using Stochastic Gradient Descent (SGD), and present the use of the QMCM as a viable features-election method, in view of the the sparse nature of the models it learns. We compare the performance of the QMCM variants with LIBLinear, a linear Support Vector Machine (SVM) library; as well as against Pegasos and the linear MCM for large datasets, along with sequential feature selection methods and ReliefF. Our results validate the superiority of the QMCM in terms of statistically significant improvements on benchmark datasets from the UCI Machine Learning repository. (c) 2020 Published by Elsevier B.V.																	0925-2312	1872-8286				JUL 25	2020	399						352	360		10.1016/j.neucom.2020.01.062													
J								Precise iterative closest point algorithm for RGB-D data registration with noise and outliers	NEUROCOMPUTING										Precise point set registration; Correntropy measurement; RGB information compensation; RGB-D scene data	SIMULTANEOUS LOCALIZATION; CLOUD REGISTRATION; CORRENTROPY; ICP; SLAM	In this paper, we propose a precise Iterative Closest Point (ICP) algorithm which is a RGB information supplemented registration method based on correntropy measurement, which can overcome noise and outliers to complete precise point cloud registration. Firstly, in order to achieve accurate point set registration of single structure geometry point cloud with a large number of planes information rather than geometry information, we add the RGB information to our initial method as information compensation, which greatly ensure our algorithm could achieve accurate registration. Secondly, due to the noise and outliers existed in RGB-D data can easily cause deviations in registration, in order to achieve precise registration in RGB-D data, we introduced the correntropy measurement method into the initial model. Experimental results on our scene dataset demonstrate the proposed algorithm can achieve precise point set registration in RGB-D scene data well. (c) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 25	2020	399						361	368		10.1016/j.neucom.2020.02.076													
J								Auto-weighted multi-view clustering via spectral embedding	NEUROCOMPUTING										Multi-view clustering; Multiple graph learning; Spectral embedding; Adaptive weight		As is well-known, multi-view clustering has attracted considerable attention since many benchmark data sets exist heterogeneous features. Previous multi-view spectral clustering methods mainly contain two steps: 1) constructing multiple similarity graphs; 2) performing K-means (KM) clustering. The two-step strategy cannot acquire optimal results since the clustering performance highly relies on the constructed similarity graphs. To address this drawback, a unified framework named as an Auto-weighted Multi-view Clustering via Spectral Embedding (AMCSE) is presented. In the new proposed method, it can consider the clustering capacity heterogeneity of different views as well as directly obtain the clustering results. More importantly, the unified framework can make multiple graph learning guide the clustering result discretization, while the latter is in turn to conduct to learn better spectral embedding. A series of experiments are conducted on six real-world data sets, and the clustering results verify that the proposed method is not only effective but also efficient, comparing with state-of-the-art graph-based multi-view clustering approaches. (c) 2020 Published by Elsevier B.V.																	0925-2312	1872-8286				JUL 25	2020	399						369	379		10.1016/j.neucom.2020.02.071													
J								Lagrange stability of memristive quaternion-valued neural networks with neutral items	NEUROCOMPUTING										Memristive quaternion-valued neutral neural networks (MQVNNNs); Lagrange stability; Neutral delay; Linear matrix inequalities (LMIs)	GLOBAL EXPONENTIAL STABILITY; DISSIPATIVITY ANALYSIS; MIXED DELAYS; SYNCHRONIZATION; SENSE	This paper concentrates on the global exponential Lagrange stability of memristive quaternion-valued neural networks (MQVNNs) with neutral items. Some classical restrictions on the activation function of neutral item are removed. A new Lyapunov-Krasovskii functional (LKF) including information of neutral items is designed to overcome the difficulty induced by the coexist of quaternions, memristor, and neutral item. Furthermore, in order to reduce the conservativeness, both reciprocally convex inequality and Wirtinger-based inequality are extended to the quaternion domain. Based on the extended inequalities, Lyapunov theory, and novel analytical techniques, concise criteria in the form of linear matrix inequalities (LMIs) are proposed to ascertain the Lagrange stability of the interested MQVNNs. Finally, the correctness and effectiveness of theoretical results are checked by two numerical examples. (c) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 25	2020	399						380	389		10.1016/j.neucom.2020.03.003													
J								Global stability analysis of S-asymptotically omega-periodic oscillation in fractional-order cellular neural networks with time variable delays	NEUROCOMPUTING										Caputo derivative; Mittag-Leffler function; Comparison principle; Laplace transform	MITTAG-LEFFLER STABILITY; DISTRIBUTED DELAYS; MODEL	A delayed cellular neural networks with Caputo fractional-order derivative has been discussed in this paper. Firstly, the existence and uniqueness of S-asymptotically omega-periodic oscillation of the model are investigated by some important features of Mittag-Leffler functions and contraction mapping principle. Secondly, global asymptotical stability of the model is also studied by using Laplace transform, comparison principle and stability theorem of linear delayed Caputo fractional-order differential equations. Some better results are derived to improve and extend a few existing research findings. The research thoughts in this literature could be applied to research other fractional-order models in neural networks and physical areas. (c) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 25	2020	399						390	398		10.1016/j.neucom.2020.03.005													
J								Robust ellipse fitting based on Lagrange programming neural network and locally competitive algorithm	NEUROCOMPUTING										Ellipse fitting; Outlier; Real-time solution; Lagrange programming neural network (LPNN); Locally competitive algorithm (LCA)	CONIC SECTIONS; VARIATIONAL-INEQUALITIES; OPTIMIZATION PROBLEMS; STABILITY; CIRCLE	Given a set of 2-dimensional (2D) scattering points, obtained from the edge detection process, the aim of ellipse fitting is to construct an elliptic equation that best fits the scattering points. However, the 2D scattering points may contain some outliers. To address this issue, we devise a robust ellipse fitting approach based on two analog neural network models, Lagrange programming neural network (LPNN) and locally competitive algorithm (LCA). We formulate the fitting task as a nonsmooth constrained optimization problem, in which the objective function is an approximated l(0)-norm term. As the LPNN model cannot handle non-differentiable functions, we utilize the internal state concept of LCA to avoid the computation of the derivative at non-differentiable points. Simulation results show that the proposed ellipse fitting approach is superior to several state-of-the-art algorithms. (c) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 25	2020	399						399	413		10.1016/j.neucom.2020.02.100													
J								Artificial intelligence using hyper-algebraic networks	NEUROCOMPUTING										Artificial intelligence; Artificial neural networks; Conformal geometric algebra; Network theory	SUPPORT VECTOR MACHINES; NEURAL-NETWORKS; CLIFFORD	This work presents a novel paradigmatic revision of traditional neural networks, using network theoretic methods and Conformal Geometric Algebra. A unique theoretical framework called the 'hyperfield cognition framework' expands upon the mathematical foundations of neural networks in five-dimensional Conformal Geometric Algebraic space. This framework allows one to construct a novel theoretical computational engine, which is similar to a standard artificial neural network, but admits numerous added benefits: permits multiple training programs simultaneously, affords computational multiplicity in a single engine, reduces sensitivity to adversarial perturbations in training sets, affords broader capabilities and plasticity in the training of networks and produces robust and streamlined 'neural networks'. We call this novel five-dimensional neural network a 'hyperfield cognition network' (HCN). This paper demonstrates the utility and merit of the proposed Hyperfield Cognition Network by presenting two case studies, the first of which investigates the fuel efficiency of various automobiles and the second of which models the residuary resistance per unit weight of displacement of ships. (c) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 25	2020	399						414	448		10.1016/j.neucom.2019.12.116													
J								XCS with opponent modelling for concurrent reinforcement learners	NEUROCOMPUTING										Opponent modelling; XCS; Markov games; Reinforcement learning		Reinforcement learning (RL) of optimal policies against an opponent agent also with learning capability is still challenging in Markov games. A variety of algorithms have been proposed for solving this problem such as the traditional Q-learning-based RL (QbRL) algorithms as well as the state-of-the-art neural-network-based RL (NNbRL) algorithms. However, the QbRL approaches have poor generalization capability for complex problems with non-stationary opponents, while the learned policies by NNbRL algorithms are lack of explainability and transparency. In this paper, we propose an algorithm X-OMQ(lambda) that integrates eXtended Classifier System (XCS) with opponent modelling for concurrent reinforcement learners in zero-sum Markov Games. The algorithm can learn general, accurate, and interpretable action selection rules and allow policy optimization using the genetic algorithm (GA). Besides, the X-OMQ(lambda) agent optimizes the established opponent's model while simultaneously learning to select actions in a goal-directed manner. In addition, we use the eligibility trace mechanism to further speed up the learning process. In the reinforcement component, not only the classifiers in the action set are updated, but other relevant classifiers are also updated in a certain proportion. We demonstrate the performance of the proposed algorithm in the hunter prey problem and two adversarial soccer scenarios where the opponent is allowed to learn with several benchmark QbRL and NNbRL algorithms. The results show that our method has similar learning performance with the NNbRL algorithms while our method requires no prior knowledge of the opponent or the environment. Moreover, the learned action selection rules are also interpretable while having generalization capability. (c) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 25	2020	399						449	466		10.1016/j.neucom.2020.02.118													
J								Online semi-supervised learning with learning vector quantization	NEUROCOMPUTING										Online learning; Semi-supervised classification; Learning vector quantization; Gaussian mixture distribution; Neural gas	NEURAL-GAS; CLASSIFICATION; ALGORITHM	Online semi-supervised learning (OSSL) is a learning paradigm simulating human learning, in which the data appear in a sequential manner with a mixture of both labeled and unlabeled samples. Despite the recent advances, there are still many unsolved problems in this area. In this paper, we propose a novel OSSL method based on learning vector quantization (LVQ). LVQ classifiers, which represent the data of each class by a set of prototypes, have found their usage in a wide range of pattern recognition problems and can be naturally adapted to the online scenario by updating the prototypes with stochastic gradient optimization. However, most of the existing LVQ algorithms were designed for supervised classification. To extract useful information from unlabeled data, we propose two simple and computationally efficient methods based on clustering assumption. To be specific, we use the maximum conditional likelihood criterion for updating prototypes when data sample is labeled, and the Gaussian mixture clustering criterion or neural gas clustering criterion for adjusting prototypes when data sample is unlabeled. These two criteria are utilized alternatively according to the availability of label information to make full use of both supervised and unsupervised data to boost the performance. By extensive experiments, we show that the proposed method exhibits higher accuracy compared with the baseline methods and graph-based methods and is much more efficient than graph-based methods in both training and test time. (c) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 25	2020	399						467	478		10.1016/j.neucom.2020.03.025													
J								Reinforcement learning control for underactuated surface vessel with output error constraints and uncertainties	NEUROCOMPUTING										Reinforcement learning; Actor-Critic (AC); Output constraints; Underactuated marine vessel; Trajectory tracking; Neural networks	TRAJECTORY TRACKING CONTROL; ADAPTIVE NEURAL-CONTROL; PATH-FOLLOWING CONTROL; NONLINEAR-SYSTEMS; MARINE VEHICLES; DESIGN; SATURATION; DYNAMICS; GUIDANCE; STATE	This study investigates the trajectory tracking control problem of an underactuated marine vessel in the presence of output constraints, model uncertainties and environmental disturbances. The error transformation technique can ensure that the tracking errors remain within the predefined constraint boundaries. The controller is designed in combination with the critic function and the reinforcement learning (RL) algorithm based on actor-critic neural networks. The RL method is applied to solve model uncertainties and disturbances, and the critic function modifies the control action to supervise the system performance. Based on Lyapunov's direct method, a stability analysis is proposed to prove that the boundedness of system signals and the desired tracking performance can be guaranteed. Finally, the simulation illustrates the effectiveness and feasibility of the proposed controller. (c) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 25	2020	399						479	490		10.1016/j.neucom.2020.03.021													
J								Probabilistic forecasting with temporal convolutional neural network	NEUROCOMPUTING										Probabilistic forecasting; Convolutional neural network; Dilated causal convolution; Demand forecasting; High-dimensional time series		We present a probabilistic forecasting framework based on convolutional neural network (CNN) for multiple related time series forecasting. The framework can be applied to estimate probability density under both parametric and non-parametric settings. More specifically, stacked residual blocks based on dilated causal convolutional nets are constructed to capture the temporal dependencies of the series. Combined with representation learning, our approach is able to learn complex patterns such as seasonality, holiday effects within and across series, and to leverage those patterns for more accurate forecasts, especially when historical data is sparse or unavailable. Extensive empirical studies are performed on several real-world datasets, including datasets from JD.com, China's largest online retailer. The results show that our framework compares favorably to the state-of-the-art in both point and probabilistic forecasting. (c) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 25	2020	399						491	501		10.1016/j.neucom.2020.03.011													
J								Solving differential equations using deep neural networks	NEUROCOMPUTING										Deep neural networks; Differential equations; Partial differential equations; Nonlinear; Shocks; Data analytics; Optimization	WAVE-PROPAGATION; RECONSTRUCTION; INFERENCE; LIMITERS; SCHEMES	Recent work on solving partial differential equations (PDEs) with deep neural networks (DNNs) is presented. The paper reviews and extends some of these methods while carefully analyzing a fundamental feature in numerical PDEs and nonlinear analysis: irregular solutions. First, the Sod shock tube solution to the compressible Euler equations is discussed and analyzed. This analysis includes a comparison of a DNN-based approach with conventional finite element and finite volume methods, and demonstrates that the DNN is competitive in terms of degrees of freedom required for a given accuracy. Further, the DNNbased approach is extended to consider performance improvements and simultaneous parameter space exploration. Next, a shock solution to compressible magnetohydrodynamics (MHD) is solved for, and used in a scenario where experimental data is utilized to enhance a PDE system that is a priori insufficient to validate against the observed/experimental data. This is accomplished by enriching the model PDE system with source terms that are then inferred via supervised training with synthetic experimental data. The resulting DNN framework for PDEs enables straightforward system prototyping and natural integration of large data sets (be they synthetic or experimental), all while simultaneously enabling single-pass exploration of an entire parameter space. Published by Elsevier B.V.																	0925-2312	1872-8286				JUL 25	2020	399						193	212		10.1016/j.neucom.2020.02.015													
J								Semi-supervised sparse representation classifier ((SRC)-R-3) with deep features on small sample sized hyperspectral images	NEUROCOMPUTING										Hyperspectral images; Deep learning; Transfer learning; Semi-supervised learning; Sparse classifier	FEATURE-EXTRACTION; FACE RECOGNITION; REGRESSION; NETWORKS; FUSION; SVM	Hyperspectral images usually have small number of labeled samples because of the labeling cost and process difficulty. In conventional classification algorithms, classifier performance is depending on the number training samples. In this study, a deep learning based semi-supervised learning framework is proposed to solve this limited labeled sample size problem by utilizing power of labeled and unlabeled samples. The main aim of the study is constructing a general purpose deep model for a specific hyperspectral sensor type and using the model with little effort for all data sets obtained from this sensor type. In proposed framework, the model is trained with a data set from a hyperspectral sensor subsequently it is fine-tuned and evaluated with another data set acquired from the same sensor. Linearly separable deep features of the evaluation data set are extracted from the fine-tuned general deep model. Additionally, a new data formation method is proposed in the transition from hyperspectral data sub-cubes to the deep neural network input. Besides that, three different clustering methods have been used for selecting the initial labeled samples in the semi-supervised learning phase to observe the effects of the sample selection comparatively. As an another contribution of the study, a new semi-supervised sparse representation classifier (S3 RC) is proposed with labeled and unlabeled sample information by using linearly separable deep features. The performance of the proposed framework is proven by the experimental results with using small sample sizes. (c) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 25	2020	399						213	226		10.1016/j.neucom.2020.02.092													
J								Stochastic reconstruction of 3D porous media from 2D images using generative adversarial networks	NEUROCOMPUTING										Artificial neural networks; Stochastic image reconstruction; Porous media		Micro computed tomography (CT) provides petrophysics laboratories with the ability to image three dimensional porous media at pore scale. However, evaluating flow properties requires the acquisition of a large number of representative images, which is often unfeasible. Stochastic reconstruction methods are algorithms able to generate novel, realistic rock images from a small sample, thus avoiding a large acquisition process. A more convenient approach would use only two dimensional images, replacing 3D scans with images of the rock cuttings made during the drilling. This would extend the technique to media having pores smaller than the resolution of the micro-CT, but that can be imaged by microscopy. We introduce a novel method for 2D-to-3D reconstruction of the structure of porous media by applying generative adversarial neural networks. We compare several measures of pore morphology between simulated and acquired images. Experiments include bead pack, Berea sandstone, and Ketton limestone images. Results show that our GANs-based method can reconstruct three-dimensional images of porous media at different scales that are representative of the morphology of the original images. Also, compared to classical stochastic methods of image reconstruction, the generation of multiple images is much faster. (c) 2020ElsevierB.V. Allrightsreserved.																	0925-2312	1872-8286				JUL 25	2020	399						227	236		10.1016/j.neucom.2019.12.040													
J								Fusion of hyperspectral and panchromatic images using structure tensor and matting model	NEUROCOMPUTING										Panchromatic image; Hyperspectral image; Matting model; Structure tensor	MULTISPECTRAL IMAGES; ENHANCEMENT; RESOLUTION; SUPERRESOLUTION; TRANSFORM; CONTRAST; QUALITY; MS	The purpose of hyperspectral image fusion is to integrate the spectral information of hyperspectral (HS) image and the complementary spatial information of panchromatic (PAN) image. The result can improve the accuracy of subsequent image processing such as classification and detection. Component-substitution (CS)-based methods are popular HS image fusion approaches, which are efficient, and simple to implement. A novel CS-based hyperspectral image fusion framework is presented by combining structure tensor and matting model in this paper. The proposed scheme not only accurately estimates the missing spatial components of the HS images by using structure tensor, but also effectively preserves their spectral components by combining the advantages of structure tensor and matting model. Specifically, a structure-tensor-based adaptive weighted fusion strategy is proposed for generating the alpha channel. The high-resolution HS image is obtained by substituting the original alpha channel of the interpolated HS image with the generated alpha channel. Experimental results with synthetic datasets and real datasets prove the potential of the proposed algorithm in preserving spectral information and enhancing spatial information. (c) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 25	2020	399						237	246		10.1016/j.neucom.2020.02.050													
J								Fine-grained vehicle type detection and recognition based on dense attention network	NEUROCOMPUTING										Intelligent transportation; Object detection; Vehicle type recognition; Faster-RCNN		Intelligent transportation is an indispensable part of the smart city and the primary development direction of the future transportation systems. Vehicle detection and recognition, which is one of the most important aspects of intelligent transportation, plays a very important role in various areas of our daily life; one such important area is criminal investigation. In the fine-grained vehicle type detection and recognition, several difficult issues such as problems in data acquisition and tagging, dramatic variance in the data of different vehicle types, and challenges in identifying vehicles of the same brand with highly similar appearances remain unsolved. For the problems of data acquisition and tagging, this paper presents a strategy for automatic data acquisition and tagging based on object detection that can label the vehicle images efficiently while rapidly acquiring all types of fine-grained models. Considering the problem of data imbalance in the training process, this paper proposes a Faster-RCNN based data equalizing strategy (Faster-BRCNN), thereby improving the performance of object detection. In view of the severe information attenuation caused by the feature information transfer obstruct between layers in the traditional deep learning network, the lack of mutual dependency of these features, and the inability of the network to focus on the important region and characteristics, we propose an intensive dense attention network (DA-Net). Through its intensive connection and attention unit, we enhance the model's detection ability. The proposed method achieves mAP of 94.5% and 95.8% in the Stanford Cars and FZU Cars datasets, respectively, thereby verifying its effectiveness. (c) 2020ElsevierB.V. Allrightsreserved.																	0925-2312	1872-8286				JUL 25	2020	399						247	257		10.1016/j.neucom.2020.02.101													
J								RNN-GWR: A geographically weighted regression approach for frequently updated data	NEUROCOMPUTING										Geographically weighted regression; Reverse nearest neighbor; Frequently updated data; Regression; Data mining	BANDWIDTH SELECTION; DENSITY; PATTERNS; INTERNET; MODEL; PRICE	Geographically weighted regression (GWR) is a local spatial regression technique to model varying relationships in many application domains, such as ecology, environmental management, public health, meteorology, and tourism. In the literature, most of the studies dealing with GWR do not take into account if the dataset is frequently updated and so these techniques are not efficient to handle such datasets. In this study, to handle frequently updated data on given locations, a computationally efficient GWR approach, RNN-GWR, which utilizes reverse nearest neighbor (RNN) strategy, is proposed. The performance of the proposed RNN-GWR approach is compared with the performances of a Naive-GWR and FastGWR approaches. Experimental evaluations show that the proposed approach is computationally efficient than the other approaches on handling frequently updated data. (c) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 25	2020	399						258	270		10.1016/j.neucom.2020.02.058													
J								Spatial-spectral weighted nuclear norm minimization for hyperspectral image denoising	NEUROCOMPUTING										Restoration; Hyperspectral image denoising; Low-rank; Band similarity; Nonlocal similar cubic patches	SPARSE; ALGORITHM; REGULARIZATION; FIELD	Hyperspectral images (HSIs) are inevitably corrupted by various kinds of noise due to the instrumental and environmental factors. This degradation of HSI data affects the subsequent applications of these images. Despite the extensive research conducted into HSI denoising, satisfactory results under heavy noise levels have not yet been obtained. Assuming that the latent clean HSI holds the low-rank (LR) property while the noisy component does not, we propose a two-step Spatial-Spectral Weighted Nuclear Norm Minimization (SSWNNM) algorithm for HSI Denoising. Considering the LR property across the spectra, a Weighted Nuclear Norm Minimization (WNNM) algorithm is conducted to recover the spectral LR structure. In the spatial domain, nonlocal similar cubic patches are found and stacked into an LR matrix, which contains the local detailed spatial texture information. We further utilize Multi-channel Weighted Nuclear Norm Minimization (MCWNNM) to recover this spatial LR matrix. Experiments conducted on simulated and real HSI data demonstrate that the proposed denoising method outperforms state-of-the-art methods, both in terms of visual quality and several quantitative assessment indices. (c) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 25	2020	399						271	284		10.1016/j.neucom.2020.01.103													
J								Extending information maximization from a rate-distortion perspective	NEUROCOMPUTING										Information maximization; Rate distortion; Unsupervised representation learning	ALGORITHM; INFOMAX	In this paper, we propose a new interpretation of the information maximization method (InfoMax) from a perspective of the rate distortion theory. We show that under specific conditions, InfoMax is equivalent to the minimization of a compression rate under the constraint of zero distortion. Zero distortion, or equivalently, zero reconstruction error between the input and its reconstruction, does not provide meaningful solutions in many cases. Based on the new interpretation, we extend InfoMax to be able to deal with non-zero distortion and also to learn under/over-complete representations. Experimental results on synthetic as well as real data show the effectiveness of our method. (c) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 25	2020	399						285	295		10.1016/j.neucom.2020.02.061													
J								Nonparametric Topic Modeling with Neural Inference	NEUROCOMPUTING												This work focuses on combining nonparametric topic models with Auto-Encoding Variational Bayes (AEVB). Specifically, we first propose iTM-VAE, where the topics are treated as trainable parameters and the document-specific topic proportions are obtained by a stick-breaking construction. The inference of iTM-VAE is modeled by neural networks such that it can be computed in a simple feed-forward manner. We also describe how to introduce a hyper-prior into iTM-VAE so as to model the uncertainty of the prior parameter. Actually, the hyper-prior technique is quite general and we show that it can be applied to other AEVB based models to alleviate the collapse-to-prior problem elegantly. Moreover, we also propose HiTM-VAE, where the document-specific topic distributions are generated in a hierarchical manner. HiTM-VAE is even more flexible and can generate topic representations with better variability and sparsity. Experimental results on 20News and Reuters RCV1-V2 datasets show that the proposed models outperform the state-of-the-art baselines significantly. The advantages of the hyper-prior technique and the hierarchical model construction are also confirmed by experiments. (c) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 25	2020	399						296	306		10.1016/j.neucom.2019.12.128													
J								MMCL-Net: Spinal disease diagnosis in global mode using progressive multi-task joint learning	NEUROCOMPUTING										Densely aggregation; Level-set; Global optimization; Progressive multi-task; Multi-structure; Medical image	SEMANTIC SEGMENTATION	Simultaneous detection, segmentation, and classification of multiple spinal structures on MRI is crucial for the early and pathogenesis-based diagnosis of multiple spine diseases in the clinical setting. It is more assistance for radiologists reflections on the disease based on the pathogenesis when the lesion area and its adjacent structures are detected. Obviously, the multiple structures of the spine are directly interdependent and influential, and the multi-tasks under a deep convolutional neural network framework can also influence each other. Multi-task joint optimization in the spinal global mode is a direct outlet to seek the dynamic balance of the above potential correlation. In this paper, we propose a novel end-to-end Multi-task Multi-structure Correlation Learning Network (MMCL-Net) for the detection, segmentation, and classification (normal, slight, marked, and severe) of three types of spine structure: disc, vertebra, and neural foramen simultaneously. And the model is locally optimized to achieve a more stable dynamic equilibrium state. Extensive experiments on T1/T2-weighted MR scans from 200 subjects demonstrate that MMCL-Net achieves high performance with mAP of 0.9187, the classification accuracy of 90.67%, and dice coefficient of 90.60%. The experimental results show that the performance of our method is comparable to that of the state-of-the-art methods. (c) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 25	2020	399						307	316		10.1016/j.neucom.2020.01.112													
J								Neuromusculoskeletal Arm Prostheses: Personal and Social Implications of Living With an Intimately Integrated Bionic Arm	FRONTIERS IN NEUROROBOTICS										prosthetics; implanted electrodes; qualitative research; social studies of science and technology; human-machine interface	INTERPRETATIVE PHENOMENOLOGICAL ANALYSIS; OSSEOINTEGRATION; EMBODIMENT; INTERFACES; AMPUTATION	People with limb loss are for the first time living chronically and uninterruptedly with intimately integrated neuromusculoskeletal prostheses. This new generation of artificial limbs are fixated to the skeleton and operated by bidirectionally transferred neural information. This unprecedented level of human-machine integration is bound to have profound psychosocial effects on the individuals living with these prostheses. Here, we examined the psychosociological impact on people as they integrate neuromusculoskeletal prostheses into their bodies and lives. Three people with transhumeral amputations participated in this study, all of whom had been living with neuromusculoskeletal prostheses in their daily lives between 2 and 6 years at the time of the interview. Direct neural sensory feedback had been enabled for 6 months to 2 years. Participants were interviewed about their experiences living with the neuromusculoskeletal prostheses in their home and professional daily lives. We analyzed these interviews to elucidate themes using an interpretive phenomenological approach that regards participants' own experiences as forms of expertise and knowledge-making. Our participant-generated results indicate that people adapted and integrated the technology into functional and social arenas of daily living, with positive psychosocial effects on self-esteem, self-image, and social relations intimately linked to improved trust of the prostheses. Participants expressed enhanced prosthetic function, increased and more diverse prosthesis use in tasks of daily living, and improved relationships between their prosthesis and phantom limb. Our interviews with patients also generated critiques of the language commonly used to describe human-prosthetic relations, including terms such as "embodiment," and the need for specificity surrounding the term "natural" with regard to control versus sensory feedback. Experiences living with neuromusculoskeletal prostheses were complex and subject-dependent, and therefore future research should consider human-machine interaction as a relationship that is constantly enacted, negotiated, and deeply contextualized.																	1662-5218					JUL 24	2020	14								39	10.3389/fnbot.2020.00039													
J								Parking Slot Detection on Around-View Images Using DCNN	FRONTIERS IN NEUROROBOTICS										autonomous driving; parking slot detection; around-view image; DCNN; directional entrance line		Due to the complex visual environment and incomplete display of parking slots on around-view images, vision-based parking slot detection is a major challenge. Previous studies in this field mostly use the existing models to solve the problem, the steps of which are cumbersome. In this paper, we propose a parking slot detection method that uses directional entrance line regression and classification based on a deep convolutional neural network (DCNN) to make it robust and simple. For parking slots with different shapes and observed from different angles, we represent the parking slot as a directional entrance line. Subsequently, we design a DCNN detector to simultaneously obtain the type, position, length, and direction of the entrance line. After that, the complete parking slot can be easily inferred using the detection results and prior geometric information. To verify our method, we conduct experiments on the public ps2.0 dataset and self-annotated parking slot dataset with 2,135 images. The results show that our method not only outperforms state-of-the-art competitors with a precision rate of 99.68% and a recall rate of 99.41% on the ps2.0 dataset but also performs a satisfying generalization on the self-annotated dataset. Moreover, it achieves a real-time detection speed of 13 ms per frame on Titan Xp. By converting the parking slot into a directional entrance line, the specially designed DCNN detector can quickly and effectively detect various types of parking slots.																	1662-5218					JUL 24	2020	14								46	10.3389/fnbot.2020.00046													
J								AutoPath: Image-Specific Inference for 3D Segmentation	FRONTIERS IN NEUROROBOTICS										segmentation; 3D residual networks; reinforcement learning; policy network; image-specific inference		In recent years, deep convolutional neural networks (CNNs) has made great achievements in the field of medical image segmentation, among which residual structure plays a significant role in the rapid development of CNN-based segmentation. However, the 3D residual networks inevitably bring a huge computational burden to machines for network inference, thus limiting their usages for many real clinical applications. To tackle this issue, we proposeAutoPath, an image-specific inference approach for more efficient 3D segmentations. The proposedAutoPathdynamically selects enabled residual blocks regarding different input images during inference, thus effectively reducing total computation without degrading segmentation performance. To achieve this, a policy network is trained using reinforcement learning, by employing the rewards of using a minimal set of residual blocks and meanwhile maintaining accurate segmentation. Experimental results on liver CT dataset show that our approach not only provides efficient inference procedure but also attains satisfactory segmentation performance.																	1662-5218					JUL 24	2020	14								49	10.3389/fnbot.2020.00049													
J								Persistent Effect of Gait Exercise Assist Robot Training on Gait Ability and Lower Limb Function of Patients With Subacute Stroke: A Matched Case-Control Study With Three-Dimensional Gait Analysis	FRONTIERS IN NEUROROBOTICS										rehabilitation; robot-assisted gait training; stroke; gait ability; lower limb function	BODY-WEIGHT SUPPORT; TREADMILL; TRIAL; REHABILITATION; INDIVIDUALS; AMBULATION; WALKING	Introduction Gait exercise assist robot (GEAR), a gait rehabilitation robot developed for poststroke gait disorder, has been shown to improve walking speed and to improve the poststroke gait pattern. However, the persistence of its beneficial effect has not been clarified. In this matched case-control study, we assessed the durability of the effectiveness of GEAR training in patients with subacute stroke on the basis of clinical evaluation and three-dimensional (3D) gait analysis. Methods Gait data of 10 patients who underwent GEAR intervention program and 10 patients matched for age, height, sex, affected side, type of stroke, and initial gait ability who underwent conventional therapy were extracted from database. The outcome measures were walk score of Functional Independence Measure (FIM-walk), Stroke Impairment Assessment Set total lower limb motor function score (SIAS-L/E), and 3D gait analysis data (spatiotemporal factors and abnormal gait patter indices) at three time points: baseline, at the end of intervention, and within 1 week before discharge. Results In the GEAR group, the FIM-walk score, SIAS-L/E score, cadence, and single stance time of paretic side at discharge were significantly higher than those at post-training (p< 0.05), whereas the stance time and double support time of the unaffected side, knee extensor thrust, insufficient knee flexion, and external rotated hip of the affected side were significantly lower (p< 005). However, no significant differences in these respects were observed in the control group between the corresponding evaluation time points. Conclusion The results indicated significant improvement in the GEAR group after the training period, with respect to both clinical parameters and the gait pattern indices. This improvement was not evident in the control group after the training period. The results possibly support the effectiveness of GEAR training in conferring persistently efficient gait patterns in patients with poststroke gait disorder. Further studies should investigate the long-term effects of GEAR training in a larger sample.																	1662-5218					JUL 24	2020	14								42	10.3389/fnbot.2020.00042													
J								Guided autoencoder for dimensionality reduction of pedestrian features	APPLIED INTELLIGENCE										Autoencoder structure; Principal component analysis; Dimensionality reduction; Feature retrieval		Autoencoder and other conventional dimensionality reduction algorithms have achieved great success in dimensionality reduction. In this paper, we present an improved autoencoder structure, which was applied it in the field of pedestrian feature dimensionality reduction. The novel method is also verified on Mnist dataset. High-dimensional deep pedestrian features outperform other descriptors while it is challenging for computing capability and memory in existing systems. The dimensionality reduction method we proposed takes advantages of autoencoder and principal component analysis to achieve high efficiency. A novel weight matrix initialization and an improved reconstruction of autoencoder are proposed. Furthermore, by fusing features labeled with the same pedestrian, the proposed structure minimizes the loss after dimensionality reduction. Experimental results demonstrate that our method outperforms traditional dimensionality reduction methods. In the experiment, the pedestrian features were generated by ResNet and Market-1501 data-set. Our method achieves up to 8.834% mAP increment compared to a principal component analysis, when 2048-dimension pedestrian features are reduced to 16-dimension features.																	0924-669X	1573-7497															10.1007/s10489-020-01813-1		JUL 2020											
J								Zero-Shot Object Detection: Joint Recognition and Localization of Novel Concepts	INTERNATIONAL JOURNAL OF COMPUTER VISION										Zero-shot learning; Zero-shot object detection; Deep learning; Loss function	CLASSIFICATION; ATTRIBUTES	Zero shot learning (ZSL) identifies unseen objects for which no training images are available. Conventional ZSL approaches are restricted to a recognition setting where each test image is categorized into one of several unseen object classes. We posit that this setting is ill-suited for real-world applications where unseen objects appear only as a part of a complete scene, warranting both 'recognition' and 'localization' of the unseen category. To address this limitation, we introduce a new'Zero-Shot Detection'(ZSD) problem setting, which aims at simultaneously recognizing and locating object instances belonging to novel categories, without any training samples. We introduce an integrated solution to the ZSD problem that jointly models the complex interplay between visual and semantic domain information. Ours is an end-to-end trainable deep network for ZSD that effectively overcomes the noise in the unsupervised semantic descriptions. To this end, we utilize the concept of meta-classes to design an original loss function that achieves synergy between max-margin class separation and semantic domain clustering. In order to set a benchmark for ZSD, we propose an experimental protocol for the large-scale ILSVRC dataset that adheres to practical challenges, e.g., rare classes are more likely to be the unseen ones. Furthermore, we present a baseline approach extended from conventional recognition to the ZSD setting. Our extensive experiments show a significant boost in performance (in terms of mAP and Recall) on the imperative yet difficult ZSD problem on ImageNet detection, MSCOCO and FashionZSD datasets.																	0920-5691	1573-1405				DEC	2020	128	12					2979	2999		10.1007/s11263-020-01355-6		JUL 2020											
J								Video Based Face Recognition by Using Discriminatively Learned Convex Models	INTERNATIONAL JOURNAL OF COMPUTER VISION										Discriminative models; Affine hulls; Convex hulls; Face recognition; Image sets	REPRESENTATION	A majority of the image set based face recognition methods use a generatively learned model for each person that is learned independently by ignoring the other persons in the gallery set. In contrast to these methods, this paper introduces a novel method that searches for discriminative convex models that best fit to an individual's face images but at the same time are as far as possible from the images of other persons in the gallery. We learn discriminative convex models for both affine and convex hulls of image sets. During testing, distances from the query set images to these models are computed efficiently by using simple matrix multiplications, and the query set is assigned to the person in the gallery whose image set is closest to the query images. The proposed method significantly outperforms other methods using generatively learned convex models in terms of both accuracy and testing time, and achieves the state-of-the-art results on six of the eight tested datasets. Especially, the accuracy improvement is significant on the challenging PaSC, COX, IJB-C and ESOGU video datasets.																	0920-5691	1573-1405				DEC	2020	128	12					3000	3014		10.1007/s11263-020-01356-5		JUL 2020											
J								Analysis of traffic engineering and fast reroute on multiprotocol label switching	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Multiprotocol label switching; Internet protocol; Asynchronous transfer mode; Traffic engineering and fast reroute		Multiprotocol Label Switching (MPLS) is a technology developed to address the bottleneck of Internet Protocol over Asynchronous Transfer Mode network. It is a packet forwarding technology using labels to make data forwarding decisions. The use of label forwarding instead of IP address consumed a lot less resources in the old days as it was faster but nowadays the speed improvement is negligible due to new lookup table algorithm. However, it is one of the most utilized technologies among Internet Service Providers due to its Traffic Engineering (TE) capability. In this research, a deductive methodology has been used to evaluate the benefit of MPLS Traffic Engineering in the presence of traffic collision and we investigate how the implementation of Fast Reroute (FRR) helps to reduce the convergence time in case of link/node failure. Results show that TE helps to improve the quality of service in terms of latency in the presence of background traffic and FRR helps to reduce convergence time to less than 100 ms.																	1868-5137	1868-5145															10.1007/s12652-020-02365-5		JUL 2020											
J								Integrating data mining with internet of things to build the music project management system	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Internet of things; Project teaching method; Music teaching management system; Music project; Quality of teaching	UNIVERSITY; QUALITY	The purpose of this study is to improve the teaching quality of music classrooms more scientifically, track the learning status of students, and manage teaching resources and teaching projects more efficiently, thus improving the teaching effect. In this study, the Internet of Things technology and data mining technology was applied to develop and design the project teaching method management system, which effectively manages the teaching work under the open laboratory project teaching method, the statistical teaching log of teachers, and the comprehensive management of teachers and students and teaching resources. The dual-chip compound RFID school badge with unique identification, remote identification, information storage and other one-card functions designed on the perception layer is used to realize the comprehensive induction of different activity states of teachers and students in the classroom, laboratory and training base. After that, the Internet, campus network, local area network and other technologies are used to realize the storage, exchange and sharing functions of campus data and cloud data, which is conducive to the non-differentiated management of different places. Finally, when processing information and data in the cloud, corresponding countermeasures can be formulated according to different locations and teaching activities, and existing resources can be mobilized to meet teaching requirements. The results showed that after applying the system proposed in this study, the students' attendance rate, evaluation of teaching satisfaction, average assessment scores, and teaching resource utilization rate in the classroom have been significantly improved. Therefore, this study has great practical application value in music teaching.																	1868-5137	1868-5145															10.1007/s12652-020-02327-x		JUL 2020											
J								Quantitative evaluation method of cloud resources based on work scheduling	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										MapReduce computing model; Cloud resources; Quantitative evaluation; Simulation	SIMULATION; MAPREDUCE; FRAMEWORK	With the appearance of the period of large information, the customary information preparing model and asset the executive's model have been not able to fulfill the developing need. In this way, the recreation of the quantitative assessment strategy for cloud asset dependent on MapReduce figuring mode was advanced right now. The quantitative assessment model and record foundation of cloud assets in Hadoop stage and MapReduce processing mode were abridged. Through the quantitative assessment strategy for cloud assets dependent on MapReduce processing mode, the information was gathered and examined. The outcomes show that the quantitative assessment technique for cloud asset dependent on MapReduce processing model has a decent figuring force and the capacity to anticipate the utilization of cloud assets.																	1868-5137	1868-5145															10.1007/s12652-020-02342-y		JUL 2020											
J								Question retrieval using combined queries in community question answering	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Question retrieval; Community question-answering services; Combined queries; Combined indexes; Inverted index		Community question answering (cQA) has emerged as a popular service on the web; users can use it to ask and answer questions and access historical question-answer (QA) pairs. cQA retrieval, as an alternative to general web searches, has several advantages. First, user can register a query in the form of natural language sentences instead of a set of keywords; thus, they can present the required information more clearly and comprehensively. Second, the system returns several possible answers instead of a long list of ranked documents, thereby enhancing the efficient location of the desired answers. Question retrieval from a cQA archive, an essential function of cQA retrieval services, aims to retrieve historical QA pairs relevant to the query question. In this study, combined queries (combined inverted and nextword indexes) are proposed for question retrieval in cQA. The method performance is investigated for two different scenarios: (a) when only questions from QA pairs are used as documents, and (b) when QA pairs are used as documents. In the proposed method, combined indexes are first created for both queries and documents; then, different information retrieval (IR) models are used to retrieve relevant questions from the cQA archive. Evaluation is performed on a public Yahoo! Answers dataset; the results thereby obtained show that using combined queries for all three IR models (vector space model, Okapi model, and language model) improves performance in terms of the retrieval precision and ranking effectiveness. Notably, by using combined indexes when both QA pairs are used as documents, the retrieval and ranking effectiveness of these cQA retrieval models increases significantly.																	0925-9902	1573-7675				OCT	2020	55	2					307	327		10.1007/s10844-020-00612-x		JUL 2020											
J								Finding discriminatory features from electronic health records for depression prediction	JOURNAL OF INTELLIGENT INFORMATION SYSTEMS										Depression; Prediction model; Discriminatory features; Medical data analysis	SUICIDE; RISK; CANCER	Depression, a common mental disorder, affects not only individuals but also families and society. In the beginning stage, most of the depressive people do not know they are suffering from depression. Some of them visit different medical departments to ask for help. However, their symptoms may not be relieved because of not having a proper diagnosis. In this paper, we find discriminatory features for establishing an early depression detection model by analyzing medical data. These features are composed of patients' medical information, including diagnosed diseases and medical departments. We use real-world electronic health records dataset from the Taiwan National Health Insurance Research Database for the analysis and focus on young people aged 10-24 years. The experiment results show that our model can detect future diagnosis of depression based on patients' records up to 90 days in advance. Furthermore, even better performance can be achieved with longer observation time.																	0925-9902	1573-7675				OCT	2020	55	2					371	396		10.1007/s10844-020-00611-y		JUL 2020											
J								Multitask learning for health condition identification and remaining useful life prediction: deep convolutional neural network approach	JOURNAL OF INTELLIGENT MANUFACTURING										Prognostics and health management; Remaining useful life; Multi-task learning; Convolution neural network	PROGNOSTICS; MANAGEMENT; FRAMEWORK; MODEL	Predicting remaining useful life (RUL) is crucial for system maintenance. Condition monitoring makes not only degradation data available for RUL estimation but also categorized health status data for health state identification. However, RUL prediction has been treated as an independent process in most cases even though potential relevance exists with health status detection process. In this paper, we propose a convolution neural network based multi-task learning method to reflect the relatedness of RUL estimation with health status detection process. The proposed method applied to the C-MAPSS dataset for aero-engine unit prognostics supported superior performances to existing baseline models.																	0956-5515	1572-8145															10.1007/s10845-020-01630-w		JUL 2020											
J								Control of deposition height in WAAM using visual inspection of previous and current layers	JOURNAL OF INTELLIGENT MANUFACTURING										Wire plus arc additive manufacturing; Automatic control; Visual sensing; Previous and current layers	THIN-WALLED PARTS; ADAPTIVE-CONTROL; WIRE; MANUFACTURE; GEOMETRY	Wire plus arc additive manufacturing (WAAM) has been demonstrated to be a powerful technique to produce large-scale metal parts with low cost. However, techniques to achieve accurate geometry control and high process stability have not yet been perfectly developed. Although implementing vision sensing and closed-loop control can contribute to promoting the levels of process automation and stability, it is difficult to markedly improve the geometry precision of parts by only performing the current layer detection due to a large detection lag with vision-based sensors. To deal with this issue, this study proposes a novel strategy of introducing the previous layer information into the current deposition height to increase the response speed of the control system. The previous and current layer heights are monitored by a passive vision sensor. The height features are extracted by image processing algorithms mainly including edge detection, threshold division, and line fitting. Deviations in deposition height are automatically compensated via controlling the wire feed speed based on a PID controller. A helpful software interface is implemented in the Visual C++ environment to study the automatic detection and control system. In comparison to the closed-loop control using only the current layer detection, the deposition height of thin-walled parts can be excellently controlled by the proposed control system using the visual inspection of previous and current layers, significantly increasing the process stability and achieving accurate height control in WAAM.																	0956-5515	1572-8145															10.1007/s10845-020-01634-6		JUL 2020											
J								Discovering dependencies with reliable mutual information	KNOWLEDGE AND INFORMATION SYSTEMS										Information theory; Knowledge discovery; Approximate functional dependency; Pattern mining; Algorithms; Branch-and-bound	FRAMEWORK; ENTROPY	We consider the task of discovering functional dependencies in data for target attributes of interest. To solve it, we have to answer two questions: How do we quantify the dependency in a model-agnostic and interpretable way as well as reliably against sample size and dimensionality biases? How can we efficiently discover the exact or alpha-approximate top-kdependencies? We address the first question by adopting information-theoretic notions. Specifically, we consider the mutual information score, for which we propose a reliable estimator that enables robust optimization in high-dimensional data. To address the second question, we then systematically explore the algorithmic implications of using this measure for optimization. We show the problem is NP-hard and justify worst-case exponential-time as well as heuristic search methods. We propose two bounding functions for the estimator, which we use as pruning criteria in branch-and-bound search to efficiently mine dependencies with approximation guarantees. Empirical evaluation shows that the derived estimator has desirable statistical properties, the bounding functions lead to effective exact and greedy search algorithms, and when combined, qualitative experiments show the framework indeed discovers highly informative dependencies.																	0219-1377	0219-3116				NOV	2020	62	11					4223	4253		10.1007/s10115-020-01494-9		JUL 2020											
J								Deep end-to-end learning for price prediction of second-hand items	KNOWLEDGE AND INFORMATION SYSTEMS										LSTM; ARIMA; SARIMA; Linear regression; Time series analysis; Price prediction; Second-hand items	TIME-SERIES; MODEL	Recent years have witnessed the rapid development of online shopping and ecommerce websites, e.g., eBay and OLX. Online shopping markets offer millions of products for sale each day. These products are categorized into many product categories. It is crucial for sellers to correctly estimate the price of the second-hand item. State-of-the-art methods can predict the price of only one item category. In addition, none of the existing methods utilized the price range of a given second-hand item in the prediction task, as there are several advertisements for the same product at different prices. In this vein, as the first contribution, we propose a deep model architecture for predicting the price of a second-hand item based on the image and textual description of the item for different sets of item types. This proposed method utilizes a deep neural network involving long short-term memory (LSTM) and convolutional neural network architectures for price prediction. The proposed model achieved a better mean absolute error accuracy score in comparison with the support vector machine baseline model. In addition, the second contribution includes twofold. First, we propose forecasting the minimum and maximum prices of the second-hand item. The models used for the forecasting task utilize linear regression, LSTM, and seasonal autoregressive integrated moving average methods. Second, we propose utilizing the model of the first contribution in predicting the item quality score. Then, the item quality score and the forecasted minimum and maximum prices are combined to provide the item's final predicted price. Using a dataset crawled from a website for second-hand items, the proposed method of combining the predicted second-hand item quality score with the forecasted minimum and maximum price outperforms the other models in all of the used accuracy metrics with a significant performance gap.																	0219-1377	0219-3116															10.1007/s10115-020-01495-8		JUL 2020											
J								Error-Tolerance and Error Management in Lightweight Description Logics	KUNSTLICHE INTELLIGENZ										Description logics; Error-tolerance; Reasoning		The construction and maintenance of ontologies is an error-prone task. As such, it is not uncommon to detect unwanted or erroneous consequences in large-scale ontologies which are already deployed in production. While waiting for a corrected version, these ontologies should still be available for use in a "safe" manner, which avoids the known errors. At the same time, the knowledge engineer in charge of producing the new version requires support to explore only the potentially problematic axioms, and reduce the number of exploration steps. In this paper, we explore the problem of deriving meaningful consequences from ontologies which contain known errors. Our work extends the ideas from inconsistency-tolerant reasoning to allow for arbitrary entailments as errors, and allows for any part of the ontology (be it the terminological elements or the facts) to be the causes of the error. Our study shows that, with a few exceptions, tasks related to this kind of reasoning are intractable in general, even for very inexpressive description logics.																	0933-1875	1610-1987															10.1007/s13218-020-00684-5		JUL 2020											
J								Extensive study on the underlying gender bias in contextualized word embeddings	NEURAL COMPUTING & APPLICATIONS										Gender bias; Contextualized embeddings; Natural Language processing		Gender bias is affecting many natural language processing applications. While we are still far from proposing debiasing methods that will solve the problem, we are making progress analyzing the impact of this bias in current algorithms. This paper provides an extensive study of the underlying gender bias in popular contextualized word embeddings. Our study provides an insightful analysis of evaluation measures applied to several English data domains and the layers of the contextualized word embeddings. It is also adapted and extended to the Spanish language. Our study points out the advantages and limitations of the various evaluation measures that we are using and aims to standardize the evaluation of gender bias in contextualized word embeddings.																	0941-0643	1433-3058															10.1007/s00521-020-05211-z		JUL 2020											
J								Parameterized neural network training for the solution of a class of stiff initial value systems	NEURAL COMPUTING & APPLICATIONS										Computational intelligence; Neural networks; Approximate solutions; Stiff initial value systems	SOLVING ORDINARY	As computational intelligence techniques become more popular in almost all scientific fields and applications nowadays, there exists an active research effort to engage them in the study of classical mathematical problems. Among these techniques, the neural networks (NN), apart from their use in classification problems, can be used to approximate the behaviour of functions and their derivatives. Towards this direction, NN solution of differential equations (DEs), in both theoretical and technical point of view, is an active scientific field for the last two decades. NN solutions for DEs, once trained, have low computational cost and can be very useful as parts of more complex algorithms where needed, as well. Among the various classes of DEs, the stiff initial value problems (IVP) reveal difficulties in their numerical treatment by classical methodologies, whereas NN solutions of stiff DEs do not seem to do so. Moreover, their continuous nature and ability to be trained to solve classes of problems makes them an interesting tool. In this study, we investigate the NN solution of Inhomogeneous Linear IVPs. We incorporate to the NN a parameter that influences problem's stiffness and train the network for a range of this. Therefore, the trained NN solution can solve different problems than the one training for. In order to reveal the good generalization properties of the NNs solution regarding the stiffness parameter, we compare them to the solutions of standardMatlabstiff solvers. The proposed solutions perform very well, similarly and in many cases better to their competitors.																	0941-0643	1433-3058															10.1007/s00521-020-05201-1		JUL 2020											
J								Optimal design of load frequency active disturbance rejection control via double-chains quantum genetic algorithm	NEURAL COMPUTING & APPLICATIONS										Load frequency control; Active disturbance rejection control; Double-chains quantum genetic algorithm	AUTOMATIC-GENERATION CONTROL; PID CONTROLLER; POWER-SYSTEMS; AGC	This paper tackles the design of active disturbance rejection controllers (ADRC) for load frequency control (LFC) of multi-area interconnected power systems. For the first time, the double-chains quantum genetic algorithm plays a role in tuning optimization of the parameters for ADRC. Not only is the proposed approach applied to the two-area reheat thermal power system, but it is also elongated to two-interconnected multi-source areas comprising thermal, hydro and gas units, two-area nonlinear non-reheat thermal power system with governor dead band in addition to three-area nonlinear reheat thermal power system with generation rate constraints. Comparison with other modern heuristic optimization strategies recently published proves the effectiveness and superiority of this method. The simulation results show that this robust approach can greatly shorten the stabilization time of the power system and meet the requirements of LFC with minimum transient deviation as well as steady-state performance indicators, which is worthy of application and promotion.																	0941-0643	1433-3058															10.1007/s00521-020-05199-6		JUL 2020											
J								UAV object tracking by background cues and aberrances response suppression mechanism	NEURAL COMPUTING & APPLICATIONS										Computer vision; Object tracking; Unmanned aerial vehicle; Correlation filter; Handcrafted feature	CORRELATION FILTERS; VISUAL TRACKING	Real-time object tracking for unmanned aerial vehicles (UAVs) is an essential and challenging research topic for computer vision. However, the scenarios that UAVs deal with are complicated, and the UAV tracking targets are small. Therefore, general trackers often fail to take full advantage of their performance in UAV scenarios. In this paper, we propose a tracking method for UAV scenes, which utilizes background cues and aberrances response suppression mechanism to track in 4 degrees-of-freedom. Firstly, we consider the tracking task as a similarity measurement problem. In this study, we decompose this problem into two subproblems for optimization. Secondly, to alleviate the problem of small targets in UAV scenes, we utilize background cues fully. Also, to reduce interference by background information, we employ an aberrance response suppression mechanism. Then, to obtain accurate target state information, we introduce a logarithmic polar coordinate system. We perform phase correlation calculations in logarithmic polar coordinates to obtain the rotation and scale changes of the target. Finally, target states are obtained through response fusion, which includes displacement, scale, and rotation angle. Our approach is carried out in a large number of experiments on various UAV datasets, such as UAV123, DBT70, and UAVDT2019. Compared with the current advanced trackers, our method is superior on UAV small target tracking.																	0941-0643	1433-3058															10.1007/s00521-020-05200-2		JUL 2020											
J								Real-time signal queue length prediction using long short-term memory neural network	NEURAL COMPUTING & APPLICATIONS										Adaptive traffic control system; Control parameters; Data-driven; Real-time queue length; Long short-term memory neural network	TRAFFIC FLOW; MODELS; OPTIMIZATION; WAVES	Optimal traffic control and signal planning can significantly reduce traffic congestion and potential delays at intersections. However, a major challenge to optimize traffic signal timing is to accurately predict traffic before commencing the next cycle. An optimal strategy cannot be achieved with a poor prediction of future traffic. In this study, using a deep learning approach, we develop a data-driven real-time queue length prediction technique. We consider a connected corridor where information from vehicle detectors (located at the intersection) will be shared to consecutive intersections. We assume that the queue length of an intersection in the next cycle will depend on the queue length of the target and two upstream intersections in the current cycle. We use InSync adaptive traffic control system data to train a long short-term memory neural network model capturing time-dependent patterns of a queue of a signal. To avoid overfitting and select the best combination of hyperparameters, we use sequential model-based optimization technique. Our experiment results show that the proposed model performs very well to predict the queue length. Although we run our experiments predicting the queue length for a single movement, the proposed method can be applied for other movements as well.																	0941-0643	1433-3058															10.1007/s00521-020-05196-9		JUL 2020											
J								Performance of machine learning classification models of autism using resting-state fMRI is contingent on sample heterogeneity	NEURAL COMPUTING & APPLICATIONS										Autism spectrum disorder; Autism diagnostic observation schedule; Conditional random forest; Functional connectivity; fMRI; Symptom severity; Machine learning; Heterogeneity	FUNCTIONAL CONNECTIVITY; SPECTRUM DISORDER	Autism spectrum disorders (ASDs) are heterogeneous neurodevelopmental conditions. In fMRI studies, including most machine learning studies seeking to distinguish ASD from typical developing (TD) samples, cohorts differing in gender and symptom severity composition are often treated statistically as one 'ASD group.' Using resting-state functional connectivity (FC) data, we implemented random forest to build diagnostic classifiers in four ASD samples including a total of 656 participants (N-ASD = 306,N-TD = 350, ages 6-18). Groups were manipulated to titrate heterogeneity of gender and symptom severity and partially overlapped. Each sample differed on inclusionary criteria: (1) all genders, unrestricted severity range; (2) only male participants, unrestricted severity; (3) all genders, higher severity only; and (4) only male participants, higher severity. Each set consisted of 200 participants per group (ASD, TD; matched on age and head motion): 160 for training and 40 for validation. FMRI time series from 237 regions of interest (ROIs) were Pearson correlated in a 237 x 237 FC matrix, and classifiers were built using random forest in training samples. Classification accuracies in validation samples were 62.5%, 65%, 70%, and 73.75%, respectively, for samples 1-4. Connectivity within cingulo-opercular task control (COTC) network, and between COTC ROIs and default mode and dorsal attention network contributed overall most informative features, but features differed across sets. Findings suggest that diagnostic classifiers vary depending on ASD sample composition. Specifically, greater homogeneity of samples regarding gender and symptom severity enhances classifier performance. However, given the true heterogeneity of ASDs, performance metrics alone may not adequately reflect classifier utility.																	0941-0643	1433-3058															10.1007/s00521-020-05193-y		JUL 2020											
J								A bio-inspired spatiotemporal contrast operator for small and low-heat-signature target detection in infrared imagery	NEURAL COMPUTING & APPLICATIONS										Infrared target detection; Small moving target detection; Bio-inspired signal processing	MODEL	Thermal infrared imaging is a promising modality for long-range small target detection. However, low target contrast, high background clutter and sensor noise are some of the key challenges that need to be resolved efficiently for robust detection performance. The spatiotemporal processing in the early stages of the visual pathway of small flying insects has a remarkable ability to simultaneously address such challenges. The first stage of the early visual system corresponds to the adaptive temporal filtering mechanisms of photoreceptor cells. This stage improves the signal-to-noise ratio, enhances target background discrimination and compresses the signal bandwidth. The second stage pertains to the adaptive spatiotemporal filtering in the lamina monopolar cells. This stage removes spatiotemporal redundancy and enhances target contrast. In this paper, we explore such two-stage bio-processing to simultaneously suppress clutter and enhance the contrast of small low-heat-signature targets in real-world infrared imagery. We also propose a simple and efficient spatial contrast operator called center-surround total differential index for target region segmentation. Small moving target detection experiments on real-world high-bit-depth infrared video sequences show that the proposed method significantly outperforms the state-of-the-art spatial and spatiotemporal infrared small target detection methods. Specifically, our method resulted in 59% better detection rate (at 10(-5) false alarm rate) than the best competing method. Our results show that the bio-inspired spatiotemporal preprocessing is an excellent tool for significantly improving the performance of existing long-range infrared target detection techniques.																	0941-0643	1433-3058															10.1007/s00521-020-05206-w		JUL 2020											
J								Distributed regularized stochastic configuration networks via the elastic net	NEURAL COMPUTING & APPLICATIONS										Distributed learning; Alternating direction method of multipliers (ADMM); Stochastic configuration networks; Elastic net		Stochastic configuration network (SCN) has great potential in developing fast learning model with sound generalization capability and can be easily extended to the distributed computing framework. This paper aims to develop a distributed regularized stochastic configuration network to solve the limitations of traditional centralized learning on the scalability and efficiency in computing and storage resources for massive datasets. The local models are constructed using a classical stochastic configuration network, and the global unified model is built by the alternating direction method of multipliers (ADMM). Elastic net regularization term combining the LASSO and ridge methods is added into loss function of the ADMM optimization to prevent the model from overfitting when the data has high-dimensional collinearity. Each layer of the local regularized SCN model of a node in the topology network is constructed incrementally; its input weights and biases are broadcast to all other nodes under the inequality constraints. Output weights and the Lagrange multipliers of each node are calculated alternately through the decomposition-coordination procedure of the ADMM optimization algorithm until it finally converges to a unified model. A comprehensive study on five benchmark datasets and the ball mill experimental data has been carried out to evaluate the proposed method. The experiment results show that the proposed distributed regularized stochastic configuration network has relative advantages in terms of accuracy and stability compared with the distributed random vector functional link network.																	0941-0643	1433-3058															10.1007/s00521-020-05178-x		JUL 2020											
J								A new approach for the detection of abnormal heart sound signals using TQWT, VMD and neural networks	ARTIFICIAL INTELLIGENCE REVIEW										Heart sound; Phonocardiogram (PCG); Tunable Q-factor wavelet transform (TQWT); Variational mode decomposition (VMD); Phase space reconstruction (PSR); System dynamics; Synthetic minority over-sampling technique (SMOTE); Neural networks	EMPIRICAL MODE DECOMPOSITION; PHASE-SPACE RECONSTRUCTION; WAVELET TRANSFORM; EEG SIGNALS; CLASSIFICATION; SEGMENTATION; IDENTIFICATION; FEATURES; DIAGNOSIS; SELECTION	Phonocardiogram (PCG) plays an important role in evaluating many cardiac abnormalities, such as the valvular heart disease, congestive heart failure and anatomical defects of the heart. However, effective cardiac auscultation requires trained physicians whose work is tough, laborious and subjective. The objective of this study is to develop an automatic classification method for anomaly (normal vs. abnormal) detection of PCG recordings without any segmentation of heart sound signals. Hybrid signal processing and artificial intelligence tools, including tunable Q-factor wavelet transform (TQWT), variational mode decomposition (VMD), phase space reconstruction (PSR) and neural networks, are utilized to extract representative features in order to model, identify and detect abnormal patterns in the dynamics of PCG system caused by heart disease. First, heart sound signal is decomposed into a set of frequency subbands with a number of decomposition levels by using the TQWT method. Second, VMD is employed to decompose the subband of the heart sound signal into different intrinsic modes, in which the first four intrinsic modes contain the majority of the heart sound signal's energy and are considered to be the predominant intrinsic modes. They are selected to construct the reference variable for analysis. Third, phase space of the reference variable is reconstructed, in which the properties associated with the nonlinear PCG system dynamics are preserved. Three-dimensional PSR together with Euclidean distance has been utilized to derive features, which demonstrate significant difference in PCG system dynamics between normal and abnormal heart sound signals. Finally, PhysioNet/CinC Challenge heart sound database is used for evaluation and the synthetic minority over-sampling technique method is applied to balance the datasets. By using the 10-fold cross-validation style, experimental results demonstrate that the proposed features with dynamical neural networks based classifier yield classification performance with sensitivity, specificity, overall score and accuracy values of 97.73% 98.05%, 97.89%, and 97.89%, respectively. The results verify the effectiveness of the proposed method which can serve as a potential candidate for the automatic anomaly detection in the clinical application.																	0269-2821	1573-7462															10.1007/s10462-020-09875-w		JUL 2020											
J								A robust zeroing neural network for solving dynamic nonlinear equations and its application to kinematic control of mobile manipulator	COMPLEX & INTELLIGENT SYSTEMS										Nonlinear equation (NE); Neural network (NN); Recurrent neural network (RNN); Fixed-time convergence; Zeroing neural network (ZNN); Robust neural network (RZNN); Activation function (AF); Power versatile activation function (PVAF)	FINITE-TIME SYNCHRONIZATION; VARYING SYLVESTER EQUATION; STATE-FEEDBACK; CIRCUIT-DESIGN; STABILIZATION; CONVERGENCE; BEHAVIOR; STABILITY; MODELS; SYSTEM	Nonlinear phenomena are often encountered in various practical systems, and most of the nonlinear problems in science and engineering can be simply described by nonlinear equation, effectively solving nonlinear equation (NE) has aroused great interests of the academic and industrial communities. In this paper, a robust zeroing neural network (RZNN) activated by a new power versatile activation function (PVAF) is proposed and analyzed for finding the solutions of dynamic nonlinear equations (DNE) within fixed time in noise polluted environment. As compared with the previous ZNN model activated by other commonly used activation functions (AF), the main improvement of the presented RZNN model is the fixed-time convergence even in the presence of noises. In addition, the convergence time of the proposed RZNN model is irrelevant to its initial states, and it can be computed directly. Both the rigorous mathematical analysis and numerical simulation results are provided for the verification of the effectiveness and robustness of the proposed RZNN model. Moreover, a successful robotic manipulator path tracking example in noise polluted environment further demonstrates the practical application prospects of the proposed RZNN models.																	2199-4536	2198-6053															10.1007/s40747-020-00178-9		JUL 2020											
J								A Novel Camera Fusion Method Based on Switching Scheme and Occlusion-Aware Object Detection for Real-Time Robotic Grasping	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Switching scheme; Pose estimation; Robotic grasping; Deep learning; Occlusion-aware	SERVO CONTROL; VISION; ROBUST	Real-time vision-based robotic grasping is challenging in clutter. In such scene, the target object should be perceived accurately, where it may be occluded and misrecognized by many distractors including irrelevant objects and the robotic arm. In addition, the limited field of view (FOV) of camera makes it prone for objects to get out of the camera view. We develop a novel camera fusion method of pose estimation based on switching scheme for real-time robotic grasping under hybrid eye-in-hand (EIH)/eye-to-hand (ETH) configurations. The objects are locked based on occlusion-aware object detection to apply switching function for single pose estimation or multiple vision fusion. This method improves the accuracy of pose estimation and robustness of dynamic grasping under occlusion. Experimental results on pose estimation and real-time robotic grasping in clutter verify the effectiveness of the proposed method.																	0921-0296	1573-0409															10.1007/s10846-020-01236-7		JUL 2020											
J								Fast and efficient difference of block means code for palmprint recognition	MACHINE VISION AND APPLICATIONS										Palmprint recognition; Online applications; Palmprint coding; Biometrics	FILTER	Over the past two decades, researchers in the field of biometrics have presented a wide variety of coding-based palmprint recognition methods. These approaches mainly rely on extracting the texture features, e.g. line orientations, and phase information, using different filters. In this paper, we propose a new efficient palmprint recognition method based on the Different of Block Means. In the proposed scheme, only basic operations (i.e. mainly additions and subtractions) are used, thus involving a much lower computational cost when compared with existing systems. This makes the system suitable for online palmprint identification and verification. Furthermore, the technique has been shown to deliver superior performance over related systems.																	0932-8092	1432-1769				JUL 24	2020	31	6							51	10.1007/s00138-020-01103-3													
J								Discovering genomic patterns in SARS-CoV-2 variants	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										common subsequence; coronavirus; COVID-19; dynamic programming; genomic patterns; genomic variants; pattern recognition; SARS-CoV-2; sequence analysis; spike protein		SARS-CoV-2 is a novel severe acute respiratory syndrome-like coronavirus (SARS-CoV), which is responsible of the ongoing world pandemic of COVID-19 disease. Although many approaches are being investigated to address this issue, nowaday there are no vaccines available and there is little evidence supporting the efficiency of potential therapeutic agents. Moreover, the high mutation rate of this virus heavily affects the understanding of its evolution and diffusion mechanisms, and, in turn, the development of effective solutions. In this study, two novel algorithms are provided for finding out recurrent patterns of nucleotide subsequences of different SARS-CoV-2 genomes as a unique signature capable of identifying the most peculiar features of the pathogen. In particular, we provide several subsequence patterns related to the Spike glycoprotein, which is believed to be the main target for developing effective drugs and vaccines against the COVID-19 disease because of its role in the entrance of coronaviruses into host cells. The experimental results, obtained by analyzing 5000 genomes of SARS-CoV-2, have shown that the extracted patterns are able to recognize the Spyke protein in the 99.35% of the considered genomes. In addition, such patterns have proven to be highly discriminating with respect to other pathogenic genomes, such as SARS, Middle East respiratory syndrome, Nipah, and the streptococcus bacteria. We hope that the findings presented in this study can help specialists in speeding up the design of more accurate drugs or vaccines against SARS-CoV-2.																	0884-8173	1098-111X				NOV	2020	35	11					1680	1698		10.1002/int.22268		JUL 2020											
J								Evolino Recurrent Neural Network Ensemble for Speculation in Exchange Market in Time of Anomalies	APPLIED ARTIFICIAL INTELLIGENCE												Sharp falls or explosive growths in exchange markets, whether expected or not, generates new challenges for investors who want to protect their investments or achieve an optimum benefit during and after the turmoil. An anomaly of the exchange market, instigated by the Swiss National Bank, occurred when the Swiss Franc decoupled from the euro unexpectedly. The United Kingdom (UK) vote to withdraw from the European Union (Brexit), in contrast, was feared but expected. A comparison of the consequences of the anomalies gives us an unprecedented opportunity to investigate prediction capabilities of the EVOLINO Recurrent Neural Network Ensemble (ERNN) model following an anomaly. By introducing this new information to the ERNN model and analyzing its response, we increase investor resources during large exchange rate fluctuations; this will provide them with additional information that will help them construct different portfolios. Reaction to the anomaly was visible only after the anomaly occurred, this is when the model began to acquire data influenced by the extreme change. Comparing different strategies which are related or unrelated to the anomaly and orthogonal or not orthogonal for conservative, moderate, or aggressive trading shows that in order to profit from the anomaly, speculation depends on prediction-accuracy and on the sets of exchange-rate associated with the anomaly.																	0883-9514	1087-6545				NOV 9	2020	34	13					957	980		10.1080/08839514.2020.1790249		JUL 2020											
J								Supervised shiftk-meansbased machine learning approach for link prediction using inherent structural properties of large online social network	COMPUTATIONAL INTELLIGENCE										k-means; link prediction; links; nodes; structural techniques		Social network analysis can be used to address the challenges of link prediction in a social network through the measuring and processing of the inherent structural properties of the network. By developing relationships between the anonymous types on networks such as Facebook and Twitter, social network analysis can determine the prediction of future and missing links among the nodes, as well as assist in developing a predictive system. Various techniques have been discussed with proposals for network link prediction methodologies. The scaling of social networks, differences in local and global features, and efficient and precise prediction of links are issues for further study. We propose a collaborative model for link prediction that offers the benefit of structural-based and machine learning techniques. The model produces meaningful results for the prediction of strong and weak links. In this study, we consider node-based feature parameters for the clustering of the social network.																	0824-7935	1467-8640															10.1111/coin.12372		JUL 2020											
J								The multiobjective stochastic CRITIC-TOPSIS approach for solving the shipboard crane selection problem	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										crane selection problem; CRITIC; interval type-2 fuzzy sets; TOPSIS	FACILITY LOCATION SELECTION; FUZZY-LOGIC SYSTEMS; SETS	The duty of shipboard cranes is to lift and lower loads, as well as to handle floating facilities to lower or higher positions by means of fixed wire ropes, pulleys, and hook, and so forth. Hence, they play an important role in the productivity of servicing or manufacturing systems. Since each crane has distinguished properties than the others with respect to criteria and decision-makers (DMs) may express the different standpoints regarding them, the crane selection problem (CSP) can be considered as a group multicriteria decision-making (MCDM) problem. In this paper, interval type-2 fuzzy sets (IT2FSs) are first used to evaluate cranes with respect to criteria. The synthetic value method of IT2FSs is then handled to integrate the ratings expressed as IT2FSs of each crane with respect to criteria into the single fuzzy rating. Finally, the multiobjective criteria importance through inter-criteria correlation (CRITIC)-technique for order of preference by similarity to ideal solution (TOPSIS) approach is applied to solve the CSP in which CRITIC and TOPSIS are used to determine the objective weights and score of cranes, respectively. In addition, the limit distance mean (LDM) is introduced for ranking interval type-2 fuzzy ratings in the above two techniques. In contrast, to demonstrate the potential application, the proposed methodology is implemented in a real case study and the ranking results are compared with those published in the literature.																	0884-8173	1098-111X				OCT	2020	35	10					1570	1598		10.1002/int.22265		JUL 2020											
J								PSO-LSTM for short term forecast of heterogeneous time series electricity price signals	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Long short-term memory (LSTM); Price forecasting; Mean absolute percentage error (MAPE); Indian energy exchange (IEX)	PREDICTION	Electricity price forecasting plays an important role in the power system network, in order to promote the decision-making process for power generation and consumption. Long term forecasting is not viable as there is an uncertainty in the forecast due to increasing the integration of renewable sources with the existing grids. Since the behavior of the electricity price time sequence signal is highly non-linear and seasonal, deep neural network is the best model for learning the non-linear behavior within the data and for the purpose of forecasting. Hence this paper proposes an enhanced particle swarm optimization based long short-term memory (LSTM) neural network model, which is used to forecast the closing price of the Indian Energy Exchange. Particle swarm optimization technique is used to optimize the LSTM network input weights, which in turn minimize the forecast error with reduced architecture. This paper discusses the statistical analysis for input data selection and investigates the performance analysis for the optimal selection of layers with hidden units' combination. Finally, the analysis deploys the best-suited configuration for forecasting the market clearing price with the least mean absolute percentage error.																	1868-5137	1868-5145															10.1007/s12652-020-02353-9		JUL 2020											
J								An analytical toast to wine: Using stacked generalization to predict wine preference	STATISTICAL ANALYSIS AND DATA MINING										Bayesian modeling; ensemble modeling; machine learning; regularization; stacked generalization; variable importance; wine quality	META-LEARNING FRAMEWORK; REGRESSION; SELECTION; MODELS	Due to the intricacies surrounding taste profiles, one's view of good wine is subjective. Therefore, it is advantageous to provide a more objective, data-driven way to assess wine preferences. Motivated by a previous study that modeled wine preferences using machine learning algorithms, this work presents an ensemble approach to predict a wine sample's quality level given its physiochemical properties. Results show the proposed framework out-performs many sophisticated models including the one recommended by the motivational study. Moreover, the proposed framework offers a simple variable importance strategy to gain insight as to the relevance of the predictor variables and is applied to both simulated and real data. Given the predictive power of using ensembles, especially when they can be interpretable, practitioners can use the following approach to provide an accurate and inferential perspective towards demystifying wine preferences.																	1932-1864	1932-1872				OCT	2020	13	5					451	464		10.1002/sam.11474		JUL 2020											
J								The future of precision health is data-driven decision support	STATISTICAL ANALYSIS AND DATA MINING										clinical decision-making; decision support systems; machine learning; precision medicine	EXPERIMENTAL-DESIGN; TREATMENT REGIMES; MODELS; RULES	In the applied sciences, the ultimate goal is not just to acquire knowledge but to turn knowledge into action. The next wave for data disciplines may be experimental designs and analytical methods for closing the gap between the "real-world" situations faced by decision-makers and their idealized representations in optimization problems, and the health sciences are poised to be the discipline where these developments substantially improve lives. We discuss three recent trends in research-experimental designs and analytical methods for precision medicine and pragmatic trials; technological developments in sensors, wearables, and smartphones for measuring health data; and methods addressing algorithmic bias and model interpretability-and argue that these seemingly disparate trends point to a future where data-driven decision support tools are increasingly used to promote wellbeing.																	1932-1864	1932-1872															10.1002/sam.11475		JUL 2020											
J								A parameter-free affinity based clustering	APPLIED INTELLIGENCE										Clustering; Parameter-free methods	NUMBER; LIKELIHOOD	Several methods have been proposed to estimate the number of clusters in a dataset; the basic idea behind all of them has been to study an index that measures inter-cluster separation and intra-cluster cohesion over a range of cluster numbers and report the number which gives an optimum value of the index. In this paper, we propose a simple parameter-free approach that is more like human cognition of clusters, where closely lying points are easily identified to form a cluster and the total number of clusters is revealed. To identify closely lying points, the affinity of two points is defined as a function of distance and a threshold affinity is identified, above which two points in a dataset are likely to be in the same cluster. Well separated clusters are identified even in the presence of outliers, whereas for a not well-separated dataset, the final number of clusters is estimated from the detected clusters. And they are merged to produce the final clusters. Experiments performed with several large dimensional synthetic and real datasets show good results with robustness to noise and density variation within a dataset.																	0924-669X	1573-7497															10.1007/s10489-020-01812-2		JUL 2020											
J								MANET security routing protocols based on a machine learning technique (Raspberry PIs)	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										MANET; Energy management; Security routing protocols; Machine learning; Design issues		Mobile ad-hoc networks (MANETs) are with no framework and comprises just of equivalent companions. This paper shows exact tests for MANET routing the protocols. The testbed dependent on Linux stage introduced in mobile computers. A few parameters are investigated, for example, development speed and the quantity of jumps which sway on routing execution. The testbed comprises of a few Raspberry Pis (RPis) without the requirement for any central master devices. For making the RPis portable and autonomous of any fixed power attachments, every one of them is controlled by a battery. Then again, for the assessment of the testbed, two directing conventions are picked. The main convention is called BABEL, which considered as a separation vector directing calculation. The other one, the optimized link state routing protocol (OLSR) which considered as proactive steering convention. The utilization case was a multi-jump download of documents with various sizes. The point is to assess how different bounces impact the transfer speed and postponement. The outcomes demonstrate that OLSR performs better with respect to the throughput. However, Babel has less deferral and quicker with respect to convergence.																	1868-5137	1868-5145															10.1007/s12652-020-02211-8		JUL 2020											
J								Optimal participation of demand response aggregators in reconfigurable distribution system considering photovoltaic and storage units	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Distribution feeder reconfiguration (DFR); Demand response program (DRP); Distributed generators (DGs); Modified honey be mating optimization (MHBMO)	DISTRIBUTION FEEDER RECONFIGURATION; DISTRIBUTION NETWORKS; LOSS REDUCTION; RELIABILITY; ALGORITHM; DG; OPTIMIZATION; MANAGEMENT; PLACEMENT	Feeder reconfiguration is an important operational process in power distribution grids, which is implemented to enhance the system's performance by managing the switches. Given variations of the electricity price and load pattern in smart distribution networks, the operational problems of the distribution system are largely time-dependent and very complex. To deal with these temporal dependencies, it is important to extend the problem across different time intervals. Toward this end, a multi-objective optimization model is presented in this study for dynamic feeder reconfiguration problem in the distribution system over multiple time intervals considering distributed generators, energy storage systems, and solar photovoltaic units. The demand response program including interruptible/curtailable service is proposed to enable energy consumers to rethink their energy consumption patterns based on incentive and punitive policies. The common objectives considered in the feeder reconfiguration problem are power loss and voltage deviation which are important objectives for traditional distribution systems, but less attention has been paid to distribution network voltage security. This study considers operational cost, energy loss and voltage stability index as objective functions that can meet operational and voltage security expectations. Dynamic feeder reconfiguration problem is a complex integer nonlinear program problem and hence it is difficult to solve which requires the use of appropriate optimization algorithms to converge to global optima or find close to global optima. In this paper, a modified honey bee matting optimization algorithm based on the new mating mechanism is presented to solve the multi-objective dynamic feeder reconfiguration problem. The proposed approach uses an eliminating zone concept to finds a set of non-dominated solutions during the search process. Furthermore, a fuzzy decision-maker is adopted to select the best compromise solution among the non-dominated solutions. The suggested approach is tested on the IEEE 33-node and 136-node test systems and its superiorities are shown through comparison with other evolutionary algorithms.																	1868-5137	1868-5145															10.1007/s12652-020-02322-2		JUL 2020											
J								Human activity recognition based on smartphone using fast feature dimensionality reduction technique	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Activity recognition; Smartphone; Accelerometer; FFDRT; Random forest	CLASSIFIERS	Human activity recognition aims to identify the activities carried out by a person. Recognition is possible by using information that is retrieved from numerous physiological signals by attaching sensors to the subject's body. Lately, sensors like accelerometer and gyroscope are built-in inside the Smartphone itself, which makes activity recognition very simple. To make the activity recognition system work properly in smartphone which has power constraint, it is very essential to use an optimization technique which can reduce the number of features used in the dataset with less time consumption. In this paper, we have proposed a dimensionality reduction technique called fast feature dimensionality reduction technique (FFDRT). A dataset (UCI HAR repository) available in the public domain is used in this work. Results from this study shows that the fast feature dimensionality reduction technique applied for the dataset has reduced the number of features from 561 to 66, while maintaining the activity recognition accuracy at 98.72% using random forest classifier and time consumption in dimensionality reduction stage using FFDRT is much below the state of the art techniques.																	1868-5137	1868-5145															10.1007/s12652-020-02351-x		JUL 2020											
J								An improved adaptive neuro fuzzy interference system for the detection of autism spectrum disorder	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Autism spectrum disorder; ASD screening methods; Machine learning; Improved adaptive neuro fuzzy interference system; Particle swarm optimization	CHILDREN	Autism spectrum disorder (ASD) has become one of the most areas of research in present decade. Previous research works have focused on varied approaches to handle Autism spectrum disorder. However, significance of the existing approaches could still be improved. The efficiency and accuracy of ASD prediction can be improved via building classification systems using artificial intelligence mechanisms like machine learning. This paper addresses this issue by designing an expert system to evaluate autism under uncertainty. An improved adaptive neuro fuzzy interference system (IANFIS) approach has been employed to develop an autism detection model for detecting ASD in individuals of all ages. The advantage of IANFIS is to give the highest classification and the lowest error rates when compared to other existing classifiers. Here the Particle Swarm Optimization (PSO) method is proposed for identifying and choosing the most important ASD feature subset. Its performance was evaluated with ISAA dataset, which comprises of data from individuals with and without autistic attributes. Experimental outcomes revealed that the proposed ASD detection model generates better outcomes in terms of sensitivity, accuracy, precision and specificity.																	1868-5137	1868-5145															10.1007/s12652-020-02332-0		JUL 2020											
J								Real time link quality based route selection and transmission in industrial Manet for improved QoS	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Manet; Link quality; Link availability; Traffic; Route selection; QoS; QoS-SM; Industrial services		The QoS (quality of service) of Manet is depending various factors but it is highly rely on quality of links between the routes. The ratio of link failure reduces the throughput performance which is directly affecting the QoS. The most industrial units provide various services which can be accessed through mobile adhoc network. This supports the industrial entities and users to access the services through various mobile devices. As the nodes of Manet moves in different speed at various directions, the frequency of link failure will be always higher. However, to support data transmission, number of routing protocols has been discussed earlier (REF-26), but suffer to achieve higher QoS performance. To improve the quality of route selection towards maximization of QoS, a novel probabilistic link availability based traffic free route selection algorithm is presented in this paper. The method uses the location information, mobility speed, direction of various nodes in the measurement of probability in link availability. Also, the value of traffic at each node, neighbors of each node, and the frequency of transmission in each hop to measure the QoS Support Measure. The method first identifies the list of routes according to the broadcast mechanism and learns the topological information and traffic details. Using the details learned, the method identifies the list of routes and probable feature routes. For each route identified, the method estimates QoS support measure to choose a traffic free route at any time. Finally a single route has been selected to achieve high performance transmission in Manet.																	1868-5137	1868-5145															10.1007/s12652-020-02331-1		JUL 2020											
J								MHO: meta heuristic optimization applied task scheduling with load balancing technique for cloud infrastructure services	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Task scheduling; Metaheurisitc; Optimization; Load balancing; Cloud infrastructure	COMPUTING ENVIRONMENTS; ALGORITHMS; PSO	The cloud computing provides on demand access to shared resources over internet in a cloud platform powerfully adaptable and metered way. Cloud computing empowers the user get to wherever to a shared pool of configurable resources and gives different administrations to the resource assignment like scientific operations, services computing through virtualization. To give guaranteed productive execution to clients, tasks ought to be proficiently mapped to accessible resources. In this manner, Task Scheduling is noteworthy issue in the cloud infrastructure administrations. The essential target of task execution planning includes reserving the infrastructure assets and limiting the goal of the execution plan. In this research work, we proposed metaheuristic optimization technique with load balancing to enhance the cloud infrastructure service provider's performance there by depleting the scheduling issues. The proposed technique is pertinent for static and dynamic task condition, where static methods VM parameters are fixed, dynamic means parameters are chosen runtime. The proposed algorithm consists of two phases MHOS-S and MHO-D for dealing with static and dynamic properties of the task submitted. The result analysis by comparing with few traditional metaheuristic algorithms proves that the proposed technique performs better in complex environments.																	1868-5137	1868-5145															10.1007/s12652-020-02282-7		JUL 2020											
J								A genetic algorithm for finding realistic sea routes considering the weather	JOURNAL OF HEURISTICS										Weather routing; Ship routing; Genetic algorithm; Uncertain weather	EVOLUTIONARY COMPUTATION	The weather has a major impact on the profitability, safety, and environmental sustainability of the routes sailed by seagoing vessels. The prevailing weather strongly influences the course of routes, affecting not only the safety of the crew, but also the fuel consumption and therefore the emissions of the vessel. Effective decision support is required to plan the route and the speed of the vessel considering the forecasted weather. We implement a genetic algorithm to minimize the fuel consumption of a vessel taking into account the two most important influences of weather on a ship: the wind and the waves. Our approach assists route planners in finding cost minimal routes that consider the weather, avoid specified areas, and meet arrival time constraints. Furthermore, it supports ship speed control to avoid areas with weather conditions that would result in high fuel costs or risk the safety of the vessel. The algorithm is evaluated for a variety of instances to show the impact of weather routing on the routes and the fuel and travel time savings that can be achieved with our approach. Including weather into the routing leads to a savings potential of over 10% of the fuel consumption. We show that ignoring the weather when constructing routes can lead to routes that cannot be sailed in practice. Furthermore, we evaluate our algorithm with stochastic weather data to show that it can provide high-quality routes under real conditions even with uncertain weather forecasts.																	1381-1231	1572-9397				DEC	2020	26	6					801	825		10.1007/s10732-020-09449-7		JUL 2020											
J								Aggregate density-based concept drift identification for dynamic sensor data models	NEURAL COMPUTING & APPLICATIONS										Concept drift; Unsupervised concept detection; IoT; Data modeling	COVARIATE SHIFT	The reduced costs of embedded systems and sensor technology coupled with the increased speed in communication enables businesses and consumers to deploy a large number of sensing devices. This conjunction of technologies has come to be known as the Internet of Things (IoT). Data collected from IoT devices are continuously increasing, and many approaches have been proposed to deal with the big data that is now generated. Multiple artificial intelligent techniques have been proposed and used to extract knowledge out of these continuously growing datasets. In this paper, we demonstrate that a better understanding of data can be achieved through dynamic modeling. This dynamic behavior is observed in many practical scenarios and needs to be taken into account to have a higher accuracy in prediction and analysis for policy making and business-related decisions. We propose and test a novel methodology to detect the dynamic nature of data over time. Machine learning models have been known to suffer from changes in streaming data over time which is defined as concept drift and therefore by detecting this phenomena such models can be improved.																	0941-0643	1433-3058															10.1007/s00521-020-05190-1		JUL 2020											
J								Data-driven system health monitoring technique using autoencoder for the safety management of commercial aircraft	NEURAL COMPUTING & APPLICATIONS										Aircraft health monitoring; Aircraft upset detection; Autoencoder; Aviation safety	ANOMALY DETECTION; NEURAL-NETWORKS; FLIGHT; MODEL; ROBUST; PERFORMANCE; CONTROLLER; PREDICTION	This paper presents the development of a real-time and data-driven aircraft health monitoring technique to monitor in-flight operations by detecting off-nominal flight operation such as aircraft upset, which is strongly related to aviation safety and is a primary contributor to fatal accidents worldwide. A deep autoencoder is adopted to effectively estimate the complex flight responses from historical flight datasets and capture potential upset precursors. Key sensing variables highly relevant to upset primary factors are selected based on aircraft accident reports. The variables are then preprocessed using decimation and Savitzky-Golay filter for sampling frequency synchronization and denoising. Statistical detection baselines are employed to extract statistically unusual operation patterns that highly correlate with upset precursors. The performance of the developed methodology is evaluated by demonstrating real-time detection of upset precursors in flight datasets. An actual accident scenario is introduced to validate detection robustness. The results show that the proposed aircraft upset detector technique can enhance pilots' situational awareness by providing early safety alert for more effective flight safety management.																	0941-0643	1433-3058															10.1007/s00521-020-05186-x		JUL 2020											
J								SHEG: summarization and headline generation of news articles using deep learning	NEURAL COMPUTING & APPLICATIONS										Extractive summarization; Abstractive summarization; Deep learning; Reinforcement learning; NLP; Headline generation		The human attention span is continuously decreasing, and the amount of time a person wants to spend on reading is declining at an alarming rate. Therefore, it is imperative to provide a quick glance of important news by generating a concise summary of the prominent news article, along with the most intuitive headline in line with the summary. When humans produce summaries of documents, they not only extract phrases and concatenate them but also produce new grammatical phrases or sentences that coincide with each other and capture the most significant information of the original article. Humans have an incredible ability to create abstractions; however, automatic summarization is a challenging problem. This paper aims to develop an end-to-end methodology that can generate brief summaries and crisp headlines that can capture the attention of readers and convey a significant amount of relevant information. In this paper, we propose a novel methodology known as SHEG, which is designed as a hybrid model. It works by integrating both extractive and abstractive mechanisms using a pipelined approach to produce a concise summary, which is then used for headline generation. Experiments were performed on publicly available datasets, viz. CNN/Daily Mail, Gigaword, and NEWSROOM. The results obtained validate our approach and demonstrate that the proposed SHEG model is effectively producing a concise summary as well as a captivating and fitting headline.																	0941-0643	1433-3058															10.1007/s00521-020-05188-9		JUL 2020											
J								Damage identification method of prestressed concrete beam bridge based on convolutional neural network	NEURAL COMPUTING & APPLICATIONS										Prestressed concrete girder bridge; Damage identification; Convolutional neural network	MODEL	Bridges play an important role in transportation, but because of overload and natural factors, bridges will inevitably be damaged, which will affect traffic and even lead to major accidents. Therefore, timely and accurate identification of bridge damage is extremely necessary. Because of the great danger of manual detection, in order to identify the damage of prestressed concrete girder bridge safely, conveniently and accurately, this paper proposes a method of damage identification of prestressed concrete girder bridge based on convolutional neural network, which realizes the intelligent identification of bridge damage. Firstly, the damage identification method based on the flexibility matrix is introduced, and the flexibility diagonal curvature index constructed by the diagonal element of flexibility matrix is introduced. Secondly, the basic principle of applying convolutional neural network to bridge damage identification is elaborated. Finally, combined with the flexibility curvature method and the convolutional neural network, the flexibility of the structure is selected as the input of the convolutional neural network to realize the bridge damage identification. Through simulation, it is found that the use of convolutional neural network for the bridge identification is feasible, and combined with the flexibility curvature method, it can well identify the damage location and damage degree of prestressed concrete beam bridge structure.																	0941-0643	1433-3058															10.1007/s00521-020-05052-w		JUL 2020											
J								The intelligent machine: a new metaphor through which to understand both corporations and AI	AI & SOCIETY										Artificial intelligence; Legal personhood; Intelligent machine; Corporation; Rights		This paper proposes to address the question of the legal status of artificial intelligence (AI) from a perspective that is unique to itself. Which means that, rather than attempting to place AI in the box of legal personhood-where many other nonhuman entities already reside, in a legal space where they are in a state of constant friction with humans-we will see whether these inhabitants can be placed in a different box: not that of legal personhood, but that of the intelligent machine. I accordingly argue that we have made a mistake in not clearly maintaining the original separation between legal persons and human persons: this is how legal persons-particularly as the concept applies to corporations-have so far been claiming and gaining the rights ascribed to humans, on the basis of their personhood. AI instead suggests the need to work outside the framework of this fiction: it suggests that we should stop using the fiction of the person for something that is not a person in the first place and in the most original and primary sense of this word. I propose a different metaphor, the metaphor of the intelligent machine, to this end drawing on the work of Dan-Cohen (2016), who advanced this metaphor to argue that it fits corporations better than the metaphor of the person. In conclusion, we should be able to see that this metaphor of the intelligent machine makes a better fit not only for corporations but also for AI and robotics.																	0951-5666	1435-5655															10.1007/s00146-020-01018-7		JUL 2020											
J								Welding quality evaluation of resistance spot welding based on a hybrid approach	JOURNAL OF INTELLIGENT MANUFACTURING										Welding quality; Dynamic resistance; Online monitoring	ELECTRODE DISPLACEMENT SIGNAL; DYNAMIC RESISTANCE; NEURAL-NETWORK; EXPULSION	In this investigation, the welding quality of TC2 titanium alloy with 0.4 mm thickness was predicted using two regression models and an artificial neural network model. The welding current and the voltage between the upper and lower electrodes were obtained using the Rogowski coil and a line voltage sensor. And then the variations of the dynamic resistance curve and the effects of the welding current and welding time on the dynamic resistance signals were investigated. The principal component analysis (PCA) was employed to eliminate the redundant information in the dynamic resistance curve and characterize the shape information of the entire dynamic resistance. A linear regression model quantifying the relationship between the nugget diameter and the principal components was established. The results of the analysis of variance indicated that the performance of this regression equation was very good. Some statistical characteristics of the dynamic resistance signal were also extracted to investigate the relationship between the nugget diameter and dynamic resistance. The results indicated that the regression model established based on the PCA technique was much more robust than the model developed on the basis of the features manually extracted from the dynamic resistance signal. The neural network model was also used to predict the nugget diameter of the welding joints utilizing the extracted features. The performances of the three established prediction models were compared and their behavioral discrepancies were also investigated. The PCA technique not only can minimize the prior assumptions about the certain shape of the dynamic resistance curve and remove the subjective factors caused by the manual extraction method, but it also can assess and monitor the welding quality with a good level of reliability.																	0956-5515	1572-8145															10.1007/s10845-020-01627-5		JUL 2020											
J								Developing a mobile activity game for stroke survivors-lessons learned	JOURNAL ON MULTIMODAL USER INTERFACES										Stroke; Activity; Game; Interaction; Design	PHYSICAL-ACTIVITY	Persons who have survived a stroke might lower the risk of having recurrent strokes by adopting a healthier lifestyle with more exercise. One way to promote exercising is by fitness or exergame apps for mobile phones. Health and fitness apps are used by a significant portion of the consumers, but these apps are not targeted to stroke survivors, who may experience cognitive limitations (like fatigue and neglect), have problems with mobility due to hemiplegia, and balance problems. We outline the design process, implementation and user involvement in the design of an exergame app that is specifically targeted to stroke survivors, and present the lessons learned during the design process.																	1783-7677	1783-8738				SEP	2020	14	3			SI		303	312		10.1007/s12193-020-00342-y		JUL 2020											
J								Upper confidence tree for planning restart strategies in multi-modal optimization	SOFT COMPUTING										Multi-modal optimization; Continuous optimization; Reinforcement learning; Evolutionary algorithm	EVOLUTION STRATEGY; ALGORITHM	In the context of gradient-free multi-modal optimization, numerous algorithms are based on restarting evolution strategies. Such an algorithm classically performs many local searches, for finding all the global optima of the objective function. The strategy used to select the restarting points (i.e., the initial points of the local searches) is a crucial step of the method. In previous works, a strategy based on reinforcement learning has been proposed: the search space is partitioned and a multi-armed bandit algorithm is used to select an interesting region to sample. This strategy significantly improves the main optimization algorithm but is limited to small dimensional problems. In this paper, we propose an algorithm tackling this problem. The main contributions are (1) a tree-based scheme for hierarchically partition the search space and (2) a multi-armed bandit strategy for traversing this tree. Thus, a node in the tree corresponds to a region of the search space, and its children partition this region according to one dimension. The multi-armed bandit strategy is used to traverse the tree by selecting interesting children recursively. We have experimented our algorithm on difficult multi-modal functions, with small and large dimensions. For small dimensions, we observe performances comparable to previous state-of-the-art algorithms. For large dimensions, we observe better results as well as lower memory consumption.																	1432-7643	1433-7479															10.1007/s00500-020-05196-w		JUL 2020											
J								Face similarity linkage: A novel biometric approach to sexually motivated serial killer victims	EXPERT SYSTEMS										biometric; crime linkage; face similarity linkage; homicide; serial killer; sexual homicide; victim	RECOGNITION; OFFENDER; MURDER	Some sexually motivated serial killers target victims on the basis of appearance. Therefore, multiple victims of a single serial killer are likely to have some facial features and geometries that are similar. The current research was undertaken to propose a technique, termed face similarity linkage, to evaluate whether victims of a serial killer have statistically more similar facial measurements than a randomly chosen person of the same gender. To test this, three of Ted Bundy's victims were randomly selected and anatomical landmarks were located and measured to produce proportionality indices of their faces. A random subject from an online database was used as a comparison. The results showed there were no statistically significant differences between the three of Bundy's victims, however there was significant difference between 11 of the 17 facial measurements of Bundy's victims when compared to a random person. This research serves as a proof of concept that, with more advanced means of data collection, may be a useful tool for law enforcement for linking serial homicides. The current method is relatively novel, and in need of expert systems interfaces to improve speed and application, which is outlined in the current study.																	0266-4720	1468-0394														e12597	10.1111/exsy.12597		JUL 2020											
J								Common set of weights in data envelopment analysis under prospect theory	EXPERT SYSTEMS										common weights; data envelopment analysis; prospect theory	EFFICIENT UNIT; IDEAL DMU; DEA; RANKING; DECISION; MANAGEMENT; EPSILON	Data envelopment analysis (DEA) is a data-driven tool for performance evaluation, measuring decision-making units (DMUs) and designating them with specific weightings. The standard DEA model typically sets up that decision-makers (DMs) are wholly rational to select the most favourable weights to obtain the maximum performance score, but does not take into account their attitude toward risk during the assessment. The prospect theory generally matches humans' psychological behaviours. Thus, our study captures the non-rational behaviours of DMs, performing under risk scenarios, in order to construct a novel common-weights DEA model that maximizes the total prospect value, which can vary more steeply for losses than for gains, hence obtaining a more realistic common weight scheme. Our proposed model not only generates DMUs, with higher total prospect values, but also greater degrees of satisfaction. The current study shows that the prospect theory can be aptly extended to the DEA research area, supplying a proper guideline for future DEA research.																	0266-4720	1468-0394														e12602	10.1111/exsy.12602		JUL 2020											
J								Investigating challenges with scattering and inner filter effects in front-face fluorescence by PARAFAC	JOURNAL OF CHEMOMETRICS										front-face fluorescence; inner filter effect; PARAllel FACtor analysis (PARAFAC); real undiluted samples analysis; scattering effect	DISSOLVED ORGANIC-MATTER; PARALLEL FACTOR-ANALYSIS; EXCITATION-EMISSION MATRICES; METAL-BINDING; SPECTROSCOPY; SPECTRA; DECOMPOSITION; COMPONENTS; COPPER; CHEMOMETRICS	The use of front-face fluorescence spectroscopy, and three-way chemometric analysis (like PARAllel FACtor analysis, PARAFAC), has been successfully applied for the analysis of unpretreated samples in the fields of food and environmental analysis. It would be desirable to evaluate the potential of this approach for the analysis of any real sample in any field of research. Even in the simplest of the real samples acquired with front-face, the presence of scattering and inner filter effects will occur, potentially hampering the subsequent data analysis due to deviations from Beer-Lambert's law. This paper addresses these concerns in practice, proposing a strategy of spectral preprocessing to mitigate these effects. This is done by measuring several data sets with different levels of scattering and inner filter effects by fluorescence in excitation-emission mode. The results show that the occurrence of these interferents (sometimes neglected with front-face mode) affects the fluorescence signal and interferes with any traditional analysis on these data, as much as they hamper the successful use of methods like PARAFAC. The proposed preprocessing strategy is based on one of the most traditional correction for the inner filter effect with right-angle mode. However, we suggest applying a tunable factor,b, that will account for the degree of deviation from linearity between concentration of a given analyte and its fluorescence signal. It is demonstrated that by choosing a properb-value, this correction helps in finding an acceptable solution for the PARAFAC algorithm, in line with Beer-Lambert's law.																	0886-9383	1099-128X				SEP	2020	34	9							e3286	10.1002/cem.3286		JUL 2020											
J								Acquiring reusable skills in intrinsically motivated reinforcement learning	JOURNAL OF INTELLIGENT MANUFACTURING										Hierarchical reinforcement learning; Skill; Option; Intrinsic motivation; Skill evaluation		This paper proposes a novel incremental model for acquiring skills and using them in Intrinsically Motivated Reinforcement Learning (IMRL). In this model, the learning process is divided into two phases. In the first phase, the agent explores the environment and acquires task-independent skills by using different intrinsic motivation mechanisms. We present two intrinsic motivation factors for acquiring skills by detecting states that can lead to other states (being a cause) and by detecting states that help the agent to transition to a different region (discounted relative novelty). In the second phase, the agent evaluates the acquired skills to find suitable ones for accomplishing a specific task. Despite the importance of assessing task-independent skills to perform a task, the idea of evaluating skills and pruning them has not been considered in IMRL literature. In this article, two methods are presented for evaluating previously learned skills based on the value function of the assigned task. Using such a two-phase learning model and the skill evaluation capability helps the agent to acquire task-independent skills that can be transferred to other similar tasks. Experimental results in four domains show that the proposed method significantly increases learning speed.																	0956-5515	1572-8145															10.1007/s10845-020-01629-3		JUL 2020											
J								Moral control and ownership in AI systems	AI & SOCIETY										Artificial Intelligence; Moral agency; Data bias; Machine learning; Autonomous systems; Decision support	ETHICS; DESIGN	AI systems are bringing an augmentation of human capabilities to shape the world. They may also drag a replacement of human conscience in large chunks of life. AI systems can be designed to leave moral control in human hands, to obstruct or diminish that moral control, or even to prevent it, replacing human morality with pre-packaged or developed 'solutions' by the 'intelligent' machine itself. Artificial Intelligent systems (AIS) are increasingly being used in multiple applications and receiving more attention from the public and private organisations. The purpose of this article is to offer a mapping of the technological architectures that support AIS, under the specific focus of the moral agency. Through a literature research and reflection process, the following areas are covered: a brief introduction and review of the literature on the topic of moral agency; an analysis using the BDI logic model (Bratman1987); an elemental review of artificial 'reasoning' architectures in AIS; the influence of the data input and the data quality; AI systems' positioning in decision support and decision making scenarios; and finally, some conclusions are offered about regarding the potential loss of moral control by humans due to AIS. This article contributes to the field of Ethics and Artificial Intelligence by providing a discussion for developers and researchers to understand how and under what circumstances the 'human subject' may, totally or partially, lose moral control and ownership over AI technologies. The topic is relevant because AIS often are not single machines but complex networks of machines that feed information and decisions into each other and to human operators. The detailed traceability of input-process-output at each node of the network is essential for it to remain within the field of moral agency. Moral agency is then at the basis of our system of legal responsibility, and social approval is unlikely to be obtained for entrusting important functions to complex systems under which no moral agency can be identified.																	0951-5666	1435-5655															10.1007/s00146-020-01020-z		JUL 2020											
J								Cardiomyopathy -induced arrhythmia classification and pre-fall alert generation using Convolutional Neural Network and Long Short-Term Memory model	EVOLUTIONARY INTELLIGENCE										Cardiomyopathy; Arrhythmia; Pre-fall; Feature extraction; Convolution Neural Network; Long Short Term Memory	FEATURES; MACHINE	Automatic analysis of Electrocardiogram (ECG) signals plays an important role in the medical field to deal with various crucial cardiac conditions. Currently, the detection of cardiomyopathy and arrhythmias considered a challenging task. Machine learning-based techniques have gained a huge attraction to classify these patterns, but most of the existing works have focused on arrhythmia classification. The researchers don't contribute much work towards, the cardiac disease case where Cardiomyopathy induced arrhythmia based classification. This work presents a novel approach to classify cardiomyopathy and cardiomyopathy with arrhythmia using a Convolutional Neural Network (CNN) based model. Along with classification, this work combines a pre-fall alert generation based on the heart rate conditions. The existing CNN models suffer from computational complexity; hence, this work incorporates the Long Short-Term Memory (LSTM) layer and developed CNN-LSTM based architecture. The proposed model is implanted on MIT-BIH data where these two classes segregated with the help of expert clinicians. In order to show the robust performance of the proposed model, the performance of CNN-LSTM has compared with state-of-art classifiers such as decision tree, support vector machine, and neural network. The comparative study shows that the proposed classification methods achieved significant improvement in the classification accuracy rate.																	1864-5909	1864-5917															10.1007/s12065-020-00454-0		JUL 2020											
J								Impact of the thermomechanical load on subsurface phase transformations during cryogenic turning of metastable austenitic steels	JOURNAL OF INTELLIGENT MANUFACTURING										Martensite; Cryogenic turning; Metastable austenitic steel; Deformation-induced phase transformation	INDUCED MARTENSITIC-TRANSFORMATION; DEFORMATION-INDUCED MARTENSITE; PLASTIC-DEFORMATION; STAINLESS-STEEL; CONSTITUTIVE MODEL; SURFACE INTEGRITY; STRAIN RATE; NUCLEATION; KINETICS; BEHAVIOR	When machining metastable austenitic stainless steel with cryogenic cooling, a deformation-induced phase transformation from gamma-austenite to alpha '-martensite can be realized in the workpiece subsurface. This leads to a higher microhardness and thus improved fatigue and wear resistance. A parametric and a non-parametric model were developed in order to investigate the correlation between the thermomechanical load in the workpiece subsurface and the resulting alpha '-martensite content. It was demonstrated that increasing passive forces and cutting forces promoted the deformation-induced phase transformation, while increasing temperatures had an inhibiting effect. The feed force had no significant influence on the alpha '-martensite content. With the proposed models it is now possible to estimate the alpha '-martensite content during cryogenic turning by means of in-situ measurement of process forces and temperatures.																	0956-5515	1572-8145															10.1007/s10845-020-01626-6		JUL 2020											
J								Statistical process monitoring in a specified period for the image data of fused deposition modeling parts with consistent layers	JOURNAL OF INTELLIGENT MANUFACTURING										Statistical process monitoring; Fused deposition modeling; Probability of alarm in a specified period; Generalized likelihood ratio	QUALITY; POWDER	Statistical process monitoring (SPM) methods have been adopted and studied to detect variations in the fused deposition modeling (FDM) process in recent years. The FDM process that builds parts layer-by-layer is accomplished in a specified manufacturing period (number of layers) without interruption or suspension. Thus, traditional SPM methods, where the average run length is used for the calculation of the control limits and the measurement of the performance, are no longer applicable to the FDM process. In this paper, an SPM method is proposed based on the surface image data of FDM parts with consistent layers and a specified period. The probability of alarm in a specified period (PASP) and the cumulative PASP are introduced to determine the control limits and evaluate the monitoring performance. Regions of interest are determined in a fixed way to cover the sizes and locations of different defects. The statistics are calculated based on the generalized likelihood ratio. The control limit is determined based on the specified period and the nominal in-control PASP. A simulation study for different locations, sizes and magnitudes of the mean shift of defects is presented. In the case study, the proposed SPM method is applied to monitor the FDM process of a cuboid, which verifies the effectiveness of the proposed method.																	0956-5515	1572-8145															10.1007/s10845-020-01628-4		JUL 2020											
J								A note to: A multiple-rule based constructive randomized search algorithm for solving assembly line worker assignment and balancing problem	JOURNAL OF INTELLIGENT MANUFACTURING										Assembly line balancing; Worker assignment; Mixed integer linear programming	BOUND ALGORITHM; CENTERS	Some of the solutions reported in a recent paper (Akyol and Baykasoglu in J Intell Manuf 30(2):557-573, 2019.) are infeasible, jeopardising the conclusion that "best results for the 75% of the 320 test instances for the ALWABP-2" were obtained. In this note, we explore the contributions of the paper, show that some of the solutions are indeed infeasible and point out possible inconsistencies in the employed heuristic that may be causing this issue.																	0956-5515	1572-8145															10.1007/s10845-020-01632-8		JUL 2020											
J								DyS-IENN: a novel multiclass imbalanced learning method for early warning of tardiness in rocket final assembly process	JOURNAL OF INTELLIGENT MANUFACTURING										Early warning of tardiness; Rocket final assembly; Imbalanced learning; Ensemble neural network; Dynamic sampling	NEURAL-NETWORKS; CYCLE TIME; FAULT-DIAGNOSIS; PREDICTION	Establishing an effective early warning mechanism for the rocket final assembly process (RFAP) is crucial for the timely delivery of rockets and the reduction of additional production costs. To solve the unsystematic design of warning indicators and warning levels in RFAP and address the problem of low warning accuracy caused by imbalanced data distribution, this paper redesigns the warning indicators and warning levels in a systematic way, and develops a novel multiclass imbalanced learning method based on dynamic sampling algorithm (DyS) and improved ensemble neural network (IENN). The DyS algorithm dynamically determines the training set after oversampling the minority class, while the IENN can effectively suppress the oscillation in the iterative process of the DyS algorithm and improve the overall classification accuracy by removing the redundant and ineffective networks from the ensemble neural network. The experiment results indicate that the proposed method outperforms other methods in terms of accuracy and stability for early warning of tardiness in RFAP.																	0956-5515	1572-8145															10.1007/s10845-020-01631-9		JUL 2020											
J								Trajectory Tracking for Aerial Robots: an Optimization-Based Planning and Control Approach	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Trajectory tracking; Trajectory planning; Aerial robotics; Multirotor; UAV; MAV; Remotely operated vehicles; Mobile robots; Model predictive control; Optimization	NONLINEAR MPC; QUADROTOR; GENERATION	In this work, we present an optimization-based trajectory tracking solution for multirotor aerial robots given a geometrically feasible path. A trajectory planner generates a minimum-time kinematically and dynamically feasible trajectory that includes not only standard restrictions such as continuity and limits on the trajectory, constraints in the waypoints, and maximum distance between the planned trajectory and the given path, but also restrictions in the actuators of the aerial robot based on its dynamic model, guaranteeing that the planned trajectory is achievable. Our novel compact multi-phase trajectory definition, as a set of two different kinds of polynomials, provides a higher semantic encoding of the trajectory, which allows calculating an optimal solution but following a predefined simple profile. A Model Predictive Controller ensures that the planned trajectory is tracked by the aerial robot with the smallest deviation. Its novel formulation takes as inputs all the magnitudes of the planned trajectory (i.e. position and heading, velocity, and acceleration) to generate the control commands, demonstrating through in-lab real flights an improvement of the tracking performance when compared with a controller that only uses the planned position and heading. To support our optimization-based solution, we discuss the most commonly used representations of orientations, as well as both the difference as well as the scalar error between two rotations, in both tridimensional and bidimensional spacesSO(3) andSO(2). We demonstrate that quaternions and error-quaternions have some advantages when compared to other formulations.																	0921-0296	1573-0409				NOV	2020	100	2					531	574		10.1007/s10846-020-01203-2		JUL 2020											
J								Sentiment lexicons and non-English languages: a survey	KNOWLEDGE AND INFORMATION SYSTEMS										Sentiment analysis; Sentiment Lexicon; Lexicon-based; Multilingual sentiment analysis	OPINION; DICTIONARIES; SUBJECTIVITY; CONSTRUCTION; STATE	The ever-increasing number of Internet users and online services, such as Amazon, Twitter and Facebook has rapidly motivated people to not just transact using the Internet but to also voice their opinions about products, services, policies, etc. Sentiment analysis is a field of study to extract and analyze public views and opinions. However, current research within this field mainly focuses on building systems and resources using the English language. The primary objective of this study is to examine existing research in building sentiment lexicon systems and to classify the methods with respect to non-English datasets. Additionally, the study also reviewed the tools used to build sentiment lexicons for non-English languages, ranging from those using machine translation to graph-based methods. Shortcomings are highlighted with the approaches along with recommendations to improve the performance of each approach and areas for further study and research.																	0219-1377	0219-3116															10.1007/s10115-020-01497-6		JUL 2020											
J								Affective analysis of patients in homecare video-assisted telemedicine using computational intelligence	NEURAL COMPUTING & APPLICATIONS										Affective computing; Convolutional neural networks; Speeded-up robust features (SURF); Emotion analysis; Telemedicine	FACIAL EXPRESSION RECOGNITION; FEATURES; VALIDATION	The affective/emotional status of patients is strongly connected to the healing process and their health. Therefore, being aware of the psychological peaks and troughs of a patient provides the advantage of timely intervention by specialists or closely related kinsfolk. In this context, this paper presents the design and implementation of an emotion analysis module integrated in an existing telemedicine platform. Two different methodologies are utilized and discussed. The first scheme exploits the fast and consistent properties of the speeded-up robust features algorithm in order to identify the existence of seven different sentiments in human faces. The second is based on convolutional neural networks. The whole functionality is provided as a Web service for the healthcare platform during regular video teleconference sessions between authorized medical personnel and patients. The paper discusses the technical details of the implementation and the incorporation of the proposed scheme and provides the initial results of its accuracy and operation in practice.																	0941-0643	1433-3058															10.1007/s00521-020-05203-z		JUL 2020											
J								Simultaneous streamflow forecasting based on hybridized neuro-fuzzy method for a river system	NEURAL COMPUTING & APPLICATIONS										Barak river system; Coactive neuro-fuzzy inference system; Hybrid intelligent system; Simultaneous streamflow forecasting	FIREFLY ALGORITHM; INFERENCE SYSTEM; LOGIC; NETWORK; PREDICTION; MODEL	Assessment in simulating river flows for a river system can be implemented simultaneously. In general, the majority of the researchers emphasize forecasting on single output for a river system. The present study investigates the applicability and capability of coactive neuro-fuzzy inference system (CANFIS) for simultaneous river flow forecasting for a Barak river system in Assam, India. Besides, two other hybrid model approaches were developed by optimizing the parameters of CANFIS to adopt model's consistency toward achieving more precise and sensitive result which includes the combination of the CANFIS using genetic algorithm (CANFIS-GA) and CANFIS using firefly algorithm (CANFIS-FA). In total, 19,728 sets of recorded hourly concurrent flows data have been collected from different gauging sites pertaining to monsoon seasons. The results of the models (CANFIS, CANFIS-GA and CANFIS-FA) are evaluated, and the best-fit forecasting model(s) is determined using various statistical performance criterions. Also, this study witnessed the significant improvement in the quality of flow forecasting of traditional CANFIS when integrated using metaheuristics algorithms GA and FA. Besides, performance comparisons of the models are made using the artificial neural networks and probabilistics neural networks. In overall, results suggested that CANFIS-FA considerably improved upon other models and provides more better and accurate results for simultaneous flow forecasting in a river system.																	0941-0643	1433-3058															10.1007/s00521-020-05194-x		JUL 2020											
J								Geometry understanding from autonomous driving scenarios based on feature refinement	NEURAL COMPUTING & APPLICATIONS										Geometry understanding; Multi-task learning; Depth; Optical flow		Nowadays, many deep learning applications benefit from multi-task learning with several related objectives. In autonomous driving scenarios, being able to infer motion and spatial information accurately is essential for scene understanding. In this paper, we propose a unified framework for unsupervised joint learning of optical flow, depth and camera pose. Specifically, we use a feature refinement module to adaptively discriminate and recalibrate feature, which can integrate local features with their global dependencies to capture rich contextual relationships. Given a monocular video, our network firstly calculates rigid optical flow by estimating depth and camera pose. Then, we design an auxiliary flow network for inferring non-rigid flow field. In addition, a forward-backward consistency check is adopted for occlusion reasoning. Extensive analyses on KITTI dataset are conducted to verify the effectiveness of our proposed approach. The experimental results show that our proposed network can produce sharper, clearer and detailed depth and flow maps. In addition, our network achieves potential performance compared to the recent state-of-the-art approaches.																	0941-0643	1433-3058															10.1007/s00521-020-05192-z		JUL 2020											
J								A multigranulation fuzzy rough approach to multisource information systems	SOFT COMPUTING										Rough set; Rough fuzzy set; Multigranulation fuzzy decision rough sets; Gaussian kernel	DECISION; SETS; GRANULATION; MODEL; APPROXIMATION; SELECTION	Multigranulation rough set is one class of the important models in rough set community. However, both pessimistic and optimistic rough sets have disadvantages in describing target concept. In this paper, a novel model, called weighted multigranulation fuzzy decision rough sets, is proposed. Gaussian kernel is used to compute the similarity between objects, which induces a fuzzy equivalence relation. We employ the relation to fuzzily partition the universe and then obtain multiple fuzzy granulations from multisource fuzzy information system. Moreover, some interesting properties of the proposed model are discussed. A comparative study between the proposed multigranulation model and Sun's multigranulation rough set model is carried out. An example is employed to illustrate the effectiveness of the proposed method, which may provide an effective approach for multisource data analysis in real applications.																	1432-7643	1433-7479															10.1007/s00500-020-05187-x		JUL 2020											
J								Analytical fuzzy triangular solutions of the wave equation	SOFT COMPUTING										Generalized Hukuhara differentiability; The D'Alembert's formula; Homogeneous and non-homogeneous fuzzy wave equation; Leibniz rule	PARTIAL-DIFFERENTIAL-EQUATIONS; EXISTENCE	The analytical fuzzy triangular solutions for both one-dimensional homogeneous and non-homogeneous wave equations with emphasis on the type of [gH-p]-differentiability of solutions are obtained by using the fuzzy D'Alembert's formulas. In the current article, the existence and uniqueness of the solutions of the homogeneous and non-homogeneous fuzzy wave equation by considering the type of [gH-p]-differentiability of solutions are provided. In a special case, the fuzzy mathematical model of a vibrating string with a fixed end is investigated. Eventually, given to the various examples represented, the efficacy and accuracy of the method are examined.																	1432-7643	1433-7479															10.1007/s00500-020-05146-6		JUL 2020											
J								Discrete uniform and binomial distributions with infinite support	SOFT COMPUTING										Binomial distribution; Poisson approximation; Charlier polynomials	APPROXIMATION; DECONVOLUTION; METHODOLOGY	We study properties of two probability distributions defined on the infinite set {0, 1, 2, ... } and generalizing the ordinary discrete uniform and binomial distributions. Both extensions use the grossone-model of infinity. The first of the two distributions we study is uniform and assigns masses 1/(1) to all points in the set {0, 1, ... , (1) - 1}, where (1) denotes the grossone. For this distribution, we study the problem of decomposing a random variable xi with this distribution as a sum xi =(d) xi(1) + ... + xi(m), where xi(1), ... , xi(m) are independent non-degenerate random variables. Then, we develop an approximation for the probability mass function of the binomial distribution Bin ((1), p) with p = c/(1)(alpha) with 1/2 < alpha <= 1. The accuracy of this approximation is assessed using a numerical study.																	1432-7643	1433-7479															10.1007/s00500-020-05190-2		JUL 2020											
J								Multi-split optimized bagging ensemble model selection for multi-class educational data mining	APPLIED INTELLIGENCE										e-Learning; Student Performance Prediction; Optimized Bagging Ensemble Learning Model Selection; Gini Index		Predicting students' academic performance has been a research area of interest in recent years, with many institutions focusing on improving the students' performance and the education quality. The analysis and prediction of students' performance can be achieved using various data mining techniques. Moreover, such techniques allow instructors to determine possible factors that may affect the students' final marks. To that end, this work analyzes two different undergraduate datasets at two different universities. Furthermore, this work aims to predict the students' performance at two stages of course delivery (20% and 50% respectively). This analysis allows for properly choosing the appropriate machine learning algorithms to use as well as optimize the algorithms' parameters. Furthermore, this work adopts a systematic multi-split approach based on Gini index and p-value. This is done by optimizing a suitable bagging ensemble learner that is built from any combination of six potential base machine learning algorithms. It is shown through experimental results that the posited bagging ensemble models achieve high accuracy for the target group for both datasets.																	0924-669X	1573-7497															10.1007/s10489-020-01776-3		JUL 2020											
J								A novel ant colony optimization based on game for traveling salesman problem	APPLIED INTELLIGENCE										Nucleolus game strategy; Entropy weight; Mean filtering; Ant colony optimization; Traveling salesman problem	PARTICLE SWARM; ALGORITHM; ALLOCATION; HEURISTICS	Ant Colony Optimization (ACO) algorithms tend to fall into local optimal and have insufficient astringency when applied to solve Traveling Salesman Problem (TSP). To address this issue, a novel game-based ACO (NACO) is proposed in this report. NACO consists of two ACOs: Ant Colony System (ACS) and Max-Min Ant System (MMAS). First, an entropy-weighted learning strategy is proposed. By improving diversity adaptively, the optimal solution precision can be optimized. Then, to improve the astringency, a nucleolus game strategy is set for ACS colonies. ACS colonies under cooperation share pheromone distribution and distribute cooperative profits through nucleolus. Finally, to jump out of the local optimum, mean filtering is introduced to process the pheromone distribution when the algorithm stalls. From the experimental results, it is demonstrated that NACO has well performance in terms of both the solution precision and the astringency.																	0924-669X	1573-7497															10.1007/s10489-020-01799-w		JUL 2020											
J								Automating test oracles from restricted natural language agile requirements	EXPERT SYSTEMS										automated test oracle; automated test-driven development; behaviour-driven development; restricted natural language; agile requirements	USER STORIES	Manual testing of software requirements written in natural language for agile or any other methodology requires more time and human resources. This leaves the testing process error prone and time consuming. For satisfied end users with bug-free software delivered on time, there is a need to automate the test oracle process for natural language or informal requirements. The automation of the test oracle is relatively easier with formal requirements, but this task is difficult to achieve with natural language requirements. This study proposes an approach calledRestricted Natural Language Agile Requirements Testing(ReNaLART) to automate the test oracle from restricted natural language agile requirements. For this purpose, it uses an existing user story template with some modifications for writing user stories. This helps in identifying test input and expected output for a user story. For comparison of expected and observed outputs it makes use of a regex pattern and string distance functions. It is capable of assigning different types of verdicts automatically depending upon the similarity/dissimilarity between observed and expected outputs of user stories. ReNaLART is validated using several case studies of different domains, namely, OLX Pakistan, Mental Health Tests, McDelivery Pakistan, BlueStacks, Power Searching with Google, TensorFlow Playground, w3Schools 2018 offline and Touch'D. It revealed several faults in five of the above listed eight applications. Plus, the proposed test oracle on an average took 0.02 s for test data generation, expected output generation and verdict assignment. Both these facts show the fault revealing effectiveness and efficiency of ReNaLART.																	0266-4720	1468-0394														e12608	10.1111/exsy.12608		JUL 2020											
J								Private rank aggregation under local differential privacy	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										KwikSort algorithm; local differential privacy; rank aggregation	RANDOMIZED-RESPONSE; DATA-COLLECTION; PLATFORMS	In answer aggregation of crowdsourced data management, rank aggregation aims to combine different agents' answers or preferences over the given alternatives into an aggregate ranking which agrees the most with the preferences. However, since the aggregation procedure relies on a data curator, the privacy within the agents' preference data could be compromised when the curator is untrusted. Existing works that guarantee differential privacy in rank aggregation all assume that the data curator is trusted. In this paper, we formulate and address the problem oflocally differentially private rank aggregation, in which the agents have no trust in the data curator. By leveraging the approximate rank aggregation algorithmKwikSort, theRandomized Responsemechanism, and theLaplacemechanism, we propose an effective and efficient protocolLDP-KwikSort. Theoretical and empirical results show that the solutionLDP-KwikSort:RRcan achieve the acceptable trade-off between the utility of aggregate ranking and the privacy protection of agents' pairwise preferences.																	0884-8173	1098-111X				OCT	2020	35	10					1492	1519		10.1002/int.22261		JUL 2020											
J								Secure mobile health system supporting search function and decryption verification	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Mobile devices; Electronic health records; Attribute-based encryption; Online; offline; Health cloud	ENCRYPTION; CLOUD	In order to protect the privacy of the data, the data owners encrypt their sensitive data before transferring it to cloud servers. This method leads to the inability to search over these data. To tackle these challenges, we propose a Secure Mobile Health System (SMHS) supporting search function and decryption verification by using online/offline attribute-based encryption. The new scheme provides a secure Electronic Health Records (EHRs) and efficient searching over the health cloud. In the security analysis, the proposed scheme is proved in the standard model. In addition, we explain via experimental results with a comparison to similar existing protocols that the SMHS can significantly reduce the computation cost in the mobile health system.																	1868-5137	1868-5145															10.1007/s12652-020-02321-3		JUL 2020											
J								Activity recognition in a smart home using local feature weighting and variants of nearest-neighbors classifiers	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Smart home; Activity recognition; Ambient assisted living; Machine learning; Nearest neighbors; Feature weighting	PATTERNS	Recognition of activities, such as preparing meal or watching TV, performed by a smart home resident, can promote the independent living of elderly in a safe and comfortable environment of their own homes, for an extended period of time. Different activities performed at the same location have commonalities resulting in less inter-class variations; while the same activity performed multiple times, or by multiple residents, varies in its execution resulting in high intra-class variations. We propose a Local Feature Weighting approach (LFW) that assigns weights based on both inter-class and intra-class importance of a feature in an activity. Multiple sensors are deployed at different locations in a smart home to gather information. We exploit the obtained information, such as frequency and duration of activation of sensors, and the total sensors in an activity for feature weighting. The weights for the same features vary among activities, since a feature may have more importance for one activity but less for the other. For the classification, we exploit the two variants of K-Nearest Neighbors (KNN): Evidence Theoretic KNN (ETKNN) and Fuzzy KNN (FKNN). The evaluation of the proposed approach on three datasets, from CASAS smart home project, demonstrates its ability in the correct recognition of activities compared to the existing approaches.																	1868-5137	1868-5145															10.1007/s12652-020-02348-6		JUL 2020											
J								An advanced artificial intelligence technique for resource allocation by investigating and scheduling parallel-distributed request/response handling	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Genetic Algorithms (GA); Artificial Neural Networks (ANN); Cloud Computing; Dynamic Voltage Frequency Scaling Technique (DVFS)	ENERGY; JOBS	Cloud computing is an emerging technology undergoing various challenges that integrate parallel and distributed computing together. In the multi-tenant environment cloud applications can be utilized as a service. User request are enormous and therefore the attributes to be concerned about are scalability, reliability, and resource availability and server response. Utilization of software, platform and infrastructure increases in this environment paving way for resource consumption. This scenario arises various types of issues through collision, traffic jam, data loss, request dropout and delay in response. The past research provides solutions for aspects like scalability, resource allocation, scheduling, load balancing and optimized request and response handling, resource management through virtualization. The process of virtualization and migration of environment is difficult. The cost for allocating VM for a single user is less. The paper proposed a novel scheduling approach for handling unlimited incoming request with quality of service through energy and throughput. The allocated resource focus on maintaining incoming job request, request for dispatch to the server and an acknowledgement for the receipt of response. The paper provides resource allocation methodology through scheduling approaches called integrating of AI techniques namely Genetic Algorithms (GA) and Artificial Neural Networks (ANN). The property of the request is analyzed and priorityis applied for scheduling the request using resource allocation.																	1868-5137	1868-5145															10.1007/s12652-020-02334-y		JUL 2020											
J								A moving vehicle tracking algorithm based on deep learning	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Deep learning; Moving vehicle; GMM; DAE; Track		It is difficult to track the moving vehicle due to various factors including complex environment, changes of illumination and scale. A moving vehicle tracking algorithm based on deep learning is proposed in this paper. First of all, traditional GMM algorithm is improved to reduce the error judgment probability of pixel state. Then, a sparse DAE neural network feature learning framework is proposed to ensure efficient extraction of vehicle features and reduce feature redundancy. Finally, the vehicle is tracked in its area. The experimental results show that the CM of proposed algorithm achieves 0.85 and has strong robustness.																	1868-5137	1868-5145															10.1007/s12652-020-02352-w		JUL 2020											
J								Development of breast papillary index for differentiation of benign and malignant lesions using ultrasound images	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Papillary breast; Benign; Malignant; CAD; BEMD; DCT; LSDA; Classification; SVM	CORE-NEEDLE-BIOPSY; FATTY LIVER-DISEASE; INTEGRATED INDEX; FEATURES; IDENTIFICATION; TRANSFORM; WAVELET; MANAGEMENT; TEXTURE; SINGLE	Papillary breast lesions include a wide spectrum of pathologies ranging from benign to malignant. The word papillary originates from finger-like projections, or papules, which are seen when these lesions are projected under a microscope. Papillary breast lesions have an array of radiological features at presentations; hence differentiation between benign and malignant based on imaging features is challenging. Histopathological diagnosis is crucial for the distinction and further management of the lesions. Traditionally, tumor and ductal excision is the treatment of choice for malignant and atypical or benign papilloma with imaging discordance. However, current clinical practice guidance advocates complete surgical excision, even for asymptomatic and purely benign papillomas diagnosed on core needle biopsy, as they are highly associated with atypia and malignant upstage on subsequent surgery. Computer aided diagnosis (CAD) is a non-invasive method of diagnosing medical signals/images using advanced image processing followed by soft computing techniques. In this study, we have developed a non-invasive CAD system for differentiating benign versus malignant papillary breast lesions using bi-dimensional empirical mode decomposition (BEMD) and the discrete cosine transform (DCT) followed by locality sensitive discriminant analysis (LSDA). The developed model is validated using a large collection of ultrasound images of papillary breast lesions, and achieved a maximum performance of 98.63% accuracy. We have also developed a breast papillary index, which may in the future be used as a substitute for the conventional soft computing techniques. The developed model can be utilized as a tool to assist radiologists in their routine clinical practice after validation with a larger database.																	1868-5137	1868-5145															10.1007/s12652-020-02310-6		JUL 2020											
J								Benchmarking the performance of genetic algorithms on constrained dynamic problems	NATURAL COMPUTING										Dynamic multi-objective optimisation; Constrained problems; Genetic algorithms; Performance benchmark; Re-initialisation	MULTIOBJECTIVE OPTIMIZATION; OBJECTIVES	The growing interest in dynamic optimisation has accelerated the development of genetic algorithms with specific mechanisms for these problems. To ensure that these developed mechanisms are capable of solving a wide range of practical problems it is important to have a diverse set of benchmarking functions to ensure the selection of the most appropriate Genetic Algorithm. However, the currently available benchmarking sets are limited to unconstrained problems with predominantly continuous characteristics. In this paper, the existing range of dynamic problems is extended with 15 novel constrained multi-objective functions. To determine how genetic algorithms perform on these constrained problems, and how this behaviour relates to unconstrained dynamic optimisation, 6 top-performing dynamic genetic algorithms are compared alongside 4 re-initialization strategies on the proposed test set, as well as the currently existing unconstrained cases. The results show that there are no differences between constrained/unconstrained optimisation, in contrast to the static problems. Therefore, dynamicity is the prevalent characteristic of these problems, which is shown to be more important than the discontinuous nature of the search and objective spaces. The best performing algorithm overall is MOEA/D, and VP is the best re-initialisation strategy. It is demonstrated that there is a need for more dynamic specific methodologies with high convergence, as it is more important to performance on dynamic problems than diversity.																	1567-7818	1572-9796															10.1007/s11047-020-09799-y		JUL 2020											
J								Fixed-Time Synchronization of Complex-Valued Memristor-Based Neural Networks with Impulsive Effects	NEURAL PROCESSING LETTERS										Fixed-time synchronization; Neural networks; Memristor-based; Impulsive; Complex-valued	EXPONENTIAL SYNCHRONIZATION; STABILITY; DELAYS	In this paper, the fixed-time synchronization of complex-valued memristor-based neural networks with impulsive effects is investigated. We first separate these complex-valued networks into real and imaginary parts, and design the appropriate controllers. Then apply the set-valued map and the differential inclusion theorem to handle the discontinuity problems at the right-hand side of the drive-response systems. By constructing the comparison systems together with the Lyapunov function, we get the fixed-time synchronization conditions. Moreover, the estimate of the settling time is also explicitly obtained. Finally, two examples and their numerical simulations are presented to show the effectiveness of the obtained theoretical results.																	1370-4621	1573-773X				OCT	2020	52	2			SI		1263	1290		10.1007/s11063-020-10304-w		JUL 2020											
J								A New Fixed-Time Stability Criterion and Its Application to Synchronization Control of Memristor-Based Fuzzy Inertial Neural Networks with Proportional Delay	NEURAL PROCESSING LETTERS										Fixed-time stability; Inertial neural networks; Memristor-based; Fuzzy	GLOBAL EXPONENTIAL STABILITY; NICHOLSONS BLOWFLIES MODEL; LIMIT-CYCLES; DYNAMICS; SYSTEMS; CONVERGENCE; PERIODICITY	In this paper, a new criterion related to fixed-time stability is derived by strict mathematical techniques such as definite integral and inequality techniques. Compared with the existing theorems, the estimate of upper bound for settling time is smaller, which is not only proved theoretically but also shown by numerical simulations. And the new criterion gets improved after introducing a new lemma. Then on the basis of the new criterion and the improved theorem, the fixed-time synchronization (FTS) of a memristor-based fuzzy inertial neural network (MFINN) with proportional delay is investigated via adopting a delay-dependent feedback controller, and several sufficient conditions are given for the FTS of the MFINN. At last, numerical simulations are raised to substaintiate the correctness of our theoretical results.																	1370-4621	1573-773X				OCT	2020	52	2			SI		1291	1315		10.1007/s11063-020-10305-9		JUL 2020											
J								The dilemma between arc and bounds consistency	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										bounds consistency; constraint satisfaction; MAC; propagation; search	CONSTRAINT; PROPAGATION	Consistency enforcement is used to prune the search tree of a constraint satisfaction problem (CSP). Arc consistency (AC) is a well-studied consistency level, with many implementations. Bounds consistency (BC), a looser consistency level, is known to have equal time complexity to AC. To solve a CSP, we have to implement an algorithm of our own or employ an existing solver. In any case, at some point, we have to decide between enforcing either AC or BC. As the choice between AC or BC is more or less predefined and currently made without considering the individualities of each CSP, this study attempts to make this decision deterministic and efficient, without the need of trial and error. We find that BC fits better while solving a CSP with its maximum domains' size being greater than its constrained variables number. We study the behavior ofmaintainingarc or bounds consistency during search, and we show how theoverallsearch methods complexity is affected by the employed consistency level.																	0884-8173	1098-111X				OCT	2020	35	10					1467	1491		10.1002/int.22259		JUL 2020											
J								Uncertainty measures for probabilistic hesitant fuzzy sets in multiple criteria decision making	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										entropy measure; multiple criteria decision making; probabilistic hesitant fuzzy set	ENTROPY MEASURES; CROSS-ENTROPY; WEIGHT	This contribution reviews critically the existing entropy measures for probabilistic hesitant fuzzy sets (PHFSs), and demonstrates that these entropy measures fail to effectively distinguish a variety of different PHFSs in some cases. In the sequel, we develop a new axiomatic framework of entropy measures for probabilistic hesitant fuzzy elements (PHFEs) by considering two facets of uncertainty associated with PHFEs which are known as fuzziness and nonspecificity. Respect to each kind of uncertainty, a number of formulae are derived to permit flexible selection of PHFE entropy measures. Moreover, based on the proposed PHFE entropy measures, we introduce some entropy-based distance measures which are used in the portion of comparative analysis. Eventually, the proposed PHFE entropy measures and PHFE entropy-based distance measures are applied to decision making in the strategy initiatives where their reliability and effectiveness are verified.																	0884-8173	1098-111X				NOV	2020	35	11					1646	1679		10.1002/int.22266		JUL 2020											
J								Efficiently mining erasable stream patterns for intelligent systems over uncertain data	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										data mining; data streams; erasable pattern mining; uncertain database	MAXIMAL FREQUENT PATTERNS; SEQUENTIAL PATTERNS; UTILITY; DATABASES; ITEMSETS; TIME	Data mining is a method for extracting useful information that is necessary for a system from a database. As the types of data processed by the system are diversified, the transformed pattern mining techniques for processing these type of data have been proposed. Unlike the traditional pattern mining methods, erasable pattern mining is a technique for finding the patterns that can be removed by coming with a small profit. Erasable pattern mining should be able to process data by considering both the environment that the data are generated from and the characteristics of the data. An uncertain database is a database that is composed of uncertain data. Since erasable patterns discovered from uncertain data contain significant information, these patterns need to be extracted. In addition, databases gradually increase, because the data from various fields is generated and accumulated over data streams. Data streams should be processed as intelligently as possible to provide the useful data to the system in real time. In this paper, we propose an efficient erasable pattern mining algorithm that processes uncertain data that is generated over data streams. The uncertain erasable patterns discovered through the suggested technique are more meaningful information by considering the probability of the item and the profit. Moreover, the proposed method can perform efficient mining operations by using both tree and list structures. The performance of the suggested algorithm is verified through the performance tests compared with state-of-the-art algorithms using real data sets and synthetic data sets.																	0884-8173	1098-111X				NOV	2020	35	11					1699	1734		10.1002/int.22269		JUL 2020											
J								A preference degree for ranking k-dimensional vectors of qualitative labels and its application in multi-attribute group decision-making	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Order-of-magnitude qualitative reasoning; preference degree; ranking method; multi-attribute group decision-making	REASONABLE PROPERTIES; FUZZY NUMBERS; CONSENSUS; SETS; CRITERIA; MODEL	In this paper, a ranking procedure is proposed for the order-of-magnitude qualitative reasoning problem. To this end, a preference degree was suggested for comparing a set of k-dimensional vectors of qualitative labels to provide a criterion to interpret the concept of 'preference' between two k-dimensional vectors of qualitative labels. Some useful properties of the proposed preference degree were also examined and discussed. It was shown that the proposed preference degree can satisfy the common and desired properties expected in the space of k-dimensional vectors of qualitative labels. Numerical examples were also employed to illustrate the proposed ranking method. Three application examples were used to show the applicability of the proposed ranking method in a single criterion group decision-making (SAGDM) and multiple criteria group decision-making (MAGDM). Moreover, the proposed preference degree was compared with a conventional method in terms of ranking the k-dimensional vector of qualitative labels. Unlike the proposed method, the numerical evaluations revealed that the previous techniques are not applicable to some situations in decision-making.																	0952-813X	1362-3079															10.1080/0952813X.2020.1794233		JUL 2020											
J								Frequency component vectorisation for image dehazing	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Image dofogging; image restoration; image de-noising; single image		Image captured in bad weather conditions confines scene prominence, appears grey and diminishes image contrast. This usually happens due to atmospheric dispersing phenomenon that affects the quality of outdoor computer vision frameworks. This deprivation relies on the gap between the object point and the camera and mostly differs for every pixel present in an image. Transmission coefficients, which state the aforementioned dependence are used to manage the haze level in each pixel. Our algorithm is subject to the presumption that the haze-free image forms clusters given in RGB space. The pixels over the image plane are often found at different locations and their distance from camera also differs. These fluctuating distances give rise to different transmission coefficients. Consequently, these colour clusters form the certain lines of colours in RGB space known as haze lines. The first step is to assure accurate estimation of the atmospheric light, for this an additional wavelet channel is recommended, based on frequency subdivision. The next step is to separate the average gradients present in the foggy regions of an image according to the frequency criteria. Lastly, the haze-free image information is retrieved by utilising the atmospheric scattering model on low and high frequencies according to the edges of unpredicted change in the field depth. Using the non-local and frequency information retrieval, proposed algorithm recovers the haze-free image, efficiently.																	0952-813X	1362-3079															10.1080/0952813X.2020.1794232		JUL 2020											
J								Structural coupling and the puzzle of surfaces: ontology of boundaries from the minimally cognitive perspective	ADAPTIVE BEHAVIOR										Boundary; surface; autonomy; structural coupling; cognition; internal-external	CONTINUITY; SYMBIOSIS; BACTERIA	Boundaries are prominent ingredients of reality, including-most importantly-the boundaries of organisms and the perceived boundaries of things (their surfaces). It is also customary to think of minds as kinds ofbounded locifor thoughts, representations, and other internal entities, targeting the borderline between theinternaldomain and theexternalworld as a genuine barrier. Therefore, not surprisingly, boundaries and surfaces have become targets of disciplined formal-ontological investigations. However, to have a boundary (or surface) is not only a purely geometrical or topological feature, for boundaries play certain roles. For that reason, this article unpacks boundaries in general, and surfaces in particular, in terms of interactions between structured entities, most importantly-living creatures capable of (minimal) cognition. It is argued that boundaries should be thought of as products of and at the same time as critical constraints imposed on structural coupling between complex beings, including the interactions between the (minimally) cognitive subjects and their surroundings.																	1059-7123	1741-2633														1059712320937475	10.1177/1059712320937475		JUL 2020											
J								Moral emotions when reading quotidian circumstances in contexts of violence: an fMRI study	ADAPTIVE BEHAVIOR										Moral emotions; compassion; Schadenfreude; Indignation; fMRI	NEURAL RESPONSES; DEFAULT MODE; BRAIN; COMPASSION; EMPATHY; COGNITION; CORTEX; SELF; ROBUST; ENVY	The increase of violence in Mexico and consequent suffering during the last decades is evident, but its effects over feelings and moral judgments remain uncertain. We used journalistic news showing real-life situations to investigate the effects of facing violence over the experience of four moral emotions which represent powerful impulses for social actions in situations of suffering linked to violence: Negative Compassion, Positive Compassion, Schadenfreude, and Indignation. We evaluate brain activation by using functional magnetic resonance imaging (fMRI) during three cognitive conditions: reading, introspection, and resting. When reading the news, only Indignation-evoking stimuli elicited salient brain activations in the posterior cerebellum, and temporal and parietal cortical regions, whose functions are related to anger experiences and processing of socially relevant circumstances. When introspecting the emotional experience, cerebellar, frontal, parietal, and occipital activations related to self-focused experiences were observed for all emotions. When resting after facing the stimuli, only the Negative Compassion emotion elicited brain activations in the posterior cingulate cortex and precuneus related to emotional self-reference processing; thus, negative compassion may produce more perdurable cognitive-affective effects related to sadness while perceiving suffering in others. Our results may suggest different emotional-based social decisions to face suffering and violence and to motivate pro-social actions in the collectivistic Mexican culture.																	1059-7123	1741-2633														1059712320939346	10.1177/1059712320939346		JUL 2020											
J								Mutual conditional independence and its applications to model selection in Markov networks	ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE										Markov networks; Mutual conditional independence; Graphical models; Graphical log-linear models; Forward model selection		The fundamental concepts underlying Markov networks are the conditional independence and the set of rules called Markov properties that translate conditional independence constraints into graphs. We introduce the concept of mutual conditional independence in an independent set of a Markov network, and we prove its equivalence to the Markov properties under certain regularity conditions. This extends the notion of similarity between separation in graph and conditional independence in probability to similarity between the mutual separation in graph and the mutual conditional independence in probability. Model selection in graphical models remains a challenging task due to the large search space. We show that mutual conditional independence property can be exploited to reduce the search space. We present a new forward model selection algorithm for graphical log-linear models using mutual conditional independence. We illustrate our algorithm with a real data set example. We show that for sparse models the size of the search space can be reduced from O(n(3)) to O(n(2)) using our proposed forward selection method rather than the classical forward selection method. We also envision that this property can be leveraged for model selection and inference in different types of graphical models.																	1012-2443	1573-7470				SEP	2020	88	9					951	972		10.1007/s10472-020-09690-7		JUL 2020											
J								Generative image inpainting for link prediction	APPLIED INTELLIGENCE										Link prediction; Image inpainting; Generative adversarial networks	ALGORITHM	Link prediction is a fundamental task that predicts whether a link exists between two nodes based on the currently observed network. Existing approaches such as heuristic-based algorithms assume that two nodes are likely to have a link in a network. In fact, they limit algorithm effectiveness when the assumptions are not correct. Moreover, these link prediction algorithms lack generalization ability, which indicates that they have different effects on different types of networks. For example, the common neighbours algorithm works well on social networks, but it shows poor performance on electric power networks. Inspired by the image inpainting technology of generative adversarial networks and the adjacency matrix representation of networks, we propose a new framework for link prediction based on the image inpainting method, named the Generative Image Inpainting for Link Prediction algorithm (GIILP), to address these problems. The key idea of the GIILP is that the network can be converted into an image (the image is a form of the adjacency matrix (two dimensions)). Pixel values represent the likelihood of two nodes. Thus, the problem of predicting a possible link between nodes is converted into filling missing pixels in an image. The link information of the network can be expressed by the image information, which means that our algorithm does not need assumptions such as heuristics, and works regardless of the dataset type. The experimental results on multiple link prediction public datasets demonstrate that our algorithm has an advantage over other algorithms, including heuristic-based and deep learning methods.																	0924-669X	1573-7497															10.1007/s10489-020-01648-w		JUL 2020											
J								Multi-stream neural network fused with local information and global information for HOI detection	APPLIED INTELLIGENCE										Human-object interactions; Global contextual information; Local region information; Information fusion		Human-Object Interaction (HOI) Detection is a new genre of human-centric visual relationship detection task, which is significant to deep understanding of visual scenes. Due to the complexity of the visual scene in the image, HOI detection is still a challenging task, the most critical part of which is feature extraction and representation. Some existing approaches rely solely on local region information for HOI detection without using global contextual information, but global contextual information contributes to this task in some HOI categories. Other approaches incorporate global contextual information for HOI detection while losing local region information. In this work, we propose a multi-stream neural network architecture composed of three special module that employs both local region information and global contextual information for HOI detection. This model can detect not only the HOI categories based on local region information but also on global contextual information. Our model more fully considers all HOI categories in the dataset. Compared with other existing approaches, the proposed model shows improved performance on V-COCO and HICO-DET benchmark datasets, especially when predicting rare HOI categories.																	0924-669X	1573-7497															10.1007/s10489-020-01794-1		JUL 2020											
J								High precision control and deep learning-based corn stand counting algorithms for agricultural robot	AUTONOMOUS ROBOTS										Machine learning; Deep learning; High precision control; Corn stand counting; Field robot; Agricultural robotics	MODEL-PREDICTIVE CONTROL	This paper presents high precision control and deep learning-based corn stand counting algorithms for a low-cost, ultra-compact 3D printed and autonomous field robot for agricultural operations. Currently, plant traits, such as emergence rate, biomass, vigor, and stand counting, are measured manually. This is highly labor-intensive and prone to errors. The robot, termedTerraSentia, is designed to automate the measurement of plant traits for efficient phenotyping as an alternative to manual measurements. In this paper, we formulate a Nonlinear Moving Horizon Estimator that identifies key terrain parameters using onboard robot sensors and a learning-based Nonlinear Model Predictive Control that ensures high precision path tracking in the presence of unknown wheel-terrain interaction. Moreover, we develop a machine vision algorithm designed to enable an ultra-compact ground robot to count corn stands by driving through the fields autonomously. The algorithm leverages a deep network to detect corn plants in images, and a visual tracking model to re-identify detected objects at different time steps. We collected data from 53 corn plots in various fields for corn plants around 14 days after emergence (stage V3 - V4). The robot predictions have agreed well with the ground truth with C-robot = 1.02 x C-human - 0.86 and a correlation coefficient R = 0.96. The mean relative error given by the algorithm is -3.78%, and the standard deviation is 6.76%. These results indicate a first and significant step towards autonomous robot-based real-time phenotyping using low-cost, ultra-compact ground robots for corn and potentially other crops.																	0929-5593	1573-7527				SEP	2020	44	7			SI		1289	1302		10.1007/s10514-020-09915-y		JUL 2020											
J								A fuzzy mid-term capacity and production planning model for a manufacturing system with cloud-based capacity	COMPLEX & INTELLIGENT SYSTEMS										Cloud manufacturing; Capacity planning; Production planning; Fuzzy mixed-integer nonlinear programming	ALGORITHM; INDUSTRY	Most of the past cloud manufacturing (CMfg) studies investigated the short-term production planning or job scheduling of a CMfg system, while the mid-term or long-term capacity and production planning of a CMfg system has rarely been addressed. In addition, most existing methods are suitable for CMfg systems comprising three-dimensional (3D) printers, computer numerical control (CNC) machines or robots, but ignore the coordination and transportation required for moving jobs across factories. To fill these gaps, a fuzzy mid-term capacity and production planning model for a manufacturer with cloud-based capacity is proposed in this study. The proposed methodology guides a manufacturer in choosing between non-cloud-based capacity and cloud-based capacity. It can be applied to factories utilizing machines with different degrees of automation including highly automatic equipment (such as 3D printers, CNC machines, and robots) and lowly automatic (legacy) machines, while existing methods assume that orders can be easily transferred between machines that are often highly automatic. In the proposed methodology, first, various types of capacity are unequally prioritized. Then, a fuzzy mixed-integer nonlinear programming model is formulated and optimized to make the mid-term or long-term capacity and production plan of a factory. The fuzzy capacity and production planning model is designed for factories with parallel machines. The proposed methodology has been applied to a case to illustrate its applicability. According to the experimental results, the proposed methodology successfully reduced total costs by up to 8%. The advantage of the proposed methodology over existing practices in fulfilling customers' demand was also obvious.																	2199-4536	2198-6053															10.1007/s40747-020-00177-w		JUL 2020											
J								Deep soccer analytics: learning an action-value function for evaluating soccer players	DATA MINING AND KNOWLEDGE DISCOVERY										Deep reinforcement learning; Action-value Q-function; Goal impact metric; Fine-tuning; Player ranking	PERFORMANCE	Given the large pitch, numerous players, limited player turnovers, and sparse scoring, soccer is arguably the most challenging to analyze of all the major team sports. In this work, we develop a new approach to evaluating all types of soccer actions from play-by-play event data. Our approach utilizes a Deep Reinforcement Learning (DRL) model to learn an action-value Q-function. To our knowledge, this is the first action-value function based on DRL methods for a comprehensive set of soccer actions. Our neural architecture fits continuous game context signals and sequential features within a play with two stacked LSTM towers, one for the home team and one for the away team separately. To validate the model performance, we illustrate both temporal and spatial projections of the learned Q-function, and conduct a calibration experiment to study the data fit under different game contexts. Our novel soccer Goal Impact Metric (GIM) applies values from the learned Q-function, to measure a player's overall performance by the aggregate impact values of his actions over all the games in a season. To interpret the impact values, a mimic regression tree is built to find the game features that influence the values most. As an application of our GIM metric, we conduct a case study to rank players in the English Football League Championship. Empirical evaluation indicates GIM is a temporally stable metric, and its correlations with standard measures of soccer success are higher than that computed with other state-of-the-art soccer metrics.																	1384-5810	1573-756X				SEP	2020	34	5			SI		1531	1559		10.1007/s10618-020-00705-9		JUL 2020											
J								Adaptive block searching and fractional rate-distortion trade-off based motion estimation for HEVC encoder	EVOLUTIONARY INTELLIGENCE										Rate-distortion; Block search; Motion estimation; HEVC encoder; Fractional calculus	OPTIMIZATION	In H.265/high efficiency video coding encoding, the cost function is a significant feature that decides the mode and structure of the encoding and the effectiveness of the methods relies on the coding efficiency, minimal rate-distortion optimization, and encoding time. The issues associated with the aforementioned factors are tackled effectively by proposing a new block search, an Adaptive block searching algorithm (AdBS) along with the fractional rate-distortion trade-off (fractional RD trade-off). The motion estimation using the proposed Adaptive block searching algorithm uses the heptagon block search patterns with eight checkpoints in such a way to assure the fast processing, contributing towards the minimal encoding process. The coding efficiency is enhanced twice with the assured reconstruction quality and bandwidth. The analysis is performed with single and multiple videos obtained from the CIPR SIF Sequences database and the analysis in terms of Structural Similarity Index (SSIM), Peak signal-to-noise ratio (PSNR), and average time reveals the effectiveness of the proposed AdBS and fractional RD-trade-off- based motion estimation. The PSNR, SSIM, and average time of the proposed method are found to be 85.6548%, 45 dB, and 565 nano-secs for single videos and 88.9841%, 45 dB, and 2584 nano-secs for the multiple videos.																	1864-5909	1864-5917															10.1007/s12065-020-00442-4		JUL 2020											
J								Stability analysis for uncertain differential equation by Lyapunov's second method	FUZZY OPTIMIZATION AND DECISION MAKING										Uncertain differential equation; Stability in measure; Stochastic differential equation; Asymptotic stability; Lyapunov's second method	THEOREMS	Uncertain differential equation is a type of differential equation driven by Liu process that is the counterpart of Wiener process in the framework of uncertainty theory. The stability theory is of particular interest among the properties of the solutions to uncertain differential equations. In this paper, we introduce the Lyapunov's second method to study stability in measure and asymptotic stability of uncertain differential equation. Different from the existing results, we present two sufficient conditions in sense of Lyapunov stability, where the strong Lipschitz condition of the drift is no longer indispensable. Finally, illustrative examples are examined to certify the effectiveness of our theoretical findings.																	1568-4539	1573-2908															10.1007/s10700-020-09336-7		JUL 2020											
J								A novel privacy preserving digital forensic readiness provable data possession technique for health care data in cloud	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Accurate; Attributable; Authentication; Authorization; Availability; Cloud computing; Confidential; Consistent; Data dynamics; Data privacy; Index hash table; Integrity; Legible; Non-repudiation; Privacy preserving; Private cloud; Provable data possession; Public cloud; Storage correctness; Secure	IDENTIFICATION	The scorching challenge calls for an importunate solution in the form of a secure, privacy preserved, economical cloud storage that can weather the turbulence of the rapidly evolving digital storage technologies. This calls for an avid solution, in the form of a public auditing protocol which stands as a testament to establish connection even with an unfamiliar person who must be trusted for sharing of health care data stored in a hermit cloud. The solution should be a technique which aids an external auditor to audit user's outsourced data in the cloud deprived of the wisdom on the data content. The contributions in this paper are (1) A comprehensive analysis of the contemporary privacy preserving PDP data integrity schemes, (2) a novel technique, which, contemporaneously supports the vital functions like Unbounded number of audits, Public auditing, Blockless verification, Stateless verification, data integrity, confidentiality. (3) A framework to direct the data to be safely stored, (4) embed digital forensic readiness into the technique in order to be digitally forensic ready, monitor, plan and formulate proactively before the occurrence of any potential security incidents, (5) implementation of the enhanced PDP technique to validate the auditing performance by existing experimental comparisons with the state of the art techniques. The results validate that the novel integrity technique can competently accomplish secure auditing and outclass the erstwhile ones. Last but not the least, this paper also gives a bird's eye view on the recent research status, and future directions of privacy preserving data integrity.																	1868-5137	1868-5145															10.1007/s12652-020-01931-1		JUL 2020											
J								Scalable and secure access control policy for healthcare system using blockchain and enhanced Bell-LaPadula model	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Blockchain; Enhanced Bell-LaPadula model (EBLP); Access control policy; Smart-contract; Distributed; Privacy; Security	FRAMEWORK	Access control is a policy in data security that controls access to resources. The current access control mechanisms are facing many problems, due to the interference of the third-party, privacy, and security of data. These problems can be addressed by blockchain, the technology that gained major attention in recent years and has many capabilities. However, in the blockchain network, every peer maintains the same state of the ledger to view the complete history of transactions that leads to scalability issues in the blockchain network. To address the problem of scalability we propose an enhanced Bell-LaPadula model and categorized the peers and transactions in different clearance and security levels. The peers don't have to maintain the complete history of transactions owing to the clearance level. To provide data security in the network we constructed a dynamic access control policies using a smart contracts. We test our model on a blockchain-based healthcare network. The Hyperledger Fabric tool is used to run a complete infrastructure of healthcare organization while the Hyperledger composer modeling tool is used to implement the smart contracts and to provide dynamic access control functionality on the blockchain network.																	1868-5137	1868-5145															10.1007/s12652-020-02346-8		JUL 2020											
J								Research on remote control algorithm for parallel implicit domain robot patrol inspection on 3D unstructured grid	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Three-dimensional unstructured grid; Dynamic partition; Parallel algorithm; DSMC; Remote control	MODEL	Aiming at the disadvantages of structural meshes that are difficult to deal with complex shapes and unstructured meshes, it is impossible to calculate the viscous flow with boundary layers. A parallel implicit algorithm based on three-dimensional unstructured meshes is developed. The turbulence model adopts the k - pi turbulence model. Due to the difficulty of the LU-SGS parallel equivalent model in time advancement, a three-dimensional unstructured grid DSMC parallel algorithm is proposed. Under the unstructured grid dynamic partitioning strategy, each sub-area is guaranteed. The number of molecules is roughly equal. Based on the implicit algorithm of DP-LUR (data-parallel lower-upper relaxation) format, the remote control combination model of robot inspection is calculated, and the mutual interference flow problem during the inspection process is accurately simulated. The operator realizes the remote control of the inspection robot through the live image transmitted by the network. The actual operation shows that the system can remotely access the service system and message transmission of the inspection robot in the state of the sub-structure grid, and successfully solve the robot tour. Check the complex shape viscous flow field calculation and efficiency problems that are difficult to handle in remote control systems.																	1868-5137	1868-5145															10.1007/s12652-020-02318-y		JUL 2020											
J								Chain Reduction for Binary and Zero-Suppressed Decision Diagrams	JOURNAL OF AUTOMATED REASONING										Binary decision diagrams; Zero-suppressed binary decision diagrams; Boolean functions		Chain reduction enables reduced ordered binary decision diagrams (BDDs) and zero-suppressed binary decision diagrams (ZDDs) to each take advantage of the other's ability to symbolically represent Boolean functions in compact form. For any Boolean function, its chain-reduced ZDD (CZDD) representation will be no larger than its ZDD representation, and at most twice the size of its BDD representation. The chain-reduced BDD (CBDD) of a function will be no larger than its BDD representation, and at most three times the size of its CZDD representation. Extensions to the standard algorithms for operating on BDDs and ZDDs enable them to operate on the chain-reduced versions. Experimental evaluations on representative benchmarks for encoding word lists, solving combinatorial problems, and operating on digital circuits indicate that chain reduction can provide significant benefits in terms of both memory and execution time. The experimental results are further validated by a quantitative model of how decision diagrams scale when encoding sets of sequences. This model explains why the combination of a one-hot encoding of the symbols in the sequences, plus a CBDD, CZDD, or ZDD representation of the set, yields the most compact form.																	0168-7433	1573-0670				OCT	2020	64	7			SI		1361	1391		10.1007/s10817-020-09569-6		JUL 2020											
J								Guidelines for the design of a virtual patient for psychiatric interview training	JOURNAL ON MULTIMODAL USER INTERFACES										Embodied conversational agent; Virtual patient; Design guidelines; Medical training; User experience; Psychiatric interview	MEDICAL-STUDENTS; EMPATHY; EDUCATION	A psychiatric diagnosis involves the physician's ability to create an empathic interaction with the patient in order to accurately extract symptomatology (i.e., clinical manifestations). Virtual patients (VPs) can be used to train these skills but need to propose a structured and multimodal interaction situation, in order to simulate a realistic psychiatric interview. In this study we present a simulated psychiatric interview with a virtual patient suffering from major depressive disorders. We suggested some design guidelines based on psychiatry theories and medicine education standards. We evaluated our VP with user testing with 35 4th year medical students, and probed their opinion during debriefing interviews. All students showed good abilities to communicate empathetically with the VP, and managed to extract symptomatology from VP's simulation. Students provided positive feedbacks regarding pedagogic usefulness, realism and enjoyment in the interaction, which suggests that our design guidelines are consistent and that such technologies are acceptable to medical students. To conclude this study is the first to simulate a realistic psychiatric interview and to measure both skills needed by future psychiatrists: symptomatology extraction and empathic communication. Results provide evidence for the use of VPs to complement existing tools and to train and evaluate healthcare professionals in the future.																	1783-7677	1783-8738															10.1007/s12193-020-00338-8		JUL 2020											
J								3D video semantic segmentation for wildfire smoke	MACHINE VISION AND APPLICATIONS										Wildfire smoke detection; Semantic segmentation; Natural scene; Real-time	CONVOLUTIONAL NEURAL-NETWORK; FIRE	Wildfires are a serious threat to ecosystems and human life. Usually, smoke is generated before the flame, and due to the diffusing nature of the smoke, we can detect smoke from a distance, so wildfire smoke detection is especially important for early warning systems. In this paper, we propose a 3D convolution-based encoder-decoder network architecture for video semantic segmentation in wildfire smoke scenes. In the encoder stage, we use 3D residual blocks to extract the spatiotemporal features of smoke. The downsampling feature from the encoder is upsampled by the decoder three times in succession. Then, three smoke map prediction modules are, respectively, passed, the output smoke prediction map is supervised by the binary image label, and finally, the final prediction is obtained by feature map fusion. Our model can achieve end-to-end training without pretraining from scratch. In addition, a dataset including 90 smoke videos is tested and trained in this paper. The experimental results of the smoke video show that our model quickly and accurately segmented the smoke area and produced few false positives.																	0932-8092	1432-1769				JUL 21	2020	31	6							50	10.1007/s00138-020-01099-w													
J								Modeling monthly streamflow in mountainous basin by MARS, GMDH-NN and DENFIS using hydroclimatic data	NEURAL COMPUTING & APPLICATIONS										Streamflow prediction; Mountainous basin; Group method of data handling-neural networks; Dynamic evolving neural-fuzzy inference system; Multivariate adaptive regression spline	ADAPTIVE REGRESSION SPLINE; SUPPORT VECTOR REGRESSION; FLOW; RIVER; MACHINE; GENERATION; RUNOFF; SCALES; ANN	Accurate estimation of streamflow has a vital importance in water resources engineering, management and planning. In the present study, the abilities of group method of data handling-neural networks (GMDH-NN), dynamic evolving neural-fuzzy inference system (DENFIS) and multivariate adaptive regression spline (MARS) methods are investigated for monthly streamflow prediction. Precipitation, temperature and streamflows from Kalam and Chakdara stations at Swat River basin (mountainous basin), Pakistan, are used as inputs to the applied models in the form of different input scenarios, and models' performances are evaluated on the basis of root mean square error (RMSE), mean absolute error (MAE), Nash-Sutcliffe efficiency (NSE) and combined accuracy (CA) indexes. Test results of the Kalam Station show that the DENFIS model provides more accurate prediction results in comparison of GMDH-NN and MARS models with the lowest RMSE (18.9 m(3)/s), MAE (13.1 m(3)/s), CA (10.6 m(3)/s) and the highest NSE (0.941). For the Chakdara Station, the MARS outperforms the GMDH-NN and DENFIS models with the lowest RMSE (47.5 m(3)/s), MAE (31.6 m(3)/s), CA (26.1 m(3)/s) and the highest NSE (0.905). Periodicity (month number of the year) effect on models' accuracies in predicting monthly streamflow is also examined. Obtained results demonstrate that the periodicity improves the models' accuracies in general but not necessarily in every case. In addition, the results also show that the monthly streamflow could be successfully predicted using only precipitation and temperature variables as inputs.																	0941-0643	1433-3058															10.1007/s00521-020-05164-3		JUL 2020											
J								Bag of feature and support vector machine based early diagnosis of skin cancer	NEURAL COMPUTING & APPLICATIONS										Skin cancer; Computer-aided detection and diagnosis; Bag of feature; Support vector machine; Classification; SURF		Skin cancer is one of the diseases which lead to death if not detected at an early stage. Computer-aided detection and diagnosis systems are designed for its early diagnosis which may prevent biopsy and use of dermoscopic tools. Numerous researches have considered this problem and achieved good results. In automatic diagnosis of skin cancer through computer-aided system, feature extraction and reduction plays an important role. The purpose of this research is to develop computer-aided detection and diagnosis systems for classifying a lesion into cancer or non-cancer owing to the usage of precise feature extraction technique. This paper proposed the fusion of bag-of-feature method with speeded up robust features for feature extraction and quadratic support vector machine for classification. The proposed method shows the accuracy of 85.7%, sensitivity of 100%, specificity of 60% and training time of 0.8507 s in classifying the lesion. The result and analysis of experiments are done on the PH(2)dataset of skin cancer. Our method improves performance accuracy with an increase of 3% than other state-of-the-art methods.																	0941-0643	1433-3058															10.1007/s00521-020-05212-y		JUL 2020											
J								Anomaly detection via blockchained deep learning smart contracts in industry 4.0	NEURAL COMPUTING & APPLICATIONS										Industry 4; 0; Industrial IoT; Blockchain; Smart contracts; Anomaly detection; Advanced persistent threat		The complexity of threats in the ever-changing environment of modern industry is constantly increasing. At the same time, traditional security systems fail to detect serious threats of increasing depth and duration. Therefore, alternative, intelligent solutions should be used to detect anomalies in the operating parameters of the infrastructures concerned, while ensuring the anonymity and confidentiality of industrial information.Blockchainis an encrypted, distributed archiving system designed to allow for the creation of real-time log files that are unequivocally linked. This ensures the security and transparency of transactions. This research presents, for the first time in the literature, an innovativeBlockchain Security Architecturethat aims to ensure network communication between traded Industrial Internet of Things devices, following the Industry 4.0 standard and based onDeep Learning Smart Contracts. The proposed smart contracts are implementing (via computer programming) a bilateral traffic control agreement to detect anomalies based on a trained Deep Autoencoder Neural Network. This architecture enables the creation of a secure distributed platform that can control and complete associated transactions in critical infrastructure networks, without the intervention of a single central authority. It is a novel approach that fuses artificial intelligence in the Blockchain, not as a supportive framework that enhances the capabilities of the network, but as an active structural element, indispensable and necessary for its completion.																	0941-0643	1433-3058															10.1007/s00521-020-05189-8		JUL 2020											
J								Temporal representation learning for time series classification	NEURAL COMPUTING & APPLICATIONS										Machine learning; Recurrent neural network; Deep representation learning; Turning points evaluation; Time series classification		Recent years have witnessed the exponential growth of time series data as the popularity of sensing devices and development of IoT techniques; time series classification has been considered as one of the most challenging studies in time series data mining, attracting great interest over the last two decades. According to the empirical evidences, temporal representation learning-based time series classification has more superiority of accuracy, efficiency and interpretability as compared to hundreds of existing time series classification methods. However, due to the high time complexity of feature process, the performance of these methods has been severely restricted. In this paper, we first presented an efficient shapelet transformation method to improve the overall efficiency of time series classification, and then, we further developed a novel enhanced recurrent neural network model for deep representation learning to further improve the classification accuracy. Experimental results on typical real-world datasets have justified the superiority of our models over several shallow and deep representation learning competitors.																	0941-0643	1433-3058															10.1007/s00521-020-05179-w		JUL 2020											
J								Evaluation of computationally intelligent techniques for breast cancer diagnosis	NEURAL COMPUTING & APPLICATIONS										Machine learning; Classifiers; Breast cancer disease (BCD); Supervised machine learning	NEURAL-NETWORK; LOGISTIC-REGRESSION; CLASSIFICATION RULES; DECISION TREES; PREDICTION; MODEL; IDENTIFICATION	Nowadays, breast cancer is a worldwide prevalent disease mostly in females. Consequently, the breast cancer patients are growing rapidly day by day. Therefore, it is quite essential to have some early detection systems which may help patients to know this disease at an early stage. As a result, they can start their medication to curb this fatal disease. In the era of machine learning, various prediction methods have been developed for early diagnosis of this disease. These algorithms use different computational classifiers and also claim good results in some aspects. But, so far, no proper analysis has been done to clarify which computationally intelligent technique is better to detect breast cancer. Therefore, it is required to find the best among the available methods. In this work, the contribution has been made toward the performance evaluation of seven different classification techniques over breast cancer disease datasets. In addition to this, the proper reasons for the superiority of the classifiers have also been explored.																	0941-0643	1433-3058															10.1007/s00521-020-05204-y		JUL 2020											
J								Neural network-based damage identification in composite laminated plates using frequency shifts	NEURAL COMPUTING & APPLICATIONS										Damage identification; Artificial neural network; Composite material; Vibrations	CFRP PLATES	Delamination is the principal mode of failure of laminated composites. It is caused by the rupture of the fiber-matrix interface and results in the separation of the layers. This failure is induced by interlaminate tension and shear that is developed due to a variety of factors such as fatigue. Composites are known for excellent structural performance, which can be significantly affected by delamination. Such damages are not always visible on the surface and can lead to sudden catastrophic failures. To ensure a structural performance and integrity, accurate methods to monitor damages are required. This work presents a methodology for damage detection and identification on laminated composite plates using artificial neural networks fed with modal data obtained by finite element analysis. The proposed neural network to quantify damage severity achieved up to 95% success rate. A comparative study was done to evaluate the effect of boundary conditions on damage location. With the comparative study results, another neural network was proposed to locate damage position, achieving excellent results by successfully locating or by significantly reducing the search area. Both proposed ANNs use only frequency variation values as inputs, an easily obtainable quantity that requires few equipment to be acquired. The obtained results from these numerical examples indicate that the proposed approach can detect true damage locations and estimate damage magnitudes with satisfactory accuracy for this particular geometry, even under high measurement noise.																	0941-0643	1433-3058															10.1007/s00521-020-05180-3		JUL 2020											
J								Exploring multiple spatio-temporal information for point-of-interest recommendation	SOFT COMPUTING										Point-of-interest recommendation; Deep learning; Kernel density estimation; User preferences; Spatio-temporal information		Traditional collaborative filtering methods perform poorly in providing location recommendations due to the high sparsity of users' check-in data, prompting the development of new location recommendation approaches that can integrate situational factors such as time and location. Using long short-term memory (LSTM) neural networks and kernel density estimation (KDE), this paper integrates the impact of point-of-interest (POI) location and category on users' check-in behavior according to check-in sequence data. First, LSTM neural networks are used to model users' periodic and repetitive daily activities for a sequence-based prediction of the probability of whether the user will visit a candidate POI. Second, the user's geographical preference in the two-dimensional space is represented by KDE and used to make a location-based check-in probability prediction. Next, the user's category preference is used to predict the check-in probability of a candidate POI. Finally, a user preference model is constructed from three perspectives of time, location, and category, and the comprehensive check-in probability is used for Top-N recommendation. The validation experiments on Foursquare dataset verifies that, in terms of recommendation precision and recall, the proposed recommendation method is superior to both the basic LSTM approach and the method that uses only location information. In addition, it is experimentally confirmed that the geographical preference, which is reflected by "clustering" of a user's check-in locations, is stable, but the user's category preference is prone to drift.																	1432-7643	1433-7479															10.1007/s00500-020-05107-z		JUL 2020											
J								A hybrid tree-based algorithm to solve asymmetric distributed constraint optimization problems	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS										DCOP; ADCOP; Complete ADCOP algorithm; Search; Inference	PRIVACY; SEARCH; ADOPT; BREAKOUT	Asymmetric distributed constraint optimization problems (ADCOPs) have emerged as an important formalism in multi-agent community due to their ability to capture personal preferences. However, the existing search-based complete algorithms for ADCOPs only exploit local knowledge to calculate lower bounds, which leads to inefficient pruning and prohibits them from solving large scale problems. On the other hand, inference-based complete algorithms (e.g., DPOP) for distributed constraint optimization problems are able to aggregate the global cost promptly but cannot be directly applied into ADCOPs due to a privacy concern. Thus, in this paper, we investigate the possibility of combining inference and search to effectively solve ADCOPs at an acceptable loss of privacy. Specifically, we propose a hybrid complete ADCOP algorithm called PT-ISABB which uses a tailored inference algorithm to provide tight lower bounds and upper bounds, and a tree-based complete search algorithm to guarantee the optimality. Furthermore, we introduce two suboptimal variants of PT-ISABB based on bounded-error approximation mechanisms to enable trade-off between theoretically guaranteed solutions and coordination overheads. We prove the correctness of PT-ISABB and its suboptimal variants. Finally, the experimental results demonstrate that PT-ISABB exhibits great superiorities over other state-of-the-art search-based complete algorithms and its suboptimal variants can quickly find a solution within the user-specified bounded-error.																	1387-2532	1573-7454				JUL 21	2020	34	2							50	10.1007/s10458-020-09476-5													
J								A robust similarity based deep siamese convolutional neural network for gait recognition across views	COMPUTATIONAL INTELLIGENCE										contour; convolutional neural network; covariate factors; deep learning; gait recognition; siamese; view change	IMAGE-ANALYSIS; FLOW; REPRESENTATION; PERFORMANCE	Gait recognition has been considered as the emerging biometric technology for identifying the walking behaviors of humans. The major challenges addressed in this article is significant variation caused by covariate factors such as clothing, carrying conditions and view angle variations will undesirably affect the recognition performance of gait. In recent years, deep learning technique has produced a phenomenal performance accuracy on various challenging problems based on classification. Due to an enormous amount of data in the real world, convolutional neural network will approximate complex nonlinear functions in models to develop a generalized deep convolutional neural network (DCNN) architecture for gait recognition. DCNN can handle relatively large multiview datasets with or without using any data augmentation and fine-tuning techniques. This article proposes a color-mapped contour gait image as gait feature for addressing the variations caused by the cofactors and gait recognition across views. We have also compared the various edge detection algorithms for gait template generation and chosen the best from among them. The databases considered for our work includes the most widely used CASIA-B dataset and OULP database. Our experiments show significant improvement in the gait recognition for fixed-view, crossview, and multiview compared with the recent methodologies.																	0824-7935	1467-8640				AUG	2020	36	3					1290	1319		10.1111/coin.12361		JUL 2020											
J								DRGAN: a deep residual generative adversarial network for PET image reconstruction	IET IMAGE PROCESSING										computer vision; image reconstruction; image representation; positron emission tomography; medical image processing; image resolution; neural nets; PET image reconstruction; positron emission tomography image reconstruction; low-count projection data; physical effects; inverse problem; computer vision tasks; medical imaging; DRGAN; PET image quality; residual PET map; RPM; image representation; anatomically realistic PET images; residual dense connections; simulation data; clinical PET data; deep residual generative adversarial network; streaking artefact reduction; pixel shuffle operations	DETECTORS; ALGORITHM	Positron emission tomography (PET) image reconstruction from low-count projection data and physical effects is challenging because the inverse problem is ill-posed and the resultant image is usually noisy. Recently, generative adversarial networks (GANs) have also shown their superior performance in many computer vision tasks and attracted growing interests in medical imaging. In this work, the authors proposed a novel model [deep residual generative adversarial network (DRGAN)] based on GANs for the reduction of streaking artefacts and the improvement of PET image quality. An innovative feature of the proposed method is that the authors trained a generator to produce 'residual PET map' (RPM) for image representation, rather than generate PET images directly. DRGAN used two discriminators (critics) to enforce anatomically realistic PET images and RPM. To better boost the contextual information, the authors designed residual dense connections followed with pixel shuffle operations (RDPS blocks) that encourage feature reuse and prevent losing resolution. Both simulation data and real clinical PET data are used to evaluate the proposed method. Compared with other state-of-the-art methods, the quantification results show that DRGAN can achieve better performance in bias-variance trade-off and provide comparable image quality. Their results were rigorously evaluated by one radiologist at the Shanxi Cancer Hospital.																	1751-9659	1751-9667				JUL 20	2020	14	9					1690	1700		10.1049/iet-ipr.2019.1107													
J								Collaborative model tracking with robust occlusion handling	IET IMAGE PROCESSING										image colour analysis; target tracking; feature extraction; object tracking; filtering theory; object detection; image motion analysis; classifier-box; OTB2015 dataset; collaborative model; robust occlusion handling; discriminative correlation filter-based trackers; higher tracking accuracy; visual tracking; heavy occlusion; scale variation; complex colour features; correlation filter tracker; multiscale method; scale problem; tracking process; locally weighted distance; OTB2013 dataset	VISUAL TRACKING; OBJECT TRACKING; FILTER	Currently, the discriminative correlation filter-based trackers have achieved higher tracking accuracy. However, visual tracking still faces challenges in terms of heavy occlusion, scale variation and so on. In this study, the authors intend to solve heavy occlusion by introducing collaborative model into classifier-box. Firstly, they introduce complex colour features into correlation filter tracker to improve the effect of the tracker. Secondly, they introduce a multi-scale method into their tracker to ease the scale problem. Thirdly, in order to solve the heavy occlusion in the tracking process, they adopt the locally weighted distance and classifier-box. Their algorithm achieves distance precision rates of 81.7 and 77.4% on OTB2013 dataset and OTB2015 dataset, respectively. Their contribution focuses on solving heavy occlusion by using colour features, locally weighted distance and classifier-box. The experimental results on OTB2013 and OTB2015 datasets demonstrate their algorithm to perform better than state-of-the-art methods.																	1751-9659	1751-9667				JUL 20	2020	14	9					1701	1709		10.1049/iet-ipr.2019.0827													
J								Fingerprint liveness detection based on guided filtering and hybrid image analysis	IET IMAGE PROCESSING										image classification; image denoising; radial basis function networks; support vector machines; fingerprint identification; feature extraction; stochastic processes; image filtering; fingerprint liveness detection; guided filtering; hybrid image analysis; artificially made fingerprint; denoised image; enhanced sharp features; guidance image; adjacent local binary pattern features; cropped images; input fingerprint preprocessing; biometric recognition; spoofing attacks; region of interest extraction; histogram equalisation; illumination condition; t-distributed stochastic neighbour embedding; data dimension reduction; two-class classification problem; support vector machine; radial basis function kernel	FEATURES	Fingerprints are widely used for biometric recognition. However, many spoofing attacks based on an artificially made fingerprint occur. In this study, the authors propose an approach to detect fingerprint liveness which uses the guided filtering and hybrid image analysis. This study deals with the problem of ignoring the contribution that is brought by the sharp features when analysing the denoised image. The method described utilises both the enhanced sharp features and denoised features from the hybrid images to get better results. The input fingerprint is pre-processed by region of interest extraction and then is filtered by a guidance image for obtaining the denoised image. Then, histogram equalisation is introduced to eliminate the impact of illumination condition. The authors extract the co-occurrence of adjacent local binary pattern features from both the cropped images and the denoised images. Whilst concatenating both the features together to form a long feature, t-Distributed Stochastic Neighbour Embedding is applied to reduce the data dimension. The authors consider the fingerprint liveness detection as a two-class classification problem and use support vector machine with radial basis function kernel to solve this problem. The authors evaluate the experiments on three benchmark data sets. Experimental results demonstrate that the accuracy of the proposed method can outperform most of the state-of-art methods.																	1751-9659	1751-9667				JUL 20	2020	14	9					1710	1715		10.1049/iet-ipr.2018.5915													
J								Novel distortion free and histogram based data hiding scheme	IET IMAGE PROCESSING										image coding; data encapsulation; data hiding methods; cover media; secretive data; medical imaging; military imaging; law enforcement; cover image; embedding schemes; wide investigated reversible data hiding schemes; marked images; embedding techniques; grey scale images; higher embedding capacity; reversible data hiding algorithms	REVERSIBLE IMAGE WATERMARKING; STRATEGY	In the most data hiding methods, the cover media is entirely distorted and it cannot be recovered when the secretive data is extracted. However, in some applications, such as medical imaging, military imaging and law enforcement, even a slight modification in the cover image is impermissible. Therefore, reversible data hiding schemes have proposed as a solution of this issue, recently. Histogram based embedding schemes are among wide investigated reversible data hiding schemes owing to the high quality of the marked images. In histogram based embedding techniques, construction of histogram greatly affects the embedding capacity and the distortion performance of schemes. In this study, the authors propose a novel reversible data hiding scheme for grey scale images. In this method, a new histogram is generated according to the differences between pixels and their neighbouring pixels values. The proposed scheme can achieve higher embedding capacity than other existing reversible data hiding algorithms in the literature by considering the same value of peak signal-to-noise ratio. Experimental and analytical results show that this scheme is successfully employed for data hiding in a wide range of images.																	1751-9659	1751-9667				JUL 20	2020	14	9					1716	1725		10.1049/iet-ipr.2018.6428													
J								Segmentation techniques for early cancer detection in red blood cells with deep learning-based classifier-a comparative approach	IET IMAGE PROCESSING										feature extraction; cellular biophysics; cancer; image segmentation; learning (artificial intelligence); medical image processing; blood; pattern classification; neural nets; early cancer detection; red blood cells; red blood corpuscles; blood composition; living cells; healthy RBCs; cancer cell; blood samples; ORBS method; deep learning classification method; imaging tools; online region-based segmentation method; feature extraction process; receiver operating characteristic curve	IMAGE SEGMENTATION; NEURAL-NETWORKS; REGION; SNAKES	Red Blood Corpuscles called Erythrocytes are the most important element in blood composition which is mainly responsible in all living cells. To detect the cancer cell various methods are employed. In this paper, proper identification of cancer cells from unaffected RBCs are detected. The proposed novel method called Online Region Based Segmentation (ORBS) method is done that is used to find the regions of corpuscles. By using properties, metric is formulated for determination of shape which is abnormal in blood cells. Overall accuracy of 96.9% is obtained using proposed ORBS method and deep learning classification (DLC) method has accuracy of 97.1% that helps to diagnose cancer cells using feature extraction process done automatically. Sensitivity, specificity and precision values of the proposed segmentation method is found to be 96.7%, 95.6% and 98.4% respectively. The computation time was found as 22 seconds. Closeness of Proposed method in relative to True Positive values at the ROC curve indicates the performance as higher. Comparative analysis is made with ResNet-50 based on the different testing and training data at rate of 90%-10%, 80%-20% and 70%-30% respectively, which proves the robustness of proposed research work. Experimental results prove proposed system effectiveness compared with other detection methods.																	1751-9659	1751-9667				JUL 20	2020	14	9					1726	1732		10.1049/iet-ipr.2019.1067													
J								Fast fractal image compression algorithm using specific update search	IET IMAGE PROCESSING										fractals; image coding; image reconstruction; data compression; adjacent domain blocks; SUSFIC algorithm; encoding time; appropriate search update threshold; maintaining image quality; state-of-the-art FIC algorithms; fast fractal image compression algorithm; real-time applications; encoding process; reconstructed image; decoding process; fractal codes; specific update search FIC algorithm; update times; acceptable matching domain blocks; selected domain blocks pool; computation time; image blocks; original blocks		The fractal image compression (FIC) algorithm is difficult to be widely used in real-time applications due to the huge consumption of its encoding time. Inspired by the fact that in the decoding process, for any original image, a fixed point is generated by the iterations of the fractal codes, the authors propose a specific update search FIC (SUSFIC) algorithm, which uses a scale number to control the update times of the fractal codes and to find acceptable matching domain blocks rather than the best ones in the selected domain blocks pool. To further reduce the computation time, in their proposed algorithm, the image blocks created by the equidistant sampling in the range of the original blocks are used to replace themselves when calculating the correlation coefficients as the distances between the adjacent domain blocks. The experimental results presented show that their proposed SUSFIC algorithm has a significant improvement in encoding time under the premise of setting an appropriate search update threshold and maintaining image quality when compared with the state-of-the-art FIC algorithms. Therefore, it is a better FIC algorithm.																	1751-9659	1751-9667				JUL 20	2020	14	9					1733	1739		10.1049/iet-ipr.2019.0522													
J								Robust graph regularised sparse matrix regression for two-dimensional supervised feature selection	IET IMAGE PROCESSING										learning (artificial intelligence); iterative methods; image classification; optimisation; sparse matrices; pattern classification; image representation; regression analysis; graph theory; feature extraction; two-dimensional supervised feature selection; bilinear matrix regression; matrix data; regression matrices; existing matrix regression methods; local geometric structure; poor classification performance; robust graph regularised sparse matrix regression method; intra-class compactness graph; regularisation item; authors	CLASSIFICATION; RECOGNITION; ILLUMINATION	Bilinear matrix regression based on matrix data can directly select the features from matrix data by deploying several couples of left and right regression matrices. However, the existing matrix regression methods do not consider the local geometric structure of the samples, which results in poor classification performance. This study proposes a robust graph regularised sparse matrix regression method for two-dimensional supervised feature selection, where the intra-class compactness graph based on the manifold learning is used as the regularisation item, and the l(2,1)-norm as loss functions to establish the authors' matrix regression model. An alternating optimisation algorithm is also devised to solve it and give its closed-form solutions in each iteration. The proposed method not only can learn the left and right regression matrices, but also can preserve the intrinsic geometry structure by using the label information. Extensive experiments on several data sets demonstrate the superiority of the proposed method.																	1751-9659	1751-9667				JUL 20	2020	14	9					1740	1749		10.1049/iet-ipr.2019.1404													
J								General generative model-based image compression method using an optimisation encoder	IET IMAGE PROCESSING										backpropagation; image restoration; encoding; optimisation; image coding; data compression; computer vision; general generative model-based image compression method; optimisation encoder; intensively studied subject; deep generative model; generative adversarial networks; GANs; GAN generator model; synthetic images; optimisation algorithm; image inpainting algorithm; generative models; encoding process; optimisation task; optimal encoded representations; extremely small shape-fixed encoded space		Image compression is an intensively studied subject in computer vision. The deep generative model, especially generative adversarial networks (GANs), is a popular new direction for this subject. In this study, the authors propose a new compression method based on a generative model and focus on its application by GANs. The decoder in the proposed method is modified from the GAN generator model, which can produce visually real-like synthetic images. It is one of the two models in GANs, which is trained through a two-players' contest game. The encoder is an optimisation algorithm called backpropagation-to-the-input, which derives from an image inpainting algorithm based on generative models. In the proposed method, the authors turn the encoding process into an optimisation task to search for optimal encoded representations. Compared with traditional methods, the proposed method can compress images from certain domains into extremely small and shape-fixed encoded space but still retain better visual representations. It is easy and convenient to apply without any retraining or additional modification to the generative models.																	1751-9659	1751-9667				JUL 20	2020	14	9					1750	1758		10.1049/iet-ipr.2019.0715													
J								Collaborative similarity metric learning for face recognition in the wild	IET IMAGE PROCESSING										face recognition; learning (artificial intelligence); feature extraction; neural nets; image classification; transforms; face recognition; face images; fusion techniques; multiple face image features; similarity metric learner; training procedure; similarity score; collaborative similarity metric learning; training Siamese neural networks; hand-crafted features; YouTube Faces; Labeled Faces; Wild data sets; feature-based baselines; higher level features	FEATURES; FUSION; VERIFICATION	Utilising different representations of face images is known to be helpful in face recognition. In this study,the authorspropose two fusion techniques that make use of multiple face image features by collaboratively training a similarity metric learner, based on Siamese neural networks. This training procedure takes two (or possibly more) features of two face images and outputs a similarity score that depicts whether the faces belong to the same person or not.The authorsinvestigate two approaches of collaborative similarity metric learning (CoSiM), both of which are based on training Siamese neural networks jointly, as a means of early fusion. The experiments are employed on hand-crafted features such as scale-invariant feature transform (SIFT) and variants of the local binary pattern (LBP), on the YouTube Faces and the Labeled Faces in the Wild data sets. The authors provide theoretical and empirical comparisons of the proposed models against the related methods in the literature. It is shown that the proposed technique improves on the verification accuracy, compared to single feature-based baselines. By only utilising simple features like SIFT and LBP, the proposed techniques are shown to yield comparable results to the state of the art techniques, which depend on deep convolutional architectures or higher level features.																	1751-9659	1751-9667				JUL 20	2020	14	9					1759	1768		10.1049/iet-ipr.2019.0510													
J								Adaptive bag-of-visual word modelling using stacked-autoencoder and particle swarm optimisation for the unsupervised categorisation of images	IET IMAGE PROCESSING										particle swarm optimisation; vector quantisation; image classification; pattern clustering; visual databases; image representation; feature extraction; learning (artificial intelligence); bag-of-visual word modelling; stacked-autoencoder; particle swarm optimisation; unsupervised categorisation; bag-of-visual words; BOVWs; effective mean; image classification; visual codebook; handcrafted image feature extraction algorithms; vector quantisation; significant computational overhead; poor classification accuracies; adaptive BOVW modelling; deep feature learning; Caltech-101 image dataset; categorisation performance; computational load		The bag-of-visual words (BOVWs) have been recognised as an effective mean of representing images for image classification. However, its reliance on a visual codebook developed using handcrafted image feature extraction algorithms and vector quantisation viak-means clustering often results in significant computational overhead, and poor classification accuracies. Therefore, this study presents an adaptive BOVW modelling, in which image feature extraction is achieved using deep feature learning and the amount of computation required for the development of visual codebook is minimised using a batch implementation of particle swarm optimisation. The proposed method is tested using Caltech-101 image dataset, and the results confirm the suitability of the proposed method in improving the categorisation performance while reducing the computational load.																	1751-9659	1751-9667				JUL 20	2020	14	9					1769	1776		10.1049/iet-ipr.2019.1160													
J								Denoising framework based on external prior guided rotational clustering	IET IMAGE PROCESSING										image denoising; computer vision; Gaussian processes; mixture models; estimation theory; transforms; image matching; pattern clustering; geometry; external prior guided rotational clustering; image denoising; nonlocal self-similarity prior; NSS; natural image patches; Gaussian mixture models; computer vision; geometric transformation; external guided rotational matching denoising; GMM	IMAGE; TRANSFORM; ALGORITHM; SPARSE	The image denoising model based on non-local self-similarity prior (NSS) has received extensive attention in recent years because of the repeated structure of natural image patches. Similar patches collected by exploiting NSS prior are sparse, which can be used to estimate potential lowrank subspace. Meanwhile, the modelling of natural images, such as Gaussian mixture models (GMMs), has been successful in all aspects of computer vision by reducing the patterns of image patches. However, the version of its geometric transformation (e.g. rotational transformation) cannot be matched directly by using distance. How to further reduce the patterns of the patches by geometric prior and accelerate rotational matching through parallel calculation is an issue that needs to be solved. In this study, an external guided rotational matching denoising framework is proposed. The proposed framework combines non-local, sparse and low-rank image priors and we design a parallel computing scheme. They demonstrate the performance improvement of the proposed algorithm on images with strong rotational properties and the comparison with traditional state-of-the-art denoising methods. The scalability and effectiveness of the new framework are verified by simulation experiments in public and real datasets.																	1751-9659	1751-9667				JUL 20	2020	14	9					1777	1786		10.1049/iet-ipr.2019.0918													
J								Neural network-based image quality comparator without collecting the human score for training	IET IMAGE PROCESSING										learning (artificial intelligence); neural nets; image classification; feature extraction; widely-adapted image quality measure tests; providing accurate evaluation results; comparison accuracy; cross-dataset validation comparison tests; image training dataset; utilising human score; assigned distortion level differences; pairwise comparison results; training labels; training phase; four-layer comparison network; input image; IQA performance; nonlinear combination strategy; IQC; similar-content image quality comparison; human bias; comparator framework; automated image quality assessment; human behaviours; neural network-based image quality comparator	NATURAL SCENE STATISTICS; CONTRAST; ENHANCEMENT	Emulating human behaviours in automated image quality assessment (IQA) enables a comparator framework to remove the differences in human bias naturally. Based on the observation of the practical applications of IQA, this study focuses on similar-content image quality comparison based on a new image quality comparator (IQC). Outstanding proven IQAs can be utilised in this comparator to achieve a new non-linear combination strategy to boost the IQAs' performance in image quality comparison. For both input images to be compared, proven IQAs are utilised to obtain nine features from each image, yielding 18 total features. Then, a four-layer comparison network conducts a classification task to indicate which input image has better quality. In the training phase, the commonly used human scores as training labels are replaced with pairwise comparison results that are automatically generated from assigned distortion level differences. By not utilising human score in training phase, this IQC shows two advantages: (i) it removes huge labor and time cost to collect the human scores and (ii) it solves the problem of over-fitting benefiting from simplicity of creating a large image training dataset. Furthermore, the experimental tests and cross-dataset validation comparison tests demonstrate its impressive performance.																	1751-9659	1751-9667				JUL 20	2020	14	9					1787	1793		10.1049/iet-ipr.2019.0809													
J								H-WordNet: a holistic convolutional neural network approach for handwritten word recognition	IET IMAGE PROCESSING										natural language processing; feature extraction; learning (artificial intelligence); image segmentation; handwriting recognition; handwritten character recognition; image classification; convolutional neural nets; handwritten word recognition; isolated characters; cursiveness; complex shapes; compound characters; recognition task; character-level segmentation; deep convolutional neural network-based holistic method; H-WordNet model; convolutional layers; standard handwritten Bangla word database; Bangla word images; atomic character classes	DEEP LEARNING TECHNIQUES; CITY NAME RECOGNITION; CHARACTER-RECOGNITION; SAR IMAGES; CLASSIFICATION; BANGLA; SEGMENTATION; MACHINE; ENERGY	Segmentation of handwritten words into isolated characters and their recognition are challenging due to the presence of high variability and cursiveness in Indian scripts. The complex shapes and availability of numerous atomic character classes, compound characters, modifiers, ascendants, and descendants make the recognition task even more difficult. A holistic approach effectively tackles such issues by avoiding the character-level segmentation and the earlier holistic methods have been mostly developed using multi-stage machine learning architecture. In this study, a deep convolutional neural network-based holistic method termed 'H-WordNet' is proposed for handwritten word recognition. The H-WordNet model includes merely four convolutional layers and one fully connected layer to effectively classify the word images', which lead to a significant reduction in parameters. The efficacy of different pooling operations with the proposed model is investigated. The main purpose of this study is to avoid the need for handcrafted feature extraction and obtain a more stable and generalised system for word recognition. The proposed model is evaluated using a standard handwritten Bangla word database (CMATERdb2.1.2), which contains 18000 Bangla word images of 120 different categories and it obtained a higher recognition accuracy of 96.17% when compared to recent state-of-the-art methods.																	1751-9659	1751-9667				JUL 20	2020	14	9					1794	1805		10.1049/iet-ipr.2019.1398													
J								Segmentation method of multiple sclerosis lesions based on 3D-CNN networks	IET IMAGE PROCESSING										medical image processing; image segmentation; neural nets; biomedical MRI; MS lesions; 3D convolutional neural network; convolution layers; pooling layers; alternative lesion voxels; final lesion voxels; MICCAI 2008 datasets; 2016 datasets; baseline methods; different evaluation indicators; lesion volume; segmentation method; multiple sclerosis lesions; 3D-CNN networks; histopathology image segmentation; computer aided diagnosis; image processing; MR images; subsequent lesion reconstruction; volume estimation; course evaluation	BRAIN-TUMOR SEGMENTATION; NEURAL-NETWORKS; MRI	Histopathology image segmentation is an important area in the field of computer aided diagnosis using image processing. The segmentation of Multiple sclerosis (MS) lesions from MR images can establish the basis for subsequent lesion reconstruction, volume estimation, and course evaluation. This study proposes a method for automatically segmenting MS lesions based on 3D convolutional neural network (CNN). The method is divided into two stages, each of which includes two convolution layers and two pooling layers. The alternative lesion voxels are selected in the first stage, while in the second stage, the final lesion voxels are segmented from the lesion voxels which are obtained in the first stage by restricting the conditions. The method has been tested on the MICCAI 2008 and 2016 datasets and compared to the other baseline methods. The experiment results show that the method has better performance than the other baseline methods on different evaluation indicators, including dice similarity coefficient, absolute difference in lesion volume, true positive rate, false positive rate, and predictive positivity value.																	1751-9659	1751-9667				JUL 20	2020	14	9					1806	1812		10.1049/iet-ipr.2019.0880													
J								Surface segmentation and environment change analysis using band ratio phenology index method - supervised aspect	IET IMAGE PROCESSING										ecology; vegetation; image classification; geophysical image processing; learning (artificial intelligence); global warming; irrigation; remote sensing; global warming analysis; satellites; multispectral imagery; principle remote sensing applications; innovative method; physical characteristics; weather forecasting; ecology assessment; irrigation management; seasonal changes; seasonal change analysis; supervised image classification; band ratio phenology index method; BRPI; seasonal impact; classification exactness; surface segmentation; supervised aspect; escalating field; vegetation assessment; coastal studies	LAND-COVER CLASSIFICATION; THIN CLOUD REMOVAL; IMAGES; REGION	Remote sensing is an escalating field that helps to monitor the earth in different perspectives like vegetation assessment, coastal studies, global warming analysis etc. Presently many satellites are orbiting the earth for taking multispectral imagery, which is working behind the principle remote sensing applications. Though there are mechanisms for image classification still innovative method is required to detect and monitor the physical characteristics of the environment. Weather forecasting, ecology assessment and irrigation management are relying upon the seasonal changes. This research study concentrates on seasonal change analysis by supervised image classification called Band Ratio Phenology Index (BRPI) method. This BRPI has helped to learn seasonal impact on the environment for the last six years. Confusion Matrix, Overall Accuracy, and Kappa Coefficient are the quality measures used to legitimise the classification exactness.																	1751-9659	1751-9667				JUL 20	2020	14	9					1813	1821		10.1049/iet-ipr.2018.6526													
J								Swift distance transformed belief propagation using a novel dynamic label pruning method	IET IMAGE PROCESSING										computer vision; computational complexity; image restoration; random processes; Markov processes; belief propagation; SDT-BP; distance transformation; LBP; message scheduling; DT-BP; MRF model; distance function; dynamic label pruning method; loopy belief propagation; Markov random field model; priority-BP; swift distance transformed belief propagation; computational complexity; mage inpainting; image processing; computer vision	IMAGE COMPLETION	Loopy belief propagation (LBP) suffers from high computational time, specifically when each node in the Markov random field (MRF) model has lots of labels. In this study, a swift distance transformed belief propagation (SDT-BP) method is proposed. SDT-BP employs an efficient dynamic label pruning approach together with distance transformation to boost the running time of the LBP. The proposed dynamic label pruning approach is independent of any specific message scheduling. The resultant solution's energy is less than Priority-BP. Furthermore, SDT-BP guarantees convergence in fewer numbers of iterations. The direct combination of distance transformed belief propagation (DT-BP) with the dynamic label pruning in Priority-BP hasO(KTNlogN) computational complexity. However, the proposed method results inO(KTN) complexity. WhereNis the number of nodes,Kis the number of labels for each node, andTis the number of iterations. The authors conduct several experiments on image inpainting case studies, to evaluate this method. According to this analysis, DT-BP faces nearly 90% speedup by preserving the energy of the solution at almost the same level. Furthermore, this method can be utilised in any MRF model where its distance function is transformable, i.e. in various image processing and computer vision problems.																	1751-9659	1751-9667				JUL 20	2020	14	9					1822	1831		10.1049/iet-ipr.2019.1035													
J								Specific category region proposal network for text detection in natural scene	IET IMAGE PROCESSING										natural scenes; feature extraction; image classification; pattern clustering; image colour analysis; text analysis; video signal processing; text detection; object detection; image segmentation; convolutional neural nets; text detection framework; SCRPN; considerable abstract semantic information; natural scene text detection; maximally stable extremal regions; fully convolutional network; text saliency map; oversegmented regions; region aggregation; segmentation regions; text region proposals; top-ranking region proposals; street view text; text proposals; category region proposal network; region convolutional neural network; two-stage text detection networks		Natural scene text usually carries considerable abstract semantic information, which is closely related to the surrounding environment. Thus, natural scene text detection plays a vital role in image content retrieval and understanding. In this study, the authors propose a novel specific category region proposal network (SCRPN) based on maximally stable extremal regions (MSER) and fully convolutional network (FCN) for natural scene text detection. First, FCN for pixel-level recognition is utilised to obtain the text saliency map and MSER is used to obtain oversegmented regions. Then, the multiple features of oversegmented regions and text saliency map are used for region aggregation. Next, single-linkage clustering method is adopted to cluster the segmentation regions to obtain a hierarchical structure of text region proposals. Finally, for the top-ranking region proposals, SCRPN built an end-to-end pipeline for scene text detection directly. Experiments on street view text and international conference on document analysis and recognition (ICDAR) 2013 have demonstrated the effectiveness of SCRPN for generating the text proposals. SCRPN could work with various two-stage text detection networks; thus, faster region convolutional neural network was used as the text detection framework to evaluate the performance of SCRPN in the ICDAR 2015 and MSRA-TD500 benchmarks. The experimental results confirmed that SCRPN makes text detection more robust in complex scenarios.																	1751-9659	1751-9667				JUL 20	2020	14	9					1832	1839		10.1049/iet-ipr.2019.0652													
J								Optimal weighted bilateral filter with dual-range kernel for Gaussian noise removal	IET IMAGE PROCESSING										image denoising; Gaussian noise; filtering theory; smoothing methods; dual-range kernel; robust range; noise levels; denoising performance; remaining image details; method noise; denoised image; optimal approach; Stein's unbiased risk estimate; standard test images; conventional bilateral filter; optimal weighted bilateral filter; Gaussian noise removal; classical technique; edge-preserving smoothing; effective image denoising approach; range distance estimation; pixel-neighbourhood similarity measurement; conventional bilateral filtering approach; noisy observation results; novel bilateral		The bilateral filter is a classical technique for edge-preserving smoothing. It has been widely used as an effective image denoising approach to remove Gaussian noise. The performance of bilateral filtering highly depends on the accuracy of its range distance estimation, which is used for pixel-neighbourhood similarity measurement. However, in the conventional bilateral filtering approach, estimating the range distance directly from noisy observation results in the degradation of denoising performance. To address this issue, the authors propose a novel bilateral filtering scheme with a dual-range kernel, which provides a more robust range of distance estimation at various noise levels compared with existing methods. To further improve the denoising performance, they employ a linear model to retrieve the remaining image details from the method noise and add them back to the denoised image by employing an optimal approach based on Stein's unbiased risk estimate. Experiments on standard test images demonstrate that the proposed method outperforms conventional bilateral filter and its major state-of-the-art variants.																	1751-9659	1751-9667				JUL 20	2020	14	9					1840	1850		10.1049/iet-ipr.2018.6272													
J								Combining highlight removal and low-light image enhancement technique for HDR-like image generation	IET IMAGE PROCESSING										image enhancement; image colour analysis; image recognition; image sensors; image processing; actual colour; low-light enhancement technique; HDR-like; highlight removal; low-light image enhancement technique; low dynamic range image; highlight areas; conventional image sensors; highlight phenomena limit colour richness; image recognition; high dynamic range-like images; rich colours; single LDR image; highlight pixels	EXPOSURE FUSION; SINGLE IMAGE; COLOR; ILLUMINATION	Low dynamic range (LDR) image may contain low-light and highlight areas due to the limitations of the dynamic range of conventional image sensors. Low-light and highlight phenomena limit colour richness and visibility of objects in an image. Therefore, it can cause a reduction in the quality of images and a loss in accuracy in the application of image recognition. To overcome this, high dynamic range (HDR)-like images have been developed with rich colours such as those seen by the human eye. In this study, the authors propose a method to obtain an HDR-like image from a single LDR image by removing the specular component from highlight pixels as well as strengthening the actual colour. Next, they select low-light image enhancement via illumination map estimation as a low-light enhancement technique by showing the comparison with gamma-based expansion operator. They evaluate their HDR-like output images with non-reference and full-reference metrics. They show the comparison of their proposed method with six other methods. Besides, visually, their proposed method delivers more pleasing output than the output of other competitive methods.																	1751-9659	1751-9667				JUL 20	2020	14	9					1851	1861		10.1049/iet-ipr.2019.1099													
J								Optimised multi-wavelet domain for decomposed electrooculogram-based eye movement classification	IET IMAGE PROCESSING										pattern clustering; wavelet transforms; medical signal processing; signal classification; feature extraction; principal component analysis; eye; electro-oculography; pattern classification; image classification; participant; input EOG signal; empirical mean curve decomposition decomposition model; output signal; dimensional reduction; dimensional reduced signal; multiwavelet decomposition model; multiwavelet decomposed signal; feature mapping model; EOG signals; human-computer interface systems; EMCD decomposition model; resultant signal; FM model; optimised multiwavelet domain; decomposed electrooculogram-based eye movement classification; human eye movement tracking; electrooculography signals; human eye tracking system	EOG; EEG; EMG; INTERFACE; SYSTEM	The human eye movement tracking is possible with the assistance of the electrooculography (EOG) signals. The human eye tracking system allows researchers to analyse the participant's eye movements during certain activities. This study offers the EOG signals to control the human-computer interface systems with the help of Empirical Mean Curve Decomposition (EMCD) decomposition model. At first, the input EOG signal is provided as input to the EMCD decomposition model, later the resultant signal is given to principal component analysis for dimensional reduction, and then the dimensional reduced signal is offered to multi-wavelet decomposition model. The resultant dimensionally reduced multi-wavelet decomposed signal is passed to the proposed Feature Mapping (FM) model, using thek-means clustering model. Then, the Grey Wolf Optimization (GWO) algorithm is utilised to tune the margin. Next to mapping, the obtained features are provided to the nearest neighbour classifier, to obtain the eye movement. Next to the implementation, the proposed method is compared with the existing methods, and it is witnessed that the proposed methodology gives the superior performance in correspondence with accuracy, sensitivity, specificity, precision, false positive rate, false negative rate, negative predictive value, false discovery rate, F1 score and Mathews correlation coefficient.																	1751-9659	1751-9667				JUL 20	2020	14	9					1862	1869		10.1049/iet-ipr.2019.0277													
J								Medical image fusion using the PCNN based on IQPSO in NSST domain	IET IMAGE PROCESSING										neural nets; image fusion; inverse transforms; image denoising; medical image processing; particle swarm optimisation; medical image fusion; NSST domain; improved quantum-behaved particle swarm optimisation based pulse-coupled neural network; high-frequency subband; low-frequency subband; conventional PCNN-based methods; IQPSO-PCNN model; nonsubsampled shearlet transform	PERFORMANCE; LINKING	In this study, an improved quantum-behaved particle swarm optimisation based pulse-coupled neural network (IQPSO-PCNN) is proposed in the non-subsampled shearlet transform (NSST) domain for medical image fusion. First, NSST tool is used to decompose the source image into low-frequency and high-frequency subbands. Then, for low-frequency subbands, the fusion rules of two different functions are presented, which simultaneously addresses two key issues of energy preservation and detail extraction. For high-frequency subbands, unlike conventional PCNN-based methods, parameters are manually set based on experience, and the decomposed high-frequency subbands share a set of parameters. The IQPSO-PCNN model can obtain the optimal parameters for each high-frequency subband adaptively according to its own information. Finally, the fused low-frequency subband and high-frequency subbands are inversely transformed by NSST to acquire the final fused image. The proposed algorithm uses >90 pairs of images with four different modalities. In addition, fusion experiments are performed on different sequences of the three modes. The experimental results demonstrate that the proposed method is superior to existing state-of-art methods in subjective visual performance and objective evaluation.																	1751-9659	1751-9667				JUL 20	2020	14	9					1870	1880		10.1049/iet-ipr.2020.0040													
J								Detecting abnormal events in traffic video surveillance using superorientation optical flow feature	IET IMAGE PROCESSING										object detection; image sequences; feature extraction; pattern clustering; video signal processing; image motion analysis; traffic engineering computing; video surveillance; nearest neighbour methods; search problems; abnormal event detection; traffic video surveillance; superorientation optical flow feature; traffic scene; abnormal activities; SOOF features; superorientation motion descriptor; K-means clustering; normal motion flow; nearest-neighbour searching; anomaly detection; motion block localisation; motion block selection		Detection of abnormal events in the traffic scene is very challenging and is a significant problem in video surveillance. The authors proposed a novel scheme called super orientation optical flow (SOOF)-based clustering for identifying the abnormal activities. The key idea behind the proposed SOOF features is to efficiently reproduce the motion information of a moving vehicle with respect to superorientation motion descriptor within the sequence of the frame. Here, the authors adopt the mean absolute temporal difference to identify the anomalies by motion block (MB) selection and localisation. SOOF features obtained from MB are used as motion descriptor for both normal and abnormal events. Simple and efficient K-means clustering is used to study the normal motion flow during the training. The abnormal events are identified using the nearest-neighbour searching technique in the testing phase. The experimental outcome shows that the proposed work is effectively detecting anomalies and found to give results better than the state-of-the-art techniques.																	1751-9659	1751-9667				JUL 20	2020	14	9					1881	1891		10.1049/iet-ipr.2019.0549													
J								Fast prediction mode selection and CU partition for HEVC intra coding	IET IMAGE PROCESSING										computational complexity; support vector machines; video coding; rate distortion theory; complex CU; CU size selection algorithm; direction complexity; coding unit size; dual support vector machine; rough mode decision; mode grouping; intra prediction mode selection; computing complexity; coding quality; double coding efficiency; video coding standard; high-efficiency video coding; HEVC intra coding; CU partition; fast prediction mode selection	DECISION ALGORITHM; SIZE DECISION; EFFICIENCY; SPLIT; SKIP	Compared with the predecessor H.264/advanced video coding, high-efficiency video coding (HEVC) is a new video coding standard with nearly double coding efficiency under the same coding quality. However, the computing complexity of HEVC increases sharply. To solve this problem, a fast algorithm for intra prediction mode selection based on mode grouping is proposed by reducing the number of modes entering rough mode decision. Moreover, the dual support vector machine is proposed to efficiently select the coding unit (CU) size, which is based on texture features of CU and sub-CUs including content complexity and direction complexity. By using the new CU size selection algorithm, the encoder can confirm the split for complex CU and terminate the split for simple CU in advance, so as to reduce the computing complexity of CU size selection. The experimental results show that by employing the two fast algorithms in intra coding, it can save 42.80% encoding time, with 0.98% increment in bit rate and 0.018 dB loss of peak-signal-to-noise ratio of luminance, compared with the reference software x265-1.7.																	1751-9659	1751-9667				JUL 20	2020	14	9					1892	1900		10.1049/iet-ipr.2019.0259													
J								Multi-objectives optimisation of features selection for the classification of thyroid nodules in ultrasound images	IET IMAGE PROCESSING										CAD; biomedical ultrasonics; feature extraction; pattern classification; image classification; support vector machines; cancer; medical image processing; biological organs; image texture; particle swarm optimisation; multiobjectives optimisation; features selection; thyroid nodules; ultrasound images; ultrasound imaging; leading diagnostic method; early-stage thyroid nodule; computer-aided diagnostic systems; benign nature; malignant nature; 447 US images; statistical features extraction methods; feature selection method; multiobjective particle swarm optimisation algorithm; benign nodules; malignant nodules	TEXTURE FEATURES; LESION CLASSIFICATION; CANCER; DIAGNOSIS	Ultrasound (US) imaging is the leading diagnostic method for assessing the early-stage thyroid nodule. However, the visual evaluation of nodules can be influenced by the subjectivity of radiologists' interpretations. Computer-aided Diagnostic (CAD) systems can be useful in classifying these nodules according to their benign or malignant nature. The extraction of the characteristics, which relate in the author's case to the US of thyroid nodules, is essential in the differentiation of these nodules. The complex nature of images, however, generates a significant number of features, many of which are either redundant or irrelevant. This study presents a new CAD system that has been developed to categorise thyroid nodules. In this survey, 447 US images of thyroid nodules were retained. These images were used to extract features using statistical features extraction methods. A feature selection method based on the multi objective particle swarm optimisation algorithm was used to choose the most relevant and non-redundant ones. Then, support vector machine (SVM) and random forests (RFs) were applied to classify these nodules. 10-fold cross-validation was used to assess the classification performance metrics. Their proposed CAD has reached a maximum accuracy of 94.28% for SVM; and 96.13% for RF using the contour-based ROI.																	1751-9659	1751-9667				JUL 20	2020	14	9					1901	1908		10.1049/iet-ipr.2019.1540													
J								Automatic tracing and extraction of text-line and word segments directly in JPEG compressed document images	IET IMAGE PROCESSING										digital libraries; image coding; image segmentation; feature extraction; document image processing; data compression; text detection; text-line; word segments; JPEG; Joint Photographic Experts Group; popular compression algorithms; efficient compression algorithms; consumer electronics world; excessive usage; mobile phones; compressed image; decompression; recompression operations; compressed data; handwritten document images; overlapping components; touching components; moving window-based space penetration algorithm; exact line boundary; word boundary tracing algorithm; segment words; spatial domains; compressed domains	HANDWRITTEN; LOCALIZATION; SCHEME	JPEG is one of the popular and efficient compression algorithms supported in the consumer electronics world. Excessive usage of mobile phones and e-governance applications have all resulted in a huge collection of JPEG compressed document images. The major challenge with these images is that its processing becomes expensive as it requires repeated decompression and recompression operations. Recently, it has been proved that developing algorithms to operate directly on the compressed data is one of the solutions in overcoming the above issue. This research study investigates a novel algorithm for segmentation of text-lines and words directly from JPEG compressed handwritten document images. Segmenting a handwritten document is challenging due to the presence of uneven spacing, variable font sizes, overlapping and touching components, and it becomes much more challenging if it is to be done directly in the compressed image. The proposed technique virtually fixes a vertical stripe at the beginning of the document to detect starting points of text-lines. Then a moving window-based space penetration algorithm is used for tracing the exact line boundary between two text-lines, resolving the issues of space and font variations, touching and overlapping components. Subsequently, a word boundary tracing algorithm is used to segment words.																	1751-9659	1751-9667				JUL 20	2020	14	9					1909	1919		10.1049/iet-ipr.2019.1437													
J								Fully-automatic raw G-band chromosome image segmentation	IET IMAGE PROCESSING										image segmentation; genetics; cellular biophysics; bioinformatics; overlapped chromosomes; fully-automatic raw G-band chromosome image segmentation; background noise; chromosome clusters; touching overlapping chromosomes; raw images; overlapped chromosome separation; segmentation process; chromosome analysis; single chromosome segmentation; time 2; 0 s to 7; 0 s	METAPHASE CELLS	Analysis of the chromosome images plays an important role in discovering one's genetic information and possible genetic disorders. Segmentation has a very substantial place in the chromosome analysis and without an automatic solution, it is a time-consuming and error-prone procedure. Many researchers tried to automate the segmentation process. However, background noise, objects other than chromosomes in the image, touching and overlapped chromosomes are still current issues. To address these issues, the authors proposed fully-automatic raw G-band chromosome image segmentation, which aims to segment every single chromosome with a minimal error. The proposed algorithm contains the following steps: clearing the background noise, eliminating the objects other than chromosomes, distinguishing single chromosomes and chromosome clusters, separating touching and overlapping chromosomes. The proposed algorithm is tested on 508 raw images and achieved an accuracy of 94.7% for touching chromosome separation, 96.3% for overlapped chromosome separation, and 98.94% for segmentation of all chromosomes. The whole segmentation process takes 2-7 s for one image, depending on the number of touching and overlapping chromosomes. The segmentation results showed that compared to the previously proposed methods, their algorithm achieved better accuracy.																	1751-9659	1751-9667				JUL 20	2020	14	9					1920	1928		10.1049/iet-ipr.2019.1104													
J								Novel fuzzy clustering-based bias field correction technique for brain magnetic resonance images	IET IMAGE PROCESSING										medical image processing; fuzzy set theory; brain; biological tissues; pattern clustering; statistical analysis; image segmentation; biomedical MRI; image classification; segmentation accuracy; spatial information; intensity inhomogeneity; equidistant pixels; single cluster; brain MR images; brain magnetic resonance images; preprocessing requirement; brain tissue segmentation task; authentic brain tissue regions; classification; poor resolution magnetic resonance image; intensity distribution; fuzzy clustering-based bias field correction technique; acquisition procedure; standard fuzzy C-means clustering; statistical analysis	C-MEANS ALGORITHM; INTENSITY INHOMOGENEITY; SEGMENTATION; INFORMATION; EXTRACTION	Bias field correction is an essential pre-processing requirement for brain tissue segmentation task. Authentic brain tissue regions are highly useful for classification and detection of abnormalities. A poor resolution magnetic resonance (MR) image is produced with irregularities in structure, abnormalities in the intensity distribution and noise during the acquisition procedure. The existing bias field correction methods do not consider the spatial information. Further, the problem of equidistant pixels while clustering is not addressed. These problems lead to poor segmentation accuracy. To solve these problems, the authors suggest a novel biased fuzzy clustering technique for the problem on hand. The basic idea is to incorporate the spatial information by altering the membership matrix of standard fuzzy C-means clustering to lower the effect of noise and intensity inhomogeneity. It also helps in improving the segmentation accuracies of the tissue regions by assigning the equidistant pixels to a single cluster. The suggested technique is validated with different modalities of brain MR images. Various evaluation indices are computed followed by the statistical analysis to justify the superiority of the suggested technique in comparison to the state-of-the-art methods.																	1751-9659	1751-9667				JUL 20	2020	14	9					1929	1936		10.1049/iet-ipr.2019.0942													
J								Reweighted infrared patch image model for small target detection based on non-convexScript capital Lp-norm minimisation and TV regularisation	IET IMAGE PROCESSING										minimisation; image denoising; infrared imaging; object detection; NNM; TV regularisation term; background patch image; infrared images; synthetic images; background suppression ability; baseline methods; target detection; nonconvex Lp-norm minimisation; complex background; infrared detection system; infrared patch image model; reweighted IPI model; nuclear norm minimisation	SALIENCY; ALGORITHM; REMOVAL	Infrared small target detection in a complex background has always been a challenging task in an infrared detection system. The existing methods based on the infrared patch image (IPI) model have achieved a good result but are sensitive to the complex background. So, to effectively detect the small target in complex background, model based on the reweighted IPI model along with total variance (TV) is proposed in this study. In this study firstly, the problem of using nuclear norm minimisation (NNM) in the existing IPI-based methods is discussed, and a solution is proposed by replacing the existing NNM with theScript capital Lp-norm minimisation of singular values in the existing IPI methods. Secondly, a TV regularisation term is added to the background patch image to suppress the noise and preserve the strong edges in the background. The proposed method is solved by the alternating direction method of the multiplier. The robustness of the proposed method is validated by experimenting with the large dataset of real infrared images as well as the synthetic images. The proposed method not only has good background suppression ability, but also enhances and detect the target well in comparisons with the other baseline methods.																	1751-9659	1751-9667				JUL 20	2020	14	9					1937	1947		10.1049/iet-ipr.2019.1660													
J								Learning second-order statistics for place recognition based on robust covariance estimation of CNN features	NEUROCOMPUTING										Place recognition; Covariance estimation; Convolutional neural network; Second-order statistics	PROBABILISTIC LOCALIZATION; SCENE; SLAM	Appearance based loop closure detection plays an important role in visual simultaneous localization and mapping systems (vSLAM) by measuring similarity of the places and checking loops to reduce the accumulated error. Traditional loop closure methods execute place recognition by image retrieval with Bag-of-Word model, which forms an orderless representation of local feature descriptors. Convolutional neural networks (CNNs) based features have been investigated for place recognition, where the final descriptors usually are generated by first-order pooling, limiting the representation ability in challenging scenarios. To handle above issue, we introduce high-order statistics into place recognition by developing a novel adaptively normalized covariance pooling method for learning place representations in an end-to-end manner. The proposed method provides robust covariance matrix estimation of high-dimensional and small-size deep features by adaptive covariance normalization (AdaCN). Experimental results on place recognition in the urban environment and image retrieval tasks show that second-order representation is effective, especially for discriminating places with confusing objects, changes in viewpoint and illumination. Besides, the proposed adaptive normalization performs favorably against its counterparts based on Log-Euclidean Riemannian metric and Power-Euclidean metric, while our method is superior to the state-of-the-art place recognition approaches. (C) 2020 Published by Elsevier B.V.																	0925-2312	1872-8286				JUL 20	2020	398						197	208		10.1016/j.neucom.2020.02.001													
J								Summarizing egocentric videos using deep features and optimal clustering	NEUROCOMPUTING										Egocentric video summarization; Deep features; Center-surround model; Integer Knapsack	FRAMEWORK	In this paper, we address the problem of summarizing egocentric videos using deep features and an optimal clustering approach. Based on an augmented pre-trained convolutional neural network (CNN), each frame in an egocentric video is represented by deep features. An optimal clustering algorithm, based on a center-surround model (CSM) and an Integer Knapsack type formulation (IK) for K-means, termed as CSMIK K-means, is applied next to obtain the summary. In the center surround model, we compute difference in entropy and the optical flow values between the central region and that of the surrounding region of each frame. In the integer knapsack formulation, each cluster is treated as an item whose cost is assigned from the center surround model. A potential set of clusters in CSMIK K-means is obtained from the chi-square distance between color histograms of successive frames. CSMIK K-Means evaluates different cluster formations and simultaneously determines the optimal number of clusters and the corresponding summary. Experimental evaluation on four well-known benchmark datasets clearly indicate the superiority of the proposed method over several state-of-the-art approaches. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 20	2020	398						209	221		10.1016/j.neucom.2020.02.099													
J								Robust IoT time series classification with data compression and deep learning	NEUROCOMPUTING										IoT applications; Energy efficiency; Lossy compression; Data reduction; Time series classification; Deep neural networks; Discrete wavelet transform; lifting scheme	NETWORKS	Internet of Things (IoT) and wearable systems are very resource limited in terms of power, memory, bandwidth and processor performance. Sensor time series compression can be regarded as a direct way to use memory and bandwidth resources efficiently. On the other hand, the time series classification has recently attracted great attention and has found numerous potential uses in areas such as finance, industry and healthcare. This paper investigates the effect of lossy compression techniques on the time series classification task using deep neural networks. Furthermore, this paper proposes an efficient compression approach for univariate and multivariate time series that combines the lifting implementation of the discrete wavelet transform with an error-bound compressor, namely Squeeze (SZ), to attain an optimal trade-off between data compression and data quality. (C) 2020 Published by Elsevier B.V.																	0925-2312	1872-8286				JUL 20	2020	398						222	234		10.1016/j.neucom.2020.02.097													
J								Structural correlation filters combined with a Gaussian particle filter for hierarchical visual tracking	NEUROCOMPUTING										Structural correlation filter; Gaussian Particle Filter (GPF); Lukas-Kanade (LK); Reliability estimation; Convolutional Neural Network (CNN)	OBJECT TRACKING	Visual tracking is a key problem for many computer vision applications such as human-computer interaction, intelligent medical diagnosis, navigation and traffic control management. Most of the existing tracking methods are mainly based on correlation filters. However, boundary effect, scale estimation and template updating have not been fully resolved. Herein, this paper presents a new hierarchical tracking method combining structural correlation filters with a Gaussian Particle Filter (GPF), named KCF-GPF. Weak KCF classifiers are constructed via a Lukas-Kanade (LK) method and the preliminary target location is presented as a weighted sum of these classifiers. Specially, a facile weight strategy is implemented to estimate the reliability of each weak classifier. On the basis of the preliminary target location, the GPF using features from a Convolutional Neural Network (CNN) is employed to predict the location and scale of a target. Extensive experiments with the OTB-2013 and the OTB-2015 databases demonstrate that the proposed algorithm performs favourably against state-of-the-art trackers. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 20	2020	398						235	246		10.1016/j.neucom.2020.02.095													
J								Multi-task learning for aspect term extraction and aspect sentiment classification	NEUROCOMPUTING										Aspect based sentiment analysis; Aspect term extraction; Aspect sentiment classification; Multi-task learning; Joint-modelling		Aspect sentiment classification has a dependency over the aspect term extraction. The majority of the existing studies tackle these two problems independently, i.e., while performing aspect sentiment classification, it is assumed that the aspect terms are pre-identified. However, such assumptions are neither practical nor appropriate. In this paper, we address these impractical limitations and propose a multitask learning framework for the identification and classification of aspect terms in a unified model. At first, the proposed approach employs a BiLSTM followed by a self-attention mechanism to identify the aspect terms in a given sentence. Subsequently, the architecture utilizes a CNN framework to predict the sentiments of the identified aspect terms. We evaluate our proposed approach for the three benchmark datasets across two languages, i.e., English and Hindi. Experimental results suggest that the proposed multi-task model achieves competitive performance with reduced complexity (i.e., a single model for the two tasks compared to two separate models for each task) for both the languages. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 20	2020	398						247	256		10.1016/j.neucom.2020.02.093													
J								Wavelet packet analysis for speaker-independent emotion recognition	NEUROCOMPUTING										Affective computing; Speech signal; Wavelet packet; Wavelet packet coefficient	SPEECH; FREQUENCY; FEATURES; DECOMPOSITION; COEFFICIENTS; MODEL	Extracting effective features from speech signals is essential to recognize different emotions. Recent studies have demonstrated that wavelet analysis is a useful technique in signal processing. In this study, we extract emotion features using wavelet packet analysis from speech signals for speaker-independent emotion recognition. We explore and evaluate these features from two databases, i.e., EMODB and EESDB. It is found that the extracted features are effective for recognizing various speech emotions. Furthermore, compared with common features such as Mel-Frequency Cepstral Coefficients (MFCC), these features can improve the recognition rates by 14.9 and 4.3 percentages on EMODB and EESDB, respectively. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 20	2020	398						257	264		10.1016/j.neucom.2020.02.085													
J								Image deblurring using tri-segment intensity prior	NEUROCOMPUTING										Blind deconvolution; Tri-segment intensity; Kernel estimation; Image restoration		Camera shake during exposure often introduces annoying blur of objects and deteriorates image quality. Existing image deblurring algorithms usually use intensity and gradient priors to alleviate the degree of blurring. However, these methods only consider the changes caused by the blur process in the low intensity range, omitting the changes caused by the blur process in the high and middle part of the intensity range. In this paper, we propose an effective blind image deblurring algorithm based on the three segments of intensity prior, i.e., low, middle and high parts. This work is motivated by the observation that the blur process destroys the sparsity of both ends of intensity, and meanwhile shrinks the distance between the two distinct gray levels. A fast numerical scheme is deployed for alternatingly computing the sharp image and the blur kernel using an image pyramid at the stage of kernel estimation. Extensive experiments on both synthetic and real-world blurred images demonstrate that our method performs favorably against the state-of-the-art image deblurring methods. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 20	2020	398						265	279		10.1016/j.neucom.2020.02.082													
J								Mix-zero-sum differential games for linear systems with unknown dynamics based on off-policy IRL	NEUROCOMPUTING										Mixed-zero-sum game; Policy iteration; Off-policy; Integral reinforcement learning	TIME; ALGORITHM	This paper discusses a multi-player mixed-zero-sum (MZS) differential games with completely unknown dynamics. Based on off-policy integral reinforcement learning (IRL), a novel algorithm is proposed to obtain the optimal control. First, a policy iteration algorithm is put forward to obtain the optimal solution for deterministic system. Next, the case that the system dynamics is completely unknown is considered. And an IRL-based off-policy algorithm is presented. Meanwhile, the convergence of the presented algorithms is proved in this paper. At the end, the effectiveness of the proposed algorithm is shown by a simulation. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 20	2020	398						280	290		10.1016/j.neucom.2020.02.078													
J								Generative collaborative networks for single image super-resolution	NEUROCOMPUTING										Super-resolution; Deep learning; GANs; Perceptual loss		A common issue of deep neural networks-based methods for the problem of Single Image Super-Resolution (SISR), is the recovery of finer texture details when super-resolving at large upscaling factors. This issue is particularly related to the choice of the objective loss function. In particular, recent works proposed the use of a VGG loss which consists in minimizing the error between the generated high resolution images and ground-truth in the feature space of a Convolutional Neural Network (VGG19), pretrained on the very "large" ImageNet dataset. When considering the problem of super-resolving images with a distribution "far" from the ImageNet images distribution (e.g., satellite images), their proposed fixed VGG loss is no longer relevant. In this paper, we present a general framework named Generative Collaborative Networks (GCN), where the idea consists in optimizing the generator (the mapping of interest) in the feature space of a features extractor network. The two networks (generator and extractor) are collaborative in the sense that the latter "helps" the former, by constructing discriminative and relevant features (not necessarily fixed and possibly learned mutually with the generator). We evaluate the GCN framework in the context of SISR, and we show that it results in a method that is adapted to superresolution domains that are "far" from the ImageNet domain. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				JUL 20	2020	398						293	303		10.1016/j.neucom.2019.02.068													
J								Benefiting from multitask learning to improve single image super-resolution	NEUROCOMPUTING										Single image super-resolution; Multitask learning; Recovering realistic textures; Semantic segmentation; Generative adversarial network		Despite significant progress toward super resolving more realistic images by deeper convolutional neural networks (CNNs), reconstructing fine and natural textures still remains a challenging problem. Recent works on single image super resolution (SISR) are mostly based on optimizing pixel and content wise similarity between recovered and high-resolution (HR) images and do not benefit from recognizability of semantic classes. In this paper, we introduce a novel approach using categorical information to tackle the SISR problem; we present an encoder architecture able to extract and use semantic information to super-resolve a given image by using multitask learning, simultaneously for image super-resolution and semantic segmentation. To explore categorical information during training, the proposed decoder only employs one shared deep network for two task-specific output layers. At run-time only layers resulting HR image are used and no segmentation label is required. Extensive perceptual experiments and a user study on images randomly selected from COCO-Stuff dataset demonstrate the effectiveness of our proposed method and it outperforms the state-of-the-art methods. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 20	2020	398						304	313		10.1016/j.neucom.2019.07.107													
J								Deep learning-based super-resolution of 3D magnetic resonance images by regularly spaced shifting	NEUROCOMPUTING										Magnetic resonance imaging; Super resolution; Convolutional neural networks; Supervised learning		The image acquisition process in the field of magnetic resonance imaging (MRI) does not always provide high resolution results that may be useful for a clinical analysis. Super-resolution (SR) techniques manage to increase the image resolution, being especially effective those based on examples that determine a correspondence between patterns of low resolution and high resolution. Deep learning neural networks have been applied in recent years to estimate this association with very competitive results. In this work, the starting point is a convolutional neuronal network to which a regularly spaced shifting mechanism over the input image is applied, with the aim of substantially improving the quality of the resulting image. This hybrid proposal has been compared with several SR techniques using the peak signal-to-noise ratio, structural similarity index and Bhattacharyya coefficient metrics. The results obtained on different MR images show a considerable improvement both in the restored image and in the residual image without an excessive increase in computing time. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 20	2020	398						314	327		10.1016/j.neucom.2019.05.107													
J								Ultra-dense GAN for satellite imagery super-resolution	NEUROCOMPUTING										Satellite imagery; Super-resolution; GAN; Ultra-dense residual block		Image super-resolution (SR) techniques improve various remote sensing applications by allowing for finer spatial details than those captured by the original acquisition sensors. Recent advances in deep learning bring a new opportunity for SR by learning the mapping from low to high resolution. The most used convolutional neural networks (CNN) based approaches are prone to excessive smoothing or blurring due to the optimization objective in mean squared error (MSE). Instead, generative adversarial network (GAN) based approaches can achieve more perceptually acceptable results. However, the preliminary design of GANs generator with simple direct- or skip-connection residual blocks compromises its SR potential. Emerging dense convolutional network (DenseNet) equipped with dense connections has shown a promising prospect in classification and super-resolution. An intuitive idea to introduce DenseNet into GAN is expected to boost SR performance. However, because convolutional kernels in the existing residual block are arranged into a one-dimensional flat structure, the formation of dense connections highly relies on skip connections (linking the current layer to all subsequent layers with a shortcut path). In order to increase connection density, the depth of the layer has to be accordingly expanded, which in turn results in training difficulties such as vanishing gradient and information propagation loss. To this end, this paper proposes an ultra-dense GAN (udGAN) for image SR, where we reform the internal layout of the residual block into a two-dimensional matrix topology. This topology can provide additional diagonal connections so that we can still accomplish enough pathways with fewer layers. In particular, the pathways are almost doubled compared to previous dense connections under the same number of layers. The achievable rich connections are flexibly adapted to the diversity of image content, thus leading to improved SR performance. Extensive experiments on public benchmark datasets and real-world satellite imagery show that our model outperforms state-of-the-art counterparts in both subjective and quantitative assessments, especially those related to perception. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 20	2020	398						328	337		10.1016/j.neucom.2019.03.106													
J								Prioritizing positive feature values: a new hierarchical feature selection method	APPLIED INTELLIGENCE										Hierarchical feature spaces; Feature selection; Classification	LIFE-SPAN	In this work we address the problem of feature selection for the classification task in hierarchical and sparse feature spaces, which characterize many real-world applications nowadays. A binary feature space is deemed hierarchical when its binary features are related via generalization-specialization relationships, and is considered sparse when in general the instances contain much fewer "positive" than "negative" feature values. In any given instance, a feature value is deemed positive (negative) when the property associated with the feature has been (has not been) observed for that instance. Although there are many methods for the traditional feature selection problem in the literature, the proper treatment to hierarchical feature structures is still a challenge. Hence, we introduce a novel hierarchical feature selection method that follows the lazy learning paradigm-selecting a feature subset tailored for each instance in the test set. Our strategy prioritizes the selection of features with positive values, since they tend to be more informative-the presence of a relatively rare property is usually a piece of more relevant information than the absence of that property. Experiments on different application domains have shown that the proposed method outperforms previous hierarchical feature selection methods and also traditional methods in terms of predictive accuracy, selecting smaller feature subsets in general.																	0924-669X	1573-7497															10.1007/s10489-020-01782-5		JUL 2020											
J								Joint local structure preservation and redundancy minimization for unsupervised feature selection	APPLIED INTELLIGENCE										Unsupervised feature selection; Local structure preservation; Redundancy minimization; Sparsity constraint; Maximal information coefficient	SUPERVISED FEATURE-SELECTION; MUTUAL INFORMATION; SPARSE REGRESSION; FRAMEWORK; RELEVANCE	Unsupervised feature selection is an indispensable pre-processing step in many data mining and pattern recognition tasks where the unlabeled high dimensional data are ubiquitous. Most of existing methods fail to explore the local geometric structure consistency (preservation) of the input data and minimize redundancy of selected features simultaneously. In this paper we propose a novel unsupervised feature selection method which jointly integrates the local geometric structure consistency and redundancy minimization (JLSPRM) into an unified framework. JLSPRM utilizes nonnegative spectral analysis to learn the cluster labels of the input data, then the local geometric structure consistency is developed to make the learned cluster labels more accurate, during which the feature selection operation is performed. To minimize the redundancy rate among selected features, the maximal information coefficient (MIC) is utilized to evaluate the correlation of the pairwise features. Besides, the l(2,1)-norm is exerted on feature selection matrix which makes the framework decent for selecting features. An efficient iterative optimization algorithm is designed to obtain the solution of the unsupervised feature selection model. The superiority and effectiveness of our proposed approach over the state-of-the-art feature selection methods have also been validated through the extensive experiments on nine benchmark datasets.																	0924-669X	1573-7497															10.1007/s10489-020-01800-6		JUL 2020											
J								Spherical fuzzy extension of DEMATEL (SF-DEMATEL)	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										DEMATEL; linguistic assessment; multiple criteria decision-making; network relations map; Spherical fuzzy sets	SUPPLY CHAIN; HYBRID APPROACH; RISK-ASSESSMENT; ANP APPROACH; MODEL; SETS; FRAMEWORK; SELECTION; LOCATION; SMART	In recent years, many versions of fuzzy sets are introduced in the literature. Spherical fuzzy sets (SFSs) concept is one of these latest developments. An SFS is a synthesis of Pythagorean fuzzy set (PFS) and neutrosophic set for providing a larger preference domain for experts and allowing them to express their hesitancy more comprehensively. The distinctive feature of SFSs is that the squared sum of membership, nonmembership, and hesitancy degrees is between 0 and 1 while each degree should be independently defined in [0,1]. The application of SFS on decision-making field may be about describing expert evaluations more informatively and explicitly. In this study, spherical fuzzy version of DEMATEL, one of the most cited multiple criteria decision-making approach, is introduced for considering the hesitancy degrees of experts when they evaluate the potential influences among the criteria. Another contribution made is the fuzzification of all the steps in DEMATEL until the last ones since the existing propositions in literature perform early defuzzification operations. The defuzzification is here applied as close to the end as possible for keeping the whole process fuzzy. The applicability of the proposition is shown in an illustrative example of a building contractor evaluation problem, and its robustness is shown by comparing the results with PFS-based and neutrosophic DEMATEL versions.																	0884-8173	1098-111X				SEP	2020	35	9					1329	1353		10.1002/int.22255		JUL 2020											
J								A novel entropy proposition for spherical fuzzy sets and its application in multiple attribute decision-making	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										entropy; linguistic assessment; spherical fuzzy sets; WASPAS	WASPAS METHOD; AGGREGATION OPERATORS; SIMILARITY; SELECTION; EXTENSION; WEIGHTS	The inherent vagueness and uncertainty in reaching decisions are effectively coped with fuzzy logic and the concept of spherical fuzzy sets is one of the latest developments in this area. The hesitancy of decision-maker(s) about an attribute can be represented more extensively since the squared sum of membership, nonmembership, and hesitancy degrees should be between 0 and 1 while each degree should be defined in [0, 1]. In this study, we propose a novel entropy measure for spherical fuzzy sets and show its ability to provide the required properties. Then its usage in determining objective attribute weights is shown in an application of SF-WASPAS (Spherical Fuzzy extension of Weighted Aggregated Sum Product Assessment) on an illustrative problem. The robustness of the novel entropy's usage as an objective weighting tool is demonstrated by comparing the ranking solution of the proposed SF-WASPAS with the rankings obtained by SF-TOPSIS, SF-VIKOR, and SF-CODAS.																	0884-8173	1098-111X				SEP	2020	35	9					1354	1374		10.1002/int.22256		JUL 2020											
J								Adversarial attacks on text classification models using layer-wise relevance propagation	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										adversarial attacks; layer-wise relevance propagation; text classification		Due to the nested nonlinear structure inside neural networks, most existing deep learning models are treated as black boxes, and they are highly vulnerable to adversarial attacks. On the one hand, adversarial examples shed light on the decision-making process of these opaque models to interrogate the interpretability. On the other hand, interpretability can be used as a powerful tool to assist in the generation of adversarial examples by affording transparency on the relative contribution of each input feature to the final prediction. Recently, a post-hoc explanatory method,layer-wise relevance propagation(LRP), shows significant value in instance-wise explanations. In this paper, we attempt to optimize the recently proposedexplanation-based attack algorithms(EAAs) on text classification models with LRP. We empirically show that LRP provides good explanations and benefits existing EAAs notably. Apart from that, we propose a LRP-based simple but effective EAA,LRPTricker. LRPTricker uses LRP to identify important words and subsequently performs typo-based perturbations on these words to generate the adversarial texts. The extensive experiments show that LRPTricker is able to reduce the performance of text classification models significantly with infinitesimal perturbations as well as lead to high scalability.																	0884-8173	1098-111X				SEP	2020	35	9					1397	1415		10.1002/int.22260		JUL 2020											
J								Optimal control of distributed multiagent systems with finite-time group flocking	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										finite-time convergence; group flocking; multiagent systems (MASs); optimal control; performance index	CONTAINMENT CONTROL; CONSENSUS; OPTIMIZATION; TRACKING; COORDINATION; VEHICLES	The flocking of multiple intelligent agents, inspired by the swarm behavior of natural phenomena, has been widely used in the engineering fields such as in unmanned aerial vehicle (UAV) and robots system. However, the performance of the system (such as response time, network throughput, and resource utilization) may be greatly affected while the intelligent agents are engaged in cooperative work. Therefore, it is concerned to accomplish the distributed cooperation while ensuring the optimal performance of the intelligent system. In this paper, we investigated the optimal control problem of distributed multiagent systems (MASs) with finite-time group flocking movement. Specifically, we propose two optimal group flocking algorithms of MASs with single-integrator model and double-integrator model. Then, we study the group consensus of distributed MASs by using modern control theory and finite-time convergence theory, where the proposed optimal control algorithms can drive MASs to achieve the group convergence in finite-time while minimizing the performance index of the intelligence system. Finally, experimental simulation shows that MASs can keep the minimum energy function under the effect of optimal control algorithm, while the intelligent agents can follow the optimal trajectory to achieve group flocking in finite time.																	0884-8173	1098-111X				SEP	2020	35	9					1416	1432		10.1002/int.22264		JUL 2020											
J								VisDroid: Android malware classification based on local and global image features, bag of visual words and machine learning techniques	NEURAL COMPUTING & APPLICATIONS										Android malware; Visualization-based malware classification; Image-based features; Bag of visual words		In this paper, VisDroid, a novel generic image-based classification method has been suggested and developed for classifying the Android malware samples into its families. To this end, five grayscale image datasets each of which contains 4850 samples have been constructed based on different files from the contents of the Android malware samples sources. Two types of image-based features have been extracted and used to train six machine learning classifiers including Random Forest, K-nearest neighbour, Decision trees, Bagging, AdaBoost and Gradient Boost classifiers. The first type of the extracted features is local features including Scale-Invariant Feature Transform, Speeded Up Robust Features, Oriented FAST and Rotated BRIEF (ORB) and KAZE features. The second type of the extracted features is global features including Colour Histogram, Hu Moments and Haralick Texture. Furthermore, a hybridized ensemble voting classifier has been proposed to test the efficiency of using a number of machine learning classifiers trained using local and global features as voters to make a decision in an ensemble voting classifier. Moreover, two well-known deep learning model, i.e. Residual Neural Network and Inception-v3 have been tested using some of the constructed image datasets. Furthermore, when the results of the proposed model have been compared with the results of some state-of-art works it has been revealed that the proposed model outperforms the compared previous models in term of classification accuracy, computational time, generality and classification mode.																	0941-0643	1433-3058															10.1007/s00521-020-05195-w		JUL 2020											
J								Deep CNN models-based ensemble approach to driver drowsiness detection	NEURAL COMPUTING & APPLICATIONS										Deep learning; CNN; AlexNet; FlowImageNet; VGG-FaceNet; ResNet		Statistics have shown that many accidents occur due to drowsy condition of drivers. In a study conducted by National Sleep Foundation, it has been found that about 20% of drivers feel drowsy during driving. These statistics paint a very scary picture. This paper proposes a system for driver drowsiness detection, in which the architecture detects sleepiness of driver. The proposed architecture consists of four deep learning models: AlexNet, VGG-FaceNet, FlowImageNet and ResNet, which use RGB videos of drivers as input and help in detecting drowsiness. Also, these models consider four types of different features such as hand gestures, facial expressions, behavioral features and head movements for the implementation. The AlexNet model is used for various background and environmental changes like indoor, outdoor, day and night. VGG-FaceNet is used to extract facial characteristics like gender ethnicities. FlowImageNet is used for behavioral features and head gestures, and ResNet is used for hand gestures. Hand gestures detection provides a precise and accurate result. These models classify these features into four classes: non-drowsiness, drowsiness with eye blinking, yawning and nodding. The output of these models is provided to ensemble algorithm to obtain a final output by putting them through a SoftMax classifier that gives us a positive (drowsy) or negative answer. The accuracy obtained from this system came out to be 85%.																	0941-0643	1433-3058															10.1007/s00521-020-05209-7		JUL 2020											
J								Higher-order and long-range synchronization effects for classification and computing in oscillator-based spiking neural networks	NEURAL COMPUTING & APPLICATIONS										Vanadium dioxide; Oscillatory neural networks; Thermal coupling; Higher-order synchronization; Classification; Oscillator-based computing; Reservoir computing	NOISE; DYNAMICS	In the circuit of two thermally coupled VO(2)oscillators, we studied a higher-order synchronization effect, which can be used in object classification techniques to increase the number of possible synchronous states of the oscillator system. We developed the phase-locking estimation method to determine the values of subharmonic ratio and synchronization effectiveness. In our experiment, the number of possible synchronous states of the oscillator system was twelve, and subharmonic ratio distributions were shaped as Arnold's tongues. In the model, the number of states may reach the maximum value of 150 at certain levels of coupling strength and noise. The long-range synchronization effect in a one-dimensional chain of oscillators occurs even at low values of synchronization effectiveness for intermediate links. We demonstrate a technique for storing and recognizing vector images, which can used for reservoir computing. In addition, we present the implementation of analog operation of multiplication, the synchronization-based logic for binary computations and the possibility to develop the interface between spike neural network and a computer. Based on the universal physical effects, the high-order synchronization can be applied to any spiking oscillators with any coupling type, enhancing the practical value of the presented results to expand spike neural network capabilities.																	0941-0643	1433-3058															10.1007/s00521-020-05177-y		JUL 2020											
J								Differential evolution with infeasible-guiding mutation operators for constrained multi-objective optimization	APPLIED INTELLIGENCE										Constrained optimization; Differential evolution; Multi-objective optimization; Infeasible solution; Mutation operator	SELF-ADAPTIVE MUTATION; GLOBAL OPTIMIZATION; CROSSOVER OPERATOR; FEASIBILITY RULE; ALGORITHM; STRATEGY; RANKING; MOEA/D	Constrained multi-objective optimization problems (CMOPs) are common in engineering design fields. To solve such problems effectively, this paper proposes a new differential evolution variant named IMDE with infeasible-guiding mutation operators and a multistrategy technique. In IMDE, an infeasible solution with lower objective values is maintained for each individual in the main population, and this infeasible solution is then incorporated into some common differential evolution's mutation operators to guide the search toward the region with promising objective values. Moreover, multiple mutation strategies and control parameters are adopted during the trial vector generation procedure to enhance both the convergence and the diversity of differential evolution. The superior performance of IMDE is validated via comparisons with some state-of-the-art constrained multi-objective evolutionary algorithms over 3 sets of artificial benchmarks and 4 widely used engineering design problems. The experiments show that IMDE outperforms other algorithms or obtains similar results. It is an effective approach for solving CMOPs, basically due to the use of infeasible-guiding mutation operators and multiple strategies.																	0924-669X	1573-7497															10.1007/s10489-020-01733-0		JUL 2020											
J								An improved moth-flame optimization algorithm with orthogonal opposition-based learning and modified position updating mechanism of moths for global optimization problems	APPLIED INTELLIGENCE										Moth-Flame optimization algorithm; Orthogonal experiment design; Opposition-based learning; Mutation operator; Engineering optimization problems	PARTICLE SWARM OPTIMIZATION; INSPIRED OPTIMIZER; EVOLUTIONARY; MUTATION	Moth-Flame Optimization (MFO) algorithm is a new population-based meta-heuristic algorithm for solving global optimization problems. Flames generation and spiral search are two key components that affect the performance of MFO. To improve the diversity of flames and the searching ability of moths, an improved Moth-Flame Optimization (IMFO) algorithm is proposed. The main features of the IMFO are: the flames are generated by orthogonal opposition-based learning (OOBL); the modified position updating mechanism of moths with linear search and mutation operator. To evaluate the performance of IMFO, the IMFO algorithm is compared with other 20 algorithms on 23 benchmark functions and IEEE (Institute of Electrical and Electronics Engineers) CEC (Congress on Evolutionary Computation) 2014 benchmark test set. The comparative results show that the IMFO is effective and has good performance in terms of jumping out of local optimum, balancing exploitation ability and exploration ability. Moreover, the IMFO is also used to solve three engineering optimization problems, and it is compared with other well-known algorithms. The comparison results show that the IMFO algorithm can improve the global search ability of MFO and effectively solve the practical engineering optimization problems.																	0924-669X	1573-7497															10.1007/s10489-020-01793-2		JUL 2020											
J								Research on personalized recommendation hybrid algorithm for interactive experience equipment	COMPUTATIONAL INTELLIGENCE										collaborative filtering recommendation; content-based recommendation; interactive calligraphy experience equipment; waterfall hybrid way	INFORMATION; SYSTEM	Interactive calligraphy experience equipment has the characteristics of a large amount of data, various types, and strong homogeneity, which makes it difficult for users to find interesting resources. In this article, a hybrid personalized recommendation algorithm is proposed, which uses collaborative filtering and content-based recommendation methods in turn to make recommendations. In the initial recommendation, Latent Dirichlet Allocation (LDA) topic model is used to reduce the dimension of high-dimensional user behavior data and establish a user-writing theme matrix to reduce inaccurate recommendation caused by high sparsity data in collaborative filtering algorithm. The user interest list is obtained by calculating the similarity between users. Then, on the basis of the preliminary recommendation results, VGG16 model is used to extract the feature vector of the calligraphy image and calculate the similarity between the user's calligraphy words and the primary recommended calligraphy words, thus obtaining the final recommendation results. The experimental results verify the effectiveness and accuracy of the recommendation algorithm, which are better than other recommendation algorithms on the whole, and have important engineering guiding significance.																	0824-7935	1467-8640				AUG	2020	36	3					1348	1373		10.1111/coin.12375		JUL 2020											
J								Rooted Spanning Superpixels	INTERNATIONAL JOURNAL OF COMPUTER VISION										Superpixels; Segmentation; Spanning forest	IMAGE; SEGMENTATION; SHIFT	This paper proposes a new approach for superpixel segmentation. It is formulated as finding a rooted spanning forest of a graph with respect to some roots and a path-cost function. The underlying graph represents an image, the roots serve as seeds for segmentation, each pixel is connected to one seed via a path, the path-cost function measures both the color similarity and spatial closeness between two pixels via a path, and each tree in the spanning forest represents one superpixel. Originating from the evenly distributed seeds, the superpixels are guided by a path-cost function to grow uniformly and adaptively, the pixel-by-pixel growing continues until they cover the whole image. The number of superpixels is controlled by the number of seeds. The connectivity is maintained by region growing. Good performances are assured by connecting each pixel to the similar seed, which are dominated by the path-cost function. It is evaluated by both the superpixel benchmark and supervoxel benchmark. Its performance is ranked as the second among top performing state-of-the-art methods. Moreover, it is much faster than the other superpixel and supervoxel methods.																	0920-5691	1573-1405				DEC	2020	128	12					2962	2978		10.1007/s11263-020-01352-9		JUL 2020											
J								Defense against adversarial attacks by low-level image transformations	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										adversarial examples; deep neural networks; flip operation; image transformations; WebP compression		Deep neural networks (DNNs) are vulnerable to adversarial examples, which can fool classifiers by maliciously adding imperceptible perturbations to the original input. Currently, a large number of research on defending adversarial examples pay little attention to the real-world applications, either with high computational complexity or poor defensive effects. Motivated by this observation, we develop an efficient preprocessing module to defend adversarial attacks. Specifically, before an adversarial example is fed into the model, we perform two low-level image transformations, WebP compression and flip operation, on the picture. Then we can get a de-perturbed sample that can be correctly classified by DNNs. WebP compression is utilized to remove the small adversarial noises. Due to the introduction of loop filtering, there will be no square effect like JPEG compression, so the visual quality of the denoised image is higher. And flip operation, which flips the image once along one side of the image, destroys the specific structure of adversarial perturbations. By taking class activation mapping to localize the discriminative image regions, we show that flipping image may mitigate adversarial effects. Extensive experiments demonstrate that the proposed scheme outperforms the state-of-the-art defense methods. It can effectively defend adversarial attacks while ensuring only slight accuracy drops on normal images.																	0884-8173	1098-111X				OCT	2020	35	10					1453	1466		10.1002/int.22258		JUL 2020											
J								Computer-aided detection of brain tumor from magnetic resonance images using deep learning network	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Computer aided; Convolution neural network; Magnetic resonance imaging; Medical images; Tumor		Brain tumor can be considered to be a fatal and life-threatening disease caused by undesirable cell proliferation in the human brain. Out of the several diseases in the field of medical science, brain tumor turns out to be one of the most uncompromising problems. A tumor can be categorized as benign or malignant types in which benign tumors are non-cancerous while malignant tumors are cancerous tumors. There are numerous tumor detection methods but there are still more research going on in this area since quantitative analysis, an accurate disease diagnosis and detection of brain tumor are very much essential with scientific proofs. Therefore, timely planning can be prepared to save a person's life with brain tumor. This paper presents a computer aided approach with a 2D convolutional neural network for classifying the brain MRI images into two classes: Normal class and tumor class. In this paper, other classification methods are also used for comparison. The results are compared in terms of precision value, Recall value, F1-Score value. This proposed method shows better accuracy of 97% than the other methods.																	1868-5137	1868-5145															10.1007/s12652-020-02336-w		JUL 2020											
J								Task assignment strategy for multi-robot based on improved Grey Wolf Optimizer	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Task allocation; Multi-robot; Gray wolf algorithm; Kent chaos; MTSP problem	ALGORITHM	Multi-robot task allocation (MRTA) is the basis of a multi-robot system to perform tasks automatically, which directly affects the execution efficiency of the whole system. A distributed cooperative task allocation strategy based on the algorithm of the improved Grey Wolf Optimizer (IGWO) was proposed to quickly and effectively plan the cooperative task path with a large number of working task points. The MRTA problem was transformed into multiple traveling salesman problems (MTSPs), and the task target points were clustered by the K-means clustering algorithm and divided into several traveling salesman problems (TSPs). The Grey Wolf Optimizer (GWO) was improved by the Kent chaotic algorithm to initialize the population and enhance the diversity of the population. Furthermore, an adaptive adjustment strategy of the control parameter (a) over right arrow was proposed to balance exploration and exploitation. The individual speed and position updates in PSO were introduced to enable the gray wolf individual to preserve its optimal location information and accelerate the convergence speed. The IGWO was used to solve the optimal solutions to multiple TSP problems. Finally, the optimal solution space was integrated to get the optimal solution of MTSP, and 16 international classical test functions simulated the IGWO. The results showed that the IGWO algorithm has faster convergence speed and higher accuracy. The task allocation strategy is reasonable, with roughly equal path length, small planning cost, fast convergence speed, and excellent stability.																	1868-5137	1868-5145															10.1007/s12652-020-02224-3		JUL 2020											
J								Interaction between a motorized walker and its user: effects of force level on within-stride speed and direction fluctuations	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Smart walkers; gait cycle; center of mass oscillations; human-machine interaction	CENTER-OF-MASS; HUMAN WALKING; MOBILITY; GAIT	Motorized walkers are among assistive devices that are currently in development that aim to help older adults maintain their independence, which is of growing importance in today's ageing world. However, motorized walkers attempt to combine two different modes of generating forward motion: the user's bipedal gait and the motorized walker's wheel-driven locomotion. In contrast to the uninterrupted propulsion and continuous motion of wheel-driven locomotion, bipedal gait is cyclical, whereas propulsion occurs mainly in the push-off phase and the velocity and position of its center of mass oscillate within each gait cycle. This study aimed to explore the consequence of combining these two incompatible modes by investigating the effects of different force magnitudes applied by the motorized walker on the within-stride fluctuations generated from interactions with its user. When a constant force is applied by a motorized walker, regardless of whether the force is assistive or resistive, the magnitudes of the within-stride speed and directional fluctuations during the steady-state use increase as the magnitude of the applied force increases. The increase in fluctuations may be attributed to the conflict between the cyclical nature of walking and the continuous force applied.																	1868-5137	1868-5145															10.1007/s12652-020-02343-x		JUL 2020											
J								Machine learning algorithms for the prediction of non-metallic inclusions in steel wires for tire reinforcement	JOURNAL OF INTELLIGENT MANUFACTURING										Machine learning; Steel wire; Continuous casting; Non-metallic inclusions; Random Forest; Imbalanced dataset	SOLIDIFICATION; PRECIPITATION; OPTIMIZATION; STRENGTH; BEHAVIOR; MODEL	Non-metallic inclusions are unavoidably produced during steel casting resulting in lower mechanical strength and other detrimental effects. This study was aimed at developing a machine learning algorithm to classify castings of steel for tire reinforcement depending on the number and properties of inclusions, experimentally determined. 855 observations were available for training, validation and testing the algorithms, obtained from the quality control of the steel. 140 parameters are monitored during fabrication, which are the features of the analysis; the output is 1 or 0 depending on whether the casting is rejected or not. The following algorithms have been employed: Logistic Regression, K-Nearest Neighbors, Support Vector Classifier (linear and RBF kernels), Random Forests, AdaBoost, Gradient Boosting and Artificial Neural Networks. The reduced value of the rejection rate implies that classification must be carried out on an imbalanced dataset. Resampling methods and specific scores for imbalanced datasets (recall, precision and AUC rather than accuracy) were used. Random Forest was the most successful algorithm providing an AUC in the test set of 0.85. No significant improvements were detected after resampling. The optimized Random Forest allows the samples with a higher probability of being rejected to be selected, thus improving the effectiveness of the quality control. In addition, the optimized Random Forest has enabled to identify the most important features, which have been satisfactorily interpreted on a metallurgical basis.																	0956-5515	1572-8145															10.1007/s10845-020-01623-9		JUL 2020											
J								A Novel Combined Model for Short-Term Electric Load Forecasting Based on Whale Optimization Algorithm	NEURAL PROCESSING LETTERS										Short-term electric load forecasting; Electricity price forecasting; LSSVM; ELM; GRNN; WOA	ARTIFICIAL NEURAL-NETWORK; EXTREME LEARNING-MACHINE; MULTIOBJECTIVE OPTIMIZATION; GENETIC ALGORITHM; HYBRID; WAVELET; DECOMPOSITION; REGRESSION; SELECTION; SYSTEM	Stable electric load forecasting plays a significant role in power system operation and grid management. Improving the accuracy of electric load forecasting is not only a hot topic for energy managers and researchers of the power system, but also a fair challenging and difficult task due to its complex nonlinearity characteristics. This paper proposes a new combination model, which uses the least squares support vector machine, extreme learning machine, and generalized regression neural network to predict the electric load in New South Wales, Australia. In addition, the model employs a heuristic algorithm-whale optimization algorithm to optimize the weight coefficient. To verify the usability and generalization ability of the model, this paper also applies the proposed combined model to electricity price forecasting and compares it with the benchmark method. The experimental results demonstrate that the combined model not only can get accurate results for short-term electric load forecasting, but also achieves fine accuracy for the same period of electricity price forecasting.																	1370-4621	1573-773X				OCT	2020	52	2			SI		1207	1232		10.1007/s11063-020-10300-0		JUL 2020											
J								Adaptive differential evolution with a new joint parameter adaptation method	SOFT COMPUTING										Differential evolution; Evolutionary computation; Control parameter adaptation; Mutation strategy adaptation	ALGORITHM; OPTIMIZATION	Differential evolution (DE) is a population-based metaheuristic algorithm that has been proved powerful in solving a wide range of real-parameter optimization tasks. However, the selection of the mutation strategy and control parameters in DE is problem dependent, and inappropriate specification of them will lead to poor performance of the algorithm such as slow convergence and early stagnation in a local optimum. This paper proposes a new method termed as Joint Adaptation of Parameters in DE (JAPDE). The key idea lies in dynamically updating the selection probabilities for a complete set of pairs of parameter generating functions based on feedback information acquired during the search by DE. Further, for mutation strategy adaptation, the Rank-Based Adaptation (RAM) method is utilized to facilitate the learning of multiple probability distributions, each of which corresponds to an interval of fitness ranks of individuals in the population. The coupling of RAM with JAPDE results in the new RAM-JAPDE algorithm that enables simultaneous adaptation of the selection probabilities for pairs of control parameters and mutation strategies in DE. The merit of RAM-JAPDE has been evaluated on the benchmark test suit proposed in CEC2014 in comparison to many well-known DE algorithms. The results of experiments demonstrate that the proposed RAM-JAPDE algorithm outperforms or is competitive to the other related DE variants that perform mutation strategy and control parameter adaptation, respectively.																	1432-7643	1433-7479				SEP	2020	24	17					12801	12819		10.1007/s00500-020-05182-2		JUL 2020											
J								Modeling optical filters based on serially coupled microring resonators using radial basis function neural network	SOFT COMPUTING										Radial basis function neural network; Modeling; Optical filter; SFG; FSR	WAVE-GUIDE; RING-RESONATOR; DESIGN	In this research for the first time by using radial basis function neural network (RBFNN), filters based on serially coupled microring resonators have been modeled. Also, signal flow graph (SFG) method based on Mason's rule has been used to simulate filters. It has been represented when RBFNN has been learned, model can extract the outputs same as what was simulated by SFG method. It has been proved that RBFNN model can properly obtain results in several cases in which some parameters of filter like the order of filter; MRRs radius; coupling coefficients; and propagation loss have been changed. In these cases to design filter by an analytical method like the SFG, we need to obtain new transfer function. Obtaining novel transfer function would make filter designating complicated in terms of calculation and simulation time while the RBFNN can match with any change as fast as possible. The RBFNN has advantages of optimization ability, straightforward topological architecture, stable generalization ability, appropriate tolerance against input noise, online learning ability, accuracy in dynamically nonlinear approximation, predictability and fast and easy learning algorithms. These properties of RBFNN make it suitable to model pliable optical systems.																	1432-7643	1433-7479															10.1007/s00500-020-05170-6		JUL 2020											
J								Patient scheduling with deteriorating treatment duration and maintenance activity	SOFT COMPUTING										Patient scheduling; Deteriorating treatment duration; Maintenance; Crow search algorithm	RADIOTHERAPY TREATMENTS; MACHINE; ALGORITHM; JOBS; SEARCH; TIMES	This paper investigates a practical scheduling problem for radiotherapy patients, who are to be scheduled on different devices at different times. The treatment duration is increasing with time because of the continuously decaying effect of the radiation source, which also results in the decline of the serving ability. Therefore, the maintenance activity (replacing radiation source) is necessary to maintain the serving ability of medical institutions. The problem is to determine the schedule of all treatments and also when to have a maintenance activity, so as to minimize the maximum completion time of all the treatments on all devices. The lower bound of the problem is given in this paper. We prove that the optimal solution of the subproblem, i.e., scheduling patients on a single device, is independent of the sequence of the patients and is only related to the division of patients who are assigned before and after the maintenance, and thus, the subproblem can be converted to a two-partition problem. An improved dynamic programming algorithm is proposed to obtain an optimal scheme for this subproblem and its performance is better than other approaches. For multiple-device problem, an effective hybrid algorithm Gaussian crow search algorithm (GCSA) combined with crow search algorithm (CSA) and Gaussian distribution is proposed to assign all patients to different treatment devices. Finally, computational experiments demonstrate the effectiveness and stability of the proposed GCSA which is compared with CSA, simulated annealing (SA) and particle swarm optimization (PSO). The comparison results show that GCSA outperforms other algorithms in a feasible time.																	1432-7643	1433-7479															10.1007/s00500-020-05156-4		JUL 2020											
J								Transfer Learning-Based Framework for Classification of Pest in Tomato Plants	APPLIED ARTIFICIAL INTELLIGENCE											MANAGEMENT	Pest in the plant is a major challenge in the agriculture sector. Hence, early and accurate detection and classification of pests could help in precautionary measures while substantially reducing economic losses. Recent developments in deep convolutional neural network (CNN) have drastically improved the accuracy of image recognition systems. In this paper, we have presented a transfer learning of pre-trained deep CNN-based framework for classification of pest in tomato plants. The dataset for this study has been collected from online sources that consist of 859 images categorized into 10 classes. This study is first of its kind where: (i) dataset with 10 classes of tomato pest are involved; (ii) an exhaustive comparison of the performance of 15 pre-trained deep CNN models has been presented on tomato pest classification. The experimental results show that the highest classification accuracy of 88.83% has been obtained using DenseNet169 model. Further, the encouraging results of transfer learning-based models demonstrate its effectiveness in pest detection and classification tasks.																	0883-9514	1087-6545				NOV 9	2020	34	13					981	993		10.1080/08839514.2020.1792034		JUL 2020											
J								Probabilistic-based expressions in behavioral multi-attribute decision making considering pre-evaluation	FUZZY OPTIMIZATION AND DECISION MAKING										Pre-evaluation; Risk attitude; Regret theory; Probabilistic interval numbers; Probabilistic linguistic terms	PROSPECT-THEORY; REGRET THEORY; SETS; SELECTION; CHOICE	A behavioral multi-attribute decision making (BMADM) problem with probabilistic-based expressions is studied by considering decision-maker's (DM) risk attitude and pre-evaluation. With consideration of information expressions for uncertainty, probabilistic interval numbers (PINs) and probabilistic linguistic terms (PLTs) are utilized to depict pre-evaluation information with respect to quantitative and qualitative attributes, respectively. Then surrounding the two kinds of probabilistic-based expressions, we propose a BMADM method with DM's risk attitude being included based on regret theory. First, through taking into account characteristics of risk, we develop a basic utility function and a regret-rejoice function by considering risk-averse, risk-neutral and risk-seeking preference coefficients. Second, risk-based utility functions are examined for measuring PINs and PLTs. The third element is the establishment of optimization models for handling probability incompleteness to fully utilize the information. In the fourth step, a weighted comprehensive risk-based utility measurement is presented as a basis for making a selection. The final phase of the research is the application of the proposed method to one case, along with sensitivity and comparative analyses, as a means of illustrating the applicability and feasibility of the new method.																	1568-4539	1573-2908															10.1007/s10700-020-09335-8		JUL 2020											
J								Parameter reductions inN-soft sets and their applications indecision-making	EXPERT SYSTEMS										decision-making; parameter reduction; soft sets; N-soft sets	DECISION-MAKING; ADJUSTABLE APPROACH; ROUGH SETS; ALGORITHM; MODEL	Parameter reduction is an important operation for improving the performance of decision-making processes in various uncertainty theories. The theory ofN-soft sets is emerging as a powerful mathematical tool for dealing with uncertainties beyond the standard formulation of the soft set theory. In this research article, we extend the notion of parameter reduction toN-soft set theory, and we also justify its practical calculation. To this purpose, we define related theoretical concepts (e.g.N-soft subset, reductN-soft set and redundant parameter) and examine some of their fundamental properties. Then, we argue that the idea of attributes reduction from the rough set theory cannot be employed in theN-soft set theory in order to reduce the number of parameters. Consequently, we take an original position in order to adequately define and compute parameter reductions inN-soft sets. Finally, we develop an application of parameter reduction ofN-soft sets.																	0266-4720	1468-0394														e12601	10.1111/exsy.12601		JUL 2020											
J								Sparse common and distinctive covariates regression	JOURNAL OF CHEMOMETRICS										common and distinctive processes; data integration; multiblock data; principal covariates regression; variable selection	VARIABLE SELECTION; COMPONENTS-ANALYSIS; INFORMATION; PREDICTION; JOINT	Having large sets of predictors from multiple sources concerning the same observation units and the same criterion is becoming increasingly common in chemometrics. When analyzing such data, chemometricians often have multiple objectives: prediction of the criterion, variable selection, and identification of underlying processes associated to individual predictor sources or to several sources jointly. Existing methods offer solutions regarding the first two aims of uncovering the predictive mechanisms and relevant variables therein for a single block of predictor variables, but the challenge of uncovering joint and distinctive predictive mechanisms and the relevant variables therein in the multisource setting still needs to be addressed. To this end, we present a multiblock extension of principal covariates regression that aims to find the complex mechanisms in which several or single sources may be involved; taken together, these mechanisms predict an outcome of interest. We call this method sparse common and distinctive covariates regression (SCD-CovR). Through a simulation study, we demonstrate that SCD-CovR provides competitive solutions when compared with related methods. The method is also illustrated via an application to a publicly available dataset.																	0886-9383	1099-128X														e3270	10.1002/cem.3270		JUL 2020											
J								Restricted Boltzmann Machine-driven Interactive Estimation of Distribution Algorithm for personalized search	KNOWLEDGE-BASED SYSTEMS										Personalized search; Interactive Estimation of Distribution Algorithm; Restricted Boltzmann Machine; Surrogate	GENETIC ALGORITHMS	Effective and efficient personalized search is one of the most pursued objectives in the era of big data. The challenge of this problem lies in its complex quantifying evaluations and dynamic user preferences. A user-involved interactive evolutionary algorithm is a good choice if it has reliable preference surrogate and powerful evolutionary strategies. A Restricted Boltzmann Machine (RBM) assisted Interactive Estimation of Distribution Algorithm (IEDA) is presented to enhance the IEDA in solving the personalized search. Specifically, a dual-RBM module is developed to simultaneously provide a preference surrogate and a probability model for conducting the individual selection and generation of the IEDA. Firstly, the positive and negative preferences of the currently involved user in IEDA are distinguished and combined to achieve a dual-RBM, and then the weighted energy functions of the RBM model together with social group information from users with similar preferences are designed as the preference surrogate. The probability of the trained positive RBM on the visible units is fetched as the reproduction model of EDA since it reflects the attribute distributions of more preferred items. Some benchmarks from the Movielens and Amazon datasets are applied to experimentally demonstrate the superiority of the proposed algorithm in improving the efficiency and effectiveness of the interactive evolutionary computations served personalized search. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUL 20	2020	200								106030	10.1016/j.knosys.2020.106030													
J								Language model based interactive estimation of distribution algorithm	KNOWLEDGE-BASED SYSTEMS										Estimation of distribution algorithm; Interactive evolutionary algorithm; Language model; Personalized search; Bayesian inference	EVOLUTIONARY COMPUTATION	It is very hard, if not impossible to use analytical objective functions for optimization of personalized search due to the difficulties in mathematically describing qualitative problems. To solve such optimization problems, interactive evolutionary algorithms, which can make use of human preferences, are highly desirable. However, due to the lack of effective encoding methods, interactive evolutionary algorithms have been limited to numerically encoded optimization problems. In practice, however, linguistic terms (words) are the most natural expression of human preferences, and they are also commonly used to describe items in personalized search or E-commerce; therefore, language models better suit encoding, and the optimization of personalized search is converted into a dynamic document matching problem. To optimize word-described personalized search, we propose a novel interactive estimation of distribution algorithm. This algorithm combines a language model-based encoding approach, a Dirichlet-Multinomial compound distribution-based preference expression, and a Bayesian inference mechanism. The proposed algorithm is applied to two personalized search cases to demonstrate the capability of the algorithm in ensuring a more efficient and accurate search with less user fatigue. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUL 20	2020	200								105980	10.1016/j.knosys.2020.105980													
J								Joint imbalanced classification and feature selection for hospital readmissions	KNOWLEDGE-BASED SYSTEMS										Hospital readmission; Imbalanced classification; Feature selection; l(1)-norm regularization; Convex optimization	RISK; OPTIMIZATION; PREDICTION; FRAMEWORK; MODELS; SMOTE	Hospital readmission is one of the most important service quality measures. Recently, numerous risk assessment models have been proposed to address the hospital readmission problem. However, poor understanding of the class-imbalance hospital readmission data still challenges the development of accurate predictive models. To overcome the issue, a new risk prediction method termed joint imbalanced classification and feature selection (JICFS) is proposed for handling such a problem. To be specific, we construct the loss function within the large margin framework, in which the sample weight is involved to deal with the class imbalanced problem. Based on this, we design an optimization objective function involving l(1)-norm regularization for improving the performance, and an iterative scheme is proposed to solve the optimization problem, thereby achieving feature selection to improve the performance. Finally, experimental results on six real-world hospital readmission datasets demonstrate that the proposed algorithm has the advantage compared with some state-of-the-art methods. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUL 20	2020	200								106020	10.1016/j.knosys.2020.106020													
J								Cumulative belief peaks evidential K-nearest neighbor clustering	KNOWLEDGE-BASED SYSTEMS										Evidential clustering; Cumulative belief; Automatic detection; Evidential K-nearest neighbor	CLASSIFICATION; QUALITY	This paper introduces a new evidential clustering algorithm based on finding the "cumulative belief peaks" and evidential K-nearest neighbor rule. The basic assumption of this algorithm is that a cluster center has the highest cumulative possibility of becoming a cluster center among its neighborhood and size of its neighborhood is relatively large. To measure such cumulative possibility, a new notion of cumulative belief is proposed in the framework of belief functions. By maximizing an objective function, an appropriate size of the relatively large neighborhood is determined. Then, the objects with highest cumulative belief among their own neighborhood of this size are automatically detected as cluster centers. Finally, a credal partition is derived by evidential K-nearest neighbor rule with the fixed cluster center. Experimental results show that the proposed evidential clustering algorithm can automatically detect cluster centers and well reveal the data structure in form of a credal partition in tolerable time, when tackling datasets with small number of data objects and dimensions. As the sizes of datasets increase, running time of such new clustering algorithm increases sharply and this reduces the practicability of it. Simulations on synthetic and real-world datasets validate our conclusions. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUL 20	2020	200								105982	10.1016/j.knosys.2020.105982													
J								Systematic ensemble model selection approach for educational data mining	KNOWLEDGE-BASED SYSTEMS										e-learning; Student performance prediction; Educational data mining; Ensemble learning model selection; Gini index; p-value	PREDICTING ACADEMIC-PERFORMANCE	A plethora of research has been done in the past focusing on predicting student's performance in order to support their development. Many institutions are focused on improving the performance and the education quality; and this can be achieved by utilizing data mining techniques to analyze and predict students' performance and to determine possible factors that may affect their final marks. To address this issue, this work starts by thoroughly exploring and analyzing two different datasets at two separate stages of course delivery (20% and 50% respectively) using multiple graphical, statistical, and quantitative techniques. The feature analysis provides insights into the nature of the different features considered and helps in the choice of the machine learning algorithms and their parameters. Furthermore, this work proposes a systematic approach based on Gini index and p-value to select a suitable ensemble learner from a combination of six potential machine learning algorithms. Experimental results show that the proposed ensemble models achieve high accuracy and low false positive rate at all stages for both datasets. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUL 20	2020	200								105992	10.1016/j.knosys.2020.105992													
J								An improved Jaya algorithm for solving the flexible job shop scheduling problem with transportation and setup times	KNOWLEDGE-BASED SYSTEMS										Flexible job shop; Improved Jaya algorithm; Energy consumption; Transportation time; Setup time	MULTIOBJECTIVE OPTIMIZATION; DESIGN OPTIMIZATION; GENERATION	Flexible job shop scheduling has been widely researched due to its application in many types of fields. However, constraints including setup time and transportation time should be considered simultaneously among the realistic requirements. Moreover, the energy consumptions during the machine processing and staying at the idle time should also be taken into account for green production. To address this issue, first, we modeled the problem by utilizing an integer programming method, wherein the energy consumption and makespan objectives are optimized simultaneously. Afterward, an improved Jaya (IJaya) algorithm was proposed to solve the problem. In the proposed algorithm, each solution is represented by a two-dimensional vector. Consequently, several problem-specific local search operators are developed to perform exploitation tasks. To enhance the exploration ability, a SA-based heuristic is embedded in the algorithm. Meanwhile, to verify the performance of the proposed IJaya algorithm, 30 instances with different scales were generated and used for simulation tests. Six efficient algorithms were selected for detailed comparisons. The simulation results confirmed that the proposed algorithm can solve the considered problem with high efficiency. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUL 20	2020	200								106032	10.1016/j.knosys.2020.106032													
J								Mixture distribution modeling for scalable graph-based semi-supervised learning	KNOWLEDGE-BASED SYSTEMS										Semi-supervised Learning; Graph-based Learning; Mixture Distribution Modeling	REGULARIZATION; EFFICIENT	Graph-based semi-supervised learning (SSL) has been widely investigated in recent works considering its powerful ability to naturally incorporate the diverse types of information and measurements. However, traditional graph-based SSL methods have cubic complexities and leading to low scalability. In this paper, we propose to perform graph-based SSL on mixture distribution components, named Mixture-distribution based Graph Smoothing (MGS), to address this challenge. Specifically, the intrinsic distributions of data are captured by a mixture density estimation model. A novel mixture-distribution based objective energy function is further proposed to incorporate few available annotations, which ensures the model complexity is irrelevant to the number of raw instances. The energy function can be simplified and effectively solved by viewing the instances and mixture components as the point clouds. Experiments on large datasets demonstrate the remarkable performance improvements and scalability of the proposed model, which proves the superiority of the MGS model. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUL 20	2020	200								105974	10.1016/j.knosys.2020.105974													
J								Evolutionary feature transformation to improve prognostic prediction of hepatitis	KNOWLEDGE-BASED SYSTEMS										Machine learning; Decision support system; Hepatitis; Feature augmentation; Evolutionary algorithms; Genetic algorithm	FEATURE-SELECTION; GENETIC ALGORITHM; CLASSIFICATION; RISK	Despite advances in Machine Learning (ML) algorithms, the clinical viability of ML-based decision support systems (DSS) to predict the prognosis of hepatitis remains limited. However, an appropriate feature selection could improve its reliability. Differently from conventional feature reduction methods, we hypothesised that applying feature reduction first and then augmenting the reduced feature space could improve classification performance further. Thus, a novel two-stage Genetic Algorithm (GA)-based feature transformation method, which involves both feature reduction and augmentation (2-Tra-GA), was developed, tested and validated. This 2-Tra-GA was later coupled to ML-based classifiers for a prognostic prediction. Clinical data with nineteen (N = 19) features on 32O patients with hepatitis obtained from the University of California-Irvine ML repository were utilised. When tested on these data, the GA-based feature reduction resulted in a reduced set with fifteen (N=15) features that led to the highest classification accuracy and reliability. Augmenting the reduced set by adding transformed features via an interpolation method (N=32 features in total, 15 reduced and 17 transformed) further improved the classification performance. Additionally, the performance of this novel hybrid algorithm was evaluated against classifiers alike and published studies. Applying feature reduction, then augmenting only such relevant features improved classification performance and computational efficiency, also over conventional wrapper-based feature selection methods. Thus, a novel hybrid DSS to improve the reliability of the prediction of prognosis for hepatitis is proposed. Findings also support the application of the proposed hybrid method to improve clinical decision making. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUL 20	2020	200								106012	10.1016/j.knosys.2020.106012													
J								Quickly calculating reduct: An attribute relationship based approach	KNOWLEDGE-BASED SYSTEMS										Approximation quality; Attribute reduction; Attribute relationship; Granularity; Rough set	ROUGH SET; FEATURE-SELECTION; KNOWLEDGE GRANULATION; DECISION-MODEL; ACCELERATOR; DISTANCE; APPROXIMATIONS; GRANULES	Presently, attribute reduction, as one of the most important topics in the field of rough set, has been widely explored from different perspectives. To derive the qualified reduct defined in attribute reduction, forward greedy searching is frequently used. However, the previous researches indicate that such searching strategy may be still computationally expensive if the volume of data is large. In view of this, two frameworks are proposed by considering the relationships between attributes, which aim to accelerate the process of searching reducts. Our consideration is actually realized based on the dissimilarity and similarity between attributes, respectively. The main mechanisms are: (1) for the dissimilarity based approach, the combination of attributes with significant difference instead of one and only one attribute will be added into potential reduct in the process of searching reduct; (2) for the similarity based approach, the candidate attributes which are similar to those attributes in potential reduct will be tentatively ignored instead of being evaluated in the process of searching reduct. The experimental results over 16 UCI data sets demonstrate that whether single granularity or multi-granularity attribute reduction is considered, our proposed approaches can not only generate the reducts which may not lead to poorer performances, but also provide superior time efficiency of calculating reducts. This study suggests new trends for quickly computing reducts. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUL 20	2020	200								106014	10.1016/j.knosys.2020.106014													
J								A particular directional multilevel transform based method for single-image rain removal	KNOWLEDGE-BASED SYSTEMS										Rain removal; Directional multilevel transform; Split Bregman algorithm	REPRESENTATIONS	In this work, the wavelet based sparse optimization approach is introduced to process rain removal problem from a single image by considering the direction and shape of rain streaks. Firstly, a new kind of wavelet, the uni-directional multilevel system is construct to describe the singularities of noisy in particular direction and scales, like rain streaks and stripes in radar images. Compared with total variation, the uni-directional multilevel transform of rainy images gives the sparse representation of singularities in different scales and frequency bands due to its multiscale structure, which includes more rain details. Secondly, a convex optimization rain removal model is proposed by considering the intrinsic directional and structure information of the rain streak and the background image. The model involves three sparse priors, including the sparse regularizer on rain streaks and two sparse regularizers on the uni-directional multilevel transform of background layer in the rain drop's direction and the multilevel transform of rain streaks across the rain direction. The split Bregman algorithm is utilized to solve the proposed convex optimization model which ensures the global optimal solution. Thirdly, comparison tests with four stat-of-the-art methods are implemented on synthetic and real rainy images, which suggests that the proposed method is efficient both in rain removal and details preservation of the background layer. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUL 20	2020	200								106000	10.1016/j.knosys.2020.106000													
J								Multiview learning with variational mixtures of Gaussian processes	KNOWLEDGE-BASED SYSTEMS										Mixtures of Gaussian processes; Co-regularization; Multiview learning; Variational inference; Supervised learning		Gaussian processes (GPs) are powerful Bayesian nonparametric tools widely used in probabilistic modeling, and the mixture of GPs (MGPs) were introduced afterwards to make data modeling more flexible. However, MGPs are not directly applicable to multiview learning. In order to improve the modeling ability of MGPs, in this paper, we propose a new framework of multiview learning for the MGPs and instantiate it for classification. We make the divergence between views as small as possible while ensuring that the posterior probability of each view is as large as possible. Specifically, we regularize the posterior distribution of latent variables with the consistency of posterior distributions of the latent functions between different views. Since it is intractable to solve the model analytically, the variational inference and optimization algorithms of the classification model are also presented in this paper. Experimental results on multiple real-world datasets have shown that the proposed method has outperformed the original MGP model and several state-of-the-art multiview learning methods, which indicate the effectiveness of the proposed multiview learning framework for MGPs. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUL 20	2020	200								105990	10.1016/j.knosys.2020.105990													
J								Automated detection of kidney abnormalities using multi-feature fusion convolutional neural networks	KNOWLEDGE-BASED SYSTEMS										Kidney abnormalities; Convolutional neural networks; Multi-feature fusion; Medical image analysis	COMPUTER-AIDED DIAGNOSIS; ULTRASOUND; SEGMENTATION; DISEASE	Kidney abnormalities have a high incidence in people of all ages. The requisite manual examinations to detect these abnormalities are costly and time consuming. The rapid and accurate detection of kidney abnormalities has emerged as a focus of computer-aided medical research. Traditional methods work on images of kidneys and rely on hand-crafted features to identify abnormal symptoms as independent classes that lack generalization to different kidney diseases. In this study, an automated architecture to detect various kidney abnormalities is proposed that works on abdominal ultrasound images using convolutional neural networks. Our model consists of three components: the first selects appropriate ultrasound images of kidneys, the second is a detection model used to locate the area occupied by the kidney in the given image, and the third component using the multi-feature fusion neural network (Mf-Net) discriminates normal and abnormal kidneys, by determining whether there are abnormal symptoms in images. The detection model is combined with a weighted ensemble method to improve performance. A multi-feature fusion layer is also designed in the Mf-Net to extract distinctive features for multiple views of images. The three components work together to automatically recognize abnormalities associated with kidneys. A large dataset containing 3,722 abdominal ultrasound images with classification and localization annotations is established to train and evaluate the model. Experimental results show that the proposed ensemble detection model performs best with an average TPF of 98.0%, and the Mf-Net achieves an average classification accuracy of 94.67%. The obtained high classification and detection accuracy demonstrate the effectiveness of the proposed method for recognizing kidney abnormalities. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUL 20	2020	200								105873	10.1016/j.knosys.2020.105873													
J								A robust density peaks clustering algorithm with density-sensitive similarity	KNOWLEDGE-BASED SYSTEMS										DPC algorithm; Density-sensitive similarity; Automatic clustering; Clustering validity index	FAST SEARCH; FIND	Density peaks clustering (DPC) algorithm is proposed to identify the cluster centers quickly by drawing a decision-graph without any prior knowledge. Meanwhile, DPC obtains arbitrary clusters with fewer parameters and no iteration. However, DPC has some shortcomings to be addressed before it is widely applied. Firstly, DPC is not suitable for manifold datasets because these datasets have multiple density peaks in one cluster. Secondly, the cut-off distance parameter has a great influence on the algorithm, especially on small-scale datasets. Thirdly, the method of decision-graph will cause uncertain cluster centers, which leads to wrong clustering. To address these issues, we propose a robust density peaks clustering algorithm with density-sensitive similarity (RDPC-DSS) to find accurate cluster centers on the manifold datasets. With density-sensitive similarity, the influence of the parameters on the clustering results is reduced. In addition, a novel density clustering index (DCI) instead of the decision-graph is designed to automatically determine the number of cluster centers. Extensive experimental results show that RDPC-DSS outperforms DPC and other state-of-the-art algorithms on the manifold datasets. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUL 20	2020	200								106028	10.1016/j.knosys.2020.106028													
J								Improving the generalization performance of deep networks by dual pattern learning with adversarial adaptation	KNOWLEDGE-BASED SYSTEMS										Image classification; Deep neural networks; Domain adaptation	NEURAL-NETWORKS	In this paper, we present a dual pattern learning network architecture with adversarial adaptation (DPLAANet). Unlike conventional networks, the proposed network has two input branches and two loss functions. This architecture forces the network to learn robust features by analysing dual inputs. The dual input structure allows the network to have a considerably large number of image pairs, which can help address the overfitting issue due to limited training data. In addition, we propose to associate the two input branches with two random interest values during training. As a stochastic regularization technique, this method can improve the generalization performance. Moreover, we introduce to use the adversarial training approach to reduce the domain difference between fused image features and single image features. Extensive experiments on CIFAR-10, CIFAR-100, FI-8, the Google commands dataset, and MNIST demonstrate that our DPLAANets exhibit better performance than the baseline networks. The experimental results on subsets of CIFAR-10, CIFAR-100, and MNIST demonstrate that DPLAANets have a good generalization performance on small datasets. The proposed architecture can be easily extended to have more than two input branches. The experimental results on subsets of MNIST show that the architecture with three branches outperforms two branches when the training set is extremely small. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUL 20	2020	200								106016	10.1016/j.knosys.2020.106016													
J								Cross-regression for multi-view feature extraction	KNOWLEDGE-BASED SYSTEMS										Multi-view; Feature extraction; Cross-regression; L2,1-norm	CANONICAL CORRELATION-ANALYSIS; DIMENSIONALITY REDUCTION; EIGENFACES; ALGORITHM	The traditional multi-view feature extraction (MvFE) method usually seeks a latent common subspace where the samples from different views are maximally correlated. Recently, the regression-based method has become one of the most effective feature extraction methods. However, the existing regression-based methods are only suitable for single-view cases. In this paper, we firstly propose a new MvFE method named as cross-regression for MvFE (CRMvFE). CRMvFE designs a novel cross-regression regularization term to discover the relationship between multiple views in the original space, and simultaneously obtains the low-dimensional projection matrix for each view. Furthermore, inspired by the robustness of L2,1-norm, we also propose a robust CRMvFE (RCRMvFE) and an iterative algorithm to find the optimal solution. Theoretical analysis of the convergence and the relationship with CRMvFE demonstrate the effectiveness of the proposed RCRMvFE. Experiments on datasets show that the proposed CRMvFE and RCRMvFE have better performance than other related methods. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUL 20	2020	200								105997	10.1016/j.knosys.2020.105997													
J								Plant species recognition based on global-local maximum margin discriminant projection	KNOWLEDGE-BASED SYSTEMS										Plant species recognition; Dimensionality reduction; Maximum neighborhood margin discriminant projection (MNMDP); Global-local maximum margin discriminant projection (GLMMDP)		Plant species recognition using leaves is an important and challenging research topic, because the plant leaves are various and irregular and they have very large within-class difference and between-class similarity. Considering that leaves have different discriminant performance and contribution to plant recognition task, based on maximum neighborhood margin discriminant projection (MNMDP), we propose a global-local maximum margin discriminant projection (GLMMDP) algorithm for plant recognition. GLMMDP utilizes the local and class information and the global structure of the data to model the intra-class and inter-class neighborhood scatters and a global scatter, obtaining the projection matrix by minimizing the local intra-class scatter and meanwhile maximizing both the local inter-class scatter and the global between-class scatter. Compared with MNMDP, GLMMDP not only can detect the true intrinsic manifold structure of the data, but also can enhance the pattern discrimination between different classes by incorporating the global between-class scatter into MNMDP. The global between-class scatter fully indicates the difference and similarity between classes. The experimental results on the ICL (Intelligent Computing Laboratory) leaf datasets and Leafsnap leaf image datasets demonstrate the effectiveness of the proposed plant recognition method. The recognition accuracy is more than 95% on the ICL datasets and more than 90% on Leafsnap datasets. (C) 2020 Elsevier B.V. All rights reserved.																	0950-7051	1872-7409				JUL 20	2020	200								105998	10.1016/j.knosys.2020.105998													
J								Particle swarm optimization for trust relationship based social network group decision making under a probabilistic linguistic environment	KNOWLEDGE-BASED SYSTEMS										Group decision making (GDM); Trust; Particle swarm optimization (PSO); Probabilistic linguistic term set (PLTS); Consensus reaching process (CRP)	CONSENSUS REACHING PROCESS; TERM SETS; MODEL	Group decision making (GDM) problems require consensus reaching processes; however, these can be time consuming and costly. As experts change their evaluations after exchanging opinions and being influenced by others, these influences are spread across the various expert trust relationships. Because of the experts' knowledge limits, the evaluations on the alternatives and the trust relationships are generally described using probabilistic linguistic terms. Therefore, to simplify the decision making process and avoid decision bias, this paper proposes a particle swarm optimization method that incorporates a trust relationship based social network for GDM under a probabilistic linguistic environment. Each expert is regarded as a particle that moves toward the final evaluation and reaches the threshold. A fitness function is built to measure the consensus levels, and the updated function is improved by the trust relationships to derive the new evaluations. A numerical example is then given to illustrate the feasibility of the proposed approach and comparisons given to further elucidate its novelty and validity. (C) 2020 Published by Elsevier B.V.																	0950-7051	1872-7409				JUL 20	2020	200								105999	10.1016/j.knosys.2020.105999													
J								Symplectic interactive support matrix machine and its application in roller bearing condition monitoring	NEUROCOMPUTING										Support matrix machine; Symplectic geometry; Interactive hyperplane; Roller bearing; Condition monitoring	CONVOLUTIONAL NEURAL-NETWORK; FAULT-DIAGNOSIS; ROLLING BEARINGS; CLASSIFICATION; DECOMPOSITION; FRAMEWORK	Support matrix machine (SMM) is an effective method to solve the problem of mechanical condition monitoring while the matrix is taken as the input. It makes full use of the effective information between rows and columns of the matrix to establish an ideal prediction model and achieve good condition monitoring results. However, Similar to support vector machine (SVM), the core principle of the SMM is to distinguish the data effectively by two parallel hyperplanes. Unfortunately, two parallel hyperplanes may not be able to maximize the interval. Therefore, the concept of interactive support matrix machine (ISMM) is proposed, which constructs a pair of interactive hyperplanes to maximize the interval between two types of data. Interactive hyperplanes may be more able to distinguish between two types of data, so that each hyperplane is as close as to one of the two types and as far away as possible from the other. However, the input of the model often contain noise information, which seriously interferes with the classification results. Therefore, a symplectic interactive support matrix machine (SISMM) method is further proposed, which combines symplectic geometry similarity transformation (SGST) with ISMM. In SISMM, it can directly get the symplectic geometry coefficient matrix without noise from the original signal, and intelligent classification recognition is realized. By analyzing and comparing the signal of roller bearings, the results show that the proposed method has better recognition performance and it is feasible for roller bearing condition monitoring. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 20	2020	398						1	10		10.1016/j.neucom.2020.01.074													
J								Enhancing the discriminative feature learning for visible-thermal cross-modality person re-identification	NEUROCOMPUTING										Person re-identification; Visible-thermal; Cross-modality discrepancy; Enhancing the discriminative feature learning	RANKING	Existing person re-identification has achieved great progress in the visible domain, capturing all the person images with visible cameras. However, in a 24-hour intelligent surveillance system, the visible cameras may be noneffective at night. In this situation, thermal cameras are the best supplemental components, which capture images without depending on visible light. Therefore, in this paper, we investigate the visible-thermal cross-modality person re-identification (VT Re-ID) problem. In VT Re-ID, there are two knotty problems should be well handled, cross-modality discrepancy and intra-modality variations. To address these two issues, we propose focusing on enhancing the discriminative feature learning (EDFL) with two extremely simple means from two core aspects, (1) skip-connection for mid-level features incorporation to improve the person features with more discriminability and robustness, and (2) dual-modality triplet loss to guide the training procedures by simultaneously considering the cross-modality discrepancy and intra-modality variations. Additionally, the two-stream CNN structure is adopted to learn the multi-modality sharable person features. The experimental results on two datasets show that our proposed EDFL approach distinctly outperforms state-of-the-art methods by large margins, demonstrating the effectiveness of our EDFL to enhance the discriminative feature learning for VT Re-ID. (C) 2020 Published by Elsevier B.V.																	0925-2312	1872-8286				JUL 20	2020	398						11	19		10.1016/j.neucom.2020.01.089													
J								Triple loss for hard face detection	NEUROCOMPUTING										Face detection; Small face; Face feature fusion; Single shot detection; Efficiency-accuracy balance		Although face detection has been well addressed in the last decades, despite the achievements in recent years, effective detection of small, blurred and partially occluded faces in the wild remains a challenging task. Meanwhile, the trade-off between computational cost and accuracy is also an open research problem in this context. To tackle these challenges, in this paper, a novel context enhanced approach is proposed with structural optimization and loss function optimization. For loss function optimization, we introduce a hierarchical loss, referring to "triple loss" in this paper, to optimize the feature pyramid network (FPN) (Lin et al., 2017) based face detector. Additional layers are only applied during the training process. As a result, the computational cost is the same as FPN during inference. For structural optimization, we propose a context sensitive structure to increase the capacity of the prediction network to improve the accuracy of the output. In details, a three-branch inception subnet (Szegedy et al., 2015) based feature fusion module is employed to refine the original FPN without increasing the computational cost significantly, further improving low-level semantic information, which is originally extracted from a single convolutional layer in the backward pathway of FPN. The proposed approach is evaluated on two publicly available face detection benchmarks, FDDB and WIDER FACE. By using a VGG-16 based detector, experimental results indicate that the proposed method achieves a good balance between the accuracy and computational cost of face detection. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 20	2020	398						20	30		10.1016/j.neucom.2020.02.060													
J								Enhanced sparse filtering with strong noise adaptability and its application on rotating machinery fault diagnosis	NEUROCOMPUTING										Intelligent fault diagnosis; Normalization; Enhanced sparse filtering; Hankel matrix; Anti-noise	CONVOLUTIONAL NEURAL-NETWORK; FEATURE-SELECTION; LEARNING-METHOD; RECOGNITION	Intelligent fault diagnosis is an effective method to guarantee the continuous and efficient operation of rotating machinery. Compared with the experimental environment, noise is inevitable in real word industrial applications, which causes serious degradation of the performance of intelligent fault diagnosis methods. In view of this, this study aims to provide a method that could accurately diagnose faults under noisy environment. In this paper, we firstly discuss the characteristics of normalization and the feature extracting process of sparse filtering. Then, we propose a novel method based on the L-3/2-norm, Hankel-training matrix, normalized weight matrix and feature normalization for rotating machinery fault diagnosis under noisy environment. The proposed method is applied to the fault diagnosis of rolling bearing and planetary gearbox with noise interference. The verification results confirm that the proposed method is a promising tool that shows strong noise adaptability using the training of original datasets without any time-consuming denoising preprocessing. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 20	2020	398						31	44		10.1016/j.neucom.2020.02.042													
J								Build a compact binary neural network through bit-level sensitivity and data pruning	NEUROCOMPUTING										Binary neural networks; Deep neural networks; Deep learning; Neural network compression		Due to the high computational complexity and memory storage requirement, it is hard to directly deploy a full-precision convolutional neural network (CNN) on embedded devices. The hardware-friendly designs are needed for resource-limited and energy-constrained embedded devices. Emerging solutions are adopted for the neural network compression, e.g., binary/ternary weight network, pruned network and quantized network. Among them, binary neural network (BNN) is believed to be the most hardware-friendly framework due to its small network size and low computational complexity. No existing work has further shrunk the size of BNN. In this work, we explore the redundancy in BNN and build a compact BNN (CBNN) based on the bit-level sensitivity analysis and bit-level data pruning. The input data is converted to a high dimensional bit-sliced format. In the post-training stage, we analyze the impact of different bit slices to the accuracy. By pruning the redundant input bit slices and shrinking the network size, we are able to build a more compact BNN. Our result shows that we can further scale down the network size of the BNN up to 3.9x with no more than 1% accuracy drop. The actual runtime can be reduced up to 2x and 9.9x compared with the baseline BNN and its full-precision counterpart, respectively. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 20	2020	398						45	54		10.1016/j.neucom.2020.02.012													
J								Document-level emotion detection using graph-based margin regularization	NEUROCOMPUTING										Sentiment classification; Polarity graph; Unstructured text; Margin-based classifier	SENTIMENT ANALYSIS; TOPIC MODEL; LEXICON; CLASSIFICATION; NETWORK	Sentiment analysis aims to automatically detect the underlying attitudes that users express. For the documents with complex unstructured data, such as reviews, emojis and surveys, it is usually hard to precisely identify the real emotion. Thus, it becomes urgent, yet challenging, to develop a technique that can process and make use of the unstructured information. In this article, we consider sentiment classification for those unstructured features extracted from texts. We propose a regularization-based framework to pursue better classification performance by (1) introducing polarity shifters assembled with sentiment words to create novel bigram features and (2) simultaneously constructing a constraint graph to encode the relative polarity among unstructured features to improve the parameter estimation procedure. Under these settings, our approach can uncover the intrinsic semantic information from the unstructured text data. Theoretically, we justify its underlying equivalent connection with the standard Bayes classifier, which is ideally optimal when the sample distribution is known. Moreover, we show that our new method yields better generalization ability due to the reduced solution search space and the appealing asymptotic consistency. The superior performance from real data experiments demonstrates the robustness and effectiveness of the proposed method. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 20	2020	398						55	63		10.1016/j.neucom.2020.01.059													
J								Understanding dropout as an optimization trick	NEUROCOMPUTING										Deep learning; Dropout; Activation function	NEURAL-NETWORKS	As one of standard approaches to train deep neural networks, dropout has been applied to regularize large models to avoid overfitting, and the improvement in performance by dropout has been explained as avoiding co-adaptation between nodes. However, when correlations between nodes are compared after training the networks with or without dropout, one question arises if co-adaptation avoidance explains the dropout effect completely. In this paper, we propose an additional explanation of why dropout works and propose a new technique to design better activation functions. First, we show that dropout can be explained as an optimization technique to push the input towards the saturation area of nonlinear activation function by accelerating gradient information flowing even in the saturation area in backpropagation. Based on this explanation, we propose a new technique for activation functions, gradient acceleration in activation function (GAAF), that accelerates gradients to flow even in the saturation area. Then, input to the activation function can climb onto the saturation area which makes the network more robust because the model converges on a flat region. Experiment results support our explanation of dropout and confirm that the proposed GAAF technique improves image classification performance with expected properties. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 20	2020	398						64	70		10.1016/j.neucom.2020.02.067													
J								Weakly supervised easy-to-hard learning for object detection in image sequences	NEUROCOMPUTING										Weakly supervised; Easy-to-hard learning; Spatio-temporal consistency	TRACKING	Object detection is an important research problem in computer vision. Convolutional Neural Networks (CNN) based deep learning models could be used for this problem, but it would require a large number of manual annotated objects for training or fine-tuning. Unfortunately, fine-grained manually annotated objects are not available in many cases. Usually, it is possible to obtain imperfect initialized detections by some weak object detectors using some weak supervisions like the prior knowledge of shape, size or motion. In some real-world applications, objects have little inter-occlusions and split/merge difficulties, so the spatio-temporal consistency in object tracking are well preserved in the image sequences/videos. Starting from the imperfect initialization, this paper proposes a new easy-to-hard learning method to incrementally improve the object detection in image sequences/videos by an unsupervised spatio-temporal analysis which involves more complex examples that are hard for object detection for next-iteration training. The proposed method does not require manual annotations, but uses weak supervisions and spatio-temporal consistency in tracking to simulate the supervisions in the CNN training. Experimental results on three different tasks show significant improvements over the initialized detections by the weak object detectors. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 20	2020	398						71	82		10.1016/j.neucom.2020.02.075													
J								Forward and backward input variable selection for polynomial echo state networks	NEUROCOMPUTING										Polynomial echo state network; Forward selection; Backward selection; Input variable selection	PARTICLE SWARM OPTIMIZATION; MUTUAL INFORMATION; NEURAL-NETWORK; MACHINE	As extension of traditional echo state networks (ESNs), the polynomial echo state networks (PESNs) have been proposed in our previous work (Yang et al., 208) by employing the polynomial function of complete input variable as output weight matrix. In practice, the generalization performance and computational burden of PESNs are perturbed by redundant or irrelevant inputs. To construct output weights with a suitable subset of input variables, the forward selection based PESN (FS-PESN) and backward selection based PESN (BS-PESN) are proposed. Firstly, the forward selection method is used in FS-PESN to choose the input variable which incurs the maximum reduction on objective function, and the backward selection shame is introduced in BS-PESN to remove the input variable which leads to the smallest increment on objective function. Then, the iterative updating strategies are designed to avoid repetitive computations in FS-PESN and BS-PESN. Specially, an accelerating scheme is introduced into BS-PESN to simplify training process. Finally, numerical simulations are carried out to illustrate effectiveness of the proposed techniques in terms of generalization ability and testing time. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 20	2020	398						83	94		10.1016/j.neucom.2020.02.034													
J								Automated diagnosis of neonatal encephalopathy on aEEG using deep neural networks	NEUROCOMPUTING										Deep neural networks; Amplitude-integrated; electroencephalography; Feature enhancing mechanism; Feature combination operator; Medical image analysis	AMPLITUDE-INTEGRATED EEG; SEIZURES; ELECTROENCEPHALOGRAPHY; CLASSIFICATION	Amplitude-integrated electroencephalography, called aEEG clinically, is an effective approach for monitoring neonatal cerebral function. Early aEEG screening is necessary to diagnose neonatal encephalopathy. The electroencephalography (EEG) signal contains lots of information about infant neurological status and prognosis. Traditional computer-aided diagnosis methods pay more attention to process EEG signal data, while doctors referring more to aEEG screening images in clinical diagnosis. In addition, clinical diagnosis of neonatal encephalopathy is a large challenge in low-level medical resource settings where few neonatologists are available to care for those newborns with cerebral disease. As a result, a method of automated diagnosing EEG-related images is requested. This paper presents an automated neonatal encephalopathy diagnosis model for aEEG images, which is divided into two sub-networks. The front part is utilized to extract high-dimensional features from multiple parts of an image. And the second portion aims to accomplish classification. There is a feature combination operator between the two parts. The feature enhancing mechanism can protrude the important region and achieve a better result. The annotated datasets, containing more than 600 aEEG screening images, are established to train and evaluate the novel model. All the annotations in our datasets are verified by case reports. Our proposed method comes up to the performances of professional human neonatologists, which can be applied to clinical diagnosis. (C) 2020 Published by Elsevier B.V.																	0925-2312	1872-8286				JUL 20	2020	398						95	107		10.1016/j.neucom.2020.01.057													
J								On a granular functional link network for classification	NEUROCOMPUTING										High-dimensional data; Fuzzy sets; Granulation; B-spline; Convergence	NEURAL-NETWORK; INTERPRETABILITY	In this paper, we present a new granular classifier in two versions (iterative and non-iterative), by adopting some ideas originating from a kind of Functional Link Artificial Neural Network and the Functional Network schemes. These two architectures are substantially the same: they both use a function basis instead of the usual activation function, but they are different for the learning algorithm. We augment them from the perspective of Granular Computing and information granules, designing a new kind of classifier and two learning algorithms, by taking into account granularity of information. The proposed classifier exhibits the advantages of the granular architectures, that is higher accuracy and transparency. We formally discuss the convergence of the iterative learning scheme. We carry out some numerical experiments using publicly available data, by comparing the results against those results produced by the state-of-the-art methods. In particular, we achieved sound results by invoking the iterative learning scheme. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 20	2020	398						108	116		10.1016/j.neucom.2020.02.090													
J								Texture feed based convolutional neural network for pansharpening	NEUROCOMPUTING										Pansharpening; 3D Gabor filter; Shearlet transform; CNN	MULTIRESOLUTION ANALYSIS; FUSION; MS	Fusion of panchromatic (PAN) and multispectral (MS) images, called pansharpening, is one of the main challenging problems in remote sensing. Convolutional neural network (CNN) based pansharpening has been introduced in some works recently. The represented frameworks often use a multi-layer CNN for fusion of MS and PAN images. Limited number of training samples and the high number of network weights to be determined may cause overfitting problem. To deal with this difficulty, a simple structure comprised from two single layer convolutional networks is proposed in this paper. The shortage of deleted layers is compensated by applying 3D Gabor filters and shearlet transform in addition to nonlinear kernel based principal component analysis. The proposed framework not only has a simple structure, but also is superior to state-of-the-art CNN based pansharpening methods in terms of various quality assessment measures. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 20	2020	398						117	130		10.1016/j.neucom.2020.02.083													
J								Neural-network-based adaptive backstepping control for a class of unknown nonlinear time-delay systems with unknown input saturation	NEUROCOMPUTING										Nonlinear systems; Input saturation; Time-varying delay; Adaptive backstepping control; Neural networks	OUTPUT-FEEDBACK CONTROL; DYNAMIC SURFACE CONTROL; TRACKING CONTROL; VARYING INPUT; STABILITY ANALYSIS; LINEAR-SYSTEMS; FUZZY CONTROL; DEAD-ZONE; DESIGN	In this paper, a neural-network-based adaptive backstepping control scheme is developed for a class of unknown nonlinear systems with unknown time-varying delayed states and unknown saturated delayed input. In the proposed method, radial basis function neural network is adopted to approximate the unknown nonlinear functions. An adaptive backstepping design is employed to compensate for the unknown time-varying delays in states. In addition, to overcome the effects of input delay, an auxiliary dynamic system is constructed. Moreover, a novel adaptive law is used for estimating unknown bounds of saturation function. Using the Lyapunov stability theorem, it is proven that the closed-loop system is semi-globally uniformly ultimately bounded and the tracking error can converge to small desired value by the proper choose of design parameters. Finally, the effectiveness of the proposed method is illustrated by applying it to a chemical reactor recycle system, one-link manipulator actuated by a DC motor and a Brusselator model. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 20	2020	398						131	152		10.1016/j.neucom.2020.02.070													
J								An improved discrete backtracking searching algorithm for fuzzy multiproduct multistage scheduling problem	NEUROCOMPUTING										Multiproduct multistage scheduling problem; Fuzzy processing time; Discrete backtracking searching algorithm; Global best solution	INTEGER PROGRAMMING-MODELS; MIXED-INTEGER; GENETIC ALGORITHM; BATCH; FORMULATION; FRAMEWORK	In this study, we address the fuzzy multiproduct multistage scheduling problem (FMMSP). The FMMSP with the objective of minimizing makespan is close to practical manufacturing circumstances in terms of the uncertainty in processing time. Backtracking searching algorithm (BSA) is a new proposed meta-algorithm for continuous optimization problems. To solve the FMMSP, an improved discrete BSA, called discrete BSA with local search (DBSA-LS), is developed. The information of the global best solution is incorporated into the mutation process to improve the convergence speed of discrete BSA. A local search related to the FMMSP enhances the exploitation ability of the improved discrete BSA. Left shift operation is applied to improve the solution quality when a hybrid encoding string is decoded into the scheduling scheme. The influence of the key parameter on the performance of the proposed algorithm is tested using different size instances. Sixteen different scales of instances are used to evaluate the performance of the proposed DBSA-LS. Several comparisons are conducted between DBSA-LS and three other algorithms. The results show the effectiveness of the proposed DBSA-LS. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 20	2020	398						153	165		10.1016/j.neucom.2020.02.066													
J								A new result on L-2-L-proportional to performance state estimation of neural networks with time-varying delay	NEUROCOMPUTING										L-2-L-infinity performance; State estimation; Time delay; Neural networks; Second-order Bessel-Legendre inequality	STABILITY ANALYSIS; LINEAR-SYSTEMS; INEQUALITY; DESIGN	This paper is concerned with the L-2-L-infinity performance state estimation problem of delayed neural networks. Firstly, the second-order Bessel-Legendre inequality based on reciprocally convex approach is proposed. Secondly, based on the improved integral inequality, a new delay-dependent condition is derived, which ensures the asymptotic stability of estimation error system with L-2-L-infinity performance. As a result, the estimator gain matrix and the optimal L-2-L-infinity performance level are obtained. Simulation results are finally shown to illustrate the effectiveness of the proposed approach. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 20	2020	398						166	171		10.1016/j.neucom.2020.02.059													
J								Dreaming machine learning: Lipschitz extensions for reinforcement learning on financial markets	NEUROCOMPUTING										Pseudo-metric; Reinforcement learning; Lipschitz extension; Mathematical economics; Financial market; Model	NEURAL-NETWORKS; TIME-SERIES; TRADING SYSTEM; CLASSIFICATION; STRATEGIES; PREDICTION; INDEX	We consider a quasi-metric topological structure for the construction of a new reinforcement learning model in the framework of financial markets. It is based on a Lipschitz type extension of reward functions defined in metric spaces. Specifically, the McShane and Whitney extensions are considered for a reward function which is defined by the total evaluation of the benefits produced by the investment decision at a given time. We define the metric as a linear combination of a Euclidean distance and an angular metric component. All information about the evolution of the system from the beginning of the time interval is used to support the extension of the reward function, but in addition this data set is enriched by adding some artificially produced states. Thus, the main novelty of our method is the way we produce more states-which we call "dreams"-to enrich learning. Using some known states of the dynamical system that represents the evolution of the financial market, we use our technique to simulate new states by interpolating real states and introducing some random variables. These new states are used to feed a learning algorithm designed to improve the investment strategy by following a typical reinforcement learning scheme. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 20	2020	398						172	184		10.1016/j.neucom.2020.02.052													
J								Sparse low rank factorization for deep neural network compression	NEUROCOMPUTING										Low-rank approximation; Singular value decomposition; Sparse matrix; Deep neural networks; Convolutional neural networks		Storing and processing millions of parameters in deep neural networks is highly challenging during the deployment of model in real-time application on resource constrained devices. Popular low-rank approximation approach singular value decomposition (SVD) is generally applied to the weights of fully connected layers where compact storage is achieved by keeping only the most prominent components of the decomposed matrices. Years of research on pruning-based neural network model compression revealed that the relative importance or contribution of each neuron in a layer highly vary among each other. Recently, synapses pruning has also demonstrated that having sparse matrices in network architecture achieve lower space and faster computation during inference time. We extend these arguments by proposing that the low-rank decomposition of weight matrices should also consider significance of both input as well as output neurons of a layer. Combining the ideas of sparsity and existence of unequal contributions of neurons towards achieving the target, we propose sparse low rank (SLR) method which sparsifies SVD matrices to achieve better compression rate by keeping lower rank for unimportant neurons. We demonstrate the effectiveness of our method in compressing famous convolutional neural networks based image recognition frameworks which are trained on popular datasets. Experimental results show that the proposed approach SLR outperforms vanilla truncated SVD and a pruning baseline, achieving better compression rates with minimal or no loss in the accuracy. Code of the proposed approach is avaialble at https://github.com/sridarah/slr. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 20	2020	398						185	196		10.1016/j.neucom.2020.02.035													
J								Deep residual network for highly accelerated fMRI reconstruction using variable density spiral trajectory	NEUROCOMPUTING										Image reconstruction; fMRI; Compressed sensing; Deep Learning	COMPRESSED SENSING RECONSTRUCTION; SPATIAL-RESOLUTION; MRI; IMAGE; BRAIN; CONTRAST	Compressed sensing has proved itself as a useful technique for accelerating time-consuming fMRI acquisition. However, its intrinsic iterative algorithm of solving optimization problems limits its practical usage. In addition, it may still suffer from residual errors and artifacts when more aggressive acceleration factors are adopted. Deep neural networks have recently shown great potential in computer vision and image processing. Nevertheless, few attempts have been made concerning fMRI reconstruction. In this paper, we propose a deep residual network for faster and better image reconstruction with 20x acceleration. The network is made up of various residual blocks, and is trained to identify the mapping relationship between an aliased image and a fully-recovered image. Mean square error criterion refined with data-consistency loss is employed to evaluate the 'distance' between the network output and ground truth. Results showed that the proposed method can achieve superior image quality and better preserves dynamic features than other state-of-art methods. In addition, the reconstruction can be extremely fast with a one-way deployment on a feed-forward network. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 20	2020	398						338	346		10.1016/j.neucom.2019.02.070													
J								Deep learning-based image super-resolution considering quantitative and perceptual quality	NEUROCOMPUTING										Perceptual super-resolution; Deep learning; Aesthetics; Image quality		Recently, it has been shown that in super-resolution, there exists a tradeoff relationship between the quantitative and perceptual quality of super-resolved images, which correspond to the similarity to the ground-truth images and the naturalness, respectively. In this paper, we propose a novel super-resolution method that can improve the perceptual quality of the upscaled images while preserving the conventional quantitative performance. The proposed method employs a deep network for multi-pass upscaling in company with a discriminator network and two qualitative score predictor networks. Experimental results demonstrate that the proposed method achieves a good balance of the quantitative and perceptual quality, showing more satisfactory results than existing methods. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 20	2020	398						347	359		10.1016/j.neucom.2019.06.103													
J								Mixed-dense connection networks for image and video super-resolution	NEUROCOMPUTING										Super-resolution; Deep learning; Residual networks; Dense connection; Video super-resolution	SINGLE IMAGE; SPARSE; RECONSTRUCTION; REGULARIZATION	Efficiency of gradient propagation in intermediate layers of convolutional neural networks is of key importance for super-resolution task. To this end, we propose a deep architecture for single image super-resolution (SISR), which is built using efficient convolutional units we refer to as mixed-dense connection blocks (MDCB). The design of MDCB combines the strengths of both residual and dense connection strategies, while overcoming their limitations. To enable super-resolution for multiple factors, we propose a scale-recurrent framework which reutilizes the filters learnt for lower scale factors recursively for higher factors. This leads to improved performance and promotes parametric efficiency for higher factors. We train two versions of our network to enhance complementary image qualities using different loss configurations. We further employ our network for video super-resolution task, where our network learns to aggregate information from multiple frames and maintain spatio-temporal consistency. The proposed networks lead to qualitative and quantitative improvements over state-of-the-art techniques on image and video super-resolution benchmarks. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 20	2020	398						360	376		10.1016/j.neucom.2019.02.069													
J								Deep recursive up-down sampling networks for single image super-resolution	NEUROCOMPUTING										Single image super-resolution (SISR); Deep learning; Recursive network; De-convolutional layer		Single image super-resolution (SISR) technology can reconstruct a high-resolution (HR) image from the corresponding low-resolution (LR) image. The emergence of deep learning pushes SISR to a new level. The successful application of the recursive network motivates us to explore a more efficient SISR method. In this paper, we propose the deep recursive up-down sampling networks (DRUDN) for SISR. In DRUDN, an original LR image is directly fed without extra interpolation. Then, we use the sophisticated recursive up-down sampling blocks (RUDB) to learn the complex mapping between the LR image and the HR image. At the reconstruction part, the feature map is up-scaled to the ideal size by a de-convolutional layer. Extensive experiments demonstrate that DRUDN outperforms the state-of-the-art methods in both subjective effects and objective evaluation. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 20	2020	398						377	388		10.1016/j.neucom.2019.04.004													
J								Deep fractal residual network for fast and accurate single image super resolution	NEUROCOMPUTING										Image super-resolution; Deep convolutional network; Fractal block; Residual learning; Weight-sharing	SUPERRESOLUTION	Recently, deep convolutional neural networks (CNNs) have achieved excellent performance improvement in Single Image Super-Resolution (SISR) tasks. However, it is still difficult for most of the existing CNN-based models to learn and make full use of adequate hierarchical features, which is crucial to ideally recover both high-frequency and low-frequency information. Generally, many networks tend to incorporate numerous parameters aiming to improve their learning capability, which demands more storage and is less applicable to mobile systems. To address this issue, we propose a novel method for SISR by using a deep fractal residual network (DFRN) composed of a series of fractal blocks containing multiple paths for feature extraction. The fractal blocks learn and combine different hierarchical features to generate finer features favoring the reconstruction of high resolution (HR) images. Moreover, we integrate both local and global residual learning to preserve the low-level features and decrease the difficulty of training. Finally, we propose a weight-sharing version with fewer parameters to reduce the space complexity while keeping comparable performance. Experimental results demonstrate that DFRN has the capability of learning different hierarchical features and outperforms other state-of-the-art methods for SISR by reconstructing low resolution (LR) images into the HR images quickly and accurately. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				JUL 20	2020	398						389	398		10.1016/j.neucom.2019.09.093													
J								SCRSR: An efficient recursive convolutional neural network for fast and accurate image super-resolution	NEUROCOMPUTING										Super-resolution; Recursive convolutional neural networks; Efficient model		Convolutional neural networks have recently demonstrated high-quality reconstruction for single image super-resolution (SR). These CNN networks effectively recover a high-resolution (HR) image from a low-resolution (LR) image, at the cost of enormous parameters and heavy computational burden. In this work, we propose a recursive efficient deep convolutional network for fast and accurate single-image SR with only 0.28M parameters. A Split-Concatenate-Residual (SCR) block is proposed to reduce computation and parameters. With downsampling block and upsampling block, we significantly reduce computational complexity and enlarge the size of the receptive field. Specifically, two-level recursive learning is proposed which can improve accuracy by increasing depth without adding any weight parameters. We also employ local, semi-global and global residual techniques to train our very deep network steadily and improve its performance. Extensive experiments indicate that our proposed method Split-Concatenate-Residual Super Resolution (SCRSR) yields promising SR performance while maintaining shorter running time and fewer parameters. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 20	2020	398						399	407		10.1016/j.neucom.2019.02.067													
J								Compact bilinear pooling via kernelized random projection for fine-grained image categorization on low computational power devices	NEUROCOMPUTING										Bilinear pooling; Deep learning; Random projection; Polynomial kernel	INDEPENDENT RANDOM PROJECTIONS; JOHNSON; CNN	Bilinear pooling is one of the most popular and effective methods for fine-grained image recognition. However, a major drawback of Bilinear pooling is the dimensionality of the resulting descriptors, which typically consist of several hundred thousand features. Even when generating the descriptor is tractable, its dimension makes any subsequent operations impractical and often results in huge computational and storage costs. We introduce a novel method to efficiently reduce the dimension of bilinear pooling descriptors by performing a Random Projection. Conveniently, this is achieved without ever computing the high-dimensional descriptor explicitly. Our experimental results show that our method outperforms existing compact bilinear pooling algorithms in most cases, while running faster on low computational power devices, where efficient extensions of bilinear pooling are most useful. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 20	2020	398						411	421		10.1016/j.neucom.2019.05.104													
J								Attentive and ensemble 3D dual path networks for pulmonary nodules classification	NEUROCOMPUTING										Dual path network; Pulmonary nodule classification; Computer-Aided diagnoses; Attention; Lung cancer	AUTOMATIC SEGMENTATION; LUNG NODULES; IMAGE; PREDICTION; MODEL	Automated pulmonary nodules classification aims at predicting whether a candidate nodule is benign or malignant. It is of great significance for computer-aided diagnosis of lung cancer. Despite the substantial progress achieved by existing methods, several challenges remain, including the lack of fine-grained representations, the interpretability of the reasoning procedure, and the trade-off between true-positive rate and false-positive rate. To tackle these challenges, in this work, we present a novel pulmonary nodule classification framework via attentive and ensemble 3D Dual Path Networks. Specially, we first devise a contextual attention mechanism to model the contextual correlations among adjacent locations, which improves the representativeness of deep features. Second, we employ a spatial attention mechanism to automatically locate the regions essential for nodule classification. Finally, we employ an ensemble of several models to improve the prediction robustness. Extensive experiments are conducted on the LIDC-IDRI database. Results demonstrate the effectiveness of the proposed techniques and the superiority of our model over previous state-of-the-art. (C) 2019 Elsevier B.V. All rights reserved.Y																	0925-2312	1872-8286				JUL 20	2020	398						422	430		10.1016/j.neucom.2019.03.103													
J								Combining active learning and local patch alignment for data-driven facial animation with fine-grained local detail	NEUROCOMPUTING										Active learning; Locally linear reconstruction; Semi-supervised local patch alignment; Face deformation; Facial feature points	REDUCTION; MODEL	Data-driven facial animation has attracted considerable attention from both academic and industrial communities in recent years. Typically, the motion data used to animate the faces are derived from either 3D or 2D facial features whose positions on the face are determined according to experience. There still lacks an automatic approach to determine the optimal positions of the features to face deformation, and current face deformation methods are incapable of providing fine-grained local geometric characteristics. This paper proposes a novel scheme for face animation in which an active learning method based on Locally Linear Reconstruction algorithm is exploited to determine the optimal feature positions on the face for face deformation, and the Semi-Supervised Local Patch Alignment algorithm is subsequently used to deform the face with the selected features according to the optimal feature positions. The active learning model can be solved by a sequential approach, and the Semi-Supervised Local Patch Alignment model can be addressed by a least-square method. Experimental results on various types of faces demonstrate the superiority of the proposed scheme to existing approaches in both feature points selection and fine-grained local characteristics preservation. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 20	2020	398						431	441		10.1016/j.neucom.2019.05.102													
J								Actionness-pooled Deep-convolutional Descriptor action recognition for fine-grained	NEUROCOMPUTING										ADD; Deep-convolutional Descriptor; Fine-grained action recognition		Recognition of general actions has witnessed great success in recent years. However, the existing general action representations cannot work well to recognize fine-grained actions, which usually share high similarities in both appearance and motion pattern. To solve this problem, we introduce the visual attention mechanism into the proposed descriptor, termed Actionness-pooled Deep-convolutional Descriptor (ADD). Instead of pooling features uniformly from the entire video, we aggregate features in sub-regions that are more likely to contain actions according to actionness maps. This endows ADD with the superior capability of capturing the subtle differences between fine-grained actions. We conduct experiments on HIT Dances dataset, one of the few existing datasets for fine-grained action analysis. Quantitative results have demonstrated that ADD remarkably outperforms traditional CNN-based representations. Extensive experiments on two general action benchmarks, JHMDB and UCF101, have additionally proved that combining ADD with end-to-end ConvNet can further boost the recognition performance. Besides, taking advantage of ADD, we reveal the sparsity characteristic existing in actions and point out a potential direction to design more effective action analysis models by extracting both representative and discriminative action patterns. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 20	2020	398						442	452		10.1016/j.neucom.2019.03.099													
J								Partial person re-identification with two-stream network and reconstruction	NEUROCOMPUTING										Partial person re-identification; Two-stream network; Reconstruction; Deep learning		Partial person re-identification is a challenging issue at present. However, affected by occlusions, features in person re-identification cannot be detected and the traditional person re-identification methods can not accurately deal with it. In order to solve this problem, we propose to match query and gallery by combining different modes from two-stream network with sparse reconstruction to realize partial person re-identification. For acquiring features, bilinear pooling is applied to fuse the two different modes from the appearance network and pose network aiming at better performance. For matching query and galley, the robust sparse representation reconstructs the features extracted by the network for flexible solution, using the parameters learned from galley. The reconstruction process achieves arbitrary size images in partial person re-identification. In addition, we extract mid-level feature and fuse it with the high-level feature for more accuracy. Experiments demonstrate the performance of the proposed method better compared with the methods of state-of-the-art person re-identification methods on dataset Market1501, CUHK03, DukeMTMC-reID and partial person dataset Partial-REID, Partial-iLIDS. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 20	2020	398						453	459		10.1016/j.neucom.2019.04.098													
J								Hierarchical-model salient object detection based on manifold ranking	NEUROCOMPUTING										Saliency detection; Hierarchical model; Manifold ranking; Weighted linear fusion	VISUAL SALIENCY	Salient object detection has attracted a lot of focused research and has resulted in many applications, it is a challenge to detect the most important scene from the input image. Different from most previous methods that only modeled the spatial connectivity of every region is modeled using k-regular graph, and do not consider the deficiency between multi-layer super pixel segmentations based on manifold ranking, we propose a multi-scale approach. First, we tackle an image from a scale point of view and use a multi-scale approach to analyze saliency cues. Second, through building a graph model which is on the basis of k-regular graph, we connect the nodes belonging to the same cluster and located in the same spatial connected area with edges, to highlight the whole goal more uniformly and evenly, then used manifold ranking to generate multi-layer saliency map. Finally, the final saliency map is got through weighted linear fusion. Extensive experiments on six benchmark datasets demonstrate efficiency of the proposed method against the state-of-the-art methods. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 20	2020	398						460	468		10.1016/j.neucom.2019.04.096													
J								Discriminative supplementary representation learning for novel-category classification	NEUROCOMPUTING										Supplementary representation; Semantic attribute; Novel-category; Sparsity; Image classification	OBJECT CLASSES; ATTRIBUTES; FEATURES	In this paper, we propose a new representation learning method by learning the discriminative supplementary features to implement semantic attribute augmentation for novel-category image classification. Most of previous methods based on semantic attributes explore the manually defined attributes, which limits their performance and applications. Because the number of manually defined attributes is limited in practice. Besides, these attributes are incomplete and may be not discriminative for image classification. To address the problems, one intuitive solution is to automatically expand the semantic attribute representation. However, it is challenging to guarantee that the supplementary features are more effective and discriminative. Towards this end, we propose a new Discriminative Supplementary Feature Learning (DSFL) method to expand the manually defined attributes by learning discriminative supplementary features. To make the learned features discriminative, the supplementary features and the classifiers for the novel-categories are jointly learned with the column sparsity constraints for the optimal compatibility of the representations and classifiers. To demonstrate the effectiveness of the proposed method, extensive experiments are conducted on two widely-used datasets. The experimental results show that the proposed method achieves encouraging performance. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 20	2020	398						469	476		10.1016/j.neucom.2019.03.100													
J								A novel learning method for multi-intersections aware traffic flow forecasting	NEUROCOMPUTING										Data-driven learning; Artificial neural networks; Relevance vector machine; Traffic flow forecasting	PREDICTION; ALGORITHM; NETWORKS; FEATURES; MACHINE; LSTM	Recent advances in machine learning have helped solve many challenges in artificial intelligence applications, such as traffic flow forecasting. Traffic flow forecasting models based on machine learning have recently been widely applied because of their great generalisation capability. This study aims to construct a multi-intersection-aware traffic flow prognostication architecture considering recent information of a nearby road, which is a significant indicator of the near-future traffic flow, and considering the selection of appropriate and essential sensors significantly correlated to the future traffic flow. To capture the inner correlation between sequential traffic flow data, a novel learning method involving the relevance vector machine is employed for the traffic flow forecasting. To optimise the kernel parameters of the relevance vector machine, a combination of the chaos theory and a simulated annealing algorithm is adopted. The proposed model is verified with the real-world data of six roads in a Minnesotan city. Then, the forecasting results of the new model are compared with those of some state-of-the-art models. These results indicate that the application of relevance vector regression to short-term traffic flow forecasting combined with a chaos-simulated annealing algorithm to optimise the corresponding parameters is a high-precision and -scalability short-term traffic flow forecasting method. The multi-intersection-aware mechanism helps improve the forecasting accuracy. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 20	2020	398						477	484		10.1016/j.neucom.2019.04.094													
J								Movie collaborative filtering with multiplex implicit feedbacks	NEUROCOMPUTING										Movie recommender system; Collaborative filtering; Multiplex implicit feedback; Matrix factorization	CLICK CONSTRAINTS; RECOMMENDATION; FEATURES; SYSTEMS	Movie recommender systems have been widely used in a variety of online networking platforms to give users reasonable advice from a large number of choices. As a representative method of movie recommendation, collaborative filtering uses explicit and implicit feedbacks to mine users' preferences. The use of implicit features can help to improve the accuracy of movie collaborative filtering. However, multiplex implicit feedbacks have not been investigated and utilized comprehensively. In this paper, we analyze different kinds of implicit feedbacks in movie recommendation, including user similarities for movie tastes, rated records of each movie and positive attitude of users, and incorporate these feedbacks for collaborative filtering. User relationships are extracted according to user similarities. We propose a recommendation method with multiplex implicit feedbacks (RMIF), which factorizes both the explicit rating matrix and implicit attitude matrix. To demonstrate the effectiveness of our method, we conduct extensive experiments on two real datasets. Experiment results prove that RMIF significantly outperforms state-of-the-art models in terms of accuracy. Among different kinds of implicit feedbacks, positive attitude has the most important role in movie collaborative filtering. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 20	2020	398						485	494		10.1016/j.neucom.2019.03.098													
J								Robust template matching with large angle localization	NEUROCOMPUTING										Template matching; Rotation invariance; Intensity centroid; Maximum correntropy criterion	OBJECT TRACKING; IMAGE; CORRENTROPY; ROTATION; ALGORITHM; FEATURES	Template matching is an important solution for the object detection in instance-level. Many kinds of matching methods have been utilized to find the template position in the target image. But most of them ignore the matching angle, which causes the result can't satisfy the detection accuracy when the object has a big in-plane rotation. In this paper, we propose a robust method for template matching with large angle localization. The basic idea is to iteratively search the corresponding patch-features and updating the template location with rotation transformation. The method firstly employs the intensity centroid to rectify local patches as rotation-invariant features. And then, Best-Buddies Pairs (BBPs) are extracted to find corresponding features between template and target images. To further enhance the robustness against outliers, a robust objective function is presented to register features based on the Maximum Correntropy Criterion and optimized for the transformation with translation and rotation parameters. Experimental results demonstrate the effectiveness and robustness of the proposed method. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 20	2020	398						495	504		10.1016/j.neucom.2019.05.105													
J								Joint local constraint and fisher discrimination based dictionary learning for image classification	NEUROCOMPUTING										Dictionary learning; Locality constrained; Fisher discriminative criterion; Sparse coding	COLLABORATIVE REPRESENTATION; FACE-RECOGNITION; ILLUMINATION; REDUCTION; ROBUST; MODEL	Dictionary learning has achieved outstanding results in the field of pattern recognition. The locality as an important structure characteristic of the data has been widely used in various learning task including dictionary learning, and Fisher discriminant has also been widely used. In this paper, to improve the performance of dictionary learning, we propose a joint local-constraint and fisher discrimination based dictionary learning method (JLCFDDL) for image classification. Our method uses the Laplacian regularized constraint of atoms rather than that of training samples to preserve the local information of the data. Meanwhile, fisher discriminative constraint is imposed on the atoms to maintain the differences between the atoms of different classes and to reduce the differences of atoms of the same class. The joint constraints make the obtained dictionary with powerful image classification performance. We also provide mathematical analysis of the proposed objective function. A large number of experiments prove that our method achieves better performance than existing state-of-the-art dictionary learning method. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 20	2020	398						505	519		10.1016/j.neucom.2019.05.103													
J								Stimulus-driven and concept-driven analysis for image caption generation	NEUROCOMPUTING										Image captioning; Stimulus-driven; Concept-driven; Attention mechanism; LSTM	NEURAL MECHANISMS; ATTENTION; PERFORMANCE; MODEL	Recently, image captioning has achieved great progress in computer vision and artificial intelligence. However, language models still failed to achieve the desired results in high-level visual tasks. Generating accurate image captions for a complex scene that contains multiple targets is a challenge. To solve these problems, we introduce the theory of attention in psychology to image caption generation. We propose two types of attention mechanisms: The stimulus-driven and the concept-driven. Our attention model relies on a combination of convolutional neural network (CNN) over images and long-short term memory (LSTM) network over sentences. Comparison of experimental results illustrates that our proposed method achieves good performance on the MSCOCO test server. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 20	2020	398						520	530		10.1016/j.neucom.2019.04.095													
J								Single image super-resolution with enhanced Laplacian pyramid network via conditional generative adversarial learning	NEUROCOMPUTING										Single image super-resolution; GAN; Conditional GAN; Laplacian pyramid		Despite much progress has been made by applying generative adversarial network (GAN) to single image super-resolution (SISR), obvious difference remains between the details of reconstructed high-frequency and ground-truth because GAN is unstable that has a very high degree of freedom. To address this issue, we exploit conditional GAN (CGAN) for SISR, which leverages the ground-truth high-resolution (HR) image as its conditional variable to guide to learn a more stable model. To better reconstruct image with a large-scale factor, we further design an enhanced Laplacian pyramid network (ELapN) as the generator model of CGAN, which progressively reconstructs HR images at multiple pyramid levels. The proposed ELapN fuses low-and high-level features for the residual image learning achieves better generalization than those only based on high-level information. Finally, we train the proposed network via deep supervision using a combination of multi-level CGAN, VGG and robust Charbonnier loss functions to obtain high-quality SR results. Extensive evaluations on three benchmark datasets including Set5, Set14, B100 demonstrate superiority of the proposed method over state-of-the-art methods in terms of PSNR, SSIM and visual effect. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 20	2020	398						531	538		10.1016/j.neucom.2019.04.097													
J								3D Model classification based on few-shot learning	NEUROCOMPUTING										Few-shot; Meta-learner; 3D model classification	FEATURES	With the development of multimedia technology, 3D model has been applied in many fields such as mechanical design, construction industry, entertainment industry, medical treatment and so on. The number of 3D model is becoming more and more in our lives. Therefore, effective automatic management and classification of 3D models become more and more important. In this paper, we propose a dual-meta-learner model based on LSTM to learn the exact optimization algorithm used to train another two learner neural network classifier in the few-shot regime. The parametrization of our model allows it to learn appropriate parameter updates specifically for the scenario where a set amount of updates will be made, while it can also achieve a general initialization of the learner (classifier) network that allows for quick convergence of training. Our method attains state-of-the-art performance by significant margins. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				JUL 20	2020	398						539	546		10.1016/j.neucom.2019.03.105													
J								Bi-adapting kernel learning for unsupervised domain adaptation	NEUROCOMPUTING										Unsupervised domain adaptation; Domain-invariant kernel; Text and object recognition		Unsupervised domain adaptation aims to use labeled instances from a source domain to train a good learning model, which can classify unlabeled instances from a target domain as accurate as possible. The biggest challenge is that datasets from the source and target domains have different distributions, thus the general classification model trained on the source domain can not perform well on the target domain data. The classic methods solve this problem mainly by narrowing the distance between the source and target domains. Those methods, however, is not optimal since the nonlinear feature space may not match the kernel-based learning machine. In this paper, we design a new method called bi-adapt kernel learning (BAKL) to learn a domain-invariant kernel by transferring the source and target domains to each other simultaneously. Specifically, we derive the new source and target domain kernel matrix according to the Mercer's theorem. The domain-invariant kernel machines are then constructed by minimizing the approximation error between the newly generated kernel matrices and the ground truth source domain kernel matrices. Experiments on benchmark tasks of text and object recognition demonstrate that it significantly improves classification accuracy compared to the state-of-art methods. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 20	2020	398						547	554		10.1016/j.neucom.2019.03.101													
J								Coarse-to-fine object detection in unmanned aerial vehicle imagery using lightweight convolutional neural network and deep motion saliency	NEUROCOMPUTING										Unmanned aerial vehicle (UAV) imagery; Object detection; Coarse-to-fine; Lightweight convolutional neural network (CNN); Deep motion saliency	KEY-FRAME EXTRACTION; VIDEO; TRACKING; SELECTION; VISION	Unmanned aerial vehicles (UAVs) have been widely applied to various fields, facing mass imagery data, object detection in UAV imagery is under extensive research for its significant status in both theoretical study and practical applications. In order to achieve the accurate object detection in UAV imagery on the premise of real-time processing, a coarse-to-fine object detection method for UAV imagery using lightweight convolutional neural network (CNN) and deep motion saliency is proposed in this paper. The proposed method includes three steps: (1) Key frame extraction using image similarity measurement is performed on the UAV imagery to accelerate the successive object detection procedure; (2) Deep features are extracted by PeleeNet, a lightweight CNN, to achieve the coarse object detection on the key frames; (3) LiteFlowNet and objects prior knowledge is utilized to analyze the deep motion saliency map, which further helps to refine the detection results. The detection results on key frames propagate to the temporally nearest non-key frames to achieve the fine detection. Five experiments are conducted to verify the effectiveness of the proposed method on Stanford drone dataset (SDD). The experimental results demonstrate that the proposed method can achieve comparable detection speed but superior accuracy to six state-of-the-art methods. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 20	2020	398						555	565		10.1016/j.neucom.2019.03.102													
J								Locality-constrained discrete graph hashing	NEUROCOMPUTING										Image retrieval; Discrete graph hashing; Locality-constrained	NEAREST-NEIGHBOR; IMAGE; QUANTIZATION; FEATURES	Hashing techniques have been widely used for large-scale image retrieval because of its low storage cost and high query speed. Hashing maps similar data onto binary codes with a smaller Hamming distance, which is essentially a discrete optimization problem with constraints. Recently, several discrete hashing methods have begun focusing on minimizing the discrete loss between hash codes and continuous values but ignoring the hidden structure among data points in the original space. Following these efforts, we propose a locality-constrained discrete graph hashing (LCH) method that attempts to minimize the similarity distance of the data points in the original space by softening the hash codes' hard constraint into two spaces: the discrete constraint space and the bit-balanced, bit-uncorrected constraint space. Furthermore, both spaces maintain the neighbourhood structure of the data points in the original space, and the best hash codes are found by minimizing the similarity distance between the two spaces. This is consistent with the nature of hashing for similarity preservation. Experiments on four real image datasets show that LCH obtains superior search accuracy over state-of-the-art hashing methods. (C) 2019 Published by Elsevier B.V.																	0925-2312	1872-8286				JUL 20	2020	398						566	573		10.1016/j.neucom.2019.03.104													
J								Sentiment topic sarcasm mixture model to distinguish sarcasm prevalent topics based on the sentiment bearing words in the tweets	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Sarcasm; Unsupervised learning; Sentiment; Opinion mining; Topic models		Sentiment analysis as we all know is a developed field in which new features keeps on adding, but most of the time, on the internet people use sarcasm to convey their message which is very difficult to understand both by people and machines. Sarcastic statements are very complex as most of the time they sound in positive context if interpreted literally but actually the speaker mean the opposite of what they speak. Sarcasm detection is a subtask of opinion mining. The main intention behind sarcasm detection is to identify the user opinions or emotions expressed by the user in the written text. It plays a critical role in sentiment analysis by correctly identifying sarcastic or non sarcastic sentences. The sarcastic sentence has mixed polarity of both positive and negative words. Understanding sarcasm is quite a difficult and a challenging task even for humans as well as for machines. Various approaches for sarcasm detection are purely based on machine learning classifiers where training the classifier is based on simple lexical or dictionary based features. The objective of the work is to develop an unsupervised probabilistic relational model to identify sarcasm prevalent topics based on the sentiment distribution of the words in the tweets. The model estimates sentiment based topic level distribution. The model evaluation shows the sentiment associated words that do appear in the short text given the sentiment related label. The model outperforms the other baseline state of art Model for sarcasm detection as shown in the experimental result and it is very much suited for the prediction of sarcasm of a short tweet.																	1868-5137	1868-5145															10.1007/s12652-020-02315-1		JUL 2020											
J								An improved approach for automatic spine canal segmentation using probabilistic boosting tree (PBT) with fuzzy support vector machine	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										PBT; FSVM; Spine canal segmentation; MR	IMAGE SEGMENTATION; COLOR	Spine canal segmentation is an emerging zone in research proposed to help interpretation and processing of advanced MRI and CT images. For instance, high resolution three-dimensional volumes can be divided to provide an estimation of spine canal atrophy. Spine canal segmentation is complex because of assortment of MRI contrasts and variation in human life structures. This investigation illustrates the details of spine canal segmentation techniques and gives a few measurements that can be utilized to contrast with other segmentation strategies. The details of background and foreground subtraction techniques, spine canal segmentation approach and optimization approach which are utilized in the different applications have been considered. In this paper, spine canal segmentation on probabilistic booting tree (PBT) with fuzzy support vector machine performance measures and metrics are analysed in state-of-the art technologies. Proposed approach is performed on the base of the automatic spine canal segmentation with the group of data MR. This proposed segmentation continue with fuzzy support vector machine (FSVM) technique to make fully automatic stream pipeline. The declaration in an automatic segmentation of stream pipeline was implemented with flexible voxel wise classification accompanying dimensions analogous with 3D Haar and labelled machine learning algorithms i.e. probabilistic boosting tree combined fuzzy support vector machine (PBT-FSVM). The novel segmentation technique correlated with MR data sets provides better accuracy than the exiting techniques and it is shown in experimental outcomes. To still improve performance of the results, online learning classification method can be in the proposed work.																	1868-5137	1868-5145															10.1007/s12652-020-02267-6		JUL 2020											
J								Economic IoT strategy: the future technology for health monitoring and diagnostic of agriculture vehicles	JOURNAL OF INTELLIGENT MANUFACTURING										IoT; Big data; Agriculture end-devices; Agriculture vehicle; Artificial intelligence; Acoustic noise; Sensors	ANT COLONY OPTIMIZATION; RESIDUAL LIFE; ALGORITHM; MODEL; INTERNET; NETWORK	Today's Agriculture vehicles (AgV)s are expected to encompass mainly the three requirements of customers; economy, the use of High technology and reliability. In this manuscript, we investigate the technology solution for efficient health monitoring and diagnostic (HM&D) strategy to maximize the field efficiency and minimize the machine cost. Based on the data captured by various IoT sensors, we demonstrate the facts to shift the HM&D technology from costly sensor to economic microphone based mechanism. The adopted strategy is capable to reduce the bulky data transmission on the internet, and to increase the up-time of AgVs. We experimented on the essential red hot chili peppers system of the AgV's backbone hydraulic system-the hydraulic filter and pump. The measurement system analysis is adopted to determine the preciseness of data captured near the considered components. The envision of the correlation between the collected data extracts significant information to draw the facts to embrace the future HM&D technology shift. Correlation between the signals captured from costly sensors and Microphone for the generated faults in hydraulic components demonstrates the effectiveness of audio to replace existing HM&D technology.																	0956-5515	1572-8145															10.1007/s10845-020-01610-0		JUL 2020											
J								A flexible alarm prediction system for smart manufacturing scenarios following a forecaster-analyzer approach	JOURNAL OF INTELLIGENT MANUFACTURING										Alarm prediction; Data-driven predictive maintenance; Long short-term memory (LSTM); Residual neural networks (ResNet); Time series forecasting	CONVOLUTIONAL NEURAL-NETWORKS; HYBRID ARIMA; SERIES; MODELS	The introduction of data-related information technologies in manufacturing allows to capture large volumes of data from the sensors monitoring the production processes and different alarms associated to them. An early prediction of those alarms can bring several benefits to manufacturing companies such as predictive maintenance of the equipment, or production optimization. This paper introduces a new system that allows to anticipate the activation of several alarms and thus, warns the operators in the plants about situations that could hamper the machines operation or stop the production process. The system follows a two-stageforecaster-analyzerapproach on which first, a long short-term memory recurrent neural network basedforecasterpredicts the future sensor's measurements and then, distinctanalyzersbased on residual neural networks determine whether the predicted measurements will trigger an alarm or not. The system supports some features that make it particularly suitable for smart manufacturing scenarios: on the one hand, theforecasteris able to predict the future measurements of different types of time-series data captured by various sensors in non-stationary environments with dynamically changing processes. On the other hand, the analyzers are able to detect alarms that can be modeled with simple rules based on the activation condition, and also more complex alarms on which it is unknown when the activation condition will be fulfilled. Moreover, the followed approach for building the system makes it flexible and extensible for other predictive analysis tasks. The system has shown a great performance to predict three different types of alarms.																	0956-5515	1572-8145															10.1007/s10845-020-01614-w		JUL 2020											
J								Fast LSTM by dynamic decomposition on cloud and distributed systems	KNOWLEDGE AND INFORMATION SYSTEMS										LSTM; Fast inference; Dynamic decomposition		Long short-term memory (LSTM) is a powerful deep learning technique that has been widely used in many real-world data-mining applications such as language modeling and machine translation. In this paper, we aim to minimize the latency of LSTM inference on cloud systems without losing accuracy. If an LSTM model does not fit in cache, the latency due to data movement will likely be greater than that due to computation. In this case, we reduce model parameters. If, as in most applications we consider, the LSTM models are able to fit the cache of cloud server processors, we focus on reducing the number of floating point operations, which has a corresponding linear impact on the latency of the inference calculation. Thus, in our system, we dynamically reduce model parameters or flops depending on which most impacts latency. Our inference system is based on singular value decomposition and canonical polyadic decomposition. Our system is accurate and low latency. We evaluate our system based on models from a series of real-world applications like language modeling, computer vision, question answering, and sentiment analysis. Users of our system can use either pre-trained models or start from scratch. Our system achieves 15x average speedup for six real-world applications without losing accuracy in inference. We also design and implement a distributed optimization system with dynamic decomposition, which can significantly reduce the energy cost and accelerate the training process.																	0219-1377	0219-3116				NOV	2020	62	11					4169	4197		10.1007/s10115-020-01487-8		JUL 2020											
J								Image Morphing in Deep Feature Spaces: Theory and Applications	JOURNAL OF MATHEMATICAL IMAGING AND VISION										Image morphing; Metamorphosis model; Variational time discretization; Mosco convergence; Convolutional neural networks	DISCRETE GEODESICS; METAMORPHOSES; CONVERGENCE; FLOWS; LIE	This paper combines image metamorphosis with deep features. To this end, images are considered as maps into a high-dimensional feature space and a structure-sensitive, anisotropic flow regularization is incorporated in the metamorphosis model proposed by Miller and Younes (Int J Comput Vis 41(1):61-84, 2001) and Trouve and Younes (Found Comput Math 5(2):173-198, 2005). For this model, a variational time discretization of the Riemannian path energy is presented and the existence of discrete geodesic paths minimizing this energy is demonstrated. Furthermore, convergence of discrete geodesic paths to geodesic paths in the time continuous model is investigated. The spatial discretization is based on a finite difference approximation in image space and a stable spline approximation in deformation space; the fully discrete model is optimized using the iPALM algorithm. Numerical experiments indicate that the incorporation of semantic deep features is superior to intensity-based approaches.																	0924-9907	1573-7683															10.1007/s10851-020-00974-5		JUL 2020											
J								The voice of optimization	MACHINE LEARNING										Parametric optimization; Interpretability; Sampling; Multiclass classification	NEURAL-NETWORKS	We introduce the idea that using optimal classification trees (OCTs) and optimal classification trees with-hyperplanes (OCT-Hs), interpretable machine learning algorithms developed by Bertsimas and Dunn (Mach Learn 106(7):1039-1082,2017), we are able to obtain insight on the strategy behind the optimal solution in continuous and mixed-integer convex optimization problem as a function of key parameters that affect the problem. In this way, optimization is not a black box anymore. Instead, we redefine optimization as a multiclass classification problem where the predictor gives insights on the logic behind the optimal solution. In other words, OCTs and OCT-Hs give optimization a voice. We show on several realistic examples that the accuracy behind our method is in the 90-100% range, while even when the predictions are not correct, the degree of suboptimality or infeasibility is very low. We compare optimal strategy predictions of OCTs and OCT-Hs and feedforward neural networks (NNs) and conclude that the performance of OCT-Hs and NNs is comparable. OCTs are somewhat weaker but often competitive. Therefore, our approach provides a novel insightful understanding of optimal strategies to solve a broad class of continuous and mixed-integer optimization problems.																	0885-6125	1573-0565															10.1007/s10994-020-05893-5		JUL 2020											
J								Semi-supervised learning using adversarial training with good and bad samples	MACHINE VISION AND APPLICATIONS										Semi-supervised learning; Generative adversarial network; Image classification; Adversarial training		In this work, we investigate semi-supervised learning (SSL) for image classification using adversarial training. Previous results have illustrated that generative adversarial networks (GANs) can be used for multiple purposes in SSL . Triple-GAN, which aims to jointly optimize model components by incorporating three players, generates suitable image-label pairs to compensate for the lack of labeled data in SSL with improved benchmark performance. Conversely, Bad (or complementary) GAN optimizes generation to produce complementary data-label pairs and force a classifier's decision boundary to lie between data manifolds. Although it generally outperforms Triple-GAN, Bad GAN is highly sensitive to the amount of labeled data used for training. Unifying these two approaches, we present unified-GAN (UGAN), a novel framework that enables a classifier to simultaneously learn from both good and bad samples through adversarial training. We perform extensive experiments on various datasets and demonstrate that UGAN: (1) achieves competitive performance among other GAN-based models, and (2) is robust to variations in the amount of labeled data used for training.																	0932-8092	1432-1769				JUL 19	2020	31	6							49	10.1007/s00138-020-01096-z													
J								Novel coupled DP system for fuzzy C-means clustering and image segmentation	APPLIED INTELLIGENCE										Coupled DP system; Fuzzy C-means clustering; Image segmentation; Membrane computing; P system	PARTICLE SWARM OPTIMIZATION; GENETIC ALGORITHM	This study proposed a novel fuzzy c-means clustering method which calculates the density of the data points based on the weighted mean distance (WMFCM). A novel coupled DNA-GA and P system (DP system) is introduced to realize the clustering process. The evolutional rules in the coupled DP system can help the WMFCM algorithm jump out of the local optimum and get the initial clustering centers. The performance of the coupled DP system in dealing with fuzzy clustering problems is measured by conducting experimental analysis on UCI datasets and BSDS300 image datasets. And the experimental results are compared with several popular algorithms. Experimental results show that our algorithm can perform better than other algorithms.																	0924-669X	1573-7497															10.1007/s10489-020-01784-3		JUL 2020											
J								A prediction strategy based on special points and multiregion knee points for evolutionary dynamic multiobjective optimization	APPLIED INTELLIGENCE										Evolutionary algorithm; Dynamic multiobjective optimization; Special point; Knee point	KRILL HERD ALGORITHM; DECOMPOSITION; OPERATORS; MODEL	Dynamic multiobjective optimization problems exist widely in the real word and require the optimization algorithms to track the Pareto front (PF) over time. A prediction strategy based on special points and multi-region knee points (MRKPs) is proposed for solving dynamic multiobjective optimization problems. Whenever a change is detected, the prediction strategy reacts effectively to the change by generating four subpopulations based on four strategies. The first subpopulation is created by selecting the representative individuals using a special point strategy. The second subpopulation consists of a solution set using a multiregion knee point strategy. The third subpopulation is introduced to the nondominated set by a convergence strategy. The fourth subpopulation comprises diverse individuals from an adaptive diversity maintenance strategy. The four subpopulations merge into a new population to accurately predict the location and distribution of the PF after an environmental change. MRKP is compared with four popular evolutionary algorithms on standard instances with different changing dynamics. Finally, MRKP provides better results than other competitors in terms of Inverted Generational Distance and Hypervolume metrics. The results reveal that MRKP can quickly adapt to changing environments and provide good tracking ability when dealing with dynamic multiobjective optimization problems.																	0924-669X	1573-7497															10.1007/s10489-020-01772-7		JUL 2020											
J								A novel additive consistency for intuitionistic fuzzy preference relations in group decision making	APPLIED INTELLIGENCE										Intuitionistic fuzzy preference relations; Additive consistency; Consensus; Tanino's normalized intuitionistic fuzzy priority vectors; Group decision making	MODELS; ISSUES	Deriving the priority vectors of the alternatives from preference relations is an interesting research topic for group decision making with preference information. This paper uses an example to show that the ranking or the optimal alternative could not always be derived from the existing additively consistent intuitionistic fuzzy preference relations. Thus, we provide novel additively consistent intuitionistic fuzzy preference relations and characterize them with Tanino's normalized (T-normalized) intuitionistic fuzzy priority vectors. Then, we propose some methods to check and achieve the T-normalization, acceptably additive consistency and consensus of the intuitionistic fuzzy preference relations in group decision making using the local, individual and optimal collective intuitionistic fuzzy priority vectors, respectively. We also give some examples to show how the proposed models work and make comparisons with the existing methods to demonstrate the advantages of the proposed methods.																	0924-669X	1573-7497															10.1007/s10489-020-01796-z		JUL 2020											
J								Learning stacking regressors for single image super-resolution	APPLIED INTELLIGENCE										Single image super-resolution (SR); Stacking learning; Residual cascaded regression	K-SVD; REGULARIZATION; MODEL	Example learning-based single image super-resolution (SR) technique has been widely recognized for its effectiveness in restoring a high-resolution (HR) image with finer details from a given low-resolution (LR) input. However, most popular approaches only choose one type of image features to learn the mapping relationship between LR and HR images, making it difficult to fit into the diversity of different natural images. In this paper, we propose a novel stacking learning-based SR framework by extracting both the gradient features and the texture features of images simultaneously to train two complementary models. Since the gradient features are helpful to represent the edge structures while the texture features are beneficial to restore the texture details, the newly proposed method cleverly combines the merits of two complementary features and makes the resultant HR images more faithful to their original counterparts. Moreover, we enhance the SR capacity by using a residual cascaded scheme to further reduce the gap between the super-resolved images and the corresponding original images. Experimental results carried out on seven benchmark datasets indicate that the proposed SR framework performs better than other seven state-of-the-art SR methods in both quantitative and qualitative quality assessments.																	0924-669X	1573-7497															10.1007/s10489-020-01787-0		JUL 2020											
J								A novel real-time design for fighting game AI	EVOLVING SYSTEMS										Real-time fighting games; Fighting game AI challenge; Generic heuristics; Monte-Carlo tree search	CARLO TREE-SEARCH; GO	Real-time fighting games are challenging for computer agents in that actions must be decided within a relatively short cycle of time, usually in milliseconds or less. That is only achievable by either very powerful machines or state-of-the-art algorithms. The former is usually a costly option while the latter remains an ongoing research topic despite countless research. This paper describes our algorithmic approach towards real-time fighting games via the fighting game AI challenge. The focus of our research is the LUD division, the most challenging category of the competition where action data is hidden to prevent methods that are dependent on prior training. In this paper, we propose several generic heuristics that can be used in combination with Monte-Carlo tree search. Our experimental results show that such an approach would provide an excellent AI outperforming pure Monte-Carlo tree search and classic algorithms such as evolutionary algorithms or deep reinforcement learning. Nonetheless, we believe that our proposed heuristics should be able to generalize to other domains beyond the scope of the fighting game AI challenge.																	1868-6478	1868-6486															10.1007/s12530-020-09351-4		JUL 2020											
J								Decidable there exists*for all* First-Order Fragments of Linear Rational Arithmetic with Uninterpreted Predicates	JOURNAL OF AUTOMATED REASONING										Bernays-Schonfinkel-Ramsey fragment; First-order arithmetic; Linear rational arithmetic; Difference constraints; Combinations of theories	SUPERPOSITION MODULO; COMBINATIONS; AUTOMATA; COMPLEXITY; LOGIC; PROPAGATION; ALGORITHM; CHECKING; DPLL(T)	First-order linear rational arithmetic enriched with uninterpreted predicates yields an interesting and very expressive modeling language. However, already the presence of a single uninterpreted predicate symbol of arity one or greater renders the associated satisfiability problem undecidable. We identify two decidable fragments, both based on the Bernays-Schonfinkel-Ramsey prefix class. Due to the inherent infiniteness of the underlying domain, a finite model property in the usual sense cannot be established. Nevertheless, we show that satisfiable sentences always have a model that can be described by finite means. To this end, we restrict the syntax of arithmetic atoms. In the first fragment that is presented, arithmetic operations are only allowed over terms without universally quantified variables. In the second fragment, arithmetic atoms are essentially confined to difference constraints over universally quantified variables with bounded range. We will call such atomsbounded difference constraints. As bounded intervals over the rationals still comprise infinitely many values, a trivial instantiation procedure is not sufficient to solve the satisfiability problem. After a slight shift of perspective, the positive decidability result for the first fragment can be restated in the framework of combinations of theories over non-disjoint vocabularies. More precisely, we combine first-order theories that share a dense total order without endpoints.																	0168-7433	1573-0670															10.1007/s10817-020-09567-8		JUL 2020											
J								An efficient and globally optimal method for camera pose estimation using line features	MACHINE VISION AND APPLICATIONS										Machine vision; Camera pose estimation; Cayley parameterization; Grobner basis; PnL	PERSPECTIVE; ROBUST; CORRESPONDENCES	The accurate estimation of camera pose using numerous line correspondences in real time is a challenging task. This paper presents a non-iterative approach to solve the Perspective-n-Line (PnL) problem. The method can provide high speed and global optimality, as well as linear complexity. A nonlinear least squares (non-LLS) objective function is first formulated by parameterizing the rotation matrix with Cayley representation. A system of three third-order equations is then derived from its optimality conditions, and then, it is solved directly based on the Grobner basis technique. Finally, the camera pose can be easily obtained by back-substitution. A major advantage of the proposed method lies in scalability, as the size of the elimination template used in the Grobner basis technique is independent to the number of line correspondences. Extensive and detailed experiments on synthetic data and real images are conducted, demonstrating that the proposed method achieves an accuracy that is equivalent or superior to the leading methods, but with reduced computational requirements. The source code is available at. https://github.com/dannyshin1/danny/tree/master/OPnL1.																	0932-8092	1432-1769				JUL 18	2020	31	6							48	10.1007/s00138-020-01100-6													
J								CDSMOTE: class decomposition and synthetic minority class oversampling technique for imbalanced-data classification	NEURAL COMPUTING & APPLICATIONS										Machine learning; Class-imbalance; Classification; Undersampling; Oversampling	DATA SET; CLUSTER; PREDICTION; ALGORITHM	Class-imbalanced datasets are common across several domains such as health, banking, security, and others. The dominance of majority class instances (negative class) often results in biased learning models, and therefore, classifying such datasets requires employing some methods to compact the problem. In this paper, we propose a new hybrid approach aiming at reducing the dominance of the majority class instances using class decomposition and increasing the minority class instances using an oversampling method. Unlike other undersampling methods, which suffer data loss, our method preserves the majority class instances, yet significantly reduces its dominance, resulting in a more balanced dataset and hence improving the results. A large-scale experiment using 60 public datasets was carried out to validate the proposed methods. The results across three standard evaluation metrics show the comparable and superior results with other common and state-of-the-art techniques.																	0941-0643	1433-3058															10.1007/s00521-020-05130-z		JUL 2020											
J								Component-based face recognition using statistical pattern matching analysis	PATTERN ANALYSIS AND APPLICATIONS										Binary facial component; Histogram linearization technique; Otsu thresholding; Probability of white pixels; Hu's moment invariants; Facial corner points	EIGENFACES	The aim of this research is to develop a fusion concept to component-based face recognition algorithms for features analysis of binary facial components (BFCs), which are invariant to illumination, expression, pose variations and partial occlusion. To analyze the features, using statistical pattern matching concepts, which are the combination of Chi-square (CSQ), Hu moment invariants (HuMIs), absolute difference probability of white pixels (AbsDifPWPs) and geometric distance values (GDVs) have been proposed for face recognition. The individual grayscale face image is cropped by applying the Viola-Jones face detection algorithm from a face database having variations in illumination, appearance, pose and partial occlusion with complex backgrounds. Doing illumination correction through histogram linearization technique, the grayscale face components such as eyes, nose and mouth regions are extracted using the 2D geometric positions. The binary face image is created by applying cumulative probability distribution function with Otsu adaptive thresholding method and then extracted BFCs such as eyes, nose and mouth regions. Five statistical pattern matching tools such as the standard deviation of CSQ values with probability of white pixels (PWPs), standard deviation of HuMIs with Hu's seven moment invariants, AbsDifPWPs and GDVs are developed for recognition purpose. GDVs are determined between two similar facial corner points (FCPs) and nine FCPs are extracted from binary whole face and BFCs. Pixel Intensity Values (PIVs) which are determined using L(2)norms from grayscale values of the whole face and grayscale values of the face components. Experiment is performed using BioID Face Database on the basis of these pattern matching tools and appropriate threshold values with logical and conditional operators and gives the best expected results from true positive rate perspective.																	1433-7541	1433-755X															10.1007/s10044-020-00895-4		JUL 2020											
J								Invariants for time-series constraints	CONSTRAINTS										Register automaton; Time-series constraints; Linear invariant; Non-linear invariant; Parameterised invariant; Finite automaton	GLOBAL CONSTRAINTS; BOUNDS	Many constraints restricting the result of some computations over an integer sequence can be compactly represented by counter automata. We improve the propagation of the conjunction of such constraints on the same sequence by synthesising a database of linear and non-linear invariants using their counter-automaton representation. The obtained invariants are formulae parameterised by the sequence length and proven to be true for any long enough sequence. To assess the quality of such linear invariants, we developed a method to verify whether a generated linear invariant is a facet of the convex hull of the feasible points. This method, as well as the proof of non-linear invariants, are based on the systematic generation of constant-size deterministic finite automata that accept all integer sequences whose result verifies some simple condition. We apply such methodology to a set of 44 time-series constraints and obtain 1400 linear invariants from which 70% are facet defining, and 600 non-linear invariants, which were tested on short-term electricity production problems.																	1383-7133	1572-9354															10.1007/s10601-020-09308-z		JUL 2020											
J								A simple empirical model for blood platelet production and inventory management under uncertainty	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Platelet concentrates; Simulation; Supply chain; Inventory control; Dynamic planning; Operation research in healthcare systems; Markov decision process	SUPPLY CHAIN NETWORK; OPTIMIZATION MODEL; MINIMIZE WASTAGE; ROUTING METHOD; DESIGN; IMPLEMENTATION; ALLOCATION; INTERNET; STRATEGY; DEMAND	The purpose of this study is to develop a pragmatic method for managing the inventory and production of blood platelets in places with inappropriate infrastructure. Thus far, we can find a rich number of papers regarding the optimization of blood products supply chain but most of them are impractical due to utilizing so many mathematical formulas and parameters. Hence, as an interdisciplinary study, we should develop a comprehensible method for medical workers and doctors to optimize the supply chain, maintain the quality of services and overcome the challenges. The inventory manager has to cope with multifaceted problems, among those are limited availability of donors, the uncertainty of demand, maintaining the quality and quantity of the products at a reasonable level and on-time response to medical centers and finally yet importantly avoid waste caused by overproduction. To tackle these problems, we employed Markov decision process and simulation to ensure the accountability of the model. Eventually, the real managerial insight provided through gathering data regarding the number of casualties caused by road accidents in Semnan province, Iran, and the number of blood platelet ordered by the hospitals and coordination between medical centers and the Blood Transfusion Center. The results indicate the accessibility of the model by inventory manager and physicians in the transfusion center and the reduction of waste due to appropriate production planning.																	1868-5137	1868-5145															10.1007/s12652-020-02254-x		JUL 2020											
J								Labeling and clustering-based level set method for automated segmentation of lung tumor stages in CT images	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Lung tumor; 8-connected component labeling (CCL); k-means; Automated level set; Shape features; Statistical metrics	HYBRID; NETWORK	An unconstrained growth of abnormal cells in the lung causes tumors. The aim of the work is the accurate diagnoses of tumor at its early stage through hybrid segmentation algorithm. The objective of this paper is to propose a novel labeled cluster active contour method to improve the performance of automated lung tumor segmentation from 2D CT slices. To the input 2D slice, an 8-connected component analysis is implemented to differentiate the image spatial information and obtain the RGB labeled data. The tumor location is foreseen using an unsupervised k-means clustering algorithm. Then, an automated level set algorithm is carried out to appropriately localize and extract the tumor region. The amount of initial level set curve evolution is controlled by the clustering efficiency of k-Means and labeling efficacy of connected component analysis. The quantitative evaluation of segmentation is carried out based on Shape features like area, perimeter, eccentricity, convex area, solidity, and roundness. The statistical object-based and distance-based metrics were used to find the similarity between manual and proposed methods. Also, performance metrics like accuracy, specificity, sensitivity, precision, and recall are used to validate the segmentation results of the proposed method. The proposed system is evaluated over 42 datasets from the commonly available large dataset LIDC (Lung Image Database Consortium). The accuracy, specificity, sensitivity and precision of the proposed hybrid method are 97.5%, 97.43%, 91.67%, and 98.79%, which exhibited the best competence as compared to traditional methods on the same dataset. The statistical and quantitative analysis shows the efficiency of the present work.																	1868-5137	1868-5145															10.1007/s12652-020-02329-9		JUL 2020											
J								Deep multilayer and nonlinear Kernelized Lasso feature learning for healthcare in big data environment	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Machine learning; Deep multilayer; Non-linear; Kernel; Lasso feature learning	DATA ANALYTICS	In this modern era, healthcare industry is being metamorphosed by the progress in machine learning (ML). By utilizing vast big data, ML is now being pre-owned in healthcare to bestow comparatively better patient care and has emerged in enhanced business consequences. In this paper an effective processing framework called deep multilayer and non-linear Kernelized Lasso feature learning (DM-NKLFL) is introduced to powerfully cope with the data explosion in image processing field. Our work dedicates to provide a general framework for both simple linear and complex non-linear relationships. This in turn helps to handle the increase in image scale without affecting the performance. The proposed DM-NKLFL method includes two parts, i.e., stepwise regression nonlinear Kernelized Lasso (SR-NKL) feature selection and deep multilayer pattern learning (DMPL). Specifically, SR-NKL is aimed at processing non-linear features to minimize time and complexity involved during feature selection whereas the DMPL is proposed to deeply learn data driven features to determine the underlying patterns. The DM-NKLFL method over the traditional state-of-the-art methods are validated both in time efficiency and quality of results using the big biological data.																	1868-5137	1868-5145															10.1007/s12652-020-02328-w		JUL 2020											
J								Xi'an tourism destination image analysis via deep learning	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Deep learning; Fine-grained image recognition; Scene recognition; Landmark recognition; Destination image	PHOTOS; MODEL	Existing methods focus on destination image construction by textual description or visual content separately. However, descriptions and images are closely related since they are taken from the same reviews and represent tourists impression of the city. It's questionable to study them separately. In this paper, we used both images and descriptions from the reviews to construct Xi'an tourism destination image. More concretely, scene recognition, landmark recognition and food image recognition are utilized to obtain visual image. Lexical analysis is applied to obtain semantic image. We further compared the differences between visual image and semantic image then we proposed the fusion image. Finally, the top 300 key words and differences of the photo contents between the adjacent 2 years are selected to discovering new changes of the destination image. Results show that the visual image and semantic image are significant different from each other and the new changes of semantic image are closely related to the events or things that happened in that year and changes of visual image are not significant.																	1868-5137	1868-5145															10.1007/s12652-020-02344-w		JUL 2020											
J								Ensemble approach for mid-long term runoff forecasting using hybrid algorithms	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Comprehensive runoff index; Factor selection; Extreme learning machine; Particle swarm optimization algorithm; Mid-long term runoff forecasting	EXTREME LEARNING-MACHINE; PARTICLE SWARM OPTIMIZATION; DATA-DRIVEN MODELS; VARIABLE SELECTION; PREDICTION	Factor selection and model construction play an important role in mid-long term runoff forecasting. Due to the indeterminacy between the data and mid-long term runoff, identifying key factors for mid-long term runoff forecasting is challenging. Another problem for mid-long term runoff forecasting is the low accuracy, which limits practical application. Aiming to solve these problems, an ensemble approach is proposed in this paper. First, we propose a novel method for constructing a comprehensive runoff index, and apply the partial mutual information approach to calculate the correlation between multiple factors and the comprehensive runoff index. Through this calculation, the key factors for the mid-long term runoff forecasting can be selected. Second, we implement mid-long term forecasting by combining improved particle swarm optimization (IPSO) and extreme learning machine (ELM) algorithms, which can improve the accuracy of runoff forecasting. The novelty of the proposed method lies in combining the construction of comprehensive runoff index, the key factor selection and the forecasting model based on IPSO-ELM for mid-long term runoff. Experimental results demonstrate that the proposed forecasting model significantly outperforms the current state-of-the-art of the extreme learning machine algorithms and other classical data-driven models for runoff forecasting in the Yalong River basin. Moreover, the performance for datasets based on different hydrological impact factors in the conducted experiments proves the robustness of the proposed method.																	1868-5137	1868-5145															10.1007/s12652-020-02345-9		JUL 2020											
J								Hierarchical Clustering Matching for Features with Repetitive Patterns in Visual Odometry	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Feature matching; Hierarchical clustering; Global ambiguity; Visual odometry	IMAGE; PERFORMANCE	In this study, a hierarchical clustering matching (HCM) algorithm is proposed to match features with ambiguity due to repetitive patterns in visual odometry. Visual odometry is a real-time system that estimates the motions of camera setups, in which feature matching is a key step for tracking and relocalization. However, it is still difficult to remove outliers fast and reliably when a high proportion of outliers exist. The proposed HCM algorithm solves this problem by clustering accurate matches hierarchically. Dubious matches excluded from any clusters are removed during the iterations, which finally converges when new clusters no longer generate. Local geometric consistency and descriptor of features are both considered to be a metric to link two clusters using a centroid linkage criterion. Experimental results demonstrate that the proposed method works well on solving the problem mentioned above. Compared to state-of-the-art methods on feature matching, HCM performs much better on efficiency with comparable accuracy.																	0921-0296	1573-0409															10.1007/s10846-020-01230-z		JUL 2020											
J								Managing consensus reaching process with self-confident double hierarchy linguistic preference relations in group decision making	FUZZY OPTIMIZATION AND DECISION MAKING										Self-confidence DHLPRs; Consensus reaching model; Priority vector; Group decision making; Simulation experiment	CONSISTENCY	Group decision making (GDM) can be defined as an environment where there exist a set of possible alternatives and a set of individuals (experts, judgements, etc.). Preference relation is one of the most widely used preference representation structures in GDM. Considering that the self-confidence degree is an important part to express preference information, and double hierarchy linguistic preference relation (DHLPR) is a cognitive complex linguistic information representation tool to express complex linguistic information, this paper presents a novel preference relation named as self-confident DHLPR. In addition, a weight-determining method is developed, which considers three kinds of information including the subjective weights and two kinds of objective weights. Furthermore, a consensus model is set up to manage the GDM problems with self-confident DHLPRs based on the priority ordering theory. The effectiveness of the proposed consensus model is illustrated by a case study concerning the selection of optimal hospitals in the field of Telemedicine. Finally, a simulation experiment is devised to testify the proposed consensus model and then some comparisons with other consensus reaching models are provided from three different angles including the number of iterations, the consensus success ratio and the distance between the original and adjusted preferences.																	1568-4539	1573-2908															10.1007/s10700-020-09331-y		JUL 2020											
J								Barrier option pricing formulas of an uncertain stock model	FUZZY OPTIMIZATION AND DECISION MAKING										Stock model; Barrier option; Option pricing formula; Uncertain finance		As applications of the uncertainty theory to finance, uncertain stock models have been presented to describe the prices of stocks strongly influenced by human uncertainty. So far, large progress has been achieved on pricing problems of path-independent options of the uncertain stock models. This paper investigates a type of path-dependent exotic options of an uncertain stock model which are named barrier options. Pricing formulas are derived based on the structure of the solutions of uncertain differential equations, and numerical algorithms are designed to calculate the prices of the barrier options based on these formulas.																	1568-4539	1573-2908															10.1007/s10700-020-09333-w		JUL 2020											
J								An efficient framework for generating robust adversarial examples	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										adversarial example; deep neural networks; robust adversarial example; security		Recent studies show that deep neural networks (DNNs) suffer adversarial examples. That is, attackers can mislead the output of a DNN by adding subtle perturbation to a benign input image. In addition, researchers propose new generation of technologies to produce robust adversarial examples. Robust adversarial examples can consistently fool DNN models under predefined hyperparameter space, which can break through some defenses against adversarial examples or even generate physical adversarial examples against real-world applications. Behind these achievements, expectation over transformation (EOT) algorithm plays as the backbone framework for generating robust adversarial examples. Though EOT framework is powerful, we know little about why such a framework can generate robust adversarial examples. To address this issue, we do the first work to explain the principle behind robust adversarial examples. Then, based on the findings, we point out that traditional EOT framework has a performance problem and propose an adaptive sampling algorithm to overcome such a problem. By modeling the sampling process as classic Coupon Collector Problem, we prove that our new framework reduces the cost fromO(n * log(n))toO(n), wherendenotes the number of sampling points. Under the view of computational complexity, the algorithm is optimal for this problem. The experimental results show that our algorithm can save up to 23% overhead in average. This is significant for black-box attack, where the cost is charged by the amount of queries.																	0884-8173	1098-111X				SEP	2020	35	9					1433	1449		10.1002/int.22267		JUL 2020											
J								ARF-Crack: rotation invariant deep fully convolutional network for pixel-level crack detection	MACHINE VISION AND APPLICATIONS										Crack detection; Corrosion detection; Fully convolutional networks; Rotation invariant; Structural health monitoring		Autonomous detection of structural defect from images is a promising, but also challenging task to replace manual inspection. With the development of deep learning algorithms, several studies have adopted deep convolutional neural networks (CNN) or fully convolutional networks (FCN) to detect cracks in pixel-level. However, a fundamental property of cracks, that they are rotation invariant, has never been exploited. Although the rotation-invariant property can be implicitly learned by data augmentation, the network needs more parameters to learn features of different orientations and thus tend to overfit the training data. In this study, a rotation-invariant FCN called ARF-Crack is proposed that utilizes the rotation-invariant property of cracks explicitly. The architecture of a state-of-the-art FCN called DeepCrack for pixel-level crack detection is adopted and revised where active rotating filters (ARFs) are used to encode the rotation-invariant property into the network. The proposed ARF-Crack is evaluated on several benchmark datasets including concrete cracks, pavement cracks and corrosion images. The experimental results show that the proposed ARF-Crack requires less number of network parameters and achieves the highest average precision values for all the benchmark datasets compared to other approaches. The proposed ARF-Crack has the potential of detecting other rotation-invariant defects.																	0932-8092	1432-1769				JUL 18	2020	31	6							47	10.1007/s00138-020-01098-x													
J								A weight optimized artificial neural network for automated software test oracle	SOFT COMPUTING										Evolutionary algorithms; Artificial neural network; Neural science; Soft computing; Software testing; Test cases; Test oracle; Stochastic diffusion search		Software testing has its main goal as designing new test case sets in a manner in which it is able to depict its maximum faults. As soon as these test cases have been designed, Oracle software provides a method in which the software has to behave for a particular test case given. Prioritization of such test cases with the execution of their components specifying inputs, their operation and their outcome will determine as to whether the application and their properties are working in the right manner. The prioritization methods are as follows: initial ordering, random ordering and finally reverse ordering that were based on fault detection abilities. For developing software applications, a test suite that was less commonly known as the suite for checking the validity of software was employed. The test suite contained a detailed set of instructions and goals for each test case collection based on the system and its configuration used during testing. Automating the generation of a test case and test oracle was researched in an extensive manner. From among the automated test oracle, the artificial neural network (ANN) was used extensively but with a high cost of computation. This work proposed a weight optimized ANN using stochastic diffusion search to find the optimal weights with a unique fitness function such that computational time is reduced and misclassification rate reduced.																	1432-7643	1433-7479				SEP	2020	24	17					13501	13511		10.1007/s00500-020-05197-9		JUL 2020											
J								Best-worst method for robot selection	SOFT COMPUTING										Group best-worst method (G-BWM); Group analytic hierarchy process (G-AHP); Multiple attribute group decision making; Subjective weights; Objective weights; integrated weights	DECISION-SUPPORT-SYSTEM; MAKING METHOD; TOPSIS METHOD; MODEL; IDENTIFICATION; ENHANCEMENT; DIGRAPH	For different applications, there are different robots having capabilities and specifications accordingly. For a particular application and industrial requirement, proper and suitable selection of robot is a difficult task. Numerous robot selection methods are available. Considering the research works on industrial robot selection, group best-worst method is employed in this paper for the proper selection of robots. Weighing the decision makers by considering their past experience is an important factor considered for expert and reliable selection of robot. Objective weights to describe the importance of the attributes along with the decision maker subjective preferences to describe the weights of the attribute are considered. Two problems are discussed for a detailed description and results are compared with the well-known group analytical hierarchy process method. The results show that due to lower minimum violation and lower total deviation, the proposed method performs better.																	1432-7643	1433-7479															10.1007/s00500-020-05169-z		JUL 2020											
J								Reversible hiding method for interpolation images featuring a multilayer center folding strategy	SOFT COMPUTING										Multilayer folding; Reversible information hiding; Re-encoding; Center folding strategy; Interpolation image	SCHEME; EXPANSION	In 2015, Lu proposed the (k,F-1) interpolation-based reversible hiding scheme by using the center folding strategy (CFS) to improve the image quality of the interpolation neighboring pixels scheme. In Lu's scheme,kis used to define the total number of thresholds for categories of the complexity of the block, andF(1)is the length of the secret message that can be concealed in the smoothest area. The hiding performance of Lu's scheme for the smooth image is great. However, the results of that scheme for complex images can still be improved. Therefore, this paper proposed an enhanced interpolation-based hiding scheme by modifying the structure of the CFS from having a single layer to having multiple layers. The secret data are encoded by the multiple-layer folding table to reduce the distortion between the original image and the stego-image. Experimental results show that the image quality and the payload of the proposed scheme are better than those of the other schemes.																	1432-7643	1433-7479															10.1007/s00500-020-05129-7		JUL 2020											
J								A novel two-stage optimized model for logo-based document image retrieval based on a soft computing framework	SOFT COMPUTING										Two-stage optimization; Logo; Document retrieval; Genetic algorithm	FEATURES; SCALE	The rapid development of internet helps in organizing documents based on their specific data for large-scale organizations to small-scale organizations. Document retrieval system aims to organize the relevant documents and its information based on specific terms. The availability of information stored by organization requires inexpensive storage, and the searching mechanism needs to get the information-based documents very quickly in real time. This research aims to provide such document retrieval system through logo-based identification model to analyse and organize the documents. A two-stage optimization is implemented to obtain the proposed logo-based document retrieval system using genetic algorithm and inverted ant colony optimization. Utilization of genetic operators in document retrieval classification based on index terms reduces time consumption, and inverted ant colony optimization improves the retrieval efficiency. Parameters such as classification accuracy, precision, retrieval efficiency are observed and compared with existing conventional and hybrid models experimentally to validate the proposed model.																	1432-7643	1433-7479															10.1007/s00500-020-05192-0		JUL 2020											
J								Strategic facility location problems with linear single-dipped and single-peaked preferences	AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS											APPROXIMATION; MECHANISMS	We consider the design of mechanisms for locating facilities on an interval. There are multiple agents on the interval, each receiving a utility determined by their distances to the facilities. The objectives considered are maximization of social welfare (sum of utilities) and egalitarian welfare (minimum utility). Agents can misreport their locations, and so we require the mechanisms to be strategyproof-no agent should be able to benefit from misreporting; subject to strategyproofness, we attempt to design mechanisms that are approximately optimal (have small worst-case approximation ratios). The novelty of our work is the consideration of models in which single-dipped and single-peaked preferences exist simultaneously. We consider two models. In the first model, there is a single facility, and agents may disagree about its nature: some agents prefer to be near the facility, while others prefer to be far from it. In the second model, there are two facilities: a desirable facility that all agents want near, and an undesirable facility that all agents want far. We design a variety of approximately optimal strategyproof mechanisms for both models, and prove several lower bounds as well. For the social welfare objective, we provide best-possible deterministic strategyproof mechanisms in the first model and the second model. We then provide improved randomized strategyproof mechanisms for each model, as well as a non-tight lower bound on the worst-case approximation ratio attainable by such mechanisms for the first model. For the egalitarian welfare objective, we provide a lower bound on randomized strategyproof mechanisms for the first model, as well as an optimal (non-approximate) strategyproof mechanism for the second model. All of our mechanisms are also group strategyproof: no coalition of agents can unanimously benefit from misreporting.																	1387-2532	1573-7454				JUL 18	2020	34	2							49	10.1007/s10458-020-09472-9													
J								Multilabel classification by exploiting data-driven pair-wise label dependence	INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS										cross-entropy; data-driven; deep neural networks; label dependent; multi-label classification; pair-wise label dependence	STACKING	Exploiting label dependence is a widely used approach to boost classification performance for multilabel classification problems. However, most of the traditional label dependence methods have high time complexity, especially when combined with deep neural networks (DNNs). Thus they usually can not be efficiently applied in large-scale data sets. Recent advances in large-scale multilabel classification widely developed pair-wise ranking and structure-driven methods, but label dependence was little exploited. In most of the structure-driven methods, binary relevance (BR) with multiple binary cross-entropy (BCE) loss functions, a simple but effective method, is still the prior solution incorporation with DNNs in large-scale data sets. In this paper, we propose a novel loss function called label dependent cross-entropy (LDCE), which directly introduces label dependence to BCE loss function by data-driven conditional probability. Combined with deep convolutional neural networks (DCNNs), LDCE introduces no extra parameters and induces very little extra computational complexity. Moreover, we develop its tiny variant with sparse label dependence and its learnable version for automatic learning pair-wise label dependence. Within the BR scheme, LDCE outperforms BCE on seven widely used benchmark datasets. We also perform two large-scale multilabel image classification tasks (VOC 2007 and ChestX-ray14) with DCNNs, and LDCE outperforms BCE and achieves comparable results to the state-of-the-art.																	0884-8173	1098-111X				SEP	2020	35	9					1375	1396		10.1002/int.22257		JUL 2020											
J								Variable selection in support vector regression using angular search algorithm and variance inflation factor	JOURNAL OF CHEMOMETRICS										ASA-VIF; H-1 NMR; MIR; NIR; SVM	NEAR-INFRARED SPECTROSCOPY; HEAVY CRUDE-OIL; PHYSICOCHEMICAL PROPERTIES; STRUCTURAL-CHARACTERIZATION; PREDICTION; H-1-NMR	Here, we combine angular search algorithm and variance inflation factor (ASA-VIF) with support vector regression (SVR) (ASA-VIF-SVR) to estimate total acid number (TAN), basic nitrogen content (BNC), and sulfur content (SC) in Brazilian crude oils. To prevent the interference of outliers, we further developed a strategy for outlier identification and applied it to nonlinear models based on RMSE (root mean square error). ASA-VIF-SVR was applied to near- and mid-infrared spectroscopy (NIR and MIR) and hydrogen nuclear magnetic resonance (H-1 NMR) spectroscopy data available in a range of 93-194 samples. The models were evaluated for accuracy (root mean square error of calibration [RMSEC] and root mean square error of prediction [RMSEP]) and linearity (coefficient of determination,R-2). The removal of outliers increased accuracy and linearity of our models. The ASA-VIF model for TAN, BNC, and SC selected 0.37%, 0.93%, and 0.30% of variables from full NIR spectra; 0.21%, 0.27%, and 0.21% from full MIR; and 0.20%, 0.42%, and 0.15% from full(1)H NMR. In most cases, the best results were obtained with variable selection compared with the full dataset. Also,H-1 NMR generated more accurate and linear models with RMSEP andR(p)(2)of 0.0071 wt% and 0.86 for BNC and 0.0623 wt% and 0.79 for SC. TAN showed a better MIR result with RMSEP of 0.1426 mg KOH g(-1)andR(p)(2)of 0.47. The most important region for(1)H NMR and MIR was the one with the largest quantity of unpaired electrons (aromatic region).																	0886-9383	1099-128X														e3282	10.1002/cem.3282		JUL 2020											
J								Predicting tool wear size across multi-cutting conditions using advanced machine learning techniques	JOURNAL OF INTELLIGENT MANUFACTURING										Machine learning; Tool condition monitoring; Tool wear prediction		The need to monitor tool wear is crucial, particularly in advanced manufacturing industries, as it aims to maximise the lifespan of the cutting tool whilst guaranteeing the quality of workpiece to be manufactured. Although there have been many studies conducted on monitoring the health of cutting tools under a specific cutting condition, the monitoring of tool wear across multi-cutting conditions still remains a challenging proposition. In addressing this, this paper presents a framework for monitoring the health of the cutting tool, operating under multi-cutting conditions. A predictive model, using advanced machine learning methods with multi-feature multi-model ensemble and dynamic smoothing scheme, is developed. The applicability of the framework is that it takes into account machining parameters, including depth of cut, cutting speed and feed rate, as inputs into the model, thus generating the key features for the predictions. Real data from the machining experiments were collected, investigated and analysed, with prediction results showing high agreement with the experiments in terms of the trends of the predictions as well as the accuracy of the averaged root mean squared error values.																	0956-5515	1572-8145															10.1007/s10845-020-01625-7		JUL 2020											
J								An Adaptive Method for Gait Event Detection of Gait Rehabilitation Robots	FRONTIERS IN NEUROROBOTICS										gait event detection; inertial sensor; adaptive threshold; gait rehabilitation robot; adaptive method	IDENTIFICATION; ACCELEROMETRY; MACHINE; SYSTEM; SENSOR	Accurate gait event detection is necessary for control strategies of gait rehabilitation robots. However, due to personal diversity between individuals, it is a challenge for robots to detect a gait event at various stride frequencies. This paper proposes a novel method for gait event detection of a gait rehabilitation robot using a single inertial sensor mounted on the thigh. A self-adaptive threshold for detecting heel strike is obtained in real time via a linear regression model. Observable thresholds for toe off detection are constant at various stride frequencies. Experiments are conducted based on 20 healthy subjects and six hemiplegic patients wearing a gait rehabilitation robot and walking at various kinds of stride frequencies. The experimental results show that the proposed method can detect heel strike and toe off gait events within an average 2% gait cycle temporal errors and never miss two-gait event detection. Compared to the conventional thresholding method, this work presents a simple and robust application for gait event detection in healthy and hemiplegic subjects by one inertial sensor. The linear regression model can be applicable to different subjects walking at various stride frequencies.																	1662-5218					JUL 17	2020	14								38	10.3389/fnbot.2020.00038													
J								Receding Horizon Optimization Method for Solving theCops and RobbersProblems in a Complex Environment with Obstacles Categories (2), (5)	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Cops and Robbersproblems; Complex environment with obstacles; Iterative hp-adaptive mesh refinement; Orthogonal collocation method; Receding horizon optimization	PURSUIT-EVASION GAMES	Cops and Robbersproblems are classical examples of pursuit and evasion problems which are parts of key researches in the field of robotics. This study shall specifically focus on the evasion strategies of robbers. This study presents the receding horizon optimization method to obtain such strategies of robbers and solves theCops and Robbersproblems in a complex environment with obstacles. In this method, the robbers estimate the control variables of the cops in real time to address the difficulties in obtaining the complete pursuit strategies of the latter for solving the evasion strategies. This method also guarantees the real-time solutions of receding horizon optimization problems. Orthogonal collocation is utilized to discretize theCops and Robbersdynamic model, and then the resulting nonlinear programming problem is solved to obtain the optimal control. To improve the accuracy of the solution, we propose an iterative hp-adaptive mesh refinement strategy to satisfy the optimality conditions by adjusting the number of finite elements and the order of Lagrange polynomials. This mesh refinement strategy also iteratively uses finite elements and collocation points as well as applies the finite element merging strategy to improve the solution efficiency. The proposed method also provides a framework for solving other pursuit and evasion problems in a complex environment with obstacles.																	0921-0296	1573-0409				OCT	2020	100	1					83	112		10.1007/s10846-020-01188-y		JUL 2020											
J								SURE estimates for high dimensional classification	STATISTICAL ANALYSIS AND DATA MINING										empirical Bayes; high dimensional classification; shrinkage estimates; Stein's unbiased estimate of risk	NONPARAMETRIC EMPIRICAL BAYES; CLASS PREDICTION; SELECTION; CANCER; SHRINKAGE; VECTOR	In the present paper, we consider the high dimensional classification problem, which has become much important in many modern statistical studies and applications. We develop new classifiers based on Fisher's linear classification rule and empirical Bayes. In particular, we propose to employ the Stein's unbiased risk estimate (SURE) to estimate the sparse or non-sparse mean difference, which could be plugged into the linear classification rules. Using simulation studies under a variety of settings, we demonstrate that our classifiers perform well especially when the features are non-sparse. We also illustrate the use of the new proposal to classification problems in some real data examples.																	1932-1864	1932-1872				OCT	2020	13	5					423	436		10.1002/sam.11472		JUL 2020											
J								Understanding the influence of impulse buying toward consumers' post-purchase dissonance and return intention: an empirical investigation of apparel websites	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Impulse buying; Emotional responses; Product dissonance; Emotional dissonance; Return intention	COGNITIVE-DISSONANCE; SOCIAL COMMERCE; PRODUCT RETURNS; MOBILE COMMERCE; ONLINE; BEHAVIOR; PURCHASE; IMPACT; EMOTIONS; MODEL	In this information-based era, online shopping has become prevalent. A well-designed webpage interface (visual appeal) could promote consumers' emotions (pleasure and arousal), thus stimulating their impulse buying. However, existing researches regarding online impulse buying are mainly focused on the influence of website attribute stimulation on impulse buying, without probing into consumers' post-purchase cognitive dissonance (product dissonance and emotional dissonance), and their return intention. Therefore, this study aims to integrate external websites stimuli (visual appeal), emotional responses (pleasure and arousal), and consumers' impulse buying tendencies, in order to explore the effect on impulse buying and the subsequent effect of post-purchase behavior (post-purchase dissonance and return intention). An online survey was conducted on 428 participants who had purchased apparel products online, in order to empirically examine the proposed research model. Partial Least Squares (PLS) was employed to analyze the research model. The results show that (1) pleasure directly and positively influences consumers' impulse buying, whereas arousal indirectly influences consumers' impulse buying through pleasure, (2) impulse buying directly and positively influences consumers' product dissonance, whereas impulse buying indirectly influences consumers' emotional dissonance through product dissonance, (3) consumers' emotional dissonance directly and positively influences consumers' return intention, whereas product dissonance indirectly influences consumers' return intention through emotional dissonance.																	1868-5137	1868-5145															10.1007/s12652-020-02333-z		JUL 2020											
J								Pre-processing on remotely sensed data with unsupervised classification analysis	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Image enhancement; Clustering; k-means; k-medoids; Rstudio; Vegetation indices	K-MEANS; FUSION	The use of remote sensing and geographic information system (GIS) technologies grow drastically in recent years by ecologists around the globe. At present, using sophisticated sensors, there is a massive challenge in handling the high dense remotely captured information with spatial, spectral, temporal, and radiometric resolutions. This article addresses how to handle such large volume remotely sensed data using R programming with the aid of RStudio. We aim broadly two categories, such as image preprocessing and classification techniques on remotely sensed data. Image preprocessing methods such as false-color composite, pan-sharpening, single event upset error mitigation, and a view on the hyper spectral image. The other category comes with a focus on landscape spatial features analysis and unsupervised classification to analyze vegetation land. The CLARA (Clustering LARge Application) algorithm is used in this study which exploits k-medoids approaches for the unsupervised classification. Also, for results comparison, different vegetation indices such as normalized difference water index (NDWI), modified NDWI (MNDWI), and soil adjusted vegetation index are used for vegetation (land) analysis. Also, for the unsupervised classification, the Silhouette index is used to compare the clustering algorithms.																	1868-5137	1868-5145															10.1007/s12652-020-02317-z		JUL 2020											
J								Construction and evaluation of the human behavior recognition model in kinematics under deep learning	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Deep learning; Human behavior recognition; Convolutional neural network; TensorFlow platform	AUTOMATED DETECTION; SENSORS; VIDEO	To explore the construction and evaluation of the human behavior recognition model in kinematics by deep learning, the convolution neural network (CNN) in the field of deep learning was applied to build the CNN human behavior recognition algorithm model. The image data were collected from the KTH and Weizmann datasets and trained; then, the proposed algorithm was simulated by the TensorFlow platform. The results suggested that in the analysis of recognition effect of kinematic description in two datasets, the accuracy of the histogram of optical flow orientation (HOF) method was the worst in both KTH and Weizmann datasets, while the accuracy of the constructed Visual Geometry Group-16 (VGG-16) algorithm model was the highest. In the analysis of the accuracy of the KTH dataset, the boxing action had the highest accuracy of recognition, and the running action had the lowest accuracy of recognition. The average recognition value of all kinds of actions was 91.93%. In the precision analysis of the Weizmann dataset, the bending and hand waving actions had the most accurate recognition rate, while the running action had the lowest recognition rate.																	1868-5137	1868-5145															10.1007/s12652-020-02335-x		JUL 2020											
J								Improving seller-customer communication process using word embeddings	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Ecommerce; QA systems; Word embeddings; Social network analysis; Humanized computing		With the progress in technology innovations, business organizations have preferred usage of online trading instead of traditional ways of trading. Online stores let businessmen offer more variety of products without the need of having big warehouses. At the same time, online shopping also saves time of customers and let them enjoy buying-at-home experience. They have the facility of looking at different qualities and prices of the same products offered by different vendors and buy the most suitable one. Online shopping business has especially got lot of attention after emergence of online social media. There are lot of self-made entrepreneurs as well as business giants with their Facebook pages available for online shopping. With this mode of online shopping getting very popular among masses, there are some inherent problems attached with this mode of online shopping. One of the most common problem on online shopping Facebook pages is question-answering on each post. For each product post published on Facebook page, customers ask lots of questions in the form of comments and then Facebook page admin has to reply each question one by one. This can be a very cumbersome job in case that page has a significant number of customers. In case someone is not replied, goodwill of the company is affected which can in turn affect sales. In this paper, we propose several semantic approaches to tackle with this very important problem of online shopping and compare their results. We propose a simple but effective approach for question extraction from comments on a post and we provide a huge data collection for this formulated task. Results show that word embeddings based approach outperform proposed baseline and other semantic approaches.																	1868-5137	1868-5145															10.1007/s12652-020-02323-1		JUL 2020											
J								The homogeneous balance method and its applications for finding the exact solutions for nonlinear equations	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Gardner equation; One dimensional burgers equation; Exact solution; Homogeneous balance method	SOLITARY WAVE SOLUTIONS	The main objective of this paper is to solve non-linear equations for engineering and science applications. Various emerging engineering and science applications are using non-linear equation to represent the entire problem. Several earlier research works have been focused on solving linear and non-linear equation using different methods but they are application based. In recent days applications are described and represented mathematically in accordance to the non-linear, discreate and non-discrete data. Few research works have used homogeneous balance method for solving multi-dimension equations. Some of the earlier approaches have used homogeneous balance method for investigating WBKL, Kaup-Kupershmidt, and CSNLPD equations, whereas all the methods are not complexity reduced. In order to provide a complexity reduced, fast and highly suitable method for solving non-linear equations representing recent engineering applications. Also, it is well known that the homogeneous balance method is a powerful technique to symbolically compute traveling wave solutions of some nonlinear wave and evolution equations. Hence, in this paper the exact solutions of Gardner equation and Burgers equation are obtained by using homogeneous balance method and verified using MATHEMATICA.																	1868-5137	1868-5145															10.1007/s12652-020-02278-3		JUL 2020											
J								Active learning for hierarchical multi-label classification	DATA MINING AND KNOWLEDGE DISCOVERY										Active learning; Hierarchical multi-label classification; Predictive clustering trees	ENSEMBLES; TREES	Due to technological advances, a massive amount of data is produced daily, presenting challenges for application areas where data needs to be labelled by a domain specialist or by expensive procedures, in order to be useful for supervised machine learning purposes. In order to select which data points will provide more information when labelled, one can make use of active learning methods. Active learning (AL) is a subfield of machine learning which addresses methods to build models with fewer, but more representative instances. Even though AL has been vastly studied, it has not been thoroughly investigated in hierarchical multi-label classification, a learning task where multiple class labels can be assigned to an instance and these labels are hierarchically structured. In this work, we provide a public framework containing baseline and state-of-the-art algorithms suitable for this task. Additionally, we also propose a new algorithm, namely Hierarchical Query-By-Committee (H-QBC), which is validated on datasets from different domains. Our results show that H-QBC is capable of providing superior predictive performance results compared to its competitors, while being computationally efficient and parameter free.																	1384-5810	1573-756X				SEP	2020	34	5			SI		1496	1530		10.1007/s10618-020-00704-w		JUL 2020											
J								A Sanskrit-to-English machine translation using hybridization of direct and rule-based approach	NEURAL COMPUTING & APPLICATIONS										Machine translation; Sanskrit language grammar; CYK parser; BLEU score; Elasticsearch; POS tagging; Nltk	PUNJABI	The work in this paper presents a MTS from Sanskrit to English language using a hybridized form of direct and rule-based machine translation technique. This paper also discusses the language divergence among Sanskrit and English languages with a recommended solution to handle the divergence. The proposed system has used two bilingual dictionaries (Sanskrit-English, Sanskrit-UNL), a tagged Sanskrit corpus, a Sanskrit analysis rule base and an ELGR base. Elasticsearch technique has enhanced the translation speed of the proposed system for accessing the data from different data dictionaries and rule bases used for the system development. The system uses CFG in CNF for Sanskrit language processing and CYK parsing technique for processing the input Sanskrit sentence. This work also presents a novel algorithm which creates a parse tree from the parsing table. ELGR base and bilingual dictionaries generate the target language sentence. The proposed system is evaluated using natural language toolkit API in python and achieved a BLEU score of 0.7606, fluency score of 3.63 and adequacy score of 3.72. A comparison of the proposed system with state-of-the-art systems shows that the proposed system outperforms existing systems.																	0941-0643	1433-3058															10.1007/s00521-020-05156-3		JUL 2020											
J								Probabilistic active filtering with gaussian processes for occluded object search in clutter	APPLIED INTELLIGENCE										Active learning; Gaussian processes; Object search	MANIPULATION; FRAMEWORK	This paper proposes a Gaussian process model-based probabilistic active learning approach for occluded object search in clutter. Due to heavy occlusions, an agent must be able to gradually reduce uncertainty during the observations of objects in its workspace by systematically rearranging them. In this work, we apply a Gaussian process to capture the uncertainties of both system dynamics and observation function. Robot manipulation is optimized by mutual information that naturally indicates the potential of moving one object to search for new objects based on the predicted uncertainties of two models. An active learning framework updates the state belief based on sensor observations. We validated our proposed method in a simulation robot task. The results demonstrate that with samples generated by random actions, the proposed method can learn intelligent object search behaviors while iteratively converging its predicted state to the ground truth.																	0924-669X	1573-7497															10.1007/s10489-020-01789-y		JUL 2020											
J								A hybrid stock price index forecasting model based on variational mode decomposition and LSTM network	APPLIED INTELLIGENCE										Decomposition-and-ensemble; VMD; LSTM; Stock price forecasting; Hybrid model	FINANCIAL TIME-SERIES; RECURRENT NEURAL-NETWORK; DEEP LEARNING-MODEL; MOVEMENT PREDICTION; COMPONENT ANALYSIS; VOLATILITY; GRAPH	Changes in the composite stock price index are a barometer of social and economic development. To improve the accuracy of stock price index prediction, this paper introduces a new hybrid model, VMD-LSTM, that combines variational mode decomposition (VMD) and a long short-term memory (LSTM) network. The proposed model is based on decomposition-and-ensemble framework. VMD is a data-processing technique through which the original complex series can be decomposed into a limited number of subseries with relatively simple modes of fluctuations. It can effectively overcome the shortcomings of mode mixing that sometimes exist in the empirical mode decomposition (EMD) method. LSTM is an improved version of recurrent neural networks (RNNs) that introduces a "gate" mechanism, and can effectively filter out the critical previous information, making it suitable for the financial time series forecasting. The capability of VMD-LSTM in stock price index forecasting is verified comprehensively by comparing with some single models and the EMD-based and other VMD-based hybrid models. Evaluated by level and directional prediction criteria, as well as a newly introduced statistic called the complexity-invariant distance (CID), the VMD-LSTM model shows an outstanding performance in stock price index forecasting. The hybrid models perform significantly better than the single models, and the forecasting accuracy of the VMD-based models is generally higher than that of the EMD-based models.																	0924-669X	1573-7497															10.1007/s10489-020-01814-0		JUL 2020											
J								Planar max flow maps and determination of lanes with clearance	AUTONOMOUS ROBOTS										Path planning; Multi-agent navigation; Maximum flow; Graphics processors		One main challenge in multi-agent navigation is to generate trajectories minimizing bottlenecks in environments cluttered with obstacles. In this paper we approach this problem globally by taking into account the maximum flow capacity of a given polygonal environment. Given the difficulty in solving the continuous maximum flow of a planar environment, we present in this paper a GPU-based methodology which leads to practical methods for computing maximum flow maps in arbitrary two-dimensional polygonal domains. Once a flow map representation is obtained, lanes can be extracted and optimized in length while keeping constant the flow capacity achieved by the system of trajectories. This work extends our previous work on max flow maps by presenting a clearance-based flow generation method which takes into account the size of the agents at the flow generation phase. In this way we ensure that the maximum possible number of lanes with the needed clearance is always obtained, a property that was found to not be always obtained with our previous method. As a result we are able to generate trajectories of maximum flow from source to sink edges across a generic set of polygonal obstacles, enabling the deployment of large numbers of agents utilizing the maximum flow capacity of a continuous description of the environment and eliminating bottlenecks.																	0929-5593	1573-7527				SEP	2020	44	7			SI		1323	1339		10.1007/s10514-020-09917-w		JUL 2020											
J								Evolving granular control with high-gain observers for feedback linearizable nonlinear systems	EVOLVING SYSTEMS										High-gain observers; Evolving systems; Adaptive feedback linearization; Robust control	TRACKING	Feedback linearization control is a simple and effective strategy whenever a faithful model of the system and its states are available. Feedback linearization may suffer from the mismatches between the model used in the design and the actual system due to e.g. uncertain parameter values, parasitic dynamics, or because of the impossibility to measure some states of the system. To aleviate such an issue, we suggest a novel robust adaptive control approach using the evolving participatory learning algorithm together with a high-gain observer. The robust evolving granular high-gain observers (RegHGO) controller is suitable to control nonlinear systems that can be input-output linearized by feedback. The approach is robust to modeling mismatches and does not require full state availability because, once the system is in a suitable canonical form, a high gain observer can be constructed to supply the state information required for control. The usefulness and efficacy of the approach is shown using a fan and plate system, and an DC motor driven angular arm-position control. The fan and plate evaluates the controller in a regulation process, and the angular arm position control evaluates reference tracking perfromance. In both cases, time-varying parameter uncertainties disturb the closed-loop control system. Both, qualitative and quantitative performance evaluation of the RegHGO controller are done. Additionally, we compare the performance of the RegHGO controller with well-established methods such as exact feedback linearization with high-gain state observer and extensions. The results show that robust evolving granular control with high-gain observers achieves better performance than its counterparts.																	1868-6478	1868-6486															10.1007/s12530-020-09349-y		JUL 2020											
J								Distributed Multi-agent Deployment for Full Visibility of 1.5D and 2.5D Polyhedral Terrains	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Multi-agent deployment; 2; 5 terrain; 1; 5 terrain; Visibility; Surveillance; Algorithms; distributed; Planar graphs; Graph coloring; Guarding; Face-spanning subgraph	ALGORITHM	This paper presents deployment strategies to achieve full visibility of 1.5D and 2.5D polyhedral environments for a team of mobile robots. Agents may only communicate if they are within line-of-sight. In 1.5D polyhedral terrains we achieve this by algorithmically determining a set of locations that the robots can occupy in a distributed fashion. We characterize the time of completion of the resulting algorithm, which is dependent on the number of peaks and the initial condition. In 2.5D polyhedral terrains we achieve full visibility by asynchronously deploying groups of agents who utilize graph coloring and may start from differential initial conditions. We characterize the total number of agents needed for deployment as a function of the environment properties and allow the algorithm to activate additional agents if necessary. We provide lower and upper bounds for the time of completion as a function of the number of vertices in a planar graph representing the environment. We illustrate our results in simulation and an implementation on a multi-agent robotics platform.																	0921-0296	1573-0409															10.1007/s10846-020-01229-6		JUL 2020											
J								Geometric Interpretation of the Multi-solution Phenomenon in the P3P Problem	JOURNAL OF MATHEMATICAL IMAGING AND VISION										P3P problem; Multiple solutions; Danger cylinder	POSE; VIEW	It is well known that the P3P problem could have 1, 2, 3 and at most 4 positive solutions under different configurations among its three control points and the position of the optical center. Since in any real applications, the knowledge on the exact number of possible solutions is a prerequisite for selecting the right one among all the possible solutions, and the study on the phenomenon of multiple solutions in the P3P problem has been an active topic since its very inception. In this work, we provide some new geometric interpretations on the multi-solution phenomenon in the P3P problem, and our main results include: (1) the necessary and sufficient condition for the P3P problem to have a pair of side-sharing solutions is the two optical centers of the solutions both lie on one of the three vertical planes to the base plane of control points; (2) the necessary and sufficient condition for the P3P problem to have a pair of point-sharing solutions is the two optical centers of the solutions both lie on one of the three so-called skewed danger cylinders;(3) if the P3P problem has other solutions in addition to a pair of side-sharing (point-sharing) solutions, these remaining solutions must be a point-sharing (side-sharing ) pair. In a sense, the side-sharing pair and the point-sharing pair are companion pairs; (4) there indeed exist such P3P problems that have four completely distinct solutions, i.e., the solutions sharing neither a side nor a point, closing a long guessing issue in the literature. In sum, our results provide some new insights into the nature of the multi-solution phenomenon in the P3P problem, and in addition to their academic value, they could also be used as some theoretical guidance for practitioners in real applications to avoid occurrence of multiple solutions by properly arranging the control points.																	0924-9907	1573-7683				NOV	2020	62	9					1214	1226		10.1007/s10851-020-00982-5		JUL 2020											
J								Service cost-based resource optimization and load balancing for edge and cloud environment	KNOWLEDGE AND INFORMATION SYSTEMS										Service cost; Resource optimization; Load balancing	USER EXPERIENCE; ALLOCATION	The application of edge clouds is becoming more and more widespread. The resource optimization is one of the important research contents of edge cloud. Generally, the edge cloud has limited computing resources and energy. Resource optimization can make tasks perform efficiently and reduce costs. Therefore, achieving high energy efficiency while ensuring a satisfying user experience is critical. This paper proposes the resource optimization and load balancing model. By considering factors such as user preferences, SLA and cost, the algorithm of resource optimization determines the resources scheme of edge cloud. The data movement after resource optimization is achieved through migration strategies. The load balancing of the edge cloud environment can be ensured. The results of the experiment prove that our proposed algorithm can better control costs.																	0219-1377	0219-3116				NOV	2020	62	11					4255	4275		10.1007/s10115-020-01489-6		JUL 2020											
J								A joint optimization framework for better community detection based on link prediction in social networks	KNOWLEDGE AND INFORMATION SYSTEMS										Social Network Analysis; Community Detection; Link Prediction; Joint Optimization	GRAPHS	Real-world network data can be incomplete (e.g., the social connections are partially observed) due to reasons such as graph sampling and privacy concerns. Consequently, communities detected based on such incomplete network information could be not as reliable as the ones identified based on the fully observed network. While existing studies first predict missing links and then detect communities, in this paper, a joint optimization framework,Communities detected on Predicted Edges, is proposed. Our goal to improve the quality of community detection through learning the probability of unseen links and the probability of community affiliation of nodes simultaneously. Link prediction and community detection are mutually reinforced to generate better results of both tasks. Experiments conducted on a number of well-known network data show that the proposed COPE stably outperforms several state-of-the-art community detection algorithms.																	0219-1377	0219-3116				NOV	2020	62	11					4277	4296		10.1007/s10115-020-01490-z		JUL 2020											
J								Representing uncertainty about fuzzy membership grade	SOFT COMPUTING										Uncertainty; Fuzzy set; Confusion; Multi-criteria decision making	SETS	A novel uncertainty representation framework is introduced based on the inter-linkage between the inherent fuzziness and the agent's confusion in its representation. The measure of fuzziness and this confusion is considered to be directly related to the lack of distinction between membership and non-membership grades. We term the proposed structure as confidence fuzzy set (CFS). It is further generalized as generalized CFS, quasi CFS and interval-valued CFS to take into consideration the DM's individualistic bias in the representation of the underlying fuzziness. The operations on CFSs are investigated. The usefulness of CFS in multi-criteria decision making is discussed, and a real application in supplier selection is included.																	1432-7643	1433-7479				SEP	2020	24	17					12691	12707		10.1007/s00500-020-05050-z		JUL 2020											
J								ECG signal processing and KNN classifier-based abnormality detection by VH-doctor for remote cardiac healthcare monitoring	SOFT COMPUTING										Advanced RISC machine; Wearable sensor; ECG data analysis; Field-programmable gate array (FPGA); Lookup table (LUT); Classification; KNN; Feature extraction	WAVELET TRANSFORM; NOISE; ALGORITHM; EFFICIENT; MIXTURE; SYSTEM; HRV	With the invent of medical expert systems, the demand for efficient innovative techniques in signal processing to detect abnormalities is ever increasing for identifying heart-related problems. The major objective of this research is to offer medical services to people in remote villages at low cost. People in villages and remote areas do not have a facility to get treated by a medical expert. This research provides them an opportunity to get medical advice through the virtual environment called the VH-doctor machine. It is a virtual environment heart doctor and reduces the human effort in testing and treating of heart diseases at the initial stages. The patients are treated and diagnosed only with the help of machines but not human effort. Biomedical sensors, ARM processor and FPGA are used to detect, test, analyze and display normal or abnormal cases. In this research, ECG signal processing, feature extraction and KNN classifier are performed and achieve the highest accuracy of 99% better than other machine learning algorithms.																	1432-7643	1433-7479				NOV	2020	24	22					17457	17466		10.1007/s00500-020-05191-1		JUL 2020											
J								A study on stegomyia indices in dengue control: a fuzzy approach	SOFT COMPUTING										Fuzzy logic; House index; Container index; Breteau index; Dengue occurrence	KRILL HERD ALGORITHM; AEDES-AEGYPTI; RISK; TRANSMISSION; MANAGEMENT; SINGAPORE; VECTOR; SYSTEM	The vector-borne disease, dengue, is becoming one of the most serious threats for the world population. Dengue hemorrhagic fever and dengue shock syndrome can cause death for an infected person. Increment in the number of dengue outbreaks on a yearly basis is making the researchers rethink about the dengue vector monitoring measures. In this paper, we have developed an artificial intelligence-based mathematical model using fuzzy logic to implement control measures timely in the dengue-prone areas. Here, a Mamdani-type fuzzy inference system is constructed taking the the three stegomyia indices, namely house index, Breteau index and container index, as the input parameters and the 'occurrence of dengue' as the output parameter. Finally, this model is implemented in a real-life scenario.																	1432-7643	1433-7479															10.1007/s00500-020-05179-x		JUL 2020											
J								Infinite games on finite graphs using grossone	SOFT COMPUTING										Infinite games; Grossone; Finite automata	COMPUTATIONS; METHODOLOGY; CONNECTION; AUTOMATA	In his seminal work, Robert McNaughton [see McNaughton (Ann Pure Appl Log 65:149-184, 1993) and Khoussainov and Nerode (Automata theory and its applications. Birkhauser, Basel, 2001)] developed a model of infinite games played on finite graphs. This paper presents a new model of infinite games played on finite graphs using the grossone paradigm. The new grossone paradigm provides certain advantages such as allowing for draws, which are common in board games, and a more accurate and decisive method for determining the winner.																	1432-7643	1433-7479															10.1007/s00500-020-05167-1		JUL 2020											
J								Shadowed type 2 fuzzy-based Markov model to predict shortest path with optimized waiting time	SOFT COMPUTING										Shadowed type 2 fuzzy; Markov model; Transition probability matrix; Shortest path and delay optimization		Recently, the traffic network exhibits a very critical situation due to the speedy rise of urbanization and population growth. This paper suggests a better solution for such traffic issues via delay-optimized Shortest Path Prediction (SPP) method. Even though Shadowed Type 2 (ST2) fuzzy logic works well for delay optimization with uncertain data, it causes a rise in fuzzy partitioning complexity. This motivates to development Shadowed Type 2 Fuzzy Markov (ST2FM) scheme for accurate prediction of the shortest path. In ST2FM, waiting for time optimization performed at rush junction based on ST2 fuzzy rules. Optimized path detail is periodically updated in the Transition Probability Matrix of the Markov model for SPP. Thus, ST2FM helps a node to easily identify the shortest path to reach the destination without waiting at traffic junctions. The absence of fuzzy partitioning and the use of Markov prediction greatly reduce the computational complexity of ST2FM. Matlab 2016a working environment is utilized for research implementation and results are compared with ST2 fuzzy, Interval Type 2 fuzzy and Fuzzy-based Convolution Neural Network. From this analysis, the proposed work demonstrates 96% of prediction accuracy with less error than existing works.																	1432-7643	1433-7479															10.1007/s00500-020-05194-y		JUL 2020											
J								Intelligent analysis framework for healthy environment spatial model of BIM horticultural therapy based on complex network information model	SOFT COMPUTING										Complex network model; Horticultural therapy; Interactive landscape; Landscape construction; Data mining		With the continuous development of modern life, people have paid more and more attention to the needs of healthy and comfortable outdoor activities. The traditional static recuperation mode has been impacted, and the modern dynamic leisure agricultural tourism health care mode is more and more popular. Horticultural therapy improves people's cognitive, psychological and physiological functions and provides people with positive values through plant-related sensory activities and horticultural operations. Therefore, horticultural therapy has become a product of the idea that designers have been exploring new forms of landscape architecture, trying to create a more comfortable environment and maximize the benefits of people in landscape architecture. In recent years, the theoretical and empirical studies of complex networks provide an important means to reveal the complexity of complex systems. As a new and active field of scientific research, complex networks have been introduced into the empirical research of real-world networks for a long time. At present, more and more people pay attention to computer science, social science, biological science, management science and many other fields. This paper constructs a BIM horticultural therapy health environment analysis system based on complex network model. The experimental results show that the proposed method can effectively analyze the application of BIM horticultural therapy in healthy environment.																	1432-7643	1433-7479															10.1007/s00500-020-05189-9		JUL 2020											
J								On a new weight tri-diagonal iterative method and its applications	SOFT COMPUTING										Iterative method; Convergence; High-order scheme	HEAT-EQUATION; ALGORITHM	In this article, a new weight tri-diagonal iterative method in solving system of linear equation is proposed and its convergence is discussed. The set of methods in solving linear system generated by high-order scheme for solving one-dimensional Poisson equation and one-dimensional heat equation with periodic boundary are presented. The numerical experiments shows that the proposed method demonstrates a better performance compared with weight Jacobi, successive-over relaxation and alternating group explicit methods.																	1432-7643	1433-7479															10.1007/s00500-020-05181-3		JUL 2020											
J								A random intuitionistic fuzzy factor analysis model for complex multi-attribute large group decision-making in dynamic environments	FUZZY OPTIMIZATION AND DECISION MAKING										Complex multi-attribute large group decision-making; Generalized intuitionistic fuzzy variable; Random intuitionistic fuzzy factor analysis; Dynamic decision environment		The challenge of complex multi-attribute large group decision-making (CMALGDM) is reflected from three perspectives: interrelated attributes, large group decision makers (DMs) and dynamic decision environments. However, there are few decision techniques that can address the three perspectives simultaneously. This paper proposes a random intuitionistic fuzzy factor analysis model, aiming to address the challenge of CMALGDM from the three perspectives. The proposed method effectively reduces the dimensionality of the original data and takes into account the underlying random environmental factors which may affect the performances of alternatives. The development of this method follows three steps. First, the random intuitionistic fuzzy variables are developed to deal with a hybrid uncertain situation where fuzziness and randomness co-exist. Second, a novel factor analysis model for random intuitionistic fuzzy variables is proposed. This model uses specific mappings or functions to define the way in which evaluations are affected by the dynamic environment vector through data learning or prior distributions. Third, multiple correlated attribute variables and DM variables are transformed into fewer independent factors by a two-step procedure using the proposed model. In addition, the objective classifications and weights for attributes and DMs are obtained from the results of orthogonal rotated factor loading. An illustrative case and detailed comparisons of decision results in different environmental conditions are demonstrated to test the feasibility and validity of the proposed method.																	1568-4539	1573-2908															10.1007/s10700-020-09334-9		JUL 2020											
J								A novel approach for non-orthogonal multiple access for delay sensitive industrial IoT communications for smart autonomous factories	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Industrial IoT; Autonomous systems; Smart factories; Cross layer design using branch and bound scheme; NOMA-VLC; Power allocation across sub-channels; BER	CHALLENGES; SYSTEMS	In recent years, there has been a proliferation of versatile IoT devices are used in industrial communications. These IoT devices facilitate automation of various monitoring, actuation, security, manufacturing applications of smart industries. Thus, the growth of data-consuming application services also inevitable. In today's industry communications, orthogonal multiple access technologies are inefficient in the presence of such massive demands for connectivity and traffic. Current NOMA designs have been shown to significantly improve traditional device performance in terms of throughput and latency. However, their effect on perceived end-user experience has not yet been fully understood. In this research, the considered network is a downlink NOMA in VLC wireless network, where the base station allows sub-channels to multiple users, assigns different powers among the users who are allotted to the same subchannel and thereby allocates the power across sub-channels. A cross-layer is established between the physical and the data link layers to enhance the efficiency of energy. This cross-layer is formulated using a branch and bound scheme. Hence the proposed power scheme is a branch and bound based cross-layer power allocation algorithm which combines the physical and the data link layers.																	1868-5137	1868-5145															10.1007/s12652-020-02330-2		JUL 2020											
J								Using Feature-Based Description Logics to avoid Duplicate Elimination in Object-Relational Query Languages	KUNSTLICHE INTELLIGENZ												A sound inference procedure is presented for removing operations that eliminate duplicates in queries formulated in a bag-algebra. The procedure is shown complete for positive queries over finite databases, and operates by appeal to logical consequence problems for feature-based description logics in which a TBox embeds an object-relational schema. For unions of conjunctive queries in which an embedded schema excludes cover constraints, the procedure runs in PTIME, and in EXPTIME otherwise.																	0933-1875	1610-1987				SEP	2020	34	3			SI		355	363		10.1007/s13218-020-00666-7		JUL 2020											
J								The Machine Scenario: A Computational Perspective on Alternative Representations of Indeterminism	MINDS AND MACHINES										Machine scenario; Open future; Representations of indeterminism; Thought experiments	BRANCHING STRUCTURES ADEQUATE; TIME-TRAVEL; SEMANTICS	In philosophical logic and metaphysics there is a long-standing debate around the most appropriate structures to represent indeterministic scenarios concerning the future. We reconstruct here such a debate in a computational setting, focusing on the fundamental difference between moment-based and history-based structures. Our presentation is centered around two versions of an indeterministic scenario in which a programmer wants a machine to perform a given task at some point after a specified time. One of the two versions includes an assumption about the future behaviour of the machine that cannot be encoded in any programming instruction; such version has models over history-based structures but no model over a moment-based structure. Therefore, our work adds a new stance to the debate: moment-based structures can be said to rule out certain indeterministic scenarios that are computationally unfeasible.																	0924-6495	1572-8641															10.1007/s11023-020-09530-x		JUL 2020											
J								Synthetic Deliberation: Can Emulated Imagination Enhance Machine Ethics?	MINDS AND MACHINES										Artificial intelligence; Machine ethics; Pragmatism; Machine learning; STS; Philosophy of technology		Artificial intelligence is becoming increasingly entwined with our daily lives: AIs work as assistants through our phones, control our vehicles, and navigate our vacuums. As these objects become more complex and work within our societies in ways that affect our well-being, there is a growing demand for machine ethics-we want a guarantee that the various automata in our lives will behave in a way that minimizes the amount of harm they create. Though many technologies exist as moral artifacts (and perhaps moral agents), the development of a truly ethical AI system is highly contentious; theorists have proposed and critiqued countless possibilities for programming these agents to become ethical. Many of these arguments, however, presuppose the possibility that an artificially intelligent system can actually be ethical. In this essay, I will explore a potential path to AI ethics by considering the role of imagination in the deliberative process via the work of John Dewey and his interpreters, showcasing one form of reinforcement learning that mimics imaginative deliberation. With these components in place, I contend that such an artificial agent is capable of something very near ethical behavior-close enough that we may consider it so.																	0924-6495	1572-8641															10.1007/s11023-020-09531-w		JUL 2020											
J								DMRAE: discriminative manifold regularized auto-encoder for sparse and robust feature learning	PROGRESS IN ARTIFICIAL INTELLIGENCE										Supervised regularized auto-encoder; Manifold regularization; Robust feature learning; Discriminative sparse representation	REPRESENTATION; AUTOENCODERS; SIMILARITY	Although the regularized over-complete auto-encoders have shown great ability to extract meaningful representation from data and reveal the underlying manifold of them, their unsupervised learning nature prevents the consideration of class distinction in the representations. The present study aimed to learn sparse, robust, and discriminative features through supervised manifold regularized auto-encoders by preserving locality on the manifold directions around each data and enhancing between-class discrimination. The combination of triplet loss manifold regularization with a novel denoising regularizer is injected to the objective function to generate features which are robust against perpendicular perturbation around data manifold and are sensitive enough to variation along the manifold. Also, the sparsity ratio of the obtained representation is adaptive based on the data distribution. The experimental results on 12 real-world classification problems show that the proposed method has better classification performance in comparison with several recently proposed relevant models.																	2192-6352	2192-6360				SEP	2020	9	3					263	274		10.1007/s13748-020-00211-5		JUL 2020											
J								DNAVS: an algorithm based on DNA-computing and vortex search algorithm for task scheduling problem	EVOLUTIONARY INTELLIGENCE										Job shop scheduling; Vortex search; DNA computing	GENETIC ALGORITHM; SHOP; OPTIMIZATION; EVOLUTION	In this paper, we present DNAVS algorithm for the general job-shop scheduling problem (also known as Flexible JSSP). JSSP is an NP-complete problem, which means there is probably no deterministic or exact algorithm that can find the optimum solution in polynomial time for arbitrary instances of the problem, unless, P = NP. In the DNA computing algorithm, although the hardware is not accessible, in the future, we will have those computers that work based on biological hardware. Their most important advantages over silicon-based computers are their capacity for data storage. DNAVS is an improvement of the DNA computing algorithm by using a metaheuristic search algorithm called vortex search (VS) algorithm. The strongness of DNA computing is its parallelization. In the unavailability of DNA computers, the DNAVS reduces the time complexity of DNA computing by employing the VS algorithm. The efficiency of the DNAVS has been tested with standard test instances of job-shop scheduling problems. Our implementation results show that the DNAVS algorithm works effectively even for large scale instances on currently silicon-based computers.																	1864-5909	1864-5917															10.1007/s12065-020-00453-1		JUL 2020											
J								Optimization based routing model for the dynamic path planning of emergency vehicles	EVOLUTIONARY INTELLIGENCE										VANET; Dynamic path planning; K-paths; Traffic density; Average speed; Travel time	INFRASTRUCTURE; PROTOCOL	With the increasing number of traffic vehicles and urbanization, the frequencies of traffic accidents are growing exponentially. The research for routing emergency vehicles has gained significant attention in Vehicular Ad hoc Network (VANET) topology. The path planning for an emergency vehicle can avoid congestion and minimize the travel time to prevent accidents. Thus, this work proposes a dynamic path planning scheme for routing emergency vehicles. At first, the VANET is simulated and the dynamic path planning is performed after the arrival of emergency vehicles. The proposed dynamic path planning includes two steps, namely travel-time forecasting, and bi-model routing. The travel time is predicted using the Support Vector Regression model and the bimodal is employed to route the emergency vehicles optimally. The bimodal consists of two steps, such as K-path discovery and optimal path selection process. Initially, the possible K shortest path is detected from which the optimal path is selected by the proposed Exponential Bird Swarm Optimization Algorithm (Exp-BSA). The proposed Exp-BSA algorithm is designed by integrating the Exponentially Weighted Moving Average concept in Bird Swarm Algorithm (BSA). Thus, the proposed Exp-BSA determines the optimal paths based on the devised fitness function, and the selected paths are suggested to emergency vehicles for faster routing. The performance of the proposed dynamic path planning for an emergency vehicle based on Exp-BSA is evaluated using three simulation setups based on the metrics, such as distance, average traffic density, average speed, and travel time. Also, the performance of the proposed method is compared with the existing methods, such as SFLA-KC, K-Path, and BSA. From the results, it is exposed that the proposed Exp-BSA method achieves the overall best performance than the existing methods with a minimal distance of 48.404 m, an average traffic density of 0.015 min, an average speed of 1.392 m/s, travel time of 19.608 s.																	1864-5909	1864-5917															10.1007/s12065-020-00448-y		JUL 2020											
J								Capacitated single-allocation hub location model for a flood relief distribution network	COMPUTATIONAL INTELLIGENCE										flood; hub location; optimization; Tabu search; variable neighborhood search	VARIABLE NEIGHBORHOOD SEARCH; ROUTING PROBLEM; TABU SEARCH; FORMULATIONS; HEURISTICS; ALGORITHMS; SERVICE; TIME	In disaster management, the logistics for disaster relief must deal with uncontrolled variables, including transportation difficulties, limited resources, and demand variations. In this work, an optimization model based on the capacitated single-allocation hub location problem is proposed to determine an optimal location of flood relief facilities with the advantage of economies of scale to transport commodities during a disaster. The objective is to minimize the total transportation cost, which depends on the flood severity. The travel time is bounded to ensure that survival packages will be delivered to victims in a reasonable time. Owing to complexity of the problem, a hybrid algorithm is developed based on a variable neighborhood search and tabu search (VNS-TS). The computational results show that the VNS found the optimal solutions within a 2% gap, while the proposed VNS-TS found the optimal solution with a 0% gap. A case study of severe flooding in Thailand is presented with consideration of related parameters such as water level, hub capacity, and discount factors. Sensitivity analyses on the number of flows, discount factors, capacity, and bound length are provided. The results indicated that demand variation has an impact on the transportation cost, number of hubs, and route patterns.																	0824-7935	1467-8640				AUG	2020	36	3					1320	1347		10.1111/coin.12374		JUL 2020											
J								CoCon: A Conference Management System with Formally Verified Document Confidentiality	JOURNAL OF AUTOMATED REASONING										Information-Flow Security; Confidentiality; Unwinding Proof Method; Theorem Proving; Isabelle; HOL; Conference Management System	VERIFICATION; DECLASSIFICATION; SECURITY; MODEL	We present a case study in formally verified security for realistic systems: the information flow security verification of the functional kernel of a web application, the CoCon conference management system. We use the Isabelle theorem prover to specify and verify fine-grained confidentiality properties, as well as complementary safety and "traceback" properties. The challenges posed by this development in terms of expressiveness have led tobounded-deducibility security, a novel security model and verification method generally applicable to systems describable as input/output automata.																	0168-7433	1573-0670															10.1007/s10817-020-09566-9		JUL 2020											
J								A comparative study of breast cancer tumor classification by classical machine learning methods and deep learning method	MACHINE VISION AND APPLICATIONS										Benign and malignant tumors; Breast cancer; Ensemble method; Machine learning; Deep learning	IMAGE-ANALYSIS; PROSTATE; ARCHITECTURES; SEGMENTATION	In contemporary times, machine learning is being used in almost every field due to its better performance. Here, we consider different machine learning methods such as logistic regression, random forest, support vector classifier (SVC), AdaBoost classifier, bagging classifier, voting classifier, and Xception model to classify the breast cancer tumor and evaluate their performances. We used a standard dataset, i.e., breast Histopathology images, that has more than two lakhs color patches, each patch of size 50 x 50 scanned at the resolution of 40x. We use 60% of the above-mentioned dataset for training, 20% for validation, and 20% testing to all above-mentioned classifiers. The logistic regression classifier provides the scores of each precision, recall, and F1 measure as 0.72. The random forest method provides the score of each precision, recall, and F1 score as 0.80. The bagging and voting classifiers both have the values of each precision, recalls, and F1 scores as 0.81. In this case, both SVC and AdaBoost classifiers have the score of each precision, recall, and F1 score as 0.82, whereas in the case of the deep learning method, Xception model is used to have the score of each precision, recall, and F1 measure as 0.90 in the same condition. Thus, the Xception method performs the best among all mentioned methods in terms of each of the performance measures, i.e., precision, recall, and F1 score for the classification of breast cancer tumors. Thus, the importance of this research work is that we can classify tumors more accurately in less time. It may increase awareness of people toward breast cancer and decrease fears of tumors.																	0932-8092	1432-1769				JUL 16	2020	31	6							46	10.1007/s00138-020-01094-1													
J								Attribute-based public integrity auditing for shared data with efficient user revocation in cloud storage	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Shared data integrity; Attribute-based signatures; User privacy; Revocation; Cloud storage	PRIVACY; SCHEME; SECURE; CHALLENGES; ENCRYPTION; CHECKING	With the exponential growth of cloud storage services, users can easily form a group and share the data with one another in the group. Since the cloud is untrusted and users deprived of direct control over data, it is essential to guarantee the integrity of shared data in the cloud. Several public auditing schemes have been proposed based on public key infrastructure or identity-based cryptography to check the integrity of outsourced data. However, they suffer from complex key management. Besides, how to achieve user privacy and efficient revocation is also a challenge in shared data auditing. To address these issues, in this paper, we propose attribute-based public auditing for shared data in the cloud storage. In our scheme, users sign data blocks over a set of attributes without disclosing any identity information, and a unique public key is used to verify the integrity instead of the individual public key of each user. Further, our scheme achieves user revocation through proxy re-signatures. Security analysis proves that our scheme is provably secure. The performance analysis demonstrates the practicality of the scheme.																	1868-5137	1868-5145															10.1007/s12652-020-02302-6		JUL 2020											
J								Development of hand gesture recognition system using machine learning	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Wavelet transform; Morphological operation; Support vector machine; Bag of word; Key descriptor; Hand gesture recognition	WAVELET DESCRIPTOR; FEATURES	Human computer interaction (HCI) systems are increasing due to the demand for non-intrusive methods for communicating with machines. In this research article, vision based hand gesture recognition (HGR) System has been proposed using machine learning. This proposed system consists of three stages: segmentation, feature extraction and classification. The developed system is to be trained and tested using Sebastian Marcel static hand posture database which is available online. Discrete wavelet transform (DWT) along with modified Speed Up Robust Feature extraction technique has been used to extract rotation and scale invariant key descriptors. Then Bag of Word technique is used to develop the fixed dimension input vector that is required for the support vector machine. The classification accuracy of class 2 and class 4 which corresponds to the 'No' and 'grasp' gesture has reached 98%. The overall classification accuracy of the HGR system using SVM classifier is 96.5% with a recognition time of 0.024 s. Due to fast recognition time, this system can be employed in real time gesture image recognition system. Our HGR system addresses the complex background problem and also improves the robustness of hand gesture recognition.																	1868-5137	1868-5145															10.1007/s12652-020-02314-2		JUL 2020											
J								Positioning of WiFi devices for indoor floor planning using principal featured Kohonen deep structure	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Indoor positioning; Principal feature; Sampling; Auto encoder; Kohonen deep structure; Medium access control (MAC) address; Receiving signal strength (RSS)	SYSTEM; TRACKING	With the evolution of Internet and mobile intelligent devices, position based servers has become the borderline in the research area of information technology. WiFi technology is extensively exploited for efficient indoor floor planning due to low erection cost, suitable access, ease of expansion and popularization, etc. For effective indoor floor planning, Indoor Positioning System has to be designed initially so that objects are said to be located inside building wirelessly with minimum computational time and overhead. To explore this design, Ipin2016 Dataset is used, Principal Feature Enhanced Sampling with Auto Encoder for feature selection and built positioning model based on Kohonen Deep Structure. The method is called as Positioning of Wi-Fi devices for indoor floor planning using Principal Featured Kohonen Deep Structure (PF-KDS). First, spatial data analysis is carried out using Principal Feature Enhanced Auto Encoder algorithm. With this algorithm, principal features are first extracted, so that dimensionality reduction is said to achieve, therefore reducing the complexity involved in positioning. Next, with the dimensionality reduced features, Kohonen Self Organizing Deep Structured Learning algorithm is designed. In this algorithm, list of single medium access control (MAC) address and analogous Receiving Signal Strength (RSS) is recorded by considering a new path loss model including walls' influence on RSSI, therefore improving the positioning accuracy. Our results indicate that combination of Principal Feature Enhanced Sampling with Kohonen Deep Structure provides high positioning accuracy with minimum time and overhead for Indoor Positioning.																	1868-5137	1868-5145															10.1007/s12652-020-02326-y		JUL 2020											
J								Model compression via pruning and knowledge distillation for person re-identification	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Person re-identification; Model compression and acceleration; Parameter pruning; Knowledge distillation	NETWORKS	Person re-identification (ReID) is an important problem in intelligent monitoring. Recently, with the development of deep learning, convolutional neural networks have achieved state-of-the-art performance on person ReID problems. However, the deep neural network models used by these methods tend to have large number of parameters and high computational cost, thereby hindering their deployment on resource-constraint devices or real-time applications. In this study, we propose a method that distills the knowledge to a pruned model to reduce the parameters, which can be divided into two stages: one is to apply unstructured pruning method on over-parameterized models, whereas the other is to carry out representation and metric learning-based knowledge distillation on the model after pruning to improve performance. Finally, the proposed method can effectively reduce the total number of parameters by 8.4 with only 0.1% drop of rank-1 accuracy on the Market1501 dataset and no drop of rank-1 accuracy on the DukeMTMC-reID dataset.																	1868-5137	1868-5145															10.1007/s12652-020-02312-4		JUL 2020											
J								Precise temporal slot filling via truth finding with data-driven commonsense	KNOWLEDGE AND INFORMATION SYSTEMS										Temporal slot; Slot filling; Truth finding; Information extraction	DISCOVERY	The task of temporal slot filling (TSF) is to extract values of specific attributes for a given entity, called "facts", as well as temporal tags of the facts, from text data. While existing work denoted the temporal tags as single time slots, in this paper, we introduce and study the task of Precise TSF (PTSF), that is to fill two precise temporal slots including the beginning and ending time points. Based on our observation from a news corpus, most of the facts should have the two points, however, fewer than 0.1% of them have time expressions in the documents. On the other hand, the documents' post time, though often available, is not as precise as the time expressions of being the time a fact was valid. Therefore, directly decomposing the time expressions or using an arbitrary post-time period cannot provide accurate results for PTSF. The challenge of PTSF lies in finding precise time tags in noisy and incomplete temporal contexts in the text. To address the challenge, we propose an unsupervised approach based on the philosophy of truth finding. The approach has two modules that mutually enhance each other: One is a reliability estimator of fact extractors conditionally on the temporal contexts; the other is a fact trustworthiness estimator based on the extractor's reliability. Commonsense knowledge (e.g., one country has only one president at a specific time) was automatically generated from data and used for inferring false claims based on trustworthy facts. For the purpose of evaluation, we manually collect hundreds of temporal facts from Wikipedia as ground truth, including country's presidential terms and sport team's player career history. Experiments on a large news dataset demonstrate the accuracy and efficiency of our proposed algorithm.																	0219-1377	0219-3116				OCT	2020	62	10					4113	4139		10.1007/s10115-020-01493-w		JUL 2020											
J								A descriptive framework for the field of knowledge management	KNOWLEDGE AND INFORMATION SYSTEMS										Knowledge management; Research direction; Literature; Grounded theory; Topic modeling; Text mining	GROUNDED THEORY; QUALITATIVE RESEARCH; BIG DATA; SYSTEMS; INFORMATION; IMPACT; EMPOWERMENT; CITATION; TRENDS; FUTURE	Despite the extensive evolution of knowledge management (KM), the field lacks an integrated description. This situation leads to difficulties in research, teaching, and learning. To bridge this gap, this study surveys 2842 articles from top-ranked KM journals to provide a descriptive framework that guides future research in the field of knowledge management. This study also seeks to provide a comprehensive depiction of current research in the field and categorizes these research activities into higher-level categories using grounded theory approach and topic modeling technique. The results show that KM studies are classified into four core research categories: technological, business, people, and domains/applications dimensions. An additional concern addressed in this study is the major research methodologies used in this field. The results raise awareness of the development of KM discipline and hold implications for research methodologies and research trends in the selected KM journals. The results obtained from this study also provide practitioners with a useful quality reference source. The framework and the components included provide researchers, practitioners, and educators with an ontology of KM topics, where they can cover deficiencies in research and provide an agenda for future research.																	0219-1377	0219-3116															10.1007/s10115-020-01492-x		JUL 2020											
J								Discrete Ricci curvature-based statistics for soft sets	SOFT COMPUTING										Soft sets; Computational simplex; Forman Ricci curvature; Ollivier Ricci curvature		Soft sets are efficient mathematical structures to model systems in multiple relations. Since a soft set is basically set system, it is possible to endow them with a proper distance function to obtain a metric space. By this embedding, we propose a discretization of the Ricci curvatures that stresses the relational character of universe elements in a soft set through the analysis of parameters rather than the elements themselves. The Forman and Ollivier-type Ricci curvatures we propose here quantifies the trade-off between parameter size and the cardinality of participation of parameterized universe elements in other parameters. Such discretizations of the Ricci curvature have already been applied to complex systems; however, it has not yet been formulated for soft sets. In this study, our main question is whether the defined geometric concept determines statistics for soft set models. Two examples are discussed for the answer to this question. The first example Ricci on soft sets model of occupational accidents occurred in Turkey in 2013-2014 is compared with the Wasserstein distance of the curvature distributions. The second example is the use of Ricci curvatures as an indicator in the soft sets model of a financial system while the system is in stress. These real world examples show that discrete Ricci curvatures for soft sets offer effective statistics.																	1432-7643	1433-7479															10.1007/s00500-020-05171-5		JUL 2020											
J								FOPA-MC: fuzzy multi-criteria group decision making for peer assessment	SOFT COMPUTING										Group decision making; Technology enhanced learning; Assessment	HIGHER-EDUCATION	Massive Open Online Courses are gaining popularity with millions of students enrolled, thousands of courses available and hundreds of learning institutions involved. Due to the high number of students and the relatively small number of tutors, student assessment, especially for complex tasks, is a typical issue of such courses. Thus, peer assessment is becoming increasingly popular to solve such a problem and several approaches have been proposed so far to improve the reliability of its outcomes. Among the most promising, there is fuzzy ordinal peer assessment (FOPA) that adopts models coming from fuzzy set theory and group decision Making. In this paper we propose an extension of FOPA supporting multi-criteria assessment based on rubrics. Students are asked to rank a small number of peer submissions against specified criteria, then provided rankings are transformed in fuzzy preference relations, expanded to obtain missing values and aggregated to estimate final grades. Results obtained are promising if compared to other peer assessment techniques both in the reconstruction of the correct ranking and on the estimation of students' grades.																	1432-7643	1433-7479															10.1007/s00500-020-05155-5		JUL 2020											
J								A novel normalized recurrent neural network for fault diagnosis with noisy labels	JOURNAL OF INTELLIGENT MANUFACTURING										Recurrent neural network; Deep neural network; Noisy labels; Fault diagnosis; Layer-wise relevance propagation	MACHINERY; CLASSIFICATION; TRANSFORM	The early fault diagnosis is a kind of important technology to ensure the normal and reliable operation of wind turbines. However, due to the potential presence of noisy labels in health condition dataset and the weakly explanation of the deep neural network decisions, the performance of fault diagnosis is severely limited. In this paper, a framework called normalized recurrent neural network (NRNN) is proposed for noisy label fault diagnosis, in which the normalized long short-term memory is used to improve the training process and the forward crossentropy loss is introduced to handle the negative effect of noisy labels. The effectiveness and superiority of the proposed framework are verified by four datasets with different noisy label proportions. Meanwhile, the layer-wise relevance propagation algorithm is applied to explore the decision of framework and by visualizing the relevances of input samples to framework decisions, the NRNN does not treat samples equally and prefers signal peaks for classification decisions.																	0956-5515	1572-8145															10.1007/s10845-020-01608-8		JUL 2020											
J								Prediction and optimization of performance measures in electrical discharge machining using rapid prototyping tool electrodes	JOURNAL OF INTELLIGENT MANUFACTURING										Electrical discharge machining (EDM); Selective laser sintering (SLS); Least square support vector machine (LSSVM); Desirability grey relational analysis (DGRA); Firefly algorithm (FA)	ARTIFICIAL NEURAL-NETWORK; MATERIAL REMOVAL RATE; EDM ELECTRODES; MULTIOBJECTIVE OPTIMIZATION; SURFACE-ROUGHNESS; FIREFLY ALGORITHM; PARAMETERS; MODEL; DESIGN; RESPONSES	In this work, the performance of rapid prototyping (RP) based rapid tool is investigated during electrical discharge machining (EDM) of titanium as work piece using EDM 30 oil as dielectric medium. Selective laser sintering, a RP technique, is used to produce the tool electrode made of AlSi10Mg. The performance of rapid tool is compared with conventional solid copper and graphite tool electrodes. The machining performance measures considered in this study are material removal rate, tool wear rate and surface integrity of the machined surface measured in terms of average surface roughness (Ra), white layer thickness, surface crack density and micro-hardness on white layer. Since the machining process is a complex one, potentiality of application of a predictive tool such as least square support vector machine has been explored to provide guidelines for the practitioners to predict various machining performance measures before actual machining. The predictive model is said to be robust one as root mean square error in the range of 0.11-0.34 is obtained for various performance measures. A hybrid optimization technique known as desirability based grey relational analysis in combination with firefly algorithm is adopted for simultaneously optimizing the performance measures. It is observed that peak current and tool type are the significant parameters influencing all the performance measures.																	0956-5515	1572-8145															10.1007/s10845-020-01624-8		JUL 2020											
J								Learning to Control a Quadcopter Qualitatively	JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS										Qualitative planning; Learning qualitative models; Qualitative simulation; Quadcopter control; Explainable control	QUADROTOR; NAVIGATION; FLY	Qualitative modeling allows autonomous agents to learn comprehensible control models, formulated in a way that is close to human intuition. By abstracting away certain numerical information, qualitative models can provide better insights into operating principles of a dynamic system in comparison to traditional numerical models. We show that qualitative models, learned from numerical traces, contain enough information to allow motion planning and path following. We demonstrate our methods on the task of flying a quadcopter. A qualitative control model is learned through motor babbling. Training is significantly faster than training times reported in papers using reinforcement learning with similar quadcopter experiments. A qualitative collision-free trajectory is computed by means of qualitative simulation, and executed reactively while dynamically adapting to numerical characteristics of the system. Experiments have been conducted and assessed in the V-REP robotic simulator.																	0921-0296	1573-0409															10.1007/s10846-020-01228-7		JUL 2020											
J								Optimal Sizing of Recycling Folded Cascode Amplifier for Low-Frequency Applications Using New Hybrid Swarm Intelligence-Based Technique	APPLIED ARTIFICIAL INTELLIGENCE											PARTICLE SWARM; OPTIMIZATION; ALGORITHM	A new efficient design approach for sizing a high-performance analog amplifier circuit namely the recycling folded cascode (RFC) amplifier is presented. An RFC amplifier is an enhanced version of the conventional folded cascode amplifier and achieves better slew rate, gain, bandwidth, offset, etc. for same area and power budget. Low-frequency amplifiers such as biomedical or neural have a demanding requirement of low area, low power, and low noise apart from meeting other optimal design specifications which have inherent trade-off among themselves. As a result, manual sizing becomes a computationally inefficient approach. Thus, swarm-based optimization techniques have been employed to efficiently determine the optimal sizing for the RFC amplifier such that the area is minimized while meeting all the optimal design specifications considering the constraints. A new hybrid whale particle swarm optimization (HWPSO) algorithm is employed which takes advantage of the good qualities of both the whale algorithm and the PSO algorithm to optimize the area with less computational complexity. Simulations and statistical analysis have been performed and comparisons with other state of art algorithms reveals that HWPSO-based approach achieves a minimum circuit area of 21 mu m(2)with a mean Friedman's statistical rank of 2.05 while meeting optimal design specifications for low-frequency systems. Finally, validation with circuit design tool Cadence Virtuoso is done and pre- as well as postlayout analysis have been performed which further illustrated a close agreement with algorithmic results.																	0883-9514	1087-6545				OCT 14	2020	34	12					880	897		10.1080/08839514.2020.1790163		JUL 2020											
J								Two-stage multi-sided matching dispatching models based on improved BPR function with probabilistic linguistic term sets	INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS										Multi-sided matching; Bureau of public road (BPR); Programming model; Probabilistic linguistic term set	TREE ANALYSIS; DISASTER	Disaster has great destructiveness and a wide influence on urban and rural construction, and more than one place is affected by disaster. Thus, it needs to take multiple disaster points into account. In view of different rescue missions, both medical rescue teams and search rescue teams also need to be dispatched. That is the multi-sided matching among medical rescue teams, search rescue teams and disaster points. Firstly, we describe the matching process, including the related symbols used in the process, and define a concept called multi-sided matching. Then, we aim to solve two problems: determining the competency degree of rescuers and calculating the time reliability of rescue teams arriving at disaster points. For the first one, we invite experts to evaluate pending rescue teams in terms of professional ability and collaboration ability using probabilistic linguistic term sets (PLTSs) because PLTSs not only keep original linguistic information but also give distributed expressions. For the second one, we determine the arriving time and calculating the time reliability by using the improved Bureau of Public Road (BPR) function. After that, we construct the two-stage multi-sided matching programming models based on the improved BPR function and PLTSs. Finally, a case study is used to demonstrate the proposed matching process, some comparative analyses and discussions are also conducted to validate the proposed models.																	1868-8071	1868-808X															10.1007/s13042-020-01162-y		JUL 2020											
J								Subject independent emotion recognition system for people with facial deformity: an EEG based approach	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Convolutional neural network; Affective computing; Valence arousal model; Scalogram		Emotion recognition from Electroencephalography (EEG) is a better choice for the people with facial deformity like where facial data is not accurate or not available for example burned or paralyzed faces. This research exploits the image processing capability of convolutional neural network (CNN) and proposes a CNN model to classify different emotions from the scalogram images of EEG data. Scalogram images from EEG obtained by applying continuous wavelet transform used for the study. The proposed model is subject independent where the objective is to extract emotion specific features from EEG data irrespective of the source of the data. The proposed emotion recognition model is evaluated on two benchmark public databases namely DEAP and SEED. In order to show the model as a purely subject independent one, the cross data base criteria is also used for evaluation. The various performance evaluation experiments show that the proposed model is comparable in terms of emotion classification accuracy.																	1868-5137	1868-5145															10.1007/s12652-020-02338-8		JUL 2020											
J								Complex interval-valued intuitionistic fuzzy TODIM approach and its application to group decision making	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										Soft computing; Fuzzy logic; Complex interval-valued intuitionistic fuzzy numbers; Prospect theory; Multi-criteria decision making	AGGREGATION OPERATORS; MATERIAL SELECTION; PROSPECT-THEORY; EXTENDED TODIM; SETS; PROJECTS; NUMBER	Present work proposes novel fuzzy information based TODIM approaches that can deal with the evaluations under complex interval-valued intuitionistic fuzzy (CIVIF) environment. The proposed approaches have been referred to as complex interval-valued intuitionistic fuzzy-TODIM (CIVIF-TODIM) approaches. The proposed method encompasses the characteristic features of a complex intuitionistic fuzzy set, interval-valued fuzzy set, and TODIM methodology. At first, the definitions associated with CIVIF have been discussed and then the methodological steps involved in classical TODIM have been delineated. The classical TODIM approach is then extended to deal with group decision-making problems under the CIVIF environment. Robustness, effectiveness, applicability, and the improvements made to the extant fuzzy TODIM methods by the proposed methodology have been adjudged through the consideration of illustrative examples solved by the past researcher. Sensitivity analysis with respect to the attenuation factor as well as the criteria weights has been provided to justify the robustness of the proposed methods. A comparative analysis with the existing fuzzy-TODIM approaches has been delineated and a comprehensive analysis of the ranking results obtained for different distance measures at each value of attenuation factor is provided towards the end of the work. The carried out inclusive analysis on the approaches that have been proposed in the present work reveals that the proposed CIVIF fuzzy TODIM approaches are superior to the existing fuzzy TODIM methods. Therefore, the present study provides its contribution to the domain of decision-making framework through the approaches that provide for dealing with complex, uncertain, and linguistic information in an efficient manner.																	1868-5137	1868-5145															10.1007/s12652-020-02308-0		JUL 2020											
J								Pragmatic results in Taiwan education system based IVFG & IVNG	SOFT COMPUTING										Interval-valued fuzzy graphs; Union and join; Interval-valued neutrosophic graphs; Complete and strong IVNG; Education system	FUZZY; GRAPHS	An interval-valued fuzzy graph (IVFG) and degree of vertices have been applied for performance evaluation in an educational system. The approach is mainly developed based on real membership values of vertices of an IVFG. In this manuscript, some definitions of generalized fuzzy graphs and neutrosophic graphs structures are improved. First, we have concentrated to improve the existing definitions for union and join of two IVFG's, complete IVFG with supporting examples. Then, their modified version is developed. Secondly, the modified version of interval-valued neutrosophic graph (IVNG) is given. Third, an algorithm and a flowchart of the proposed method are described. Fourth, the generalized form of complete and strong IVNG is given with examples. Finally, a real-life application using interval-valued fuzzy graph in education system in Taiwan is exhibited.																	1432-7643	1433-7479															10.1007/s00500-020-05180-4		JUL 2020											
J								Research on the integration of heterogeneous information resources in university management informatization based on data mining algorithms	COMPUTATIONAL INTELLIGENCE										colleges and universities; data mining; heterogeneous information; informatization; resource integration	SEMANTIC INTEGRATION	As far as the current higher education is concerned, the information system used in teaching and management is becoming more and more abundant, and the construction of its informatization is more and more rapid. However, the construction of university informatization project is too long, and there is no unified planning and other reasons, resulting in the difficulty of information and information management cannot be shared across departments. With the deepening of informationization and inaccurate information resources, how to use data mining technology to obtain valuable information is an important issue in the research of heterogeneous information in university management informationization. Based on the analysis of the information island problem in the informatization construction of colleges and universities, taking the student management work as a pilot, a heterogeneous information resource integration platform based on distributed query is proposed. Based on the platform, the information can be effectively applied globally. By analyzing its application in student information management, it shows that data mining is an effective technology to improve the management ability and scientific research level of colleges and universities in the construction of information technology.																	0824-7935	1467-8640															10.1111/coin.12365		JUL 2020											
J								The archaeology of the social brain revisited: rethinking mind and material culture from a material engagement perspective	ADAPTIVE BEHAVIOR										Social brain hypothesis; Material Engagement Theory; cognitive archaeology; Palaeolithic; material agency; human evolution	NEOCORTEX SIZE; DIRECT PERCEPTION; EVOLUTION; COGNITION; METAPLASTICITY; INTELLIGENCE; LANGUAGE; HYPOTHESIS; CHIMPANZEE; BEHAVIOR	The social brain hypothesis (SBH) has played a prominent role in interpreting the relationship between human social, cognitive and technological evolution in archaeology and beyond. This article examines how the SBH has been applied to the Palaeolithic material record, and puts forward a critique of the approach. Informed by Material Engagement Theory (MET) and its understanding of material agency, it is argued that the SBH has an inherently cognitivist understanding of mind and matter at its core. This Cartesian basis has not been fully resolved by archaeological attempts to integrate the SBH with relational models of cognition. At the heart of the issue has been a lack of meaningful consideration of the cognitive agency of things and the evolutionary efficacy of material engagement. This article proposes MET as a useful starting point for rethinking future approaches to human social cognitive becoming in a way that appreciates the co-constitution of brains, bodies and worlds. It also suggests how MET may bridge archaeological and 4E approaches to reconsider concepts such as the 'mental template' and Theory of Mind.																	1059-7123	1741-2633														1059712320941945	10.1177/1059712320941945		JUL 2020											
J								MR-OVnTSA: a heuristics based sensitive pattern hiding approach for big data	APPLIED INTELLIGENCE										Data quality; Knowledge hiding; Large-scale data; MapReduce; Non-feasibility; Sensitive frequent itemsets	ASSOCIATION RULES	This paper presents a novel 'MapReduce Based Optimum Victim Item and Transaction Selection Approach (MR-OVnTSA)' that provides a feasible and intelligent solution for protecting sensitive frequent itemsets present in big data. The approach advocates to resolve the captious challenges, existing knowledge hiding algorithms are encountering. The proposed solution optimally minimizes the side effect of hiding process on non-sensitive information, and maintains a balance between knowledge and privacy as well as handles the exponential growth in data volume efficiently. The algorithm plugs the most optimum item and transaction as victim, by intelligently analyzing their coverage value i.e. it chooses one with maximal impact on sensitive knowledge but minimal on non-sensitive information. Further, the MapReduce version of the proposed scheme resolves the issue of non-feasibility by processing large-scale data (big data) in a parallel fashion. Experiments have been demonstrated over real and synthetically generated large-scale datasets. Results evince that the proffered scheme is much more efficient and maintains the balance between the privacy preservation, data quality maintenance, and CPU time, when dealing with large voluminous big datasets compared to existing knowledge hiding techniques.																	0924-669X	1573-7497															10.1007/s10489-020-01749-6		JUL 2020											
J								Cycle representation-disentangling network: learning to completely disentangle spatial-temporal features in video	APPLIED INTELLIGENCE										Representation-disentangling; Representation-decoupling; Video representation learning; Motion and static features	IMAGE	Video representation learning is a significant problem of video understanding. However, the complex entangled spatiotemporal information in frames makes video representation learning a very tough task. Many studies have been conducted on decomposing video representation into dynamic and static features; existing works use either a two-stream structure or a compression strategy to learn the static and motion features from videos. However, neither approach can guarantee that networks learn features with low coupling degrees. To address this problem, we propose the exchangeable property (EP), a constraint that encourages networks to learn disentangled features from videos. Based on the EP, we propose a novel network called the Cycle Representation-Disentangling Network (CRD-Net), which adopts the strategy of exchanging features and reconstructing videos to factorize videos into stationary and temporal varying components. CRD-Net adopts a new training paradigm, as it is trained on paired videos with different static features but similar dynamic features. In addition, we introduce the pair loss and cycle loss, which forces the motion encoder to abandon time-invariant features and the consistent loss, which forces the static encoder to abandon time-variant features in the whole video. In experiments, we show the advantages of CRD-Net in completely disentangling video features and obtain better results than the state of the art on several video understanding tasks.																	0924-669X	1573-7497															10.1007/s10489-020-01750-z		JUL 2020											
J								Long-Short Temporal-Spatial Clues Excited Network for Robust Person Re-identification	INTERNATIONAL JOURNAL OF COMPUTER VISION										Person re-identification; Temporal-spatial clues; Long-short appearance model; Motion-refinement; Low-rank analysis		Directly benefiting from the rapid advancement of deep learning methods, person re-identification (Re-ID) applications have been widespread with remarkable successes in recent years. Nevertheless, cross-scene Re-ID is still hindered by large view variation, since it is challenging to effectively exploit and leverage the temporal clues due to heavy computational burden and the difficulty in flexibly incorporating discriminative features. To alleviate, we articulate a long-short temporal-spatial clues excited network (LSTS-NET) for robust person Re-ID across different scenes. In essence, our LSTS-NET comprises a motion appearance model and a motion-refinement aggregating scheme. Of which, the former abstracts temporal clues based on multi-range low-rank analysis both in consecutive frames and in cross-camera videos, which can augment the person-related features with details while suppressing the clutter background across different scenes. In addition, to aggregate the temporal clues with spatial features, the latter is proposed to automatically activate the person-specific features by incorporating personalized motion-refinement layers and several motion-excitation CNN blocks into deep networks, which expedites the extraction and learning of discriminative features from different temporal clues. As a result, our LSTS-NET can robustly distinguish persons across different scenes. To verify the improvement of our LSTS-NET, we conduct extensive experiments and make comprehensive evaluations on 8 widely-recognized public benchmarks. All the experiments confirm that, our LSTS-NET can significantly boost the Re-ID performance of existing deep learning methods, and outperforms the state-of-the-art methods in terms of robustness and accuracy.																	0920-5691	1573-1405				DEC	2020	128	12					2936	2961		10.1007/s11263-020-01349-4		JUL 2020											
J								Evolutionary multi-level acyclic graph partitioning	JOURNAL OF HEURISTICS										Graph partitioning; Evolutionary algorithm; Computer vision; Imaging; Embedded systems	PT-SCOTCH	Directed graphs are widely used to model data flow and execution dependencies in streaming applications. This enables the utilization of graph partitioning algorithms for the problem of parallelizing execution on multiprocessor architectures under hardware resource constraints. However due to program memory restrictions in embedded multiprocessor systems, applications need to be divided into parts without cyclic dependencies. We found that this can be done by a subsequent second graph partitioning step with an additional acyclicity constraint. We have four main contributions. First, we show that this more constrained version of the graph partitioning problem is NP-complete and present linear time heuristics. We then integrate them into an existingmulti-levelgraph partitioning framework to better handle large graphs. This achieves a 9% reduction of the edge cut compared to the previous single-level algorithm. Based on this, we engineer an evolutionary algorithm tofurtherreduce the cut, achieving a 30% reduction on average compared to the state of the art. Finally, we integrate the partitioning heuristics into a graph compiler for an embedded multiprocessor architecture and show that this can reduce the amount of communication for a real-world imaging application and thereby accelerate it by an average of 11%. It is shown that the compiler can emit optimized code for vastly different hardware platforms using the heuristics. In addition, we demonstrate how a custom fitness function for the evolutionary algorithm can be used to optimize other objectives like load balancing if the communication volume is not predominantly important on a given hardware platform.																	1381-1231	1572-9397				OCT	2020	26	5					771	799		10.1007/s10732-020-09448-8		JUL 2020											
J								The combination of visual communication cues in mixed reality remote collaboration	JOURNAL ON MULTIMODAL USER INTERFACES										Mixed reality; Remote collaboration; Communication cue; 3D scene reconstruction; Hand gesture; Sketch; pointer	GESTURE	Many researchers have studied various visual communication cues (e.g. pointer, sketch, and hand gesture) in Mixed Reality remote collaboration systems for real-world tasks. However, the effect of combining them has not been so well explored. We studied the effect of these cues in four combinations: hand only, hand + pointer, hand + sketch, and hand + pointer + sketch, within the two user studies when a dependent view and independent view are supported respectively. In the first user study with the dependent view, the results showed that the hand gesture cue was the main visual communication cue and adding sketch cue to the hand gesture cue helped participants complete the task faster. In the second study with the independent view, the results showed that the hand gesture had an issue of local worker understanding remote expert's hand gesture cue and the main visual communication cue was the pointer cue with fast completion time and high level of co-presence.																	1783-7677	1783-8738				DEC	2020	14	4			SI		321	335		10.1007/s12193-020-00335-x		JUL 2020											
J								Embedded chaotic whale survival algorithm for filter-wrapper feature selection	SOFT COMPUTING										Whale optimization algorithm; Feature selection; Embedded systems; Chaotic mapping; UCI dataset; Optimization; Heuristic; Algorithm; Benchmark	HYBRID GENETIC ALGORITHM; OPTIMIZATION ALGORITHM; BINARY PSO; CLASSIFICATION; BPSO; GA	Classification accuracy provided by a machine learning model depends a lot on the feature set used in the learning process. Feature selection (FS) is an important and challenging preprocessing technique which helps to identify only the relevant features from a dataset, thereby reducing the feature dimension as well as improving the classification accuracy at the same time. The binary version of whale optimization algorithm (WOA) is a popular FS technique which is inspired from the foraging behavior of humpback whales. In this paper, an embedded version of WOA called embedded chaotic whale survival algorithm (ECWSA) has been proposed which uses its wrapper process to achieve high classification accuracy and a filter approach to further refine the selected subset with low computation cost. Chaos has been introduced in the ECWSA to guide selection of the type of movement followed by the whales while searching for prey. A fitness-dependent death mechanism has also been introduced in the system of whales which is inspired from the real-life scenario in which whales die if they are unable to catch their prey. The proposed method has been evaluated on 18 well-known UCI datasets and compared with its predecessors as well as some other popular FS methods. The source code of ECWSA can be found in https://github.com/Ritam-Guha/ECWSA.																	1432-7643	1433-7479				SEP	2020	24	17					12821	12843		10.1007/s00500-020-05183-1		JUL 2020											
J								A load balance multi-scheduling model for OpenCL kernel tasks in an integrated cluster	SOFT COMPUTING										Machine learning; Classification; Feature selection; OpenCL; Optimization	GPU; CPU; TIME	Nowadays, embedded systems are comprised of heterogeneous multi-core architectures, i.e., CPUs and GPUs. If the application is mapped to an appropriate processing core, then these architectures provide many performance benefits to applications. Typically, programmers map sequential applications to CPU and parallel applications to GPU. The task mapping becomes challenging because of the usage of evolving and complex CPU- and GPU-based architectures. This paper presents an approach to map the OpenCL application to heterogeneous multi-core architecture by determining the application suitability and processing capability. The classification is achieved by developing a machine learning-based device suitability classifier that predicts which processor has the highest computational compatibility to run OpenCL applications. In this paper, 20 distinct features are proposed that are extracted by using the developed LLVM-based static analyzer. In order to select the best subset of features, feature selection is performed by using both correlation analysis and the feature importance method. For the class imbalance problem, we use and compare synthetic minority over-sampling method with and without feature selection. Instead of hand-tuning the machine learning classifier, we use the tree-based pipeline optimization method to select the best classifier and its hyper-parameter. We then compare the optimized selected method with traditional algorithms, i.e., random forest, decision tree, Naive Bayes and KNN. We apply our novel approach on extensively used OpenCL benchmarks, i.e., AMD and Polybench. The dataset contains 653 training and 277 testing applications. We test the classification results using four performance metrics, i.e., F-measure, precision, recall and R-2. The optimized and reduced feature subset model achieved a high F-measure of 0.91 and R-2 of 0.76. The proposed framework automatically distributes the workload based on the application requirement and processor compatibility.																	1432-7643	1433-7479															10.1007/s00500-020-05152-8		JUL 2020											
J								Handling of revenue sharing contracts within the scope of game theory	SOFT COMPUTING										Supply chain; Revenue sharing contract; Game theory; Two-person non-constant sum game	SUPPLY CHAIN COORDINATION; MANAGEMENT	One of the most important problems faced in supply chain management is the coordination between supply chain members. Supply chain contracts, such as buyback contract, quantity-flexibility contract, revenue sharing contract, etc., can be used for solving this problem. In revenue sharing contracts, the manufacturer applies a low wholesale price to the retailer and in return shares a portion of the retailer's revenue. In this study, the revenue sharing contracts between the supplier and the retailer are handled as two-person non-constant sum games. There are two players in the game, supplier and retailer, and the main motivation of both is to increase their profitability. To this end, the supplier applies different wholesale prices, while the retailer responds by using different revenue sharing rates. In the study, profit functions of the supply chain members are calculated in the event of an income sharing agreement. These functions differ due to changes in supply chain parameters. The profits of the retailer and supplier corresponding to different wholesale prices and revenue sharing rates have been transferred to the two-person non-constant sum game revenue matrix. Then, the best strategies of the supplier and retailer are tried to be determined by the search of equilibrium point which is one of the solution methods that can be used to determine the best strategies in two-person non-constant sum games. The results showed that the solution approaches of game theory can be applied successfully in determining the best strategies that the parties can implement in revenue sharing contracts between the supplier and the retailer.																	1432-7643	1433-7479															10.1007/s00500-020-05142-w		JUL 2020											
J								A constructive sequence algebra for the calculus of indications	SOFT COMPUTING											INFINITE; METHODOLOGY; GROSSONE; WORKING	In this paper, we investigate some aspects of Spencer-Brown's Calculus of Indications. Drawing from earlier work by Kauffman and Varela, we present a new categorical framework that allows to characterize the construction of infinite arithmetic expressions as sequences taking values in grossone.																	1432-7643	1433-7479															10.1007/s00500-020-05121-1		JUL 2020											
J								alpha-Paramodulation method for a lattice-valued logic LnF(X) with equality	SOFT COMPUTING										Lattice-valued logic; Equality; alpha-Equality axioms; alpha-Paramodulation; alpha-GH paramodulation	RESOLUTION PRINCIPLE; ALGORITHM	In this paper, alpha-paramodulation and alpha-GH paramodulation methods are proposed for handling logical formulae with equality in a lattice-valued logic LnF(X), which has unique ability for representing and reasoning uncertain information from a logical point of view. As an extension of the work of He et al. (in: 2015 10th international conference on intelligent systems and knowledge engineering (ISKE), pp 18-20. IEEE, 2015; Uncertainty modelling in knowledge engineering and decision making: proceedings of the 12th international FLINS conference, pp 477-482. World Scientific, 2016), a new form of alpha-equality axioms set is proposed. The equivalence between alpha-equality axioms set and E-alpha-interpretation in LnF(X) with an appropriate level is also established, which may provide a key foundation for equality reasoning in lattice-valued logic. Based on its equivalence, E-alpha-unsatisfiability equivalent transformation is given. Furthermore, alpha-paramodulation and its restricted method (i.e., alpha-GH paramodulation) are given. The soundness and completeness of the proposed methods are also examined.																	1432-7643	1433-7479															10.1007/s00500-020-05136-8		JUL 2020											
J								Option pricing formulas for uncertain exponential Ornstein-Uhlenbeck model with dividends	SOFT COMPUTING										Uncertainty theory; Uncertain finance; Dividend; Exponential Ornstein-Uhlenbeck model	STOCK MODEL; STABILITY	Uncertain finance is an application to the finance of the uncertainty theory which provides an alternative analysis method from the probability theory under the circumstance that few samples are available. Under the research paradigm of uncertain finance, some financial assets are investigated and modeled with various tools in order to describe the price fluctuation accurately. This paper models the prices of stocks under uncertain finance based on the exponential Ornstein-Uhlenbeck model with periodic dividends. Through the alpha-path method, the pricing formulas are derived for European call and put options whose underlying assets follow the proposed stock model. In addition, some numerical algorithms to calculate the option prices are designed.																	1432-7643	1433-7479															10.1007/s00500-020-05177-z		JUL 2020											
J								HBDCWS: heuristic-based budget and deadline constrained workflow scheduling approach for heterogeneous clouds	SOFT COMPUTING										Workflow scheduling; Budget; Deadline; Planning success ratio (PSR)	SCIENTIFIC WORKFLOWS; ALGORITHM; PERFORMANCE	The predilection of scientific applications toward a high-performance computing system is attained through the emergence of the cloud. Large-scale scientific applications can be modeled as workflows and are scheduled on the cloud. However, such scheduling becomes even more onerous due to the dynamic and heterogeneous nature of cloud and therefore considered as a problem of NP-Complete. The scheduling of workflows is always constrained to QoS parameters. Most of the applications are bound to time and cost, which is observed to be the most crucial parameter. Therefore, in this paper, a heuristic-based budget and deadline constrained workflow scheduling algorithm (HBDCWS) has been proposed to utilize those applications that have the budget and deadline constraints. The novelty of the proposed work is to provide a simple budget and deadline distribution strategy where budget and deadline of workflow are converted to level budget and level deadline. Additionally, the level budget is again transferred to each task. This strategy not only satisfies the given constraints but also proves to be efficient for minimizing the makespan and reducing the cost of execution. Experimental results on several workflows demonstrate that the proposed HBDCWS algorithm finds a feasible solution that accomplishes the given constraints with a higher success rate in most cases.																	1432-7643	1433-7479															10.1007/s00500-020-05127-9		JUL 2020											
J								Sentiment classification model for Chinese micro-blog comments based on key sentences extraction	SOFT COMPUTING										Sentiment classification; Chinese micro-blog; Key sentences; Modification distance; Multi-rules	CONVOLUTIONAL NEURAL-NETWORK	With the advancements of communication technology, the growth in the number of blogs has been remarkable. Information sharing has been an interesting matter of interest for researches, as people contribute to information shared through the Internet. Such sharing is common in the Chinese language, as approximately 15% of the world's population are native speakers of Mandarin. Due to comments shared that may contain complex sentences in such micro-blogs, the result of sentiment classification may be affected, reducing its accuracy. Aimed at this point, a Sentiment Classification model of Chinese Micro-blog Comments based on Key Sentences (SC-CMC-KS) is proposed. The key sentence extraction algorithm for Chinese Microblog Comments is presented by considering three factors to recognize the key sentences of a given comment: the sentiment attributes, location attributes, and the critical feature word attributes. Besides, a computing algorithm of sentence sentiment value that integrates both dependency relationships and multi-rules (i.e., sentence-type rule and inter-sentence rule) is designed, as well defined a modification distance describing the relationship between modification words and core words, in which the sentiment value in the phrase level is computed according to the calculation rules so that the sentiment value in sentence level is obtained based on the multi-rules. Furthermore, the sentiment classification algorithm of micro-blog comments is presented, so that the key sentences and emoticon of the complete micro-blog are weighted to compute the final sentiment value, and comments are classified according to the threshold set. Experimental results show the effectiveness of this model yet promising.																	1432-7643	1433-7479															10.1007/s00500-020-05160-8		JUL 2020											
J								A Classification Model for Predicting Fetus with down Syndrome - A Study from Turkey	APPLIED ARTIFICIAL INTELLIGENCE											PERCEPTRON NEURAL-NETWORK; MATERNAL SERUM; MULTILAYER PERCEPTRON; ALPHA-FETOPROTEIN; TRIPLE-TEST; MEDICAL DATA; DATA SETS; AGE; IMPACT; AMNIOCENTESIS	The triple test is a screening test (blood test) used to calculate the probability of a pregnant woman having a fetus that has a chromosomal abnormality like Down Syndrome (DS). AFP (Alpha-Fetoprotein), hCG (Human Chorionic Gonadotropin), and uE3 (Unconjugated Estriol) values in the blood sample of pregnant women are computed and compared with the similar real records where the outputs (healthy fetus or a fetus with DS) are actually known. The likelihood of the indicators is used to calculate the probability of having a fetus with chromosomal abnormality like DS. However, high false positive rate of the triple test has been a problematic issue. One of the reasons of the high false positives is the differences in the norm values of indicators for the pregnant women from different geographical regions of a country. We use 81 patient records retrieved from Sahinbey Training and Research Hospital of Gaziantep University; Turkey. In our study, nine different classification algorithms were trained based on triple test indicators. Multilayer perceptron outperformed with 94.24% detection rate and 13% false positive rate. The multilayer perceptron can predict the outcome of triple test with a high level of accuracy and fewer patients are suggested for amniocentesis. This study is the first study using the MLP model for Turkish triple test data. Regional MLP models can eliminate the bias due to local biological differences.																	0883-9514	1087-6545				OCT 14	2020	34	12					898	915		10.1080/08839514.2020.1790246		JUL 2020											
J								Dynamic knowledge graph based fake-review detection	APPLIED INTELLIGENCE										Fake review; Lstm; Knowledge graph; Data mining; Electronic commerce	SYSTEM	Online product reviews are an important driver of customers' purchasing behavior. Fake reviews seriously mislead consumers, challenging the fairness of the online shopping environment. Although the detection of fake reviews has progressed, several problems remain. First, fake comment recognition ignores the correlation between time and the semantics of the comment texts, which is always hidden in the context of the reviews. Second, the impact of multi-source information on fake comment recognition is not considered, as it constitutes a complex, high-dimensional, heterogeneous relationship between reviewers, reviews, stores and commodities. To overcome these problems, the present paper proposes a dynamic knowledge graph-based method for fake-review detection. Based on the characteristics of online product reviews, it first extracts four types of entities using a developed neural network model called sentence vector/twin-word embedding conditioned bidirectional long short-term memory. Time series related features are then added to the knowledge graph construction process, forming dynamic graph networks. To enhance the fake-review detection, four indicators are newly defined for determining the relationships among the four types of nodes. In experimental evaluations, our method surpassed the state-of-the-art results.																	0924-669X	1573-7497															10.1007/s10489-020-01761-w		JUL 2020											
J								Disturbance observer enhanced variable gain controller for robot teleoperation with motion capture using wearable armbands	AUTONOMOUS ROBOTS										Disturbance observer; Motion capture; Radial basis function neural networks; Teleoperation; Variable gain control	SPACE MANIPULATORS; TRACKING CONTROL; TIME; SYSTEMS; DELAY	Disturbance observer (DOB) based controller performs well in estimating and compensating for perturbation when the external or internal unknown disturbance is slowly time varying. However, to some extent, robot manipulators usually work in complex environment with high-frequency disturbance. Thereby, to enhance tracking performance in a teleoperation system, only traditional DOB technique is insufficient. In this paper, for the purpose of constructing a feasible teleoperation scheme, we develop a novel controller that contains a variable gain scheme to deal with fast-time varying perturbation, whose gain is adjusted linearly according to human surface electromyographic signals collected from Myo wearable armband. In addition, for tracking the motion of operator's arm, we derive five-joint-angle data of a moving human arm through two groups of quaternions generated from the armbands. Besides, the radial basis function neural networks and the disturbance observer-based control (DOBC) approaches are fused together into the proposed controller to compensate the unknown dynamics uncertainties of the slave robot as well as environmental perturbation. Experiments and simulations are conducted to demonstrated the effectiveness of the proposed strategy.																	0929-5593	1573-7527				SEP	2020	44	7			SI		1217	1231		10.1007/s10514-020-09928-7		JUL 2020											
J								Pix2Vox++: Multi-scale Context-aware 3D Object Reconstruction from Single and Multiple Images	INTERNATIONAL JOURNAL OF COMPUTER VISION										3D object reconstruction; Multi-scale; Context-aware; Convolutional neural network		Recovering the 3D shape of an object from single or multiple images with deep neural networks has been attracting increasing attention in the past few years. Mainstream works (e.g. 3D-R2N2) use recurrent neural networks (RNNs) to sequentially fuse feature maps of input images. However, RNN-based approaches are unable to produce consistent reconstruction results when given the same input images with different orders. Moreover, RNNs may forget important features from early input images due to long-term memory loss. To address these issues, we propose a novel framework for single-view and multi-view 3D object reconstruction, named Pix2Vox++. By using a well-designed encoder-decoder, it generates a coarse 3D volume from each input image. A multi-scale context-aware fusion module is then introduced to adaptively select high-quality reconstructions for different parts from all coarse 3D volumes to obtain a fused 3D volume. To further correct the wrongly recovered parts in the fused 3D volume, a refiner is adopted to generate the final output. Experimental results on the ShapeNet, Pix3D, and Things3D benchmarks show that Pix2Vox++ performs favorably against state-of-the-art methods in terms of both accuracy and efficiency.																	0920-5691	1573-1405				DEC	2020	128	12					2919	2935		10.1007/s11263-020-01347-6		JUL 2020											
J								An Optimization Method for the Initial Parameters Selection of Fuzzy Cerebellar Model Neural Networks in Parametric Fault Diagnosis	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Fault diagnosis; Fuzzy cerebellar model neural network (FCMNN); Genetic algorithm (GA); Parameter optimization		When the initial parameters of the fuzzy cerebellar model neural network (FCMNN) are not properly selected, there is a great possibility that the error function converges to the local minimum region or diverges under the gradient descent back propagation (BP) algorithm, which will affect the classification ability of FCMNN. Aiming at this problem, GA is used to optimize the initial center positions and width of the activation function and weight of FCMNN. After obtaining the optimal initial parameters, the internal parameters of FCMNN can approach the convergence value of minimum error more quickly and accurately through further training of the network, so as to obtain better network learning performance. The introduction of GA to optimize the initial value of FCMNN can effectively reduce the blindness and time cost of manual selection of initial parameters, and further improve the intelligence of neural network diagnosers. The simulation and experimental results show that the classification ability of the GA-FCMNN and GA can effectively find the optimal combination in the set data domain.																	1562-2479	2199-3211				OCT	2020	22	7					2071	2082		10.1007/s40815-020-00908-8		JUL 2020											
J								Finite-Time Adaptive Fuzzy DSC for Uncertain Switched Systems	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Uncertain switched systems; Fuzzy approximation; Adaptive DSC technique; Finite time stability	NONLINEAR-SYSTEMS; TRACKING CONTROL; OUTPUT-FEEDBACK; NEURAL-NETWORKS; STABILIZATION; STABILITY; BOUNDEDNESS	The finite-time adaptive fuzzy tracking control problem for a class of strict-feedback uncertain switched systems is investigated in this paper. Based on fuzzy approximation and adaptive dynamic surface control (DSC) technique, a finite-time adaptive state feedback fuzzy controller is developed via the common Lyapunov functions. Different from the existing works on uncertain switched systems, the DSC control scheme is developed based on a nonlinear filter to solve the "explosion of complexity" problem, and the structure of the proposed fuzzy controller is simple. Under the designed controller, all the signals of the closed-loop system remain semi-globally bounded, and within a finite-time interval, the system tracking error converges to an arbitrarily small region. That is, the semi-globally practical finite-time stability of the controlled system is guaranteed. To show the availability of the presented control scheme, a simulation example is given in this paper.																	1562-2479	2199-3211				OCT	2020	22	7					2258	2270		10.1007/s40815-020-00893-y		JUL 2020											
J								A New Soft Likelihood Function Based onDNumbers in Handling Uncertain Information	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Dnumbers; Soft likelihood functions; Ordered weighted average; Uncertain information fusion; Reliability	ENVIRONMENTAL-IMPACT ASSESSMENT; FUZZY AGGREGATION OPERATORS; DECISION-MAKING; D NUMBERS; FRAMEWORK; EVIDENCES; SELECTION; MODEL	How to effectively deal with uncertain information has traditionally been a concern. TheDnumbers theory overcomes the limitations of Dempster-Shafer theory and further strengthens the ability of uncertainty modeling. Recently, Yager et al. proposed a soft likelihood function which can effectively combine probability information. Related research has enriched and expanded its connotation, but there are still problems to be solved. This paper conducted further research and proposed a new soft likelihood function based onDnumbers. Comparison and discussion illustrate the rationality and superiority of the proposed methodology.																	1562-2479	2199-3211				OCT	2020	22	7					2333	2349		10.1007/s40815-020-00911-z		JUL 2020											
J								Measures of Uncertainty Based on Gaussian Kernel for Type-2 Fuzzy Information Systems	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Rough set; Uncertainty measure; Type-2 fuzzy information system; Fuzzy information structure; Roughness; Rough entropy; Gaussian kernel	ROUGH SET; KNOWLEDGE GRANULATION; GRANULARITY MEASURES; ATTRIBUTE REDUCTION; ENTROPY MEASURES; 2 UNIVERSES; INTERVAL; APPROXIMATION; SELECTION; MODEL	In data processing, measurement of uncertainty is one of the significant evaluation tools, which can describe the uncertainty essence of data. So far, there are few measurable tools to study the uncertainty of type-2 fuzzy information systems (TFISs-2) (the expanded models of fuzzy information systems). This paper is devoted to looking for effective indicators to describe the uncertainty of TFISs-2. The fuzzy T-cos-similarity relations are first introduced, which are generated by TFISs-2 based on Gaussian kernel. Then, the fuzzy information structures are defined on account of this fuzzy T-cos-similarity relation. Next, two measures constructed from the upper and lower approximations are given for TFISs-2, that is, delta-accuracy and delta-roughness, which are used to reflect the degree of accuracy and inaccuracy of information by numerical form. Furthermore, by combining roughness and entropy, the delta-rough entropy is investigated. Finally, the practicability of proposed measures is tested by a numerical experiment. The experimental results show that the delta-rough entropy is efficacious and applicable for TFISs-2.																	1562-2479	2199-3211															10.1007/s40815-020-00895-w		JUL 2020											
J								An Enhanced Technique for Order Preference by Similarity to Ideal Solutions and its Application to Renewable Energy Resources Selection Problem	INTERNATIONAL JOURNAL OF FUZZY SYSTEMS										Enhanced TOPSIS method; Interval type-2 fuzzy projection model; Renewable energy resources selection; Interval type-2 fuzzy sets	GROUP DECISION-MAKING; LINGUISTIC TERM SETS; PROSPECT-THEORY; TOPSIS METHOD; FUZZY; MODEL; METHODOLOGY; EXTENSION; SYSTEMS; VIKOR	Selecting right renewable energy resources (RESs) is emerging as a solution to alleviate energy crisis and environmental pollution. To better address the RESs selection problems, this paper proposes an enhanced Technique for Order Preference by Similarity to Ideal Solutions (TOPSIS). Specifically, the decision information is characterized by linguistic terms and then encoded by interval type-2 fuzzy sets (IT2FSs). The current IT2FSs preprocessing models recklessly transform IT2FSs into crisp numbers, which may discount the superiority of applying fuzzy sets. Hence, we define a novel interval type-2 fuzzy projection model to measure the IT2FSs and some related theorems about the projection model are explored mathematically. Moreover, based on this new interval type-2 fuzzy projection model, an enhanced TOPSIS is proposed to calculate the closeness coefficients of alternatives. Of note, to keep information as much as possible, the obtained closeness coefficients are still IT2FSs. Finally, the Karnik-Mendel (KM) algorithms are employed to compare and rank those closeness coefficients. The effectiveness of the proposed method is demonstrated by a RESs selection case. Comparisons are also conducted to illustrate its advantages.																	1562-2479	2199-3211															10.1007/s40815-020-00914-w		JUL 2020											
J								Increasing fault tolerance ability and network lifetime with clustered pollination in wireless sensor networks	JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING										WSN; Fault tolerance; Revitalization; Management strategy; Network lifetime	OPTIMIZATION; MAXIMIZATION	The significance of wireless sensor network (WSN) applications is to examine dangerous and remote fields that are not reachable or complex or expensive to reach human insights. This feature leads to self-managed networking that may face enormous confronts in energy consumption, fault tolerance and network lifetime based restraints owing certain non-renewable energy resources. From this research, an effectual managing strategy has been provided and methodology to tolerate faults in a network has been anticipated. To model these kinds of framework, fault recognition and revitalization approaches for handling diverse faulty levels have to be considered, i.e. communication and network nodes among them are used. The anticipated management strategy and protocol model may improve network-based fault tolerance ability in network nodes and corresponding transmission among them. As well as, network lifetime is increased for about five times higher than existing methods. Subsequently, it is validated that network fault tolerance proficiency is decreased with 2% and improved network lifetime with 10%. It improves data transmission and reduces energy consumption and increases network lifetime. Simulation is carried out in MATLAB 2018a environment. This model provides the finest results in terms of fault tolerance, network lifetime and energy consumption.																	1868-5137	1868-5145															10.1007/s12652-020-02325-z		JUL 2020											
J								Expectations for agents with goal-driven autonomy	JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE										Goal-driven autonomy; agent's expectations; goal reasoning	COMPLEXITY; STRIPS	Goal-driven autonomy is an agent model for managing a dynamic environment by reasoning about current and potential goals while planning and acting. Since unexpected events and conditions may cause an agent's goals and plans to become invalid or infeasible, an agent with goal-driven autonomy should monitor the environment against its expectations. Designed for dynamic, open, and partially observable environments, such an agent can create new goals or change its existing goals as needed. We present a formalisation of expectations for agents operating in these kinds of environments. Our formalisation includes situations where agents have the capability to sense the environment with some associated costs. We examine agent choices and behaviour in these domains and evaluate multiple approaches for selecting a subset of the agent's sensing actions to execute. The contributions of this work are (1) a specification of different approaches to generating expectations; (2) a formalisation of the autonomy problem that minimises sensing costs; (3) a complexity analysis of the problem; (4) new algorithms for deciding which sensing actions to perform; and (5) empirical results demonstrating the benefit and cost of these approaches.																	0952-813X	1362-3079															10.1080/0952813X.2020.1789755		JUL 2020											
J								Study on computer vision target tracking algorithm based on sparse representation	JOURNAL OF REAL-TIME IMAGE PROCESSING										Visual target tracking; Sparse representation; Base tracking algorithm; The BOMP algorithm; Particle filter	VISUAL TRACKING; ROBUST	Video target tracking covers a variety of interdisciplinary subjects such as pattern recognition, image processing, computer graphics and artificial intelligence. In recent years, visual tracking research methods have made significant progress, and scholars have proposed many excellent algorithms. Based on this, this paper uses the basic tracking algorithm and block orthogonal matching pursuit (BOMP) algorithm of image reconstruction, respectively, from the run time, the quality of reconstruction, reconstruction error of the two algorithms to do simulation experiments, and compare their performance, the results show that the BOMP algorithm running time is short, has extensive application, therefore, to determine the BOMP algorithm as a sparse representation model is the core of the method. Then establish the target observation model, introduce the sparse display into the particle filter framework, and update the sparse representation coefficients so that the norm reaches the optimal solution and ensure the accuracy of the tracking target. Finally, through the simulation experiment, the success rate of the target coverage is calculated. The results show that the BOMP algorithm can maintain high tracking accuracy and strong stability in the case of appearance changes caused by illumination changes, partial occlusion and attitude changes.																	1861-8200	1861-8219															10.1007/s11554-020-00999-4		JUL 2020											
J								Artificial intelligence-based novel scheme for location area planning in cellular networks	COMPUTATIONAL INTELLIGENCE										cell attributes-based algorithm; cost per call; location management	MANAGEMENT; OPTIMIZATION; ASSIGNMENT; SWITCHES; CELLS	Planning of location area (LA) in cellular networks plays a vital role in utilizing the resources efficiently and economically. Location update and paging costs directly affect the cost to the service provider. When the size of the Mobile Switching Centre (MSC) is large, the paging cost becomes maximum. Though, no location update is required for this case. On the contrary, paging cost shoots high if each cell forms individual LA, but in this case, location update cost shoots to the maximum value. Therefore, deducing the network to the optimal amount of LA's is an intractable combinatorial optimization problem. In this paper, we intend to optimize the partitions of the MSC service area into an optimal number of LA, to curtail the total location management cost. We propose a novel scheme bearing in mind cell attributes of the network to deduce an optimum number of location areas, leading to minimum cost per call. It is revealed by the results of this work, attained after simulating the scenario on MATLAB 2018a that Cell Attributes Based Algorithm is a very promising scheme for cellular networks, and it outperforms the existing algorithms in a practical scenario.																	0824-7935	1467-8640															10.1111/coin.12371		JUL 2020											
J								A chemometric approach to compare Portuguese native hops with worldwide commercial varieties	JOURNAL OF CHEMOMETRICS										correspondence analysis; hierarchical clustering on principal components; Humulus lupulus; robust PCA; sensory analysis; volatile profile	HUMULUS-LUPULUS L.; THAT-APPLY CATA; MASS-SPECTROMETRY; VARIABILITY; FRACTION; FLAVOR; AROMA; TOFMS	A diversity of native hops can be found in Portugal, but little is known concerning their volatile and sensory profiles. Nowadays, the exponential growth of the craft beer sector and the preference for more flavoured beers promote the research of unexplored wild hops that have the advantage of being well adapted to the Portuguese edaphoclimatic conditions. Therefore, the goal of this study was to characterize the volatile profile of 75 native Portuguese hops and compare with 34 commercial varieties by means of headspace solid-phase microextraction gas chromatography/mass spectrometry (HS-SPME-GC/MS), in order to select those that present similarities with commercialized hops and confirm by check-all-that-apply (CATA) analysis if they present similar organoleptic characteristics. Due to the complexity of hop volatile profile and the great number of samples analysed, robust chemometric treatment of chromatographic and sensorial data was required to make reliable conclusions. Twelve Portuguese hops present a volatile profile and sensory characteristics quite similar to some commercial varieties, because 11 Portuguese hops were grouped with the European varieties Challenger, Hallertauer Magnum and Perle, both in volatile profile and sensory analysis and one clustered with American registered varieties.																	0886-9383	1099-128X				SEP	2020	34	9							e3285	10.1002/cem.3285		JUL 2020											
J								HydLoc: A tool for hydroxyproline and hydroxylysine sites prediction in the human proteome	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Hydroxylation; Post-translational modification; Prediction; Proteome; Random forest	COMPUTATIONAL PREDICTION; IDENTIFICATION; BIOLOGY	As a kind of post-translational modifications, hydroxylation drew less attention than other modifications, such as phosphorylation and acetylation. However, besides protein stability regulation, it has been found that hydroxylation may affect the activity of proteins. Therefore, it is necessary to better understand the biological processes of hydroxylation. Identification of hydroxylated substrates and their corresponding sites is important for the studies of its molecular mechanism. Fast and convenient computational methods for hydroxylation sites identification are much desired, because experimental approaches are time-consuming and labor-intensive. Here, we present HydLoc (Hydroxylation sites Location), a random forest-based hydroxylation sites predictor for human proteins using sequential information and physicochemical properties. The accuracies of leave-one-out cross-validation on the training dataset are 84.25% and 80.61% for residue proline (P) and lysine (K), respectively. Based on the independent WA dataset, it achieved an accuracy of 90.74% and 81.25% for P and K hydroxylation sites prediction, respectively. Meanwhile, the sensitivity values of 96.29% and 75.00% were obtained for residue P and K, which outperforms the existing methods. A user-friendly web server of HydLoc is now available at https://www.gdpu-bioinfolab.com/hydloc/																	0169-7439	1873-3239				JUL 15	2020	202								104035	10.1016/j.chemolab.2020.104035													
J								Pixels of chemical structures correlate to chromatographic detector responses using genetic algorithm-adaptive neuro-fuzzy inference system as a novel nonlinear feature selection method	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Genetic algorithm-adaptive neuro-fuzzy inference system; Multivariate image analysis; Polychlorinated biphenyls; Quantitative structure-(chromatographic) property relationship	PRINCIPAL COMPONENT REGRESSION; MULTIVARIATE IMAGE-ANALYSIS; FLAME IONIZATION; MIA-QSAR; PREDICTION; DERIVATIVES; VALIDATION; NETWORKS; DESIGN	This paper introduces the genetic algorithm-adaptive neuro-fuzzy inference system (GA-ANFIS), as a novel nonlinear feature selection method. This hybrid technique combines genetic algorithms (GAs) as powerful optimization methods with ANFIS as a robust nonlinear statistical method. Multivariate image analysis whose descriptors achieved from bidimensional images coupled to principal component analysis (PCA) and the most significant principal components (PCs) were extracted. Eigen value ranking (EV), correlation ranking (CR), GA-partial least square (GA-PLS) as well the method proposed in this work were used to select the most relevant set of PCs as inputs for ANFIS to assess electron capture detector responses of 207 polychlorinated biphenyls. The results indicated that GA-ANFIS is superior over the others in both selecting the most relevant set of PCs and correlating the inputs (PCs) with the detector responses. The best model was statistically validated for its predictive power using cross-validation, applicability domain and Y-scrambling evaluation procedures. Moreover, the superiority of this model obtained from pixels of chemical structures over the nonlinear one obtained from original molecular descriptors in a previous work indicates that the image analysis is a powerful tool in quantitative structure-(chromatographic) property relationship (QSPR) studies.																	0169-7439	1873-3239				JUL 15	2020	202								104032	10.1016/j.chemolab.2020.104032													
J								Improving discrimination of Raman spectra by optimising preprocessing strategies on the basis of the ability to refine the relationship between variance components	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Signals preprocessing; Regularised MANOVA; Discrimination; Raman spectra; Likelihood ratio	MULTIPLICATIVE SIGNAL CORRECTION; VIBRATIONAL SPECTROSCOPY; BACKGROUND SUBTRACTION; GENETIC ALGORITHMS; FLUORESCENCE; REGRESSION; TRANSFORM; SAMPLES	Discrimination of the samples into predefined groups is the issue at hand in many fields, such as medicine, environmental and forensic studies, etc. Its success strongly depends on the effectiveness of groups separation, which is optimal when the group means are much more distant than the data within the groups, i.e. the variation of the group means is greater than the variation of the data averaged over all groups. The task is particularly demanding for signals (e.g. spectra) as a lot of effort is required to prepare them in a way to uncover interesting features and turn them into more meaningful information that better fits for the purpose of data analysis. The solution can be adequately handled by using preprocessing strategies which should highlight the features relevant for further analysis (e.g. discrimination) by removing unwanted variation, deteriorating effects, such as noise or baseline drift, and standardising the signals. The aim of the research was to develop an automated procedure for optimising the choice of the preprocessing strategy to make it most suitable for discrimination purposes. The authors propose a novel concept to assess the goodness of the preprocessing strategy using the ratio of the between-groups to within-groups variance on the first latent variable derived from regularised MANOVA that is capable of exposing the groups differences for highly multidimensional data. The quest for the best preprocessing strategy was carried out using the grid search and much more efficient genetic algorithm. The adequacy of this novel concept, that remarkably supports the discrimination analysis, was verified through the assessment of the capability of solving two forensic comparison problems - discrimination between differently-aged bloodstains and various car paints described by Raman spectra - using likelihood ratio framework, as a recommended tool for discriminating samples in the forensics.																	0169-7439	1873-3239				JUL 15	2020	202								104029	10.1016/j.chemolab.2020.104029													
J								Various aspects of retention index usage for GC-MS library search: A statistical investigation using a diverse data set	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Gas chromatography-mass spectrometry; Library search; Retention index; Quantitative structure-retention relationships	COMPOUND IDENTIFICATION; MASS; ALGORITHMS; PREDICTION; SPECTRUM; SYSTEM	This work is devoted to the large-scale statistical evaluation of various aspects of using the retention index for GC-MS library search with a diverse data set. A search in a large library often does not give a correct compound even if a library contains it. One of the methods to improve a spectral library search procedure is to use the retention index information. The aim of this study is to explore some statistical peculiarities which can be helpful for development of automated software which uses a library search of diverse completely unknown compounds in a large database. A data set that was used in this work as a source of queries contains similar to 11 thousand spectra of compounds which belong to diverse chemical classes. Six equations for matching reference and experimental "retention index - spectrum" pairs were compared. It was found that good results can be obtained when a linear equation for similarity of pairs is used. Similarity of pairs is found as a sum of spectral similarity and of a product of a negative adjustable weight parameter and the absolute difference between reference and query retention indices. This equation performs equal or better than much more complex equations which contain two instead of one adjustable parameters. Widely used threshold-based approach, when candidates with high retention index deviation are rejected, performs worse than other equations. The use of predicted with neural networks retention indices as reference was also considered. Modern universal retention prediction models which are applicable to a wide variety of compounds are still quite inaccurate comparing with values from databases, but these predicted values allow to improve a library search as well. When predicted retention indices are used as reference, the linear equation for matching "retention index - spectrum" pairs also performs equal or better than other equations. The distribution of differences between query indices and reference indices (both calculated and experimental) was found close to exponential distribution near zero. The dependence of a fraction of correct identifications on the reference retention indices accuracy was studied. The addition of random noise with double exponential distribution to exact values was used to create "reference" retention indices with the predefined accuracy. The use of the molecular mass and molecular formula as additional constraints during a library search was also considered.																	0169-7439	1873-3239				JUL 15	2020	202								104042	10.1016/j.chemolab.2020.104042													
J								A comparison of experimental designs for calibration	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS											LIQUID-LIQUID MICROEXTRACTION; CENTRAL COMPOSITE DESIGN; COMPUTER EXPERIMENTS; INVERSE PREDICTION; EXTRACTION	The impact of experimental design choice on the performance of statistical calibration is largely unknown. Calibration is a technique that uses available experimental data to model the relationship between input and response variables to ultimately infer inputs based on newly observed response values. The purpose of this article is to investigate the performance of several experimental designs with regards to inverse prediction via a comprehensive simulation study. Specifically, we compare several design types including traditional response surface designs, algorithmically generated variance optimal designs, and space-filling designs. Results indicate that the choice of design has an impact on calibration performance and provides overall support for the use of I-optimal designs.																	0169-7439	1873-3239				JUL 15	2020	202								104025	10.1016/j.chemolab.2020.104025													
J								Modeling and optimization of lead and cobalt biosorption from water with Rafsanjan pistachio shell, using experiment based models of ANN and GP, and the grey wolf optimizer	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Biosorption; Heavy metal; Rafsanjan pistachio shell (RPS); Feed-forward neural network (FFNN); Genetic programming (GP); Grey wolf optimization (GWO)	ARTIFICIAL NEURAL-NETWORK; INDUSTRIAL WASTE-WATER; TOXIC HEAVY-METALS; ACTIVATED CARBON; AQUEOUS-SOLUTION; DYE ADSORPTION; BANANA PEELS; REMOVAL; BIOMASS; COPPER	The biosorption of lead and cobalt from an aqueous solution is studied using Rafsanjan pistachio shell (RPS) as a biosorbent. The amount of removed metal depends on four factors including pH of the aqueous solution, initial concentration of metal (C-0), biosorbent dosage (D-B), and temperature (T). An efficient set of experiments is obtained in a lab-scale batch study. Feed-forward neural network (FFNN) and genetic programming (GP) methods are used for process modeling. The FFNN formula is further improved using the grey wolf optimization (GWO) algorithm and it converges to the test observations with regression index (R-2) of 0.9932 and 0.9908 for Pb(II) and Co(II). The GP formula also gives an R-2 value of 0.9657 and 0.9518 for Pb(II) and Co(II) adsorptions respectively. Using the grey wolf optimization (GWO) method proves that at pH = 5, C-0 = 10.2 mg/l, D-B = 0.8 g/l, and T = 25 degrees C, the adsorption of Pb(II) and Co(II) together can be maximized up to 81.5% and 69.4%, respectively.																	0169-7439	1873-3239				JUL 15	2020	202								104041	10.1016/j.chemolab.2020.104041													
J								Classification of honey applying high performance liquid chromatography, near-infrared spectroscopy and chemometrics	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Honey; Chemometrics; High performance liquid chromatography; Near-infrared spectroscopy; Partial least squares discriminant analysis; Data fusion	PHENOLIC-ACIDS; GEOGRAPHICAL ORIGIN; NIR SPECTROSCOPY; FLORAL ORIGIN; OLIVE OIL; HPLC-DAD; DISCRIMINATION; AUTHENTICATION; IDENTIFICATION; EXTRACTION	The potential of Fourier Transform Near-Infrared spectroscopy (FT-NIR) and High-Performance Liquid Chromatography with Diode-Array Detection (HPLC-DAD) in combination with multivariate data analysis was examined to classify 70 honey samples (belonging to 7 different varieties) according to their botanical origin. In the first part of the work, classification was achieved by applying PLS-DA to the individual data blocks: this approach led to promising results from the prediction point of view. In the second part of the study, the multi-block data set has been handled by data-fusion techniques which led to comparable or better results than those obtained by the analysis of individual matrices. These satisfactory results confirm the feasibility of the proposed methodology and encourage the development of similar approaches for honey quality assessment.																	0169-7439	1873-3239				JUL 15	2020	202								104037	10.1016/j.chemolab.2020.104037													
J								Rapid quality control of industrial flocculents using Fourier transform mid-infrared spectra and multivariate analysis	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										ATR-FTIR spectroscopy; Flocculent; Multivariate analysis; Sedimentation rate; Solid rate	INFRARED-SPECTROSCOPY; SELECTION; PREDICTION; REMOVAL; ACIDITY; PLS	This study aimed to use the Attenuated Total Reflectance-Fourier transform mid-infrared (ATR-FTIR) spectroscopy coupled with chemometrics, as an alternative method for the qualitative-quantitative determinations of flocculent quality. To the best of our knowledge, there is no work that has treated the properties of flocculants by direct spectroscopic methods. The quality of the flocculent was ensured by the determination of two parameters namely, sedimentation rate (VS) and solid rate (TS), during the coagulation/flocculation process. Firstly, spectral and reference data of 36 samples from three different lots of flocculent, were analyzed by principal component analysis (PCA), Partial Least Squares Regression (PLSR) and results were used to establish calibration models. PLSR models revealed the best calibration models for predicting VS and TS values for studied samples, with the coefficient of determination (R-2) of 0.99. In addition, cluster analysis (CA) and linear discriminant analysis (LDA) were adopted as differentiation methods of studied samples. On the basis of CA, three distinctive clusters were recognized. The model built by LDA could classify the samples with an accuracy value of 100%. Hence, it can be concluded that ATR-FTIR spectroscopy combined multivariate analysis can be used for the rapid prediction of flocculent quality.																	0169-7439	1873-3239				JUL 15	2020	202								104030	10.1016/j.chemolab.2020.104030													
J								Bioengineering for multiple PAHs degradation for contaminated sediments: Response surface methodology (RSM) and artificial neural network (ANN)	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Polycyclic aromatic hydrocarbons (PAHs); Environmental development; Response surface methodology (RSM); Artificial neural network (ANN); Bioremediation	POLYCYCLIC AROMATIC-HYDROCARBONS; CRUDE-OIL; OPTIMIZATION; BIODEGRADATION; PREDICTION; MODEL; SOIL	Scientific community around the globe have major focus on designing bioremediation strategies for persistent, recalcitrant, highly toxic and carcinogen/mutagen polycyclic aromatic hydrocarbons (PAHs) present in marine environment. For the bioremediation strategy, components of growth medium are a key factor, which enhance degradation of the PAHs through simulating the microbial growth. Thus, present study involves bioengineering of growth medium (ONR7a) using response surface methodology (RSM) and artificial neural network (ANN) for enhanced multiple PAHs biodegradation. Microbes were isolated from contaminated sediments of Alang Sosiya Ship Breaking Yard (ASSBRY), Gulf of Khambhat, Gujarat, India. RSM - a process centric approach has resulted in an increase in PAHs degradation from 69% (Unoptimized) to 90.03% with 1.29 folds increase on 5th day with R-2 value of 0.98. Moreover, use of Artificial Neural Network (ANN) - a data centric approach resulted in better prediction of PAHs degradation of 93.36% compared to the CCD-RSM predicted PAHs degradation of 90.03% with R-2 value of 0.98. Based on various error functions such as mean absolute deviation (MAD), mean squared error (MSE), root mean squared error (RMSE) and mean absolute percentage error (MAPE), the predictive ability of the constructed ANN models was found to be higher compared to RSM. As this is the first ever report on PAHs degradation by bacterial mixed culture using data centric approach, this study bridges the gap between fundamental research and its application for policymakers and stakeholders which would be helpful in designing appropriate bioremediation technologies.																	0169-7439	1873-3239				JUL 15	2020	202								104033	10.1016/j.chemolab.2020.104033													
J								An intelligent computational model for prediction of promoters and their strength via natural language processing	CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS										Promoters; Convolution neural network (CNN); Natural language processing; DNA; word2vec	SEQUENCE-BASED PREDICTOR; RECOMBINATION SPOTS; ENSEMBLE CLASSIFIER; PROTEIN TYPES; IDENTIFICATION; SITES; FEATURES; SPACE; DISCRIMINATION; TRINUCLEOTIDE	In DNA, a promoter is an essential part of genes that controls the transcription of specific genes in a particular tissue or cells. The combination of RNA polymerase and a number of various proteins named "sigma-factors" can define the transcription start site (TSS) by inducing RNA holoenzyme. Further, Promoter is categorized into strong and weak promoters on the basis of promoter strength. Owing to exponential increase of RNA/DNA and protein samples in the post-genomic era, developing a simple and efficient sequential-based intelligent computational model for the discrimination of promoters is a challenging job. An intelligent computational model namely: 2L-iPSW(word2vec) was introduced for discrimination of promoters and their strength, in this regard. Machine learning and Deep learning algorithms in conjunction with natural language processing method i.e., "word2vec" are used. The proposed computational model 2L-iPSW(word2vec) achieved 91.42% of accuracy for 1st layer contains promoters and non-promoters which is 8.29% higher than the existing model, whereas 82.42% of accuracy for 2nd layer identifies strong promoter and weak promoter which is 11.22% advanced than the present model. Proposed 2L-iPSW(word2vec) model obtained efficient success rates than the present models in terms of all assessment metrics. It is thus greatly observed that the 2L-iPSW(word2vec) model will lead a useful tool for academic research on promoter identification.																	0169-7439	1873-3239				JUL 15	2020	202								104034	10.1016/j.chemolab.2020.104034													
J								Adaptive neural finite-time bipartite consensus tracking of nonstrict feedback nonlinear coopetition multi-agent systems with input saturation	NEUROCOMPUTING										Adaptive neural control; Nonstrict feedback nonlinear multi-agent systems; Bipartite consensus; Finite-time control; Input saturation	DYNAMIC SURFACE CONTROL; UNKNOWN DEAD-ZONE; NETWORKS; LEADER	This paper concentrates on the adaptive neural finite-time bipartite consensus tracking of nonstrict feedback nonlinear coopetition multi-agent systems with input saturation. A novel consensus tracking method combined the adaptive neural control with the finite-time command filtered backstepping is proposed. During each backstepping process, the Radical Basis Function Neural Network (RBF NN) is used to approximate the unknown nonlinear dynamics and the finite-time sliding mode differentiator (FTSMD) is used to obtain intermediate signals and their derivative. Moreover, the filtering errors are eliminated by using error compensation signals. By using the finite-time Lyapunov stability theory, it can be proved that the bipartite consensus tracking errors can converge to a sufficient small region of the origin in finite-time and all signals in the closed-loop systems are bounded in finite-time although there exists the input saturation. The effectiveness of the proposed method is shown by simulation results. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 15	2020	397						168	178		10.1016/j.neucom.2020.02.054													
J								Deep octonion networks	NEUROCOMPUTING										Convolutional neural network; Complex; Quaternion; Octonion; Image classification	GLOBAL EXPONENTIAL STABILITY; VALUED NEURAL-NETWORKS; ALGORITHM	Deep learning is a hot research topic in the field of machine learning methods and applications. Real-value neural networks (Real NNs), especially deep real networks (DRNs), have been widely used in many research fields. In recent years, the deep complex networks (DCNs) and the deep quaternion networks (DQNs) have attracted more and more attentions. The octonion algebra, which is an extension of complex algebra and quaternion algebra, can provide more efficient and compact expressions. This paper constructs a general framework of deep octonion networks (DONs) and provides the main building blocks of DONs such as octonion convolution, octonion batch normalization and octonion weight initialization; DONs are then used in image classification tasks for CIFAR-10 and CIFAR-100 data sets. Compared with the DRNs, the DCNs, and the DQNs, the proposed DONs have better convergence and higher classification accuracy. The success of DONs is also explained by multi-task learning. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 15	2020	397						179	191		10.1016/j.neucom.2020.02.053													
J								Learning 3D spatiotemporal gait feature by convolutional network for person identification	NEUROCOMPUTING										3D Gait recognition; Person identification; Deep convolutional neural network; Spatiotemporal gait information	ACTION RECOGNITION	For person identification in non-interaction biometric systems, gait recognition has been recently encouraged in literature and industrial applications instead of face recognition. Although numerous advanced methods that learn object appearance by conventional machine learning models have been discussed in the last decade, most of them are strongly sensitive to scene background motion. In this research, we address the drawbacks of existing works by comprehensively studying gait information from 3D human skeleton data with a deep learning-based identifier. To capture the statistic gait information in the spatial dimension, we first extract the geometric gait features of joint distance and orientation. The dynamic gait information is then obtained by calculating the temporal description features with the mean and standard deviation of geometric features. Accordingly, the fully gait information of an individual is finally learned via a compact Deep Convolutional Neural Network which is explicitly designed with multiple stacks of asymmetric convolutional filters to fully gain the spatial correlation of in-frame body joints and the temporal relation of frame-wise posture at multiple scales. Based on the experimental results evaluated on four benchmark 3D gait datasets commonly used for person identification, including UPCV Gait, UPCV Gait K2, KS20 VisLab Multi-View Kinect Skeleton, and SDUGait, the proposed method presents the superior performance over that of several state-of-the-art approaches while maintaining a low computational capacity. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 15	2020	397						192	202		10.1016/j.neucom.2020.02.048													
J								Explicitly exploiting hierarchical features in visual object tracking	NEUROCOMPUTING										Siamese network; Hybrid tracker; Color histogram		A common drawback of convolutional features based trackers is the incapability of distinguishing tracking targets from distractors, even in the presence of distinct color difference. In this paper, we design a robust hybrid tracker that explicitly combines color features with convolutional features. We design our tracker on the basis of fully convolutional Siamese network (SiamFC) to emphasize the performance promotion by introducing color features instead of using a more advanced network architecture. A novel approach to integrate two score maps from different channels is proposed. Techniques include cropping out the ineffective area and denoising via Gaussian smoothing. Experiments conducted on OTB2015 and VOT2018 benchmarks show the superiority of our hybrid tracker over the original SiamFC. (C) 2020 Published by Elsevier B.V.																	0925-2312	1872-8286				JUL 15	2020	397						203	211		10.1016/j.neucom.2020.02.038													
J								Bipartite consensus control for fractional-order nonlinear multi-agent systems: An output constraint approach	NEUROCOMPUTING										Fractional-order systems; Multi-agent systems; Barrier Lyapunov function; Backstepping nonlinear control; Consensus	BARRIER LYAPUNOV FUNCTIONS; ADAPTIVE BACKSTEPPING CONTROL; LEADER-FOLLOWING CONSENSUS; STRICT-FEEDBACK SYSTEMS; TRACKING; SYNCHRONIZATION; NETWORKS; AGENTS	Bipartite consensus of multiple fractional-order nonlinear systems with output constraints is assessed under signed graph. The agents' model is completely unknown with high-order heterogeneous strict-feedback dynamics and external disturbances, which cover single- and double-integrator integer-order systems as special forms. To ensure the bipartite consensus task, a novel fully distributed controller is developed based on backstepping technique and neuro-adaptive update mechanism. A barrier Lyapunov function is introduced to limit the followers' outputs within the preset bounds. Algebraic graph theory and Lyapunov fractional-order stability theorem are employed to deal with the analysis difficulties caused by the network of fractional-order dynamics. Sufficient conditions on bipartite consensus is established, and it is also shown that all the closed-loop error signals are uniformly ultimately bounded. The simulation results are carried out to demonstrate the effectiveness of the proposed approach. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 15	2020	397						212	223		10.1016/j.neucom.2020.02.036													
J								Rumor events detection enhanced by encoding sentimental information into time series division and word representations	NEUROCOMPUTING										Rumor events detection; Sentiment dictionary; Dynamic time series; Cascaded gated recurrent unit; Online social networks	OPTIMIZATION; NETWORK	Online Social Networks (OSNs) is an ideal place for spreading rumor events as it is convenient in information production and dissemination. Automatically debunking these rumor events is important to pursue and restore the truth. However, it is a challenging task to employ traditional classification approaches for rumor events detection since they rely on hand-crafted features that require daunting manual efforts. Besides, we observe that the various posts of each rumor event will debate its realness over time. Different individuals also have different emotional reactions to events, which will affect others' identification. Thus, this paper firstly employs an automatic construction method to develop a Sentiment Dictionary (SD) to capture the fine-grained human emotional reactions to different events. Secondly, a Two-steps Dynamic Time Series (TsDTS) algorithm, involving the sentimental information in the division process, is elaborated to retain the time-span distribution information of microblog events in a natural manner. At last, a novel two-layer Cascaded Gated Recurrent Unit (CGRU) model based on the SD and the TsDTS algorithm is proposed for rumor events detection, named as SD-TsDTS-CGRU. Experimental results on real datasets from OSNs demonstrate that our proposed SD-TsDTS-CGRU model outperforms the latest rumor events detection algorithms. (C) 2020 Published by Elsevier B.V.																	0925-2312	1872-8286				JUL 15	2020	397						224	243		10.1016/j.neucom.2020.01.095													
J								Distributed time-varying group formation control for generic linear systems with observer-based protocols	NEUROCOMPUTING										Time-varying group formation; Multi-agent systems; Generic linear dynamics; Observer-based protocol	NONHOLONOMIC MOBILE ROBOTS; MULTIAGENT SYSTEMS; NETWORKS; TRACKING	In this paper, the time-varying group formation control for linear multi-agent systems under directed communication topology is investigated from an observer viewpoint. Different from the existing works on the time-varying group formation, the groups herein could have a cyclic partition, which is more common in real applications than the topology with the acyclic groups. The leaderless time-varying group formation problem is studied first. An observer-based distributed protocol is presented for each agent, where the observer is used to estimate the unmeasurable state utilizing the output information. Then, to broaden the scope of applications, we further study the leader-following case, in which there exists a leader with nonzero and bounded input for each subgroup. To tackle this problem, we take the input of the leader as a disturbance, and develop the new forms of control protocols with nonlinear functions. Furthermore, for both cases, it is shown that under the formation feasible conditions, the desired time-varying group formation can be achieved if the strong enough intra-group coupling is selected and the corresponding digraph of each subgraph contains a directed spanning tree. Finally, two simulation examples are given. (C) 2020 Published by Elsevier B.V.																	0925-2312	1872-8286				JUL 15	2020	397						244	252		10.1016/j.neucom.2020.01.065													
J								l(1/2)-based penalized clustering with half thresholding algorithm	NEUROCOMPUTING										Half thresholding algorithm; l(1/2) regularization; Penalized clustering	L-1/2 REGULARIZATION; CONVERGENCE	Clustering is a widely applied method in data analysis. As a novel framework of clustering analysis, penalized clustering bases itself on the sparsity of solution, which contributes to its ability of determining the best number of clusters automatically rather than specified in advance. Moreover, l(1/2) regularization has been recognized extensively in recent studies. Compared with other l(p) (0 < p < 1) regularization, it can always obtain sparser solution. Motivated by these two points, we propose a l(1/2)-basedpenalized clustering model, and further transform it into a more general form by introducing the vector comprising pairwise differences between centroids and an auxiliary transfer matrix composed of identity matrixes and null matrixes. Finally, the transformed model is solved by the efficient half thresholding algorithm, which can not only obtain an exact analytic expression of l(1/2) solutions, but also provide an effective parameter selection strategy. We also prove the convergence of the proposed half thresholding algorithm solving l(1/2)-based penalized clustering model. Lastly but not least importantly, benchmark experiments on both synthesis and real data sets from UCI have been conducted to prove the superiority of the proposed method. (C) 2020 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 15	2020	397						253	263		10.1016/j.neucom.2020.01.058													
J								Efficient conformal predictor ensembles	NEUROCOMPUTING										Conformal prediction; Classification; Ensembles		In this paper, we study a generalization of a recently developed strategy for generating conformal predictor ensembles: out-of-bag calibration. The ensemble strategy is evaluated, both theoretically and empirically, against a commonly used alternative ensemble strategy, bootstrap conformal prediction, as well as common non-ensemble strategies. A thorough analysis is provided of out-of-bag calibration, with respect to theoretical validity, empirical validity (error rate), efficiency (prediction region size) and p-value stability (the degree of variance observed over multiple predictions for the same object). Empirical results show that out-of-bag calibration displays favorable characteristics with regard to these criteria, and we propose that out-of-bag calibration be adopted as a standard method for constructing conformal predictor ensembles. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 15	2020	397						266	278		10.1016/j.neucom.2019.07.113													
J								Multi-level conformal clustering: A distribution-free technique for clustering and anomaly detection	NEUROCOMPUTING										Clustering; Conformal prediction; Dendrograms		In this work we present a clustering technique called multi-level conformal clustering (MLCC). The technique is hierarchical in nature because it can be performed at multiple significance levels which yields greater insight into the data than performing it at just one level. We describe the theoretical underpinnings of MLCC, compare and contrast it with the hierarchical clustering algorithm, and then apply it to real world datasets to assess its performance. There are several advantages to using MLCC over more classical clustering techniques: Once a significance level has been set, MLCC is able to automatically select the number of clusters. Furthermore, thanks to the conformal prediction framework the resulting clustering model has a clear statistical meaning without any assumptions about the distribution of the data. This statistical robustness also allows us to perform clustering and anomaly detection simultaneously. Moreover, due to the flexibility of the conformal prediction framework, our algorithm can be used on top of many other machine learning algorithms. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 15	2020	397						279	291		10.1016/j.neucom.2019.07.114													
J								Computationally efficient versions of conformal predictive distributions	NEUROCOMPUTING										Conformal prediction; Cross-conformal prediction; Inductive conformal prediction; Predictive distributions; Split conformal prediction; Regression		Conformal predictive systems are a recent modification of conformal predictors that output, in regression problems, probability distributions for labels of test observations rather than set predictions. The extra information provided by conformal predictive systems may be useful, e.g., in decision making problems. Conformal predictive systems inherit the relative computational inefficiency of conformal predictors. In this paper we discuss two computationally efficient versions of conformal predictive systems, which we call split conformal predictive systems and cross-conformal predictive systems. The main advantage of split conformal predictive systems is their guaranteed validity, whereas for cross-conformal predictive systems validity only holds empirically and in the absence of excessive randomization. The main advantage of cross-conformal predictive systems is their greater predictive efficiency. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 15	2020	397						292	308		10.1016/j.neucom.2019.10.110													
J								Conformal Feature-Selection Wrappers and ensembles for negative-transfer avoidance	NEUROCOMPUTING										Instance transfer; Conformal prediction; Feature Selection; Wrappers; Ensembles	STANDARD MEDICAL THERAPY; CONGESTIVE-HEART-FAILURE; ELDERLY-PATIENTS; TRIAL	In this paper we propose two methods for instance transfer based on conformal prediction. As a distinctive character, both of the methods are model independent and combine feature selection and source-instance selection to avoid negative transfer. The methods have been tested experimentally for different types of classification model on several benchmark data sets. The experimental results demonstrate that the new methods are capable of outperforming significantly standard instance transfer methods. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 15	2020	397						309	319		10.1016/j.neucom.2019.09.105													
J								Transfer learning extensions for the probabilistic classification vector machine	NEUROCOMPUTING										Transfer learning; Probabilistic classification vector machine; Transfer kernel learning; Nystrom approximation; Basis transfer; Sparsity	NYSTROM METHOD; KERNEL	Transfer learning is focused on the reuse of supervised learning models in a new context. Prominent applications can be found in robotics, image processing or web mining. In these fields, the learning scenarios are naturally changing but often remain related to each other motivating the reuse of existing supervised models. Current transfer learning models are neither sparse nor interpretable. Sparsity is very desirable if the methods have to be used in technically limited environments and interpretability is getting more critical due to privacy regulations. In this work, we propose two transfer learning extensions integrated into the sparse and interpretable probabilistic classification vector machine. They are compared to standard benchmarks in the field and show their relevance either by sparsity or performance improvements. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 15	2020	397						320	330		10.1016/j.neucom.2019.09.104													
J								Audio-visual domain adaptation using conditional semi-supervised Generative Adversarial Networks	NEUROCOMPUTING										Domain adaptation; Conformal prediction; Generative adversarial; Networks	FACIAL EXPRESSION RECOGNITION	Accessing large, manually annotated audio databases in an effort to create robust models for emotion recognition is a notably difficult task, handicapped by the annotation cost and label ambiguities. On the contrary, there are plenty of publicly available datasets for emotion recognition which are based on facial expressivity due to the prevailing role of computer vision in deep learning research, nowadays. Thereby, in the current work, we performed a study on cross-modal transfer knowledge between audio and facial modalities within the emotional context. More concretely, we investigated whether facial information from videos could be used to boost the awareness and the prediction tracking of emotions in audio signals. Our approach was based on a simple hypothesis: that the emotional state's content of a person's oral expression correlates with the corresponding facial expressions. Research in the domain of cognitive psychology was affirmative to our hypothesis and suggests that visual information related to emotions fused with the auditory signal is used from humans in a cross-modal integration schema to better understand emotions. In this regard, a method called dacssGAN (which stands for Domain Adaptation Conditional Semi-Supervised Generative Adversarial Networks) is introduced in this work, in an effort to bridge these two inherently different domains. Given as input the source domain (visual data) and some conditional information that is based on inductive conformal prediction, the proposed architecture generates data distributions that are as close as possible to the target domain (audio data). Through experimentation, it is shown that classification performance of an expanded dataset using real audio enhanced with generated samples produced using dacssGAN (50.29% and 48.65%) outperforms the one obtained merely using real audio samples (49.34% and 46.90%) for two publicly available audio-visual emotion datasets. (C) 2019 The Authors. Published by Elsevier B.V.																	0925-2312	1872-8286				JUL 15	2020	397						331	344		10.1016/j.neucom.2019.09.106													
J								Gaussian process classification for variable fidelity data	NEUROCOMPUTING										Gaussian process classification; Variable fidelity data; Laplace inference	APPROXIMATIONS; OUTPUT; MODEL	In this paper we address a classification problem where two sources of labels with different levels of fidelity are available. Our approach is to combine data from both sources by applying a co-kriging schema on latent functions, which allows the model to account item-dependent labeling discrepancy. We provide an extension of Laplace inference for Gaussian process classification, that takes into account multi-fidelity data. We evaluate the proposed method on real and synthetic datasets and show that it is more resistant to different levels of discrepancy between sources than other approaches for data fusion. Our method can provide accuracy/cost trade-offfor a number of practical tasks such as crowd-sourced data annotation and feasibility regions construction in engineering design. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 15	2020	397						345	355		10.1016/j.neucom.2019.10.111													
J								Adaptive hedging under delayed feedback	NEUROCOMPUTING										Hedging; Decision-theoretic online learning; Experts problem; Delayed feedback; Adaptive algorithms; Non-replicating algorithms; Adversarial setting	PREDICTION; GAME	The article is devoted to investigating the application of hedging strategies to online expert weight allocation under delayed feedback. As the main result we develop the General Hedging algorithm G based on the exponential reweighing of experts' losses. We build the artificial probabilistic framework and use it to prove the adversarial loss bounds for the algorithm G in the delayed feedback setting. The designed algorithm G can be applied to both countable and continuous sets of experts. We also show how algorithm G extends classical Hedge (Multiplicative Weights) and adaptive Fixed Share algorithms to the delayed feedback and derive their regret bounds for the delayed setting by using our main result. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 15	2020	397						356	368		10.1016/j.neucom.2019.06.106													
J								Universal algorithms for multinomial logistic regression under Kullback-Leibler game	NEUROCOMPUTING										Online learning; Sequential prediction; Competitive prediction; Universal algorithms; Loss bounds; Kullback-Leibler game		We consider the framework of competitive prediction, where one provides guarantees compared to other predictive models that are called experts. We propose a universal algorithm predicting finite-dimensional distributions, i.e. points from a simplex, under Kullback-Leibler game. In the standard framework for prediction with expert advice, the performance of the learner is measured by means of the cumulative loss. In this paper we consider a generalisation of this setting and discount losses with time. A natural choice of predictors for the probability games is a class of multinomial logistic regression functions as they output a distribution that lies inside a probability simplex. We consider the class of multinomial logistic regressions to be our experts. We provide a strategy that allows us to 'track the best expert' of this type and derive the theoretical bound on the discounted loss of the strategy. We provide the kernelized version of our algorithm, which competes with a wider set of experts from Reproducing Kernel Hilbert Space (RKHS) and prove a theoretical guarantee for the kernelized strategy. We carry out experiments on three data sets and compare the cumulative losses of our algorithm and multinomial logistic regression. (C) 2019 Elsevier B.V. All rights reserved.																	0925-2312	1872-8286				JUL 15	2020	397						369	380		10.1016/j.neucom.2019.06.105													
